<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-08</h1>
<h3>Title: CageDroneRF: A Large-Scale RF Benchmark and Toolkit for Drone Perception</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Rostami, Atik Faysal, Hongtao Xia, Hadi Kasasbeh, Ziang Gao, Huaxia Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03302">https://arxiv.org/abs/2601.03302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03302">https://arxiv.org/pdf/2601.03302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03302]] CageDroneRF: A Large-Scale RF Benchmark and Toolkit for Drone Perception(https://arxiv.org/abs/2601.03302)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present CageDroneRF (CDRF), a large-scale benchmark for Radio-Frequency (RF) drone detection and identification built from real-world captures and systematically generated synthetic variants. CDRF addresses the scarcity and limited diversity of existing RF datasets by coupling extensive raw recordings with a principled augmentation pipeline that (i) precisely controls Signal-to-Noise Ratio (SNR), (ii) injects interfering emitters, and (iii) applies frequency shifts with label-consistent bounding-box transformations for detection. This dataset spans a wide range of contemporary drone models, many unavailable in current public datasets, and acquisition conditions, derived from data collected at the Rowan University campus and within a controlled RF-cage facility. CDRF is released with interoperable open-source tools for data generation, preprocessing, augmentation, and evaluation that also operate on existing public benchmarks. CDRF enables standardized benchmarking for classification, open-set recognition, and object detection, supporting rigorous comparisons and reproducible pipelines. By releasing this comprehensive benchmark and tooling, CDRF aims to accelerate progress toward robust, generalizable RF perception models.</li>
<li><strong>摘要：</strong>我们推出了 CageDroneRF (CDRF)，这是一个基于现实世界捕获和系统生成的合成变体构建的射频 (RF) 无人机检测和识别的大型基准。 CDRF 通过将大量原始记录与原则性增强管道相结合，解决了现有 RF 数据集的稀缺性和有限多样性问题，该增强管道 (i) 精确控制信噪比 (SNR)，(ii) 注入干扰发射器，以及 (iii) 通过标签一致的边界框变换应用频移进行检测。该数据集涵盖了广泛的当代无人机模型，其中许多在当前公共数据集中不可用，以及从罗文大学校园和受控射频笼设施内收集的数据得出的采集条件。 CDRF 与可互操作的开源工具一起发布，用于数据生成、预处理、增强和评估，这些工具也可在现有公共基准上运行。 CDRF 支持分类、开放集识别和对象检测的标准化基准测试，支持严格的比较和可重复的流程。通过发布这一综合基准测试和工具，CDRF 旨在加速构建稳健、通用的射频感知模型。</li>
</ul>

<h3>Title: Mass Concept Erasure in Diffusion Models with Concept Hierarchy</h3>
<ul>
<li><strong>Authors: </strong>Jiahang Tu, Ye Li, Yiming Wu, Hanbin Zhao, Chao Zhang, Hui Qian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03305">https://arxiv.org/abs/2601.03305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03305">https://arxiv.org/pdf/2601.03305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03305]] Mass Concept Erasure in Diffusion Models with Concept Hierarchy(https://arxiv.org/abs/2601.03305)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The success of diffusion models has raised concerns about the generation of unsafe or harmful content, prompting concept erasure approaches that fine-tune modules to suppress specific concepts while preserving general generative capabilities. However, as the number of erased concepts grows, these methods often become inefficient and ineffective, since each concept requires a separate set of fine-tuned parameters and may degrade the overall generation quality. In this work, we propose a supertype-subtype concept hierarchy that organizes erased concepts into a parent-child structure. Each erased concept is treated as a child node, and semantically related concepts (e.g., macaw, and bald eagle) are grouped under a shared parent node, referred to as a supertype concept (e.g., bird). Rather than erasing concepts individually, we introduce an effective and efficient group-wise suppression method, where semantically similar concepts are grouped and erased jointly by sharing a single set of learnable parameters. During the erasure phase, standard diffusion regularization is applied to preserve denoising process in unmasked regions. To mitigate the degradation of supertype generation caused by excessive erasure of semantically related subtypes, we propose a novel method called Supertype-Preserving Low-Rank Adaptation (SuPLoRA), which encodes the supertype concept information in the frozen down-projection matrix and updates only the up-projection matrix during erasure. Theoretical analysis demonstrates the effectiveness of SuPLoRA in mitigating generation performance degradation. We construct a more challenging benchmark that requires simultaneous erasure of concepts across diverse domains, including celebrities, objects, and pornographic content.</li>
<li><strong>摘要：</strong>扩散模型的成功引起了人们对不安全或有害内容生成的担忧，从而催生了概念擦除方法，这些方法可以微调模块以抑制特定概念，同时保留一般生成能力。然而，随着擦除概念数量的增长，这些方法通常变得低效且无效，因为每个概念都需要一组单独的微调参数，并且可能会降低整体生成质量。在这项工作中，我们提出了一种超类型-子类型概念层次结构，它将被删除的概念组织成父子结构。每个被删除的概念都被视为子节点，语义相关的概念（例如金刚鹦鹉和秃头鹰）被分组在共享父节点下，称为超类型概念（例如鸟）。我们引入了一种有效且高效的分组抑制方法，而不是单独擦除概念，其中通过共享一组可学习参数来对语义相似的概念进行分组和联合擦除。在擦除阶段，应用标准扩散正则化以保留未掩蔽区域中的去噪过程。为了减轻由于过度擦除语义相关子类型而导致的超类型生成的退化，我们提出了一种称为超类型保留低秩适应（SuPLoRA）的新方法，它将超类型概念信息编码在冻结的下投影矩阵中，并在擦除期间仅更新上投影矩阵。理论分析证明了 SuPLoRA 在缓解发电性能下降方面的有效性。我们构建了一个更具挑战性的基准，需要同时删除不同领域的概念，包括名人、物体和色情内容。</li>
</ul>

<h3>Title: Aligning Findings with Diagnosis: A Self-Consistent Reinforcement Learning Framework for Trustworthy Radiology Reporting</h3>
<ul>
<li><strong>Authors: </strong>Kun Zhao, Siyuan Dai, Pan Wang, Jifeng Song, Hui Ji, Chenghua Lin, Liang Zhan, Haoteng Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03321">https://arxiv.org/abs/2601.03321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03321">https://arxiv.org/pdf/2601.03321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03321]] Aligning Findings with Diagnosis: A Self-Consistent Reinforcement Learning Framework for Trustworthy Radiology Reporting(https://arxiv.org/abs/2601.03321)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have shown strong potential for radiology report generation, yet their clinical translation is hindered by architectural heterogeneity and the prevalence of factual hallucinations. Standard supervised fine-tuning often fails to strictly align linguistic outputs with visual evidence, while existing reinforcement learning approaches struggle with either prohibitive computational costs or limited exploration. To address these challenges, we propose a comprehensive framework for self-consistent radiology report generation. First, we conduct a systematic evaluation to identify optimal vision encoder and LLM backbone configurations for medical imaging. Building on this foundation, we introduce a novel "Reason-then-Summarize" architecture optimized via Group Relative Policy Optimization (GRPO). This framework restructures generation into two distinct components: a think block for detailed findings and an answer block for structured disease labels. By utilizing a multi-dimensional composite reward function, we explicitly penalize logical discrepancies between the generated narrative and the final diagnosis. Extensive experiments on the MIMIC-CXR benchmark demonstrate that our method achieves state-of-the-art performance in clinical efficacy metrics and significantly reduces hallucinations compared to strong supervised baselines.</li>
<li><strong>摘要：</strong>多模态大语言模型 (MLLM) 在生成放射学报告方面显示出强大的潜力，但其临床翻译却受到架构异质性和事实幻觉盛行的阻碍。标准的监督微调通常无法严格使语言输出与视觉证据保持一致，而现有的强化学习方法要么面临过高的计算成本，要么面临有限的探索。为了应对这些挑战，我们提出了一个用于生成自洽放射学报告的综合框架。首先，我们进行系统评估，以确定用于医学成像的最佳视觉编码器和 LLM 主干配置。在此基础上，我们引入了一种新颖的“推理然后总结”架构，通过组相对策略优化（GRPO）进行优化。该框架将生成重组为两个不同的组成部分：用于详细发现的思考块和用于结构化疾病标签的答案块。通过利用多维复合奖励函数，我们明确惩罚生成的叙述与最终诊断之间的逻辑差异。 MIMIC-CXR 基准的大量实验表明，我们的方法在临床疗效指标方面实现了最先进的性能，并且与强监督基线相比，显着减少了幻觉。</li>
</ul>

<h3>Title: Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views</h3>
<ul>
<li><strong>Authors: </strong>Xiang Zhang, Yang Zhang, Lukas Mehl, Markus Gross, Christopher Schroers</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03362">https://arxiv.org/abs/2601.03362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03362">https://arxiv.org/pdf/2601.03362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03362]] Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views(https://arxiv.org/abs/2601.03362)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.</li>
<li><strong>摘要：</strong>软边界（如细细的头发）通常在自然和计算机生成的图像中观察到，但由于前景和背景线索的模糊混合，它们对于 3D 视觉仍然具有挑战性。本文介绍了头发守护者 (HairGuard)，这是一个旨在恢复 3D 视觉任务中细粒度软边界细节的框架。具体来说，我们首先提出了一种新颖的数据管理管道，该管道利用图像抠图数据集进行训练并设计深度固定器网络来自动识别软边界区域。通过门控残差模块，深度修复器可以在软边界周围精确地细化深度，同时保持全局深度质量，从而实现与最先进的深度模型的即插即用集成。对于视图合成，我们执行基于深度的前向扭曲以保留高保真纹理，然后使用生成场景绘制器填充被遮挡的区域并消除软边界内的冗余背景伪影。最后，颜色融合器自适应地结合扭曲和修复的结果，以产生具有一致几何形状和细粒度细节的新颖视图。大量的实验表明，HairGuard 在单目深度估计、立体图像/视频转换和新颖的视图合成方面实现了最先进的性能，并且在软边界区域方面有了显着的改进。</li>
</ul>

<h3>Title: SIGMA: Scalable Spectral Insights for LLM Collapse</h3>
<ul>
<li><strong>Authors: </strong>Yi Gu, Lingyou Pang, Xiangkun Ye, Tianyu Wang, Jianyu Lin, Carey E. Priebe, Alexander Aue</a></li>
<li><strong>Subjects: </strong>cs.LG, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03385">https://arxiv.org/abs/2601.03385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03385">https://arxiv.org/pdf/2601.03385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03385]] SIGMA: Scalable Spectral Insights for LLM Collapse(https://arxiv.org/abs/2601.03385)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid adoption of synthetic data for training Large Language Models (LLMs) has introduced the technical challenge of "model collapse"-a degenerative process where recursive training on model-generated content leads to a contraction of distributional variance and representational quality. While the phenomenology of collapse is increasingly evident, rigorous methods to quantify and predict its onset in high-dimensional spaces remain elusive. In this paper, we introduce SIGMA (Spectral Inequalities for Gram Matrix Analysis), a unified framework that benchmarks model collapse through the spectral lens of the embedding Gram matrix. By deriving and utilizing deterministic and stochastic bounds on the matrix's spectrum, SIGMA provides a mathematically grounded metric to track the contraction of the representation space. Crucially, our stochastic formulation enables scalable estimation of these bounds, making the framework applicable to large-scale foundation models where full eigendecomposition is intractable. We demonstrate that SIGMA effectively captures the transition towards degenerate states, offering both theoretical insights into the mechanics of collapse and a practical, scalable tool for monitoring the health of recursive training pipelines.</li>
<li><strong>摘要：</strong>快速采用合成数据来训练大型语言模型（LLM）带来了“模型崩溃”的技术挑战，这是一种退化过程，对模型生成的内容进行递归训练会导致分布方差和表征质量的收缩。尽管崩溃的现象越来越明显，但量化和预测其在高维空间中发生的严格方法仍然难以捉摸。在本文中，我们介绍了 SIGMA（格拉姆矩阵分析的谱不等式），这是一个统一框架，通过嵌入格拉姆矩阵的谱透镜对模型崩溃进行基准测试。通过导出和利用矩阵谱上的确定性和随机界限，SIGMA 提供了一种基于数学的度量来跟踪表示空间的收缩。至关重要的是，我们的随机公式能够对这些边界进行可扩展的估计，使该框架适用于难以进行完整特征分解的大规模基础模型。我们证明，SIGMA 有效地捕获了向简并状态的转变，提供了对崩溃机制的理论见解，以及用于监控递归训练管道健康状况的实用、可扩展的工具。</li>
</ul>

<h3>Title: VNU-Bench: A Benchmarking Dataset for Multi-Source Multimodal News Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Zibo Liu, Muyang Li, Zhe Jiang, Shigang Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03434">https://arxiv.org/abs/2601.03434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03434">https://arxiv.org/pdf/2601.03434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03434]] VNU-Bench: A Benchmarking Dataset for Multi-Source Multimodal News Video Understanding(https://arxiv.org/abs/2601.03434)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>News videos are carefully edited multimodal narratives that combine narration, visuals, and external quotations into coherent storylines. In recent years, there have been significant advances in evaluating multimodal large language models (MLLMs) for news video understanding. However, existing benchmarks largely focus on single-source, intra-video reasoning, where each report is processed in isolation. In contrast, real-world news consumption is inherently multi-sourced: the same event is reported by different outlets with complementary details, distinct narrative choices, and sometimes conflicting claims that unfold over time. Robust news understanding, therefore, requires models to compare perspectives from different sources, align multimodal evidence across sources, and synthesize multi-source information. To fill this gap, we introduce VNU-Bench, the first benchmark for multi-source, cross-video understanding in the news domain. We design a set of new question types that are unique in testing models' ability of understanding multi-source multimodal news from a variety of different angles. We design a novel hybrid human-model QA generation process that addresses the issues of scalability and quality control in building a large dataset for cross-source news understanding. The dataset comprises 429 news groups, 1,405 videos, and 2,501 high-quality questions. Comprehensive evaluation of both closed- and open-source multimodal models shows that VNU-Bench poses substantial challenges for current MLLMs.</li>
<li><strong>摘要：</strong>新闻视频是经过精心编辑的多模式叙事，将旁白、视觉效果和外部引用结合成连贯的故事情节。近年来，在评估新闻视频理解的多模态大语言模型（MLLM）方面取得了重大进展。然而，现有的基准主要集中于单一来源的视频内推理，其中每个报告都是单独处理的。相比之下，现实世界的新闻消费本质上是多源的：同一事件由不同的媒体报道，其细节互补，叙事选择不同，有时随着时间的推移，说法会相互矛盾。因此，强大的新闻理解需要模型来比较不同来源的观点，调整跨来源的多模式证据，并综合多源信息。为了填补这一空白，我们推出了 VNU-Bench，这是新闻领域第一个多源、跨视频理解的基准测试。我们设计了一组独特的新问题类型，测试模型从不同角度理解多源多模态新闻的能力。我们设计了一种新颖的混合人类模型 QA 生成过程，该过程解决了构建跨源新闻理解的大型数据集时的可扩展性和质量控制问题。该数据集包含 429 个新闻组、1,405 个视频和 2,501 个高质量问题。对闭源和开源多模态模型的综合评估表明，VNU-Bench 给当前的 MLLM 带来了巨大的挑战。</li>
</ul>

<h3>Title: FROST-Drive: Scalable and Efficient End-to-End Driving with a Frozen Vision Encoder</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Dong, Yimin Zhu, Yu Wu, Yu Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03460">https://arxiv.org/abs/2601.03460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03460">https://arxiv.org/pdf/2601.03460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03460]] FROST-Drive: Scalable and Efficient End-to-End Driving with a Frozen Vision Encoder(https://arxiv.org/abs/2601.03460)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>End-to-end (E2E) models in autonomous driving aim to directly map sensor inputs to control commands, but their ability to generalize to novel and complex scenarios remains a key challenge. The common practice of fully fine-tuning the vision encoder on driving datasets potentially limits its generalization by causing the model to specialize too heavily in the training data. This work challenges the necessity of this training paradigm. We propose FROST-Drive, a novel E2E architecture designed to preserve and leverage the powerful generalization capabilities of a pretrained vision encoder from a Vision-Language Model (VLM). By keeping the encoder's weights frozen, our approach directly transfers the rich, generalized world knowledge from the VLM to the driving task. Our model architecture combines this frozen encoder with a transformer-based adapter for multimodal fusion and a GRU-based decoder for smooth waypoint generation. Furthermore, we introduce a custom loss function designed to directly optimize for Rater Feedback Score (RFS), a metric that prioritizes robust trajectory planning. We conduct extensive experiments on Waymo Open E2E Dataset, a large-scale datasets deliberately curated to capture the long-tail scenarios, demonstrating that our frozen-encoder approach significantly outperforms models that employ full fine-tuning. Our results provide substantial evidence that preserving the broad knowledge of a capable VLM is a more effective strategy for achieving robust, generalizable driving performance than intensive domain-specific adaptation. This offers a new pathway for developing vision-based models that can better handle the complexities of real-world application domains.</li>
<li><strong>摘要：</strong>自动驾驶中的端到端（E2E）模型旨在将传感器输入直接映射到控制命令，但它们泛化到新颖和复杂场景的能力仍然是一个关键挑战。在驾驶数据集上完全微调视觉编码器的常见做法可能会导致模型过于专注于训练数据，从而限制其泛化。这项工作挑战了这种培训范式的必要性。我们提出了 FROST-Drive，这是一种新颖的 E2E 架构，旨在保留和利用来自视觉语言模型 (VLM) 的预训练视觉编码器的强大泛化功能。通过保持编码器的权重冻结，我们的方法直接将丰富的、通用的世界知识从 VLM 传输到驾驶任务。我们的模型架构将这种冻结编码器与用于多模态融合的基于变压器的适配器和用于平滑路点生成的基于 GRU 的解码器相结合。此外，我们引入了一个自定义损失函数，旨在直接优化评估者反馈分数（RFS），这是一个优先考虑稳健轨迹规划的指标。我们在 Waymo Open E2E 数据集（一个专门为捕获长尾场景而精心设计的大型数据集）上进行了广泛的实验，证明我们的冻结编码器方法明显优于采用完全微调的模型。我们的结果提供了充分的证据，表明保留有能力的 VLM 的广泛知识是比密集的特定领域适应更有效的策略，可以实现稳健、通用的驾驶性能。这为开发基于视觉的模型提供了一条新途径，可以更好地处理现实应用领域的复杂性。</li>
</ul>

<h3>Title: ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Hengjia Li, Liming Jiang, Qing Yan, Yizhi Song, Hao Kang, Zichuan Liu, Xin Lu, Boxi Wu, Deng Cai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03467">https://arxiv.org/abs/2601.03467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03467">https://arxiv.org/pdf/2601.03467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03467]] ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing(https://arxiv.org/abs/2601.03467)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Instruction-driven image editing with unified multimodal generative models has advanced rapidly, yet their underlying visual reasoning remains limited, leading to suboptimal performance on reasoning-centric edits. Reinforcement learning (RL) has been investigated for improving the quality of image editing, but it faces three key challenges: (1) limited reasoning exploration confined to denoising stochasticity, (2) biased reward fusion, and (3) unstable VLM-based instruction rewards. In this work, we propose ThinkRL-Edit, a reasoning-centric RL framework that decouples visual reasoning from image synthesis and expands reasoning exploration beyond denoising. To the end, we introduce Chain-of-Thought (CoT)-based reasoning sampling with planning and reflection stages prior to generation in online sampling, compelling the model to explore multiple semantic hypotheses and validate their plausibility before committing to a visual outcome. To avoid the failures of weighted aggregation, we propose an unbiased chain preference grouping strategy across multiple reward dimensions. Moreover, we replace interval-based VLM scores with a binary checklist, yielding more precise, lower-variance, and interpretable rewards for complex reasoning. Experiments show our method significantly outperforms prior work on reasoning-centric image editing, producing instruction-faithful, visually coherent, and semantically grounded edits.</li>
<li><strong>摘要：</strong>具有统一多模态生成模型的指令驱动图像编辑发展迅速，但其底层视觉推理仍然有限，导致以推理为中心的编辑性能不佳。强化学习 (RL) 已被研究用于提高图像编辑的质量，但它面临三个关键挑战：(1) 仅限于去噪随机性的有限推理探索，(2) 有偏差的奖励融合，以及 (3) 基于 VLM 的指令奖励不稳定。在这项工作中，我们提出了 ThinkRL-Edit，这是一种以推理为中心的 RL 框架，它将视觉推理与图像合成解耦，并将推理探索扩展到去噪之外。最后，我们引入了基于思想链（CoT）的推理采样，在在线采样生成之前具有规划和反思阶段，迫使模型探索多种语义假设并在提交视觉结果之前验证其合理性。为了避免加权聚合的失败，我们提出了一种跨多个奖励维度的无偏链偏好分组策略。此外，我们用二进制检查表替换基于区间的 VLM 分数，从而为复杂推理产生更精确、方差更低且可解释的奖励。实验表明，我们的方法明显优于之前以推理为中心的图像编辑工作，产生忠实于指令、视觉连贯且基于语义的编辑。</li>
</ul>

<h3>Title: Understanding Reward Hacking in Text-to-Image Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yunqi Hong, Kuei-Chun Kao, Hengguang Zhou, Cho-Jui Hsieh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03468">https://arxiv.org/abs/2601.03468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03468">https://arxiv.org/pdf/2601.03468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03468]] Understanding Reward Hacking in Text-to-Image Reinforcement Learning(https://arxiv.org/abs/2601.03468)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has become a standard approach for post-training large language models and, more recently, for improving image generation models, which uses reward functions to enhance generation quality and human preference alignment. However, existing reward designs are often imperfect proxies for true human judgment, making models prone to reward hacking--producing unrealistic or low-quality images that nevertheless achieve high reward scores. In this work, we systematically analyze reward hacking behaviors in text-to-image (T2I) RL post-training. We investigate how both aesthetic/human preference rewards and prompt-image consistency rewards individually contribute to reward hacking and further show that ensembling multiple rewards can only partially mitigate this issue. Across diverse reward models, we identify a common failure mode: the generation of artifact-prone images. To address this, we propose a lightweight and adaptive artifact reward model, trained on a small curated dataset of artifact-free and artifact-containing samples. This model can be integrated into existing RL pipelines as an effective regularizer for commonly used reward models. Experiments demonstrate that incorporating our artifact reward significantly improves visual realism and reduces reward hacking across multiple T2I RL setups, demonstrating the effectiveness of lightweight reward augment serving as a safeguard against reward hacking.</li>
<li><strong>摘要：</strong>强化学习 (RL) 已成为大型语言模型训练后的标准方法，最近还成为改进图像生成模型的标准方法，该模型使用奖励函数来提高生成质量和人类偏好一致性。然而，现有的奖励设计通常不能完美地代表人类的真实判断，使得模型容易受到奖励黑客攻击——产生不切实际或低质量的图像，但仍能获得高奖励分数。在这项工作中，我们系统地分析了文本到图像（T2I）强化学习训练后的奖励黑客行为。我们研究了审美/人类偏好奖励和提示图像一致性奖励如何单独导致奖励黑客行为，并进一步表明，组合多个奖励只能部分缓解这个问题。在不同的奖励模型中，我们确定了一种常见的失败模式：生成容易出现伪影的图像。为了解决这个问题，我们提出了一种轻量级的自适应工件奖励模型，该模型在无工件和包含工件的样本的小型精选数据集上进行训练。该模型可以集成到现有的 RL 管道中，作为常用奖励模型的有效正则化器。实验表明，结合我们的工件奖励可以显着提高视觉真实感，并减少跨多个 T2I RL 设置的奖励黑客攻击，从而证明轻量级奖励增强作为防止奖励黑客攻击的有效性。</li>
</ul>

<h3>Title: VeRPO: Verifiable Dense Reward Policy Optimization for Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Longwen Wang, Xuan'er Wu, Xiaohui Hu, Yirui Liu, Yuankai Fan, Kaidong Yu, Qizhen Weng, Wei Xi, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03525">https://arxiv.org/abs/2601.03525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03525">https://arxiv.org/pdf/2601.03525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03525]] VeRPO: Verifiable Dense Reward Policy Optimization for Code Generation(https://arxiv.org/abs/2601.03525)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Effective reward design is a central challenge in Reinforcement Learning (RL) for code generation. Mainstream pass/fail outcome rewards enforce functional correctness via executing unit tests, but the resulting sparsity limits potential performance gains. While recent work has explored external Reward Models (RM) to generate richer, continuous rewards, the learned RMs suffer from reward misalignment and prohibitive computational cost. In this paper, we introduce \textbf{VeRPO} (\textbf{V}erifiable D\textbf{e}nse \textbf{R}eward \textbf{P}olicy \textbf{O}ptimization), a novel RL framework for code generation that synthesizes \textit{robust and dense rewards fully grounded in verifiable execution feedback}. The core idea of VeRPO is constructing dense rewards from weighted partial success: by dynamically estimating the difficulty weight of each unit test based on the execution statistics during training, a dense reward is derived from the sum of weights of the passed unit tests. To solidify the consistency between partial success and end-to-end functional correctness, VeRPO further integrates the dense signal with global execution outcomes, establishing a robust and dense reward paradigm relying solely on verifiable execution feedback. Extensive experiments across diverse benchmarks and settings demonstrate that VeRPO consistently outperforms outcome-driven and RM-based baselines, achieving up to +8.83\% gain in pass@1 with negligible time cost (< 0.02\%) and zero GPU memory overhead.</li>
<li><strong>摘要：</strong>有效的奖励设计是代码生成的强化学习 (RL) 的核心挑战。主流的通过/失败结果奖励通过执行单元测试来强制功能正确性，但由此产生的稀疏性限制了潜在的性能提升。虽然最近的工作探索了外部奖励模型（RM）来产生更丰富、持续的奖励，但学习的 RM 遭受奖励错位和过高的计算成本的困扰。在本文中，我们介绍了 \textbf{VeRPO} (\textbf{V}erabilized D\textbf{e}nse \textbf{R}eward \textbf{P}olicy \textbf{O}ptimization)，这是一种用于代码生成的新型强化学习框架，它综合了 \textit{完全基于可验证执行反馈的鲁棒而密集的奖励}。 VeRPO的核心思想是从加权部分成功中构建密集奖励：通过在训练期间根据执行统计动态估计每个单元测试的难度权重，从通过的单元测试的权重总和中得出密集奖励。为了巩固部分成功和端到端功能正确性之间的一致性，VeRPO进一步将密集信号与全局执行结果相结合，建立仅依赖于可验证的执行反馈的稳健而密集的奖励范式。跨不同基准和设置的大量实验表明，VeRPO 始终优于结果驱动和基于 RM 的基准，在 pass@1 中实现高达 +8.83\% 的增益，而时间成本可以忽略不计 (< 0.02\%) 且 GPU 内存开销为零。</li>
</ul>

<h3>Title: Physics-Constrained Cross-Resolution Enhancement Network for Optics-Guided Thermal UAV Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Zhicheng Zhao, Fengjiao Peng, Jinquan Yan, Wei Lu, Chenglong Li, Jin Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03526">https://arxiv.org/abs/2601.03526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03526">https://arxiv.org/pdf/2601.03526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03526]] Physics-Constrained Cross-Resolution Enhancement Network for Optics-Guided Thermal UAV Image Super-Resolution(https://arxiv.org/abs/2601.03526)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Optics-guided thermal UAV image super-resolution has attracted significant research interest due to its potential in all-weather monitoring applications. However, existing methods typically compress optical features to match thermal feature dimensions for cross-modal alignment and fusion, which not only causes the loss of high-frequency information that is beneficial for thermal super-resolution, but also introduces physically inconsistent artifacts such as texture distortions and edge blurring by overlooking differences in the imaging physics between modalities. To address these challenges, we propose PCNet to achieve cross-resolution mutual enhancement between optical and thermal modalities, while physically constraining the optical guidance process via thermal conduction to enable robust thermal UAV image super-resolution. In particular, we design a Cross-Resolution Mutual Enhancement Module (CRME) to jointly optimize thermal image super-resolution and optical-to-thermal modality conversion, facilitating effective bidirectional feature interaction across resolutions while preserving high-frequency optical priors. Moreover, we propose a Physics-Driven Thermal Conduction Module (PDTM) that incorporates two-dimensional heat conduction into optical guidance, modeling spatially-varying heat conduction properties to prevent inconsistent artifacts. In addition, we introduce a temperature consistency loss that enforces regional distribution consistency and boundary gradient smoothness to ensure generated thermal images align with real-world thermal radiation principles. Extensive experiments on VGTSR2.0 and DroneVehicle datasets demonstrate that PCNet significantly outperforms state-of-the-art methods on both reconstruction quality and downstream tasks including semantic segmentation and object detection.</li>
<li><strong>摘要：</strong>光学制导热无人机图像超分辨率由于其在全天候监控应用中的潜力而引起了人们的广泛研究兴趣。然而，现有方法通常压缩光学特征以匹配热特征尺寸以进行跨模态对准和融合，这不仅导致有利于热超分辨率的高频信息的丢失，而且由于忽略了模态之间成像物理的差异而引入了物理不一致的伪影，例如纹理扭曲和边缘模糊。为了应对这些挑战，我们建议 PCNet 实现光学和热模态之间的交叉分辨率相互增强，同时通过热传导物理约束光学制导过程，以实现鲁棒的热无人机图像超分辨率。特别是，我们设计了跨分辨率互增强模块（CRME）来联合优化热图像超分辨率和光热模态转换，促进跨分辨率的有效双向特征交互，同时保留高频光学先验。此外，我们提出了一种物理驱动的热传导模块（PDTM），它将二维热传导纳入光学引导中，对空间变化的热传导特性进行建模，以防止不一致的伪影。此外，我们引入了温度一致性损失，可以强制区域分布一致性和边界梯度平滑度，以确保生成的热图像与现实世界的热辐射原理保持一致。对 VGTSR2.0 和 DroneVehicle 数据集的大量实验表明，PCNet 在重建质量和下游任务（包括语义分割和对象检测）方面均显着优于最先进的方法。</li>
</ul>

<h3>Title: CloudMatch: Weak-to-Strong Consistency Learning for Semi-Supervised Cloud Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Zhao, Changlu Chen, Jingsheng Li, Tianxiang Xue, Kun Zhan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03528">https://arxiv.org/abs/2601.03528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03528">https://arxiv.org/pdf/2601.03528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03528]] CloudMatch: Weak-to-Strong Consistency Learning for Semi-Supervised Cloud Detection(https://arxiv.org/abs/2601.03528)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Due to the high cost of annotating accurate pixel-level labels, semi-supervised learning has emerged as a promising approach for cloud detection. In this paper, we propose CloudMatch, a semi-supervised framework that effectively leverages unlabeled remote sensing imagery through view-consistency learning combined with scene-mixing augmentations. An observation behind CloudMatch is that cloud patterns exhibit structural diversity and contextual variability across different scenes and within the same scene category. Our key insight is that enforcing prediction consistency across diversely augmented views, incorporating both inter-scene and intra-scene mixing, enables the model to capture the structural diversity and contextual richness of cloud patterns. Specifically, CloudMatch generates one weakly augmented view along with two complementary strongly augmented views for each unlabeled image: one integrates inter-scene patches to simulate contextual variety, while the other employs intra-scene mixing to preserve semantic coherence. This approach guides pseudolabel generation and enhances generalization. Extensive experiments show that CloudMatch achieves good performance, demonstrating its capability to utilize unlabeled data efficiently and advance semi-supervised cloud detection.</li>
<li><strong>摘要：</strong>由于注释准确的像素级标签的成本很高，半监督学习已成为云检测的一种有前途的方法。在本文中，我们提出了 CloudMatch，这是一种半监督框架，通过视图一致性学习与场景混合增强相结合，有效地利用未标记的遥感图像。 CloudMatch 背后的一个观察结果是，云模式在不同场景和同一场景类别内表现出结构多样性和上下文可变性。我们的主要见解是，在不同的增强视图之间强制执行预测一致性，结合场景间和场景内混合，使模型能够捕获云模式的结构多样性和上下文丰富性。具体来说，CloudMatch 为每个未标记图像生成一个弱增强视图以及两个互补的强增强视图：一个集成场景间补丁来模拟上下文变化，而另一个则采用场景内混合来保持语义一致性。这种方法指导伪标签的生成并增强泛化能力。大量实验表明，CloudMatch 取得了良好的性能，展示了其有效利用未标记数据并推进半监督云检测的能力。</li>
</ul>

<h3>Title: Detecting AI-Generated Images via Distributional Deviations from Real Images</h3>
<ul>
<li><strong>Authors: </strong>Yakun Niu, Yingjian Chen, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03586">https://arxiv.org/abs/2601.03586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03586">https://arxiv.org/pdf/2601.03586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03586]] Detecting AI-Generated Images via Distributional Deviations from Real Images(https://arxiv.org/abs/2601.03586)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of generative models has significantly enhanced the quality of AI-generated images, raising concerns about misinformation and the erosion of public trust. Detecting AI-generated images has thus become a critical challenge, particularly in terms of generalizing to unseen generative models. Existing methods using frozen pre-trained CLIP models show promise in generalization but treat the image encoder as a basic feature extractor, failing to fully exploit its potential. In this paper, we perform an in-depth analysis of the frozen CLIP image encoder (CLIP-ViT), revealing that it effectively clusters real images in a high-level, abstract feature space. However, it does not truly possess the ability to distinguish between real and AI-generated images. Based on this analysis, we propose a Masking-based Pre-trained model Fine-Tuning (MPFT) strategy, which introduces a Texture-Aware Masking (TAM) mechanism to mask textured areas containing generative model-specific patterns during fine-tuning. This approach compels CLIP-ViT to attend to the "distributional deviations"from authentic images for AI-generated image detection, thereby achieving enhanced generalization performance. Extensive experiments on the GenImage and UniversalFakeDetect datasets demonstrate that our method, fine-tuned with only a minimal number of images, significantly outperforms existing approaches, achieving up to 98.2% and 94.6% average accuracy on the two datasets, respectively.</li>
<li><strong>摘要：</strong>生成模型的快速发展显着提高了人工智能生成图像的质量，引发了人们对错误信息和公众信任侵蚀的担忧。因此，检测人工智能生成的图像已成为一项关键挑战，特别是在推广到看不见的生成模型方面。使用冻结预训练 CLIP 模型的现有方法在泛化方面显示出前景，但将图像编码器视为基本特征提取器，未能充分发挥其潜力。在本文中，我们对冻结 CLIP 图像编码器 (CLIP-ViT) 进行了深入分析，揭示了它可以在高级抽象特征空间中有效地聚类真实图像。然而，它并不真正具备区分真实图像和人工智能生成图像的能力。基于此分析，我们提出了一种基于掩蔽的预训练模型微调（MPFT）策略，该策略引入了纹理感知掩蔽（TAM）机制，以在微调期间掩蔽包含生成模型特定模式的纹理区域。这种方法迫使 CLIP-ViT 关注 AI 生成图像检测中真实图像的“分布偏差”，从而实现增强的泛化性能。对 GenImage 和 UniversalFakeDetect 数据集的大量实验表明，我们的方法仅使用最少数量的图像进行微调，显着优于现有方法，在两个数据集上分别实现了高达 98.2% 和 94.6% 的平均准确率。</li>
</ul>

<h3>Title: A Comparative Study of Traditional Machine Learning, Deep Learning, and Large Language Models for Mental Health Forecasting using Smartphone Sensing Data</h3>
<ul>
<li><strong>Authors: </strong>Kaidong Feng, Zhu Sun, Roy Ka-Wei Lee, Xun Jiang, Yin-Leng Theng, Yi Ding</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03603">https://arxiv.org/abs/2601.03603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03603">https://arxiv.org/pdf/2601.03603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03603]] A Comparative Study of Traditional Machine Learning, Deep Learning, and Large Language Models for Mental Health Forecasting using Smartphone Sensing Data(https://arxiv.org/abs/2601.03603)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Smartphone sensing offers an unobtrusive and scalable way to track daily behaviors linked to mental health, capturing changes in sleep, mobility, and phone use that often precede symptoms of stress, anxiety, or depression. While most prior studies focus on detection that responds to existing conditions, forecasting mental health enables proactive support through Just-in-Time Adaptive Interventions. In this paper, we present the first comprehensive benchmarking study comparing traditional machine learning (ML), deep learning (DL), and large language model (LLM) approaches for mental health forecasting using the College Experience Sensing (CES) dataset, the most extensive longitudinal dataset of college student mental health to date. We systematically evaluate models across temporal windows, feature granularities, personalization strategies, and class imbalance handling. Our results show that DL models, particularly Transformer (Macro-F1 = 0.58), achieve the best overall performance, while LLMs show strength in contextual reasoning but weaker temporal modeling. Personalization substantially improves forecasts of severe mental health states. By revealing how different modeling approaches interpret phone sensing behavioral data over time, this work lays the groundwork for next-generation, adaptive, and human-centered mental health technologies that can advance both research and real-world well-being.</li>
<li><strong>摘要：</strong>智能手机传感提供了一种不显眼且可扩展的方式来跟踪与心理健康相关的日常行为，捕捉睡眠、行动能力和手机使用的变化，这些变化通常先于压力、焦虑或抑郁症状。虽然大多数先前的研究侧重于对现有状况做出反应的检测，但预测心理健康状况可以通过及时的适应性干预措施提供主动支持。在本文中，我们提出了第一个全面的基准研究，使用迄今为止最广泛的大学生心理健康纵向数据集，比较传统机器学习（ML）、深度学习（DL）和大语言模型（LLM）心理健康预测方法。我们系统地评估跨时间窗口、特征粒度、个性化策略和类别不平衡处理的模型。我们的结果表明，DL 模型，特别是 Transformer（Macro-F1 = 0.58），实现了最佳的整体性能，而 LLM 在上下文推理方面表现出优势，但时间建模较弱。个性化极大地改善了对严重心理健康状态的预测。通过揭示不同的建模方法如何随着时间的推移解释手机传感行为数据，这项工作为下一代、适应性和以人为本的心理健康技术奠定了基础，这些技术可以促进研究和现实世界的福祉。</li>
</ul>

<h3>Title: Policy-Guided Search on Tree-of-Thoughts for Efficient Problem Solving with Bounded Language Model Queries</h3>
<ul>
<li><strong>Authors: </strong>Sumedh Pendurkar, Guni Sharon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03606">https://arxiv.org/abs/2601.03606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03606">https://arxiv.org/pdf/2601.03606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03606]] Policy-Guided Search on Tree-of-Thoughts for Efficient Problem Solving with Bounded Language Model Queries(https://arxiv.org/abs/2601.03606)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent studies explored integrating state-space search algorithms with Language Models (LM) to perform look-ahead on the token generation process, the ''Tree-of-Thoughts'' (ToT), generated by LMs, thereby improving performance on problem-solving tasks. However, the affiliated search algorithms often overlook the significant computational costs associated with LM inference, particularly in scenarios with constrained computational budgets. Consequently, we address the problem of improving LM performance on problem-solving tasks under limited computational budgets. We demonstrate how the probabilities assigned to thoughts by LMs can serve as a heuristic to guide search within the ToT framework, thereby reducing the number of thought evaluations. Building on this insight, we adapt a heuristic search algorithm, Levin Tree Search (LTS), to the ToT framework, which leverages LMs as policies to guide the tree exploration efficiently. We extend the theoretical results of LTS by showing that, for ToT (a pruned tree), LTS guarantees a bound on the number of states expanded, and consequently, on the number of thoughts generated. Additionally, we analyze the sensitivity of this bound to the temperature values commonly used in the final softmax layer of the LM. Empirical evaluation under a fixed LM query budget demonstrates that LTS consistently achieves comparable or higher accuracy than baseline search algorithms within the ToT framework, across three domains (Blocksworld, PrOntoQA, Array Sorting) and four distinct LMs. These findings highlight the efficacy of LTS on ToT, particularly in enabling cost-effective and time-efficient problem-solving, making it well-suited for latency-critical and resource-constrained applications.</li>
<li><strong>摘要：</strong>最近的研究探索了将状态空间搜索算法与语言模型 (LM) 集成，以对 LM 生成的标记生成过程（即“思想树”(ToT)）进行前瞻，从而提高解决问题任务的性能。然而，附属的搜索算法经常忽视与 LM 推理相关的大量计算成本，特别是在计算预算有限的情况下。因此，我们解决了在有限的计算预算下提高 LM 在解决问题任务上的性能的问题。我们演示了 LM 分配给思想的概率如何作为启发式方法来指导 ToT 框架内的搜索，从而减少思想评估的数量。基于这一见解，我们将启发式搜索算法 Levin Tree Search (LTS) 应用于 ToT 框架，该框架利用 LM 作为策略来有效地指导树探索。我们扩展了 LTS 的理论结果，表明对于 ToT（修剪树），LTS 保证了扩展的状态数量的界限，从而保证了生成的想法数量的界限。此外，我们还分析了该约束与 LM 最终 softmax 层中常用的温度值的敏感性。在固定 LM 查询预算下的实证评估表明，在三个领域（Blocksworld、PrOntoQA、Array Sorting）和四个不同的 LM 中，LTS 始终能够达到与 ToT 框架内的基线搜索算法相当或更高的精度。这些发现凸显了 LTS 在 ToT 上的功效，特别是在实现经济有效且省时的问题解决方面，使其非常适合延迟关键和资源受限的应用程序。</li>
</ul>

<h3>Title: Mathematical Foundations of Polyphonic Music Generation via Structural Inductive Bias</h3>
<ul>
<li><strong>Authors: </strong>Joonwon Seo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03612">https://arxiv.org/abs/2601.03612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03612">https://arxiv.org/pdf/2601.03612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03612]] Mathematical Foundations of Polyphonic Music Generation via Structural Inductive Bias(https://arxiv.org/abs/2601.03612)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This monograph introduces a novel approach to polyphonic music generation by addressing the "Missing Middle" problem through structural inductive bias. Focusing on Beethoven's piano sonatas as a case study, we empirically verify the independence of pitch and hand attributes using normalized mutual information (NMI=0.167) and propose the Smart Embedding architecture, achieving a 48.30% reduction in parameters. We provide rigorous mathematical proofs using information theory (negligible loss bounded at 0.153 bits), Rademacher complexity (28.09% tighter generalization bound), and category theory to demonstrate improved stability and generalization. Empirical results show a 9.47% reduction in validation loss, confirmed by SVD analysis and an expert listening study (N=53). This dual theoretical and applied framework bridges gaps in AI music generation, offering verifiable insights for mathematically grounded deep learning.</li>
<li><strong>摘要：</strong>本专着通过结构归纳偏差解决“中间缺失”问题，介绍了一种新的复调音乐生成方法。以贝多芬的钢琴奏鸣曲为案例研究，我们利用归一化互信息（NMI=0.167）实证验证了音高和手属性的独立性，并提出了智能嵌入架构，实现了参数减少48.30%。我们使用信息论（可忽略的损失限制在 0.153 位）、Rademacher 复杂度（更严格的泛化范围为 28.09%）和类别论提供严格的数学证明，以证明稳定性和泛化能力的提高。实证结果显示，验证损失减少了 9.47%，这一点经 SVD 分析和专家听力研究 (N=53) 证实。这种理论和应用的双重框架弥合了人工智能音乐生成的差距，为基于数学的深度学习提供了可验证的见解。</li>
</ul>

<h3>Title: Systematic Evaluation of Depth Backbones and Semantic Cues for Monocular Pseudo-LiDAR 3D Detection</h3>
<ul>
<li><strong>Authors: </strong>Samson Oseiwe Ajadalu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03617">https://arxiv.org/abs/2601.03617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03617">https://arxiv.org/pdf/2601.03617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03617]] Systematic Evaluation of Depth Backbones and Semantic Cues for Monocular Pseudo-LiDAR 3D Detection(https://arxiv.org/abs/2601.03617)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Monocular 3D object detection offers a low-cost alternative to LiDAR, yet remains less accurate due to the difficulty of estimating metric depth from a single image. We systematically evaluate how depth backbones and feature engineering affect a monocular Pseudo-LiDAR pipeline on the KITTI validation split. Specifically, we compare NeWCRFs (supervised metric depth) against Depth Anything V2 Metric-Outdoor (Base) under an identical pseudo-LiDAR generation and PointRCNN detection protocol. NeWCRFs yields stronger downstream 3D detection, achieving 10.50\% AP$_{3D}$ at IoU$=0.7$ on the Moderate split using grayscale intensity (Exp~2). We further test point-cloud augmentations using appearance cues (grayscale intensity) and semantic cues (instance segmentation confidence). Contrary to the expectation that semantics would substantially close the gap, these features provide only marginal gains, and mask-based sampling can degrade performance by removing contextual geometry. Finally, we report a depth-accuracy-versus-distance diagnostic using ground-truth 2D boxes (including Ped/Cyc), highlighting that coarse depth correctness does not fully predict strict 3D IoU. Overall, under an off-the-shelf LiDAR detector, depth-backbone choice and geometric fidelity dominate performance, outweighing secondary feature injection.</li>
<li><strong>摘要：</strong>单目 3D 物体检测提供了 LiDAR 的低成本替代方案，但由于难以从单个图像估计度量深度，因此准确性较低。我们系统地评估了深度主干和特征工程如何影响 KITTI 验证分割上的单目伪 LiDAR 管道。具体来说，我们在相同的伪 LiDAR 生成和 PointRCNN 检测协议下将 NeWCRF（监督度量深度）与 Depth Anything V2 Metric-Outdoor（Base）进行比较。 NeWCRF 产生更强的下游 3D 检测，在使用灰度强度的中等分割 (Exp~2) 上，在 IoU$=0.7$ 时实现 10.50\% AP$_{3D}$。我们使用外观线索（灰度强度）和语义线索（实例分割置信度）进一步测试点云增强。与语义将大大缩小差距的预期相反，这些功能仅提供边际增益，并且基于掩模的采样会通过删除上下文几何来降低性能。最后，我们报告了使用真实 2D 框（包括 Ped/Cyc）的深度精度与距离诊断，强调粗略深度正确性并不能完全预测严格的 3D IoU。总体而言，在现成的 LiDAR 探测器下，深度骨干选择和几何保真度主导着性能，超过了次要特征注入。</li>
</ul>

<h3>Title: MFC-RFNet: A Multi-scale Guided Rectified Flow Network for Radar Sequence Prediction</h3>
<ul>
<li><strong>Authors: </strong>Wenjie Luo, Chuanhu Deng, Chaorong Li, Rongyao Deng, Qiang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03633">https://arxiv.org/abs/2601.03633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03633">https://arxiv.org/pdf/2601.03633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03633]] MFC-RFNet: A Multi-scale Guided Rectified Flow Network for Radar Sequence Prediction(https://arxiv.org/abs/2601.03633)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate and high-resolution precipitation nowcasting from radar echo sequences is crucial for disaster mitigation and economic planning, yet it remains a significant challenge. Key difficulties include modeling complex multi-scale evolution, correcting inter-frame feature misalignment caused by displacement, and efficiently capturing long-range spatiotemporal context without sacrificing spatial fidelity. To address these issues, we present the Multi-scale Feature Communication Rectified Flow (RF) Network (MFC-RFNet), a generative framework that integrates multi-scale communication with guided feature fusion. To enhance multi-scale fusion while retaining fine detail, a Wavelet-Guided Skip Connection (WGSC) preserves high-frequency components, and a Feature Communication Module (FCM) promotes bidirectional cross-scale interaction. To correct inter-frame displacement, a Condition-Guided Spatial Transform Fusion (CGSTF) learns spatial transforms from conditioning echoes to align shallow features. The backbone adopts rectified flow training to learn near-linear probability-flow trajectories, enabling few-step sampling with stable fidelity. Additionally, lightweight Vision-RWKV (RWKV) blocks are placed at the encoder tail, the bottleneck, and the first decoder layer to capture long-range spatiotemporal dependencies at low spatial resolutions with moderate compute. Evaluations on four public datasets (SEVIR, MeteoNet, Shanghai, and CIKM) demonstrate consistent improvements over strong baselines, yielding clearer echo morphology at higher rain-rate thresholds and sustained skill at longer lead times. These results suggest that the proposed synergy of RF training with scale-aware communication, spatial alignment, and frequency-aware fusion presents an effective and robust approach for radar-based nowcasting.</li>
<li><strong>摘要：</strong>根据雷达回波序列进行准确、高分辨率的临近降水预报对于减灾和经济规划至关重要，但它仍然是一个重大挑战。主要困难包括对复杂的多尺度演化进行建模、纠正由位移引起的帧间特征错位，以及在不牺牲空间保真度的情况下有效捕获远程时空背景。为了解决这些问题，我们提出了多尺度特征通信整流流（RF）网络（MFC-RFNet），这是一种将多尺度通信与引导特征融合相集成的生成框架。为了增强多尺度融合，同时保留精细细节，小波引导跳跃连接 (WGSC) 保留高频分量，特征通信模块 (FCM) 促进双向跨尺度交互。为了纠正帧间位移，条件引导空间变换融合 (CGSTF) 从条件回波中学习空间变换以对齐浅层特征。主干网采用修正流训练来学习近线性概率流轨迹，从而实现稳定保真度的少步采样。此外，轻量级 Vision-RWKV (RWKV) 块被放置在编码器尾部、瓶颈和第一个解码器层，以通过适度的计算捕获低空间分辨率下的远程时空依赖性。对四个公共数据集（SEVIR、MeteoNet、上海和 CIKM）的评估表明，在强基线上取得了一致的改进，在较高的降雨率阈值下产生了更清晰的回波形态，并在较长的交付时间内保持了技能。这些结果表明，所提出的射频训练与尺度感知通信、空间对准和频率感知融合的协同作用为基于雷达的临近预报提供了一种有效且稳健的方法。</li>
</ul>

<h3>Title: VideoMemory: Toward Consistent Video Generation via Memory Integration</h3>
<ul>
<li><strong>Authors: </strong>Jinsong Zhou, Yihua Du, Xinli Xu, Luozhou Wang, Zijie Zhuang, Yehang Zhang, Shuaibo Li, Xiaojun Hu, Bolan Su, Ying-cong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03655">https://arxiv.org/abs/2601.03655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03655">https://arxiv.org/pdf/2601.03655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03655]] VideoMemory: Toward Consistent Video Generation via Memory Integration(https://arxiv.org/abs/2601.03655)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Maintaining consistent characters, props, and environments across multiple shots is a central challenge in narrative video generation. Existing models can produce high-quality short clips but often fail to preserve entity identity and appearance when scenes change or when entities reappear after long temporal gaps. We present VideoMemory, an entity-centric framework that integrates narrative planning with visual generation through a Dynamic Memory Bank. Given a structured script, a multi-agent system decomposes the narrative into shots, retrieves entity representations from memory, and synthesizes keyframes and videos conditioned on these retrieved states. The Dynamic Memory Bank stores explicit visual and semantic descriptors for characters, props, and backgrounds, and is updated after each shot to reflect story-driven changes while preserving identity. This retrieval-update mechanism enables consistent portrayal of entities across distant shots and supports coherent long-form generation. To evaluate this setting, we construct a 54-case multi-shot consistency benchmark covering character-, prop-, and background-persistent scenarios. Extensive experiments show that VideoMemory achieves strong entity-level coherence and high perceptual quality across diverse narrative sequences.</li>
<li><strong>摘要：</strong>在多个镜头中保持一致的角色、道具和环境是叙事视频生成的主要挑战。现有模型可以生成高质量的短片，但当场景发生变化或实体在长时间间隔后重新出现时，通常无法保留实体的身份和外观。我们提出了 VideoMemory，一个以实体为中心的框架，通过动态内存库将叙事规划与视觉生成集成在一起。给定结构化脚本，多智能体系统将叙述分解为镜头，从内存中检索实体表示，并根据这些检索到的状态合成关键帧和视频。动态内存库存储角色、道具和背景的明确视觉和语义描述符，并在每次拍摄后进行更新，以反映故事驱动的变化，同时保留身份。这种检索更新机制可以对远处镜头中的实体进行一致的描绘，并支持连贯的长格式生成。为了评估此设置，我们构建了一个 54 例多镜头一致性基准，涵盖角色、道具和背景持续场景。大量实验表明，VideoMemory 在不同的叙事序列中实现了强大的实体级连贯性和高感知质量。</li>
</ul>

<h3>Title: MGPC: Multimodal Network for Generalizable Point Cloud Completion With Modality Dropout and Progressive Decoding</h3>
<ul>
<li><strong>Authors: </strong>Jiangyuan Liu, Hongxuan Ma, Yuhao Zhao, Zhe Liu, Jian Wang, Wei Zou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03660">https://arxiv.org/abs/2601.03660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03660">https://arxiv.org/pdf/2601.03660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03660]] MGPC: Multimodal Network for Generalizable Point Cloud Completion With Modality Dropout and Progressive Decoding(https://arxiv.org/abs/2601.03660)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Point cloud completion aims to recover complete 3D geometry from partial observations caused by limited viewpoints and occlusions. Existing learning-based works, including 3D Convolutional Neural Network (CNN)-based, point-based, and Transformer-based methods, have achieved strong performance on synthetic benchmarks. However, due to the limitations of modality, scalability, and generative capacity, their generalization to novel objects and real-world scenarios remains challenging. In this paper, we propose MGPC, a generalizable multimodal point cloud completion framework that integrates point clouds, RGB images, and text within a unified architecture. MGPC introduces an innovative modality dropout strategy, a Transformer-based fusion module, and a novel progressive generator to improve robustness, scalability, and geometric modeling capability. We further develop an automatic data generation pipeline and construct MGPC-1M, a large-scale benchmark with over 1,000 categories and one million training pairs. Extensive experiments on MGPC-1M and in-the-wild data demonstrate that the proposed method consistently outperforms prior baselines and exhibits strong generalization under real-world conditions.</li>
<li><strong>摘要：</strong>点云补全旨在从有限视点和遮挡引起的部分观察中恢复完整的 3D 几何形状。现有的基于学习的工作，包括基于 3D 卷积神经网络 (CNN)、基于点和基于 Transformer 的方法，在综合基准上取得了出色的性能。然而，由于模态、可扩展性和生成能力的限制，它们对新颖对象和现实场景的泛化仍然具有挑战性。在本文中，我们提出了 MGPC，这是一种可通用的多模式点云完成框架，它将点云、RGB 图像和文本集成在一个统一的架构中。 MGPC 引入了创新的模态 dropout 策略、基于 Transformer 的融合模块和新颖的渐进式生成器，以提高鲁棒性、可扩展性和几何建模能力。我们进一步开发了自动数据生成管道，并构建了 MGPC-1M，这是一个包含超过 1,000 个类别和 100 万个训练对的大型基准。对 MGPC-1M 和野外数据的大量实验表明，所提出的方法始终优于先前的基线，并在现实条件下表现出很强的泛化能力。</li>
</ul>

<h3>Title: PhysVideoGenerator: Towards Physically Aware Video Generation via Latent Physics Guidance</h3>
<ul>
<li><strong>Authors: </strong>Siddarth Nilol Kundur Satish, Devesh Jaiswal, Hongyu Chen, Abhishek Bakshi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03665">https://arxiv.org/abs/2601.03665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03665">https://arxiv.org/pdf/2601.03665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03665]] PhysVideoGenerator: Towards Physically Aware Video Generation via Latent Physics Guidance(https://arxiv.org/abs/2601.03665)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Current video generation models produce high-quality aesthetic videos but often struggle to learn representations of real-world physics dynamics, resulting in artifacts such as unnatural object collisions, inconsistent gravity, and temporal flickering. In this work, we propose PhysVideoGenerator, a proof-of-concept framework that explicitly embeds a learnable physics prior into the video generation process. We introduce a lightweight predictor network, PredictorP, which regresses high-level physical features extracted from a pre-trained Video Joint Embedding Predictive Architecture (V-JEPA 2) directly from noisy diffusion latents. These predicted physics tokens are injected into the temporal attention layers of a DiT-based generator (Latte) via a dedicated cross-attention mechanism. Our primary contribution is demonstrating the technical feasibility of this joint training paradigm: we show that diffusion latents contain sufficient information to recover V-JEPA 2 physical representations, and that multi-task optimization remains stable over training. This report documents the architectural design, technical challenges, and validation of training stability, establishing a foundation for future large-scale evaluation of physics-aware generative models.</li>
<li><strong>摘要：</strong>当前的视频生成模型可以生成高质量的美学视频，但通常很难学习现实世界物理动力学的表示，从而导致诸如不自然的物体碰撞、不一致的重力和时间闪烁等伪影。在这项工作中，我们提出了 PhysVideoGenerator，这是一个概念验证框架，它将可学习的物理学先验嵌入到视频生成过程中。我们引入了一种轻量级预测器网络 PredictorP，它直接从噪声扩散潜伏中回归从预训练视频联合嵌入预测架构（V-JEPA 2）中提取的高级物理特征。这些预测的物理标记通过专用的交叉注意力机制注入到基于 DiT 的生成器 (Latte) 的时间注意力层中。我们的主要贡献是证明了这种联合训练范例的技术可行性：我们证明扩散潜伏包含足够的信息来恢复 V-JEPA 2 物理表示，并且多任务优化在训练过程中保持稳定。该报告记录了架构设计、技术挑战和训练稳定性验证，为未来大规模评估物理感知生成模型奠定了基础。</li>
</ul>

<h3>Title: Inference Attacks Against Graph Generative Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xiuling Wang, Xin Huang, Guibo Luo, Jianliang Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03701">https://arxiv.org/abs/2601.03701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03701">https://arxiv.org/pdf/2601.03701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03701]] Inference Attacks Against Graph Generative Diffusion Models(https://arxiv.org/abs/2601.03701)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Graph generative diffusion models have recently emerged as a powerful paradigm for generating complex graph structures, effectively capturing intricate dependencies and relationships within graph data. However, the privacy risks associated with these models remain largely unexplored. In this paper, we investigate information leakage in such models through three types of black-box inference attacks. First, we design a graph reconstruction attack, which can reconstruct graphs structurally similar to those training graphs from the generated graphs. Second, we propose a property inference attack to infer the properties of the training graphs, such as the average graph density and the distribution of densities, from the generated graphs. Third, we develop two membership inference attacks to determine whether a given graph is present in the training set. Extensive experiments on three different types of graph generative diffusion models and six real-world graphs demonstrate the effectiveness of these attacks, significantly outperforming the baseline approaches. Finally, we propose two defense mechanisms that mitigate these inference attacks and achieve a better trade-off between defense strength and target model utility than existing methods. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>图生成扩散模型最近已成为生成复杂图结构的强大范例，可有效捕获图数据中复杂的依赖关系和关系。然而，与这些模型相关的隐私风险在很大程度上仍未得到探索。在本文中，我们通过三种类型的黑盒推理攻击来研究此类模型中的信息泄漏。首先，我们设计了一种图重建攻击，它可以从生成的图中重建结构类似于训练图的图。其次，我们提出了一种属性推断攻击，从生成的图中推断训练图的属性，例如平均图密度和密度分布。第三，我们开发两种成员推理攻击来确定训练集中是否存在给定的图。对三种不同类型的图生成扩散模型和六个现实世界图的广泛实验证明了这些攻击的有效性，显着优于基线方法。最后，我们提出了两种防御机制，可以减轻这些推理攻击，并比现有方法在防御强度和目标模型效用之间实现更好的权衡。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: A Comparative Study of 3D Model Acquisition Methods for Synthetic Data Generation of Agricultural Products</h3>
<ul>
<li><strong>Authors: </strong>Steven Moonen, Rob Salaets, Kenneth Batstone, Abdellatif Bey-Temsamani, Nick Michiels</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03784">https://arxiv.org/abs/2601.03784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03784">https://arxiv.org/pdf/2601.03784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03784]] A Comparative Study of 3D Model Acquisition Methods for Synthetic Data Generation of Agricultural Products(https://arxiv.org/abs/2601.03784)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In the manufacturing industry, computer vision systems based on artificial intelligence (AI) are widely used to reduce costs and increase production. Training these AI models requires a large amount of training data that is costly to acquire and annotate, especially in high-variance, low-volume manufacturing environments. A popular approach to reduce the need for real data is the use of synthetic data that is generated by leveraging computer-aided design (CAD) models available in the industry. However, in the agricultural industry these models are not readily available, increasing the difficulty in leveraging synthetic data. In this paper, we present different techniques for substituting CAD files to create synthetic datasets. We measure their relative performance when used to train an AI object detection model to separate stones and potatoes in a bin picking environment. We demonstrate that using highly representative 3D models acquired by scanning or using image-to-3D approaches can be used to generate synthetic data for training object detection models. Finetuning on a small real dataset can significantly improve the performance of the models and even get similar performance when less representative models are used.</li>
<li><strong>摘要：</strong>在制造业中，基于人工智能（AI）的计算机视觉系统被广泛应用，以降低成本、提高产量。训练这些人工智能模型需要大量的训练数据，这些数据的获取和注释成本很高，特别是在高方差、小批量的制造环境中。减少对真实数据需求的一种流行方法是使用通过利用行业中可用的计算机辅助设计 (CAD) 模型生成的合成数据。然而，在农业领域，这些模型并不容易获得，这增加了利用合成数据的难度。在本文中，我们提出了替代 CAD 文件来创建合成数据集的不同技术。当用于训练人工智能对象检测模型以在垃圾箱拣选环境中分离石头和土豆时，我们测量了它们的相对性能。我们证明，使用通过扫描或使用图像到 3D 方法获取的高度代表性的 3D 模型可用于生成用于训练对象检测模型的合成数据。在小型真实数据集上进行微调可以显着提高模型的性能，甚至在使用代表性较低的模型时获得相似的性能。</li>
</ul>

<h3>Title: Prompt Tuning without Labeled Samples for Zero-Shot Node Classification in Text-Attributed Graphs</h3>
<ul>
<li><strong>Authors: </strong>Sethupathy Parameswaran, Suresh Sundaram, Yuan Fang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03793">https://arxiv.org/abs/2601.03793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03793">https://arxiv.org/pdf/2601.03793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03793]] Prompt Tuning without Labeled Samples for Zero-Shot Node Classification in Text-Attributed Graphs(https://arxiv.org/abs/2601.03793)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Node classification is a fundamental problem in information retrieval with many real-world applications, such as community detection in social networks, grouping articles published online and product categorization in e-commerce. Zero-shot node classification in text-attributed graphs (TAGs) presents a significant challenge, particularly due to the absence of labeled data. In this paper, we propose a novel Zero-shot Prompt Tuning (ZPT) framework to address this problem by leveraging a Universal Bimodal Conditional Generator (UBCG). Our approach begins with pre-training a graph-language model to capture both the graph structure and the associated textual descriptions of each node. Following this, a conditional generative model is trained to learn the joint distribution of nodes in both graph and text modalities, enabling the generation of synthetic samples for each class based solely on the class name. These synthetic node and text embeddings are subsequently used to perform continuous prompt tuning, facilitating effective node classification in a zero-shot setting. Furthermore, we conduct extensive experiments on multiple benchmark datasets, demonstrating that our framework performs better than existing state-of-the-art baselines. We also provide ablation studies to validate the contribution of the bimodal generator. The code is provided at: this https URL.</li>
<li><strong>摘要：</strong>节点分类是许多现实应用中信息检索的基本问题，例如社交网络中的社区检测、在线发布的文章分组以及电子商务中的产品分类。文本属性图（TAG）中的零样本节点分类提出了重大挑战，特别是由于缺乏标记数据。在本文中，我们提出了一种新颖的零样本提示调整（ZPT）框架，通过利用通用双峰条件生成器（UBCG）来解决这个问题。我们的方法从预训练图形语言模型开始，以捕获每个节点的图形结构和相关文本描述。接下来，训练条件生成模型来学习图形和文本模式中节点的联合分布，从而能够仅根据类名称为每个类生成合成样本。这些合成节点和文本嵌入随后用于执行连续提示调整，促进零样本设置中的有效节点分类。此外，我们对多个基准数据集进行了广泛的实验，证明我们的框架比现有的最先进的基线表现更好。我们还提供消融研究来验证双峰发生器的贡献。代码位于：此 https URL。</li>
</ul>

<h3>Title: Quantum vs. Classical Machine Learning: A Benchmark Study for Financial Prediction</h3>
<ul>
<li><strong>Authors: </strong>Rehan Ahmad, Muhammad Kashif, Nouhaila Innan, Muhammad Shafique</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03802">https://arxiv.org/abs/2601.03802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03802">https://arxiv.org/pdf/2601.03802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03802]] Quantum vs. Classical Machine Learning: A Benchmark Study for Financial Prediction(https://arxiv.org/abs/2601.03802)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this paper, we present a reproducible benchmarking framework that systematically compares QML models with architecture-matched classical counterparts across three financial tasks: (i) directional return prediction on U.S. and Turkish equities, (ii) live-trading simulation with Quantum LSTMs versus classical LSTMs on the S\&P 500, and (iii) realized volatility forecasting using Quantum Support Vector Regression. By standardizing data splits, features, and evaluation metrics, our study provides a fair assessment of when current-generation QML models can match or exceed classical methods. Our results reveal that quantum approaches show performance gains when data structure and circuit design are well aligned. In directional classification, hybrid quantum neural networks surpass the parameter-matched ANN by \textbf{+3.8 AUC} and \textbf{+3.4 accuracy points} on \texttt{AAPL} stock and by \textbf{+4.9 AUC} and \textbf{+3.6 accuracy points} on Turkish stock \texttt{KCHOL}. In live trading, the QLSTM achieves higher risk-adjusted returns in \textbf{two of four} S\&P~500 regimes. For volatility forecasting, an angle-encoded QSVR attains the \textbf{lowest QLIKE} on \texttt{KCHOL} and remains within $\sim$0.02-0.04 QLIKE of the best classical kernels on \texttt{S\&P~500} and \texttt{AAPL}. Our benchmarking framework clearly identifies the scenarios where current QML architectures offer tangible improvements and where established classical methods continue to dominate.</li>
<li><strong>摘要：</strong>在本文中，我们提出了一个可重复的基准测试框架，该框架在三个金融任务中系统地将 QML 模型与架构匹配的经典模型进行比较：(i) 对美国和土耳其股票的定向回报预测，(ii) 使用量子 LSTM 与标准普尔 500 指数上的经典 LSTM 进行实时交易模拟，以及 (iii) 使用量子支持向量回归实现波动率预测。通过标准化数据分割、特征和评估指标，我们的研究为当前一代 QML 模型何时可以匹配或超越经典方法提供了公平的评估。我们的结果表明，当数据结构和电路设计良好结合时，量子方法会显示出性能提升。在定向分类中，混合量子神经网络在 \texttt{AAPL} 股票上超过了参数匹配的 ANN，超过了参数匹配的 ANN，在 \texttt{AAPL} 股票上超过了 \textbf{+3.8 AUC} 和 \textbf{+3.4 准确度点}，在土耳其股票 \texttt{KCHOL} 上超过了 \textbf{+4.9 AUC} 和 \textbf{+3.6 准确度点}。在实时交易中，QLSTM 在 S\&P~500 体系中的四个体系中的两个体系中实现了更高的风险调整回报。对于波动率预测，角度编码 QSVR 在 \texttt{KCHOL} 上获得 \textbf{最低 QLIKE}，并在 \texttt{S\&P~500} 和 \texttt{AAPL} 上保持在最佳经典内核的 $\sim$0.02-0.04 QLIKE 范围内。我们的基准测试框架清楚地识别了当前 QML 架构提供切实改进以及已建立的经典方法继续占主导地位的场景。</li>
</ul>

<h3>Title: From Brute Force to Semantic Insight: Performance-Guided Data Transformation Design with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Usha Shrestha, Dmitry Ignatov, Radu Timofte</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03808">https://arxiv.org/abs/2601.03808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03808">https://arxiv.org/pdf/2601.03808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03808]] From Brute Force to Semantic Insight: Performance-Guided Data Transformation Design with LLMs(https://arxiv.org/abs/2601.03808)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved notable performance in code synthesis; however, data-aware augmentation remains a limiting factor, handled via heuristic design or brute-force approaches. We introduce a performance-aware, closed-loop solution in the NNGPT ecosystem of projects that enables LLMs to autonomously engineer optimal transformations by internalizing empirical performance cues. We fine-tune LLMs with Low-Rank Adaptation on a novel repository of more than 6,000 empirically evaluated PyTorch augmentation functions, each annotated solely by downstream model accuracy. Training uses pairwise performance ordering (better-worse transformations), enabling alignment through empirical feedback without reinforcement learning, reward models, or symbolic objectives. This reduces the need for exhaustive search, achieving up to 600x times fewer evaluated candidates than brute-force discovery while maintaining competitive peak accuracy and shifting generation from random synthesis to task-aligned design. Ablation studies show that structured Chain-of-Thought prompting introduces syntactic noise and degrades performance, whereas direct prompting ensures stable optimization in performance-critical code tasks. Qualitative and quantitative analyses demonstrate that the model internalizes semantic performance cues rather than memorizing syntax. These results show that LLMs can exhibit task-level reasoning through non-textual feedback loops, bypassing explicit symbolic rewards.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在代码合成方面取得了显着的性能；然而，数据感知增强仍然是一个限制因素，需要通过启发式设计或强力方法来处理。我们在 NNGPT 项目生态系统中引入了一种性能感知的闭环解决方案，使法学硕士能够通过内化经验性能线索来自主设计最佳转换。我们在一个包含 6,000 多个经过经验评估的 PyTorch 增强函数的新颖存储库上对 LLM 进行低秩适应微调，每个函数仅由下游模型精度进行注释。训练使用成对性能排序（更好-更差转换），通过经验反馈实现对齐，无需强化学习、奖励模型或符号目标。这减少了穷举搜索的需要，与强力发现相比，评估的候选者数量减少了 600 倍，同时保持有竞争力的峰值精度，并将生成从随机合成转变为任务对齐设计。消融研究表明，结构化思维链提示会引入语法噪音并降低性能，而直接提示可确保性能关键代码任务的稳定优化。定性和定量分析表明，该模型内化了语义表现线索，而不是记住语法。这些结果表明，法学硕士可以通过非文本反馈循环展示任务级推理，绕过明确的符号奖励。</li>
</ul>

<h3>Title: Logic Tensor Network-Enhanced Generative Adversarial Network</h3>
<ul>
<li><strong>Authors: </strong>Nijesh Upreti (The University of Edinburgh), Vaishak Belle (The University of Edinburgh)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03839">https://arxiv.org/abs/2601.03839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03839">https://arxiv.org/pdf/2601.03839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03839]] Logic Tensor Network-Enhanced Generative Adversarial Network(https://arxiv.org/abs/2601.03839)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce Logic Tensor Network-Enhanced Generative Adversarial Network (LTN-GAN), a novel framework that enhances Generative Adversarial Networks (GANs) by incorporating Logic Tensor Networks (LTNs) to enforce domain-specific logical constraints during the sample generation process. Although GANs have shown remarkable success in generating realistic data, they often lack mechanisms to incorporate prior knowledge or enforce logical consistency, limiting their applicability in domains requiring rule adherence. LTNs provide a principled way to integrate first-order logic with neural networks, enabling models to reason over and satisfy logical constraints. By combining the strengths of GANs for realistic data synthesis with LTNs for logical reasoning, we gain valuable insights into how logical constraints influence the generative process while improving both the diversity and logical consistency of the generated samples. We evaluate LTN-GAN across multiple datasets, including synthetic datasets (gaussian, grid, rings) and the MNIST dataset, demonstrating that our model significantly outperforms traditional GANs in terms of adherence to predefined logical constraints while maintaining the quality and diversity of generated samples. This work highlights the potential of neuro-symbolic approaches to enhance generative modeling in knowledge-intensive domains.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了逻辑张量网络增强型生成对抗网络（LTN-GAN），这是一种新颖的框架，通过合并逻辑张量网络（LTN）来增强生成对抗网络（GAN），以在样本生成过程中强制执行特定于域的逻辑约束。尽管 GAN 在生成真实数据方面取得了显着的成功，但它们通常缺乏整合先验知识或强制逻辑一致性的机制，限制了它们在需要遵守规则的领域的适用性。 LTN 提供了一种将一阶逻辑与神经网络集成的原则性方法，使模型能够推理并满足逻辑约束。通过将 GAN 用于现实数据合成的优势与 LTN 用于逻辑推理的优势相结合，我们获得了关于逻辑约束如何影响生成过程的宝贵见解，同时提高了生成样本的多样性和逻辑一致性。我们跨多个数据集评估 LTN-GAN，包括合成数据集（高斯、网格、环）和 MNIST 数据集，证明我们的模型在遵守预定义逻辑约束方面显着优于传统 GAN，同时保持生成样本的质量和多样性。这项工作强调了神经符号方法在增强知识密集型领域的生成模型方面的潜力。</li>
</ul>

<h3>Title: Feature-Aware One-Shot Federated Learning via Hierarchical Token Sequences</h3>
<ul>
<li><strong>Authors: </strong>Shudong Liu, Hanwen Zhang, Xiuling Wang, Yuesheng Zhu, Guibo Luo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03882">https://arxiv.org/abs/2601.03882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03882">https://arxiv.org/pdf/2601.03882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03882]] Feature-Aware One-Shot Federated Learning via Hierarchical Token Sequences(https://arxiv.org/abs/2601.03882)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>One-shot federated learning (OSFL) reduces the communication cost and privacy risks of iterative federated learning by constructing a global model with a single round of communication. However, most existing methods struggle to achieve robust performance on real-world domains such as medical imaging, or are inefficient when handling non-IID (Independent and Identically Distributed) data. To address these limitations, we introduce FALCON, a framework that enhances the effectiveness of OSFL over non-IID image data. The core idea of FALCON is to leverage the feature-aware hierarchical token sequences generation and knowledge distillation into OSFL. First, each client leverages a pretrained visual encoder with hierarchical scale encoding to compress images into hierarchical token sequences, which capture multi-scale semantics. Second, a multi-scale autoregressive transformer generator is used to model the distribution of these token sequences and generate the synthetic sequences. Third, clients upload the synthetic sequences along with the local classifier trained on the real token sequences to the server. Finally, the server incorporates knowledge distillation into global training to reduce reliance on precise distribution modeling. Experiments on medical and natural image datasets validate the effectiveness of FALCON in diverse non-IID scenarios, outperforming the best OSFL baselines by 9.58% in average accuracy.</li>
<li><strong>摘要：</strong>一次性联邦学习（OSFL）通过单轮通信构建全局模型，降低了迭代联邦学习的通信成本和隐私风险。然而，大多数现有方法很难在医学成像等现实领域实现稳健的性能，或者在处理非 IID（独立同分布）数据时效率低下。为了解决这些限制，我们引入了 FALCON，这是一个可以增强 OSFL 相对于非 IID 图像数据的有效性的框架。 FALCON 的核心思想是将特征感知的分层令牌序列生成和知识蒸馏利用到 OSFL 中。首先，每个客户端利用具有分层尺度编码的预训练视觉编码器将图像压缩为分层令牌序列，从而捕获多尺度语义。其次，使用多尺度自回归变压器生成器对这些令牌序列的分布进行建模并生成合成序列。第三，客户端将合成序列以及在真实令牌序列上训练的本地分类器上传到服务器。最后，服务器将知识蒸馏纳入全局训练中，以减少对精确分布建模的依赖。在医学和自然图像数据集上的实验验证了 FALCON 在各种非 IID 场景中的有效性，其平均准确度比最佳 OSFL 基线高出 9.58%。</li>
</ul>

<h3>Title: FLNet: Flood-Induced Agriculture Damage Assessment using Super Resolution of Satellite Images</h3>
<ul>
<li><strong>Authors: </strong>Sanidhya Ghosal, Anurag Sharma, Sushil Ghildiyal, Mukesh Saini</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03884">https://arxiv.org/abs/2601.03884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03884">https://arxiv.org/pdf/2601.03884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03884]] FLNet: Flood-Induced Agriculture Damage Assessment using Super Resolution of Satellite Images(https://arxiv.org/abs/2601.03884)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Distributing government relief efforts after a flood is challenging. In India, the crops are widely affected by floods; therefore, making rapid and accurate crop damage assessment is crucial for effective post-disaster agricultural management. Traditional manual surveys are slow and biased, while current satellite-based methods face challenges like cloud cover and low spatial resolution. Therefore, to bridge this gap, this paper introduced FLNet, a novel deep learning based architecture that used super-resolution to enhance the 10 m spatial resolution of Sentinel-2 satellite images into 3 m resolution before classifying damage. We tested our model on the Bihar Flood Impacted Croplands Dataset (BFCD-22), and the results showed an improved critical "Full Damage" F1-score from 0.83 to 0.89, nearly matching the 0.89 score of commercial high-resolution imagery. This work presented a cost-effective and scalable solution, paving the way for a nationwide shift from manual to automated, high-fidelity damage assessment.</li>
<li><strong>摘要：</strong>洪水过后分配政府救援工作具有挑战性。在印度，农作物受到洪水的广泛影响；因此，快速、准确地评估农作物受损情况对于灾后农业管理至关重要。传统的手动调查速度缓慢且存在偏差，而当前基于卫星的方法面临云层覆盖和空间分辨率低等挑战。因此，为了弥补这一差距，本文引入了 FLNet，这是一种基于深度学习的新型架构，在对损坏进行分类之前，它使用超分辨率将 Sentinel-2 卫星图像的 10 m 空间分辨率提高到 3 m 分辨率。我们在比哈尔邦洪水影响农田数据集 (BFCD-22) 上测试了我们的模型，结果显示临界“完全损坏”F1 分数从 0.83 提高到 0.89，几乎与商业高分辨率图像的 0.89 分数相匹配。这项工作提出了一种经济高效且可扩展的解决方案，为全国范围内从手动到自动化、高保真损失评估的转变铺平了道路。</li>
</ul>

<h3>Title: ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Xu Zhang, Cheng Da, Huan Yang, Kun Gai, Ming Lu, Zhan Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03955">https://arxiv.org/abs/2601.03955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03955">https://arxiv.org/pdf/2601.03955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03955]] ResTok: Learning Hierarchical Residuals in 1D Visual Tokenizers for Autoregressive Image Generation(https://arxiv.org/abs/2601.03955)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing 1D visual tokenizers for autoregressive (AR) generation largely follow the design principles of language modeling, as they are built directly upon transformers whose priors originate in language, yielding single-hierarchy latent tokens and treating visual data as flat sequential token streams. However, this language-like formulation overlooks key properties of vision, particularly the hierarchical and residual network designs that have long been essential for convergence and efficiency in visual models. To bring "vision" back to vision, we propose the Residual Tokenizer (ResTok), a 1D visual tokenizer that builds hierarchical residuals for both image tokens and latent tokens. The hierarchical representations obtained through progressively merging enable cross-level feature fusion at each layer, substantially enhancing representational capacity. Meanwhile, the semantic residuals between hierarchies prevent information overlap, yielding more concentrated latent distributions that are easier for AR modeling. Cross-level bindings consequently emerge without any explicit constraints. To accelerate the generation process, we further introduce a hierarchical AR generator that substantially reduces sampling steps by predicting an entire level of latent tokens at once rather than generating them strictly token-by-token. Extensive experiments demonstrate that restoring hierarchical residual priors in visual tokenization significantly improves AR image generation, achieving a gFID of 2.34 on ImageNet-256 with only 9 sampling steps. Code is available at this https URL.</li>
<li><strong>摘要：</strong>现有的用于自回归（AR）生成的一维视觉分词器很大程度上遵循语言建模的设计原则，因为它们直接构建在先验源自语言的变压器上，产生单层次的潜在标记并将视觉数据视为平面顺序标记流。然而，这种类似语言的表述忽略了视觉的关键属性，特别是长期以来对于视觉模型的收敛和效率至关重要的分层和残差网络设计。为了让“视觉”回归视觉，我们提出了残差分词器（ResTok），这是一种一维视觉分词器，可为图像标记和潜在标记构建分层残差。通过逐步合并获得的分层表示可以实现每一层的跨级特征融合，从而大大增强表示能力。同时，层次结构之间的语义残差可以防止信息重叠，从而产生更集中的潜在分布，更容易进行 AR 建模。因此，跨级别绑定的出现没有任何明确的约束。为了加速生成过程，我们进一步引入了分层 AR 生成器，该生成器通过一次性预测整个级别的潜在标记而不是严格逐个标记地生成它们，从而大大减少了采样步骤。大量实验表明，恢复视觉标记化中的分层残差先验可显着改善 AR 图像生成，仅用 9 个采样步骤即可在 ImageNet-256 上实现 2.34 的 gFID。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: PosterVerse: A Full-Workflow Framework for Commercial-Grade Poster Generation with HTML-Based Scalable Typography</h3>
<ul>
<li><strong>Authors: </strong>Junle Liu, Peirong Zhang, Yuyi Zhang, Pengyu Yan, Hui Zhou, Xinyue Zhou, Fengjun Guo, Lianwen Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.03993">https://arxiv.org/abs/2601.03993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.03993">https://arxiv.org/pdf/2601.03993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.03993]] PosterVerse: A Full-Workflow Framework for Commercial-Grade Poster Generation with HTML-Based Scalable Typography(https://arxiv.org/abs/2601.03993)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Commercial-grade poster design demands the seamless integration of aesthetic appeal with precise, informative content delivery. Current automated poster generation systems face significant limitations, including incomplete design workflows, poor text rendering accuracy, and insufficient flexibility for commercial applications. To address these challenges, we propose PosterVerse, a full-workflow, commercial-grade poster generation method that seamlessly automates the entire design process while delivering high-density and scalable text rendering. PosterVerse replicates professional design through three key stages: (1) blueprint creation using fine-tuned LLMs to extract key design elements from user requirements, (2) graphical background generation via customized diffusion models to create visually appealing imagery, and (3) unified layout-text rendering with an MLLM-powered HTML engine to guarantee high text accuracy and flexible customization. In addition, we introduce PosterDNA, a commercial-grade, HTML-based dataset tailored for training and validating poster design models. To the best of our knowledge, PosterDNA is the first Chinese poster generation dataset to introduce HTML typography files, enabling scalable text rendering and fundamentally solving the challenges of rendering small and high-density text. Experimental results demonstrate that PosterVerse consistently produces commercial-grade posters with appealing visuals, accurate text alignment, and customizable layouts, making it a promising solution for automating commercial poster design. The code and model are available at this https URL.</li>
<li><strong>摘要：</strong>商业级海报设计需要将审美吸引力与精确、信息丰富的内容传递无缝结合。当前的自动海报生成系统面临着重大限制，包括不完整的设计工作流程、文本渲染精度差以及商业应用灵活性不足。为了应对这些挑战，我们提出了 PosterVerse，这是一种完整工作流程的商业级海报生成方法，可以无缝地自动化整个设计过程，同时提供高密度和可扩展的文本渲染。 PosterVerse 通过三个关键阶段复制专业设计：(1) 使用微调的 LLM 创建蓝图，从用户需求中提取关键设计元素，(2) 通过定制的扩散模型生成图形背景，以创建视觉上吸引人的图像，以及 (3) 使用 MLLM 支持的 HTML 引擎进行统一的布局文本渲染，以保证高文本准确性和灵活的定制。此外，我们还推出了 PosterDNA，这是一个商业级的、基于 HTML 的数据集，专为训练和验证海报设计模型而定制。据我们所知，PosterDNA是第一个引入HTML排版文件的中文海报生成数据集，实现可扩展的文本渲染，从根本上解决渲染小而高密度文本的挑战。实验结果表明，PosterVerse 始终能够生成具有吸引人的视觉效果、准确的文本对齐和可定制布局的商业级海报，使其成为自动化商业海报设计的有前途的解决方案。代码和模型可从此 https URL 获取。</li>
</ul>

<h3>Title: Padé Neurons for Efficient Neural Models</h3>
<ul>
<li><strong>Authors: </strong>Onur Keleş, A. Murat Tekalp</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04005">https://arxiv.org/abs/2601.04005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04005">https://arxiv.org/pdf/2601.04005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04005]] Padé Neurons for Efficient Neural Models(https://arxiv.org/abs/2601.04005)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>Neural networks commonly employ the McCulloch-Pitts neuron model, which is a linear model followed by a point-wise non-linear activation. Various researchers have already advanced inherently non-linear neuron models, such as quadratic neurons, generalized operational neurons, generative neurons, and super neurons, which offer stronger non-linearity compared to point-wise activation functions. In this paper, we introduce a novel and better non-linear neuron model called Padé neurons (Paons), inspired by Padé approximants. Paons offer several advantages, such as diversity of non-linearity, since each Paon learns a different non-linear function of its inputs, and layer efficiency, since Paons provide stronger non-linearity in much fewer layers compared to piecewise linear approximation. Furthermore, Paons include all previously proposed neuron models as special cases, thus any neuron model in any network can be replaced by Paons. We note that there has been a proposal to employ the Padé approximation as a generalized point-wise activation function, which is fundamentally different from our model. To validate the efficacy of Paons, in our experiments, we replace classic neurons in some well-known neural image super-resolution, compression, and classification models based on the ResNet architecture with Paons. Our comprehensive experimental results and analyses demonstrate that neural models built by Paons provide better or equal performance than their classic counterparts with a smaller number of layers. The PyTorch implementation code for Paon is open-sourced at this https URL.</li>
<li><strong>摘要：</strong>神经网络通常采用 McCulloch-Pitts 神经元模型，这是一个线性模型，后跟逐点非线性激活。许多研究人员已经提出了固有的非线性神经元模型，例如二次神经元、广义运算神经元、生成神经元和超级神经元，与逐点激活函数相比，它们提供了更强的非线性。在本文中，我们介绍了一种新颖且更好的非线性神经元模型，称为 Padé 神经元（Paons），其灵感来自 Padé 近似。 Paons 具有多种优势，例如非线性多样性（因为每个 Paon 都会学习其输入的不同非线性函数）和层效率（因为与分段线性逼近相比，Paons 在更少的层中提供更强的非线性）。此外，Paons 包括所有先前提出的神经元模型作为特例，因此任何网络中的任何神经元模型都可以被 Paons 替换。我们注意到有人建议采用 Padé 近似作为广义的逐点激活函数，这与我们的模型有根本的不同。为了验证Paons的功效，在我们的实验中，我们将一些著名的基于ResNet架构的神经图像超分辨率、压缩和分类模型中的经典神经元替换为Paons。我们全面的实验结果和分析表明，Paons 构建的神经模型比层数较少的经典模型提供更好或相同的性能。 Paon 的 PyTorch 实现代码在此 https URL 上开源。</li>
</ul>

<h3>Title: Thinking with Frames: Generative Video Distortion Evaluation via Frame Reward Model</h3>
<ul>
<li><strong>Authors: </strong>Yuan Wang, Borui Liao, Huijuan Huang, Jinda Lu, Ouxiang Li, Kuien Liu, Meng Wang, Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04033">https://arxiv.org/abs/2601.04033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04033">https://arxiv.org/pdf/2601.04033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04033]] Thinking with Frames: Generative Video Distortion Evaluation via Frame Reward Model(https://arxiv.org/abs/2601.04033)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in video reward models and post-training strategies have improved text-to-video (T2V) generation. While these models typically assess visual quality, motion quality, and text alignment, they often overlook key structural distortions, such as abnormal object appearances and interactions, which can degrade the overall quality of the generative video. To address this gap, we introduce REACT, a frame-level reward model designed specifically for structural distortions evaluation in generative videos. REACT assigns point-wise scores and attribution labels by reasoning over video frames, focusing on recognizing distortions. To support this, we construct a large-scale human preference dataset, annotated based on our proposed taxonomy of structural distortions, and generate additional data using a efficient Chain-of-Thought (CoT) synthesis pipeline. REACT is trained with a two-stage framework: ((1) supervised fine-tuning with masked loss for domain knowledge injection, followed by (2) reinforcement learning with Group Relative Policy Optimization (GRPO) and pairwise rewards to enhance reasoning capability and align output scores with human preferences. During inference, a dynamic sampling mechanism is introduced to focus on frames most likely to exhibit distortion. We also present REACT-Bench, a benchmark for generative video distortion evaluation. Experimental results demonstrate that REACT complements existing reward models in assessing structutal distortion, achieving both accurate quantitative evaluations and interpretable attribution analysis.</li>
<li><strong>摘要：</strong>视频奖励模型和训练后策略的最新进展改进了文本到视频（T2V）的生成。虽然这些模型通常评估视觉质量、运动质量和文本对齐，但它们经常忽略关键的结构扭曲，例如异常的对象外观和交互，这可能会降低生成视频的整体质量。为了解决这一差距，我们引入了 REACT，这是一种专门为生成视频中的结构扭曲评估而设计的帧级奖励模型。 REACT 通过对视频帧进行推理来分配逐点分数和归因标签，重点是识别失真。为了支持这一点，我们构建了一个大规模的人类偏好数据集，根据我们提出的结构扭曲分类法进行注释，并使用高效的思想链（CoT）合成管道生成附加数据。 REACT 采用两阶段框架进行训练：（（1）使用屏蔽损失进行监督微调，以进行领域知识注入，然后是（2）使用组相对策略优化（GRPO）和成对奖励进行强化学习，以增强推理能力并使输出分数与人类偏好保持一致。在推理过程中，引入动态采样机制来关注最有可能表现出失真的帧。我们还提出了 REACT-Bench，这是生成视频失真评估的基准。实验结果表明，REACT 补充了现有的奖励模型评估结构扭曲，实现准确的定量评估和可解释的归因分析。</li>
</ul>

<h3>Title: LinkD: AutoRegressive Diffusion Model for Mechanical Linkage Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Yayati Jadhav, Amir Barati Farimani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04054">https://arxiv.org/abs/2601.04054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04054">https://arxiv.org/pdf/2601.04054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04054]] LinkD: AutoRegressive Diffusion Model for Mechanical Linkage Synthesis(https://arxiv.org/abs/2601.04054)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Designing mechanical linkages to achieve target end-effector trajectories presents a fundamental challenge due to the intricate coupling between continuous node placements, discrete topological configurations, and nonlinear kinematic constraints. The highly nonlinear motion-to-configuration relationship means small perturbations in joint positions drastically alter trajectories, while the combinatorially expanding design space renders conventional optimization and heuristic methods computationally intractable. We introduce an autoregressive diffusion framework that exploits the dyadic nature of linkage assembly by representing mechanisms as sequentially constructed graphs, where nodes correspond to joints and edges to rigid links. Our approach combines a causal transformer with a Denoising Diffusion Probabilistic Model (DDPM), both conditioned on target trajectories encoded via a transformer encoder. The causal transformer autoregressively predicts discrete topology node-by-node, while the DDPM refines each node's spatial coordinates and edge connectivity to previously generated nodes. This sequential generation enables adaptive trial-and-error synthesis where problematic nodes exhibiting kinematic locking or collisions can be selectively regenerated, allowing autonomous correction of degenerate configurations during design. Our graph-based, data-driven methodology surpasses traditional optimization approaches, enabling scalable inverse design that generalizes to mechanisms with arbitrary node counts. We demonstrate successful synthesis of linkage systems containing up to 20 nodes with extensibility to N-node architectures. This work advances autoregressive graph generation methodologies and computational kinematic synthesis, establishing new paradigms for scalable inverse design of complex mechanical systems.</li>
<li><strong>摘要：</strong>由于连续节点放置、离散拓扑配置和非线性运动学约束之间的复杂耦合，设计机械连杆以实现目标末端执行器轨迹提出了根本性挑战。高度非线性的运动与配置关系意味着关节位置的小扰动会极大地改变轨迹，而组合扩展的设计空间使得传统的优化和启发式方法在计算上变得难以处理。我们引入了一种自回归扩散框架，该框架通过将机制表示为顺序构造的图来利用连杆组装的二元性质，其中节点对应于关节，边缘对应于刚性连杆。我们的方法将因果变换器与去噪扩散概率模型（DDPM）相结合，两者都以通过变换器编码器编码的目标轨迹为条件。因果变换器自回归逐节点预测离散拓扑，而 DDPM 则细化每个节点的空间坐标和与先前生成的节点的边缘连接。这种顺序生成实现了自适应试错综合，其中表现出运动锁定或碰撞的有问题的节点可以选择性地重新生成，从而允许在设计期间自主纠正简并配置。我们基于图形的数据驱动方法超越了传统的优化方法，实现了可扩展的逆向设计，可推广到具有任意节点数的机制。我们展示了包含多达 20 个节点的链接系统的成功综合，并且可扩展至 N 节点架构。这项工作推进了自回归图生成方法和计算运动学综合，为复杂机械系统的可扩展逆向设计建立了新的范例。</li>
</ul>

<h3>Title: Using Legacy Polysomnography Data to Train a Radar System to Quantify Sleep in Older Adults and People living with Dementia</h3>
<ul>
<li><strong>Authors: </strong>M. Yin, K. G. Ravindran, C. Hadjipanayi, A. Bannon, A. Rapeaux, C. Della Monica, T. S. Lande, Derk-Jan Dijk, T. G. Constandinou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04057">https://arxiv.org/abs/2601.04057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04057">https://arxiv.org/pdf/2601.04057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04057]] Using Legacy Polysomnography Data to Train a Radar System to Quantify Sleep in Older Adults and People living with Dementia(https://arxiv.org/abs/2601.04057)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Objective: Ultra-wideband radar technology offers a promising solution for unobtrusive and cost-effective in-home sleep monitoring. However, the limited availability of radar sleep data poses challenges in building robust models that generalize across diverse cohorts and environments. This study proposes a novel deep transfer learning framework to enhance sleep stage classification using radar data. Methods: An end-to-end neural network was developed to classify sleep stages based on nocturnal respiratory and motion signals. The network was trained using a combination of large-scale polysomnography (PSG) datasets and radar data. A domain adaptation approach employing adversarial learning was utilized to bridge the knowledge gap between PSG and radar signals. Validation was performed on a radar dataset of 47 older adults (mean age: 71.2), including 18 participants with prodromal or mild Alzheimer disease. Results: The proposed network structure achieves an accuracy of 79.5% with a Kappa value of 0.65 when classifying wakefulness, rapid eye movement, light sleep and deep sleep. Experimental results confirm that our deep transfer learning approach significantly enhances automatic sleep staging performance in the target domain. Conclusion: This method effectively addresses challenges associated with data variability and limited sample size, substantially improving the reliability of automatic sleep staging models, especially in contexts where radar data is limited. Significance: The findings underscore the viability of UWB radar as a nonintrusive, forward-looking sleep assessment tool that could significantly benefit care for older people and people with neurodegenerative disorders.</li>
<li><strong>摘要：</strong>目标：超宽带雷达技术为不引人注目且经济高效的家庭睡眠监测提供了一种有前景的解决方案。然而，雷达睡眠数据的可用性有限，这给构建跨不同群体和环境的稳健模型带来了挑战。这项研究提出了一种新颖的深度迁移学习框架，以利用雷达数据增强睡眠阶段分类。方法：开发了一种端到端神经网络，根据夜间呼吸和运动信号对睡眠阶段进行分类。该网络使用大规模多导睡眠图 (PSG) 数据集和雷达数据的组合进行训练。采用对抗性学习的领域适应方法被用来弥合 PSG 和雷达信号之间的知识差距。对 47 名老年人（平均年龄：71.2 岁）的雷达数据集进行了验证，其中包括 18 名患有前驱期或轻度阿尔茨海默病的参与者。结果：所提出的网络结构在对清醒、快速眼动、浅度睡眠和深度睡眠进行分类时，准确率达到 79.5%，Kappa 值为 0.65。实验结果证实，我们的深度迁移学习方法显着增强了目标领域的自动睡眠分期性能。结论：该方法有效地解决了与数据可变性和有限样本量相关的挑战，显着提高了自动睡眠分期模型的可靠性，特别是在雷达数据有限的情况下。意义：研究结果强调了超宽带雷达作为一种非侵入式、前瞻性睡眠评估工具的可行性，可以显着有益于老年人和神经退行性疾病患者的护理。</li>
</ul>

<h3>Title: Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zitong Huang, Kaidong Zhang, Yukang Ding, Chao Gao, Rui Ding, Ying Chen, Wangmeng Zuo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04068">https://arxiv.org/abs/2601.04068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04068">https://arxiv.org/pdf/2601.04068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04068]] Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models(https://arxiv.org/abs/2601.04068)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Aligning text-to-video diffusion models with human preferences is crucial for generating high-quality videos. Existing Direct Preference Otimization (DPO) methods rely on multi-sample ranking and task-specific critic models, which is inefficient and often yields ambiguous global supervision. To address these limitations, we propose LocalDPO, a novel post-training framework that constructs localized preference pairs from real videos and optimizes alignment at the spatio-temporal region level. We design an automated pipeline to efficiently collect preference pair data that generates preference pairs with a single inference per prompt, eliminating the need for external critic models or manual annotation. Specifically, we treat high-quality real videos as positive samples and generate corresponding negatives by locally corrupting them with random spatio-temporal masks and restoring only the masked regions using the frozen base model. During training, we introduce a region-aware DPO loss that restricts preference learning to corrupted areas for rapid convergence. Experiments on Wan2.1 and CogVideoX demonstrate that LocalDPO consistently improves video fidelity, temporal coherence and human preference scores over other post-training approaches, establishing a more efficient and fine-grained paradigm for video generator alignment.</li>
<li><strong>摘要：</strong>将文本到视频的扩散模型与人类偏好保持一致对于生成高质量视频至关重要。现有的直接偏好优化（DPO）方法依赖于多样本排序和特定于任务的批评模型，这种方法效率低下，并且经常产生模糊的全局监督。为了解决这些限制，我们提出了 LocalDPO，这是一种新颖的后训练框架，它从真实视频构建本地化偏好对并优化时空区域级别的对齐。我们设计了一个自动化管道来有效地收集偏好对数据，该数据通过每个提示进行一次推理来生成偏好对，从而消除了对外部批评家模型或手动注释的需要。具体来说，我们将高质量的真实视频视为正样本，并通过使用随机时空掩模局部破坏它们并使用冻结的基础模型仅恢复掩模区域来生成相应的负样本。在训练过程中，我们引入了区域感知 DPO 损失，将偏好学习限制在损坏区域以实现快速收敛。 Wan2.1 和 CogVideoX 上的实验表明，与其他训练后方法相比，LocalDPO 持续提高了视频保真度、时间一致性和人类偏好分数，为视频生成器对齐建立了更高效、更细粒度的范例。</li>
</ul>

<h3>Title: Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Huang, Yuanbo Yang, Bangbang Yang, Lin Ma, Yuewen Ma, Yiyi Liao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04090">https://arxiv.org/abs/2601.04090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04090">https://arxiv.org/pdf/2601.04090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04090]] Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction(https://arxiv.org/abs/2601.04090)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We present Gen3R, a method that bridges the strong priors of foundational reconstruction models and video diffusion models for scene-level 3D generation. We repurpose the VGGT reconstruction model to produce geometric latents by training an adapter on its tokens, which are regularized to align with the appearance latents of pre-trained video diffusion models. By jointly generating these disentangled yet aligned latents, Gen3R produces both RGB videos and corresponding 3D geometry, including camera poses, depth maps, and global point clouds. Experiments demonstrate that our approach achieves state-of-the-art results in single- and multi-image conditioned 3D scene generation. Additionally, our method can enhance the robustness of reconstruction by leveraging generative priors, demonstrating the mutual benefit of tightly coupling reconstruction and generative models.</li>
<li><strong>摘要：</strong>我们提出了 Gen3R，一种将基础重建模型和视频扩散模型的强大先验联系起来的方法，用于场景级 3D 生成。我们重新调整 VGGT 重建模型的用途，通过在其标记上训练适配器来产生几何潜在特征，这些标记被正则化以与预训练视频扩散模型的外观潜在特征保持一致。通过联合生成这些解开但对齐的潜在变量，Gen3R 可以生成 RGB 视频和相应的 3D 几何图形，包括相机姿势、深度图和全局点云。实验表明，我们的方法在单图像和多图像条件 3D 场景生成方面取得了最先进的结果。此外，我们的方法可以通过利用生成先验来增强重建的鲁棒性，证明了重建和生成模型紧密耦合的互惠互利。</li>
</ul>

<h3>Title: Klear: Unified Multi-Task Audio-Video Joint Generation</h3>
<ul>
<li><strong>Authors: </strong>Jun Wang, Chunyu Qiang, Yuxin Guo, Yiran Wang, Xijuan Zeng, Chen Zhang, Pengfei Wan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04151">https://arxiv.org/abs/2601.04151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04151">https://arxiv.org/pdf/2601.04151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04151]] Klear: Unified Multi-Task Audio-Video Joint Generation(https://arxiv.org/abs/2601.04151)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.</li>
<li><strong>摘要：</strong>音视频联合生成进展迅速，但仍然存在巨大挑战。非商业方法仍然面临视听异步、口语对齐不佳和单峰退化的问题，这可能是由于视听对应建模薄弱、泛化能力有限和高质量密集字幕数据稀缺造成的。为了解决这些问题，我们引入 Klear 并深入研究三个轴：模型架构、训练策略和数据管理。在架构上，我们采用单塔设计，具有统一的DiT块和Omni-Full Attention机制，实现了紧密的视听对齐和强大的可扩展性。在训练方面，我们采用渐进式多任务机制——随机模态掩蔽到跨任务联合优化，以及多阶段课程，产生稳健的表示，加强 A-V 一致的世界知识，并防止单峰崩溃。对于数据集，我们提出了第一个具有密集字幕的大规模音频视频数据集，并引入了一种新颖的自动化数据构建管道，该管道可以注释和过滤数百万个多样化、高质量、严格对齐的音频视频字幕三元组。在此基础上，Klear 可以扩展到大型数据集，在联合和单模态设置中提供高保真、语义和时间对齐的指令跟踪生成，同时稳健地推广到分布外场景。在各种任务中，它的性能大幅优于以前的方法，并实现了与 Veo 3 相当的性能，为下一代音频视频合成提供了统一、可扩展的路径。</li>
</ul>

<h3>Title: Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yifan Wang, Yanyu Li, Sergey Tulyakov, Yun Fu, Anil Kag</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04153">https://arxiv.org/abs/2601.04153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04153">https://arxiv.org/pdf/2601.04153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04153]] Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning(https://arxiv.org/abs/2601.04153)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Direct Preference Optimization (DPO) has recently improved Text-to-Video (T2V) generation by enhancing visual fidelity and text alignment. However, current methods rely on non-differentiable preference signals from human annotations or learned reward models. This reliance makes training label-intensive, bias-prone, and easy-to-game, which often triggers reward hacking and unstable training. We propose Diffusion-DRF, a differentiable reward flow for fine-tuning video diffusion models using a frozen, off-the-shelf Vision-Language Model (VLM) as a training-free critic. Diffusion-DRF directly backpropagates VLM feedback through the diffusion denoising chain, converting logit-level responses into token-aware gradients for optimization. We propose an automated, aspect-structured prompting pipeline to obtain reliable multi-dimensional VLM feedback, while gradient checkpointing enables efficient updates through the final denoising steps. Diffusion-DRF improves video quality and semantic alignment while mitigating reward hacking and collapse -- without additional reward models or preference datasets. It is model-agnostic and readily generalizes to other diffusion-based generative tasks.</li>
<li><strong>摘要：</strong>直接偏好优化 (DPO) 最近通过增强视觉保真度和文本对齐来改进文本到视频 (T2V) 的生成。然而，当前的方法依赖于来自人类注释或学习奖励模型的不可微偏好信号。这种依赖使得训练标签密集、容易出现偏差且易于博弈，这通常会引发奖励黑客攻击和不稳定的训练。我们提出了 Diffusion-DRF，这是一种可微分奖励流，用于使用冻结的现成视觉语言模型（VLM）作为免训练批评家来微调视频扩散模型。 Diffusion-DRF 通过扩散去噪链直接反向传播 VLM 反馈，将 logit 级响应转换为令牌感知梯度以进行优化。我们提出了一个自动化的、方面结构的提示管道来获得可靠的多维 VLM 反馈，而梯度检查点可以通过最终的去噪步骤实现高效的更新。 Diffusion-DRF 提高了视频质量和语义对齐，同时减轻奖励黑客攻击和崩溃——无需额外的奖励模型或偏好数据集。它与模型无关，并且很容易推广到其他基于扩散的生成任务。</li>
</ul>

<h3>Title: Choreographing a World of Dynamic Objects</h3>
<ul>
<li><strong>Authors: </strong>Yanzhe Lyu, Chen Geng, Karthik Dharmarajan, Yunzhi Zhang, Hadi Alzayer, Shangzhe Wu, Jiajun Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.04194">https://arxiv.org/abs/2601.04194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.04194">https://arxiv.org/pdf/2601.04194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.04194]] Choreographing a World of Dynamic Objects(https://arxiv.org/abs/2601.04194)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: this https URL</li>
<li><strong>摘要：</strong>物理 4D（3D + 时间）世界中的动态对象不断演化、变形并与其他对象交互，从而产生多样化的 4D 场景动态。在本文中，我们提出了一种通用生成管道 CHORD，用于 CHOReographing 动态对象和场景并合成此类现象。用于创建这些动态的传统基于规则的图形管道基于特定类别的启发式方法，但属于劳动密集型且不可扩展。最近基于学习的方法通常需要大规模数据集，这可能无法涵盖所有​​感兴趣的对象类别。相反，我们的方法通过提出基于蒸馏的管道来提取隐藏在 2D 视频的欧拉表示中的丰富拉格朗日运动信息，从而继承了视频生成模型的普遍性。我们的方法是通用的、通用的并且与类别无关。我们通过实验生成各种多体 4D 动力学来证明其有效性，展示其与现有方法相比的优势，并证明其在生成机器人操纵策略方面的适用性。项目页面：此 https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
