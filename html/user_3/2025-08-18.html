<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-08-18</h1>
<h3>Title: VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By \underline{V}alue \underline{S}ign \underline{F}lip</h3>
<ul>
<li><strong>Authors: </strong>Wenqi Guo, Shan Du</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.10931">https://arxiv.org/abs/2508.10931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.10931">https://arxiv.org/pdf/2508.10931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.10931]] VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By \underline{V}alue \underline{S}ign \underline{F}lip(https://arxiv.org/abs/2508.10931)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce Value Sign Flip (VSF), a simple and efficient method for incorporating negative prompt guidance in few-step diffusion and flow-matching image generation models. Unlike existing approaches such as classifier-free guidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by flipping the sign of attention values from negative prompts. Our method requires only small computational overhead and integrates effectively with MMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as cross-attention-based models like Wan. We validate VSF on challenging datasets with complex prompt pairs and demonstrate superior performance in both static image and video generation tasks. Experimental results show that VSF significantly improves negative prompt adherence compared to prior methods in few-step models, and even CFG in non-few-step models, while maintaining competitive image quality. Code and ComfyUI node are available in this https URL.</li>
<li><strong>摘要：</strong>我们引入了值符号翻转（VSF），这是一种简单有效的方法，用于将负及时指导纳入几步扩散和流程匹配图像生成模型中。与现有的方法（例如无分类器指导（CFG），NASA和NAG）不同，VSF通过从负面提示中翻转注意值的迹象来动态抑制不希望的内容。我们的方法仅需要小的计算开销，并与MMDIT风格的体系结构有效整合，例如稳定的扩散3.5涡轮增压涡轮增压，以及基于WAN（WAN）的基于跨注意的模型。我们在具有复杂提示对的挑战数据集上验证VSF，并在静态图像和视频生成任务中展示出色的性能。实验结果表明，与少数步骤模型中的先前方法相比，VSF显着提高了负及时依从性，甚至在非步骤模型中甚至CFG，同时保持竞争性图像质量。代码和comfyui节点在此HTTPS URL中可用。</li>
</ul>

<h3>Title: HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model</h3>
<ul>
<li><strong>Authors: </strong>Qi Liu, Yabei Li, Hongsong Wang, Lei He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.10935">https://arxiv.org/abs/2508.10935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.10935">https://arxiv.org/pdf/2508.10935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.10935]] HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model(https://arxiv.org/abs/2508.10935)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Traditional closed-set 3D detection frameworks fail to meet the demands of open-world applications like autonomous driving. Existing open-vocabulary 3D detection methods typically adopt a two-stage pipeline consisting of pseudo-label generation followed by semantic alignment. While vision-language models (VLMs) recently have dramatically improved the semantic accuracy of pseudo-labels, their geometric quality, particularly bounding box precision, remains commonly this http URL address this issue, we propose a High Box Quality Open-Vocabulary 3D Detection (HQ-OV3D) framework, dedicated to generate and refine high-quality pseudo-labels for open-vocabulary classes. The framework comprises two key components: an Intra-Modality Cross-Validated (IMCV) Proposal Generator that utilizes cross-modality geometric consistency to generate high-quality initial 3D proposals, and an Annotated-Class Assisted (ACA) Denoiser that progressively refines 3D proposals by leveraging geometric priors from annotated categories through a DDIM-based denoising this http URL to the state-of-the-art method, training with pseudo-labels generated by our approach achieves a 7.37% improvement in mAP on novel classes, demonstrating the superior quality of the pseudo-labels produced by our framework. HQ-OV3D can serve not only as a strong standalone open-vocabulary 3D detector but also as a plug-in high-quality pseudo-label generator for existing open-vocabulary detection or annotation pipelines.</li>
<li><strong>摘要：</strong>传统的封闭式3D检测框架无法满足自动驾驶等开放世界应用的需求。现有的开放式Vocabulary 3D检测方法通常采用由伪标签生成组成的两阶段管道，然后进行语义比对。 While vision-language models (VLMs) recently have dramatically improved the semantic accuracy of pseudo-labels, their geometric quality, particularly bounding box precision, remains commonly this http URL address this issue, we propose a High Box Quality Open-Vocabulary 3D Detection (HQ-OV3D) framework, dedicated to generate and refine high-quality pseudo-labels for open-vocabulary课程。该框架包括两个关键组成部分：模式内交叉验证（IMCV）提案生成器，利用跨模式几何的一致性来产生高质量的初始3D建议，并通过逐步完善3D建议，从而逐步提出3D建议，从而逐步提高该基础的基础。最先进方法的URL，通过我们的方法生成的伪标签训练可在新颖类中提高7.37％的速度，这证明了我们框架产生的伪标签的优异品质。 HQ-OV3D不仅可以用作强大的独立开放式Vocabulary 3D检测器，而且还可以用作现有的开放式摄取检测或注释管道的插件高质量的伪标签生成器。</li>
</ul>

<h3>Title: Personalized Face Super-Resolution with Identity Decoupling and Fitting</h3>
<ul>
<li><strong>Authors: </strong>Jiarui Yang, Hang Guo, Wen Huang, Tao Dai, Shutao Xia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.10937">https://arxiv.org/abs/2508.10937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.10937">https://arxiv.org/pdf/2508.10937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.10937]] Personalized Face Super-Resolution with Identity Decoupling and Fitting(https://arxiv.org/abs/2508.10937)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution</a></li>
<li><strong>Abstract: </strong>In recent years, face super-resolution (FSR) methods have achieved remarkable progress, generally maintaining high image fidelity and identity (ID) consistency under standard settings. However, in extreme degradation scenarios (e.g., scale $> 8\times$), critical attributes and ID information are often severely lost in the input image, making it difficult for conventional models to reconstruct realistic and ID-consistent faces. Existing methods tend to generate hallucinated faces under such conditions, producing restored images lacking authentic ID constraints. To address this challenge, we propose a novel FSR method with Identity Decoupling and Fitting (IDFSR), designed to enhance ID restoration under large scaling factors while mitigating hallucination effects. Our approach involves three key designs: 1) \textbf{Masking} the facial region in the low-resolution (LR) image to eliminate unreliable ID cues; 2) \textbf{Warping} a reference image to align with the LR input, providing style guidance; 3) Leveraging \textbf{ID embeddings} extracted from ground truth (GT) images for fine-grained ID modeling and personalized adaptation. We first pretrain a diffusion-based model to explicitly decouple style and ID by forcing it to reconstruct masked LR face regions using both style and identity embeddings. Subsequently, we freeze most network parameters and perform lightweight fine-tuning of the ID embedding using a small set of target ID images. This embedding encodes fine-grained facial attributes and precise ID information, significantly improving both ID consistency and perceptual quality. Extensive quantitative evaluations and visual comparisons demonstrate that the proposed IDFSR substantially outperforms existing approaches under extreme degradation, particularly achieving superior performance on ID consistency.</li>
<li><strong>摘要：</strong>近年来，面部超分辨率（FSR）方法取得了显着的进步，通常在标准设置下保持高图像保真度和身份（ID）一致性。但是，在极端退化方案（例如，比例$> 8 \ times $）中，关键属性和ID信息在输入图像中通常会严重丢失，这使得常规模型很难重建现实且ID一致的面孔。现有方法倾向于在这种情况下产生幻觉的面孔，从而产生缺乏真实ID限制的恢复图像。为了应对这一挑战，我们提出了一种具有身份去耦和拟合（IDFSR）的新型FSR方法，旨在增强大型缩放因子下的ID恢复，同时减轻幻觉效果。我们的方法涉及三个关键设计：1）\ textbf {basking}低分辨率（LR）图像中的面部区域以消除不可靠的ID提示； 2）\ textbf {warping}参考图像与LR输入保持一致，提供样式指导； 3）利用\ textbf {id嵌入}从地面真相（GT）图像中提取以进行细粒度的ID建模和个性化适应。我们首先通过强迫使用样式和身份嵌入式重建蒙面的LR面部区域来重建蒙面的LR面部区域，从而明确地将基于扩散的模型预算出来。随后，我们使用一组少量的目标ID映像冻结大多数网络参数，并对ID嵌入的轻巧微调。这嵌入了编码细粒的面部属性和精确的ID信息，从而显着提高ID的一致性和感知质量。广泛的定量评估和视觉比较表明，所提出的IDFSR在极端降级下大大优于现有方法，尤其是在ID一致性上实现了卓越的性能。</li>
</ul>

<h3>Title: IPG: Incremental Patch Generation for Generalized Adversarial Patch Training</h3>
<ul>
<li><strong>Authors: </strong>Wonho Lee, Hyunsik Na, Jisu Lee, Daeseon Choi</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.10946">https://arxiv.org/abs/2508.10946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.10946">https://arxiv.org/pdf/2508.10946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.10946]] IPG: Incremental Patch Generation for Generalized Adversarial Patch Training(https://arxiv.org/abs/2508.10946)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The advent of adversarial patches poses a significant challenge to the robustness of AI models, particularly in the domain of computer vision tasks such as object detection. In contradistinction to traditional adversarial examples, these patches target specific regions of an image, resulting in the malfunction of AI models. This paper proposes Incremental Patch Generation (IPG), a method that generates adversarial patches up to 11.1 times more efficiently than existing approaches while maintaining comparable attack performance. The efficacy of IPG is demonstrated by experiments and ablation studies including YOLO's feature distribution visualization and adversarial training results, which show that it produces well-generalized patches that effectively cover a broader range of model vulnerabilities. Furthermore, IPG-generated datasets can serve as a robust knowledge foundation for constructing a robust model, enabling structured representation, advanced reasoning, and proactive defenses in AI security ecosystems. The findings of this study suggest that IPG has considerable potential for future utilization not only in adversarial patch defense but also in real-world applications such as autonomous vehicles, security systems, and medical imaging, where AI models must remain resilient to adversarial attacks in dynamic and high-stakes environments.</li>
<li><strong>摘要：</strong>对抗贴片的出现对AI模型的鲁棒性构成了重大挑战，尤其是在计算机视觉任务（例如对象检测）的领域。与传统的对抗示例相反，这些斑块针对图像的特定区域，导致AI模型的故障。本文提出了增量补丁生成（IPG），该方法在维持可比的攻击性能的同时，在维持可比较的攻击性能的同时，生成对抗性补丁的效率高出11.1倍。 IPG的功效通过实验和消融研究（包括Yolo的特征分布可视化和对抗性训练结果）证明，这表明它会产生良好的拟及贴片，从而有效地涵盖了更广泛的模型脆弱性。此外，IPG生成的数据集可以作为构建强大模型，为AI安全生态系统中的结构化表示，高级推理和主动防御的强大知识基础。这项研究的结果表明，IPG不仅在对抗性贴片防御中，而且在实际应用中具有巨大的利用潜力，还具有自动驾驶汽车，安全系统和医学成像等实际应用，在该应用程序中，AI模型必须在动态和高风险环境中对对抗性攻击保持弹性。</li>
</ul>

<h3>Title: Empowering Multimodal LLMs with External Tools: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Wenbin An, Jiahao Nie, Yaqiang Wu, Feng Tian, Shijian Lu, Qinghua Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.10955">https://arxiv.org/abs/2508.10955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.10955">https://arxiv.org/pdf/2508.10955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.10955]] Empowering Multimodal LLMs with External Tools: A Comprehensive Survey(https://arxiv.org/abs/2508.10955)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>By integrating the perception capabilities of multimodal encoders with the generative power of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), exemplified by GPT-4V, have achieved great success in various multimodal tasks, pointing toward a promising pathway to artificial general intelligence. Despite this progress, the limited quality of multimodal data, poor performance on many complex downstream tasks, and inadequate evaluation protocols continue to hinder the reliability and broader applicability of MLLMs across diverse domains. Inspired by the human ability to leverage external tools for enhanced reasoning and problem-solving, augmenting MLLMs with external tools (e.g., APIs, expert models, and knowledge bases) offers a promising strategy to overcome these challenges. In this paper, we present a comprehensive survey on leveraging external tools to enhance MLLM performance. Our discussion is structured along four key dimensions about external tools: (1) how they can facilitate the acquisition and annotation of high-quality multimodal data; (2) how they can assist in improving MLLM performance on challenging downstream tasks; (3) how they enable comprehensive and accurate evaluation of MLLMs; (4) the current limitations and future directions of tool-augmented MLLMs. Through this survey, we aim to underscore the transformative potential of external tools in advancing MLLM capabilities, offering a forward-looking perspective on their development and applications. The project page of this paper is publicly available athttps://github.com/Lackel/Awesome-Tools-for-MLLMs.</li>
<li><strong>摘要：</strong>通过将多模式编码器的感知能力与大型语言模型（LLMS）的生成力量（由GPT-4V举例说明）的多模式大型语言模型（MLLM）相结合，在各种多模式任务中取得了巨大的成功，朝着迈向人工通用人工智能的有希望的途径。尽管取得了这种进步，但多模式数据的质量有限，许多复杂的下游任务的性能差，并且评估协议不足继续阻碍MLLM在不同域中的可靠性和更广泛的适用性。受到人类利用外部工具来增强推理和解决问题的能力的启发，使用外部工具（例如API，专家模型和知识基础）增强MLLM，提供了一个有希望的策略来克服这些挑战。在本文中，我们提出了一项有关利用外部工具来提高MLLM性能的综合调查。我们的讨论沿着有关外部工具的四个关键维度进行了结构：（1）它们如何促进高质量多模式数据的获取和注释； （2）他们如何帮助改善挑战下游任务的MLLM绩效； （3）它们如何实现MLLM的全面和准确评估； （4）工具调格的MLLM的当前局限性和未来方向。通过这项调查，我们旨在强调外部工具在推进MLLM功能方面的变革潜力，从而对其开发和应用提供前瞻性的看法。本文的项目页面可公开可用athttps：//github.com/lackel/awsome-tools-for-mllms。</li>
</ul>

<h3>Title: EVCtrl: Efficient Control Adapter for Visual Generation</h3>
<ul>
<li><strong>Authors: </strong>Zixiang Yang, Yue Ma, Yinhan Zhang, Shanhui Mo, Dongrui Liu, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.10963">https://arxiv.org/abs/2508.10963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.10963">https://arxiv.org/pdf/2508.10963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.10963]] EVCtrl: Efficient Control Adapter for Visual Generation(https://arxiv.org/abs/2508.10963)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Visual generation includes both image and video generation, training probabilistic models to create coherent, diverse, and semantically faithful content from scratch. While early research focused on unconditional sampling, practitioners now demand controllable generation that allows precise specification of layout, pose, motion, or style. While ControlNet grants precise spatial-temporal control, its auxiliary branch markedly increases latency and introduces redundant computation in both uncontrolled regions and denoising steps, especially for video. To address this problem, we introduce EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead without retraining the model. Specifically, we propose a spatio-temporal dual caching strategy for sparse control information. For spatial redundancy, we first profile how each layer of DiT-ControlNet responds to fine-grained control, then partition the network into global and local functional zones. A locality-aware cache focuses computation on the local zones that truly need the control signal, skipping the bulk of redundant computation in global regions. For temporal redundancy, we selectively omit unnecessary denoising steps to improve efficiency. Extensive experiments on CogVideo-Controlnet, Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image and video control generation without the need for training. For example, it achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and Wan2.1-Controlnet, respectively, with almost no degradation in generation this http URL are available in the supplementary materials.</li>
<li><strong>摘要：</strong>视觉生成包括图像和视频生成，训练概率模型，以创建从头开始创建连贯，多样和语义上忠实的内容。尽管早期研究以无条件的采样为重点，但从业人员现在需要可控制的生成，以精确规范布局，姿势，运动或样式。尽管ControlNet授予精确的时空控制，但其辅助分支显着增加了延迟，并在不受控制的区域和DeNoising步骤中引入了冗余计算，尤其是对于视频。为了解决这个问题，我们介绍了EVCTRL，这是一个轻巧的插件控制适配器，该适配器在不重新训练模型的情况下削减开销。具体而言，我们为稀疏控制信息提出了一个时空双缓存策略。对于空间冗余，我们首先介绍了DIT-Controlnet的每一层如何响应细粒度控制，然后将网络分配到全局和局部功能区域。局部感知的缓存将计算集中在真正需要控制信号的本地区域上，从而跳过了全球区域中的大部分冗余计算。对于时间冗余，我们有选择地省略了不必要的降解步骤以提高效率。关于Cogvideo-Controlnet，WAN2.1-Controlnet和Flux的广泛实验表明，我们的方法在图像和视频控制生成中无需训练而有效。例如，它分别在Cogvideo-Controlnet和WAN2.1-Controlnet上达到了2.16和2.05倍的加速，几乎没有降解该HTTP URL在补充材料中可用。</li>
</ul>

<h3>Title: BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Pratyush Maini, Vineeth Dorna, Parth Doshi, Aldo Carranza, Fan Pan, Jack Urbanek, Paul Burstein, Alex Fang, Alvin Deng, Amro Abbas, Brett Larsen, Cody Blakeney, Charvi Bannur, Christina Baek, Darren Teh, David Schwab, Haakon Mongstad, Haoli Yin, Josh Wills, Kaleigh Mentzer, Luke Merrick, Ricardo Monti, Rishabh Adiga, Siddharth Joshi, Spandan Das, Zhengping Wang, Bogdan Gaza, Ari Morcos, Matthew Leavitt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.10975">https://arxiv.org/abs/2508.10975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.10975">https://arxiv.org/pdf/2508.10975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.10975]] BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining(https://arxiv.org/abs/2508.10975)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in large language model (LLM) pretraining have shown that simply scaling data quantity eventually leads to diminishing returns, hitting a data wall. In response, the use of synthetic data for pretraining has emerged as a promising paradigm for pushing the frontier of performance. Despite this, the factors affecting synthetic data quality remain poorly understood. In this work, we introduce BeyondWeb, a synthetic data generation framework that produces high-quality synthetic data for pretraining. BeyondWeb significantly extends the capabilities of traditional web-scale datasets, outperforming state-of-the-art synthetic pretraining datasets such as Cosmopedia and Nemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1 percentage points (pp) and 2.6pp, respectively, when averaged across a suite of 14 benchmark evaluations. It delivers up to 7.7x faster training than open web data and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for 180B tokens on BeyondWeb outperforms an 8B model trained for the same token budget on Cosmopedia. We also present several insights from BeyondWeb on synthetic data for pretraining: what drives its benefits, which data to rephrase and how, and the impact of model size and family on data quality. Overall, our work shows that there's no silver bullet for generating high-quality synthetic pretraining data. The best outcomes require jointly optimizing many factors, a challenging task that requires rigorous science and practical expertise. Naive approaches can yield modest improvements, potentially at great cost, while well-executed methods can yield transformative improvements, as exemplified by BeyondWeb.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）预处理的最新进展表明，简单地缩放数据数量最终会导致回报减少，并击中数据墙。作为响应，将合成数据用于预处理已成为推动性能前沿的有希望的范式。尽管如此，影响综合数据质量的因素仍然知之甚少。在这项工作中，我们介绍了BeyondWeb，这是一个合成数据生成框架，该框架可产生高质量的合成数据进行预处理。 Beyondweb显着扩展了传统的网络尺度数据集的功能，超过了最先进的合成预读数据集，例如Cosmopedia和Nemotron-CC的高质量合成子集（Nemotron-nemotron-synth），分别在评估14 bench时，分别评估了14个BENCH，分别评估了2.6个百分点（pp）和2.6ppp。它比打开的网络数据更快地提供了高达7.7倍的训练，并且比Nemotron-synth更快。值得注意的是，在BeyondWeb上为180B代币培训的3B型号优于训练Cosmopedia的8B模型。我们还提供了《超越网络》的几个见解，以了解综合数据的预处理：是什么驱动其好处，哪些数据重塑以及模型大小和家庭对数据质量的影响。总体而言，我们的工作表明，没有用于生成高质量合成预审计数据的银弹。最好的结果需要共同优化许多因素，这是一项具有挑战性的任务，需要严格的科学和实践专业知识。天真的方法可以产生适度的改进，可能以巨大的成本产生适度的改进，而执行良好的方法可以产生变革性的改进，例如BeyondWeb的例子。</li>
</ul>

<h3>Title: CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention</h3>
<ul>
<li><strong>Authors: </strong>Qingbin Li, Rongkun Xue, Jie Wang, Ming Zhou, Zhi Li, Xiaofeng Ji, Yongqi Wang, Miao Liu, Zheming Yang, Minghui Qiu, Jing Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11016">https://arxiv.org/abs/2508.11016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11016">https://arxiv.org/pdf/2508.11016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11016]] CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention(https://arxiv.org/abs/2508.11016)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in Reinforcement Learning with Verified Reward (RLVR) have driven the emergence of more sophisticated cognitive behaviors in large language models (LLMs), thereby enhancing their reasoning capabilities. However, in prior RLVR pipelines, the repeated use of static initial-state sampling drawn exactly from the dataset distribution during each sampling phase produced overly deterministic, low diversity model behavior, which manifested as rapid entropy collapse and hindered sustained performance gains during prolonged training. To address this issue, we introduce CURE (Critical-token-gUided Re concatenation for Entropy-collapse prevention), a two-stage framework that balances exploration and exploitation. Specifically, in the first stage, to deliberately steer the model toward novel yet coherent contexts, we re-generate at high-entropy critical tokens and jointly optimize the original and the branched trajectories. The further comparison with vanilla DAPO shows that the regeneration process achieves a better performance on math reasoning tasks while sustaining a high-level entropy degree for exploration. In the second stage, we continue training with static initial-state sampling by DAPO, intentionally placing the model in a familiar state to gradually strengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that, compared to other RLVR methods, CURE achieves a 5% performance gain across six math benchmarks, establishing state-of-the-art performance in both entropy and accuracy. A series of experiments further validate the effectiveness of our approach. Code is available at this https URL.</li>
<li><strong>摘要：</strong>通过经过验证的奖励（RLVR）的强化学习进展最近推动了大语言模型（LLMS）中更复杂的认知行为的出现，从而增强了其推理能力。然而，在先前的RLVR管道中，在每个采样阶段中重复使用静态初始状态采样，从数据集分布中精确地绘制，从而产生过度确定性的，低的多样性模型行为，这表现为快速的熵崩溃，并在长期训练期间表现为持续的持续性能增长。为了解决这个问题，我们引入了治疗（核心指导的重新串联预防置换），这是一个平衡探索和剥削的两阶段框架。具体而言，在第一阶段，要故意将模型转向新颖而连贯的环境，我们在高渗透关键令牌上重新生成，并共同优化原始轨迹和分支轨迹。与香草DAPO的进一步比较表明，再生过程在数学推理任务上取得了更好的性能，同时维持了高级熵学位以进行探索。在第二阶段，我们继续通过DAPO进行静态初始状态采样的训练，并有意将模型置于熟悉的状态，以逐渐增强剥削。 QWEN-2.5-MATH-7B的广泛实验表明，与其他RLVR方法相比，CURE在六个数学基准中获得了5％的性能增长，从而在熵和准确性中建立了最先进的性能。一系列实验进一步验证了我们方法的有效性。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: HierOctFusion: Multi-scale Octree-based 3D Shape Generation via Part-Whole-Hierarchy Message Passing</h3>
<ul>
<li><strong>Authors: </strong>Xinjie Gao, Bi'an Du, Wei Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11106">https://arxiv.org/abs/2508.11106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11106">https://arxiv.org/pdf/2508.11106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11106]] HierOctFusion: Multi-scale Octree-based 3D Shape Generation via Part-Whole-Hierarchy Message Passing(https://arxiv.org/abs/2508.11106)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D content generation remains a fundamental yet challenging task due to the inherent structural complexity of 3D data. While recent octree-based diffusion models offer a promising balance between efficiency and quality through hierarchical generation, they often overlook two key insights: 1) existing methods typically model 3D objects as holistic entities, ignoring their semantic part hierarchies and limiting generalization; and 2) holistic high-resolution modeling is computationally expensive, whereas real-world objects are inherently sparse and hierarchical, making them well-suited for layered generation. Motivated by these observations, we propose HierOctFusion, a part-aware multi-scale octree diffusion model that enhances hierarchical feature interaction for generating fine-grained and sparse object structures. Furthermore, we introduce a cross-attention conditioning mechanism that injects part-level information into the generation process, enabling semantic features to propagate effectively across hierarchical levels from parts to the whole. Additionally, we construct a 3D dataset with part category annotations using a pre-trained segmentation model to facilitate training and evaluation. Experiments demonstrate that HierOctFusion achieves superior shape quality and efficiency compared to prior methods.</li>
<li><strong>摘要：</strong>由于3D数据的固有结构复杂性，3D内容生成仍然是一项基本而又具有挑战性的任务。尽管最近基于OCTREE的扩散模型通过层次结构提供了有希望的平衡，但它们通常会忽略两个关键的见解：1）现有方法通常将3D对象建模为整体实体，而忽略了其语义部分层次结构并限制了概括； 2）整体高分辨率建模在计算上是昂贵的，而现实世界本质上是稀疏和分层的，使其非常适合分层生成。在这些观察结果的推动下，我们提出了HieroCtfusion，这是一种部分意识到的多尺度OCTREE扩散模型，可增强用于生成细粒和稀疏对象结构的层次特征相互作用。此外，我们引入了一种跨注意调节机制，该机制将零件级别的信息注入生成过程，从而使语义特征能够从层次到整个部分有效地传播跨层次级别。此外，我们使用预训练的分割模型构建了一个具有部分类别注释的3D数据集，以促进培训和评估。实验表明，与先前的方法相比，HieroCtfusion具有优异的形状质量和效率。</li>
</ul>

<h3>Title: Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation</h3>
<ul>
<li><strong>Authors: </strong>Bing Liu, Le Wang, Hao Liu, Mingming Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11134">https://arxiv.org/abs/2508.11134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11134">https://arxiv.org/pdf/2508.11134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11134]] Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation(https://arxiv.org/abs/2508.11134)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Current deep dehazing methods only focus on removing haze from hazy images, lacking the capability to translate between hazy and haze-free images. To address this issue, we propose a residual-based efficient bidirectional diffusion model (RBDM) that can model the conditional distributions for both dehazing and haze generation. Firstly, we devise dual Markov chains that can effectively shift the residuals and facilitate bidirectional smooth transitions between them. Secondly, the RBDM perturbs the hazy and haze-free images at individual timesteps and predicts the noise in the perturbed data to simultaneously learn the conditional distributions. Finally, to enhance performance on relatively small datasets and reduce computational costs, our method introduces a unified score function learned on image patches instead of entire images. Our RBDM successfully implements size-agnostic bidirectional transitions between haze-free and hazy images with only 15 sampling steps. Extensive experiments demonstrate that the proposed method achieves superior or at least comparable performance to state-of-the-art methods on both synthetic and real-world datasets.</li>
<li><strong>摘要：</strong>当前的深层飞行方法仅着眼于从朦胧的图像中去除阴霾，缺乏在朦胧和无雾图之间翻译的能力。为了解决这个问题，我们提出了一个基于残差的有效双向扩散模型（RBDM），该模型可以为脱壳和雾霾产生的条件分布建模。首先，我们设计了双重马尔可夫链，这些链可以有效地转移残差并促进它们之间的双向平滑转变。其次，RBDM在各个时间段上散发出朦胧和无雾图的图像，并预测扰动数据中的噪声同时学习条件分布。最后，为了提高相对较小的数据集的性能并降低计算成本，我们的方法引入了在图像补丁而不是整个图像上学习的统一分数功能。我们的RBDM成功地实现了无雾和朦胧的图像之间的尺寸不足双向转变，仅使用15个采样步骤。广泛的实验表明，所提出的方法在合成和现实世界数据集上的最先进方法具有优越或至少可比性的性能。</li>
</ul>

<h3>Title: Towards the Next-generation Bayesian Network Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Huan Zhang, Daokun Zhang, Kexin Meng, Geoffrey I. Webb</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11145">https://arxiv.org/abs/2508.11145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11145">https://arxiv.org/pdf/2508.11145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11145]] Towards the Next-generation Bayesian Network Classifiers(https://arxiv.org/abs/2508.11145)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Bayesian network classifiers provide a feasible solution to tabular data classification, with a number of merits like high time and memory efficiency, and great explainability. However, due to the parameter explosion and data sparsity issues, Bayesian network classifiers are restricted to low-order feature dependency modeling, making them struggle in extrapolating the occurrence probabilities of complex real-world data. In this paper, we propose a novel paradigm to design high-order Bayesian network classifiers, by learning distributional representations for feature values, as what has been done in word embedding and graph representation learning. The learned distributional representations are encoded with the semantic relatedness between different features through their observed co-occurrence patterns in training data, which then serve as a hallmark to extrapolate the occurrence probabilities of new test samples. As a classifier design realization, we remake the K-dependence Bayesian classifier (KDB) by extending it into a neural version, i.e., NeuralKDB, where a novel neural network architecture is designed to learn distributional representations of feature values and parameterize the conditional probabilities between interdependent features. A stochastic gradient descent based algorithm is designed to train the NeuralKDB model efficiently. Extensive classification experiments on 60 UCI datasets demonstrate that the proposed NeuralKDB classifier excels in capturing high-order feature dependencies and significantly outperforms the conventional Bayesian network classifiers, as well as other competitive classifiers, including two neural network based classifiers without distributional representation learning.</li>
<li><strong>摘要：</strong>贝叶斯网络分类器为表格数据分类提供了可行的解决方案，具有许多优点，例如高时间和记忆效率，并且可以解释。但是，由于参数爆炸和数据稀疏性问题，贝叶斯网络分类器仅限于低阶特征依赖性建模，这使它们在推断复杂现实世界数据的发生概率方面挣扎。在本文中，我们提出了一种新颖的范式来设计高阶贝叶斯网络分类器，通过学习特征值的分配表示，就像在单词嵌入和图表表示学习中所做的那样。通过观察到的训练数据中观察到的共发生模式，学习的分布表示与不同特征之间的语义相关性编码，然后将其作为推断新测试样本的发生概率的标志。作为分类器设计实现，我们通过将其扩展到神经版本（即NeuroalkDB）中来重塑K依赖性贝叶斯分类器（KDB），在该神经版本中，新型神经网络体系结构旨在学习特征值的分布表示并参数化相互依赖性特征之间的条件概率。基于随机梯度下降算法的设计旨在有效地训练神经AlalalkDB模型。在60个UCI数据集上进行的广泛分类实验表明，所提出的神经ARALKDB分类器在捕获高阶依赖性方面表现出色，并显着胜过常规的贝叶斯网络分类器以及其他竞争性分类器，以及包括两个基于基于分布的神经网络的基于神经网络的分类器。</li>
</ul>

<h3>Title: LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction</h3>
<ul>
<li><strong>Authors: </strong>Maoquan Zhang, Bisser Raytchev, Xiujuan Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11153">https://arxiv.org/abs/2508.11153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11153">https://arxiv.org/pdf/2508.11153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11153]] LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction(https://arxiv.org/abs/2508.11153)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>LEARN is a layout-aware diffusion framework designed to generate pedagogically aligned illustrations for STEM education. It leverages a curated BookCover dataset that provides narrative layouts and structured visual cues, enabling the model to depict abstract and sequential scientific concepts with strong semantic alignment. Through layout-conditioned generation, contrastive visual-semantic training, and prompt modulation, LEARN produces coherent visual sequences that support mid-to-high-level reasoning in line with Bloom's taxonomy while reducing extraneous cognitive load as emphasized by Cognitive Load Theory. By fostering spatially organized and story-driven narratives, the framework counters fragmented attention often induced by short-form media and promotes sustained conceptual focus. Beyond static diagrams, LEARN demonstrates potential for integration with multimodal systems and curriculum-linked knowledge graphs to create adaptive, exploratory educational content. As the first generative approach to unify layout-based storytelling, semantic structure learning, and cognitive scaffolding, LEARN represents a novel direction for generative AI in education. The code and dataset will be released to facilitate future research and practical deployment.</li>
<li><strong>摘要：</strong>学习是一个布局感知的扩散框架，旨在为STEM教育生成教学上的插图。它利用了一个精心策划的书记数据集，该数据集提供了叙事布局和结构化的视觉提示，从而使模型能够用强烈的语义对齐来描绘抽象和顺序的科学概念。通过布局条件的产生，对比的视觉语义训练和迅速调制，学习产生连贯的视觉序列，以与Bloom的分类学相一致，同时减少多余的认知负荷，从而支持中高级推理，同时降低了认知负载理论的强调。通过促进空间组织和故事驱动的叙述，该框架反驳了零散的注意力，通常是由短形式媒体引起的，并促进了持续的概念重点。除了静态图外，Learn还展示了与多模式系统和课程链接知识图集成的潜力，以创建自适应的探索性教育内容。作为统一基于布局的讲故事，语义结构学习和认知脚手架的第一种生成型方法，学习代表了教育中生成AI的新方向。该代码和数据集将发布，以促进未来的研究和实际部署。</li>
</ul>

<h3>Title: Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Heqiang Wang, Weihong Yang, Xiaoxiong Zhong, Jia Zhou, Fangming Liu, Weizhe Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11159">https://arxiv.org/abs/2508.11159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11159">https://arxiv.org/pdf/2508.11159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11159]] Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning(https://arxiv.org/abs/2508.11159)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The Internet of Things (IoT) ecosystem produces massive volumes of multimodal data from diverse sources, including sensors, cameras, and microphones. With advances in edge intelligence, IoT devices have evolved from simple data acquisition units into computationally capable nodes, enabling localized processing of heterogeneous multimodal data. This evolution necessitates distributed learning paradigms that can efficiently handle such data. Furthermore, the continuous nature of data generation and the limited storage capacity of edge devices demand an online learning framework. Multimodal Online Federated Learning (MMO-FL) has emerged as a promising approach to meet these requirements. However, MMO-FL faces new challenges due to the inherent instability of IoT devices, which often results in modality quantity and quality imbalance (QQI) during data collection. In this work, we systematically investigate the impact of QQI within the MMO-FL framework and present a comprehensive theoretical analysis quantifying how both types of imbalance degrade learning performance. To address these challenges, we propose the Modality Quantity and Quality Rebalanced (QQR) algorithm, a prototype learning based method designed to operate in parallel with the training process. Extensive experiments on two real-world multimodal datasets show that the proposed QQR algorithm consistently outperforms benchmarks under modality imbalance conditions with promising learning performance.</li>
<li><strong>摘要：</strong>物联网（IoT）生态系统产生大量来自不同来源的多模式数据，包括传感器，相机和麦克风。随着边缘智能的进步，IoT设备已从简单的数据采集单元演变为具有计算能力的节点，从而实现了异质多模式数据的局部处理。这种演变需要可以有效处理此类数据的分布式学习范例。此外，数据生成的连续性质和边缘设备的存储能力有限需要在线学习框架。多模式在线联合学习（MMO-FL）已成为满足这些要求的有前途的方法。但是，由于物联网设备的固有不稳定，MMO-FL面临着新的挑战，这通常会导致数据收集过程中的模态数量和质量失衡（QQI）。在这项工作中，我们系统地研究了QQI在MMO-FL框架中的影响，并提出了全面的理论分析，以量化两种类型的失衡降低学习绩效。为了应对这些挑战，我们提出了一种基于原型学习的方法的方式和质量重新平衡（QQR）算法，该方法旨在与培训过程并行运行。在两个真实世界多模式数据集上进行的广泛实验表明，所提出的QQR算法在模态不平衡条件下始终优于基准，并具有有希望的学习表现。</li>
</ul>

<h3>Title: Better Supervised Fine-tuning for VQA: Integer-Only Loss</h3>
<ul>
<li><strong>Authors: </strong>Baihong Qian, Haotian Fan, Wenjie Liao, Yunqiu Wang, Tao Li, Junhui Cui</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11170">https://arxiv.org/abs/2508.11170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11170">https://arxiv.org/pdf/2508.11170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11170]] Better Supervised Fine-tuning for VQA: Integer-Only Loss(https://arxiv.org/abs/2508.11170)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of vision language models(VLM), their ability to assess visual content based on specific criteria and dimensions has become increasingly critical for applications such as video-theme consistency assessment and visual quality scoring. However, existing methods often suffer from imprecise results and inefficient loss calculation, which limit the focus of the model on key evaluation indicators. To address this, we propose IOVQA(Integer-only VQA), a novel fine-tuning approach tailored for VLMs to enhance their performance in video quality assessment tasks. The key innovation of IOVQA lies in its label construction and its targeted loss calculation mechanism. Specifically, during dataset curation, we constrain the model's output to integers within the range of [10,50], ensuring numerical stability, and convert decimal Overall_MOS to integer before using them as labels. We also introduce a target-mask strategy: when computing the loss, only the first two-digit-integer of the label is unmasked, forcing the model to learn the critical components of the numerical evaluation. After fine-tuning the Qwen2.5-VL model using the constructed dataset, experimental results demonstrate that the proposed method significantly improves the model's accuracy and consistency in the VQA task, ranking 3rd in VQualA 2025 GenAI-Bench AIGC Video Quality Assessment Challenge -- Track I. Our work highlights the effectiveness of merely leaving integer labels during fine-tuning, providing an effective idea for optimizing VLMs in quantitative evaluation scenarios.</li>
<li><strong>摘要：</strong>随着视觉语言模型（VLM）的快速发展，它们基于特定标准和维度评估视觉内容的能力已经变得越来越重要，例如视频主题一致性评估和视觉质量评分等应用程序。但是，现有方法通常会遭受不精确的结果和效率低下的损失计算，这将模型的重点限制在关键评估指标上。为了解决这个问题，我们提出了IOVQA（仅Integer-folly VQA），这是一种针对VLMS量身定制的新颖的微调方法，可增强其在视频质量评估任务中的性能。 IOVQA的关键创新在于其标签构建及其目标损失计算机制。具体来说，在数据集策划期间，我们将模型的输出限制为[10,50]范围内的整数，确保数值稳定性，并在将其用作标签之前将其转换为整数。我们还引入了目标掩盖策略：计算损失时，只有标签的第一个两位数授权者才被删除，迫使模型学习数值评估的关键组成部分。 After fine-tuning the Qwen2.5-VL model using the constructed dataset, experimental results demonstrate that the proposed method significantly improves the model's accuracy and consistency in the VQA task, ranking 3rd in VQualA 2025 GenAI-Bench AIGC Video Quality Assessment Challenge -- Track I. Our work highlights the effectiveness of merely leaving integer labels during fine-tuning, providing an effective idea for optimizing VLMs in quantitative评估方案。</li>
</ul>

<h3>Title: A Semi-supervised Generative Model for Incomplete Multi-view Data Integration with Missing Labels</h3>
<ul>
<li><strong>Authors: </strong>Yiyang Shen, Weiran Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11180">https://arxiv.org/abs/2508.11180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11180">https://arxiv.org/pdf/2508.11180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11180]] A Semi-supervised Generative Model for Incomplete Multi-view Data Integration with Missing Labels(https://arxiv.org/abs/2508.11180)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multi-view learning is widely applied to real-life datasets, such as multiple omics biological data, but it often suffers from both missing views and missing labels. Prior probabilistic approaches addressed the missing view problem by using a product-of-experts scheme to aggregate representations from present views and achieved superior performance over deterministic classifiers, using the information bottleneck (IB) principle. However, the IB framework is inherently fully supervised and cannot leverage unlabeled data. In this work, we propose a semi-supervised generative model that utilizes both labeled and unlabeled samples in a unified framework. Our method maximizes the likelihood of unlabeled samples to learn a latent space shared with the IB on labeled data. We also perform cross-view mutual information maximization in the latent space to enhance the extraction of shared information across views. Compared to existing approaches, our model achieves better predictive and imputation performance on both image and multi-omics data with missing views and limited labeled samples.</li>
<li><strong>摘要：</strong>多视图学习被广泛应用于现实生活中的数据集，例如多个OMICS生物学数据，但它通常遭受丢失的视图和缺少的标签。先前的概率方法通过使用Expracters Offerts方案来解决缺失的视图问题，从当前视图中汇总表示形式，并使用信息瓶颈（IB）原理从当前视图中汇总表征，并实现了优于确定性分类器的卓越性能。但是，IB框架本质上是完全监督的，无法利用未标记的数据。在这项工作中，我们提出了一个半监督的生成模型，该模型在统一框架中使用标记和未标记的样品。我们的方法最大程度地提高了未标记样本的可能性，以在标记的数据上学习与IB共享的潜在空间。我们还在潜在空间中执行跨视图互信息最大化，以增强跨视图的共享信息的提取。与现有方法相比，我们的模型在图像和多摩变数据上都具有更好的预测性和归纳性能，而视图缺少和有限的标记样品。</li>
</ul>

<h3>Title: Versatile Video Tokenization with Generative 2D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Zhenghao Chen, Zicong Chen, Lei Liu, Yiming Wu, Dong Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11183">https://arxiv.org/abs/2508.11183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11183">https://arxiv.org/pdf/2508.11183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11183]] Versatile Video Tokenization with Generative 2D Gaussian Splatting(https://arxiv.org/abs/2508.11183)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Video tokenization procedure is critical for a wide range of video processing tasks. Most existing approaches directly transform video into fixed-grid and patch-wise tokens, which exhibit limited versatility. Spatially, uniformly allocating a fixed number of tokens often leads to over-encoding in low-information regions. Temporally, reducing redundancy remains challenging without explicitly distinguishing between static and dynamic content. In this work, we propose the Gaussian Video Transformer (GVT), a versatile video tokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We first extract latent rigid features from a video clip and represent them with a set of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian Embedding (STGE) mechanism in a feed-forward manner. Such generative 2D Gaussians not only enhance spatial adaptability by assigning higher (resp., lower) rendering weights to regions with higher (resp., lower) information content during rasterization, but also improve generalization by avoiding per-video this http URL enhance the temporal versatility, we introduce a Gaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into static and dynamic sets, which explicitly model static content shared across different time-steps and dynamic content specific to each time-step, enabling a compact this http URL primarily evaluate GVT on the video reconstruction, while also assessing its performance on action recognition and compression using the UCF101, Kinetics, and DAVIS datasets. Extensive experiments demonstrate that GVT achieves a state-of-the-art video reconstruction quality, outperforms the baseline MAGVIT-v2 in action recognition, and delivers comparable compression performance.</li>
<li><strong>摘要：</strong>视频令牌化过程对于广泛的视频处理任务至关重要。大多数现有方法将视频直接转化为固定网格和贴片的代币，这些代币具有有限的多功能性。在空间上，统一分配固定数量的令牌通常会导致低信息区域中的编码过多。在时间上，减少冗余仍然具有挑战性，而无需明确区分静态和动态内容。在这项工作中，我们提出了高斯视频变压器（GVT），这是一种基于生成的2D高斯（2DGS）策略的多功能视频令牌。我们首先从视频剪辑中提取潜在的刚性特征，并用我们提出的时空高斯嵌入（STGE）机制产生的一组2D高斯人以馈送方式。 Such generative 2D Gaussians not only enhance spatial adaptability by assigning higher (resp., lower) rendering weights to regions with higher (resp., lower) information content during rasterization, but also improve generalization by avoiding per-video this http URL enhance the temporal versatility, we introduce a Gaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into static and dynamic sets, which明确模型在不同的时间步长和每个时间阶的动态内容上共享的静态内容，使该HTTP URL的紧凑型主要在视频重建上评估GVT，同时还可以使用UCF101，动力学和Davisets和Davisetset评估其在动作识别和压缩方面的性能。广泛的实验表明，GVT可以达到最先进的视频重建质量，在动作识别中优于基线MAGVIT-V2，并提供可比的压缩性能。</li>
</ul>

<h3>Title: Quantum-Boosted High-Fidelity Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Feng-ao Wang, Shaobo Chen, Yao Xuan, Junwei Liu, Qi Gao, Hongdong Zhu, Junjie Hou, Lixin Yuan, Jinyu Cheng, Chenxin Yi, Hai Wei, Yin Ma, Tao Xu, Kai Wen, Yixue Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11190">https://arxiv.org/abs/2508.11190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11190">https://arxiv.org/pdf/2508.11190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11190]] Quantum-Boosted High-Fidelity Deep Learning(https://arxiv.org/abs/2508.11190)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A fundamental limitation of probabilistic deep learning is its predominant reliance on Gaussian priors. This simplistic assumption prevents models from accurately capturing the complex, non-Gaussian landscapes of natural data, particularly in demanding domains like complex biological data, severely hindering the fidelity of the model for scientific discovery. The physically-grounded Boltzmann distribution offers a more expressive alternative, but it is computationally intractable on classical computers. To date, quantum approaches have been hampered by the insufficient qubit scale and operational stability required for the iterative demands of deep learning. Here, we bridge this gap by introducing the Quantum Boltzmann Machine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable hybrid quantum-classical architecture. Our framework leverages a quantum processor for efficient sampling from the Boltzmann distribution, enabling its use as a powerful prior within a deep generative model. Applied to million-scale single-cell datasets from multiple sources, the QBM-VAE generates a latent space that better preserves complex biological structures, consistently outperforming conventional Gaussian-based deep learning models like VAE and SCVI in essential tasks such as omics data integration, cell-type classification, and trajectory inference. It also provides a typical example of introducing a physics priori into deep learning to drive the model to acquire scientific discovery capabilities that breaks through data limitations. This work provides the demonstration of a practical quantum advantage in deep learning on a large-scale scientific problem and offers a transferable blueprint for developing hybrid quantum AI models.</li>
<li><strong>摘要：</strong>概率深度学习的基本局限性是其对高斯先验的主要依赖。这种简单的假设阻止了模型准确捕获自然数据的复杂的非高斯景观，尤其是在诸如复杂的生物学数据之类的苛刻领域，从而严重阻碍了该模型的科学发现忠诚度。物理地面的玻尔兹曼分布提供了一种更具表现力的替代方案，但在经典计算机上是计算上棘手的。迄今为止，量子方法受到了量化量表的不足和深度学习需求所需的操作稳定性的阻碍。在这里，我们通过引入量子Boltzmann机器变量自动编码器（QBM-VAE）来弥合这一差距。我们的框架利用量子处理器从玻尔兹曼分布中进行有效采样，从而使其在深层生成模型中用作强大的先验。 QBM-VAE应用于来自多个来源的百万级单细胞数据集，生成了一个潜在的空间，可以更好地保留复杂的生物结构，从而在诸如Omics数据集成，单元型分类和轨迹推理等基本任务（例如基本任务）中始终优于常规的高斯基于高斯的深度学习模型，例如VAE和SCVI。它还提供了一个典型的例子，将物理学先验引入深度学习中，以推动模型以获取破坏数据限制的科学发现功能。这项工作为大规模科学问题的深度学习提供了实用的量子优势，并为开发混合量子AI模型提供了可转移的蓝图。</li>
</ul>

<h3>Title: A Coarse-to-Fine Human Pose Estimation Method based on Two-stage Distillation and Progressive Graph Neural Network</h3>
<ul>
<li><strong>Authors: </strong>Zhangjian Ji, Wenjin Zhang, Shaotong Qiao, Kai Feng, Yuhua Qian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11212">https://arxiv.org/abs/2508.11212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11212">https://arxiv.org/pdf/2508.11212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11212]] A Coarse-to-Fine Human Pose Estimation Method based on Two-stage Distillation and Progressive Graph Neural Network(https://arxiv.org/abs/2508.11212)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Human pose estimation has been widely applied in the human-centric understanding and generation, but most existing state-of-the-art human pose estimation methods require heavy computational resources for accurate predictions. In order to obtain an accurate, robust yet lightweight human pose estimator, one feasible way is to transfer pose knowledge from a powerful teacher model to a less-parameterized student model by knowledge distillation. However, the traditional knowledge distillation framework does not fully explore the contextual information among human joints. Thus, in this paper, we propose a novel coarse-to-fine two-stage knowledge distillation framework for human pose estimation. In the first-stage distillation, we introduce the human joints structure loss to mine the structural information among human joints so as to transfer high-level semantic knowledge from the teacher model to the student model. In the second-stage distillation, we utilize an Image-Guided Progressive Graph Convolutional Network (IGP-GCN) to refine the initial human pose obtained from the first-stage distillation and supervise the training of the IGP-GCN in the progressive way by the final output pose of teacher model. The extensive experiments on the benchmark dataset: COCO keypoint and CrowdPose datasets, show that our proposed method performs favorably against lots of the existing state-of-the-art human pose estimation methods, especially for the more complex CrowdPose dataset, the performance improvement of our model is more significant.</li>
<li><strong>摘要：</strong>人姿势估计已被广泛应用于以人为中心的理解和产生，但是大多数现有的最新人类姿势估计方法都需要大量的计算资源来进行准确的预测。为了获得一种准确，健壮但轻巧的人姿势估计器，一种可行的方法是通过知识蒸馏将姿势知识从强大的教师模型转移到较少参数化的学生模型。但是，传统的知识蒸馏框架并不能完全探索人类关节之间的上下文信息。因此，在本文中，我们提出了一个新型的粗到精细的两阶段知识蒸馏框架，用于人类姿势估计。在第一阶段蒸馏中，我们引入了人类关节结构的损失，以挖掘人类关节之间的结构信息，以便将高级语义知识从教师模型转移到学生模型。在第二阶段蒸馏中，我们利用图像引导的渐进式卷积网络（IGP-GCN）来完善从第一阶段蒸馏获得的初始人姿势，并通过最终的教师模型的最终输出姿势以渐进的方式监督IGP-GCN的训练。基准数据集（可可键和人群数据集）上的广泛实验表明，我们所提出的方法对许多现有的最新人类姿势估计方法表现出色，尤其是对于更复杂的人群数据集，我们模型的性能改进更为重要。</li>
</ul>

<h3>Title: Probing the Representational Power of Sparse Autoencoders in Vision Models</h3>
<ul>
<li><strong>Authors: </strong>Matthew Lyle Olson, Musashi Hinck, Neale Ratzlaff, Changbai Li, Phillip Howard, Vasudev Lal, Shao-Yen Tseng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11277">https://arxiv.org/abs/2508.11277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11277">https://arxiv.org/pdf/2508.11277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11277]] Probing the Representational Power of Sparse Autoencoders in Vision Models(https://arxiv.org/abs/2508.11277)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting the hidden states of large language models (LLMs). By learning to reconstruct activations from a sparse bottleneck layer, SAEs discover interpretable features from the high-dimensional internal representations of LLMs. Despite their popularity with language models, SAEs remain understudied in the visual domain. In this work, we provide an extensive evaluation the representational power of SAEs for vision models using a broad range of image-based tasks. Our experimental results demonstrate that SAE features are semantically meaningful, improve out-of-distribution generalization, and enable controllable generation across three vision model architectures: vision embedding models, multi-modal LMMs and diffusion models. In vision embedding models, we find that learned SAE features can be used for OOD detection and provide evidence that they recover the ontological structure of the underlying model. For diffusion models, we demonstrate that SAEs enable semantic steering through text encoder manipulation and develop an automated pipeline for discovering human-interpretable attributes. Finally, we conduct exploratory experiments on multi-modal LLMs, finding evidence that SAE features reveal shared representations across vision and language modalities. Our study provides a foundation for SAE evaluation in vision models, highlighting their strong potential improving interpretability, generalization, and steerability in the visual domain.</li>
<li><strong>摘要：</strong>稀疏的自动编码器（SAE）已成为解释大语模型（LLMS）隐藏状态的流行工具。通过学习从稀疏瓶颈层重建激活，SAE从LLM的高维内部表示可解释的特征。尽管他们在语言模型中很受欢迎，但SAE在视觉域中仍在研究。在这项工作中，我们提供了广泛的评估，使用广泛的基于图像的任务为视觉模型提供了SAE的代表力。我们的实验结果表明，SAE特征在语义上是有意义的，改善了分布的概括，并在三种视觉模型体系结构中启用可控生成：视觉嵌入模型，多模式LMMS和扩散模型。在视觉嵌入模型中，我们发现学习的SAE特征可用于OOD检测，并提供证据表明它们恢复了基础模型的本体论结构。对于扩散模型，我们证明SAE可以通过文本编码器操纵来实现语义转向，并开发出一种自动化管道，以发现人解剖属性。最后，我们对多模式LLM进行探索性实验，发现SAE特征揭示了跨视觉和语言方式共享表示的证据。我们的研究为视觉模型评估的基础为基础提供了基础，突出了它们在视觉领域中强大的潜在提高可解释性，概括性和可接收性。</li>
</ul>

<h3>Title: TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation</h3>
<ul>
<li><strong>Authors: </strong>Yilin Mi, Qixin Yan, Zheng-Peng Duan, Chunle Guo, Hubery Yin, Hao Liu, Chen Li, Chongyi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11284">https://arxiv.org/abs/2508.11284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11284">https://arxiv.org/pdf/2508.11284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11284]] TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation(https://arxiv.org/abs/2508.11284)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the advancement of generative models, facial image editing has made significant progress. However, achieving fine-grained age editing while preserving personal identity remains a challenging this http URL this paper, we propose TimeMachine, a novel diffusion-based framework that achieves accurate age editing while keeping identity features unchanged. To enable fine-grained age editing, we inject high-precision age information into the multi-cross attention module, which explicitly separates age-related and identity-related features. This design facilitates more accurate disentanglement of age attributes, thereby allowing precise and controllable manipulation of facial this http URL, we propose an Age Classifier Guidance (ACG) module that predicts age directly in the latent space, instead of performing denoising image reconstruction during training. By employing a lightweight module to incorporate age constraints, this design enhances age editing accuracy by modest increasing training cost. Additionally, to address the lack of large-scale, high-quality facial age datasets, we construct a HFFA dataset (High-quality Fine-grained Facial-Age dataset) which contains one million high-resolution images labeled with identity and facial attributes. Experimental results demonstrate that TimeMachine achieves state-of-the-art performance in fine-grained age editing while preserving identity consistency.</li>
<li><strong>摘要：</strong>随着生成模型的发展，面部图像编辑取得了重大进展。但是，在保留个人身份的同时，在本文中实现细粒度的编辑仍然是一个具有挑战性的本文，我们提出了Timemachine，这是一种基于扩散的新型框架，可以实现准确的年龄编辑，同时保持身份不变的特征。为了启用细粒度的编辑，我们将高精度年龄信息注入多跨度注意模块，该模块明确将与年龄有关的特征分开。这种设计促进了更准确的年龄属性分解，从而允许对这个HTTP URL进行精确且可控制的面部操作，我们提出了一个年龄分类器指南（ACG）模块，该模块直接预测了潜在空间中的年龄，而不是在训练过程中表现出剥离图像构造。通过使用轻质模块来纳入年龄限制，该设计通过适度提高培训成本提高了年龄编辑的准确性。此外，为了解决缺乏大规模的高质量面部年龄数据集，我们构建了一个HFFA数据集（高质量的高粒度面部年龄数据集），其中包含一百万个带有身份和面部属性的高分辨率图像。实验结果表明，Timemachine在细粒度编辑中实现最先进的表现，同时保持身份一致性。</li>
</ul>

<h3>Title: Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Zhou, Jiayu Tang, Shuo Yang, Xiaoyan Xiao, Yuqin Dai, Wenhao Yang, Chao Gou, Xiaobo Xia, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11317">https://arxiv.org/abs/2508.11317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11317">https://arxiv.org/pdf/2508.11317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11317]] Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models(https://arxiv.org/abs/2508.11317)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs), exemplified by CLIP, have emerged as foundational for multimodal intelligence. However, their capacity for logical understanding remains significantly underexplored, resulting in critical ''logical blindspots'' that limit their reliability in practical applications. To systematically diagnose this, we introduce LogicBench, a comprehensive benchmark with over 50,000 vision-language pairs across 9 logical categories and 4 diverse scenarios: images, videos, anomaly detection, and medical diagnostics. Our evaluation reveals that existing VLMs, even the state-of-the-art ones, fall at over 40 accuracy points below human performance, particularly in challenging tasks like Causality and Conditionality, highlighting their reliance on surface semantics over critical logical structures. To bridge this gap, we propose LogicCLIP, a novel training framework designed to boost VLMs' logical sensitivity through advancements in both data generation and optimization objectives. LogicCLIP utilizes logic-aware data generation and a contrastive learning strategy that combines coarse-grained alignment, a fine-grained multiple-choice objective, and a novel logical structure-aware objective. Extensive experiments demonstrate LogicCLIP's substantial improvements in logical comprehension across all LogicBench domains, significantly outperforming baselines. Moreover, LogicCLIP retains, and often surpasses, competitive performance on general vision-language benchmarks, demonstrating that the enhanced logical understanding does not come at the expense of general alignment. We believe that LogicBench and LogicCLIP will be important resources for advancing VLM logical capabilities.</li>
<li><strong>摘要：</strong>通过剪辑举例说明的视觉语言模型（VLM）已成为多模式智能的基础。但是，它们的逻辑理解能力仍然显着不流失，从而限制了其在实际应用中的可靠性。为了系统地诊断这一点，我们介绍了LogicBench，这是一个全面的基准测试，其中9个逻辑类别和4种不同的情景具有超过50,000个视觉 - 语言对：图像，视频，异常检测和医学诊断。我们的评估表明，现有的VLM，甚至是最先进的VLM，都低于人类绩效的40多个精度，尤其是在因果关系和条件性等挑战性的任务中，强调了它们对批判性逻辑结构的依赖。为了弥合这一差距，我们提出了LogicClip，这是一个新颖的培训框架，旨在通过数据生成和优化目标来提高VLMS的逻辑灵敏度。 LogicClip利用了逻辑感知的数据生成和对比的学习策略，该策略结合了粗粒粒度的对齐，一个细粒度的多项选择目标以及一种新颖的逻辑结构感知目标。广泛的实验证明了LogicClip在所有LogicBench领域的逻辑理解方面的实质性改进，这显着超过了基线。此外，LogicClip保留并经常超过一般视觉基准的竞争性能，这表明增强的逻辑理解并不能以一般一致性为基础。我们认为，LogicBench和LogicClip将是推进VLM逻辑功能的重要资源。</li>
</ul>

<h3>Title: Noise Matters: Optimizing Matching Noise for Diffusion Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Yanghao Wang, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11330">https://arxiv.org/abs/2508.11330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11330">https://arxiv.org/pdf/2508.11330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11330]] Noise Matters: Optimizing Matching Noise for Diffusion Classifiers(https://arxiv.org/abs/2508.11330)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Although today's pretrained discriminative vision-language models (e.g., CLIP) have demonstrated strong perception abilities, such as zero-shot image classification, they also suffer from the bag-of-words problem and spurious bias. To mitigate these problems, some pioneering studies leverage powerful generative models (e.g., pretrained diffusion models) to realize generalizable image classification, dubbed Diffusion Classifier (DC). Specifically, by randomly sampling a Gaussian noise, DC utilizes the differences of denoising effects with different category conditions to classify categories. Unfortunately, an inherent and notorious weakness of existing DCs is noise instability: different random sampled noises lead to significant performance changes. To achieve stable classification performance, existing DCs always ensemble the results of hundreds of sampled noises, which significantly reduces the classification speed. To this end, we firstly explore the role of noise in DC, and conclude that: there are some ``good noises'' that can relieve the instability. Meanwhile, we argue that these good noises should meet two principles: Frequency Matching and Spatial Matching. Regarding both principles, we propose a novel Noise Optimization method to learn matching (i.e., good) noise for DCs: NoOp. For frequency matching, NoOp first optimizes a dataset-specific noise: Given a dataset and a timestep t, optimize one randomly initialized parameterized noise. For Spatial Matching, NoOp trains a Meta-Network that adopts an image as input and outputs image-specific noise offset. The sum of optimized noise and noise offset will be used in DC to replace random noise. Extensive ablations on various datasets demonstrated the effectiveness of NoOp.</li>
<li><strong>摘要：</strong>尽管当今预算的判别视觉语言模型（例如剪辑）表现出了强大的感知能力，例如零照片的图像分类，但它们也遭受了词具问题和虚假偏见的困扰。为了减轻这些问题，一些开创性的研究利用强大的生成模型（例如，经过预告片的扩散模型）实现可概括的图像分类，称为扩散分类器（DC）。具体而言，通过随机采样高斯噪声，DC利用具有不同类别条件的deNoise效应的差异来对类别进行分类。不幸的是，现有DC的固有且臭名昭著的弱点是噪声不稳定性：不同的随机抽样噪声会导致性能发生重大变化。为了达到稳定的分类性能，现有的DC始终将数百个抽样噪声的结果整合，这大大降低了分类速度。为此，我们首先探讨了噪声在DC中的作用，并得出结论：有一些``好声音''可以缓解不稳定性。同时，我们认为这些好声音应该符合两个原则：频率匹配和空间匹配。关于这两个原则，我们提出了一种新型的噪声优化方法，以学习DCS：NOOP的匹配（即良好）噪声。对于频率匹配，NOOP首先优化了数据集特定的噪声：给定数据集和一个时间段t，优化了一个随机初始初始化的参数化噪声。对于空间匹配，NOOP训练一种元网络，该元网络采用图像作为输入，并输出特定图像的噪声偏移。 DC中将使用优化的噪声和噪声偏移的总和来替换随机噪声。各种数据集的大量消融证明了NOOP的有效性。</li>
</ul>

<h3>Title: GANDiff FR: Hybrid GAN Diffusion Synthesis for Causal Bias Attribution in Face Recognition</h3>
<ul>
<li><strong>Authors: </strong>Md Asgor Hossain Reaj, Rajan Das Gupta, Md Yeasin Rahat, Nafiz Fahad, Md Jawadul Hasan, Tze Hui Liew</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11334">https://arxiv.org/abs/2508.11334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11334">https://arxiv.org/pdf/2508.11334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11334]] GANDiff FR: Hybrid GAN Diffusion Synthesis for Causal Bias Attribution in Face Recognition(https://arxiv.org/abs/2508.11334)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce GANDiff FR, the first synthetic framework that precisely controls demographic and environmental factors to measure, explain, and reduce bias with reproducible rigor. GANDiff FR unifies StyleGAN3-based identity-preserving generation with diffusion-based attribute control, enabling fine-grained manipulation of pose around 30 degrees, illumination (four directions), and expression (five levels) under ceteris paribus conditions. We synthesize 10,000 demographically balanced faces across five cohorts validated for realism via automated detection (98.2%) and human review (89%) to isolate and quantify bias drivers. Benchmarking ArcFace, CosFace, and AdaFace under matched operating points shows AdaFace reduces inter-group TPR disparity by 60% (2.5% vs. 6.3%), with illumination accounting for 42% of residual bias. Cross-dataset evaluation on RFW, BUPT, and CASIA WebFace confirms strong synthetic-to-real transfer (r 0.85). Despite around 20% computational overhead relative to pure GANs, GANDiff FR yields three times more attribute-conditioned variants, establishing a reproducible, regulation-aligned (EU AI Act) standard for fairness auditing. Code and data are released to support transparent, scalable bias evaluation.</li>
<li><strong>摘要：</strong>我们介绍了Gandiff FR，这是第一个精确控制人口统计学和环境因素以测量，解释和减少可重复性严格的偏见的合成框架。 Gandiff FR通过基于扩散的属性控制统一了基于StyleGAN的身份的生成，从而可以对姿势进行细粒度的操纵约30度，照明（四个方向）和表达（五个级别）（在ceteris paribus条件下）。我们通过自动检测（98.2％）和人类审查（89％）综合了五个人口平衡的面孔，以隔离和量化偏见驱动因素。在匹配的操作点下，基准测试弧形，圆顶和Adaface显示，Adaface将组间TPR差异降低了60％（2.5％vs. 6.3％），照明占残留偏置的42％。 RFW，BUPT和CASIA WebFace上的跨数据库评估证实了强综合到现实的转移（r 0.85）。尽管相对于纯gans，但甘道夫fr的计算间接费用约为20％，但对属性条件的变体产生了三倍，建立了可再现的，调节的（欧盟AI ACT）的公平审核标准。释放代码和数据以支持透明，可扩展的偏见评估。</li>
</ul>

<h3>Title: RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator</h3>
<ul>
<li><strong>Authors: </strong>Zhiming Liu, Nantheera Anantrasirichai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11409">https://arxiv.org/abs/2508.11409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11409">https://arxiv.org/pdf/2508.11409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11409]] RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator(https://arxiv.org/abs/2508.11409)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Atmospheric turbulence severely degrades video quality by introducing distortions such as geometric warping, blur, and temporal flickering, posing significant challenges to both visual clarity and temporal consistency. Current state-of-the-art methods are based on transformer and 3D architectures and require multi-frame input, but their large computational cost and memory usage limit real-time deployment, especially in resource-constrained scenarios. In this work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator, designed for efficient and temporally consistent video restoration under AT conditions. RMFAT adopts a lightweight recurrent framework that restores each frame using only two inputs at a time, significantly reducing temporal window size and computational burden. It further integrates multi-scale feature encoding and decoding with temporal warping modules at both encoder and decoder stages to enhance spatial detail and temporal coherence. Extensive experiments on synthetic and real-world atmospheric turbulence datasets demonstrate that RMFAT not only outperforms existing methods in terms of clarity restoration (with nearly a 9\% improvement in SSIM) but also achieves significantly improved inference speed (more than a fourfold reduction in runtime), making it particularly suitable for real-time atmospheric turbulence suppression tasks.</li>
<li><strong>摘要：</strong>大气湍流通过引入诸如几何翘曲，模糊和时间闪烁之类的扭曲来严重降低视频质量，并对视觉清晰度和时间一致性构成重大挑战。当前的最新方法基于变压器和3D体系结构，需要多帧输入，但是它们的大量计算成本和内存使用限制了实时部署，尤其是在资源受限的场景中。在这项工作中，我们提出了RMFAT：经常性的多尺度特征大气湍流缓解剂，旨在在条件下有效且具有时间一致的视频修复。 RMFAT采用轻巧的经过重复的框架，一次仅使用两个输入来恢复每个帧，从而大大减少了时间窗口大小和计算负担。它进一步集成了多尺度的特征编码和解码，并在编码器和解码器阶段都与时间翘曲模块进行了编码，以增强空间细节和时间连贯性。关于合成和现实世界大气湍流数据集的广泛实验表明，RMFAT不仅在清晰度恢复方面胜过现有方法（SSIM的差异几乎是9 \％），而且还可以显着提高推理的速度（不比运行时降低了四倍的降低），使其特别适合于实时抑制大气构件。</li>
</ul>

<h3>Title: Generative Co-Design of Antibody Sequences and Structures via Black-Box Guidance in a Shared Latent Space</h3>
<ul>
<li><strong>Authors: </strong>Yinghua Yao, Yuangang Pan, Xixian Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11424">https://arxiv.org/abs/2508.11424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11424">https://arxiv.org/pdf/2508.11424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11424]] Generative Co-Design of Antibody Sequences and Structures via Black-Box Guidance in a Shared Latent Space(https://arxiv.org/abs/2508.11424)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Advancements in deep generative models have enabled the joint modeling of antibody sequence and structure, given the antigen-antibody complex as context. However, existing approaches for optimizing complementarity-determining regions (CDRs) to improve developability properties operate in the raw data space, leading to excessively costly evaluations due to the inefficient search process. To address this, we propose LatEnt blAck-box Design (LEAD), a sequence-structure co-design framework that optimizes both sequence and structure within their shared latent space. Optimizing shared latent codes can not only break through the limitations of existing methods, but also ensure synchronization of different modality designs. Particularly, we design a black-box guidance strategy to accommodate real-world scenarios where many property evaluators are non-differentiable. Experimental results demonstrate that our LEAD achieves superior optimization performance for both single and multi-property objectives. Notably, LEAD reduces query consumption by a half while surpassing baseline methods in property optimization. The code is available at this https URL.</li>
<li><strong>摘要：</strong>考虑到抗原抗体复合物作为背景，深层生成模型的进步使抗体序列和结构的关节建模。但是，现有用于优化互补性确定区域（CDR）以提高开发性能的方法在原始数据空间中起作用，从而导致由于搜索过程效率低下而导致过分昂贵的评估。为了解决这个问题，我们提出了潜在的黑框设计（Lead），这是一个序列结构共同设计框架，可在其共享潜在空间内优化序列和结构。优化共享的潜在代码不仅可以突破现有方法的局限性，而且还可以确保不同模态设计的同步。特别是，我们设计了一个黑盒指导策略，以适应许多物业评估者不可差异的现实情况。实验结果表明，我们的铅为单一和多秘诀目标都实现了出色的优化性能。值得注意的是，铅可将查询消耗减少一半，同时超过属性优化中的基线方法。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Jingyu Li, Bozhou Zhang, Xin Jin, Jiankang Deng, Xiatian Zhu, Li Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11428">https://arxiv.org/abs/2508.11428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11428">https://arxiv.org/pdf/2508.11428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11428]] ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving(https://arxiv.org/abs/2508.11428)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autonomous driving requires rich contextual comprehension and precise predictive reasoning to navigate dynamic and complex environments safely. Vision-Language Models (VLMs) and Driving World Models (DWMs) have independently emerged as powerful recipes addressing different aspects of this challenge. VLMs provide interpretability and robust action prediction through their ability to understand multi-modal context, while DWMs excel in generating detailed and plausible future driving scenarios essential for proactive planning. Integrating VLMs with DWMs is an intuitive, promising, yet understudied strategy to exploit the complementary strengths of accurate behavioral prediction and realistic scene generation. Nevertheless, this integration presents notable challenges, particularly in effectively connecting action-level decisions with high-fidelity pixel-level predictions and maintaining computational efficiency. In this paper, we propose ImagiDrive, a novel end-to-end autonomous driving framework that integrates a VLM-based driving agent with a DWM-based scene imaginer to form a unified imagination-and-planning loop. The driving agent predicts initial driving trajectories based on multi-modal inputs, guiding the scene imaginer to generate corresponding future scenarios. These imagined scenarios are subsequently utilized to iteratively refine the driving agent's planning decisions. To address efficiency and predictive accuracy challenges inherent in this integration, we introduce an early stopping mechanism and a trajectory selection strategy. Extensive experimental validation on the nuScenes and NAVSIM datasets demonstrates the robustness and superiority of ImagiDrive over previous alternatives under both open-loop and closed-loop conditions.</li>
<li><strong>摘要：</strong>自主驾驶需要丰富的上下文理解和精确的预测推理，以安全地浏览动态和复杂的环境。视觉语言模型（VLM）和驱动世界模型（DWM）已独立出现，作为解决此挑战不同方面的强大食谱。 VLM通过理解多模式上下文的能力提供了可解释性和强大的行动预测，而DWM excel在生成详细且合理的未来驾驶场景方面对于积极的计划必不可少。将VLM与DWMS相结合是一种直观，有前途但经过研究的策略，可以利用准确的行为预测和现实场景产生的互补优势。然而，这种整合提出了显着的挑战，尤其是在有效地将动作级别的决策与高保真像素级的预测联系起来并保持计算效率时。在本文中，我们提出了一个新颖的端到端自动驾驶框架Imagidrive，将基于VLM的驱动器与基于DWM的场景Imaginer集成在一起，以形成统一的想象力和规划循环。驱动剂预测基于多模式输入的初始驾驶轨迹，从而指导场景Imaginer生成相应的未来场景。这些想象中的方案随后被用来迭代地完善驾驶代理的计划决策。为了解决此整合固有的效率和预测准确性挑战，我们引入了早期停止机制和轨迹选择策略。对Nuscenes和NAVSIM数据集的广泛实验验证证明了Imagidrive在开环和闭环条件下的鲁棒性和优越性。</li>
</ul>

<h3>Title: MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Qian Liang, Yujia Wu, Kuncheng Li, Jiwei Wei, Shiyuan He, Jinyu Guo, Ning Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11433">https://arxiv.org/abs/2508.11433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11433">https://arxiv.org/pdf/2508.11433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11433]] MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation(https://arxiv.org/abs/2508.11433)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) with unified architectures excel across a wide range of vision-language tasks, yet aligning them with personalized image generation remains a significant challenge. Existing methods for MLLMs are frequently subject-specific, demanding a data-intensive fine-tuning process for every new subject, which limits their scalability. In this paper, we introduce MM-R1, a framework that integrates a cross-modal Chain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of unified MLLMs for personalized image generation. Specifically, we structure personalization as an integrated visual reasoning and generation process: (1) grounding subject concepts by interpreting and understanding user-provided images and contextual cues, and (2) generating personalized images conditioned on both the extracted subject representations and user prompts. To further enhance the reasoning capability, we adopt Grouped Reward Proximal Policy Optimization (GRPO) to explicitly align the generation. Experiments demonstrate that MM-R1 unleashes the personalization capability of unified MLLMs to generate images with high subject fidelity and strong text alignment in a zero-shot manner.</li>
<li><strong>摘要：</strong>具有统一体系结构的多模式大语言模型（MLLM）在各种视觉语言任务中都表现出色，但是将它们与个性化的图像生成保持一致仍然是一个重大挑战。现有的MLLM方法通常是特定于主题的，要求每个新主题进行数据密集的微调过程，从而限制其可扩展性。在本文中，我们介绍了MM-R1，该框架集成了跨模式链（X-COT）推理策略，以解锁统一MLLM的固有潜力以产生个性化图像生成。具体而言，我们将个性化作为一个集成的视觉推理和生成过程：（1）通过解释和理解用户提供的图像和上下文提示来扎根主题概念，以及（2）生成以提取的主题表示和用户提示为条件的个性化图像。为了进一步增强推理能力，我们采用了分组的奖励近端政策优化（GRPO），以明确调整这一代。实验表明，MM-R1释放了统一MLLM的个性化能力，可以以零拍的方式生成具有高主题忠诚度和强大文本对齐的图像。</li>
</ul>

<h3>Title: Multi-Sensory Cognitive Computing for Learning Population-level Brain Connectivity</h3>
<ul>
<li><strong>Authors: </strong>Mayssa Soussia, Mohamed Ali Mahjoub, Islem Rekik</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11436">https://arxiv.org/abs/2508.11436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11436">https://arxiv.org/pdf/2508.11436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11436]] Multi-Sensory Cognitive Computing for Learning Population-level Brain Connectivity(https://arxiv.org/abs/2508.11436)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The generation of connectional brain templates (CBTs) has recently garnered significant attention for its potential to identify unique connectivity patterns shared across individuals. However, existing methods for CBT learning such as conventional machine learning and graph neural networks (GNNs) are hindered by several limitations. These include: (i) poor interpretability due to their black-box nature, (ii) high computational cost, and (iii) an exclusive focus on structure and topology, overlooking the cognitive capacity of the generated CBT. To address these challenges, we introduce mCOCO (multi-sensory COgnitive COmputing), a novel framework that leverages Reservoir Computing (RC) to learn population-level functional CBT from BOLD (Blood-Oxygen-level-Dependent) signals. RC's dynamic system properties allow for tracking state changes over time, enhancing interpretability and enabling the modeling of brain-like dynamics, as demonstrated in prior literature. By integrating multi-sensory inputs (e.g., text, audio, and visual data), mCOCO captures not only structure and topology but also how brain regions process information and adapt to cognitive tasks such as sensory processing, all in a computationally efficient manner. Our mCOCO framework consists of two phases: (1) mapping BOLD signals into the reservoir to derive individual functional connectomes, which are then aggregated into a group-level CBT - an approach, to the best of our knowledge, not previously explored in functional connectivity studies - and (2) incorporating multi-sensory inputs through a cognitive reservoir, endowing the CBT with cognitive traits. Extensive evaluations show that our mCOCO-based template significantly outperforms GNN-based CBT in terms of centeredness, discriminativeness, topological soundness, and multi-sensory memory retention. Our source code is available at this https URL.</li>
<li><strong>摘要：</strong>连接脑模板（CBT）的产生最近引起了人们对识别个人共享独特连通模式的潜力的极大关注。但是，现有的CBT学习方法，例如传统的机器学习和图形神经网络（GNN）受到了一些局限性的阻碍。其中包括：（i）由于其黑盒性质，（ii）高计算成本以及（iii）对结构和拓扑的独家重点，忽略了生成的CBT的认知能力。为了应对这些挑战，我们介绍了MCOCO（多感觉认知计算），该框架利用储层计算（RC）从BOLD（血液氧依赖性依赖性）信号中学习群体级功能CBT。 RC的动态系统属性允许随着时间的推移跟踪状态变化，增强可解释性并实现了脑样动力学的建模，如先前的文献所示。通过整合多感官输入（例如文本，音频和视觉数据），MCOCO不仅捕获了结构和拓扑，而且捕获了大脑区域如何处理信息并适应认知任务，例如感官处理，所有这些都以计算上有效的方式进行。 Our mCOCO framework consists of two phases: (1) mapping BOLD signals into the reservoir to derive individual functional connectomes, which are then aggregated into a group-level CBT - an approach, to the best of our knowledge, not previously explored in functional connectivity studies - and (2) incorporating multi-sensory inputs through a cognitive reservoir, endowing the CBT with cognitive traits.广泛的评估表明，我们基于MCOCO的模板在集中性，判别性，拓扑声学性和多感官内存保留方面显着优于基于GNN的CBT。我们的源代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation</h3>
<ul>
<li><strong>Authors: </strong>Daniel Airinei, Elena Burceanu, Marius Leordeanu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11446">https://arxiv.org/abs/2508.11446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11446">https://arxiv.org/pdf/2508.11446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11446]] Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation(https://arxiv.org/abs/2508.11446)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Indoor navigation is a difficult task, as it generally comes with poor GPS access, forcing solutions to rely on other sources of information. While significant progress continues to be made in this area, deployment to production applications is still lacking, given the complexity and additional requirements of current solutions. Here, we introduce an efficient, real-time and easily deployable deep learning approach, based on visual input only, that can predict the direction towards a target from images captured by a mobile device. Our technical approach, based on a novel graph-based path generation method, combined with explainable data augmentation and curriculum learning, includes contributions that make the process of data collection, annotation and training, as automatic as possible, efficient and robust. On the practical side, we introduce a novel largescale dataset, with video footage inside a relatively large shopping mall, in which each frame is annotated with the correct next direction towards different specific target destinations. Different from current methods, ours relies solely on vision, avoiding the need of special sensors, additional markers placed along the path, knowledge of the scene map or internet access. We also created an easy to use application for Android, which we plan to make publicly available. We make all our data and code available along with visual demos on our project site</li>
<li><strong>摘要：</strong>室内导航是一项艰巨的任务，因为它通常伴随着GPS不良的访问，迫使解决方案依靠其他信息来源。尽管该领域仍在继续取得重大进展，但考虑到当前解决方案的复杂性和其他要求，仍缺乏生产应用程序的部署。在这里，我们仅基于视觉输入引入了一种高效，实时且易于部署的深度学习方法，该方法可以从移动设备捕获的图像中预测目标方向。我们的技术方法基于一种基于图形的新型路径生成方法，结合了可解释的数据增强和课程学习，其中包括贡献，使数据收集，注释和培训的过程尽可能自动，高效且健壮。在实用方面，我们介绍了一个新颖的LargeScale数据集，并在一个相对较大的购物中心内部带有录像带，其中每个框架都以正确的下一个方向向不同的特定目标目的地提供了注释。与当前方法不同，我们的方法仅依赖视觉，避免了特殊传感器的需求，沿路径上放置的其他标记，场景图的知识或互联网访问。我们还创建了一个易于使用的Android应用程序，我们计划公开使用。我们将所有数据和代码与我们的项目网站上的视觉演示一起提供</li>
</ul>

<h3>Title: Data-Driven Deepfake Image Detection Method -- The 2024 Global Deepfake Image Detection Challenge</h3>
<ul>
<li><strong>Authors: </strong>Xiaoya Zhu, Yibing Nan, Shiguo Lian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11464">https://arxiv.org/abs/2508.11464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11464">https://arxiv.org/pdf/2508.11464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11464]] Data-Driven Deepfake Image Detection Method -- The 2024 Global Deepfake Image Detection Challenge(https://arxiv.org/abs/2508.11464)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rapid development of technology in the field of AI, deepfake technology has emerged as a double-edged sword. It has not only created a large amount of AI-generated content but also posed unprecedented challenges to digital security. The task of the competition is to determine whether a face image is a Deepfake image and output its probability score of being a Deepfake image. In the image track competition, our approach is based on the Swin Transformer V2-B classification network. And online data augmentation and offline sample generation methods are employed to enrich the diversity of training samples and increase the generalization ability of the model. Finally, we got the award of excellence in Deepfake image detection.</li>
<li><strong>摘要：</strong>随着AI领域技术的快速发展，DeepFake技术已成为双刃剑。它不仅创造了大量的AI生成的内容，而且对数字安全构成了前所未有的挑战。竞争的任务是确定面部图像是否是深泡图像，并输出其作为深击图像的概率得分。在图像轨道竞争中，我们的方法基于Swin Transformer V2-B分类网络。并采用了在线数据增强和离线样本生成方法来丰富培训样本的多样性并提高模型的概括能力。最后，我们获得了DeepFake图像检测的卓越奖。</li>
</ul>

<h3>Title: CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxue Wu, Bingjie Gao, Yu Qiao, Yaohui Wang, Xinyuan Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11484">https://arxiv.org/abs/2508.11484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11484">https://arxiv.org/pdf/2508.11484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11484]] CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models(https://arxiv.org/abs/2508.11484)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite significant advances in video synthesis, research into multi-shot video generation remains in its infancy. Even with scaled-up models and massive datasets, the shot transition capabilities remain rudimentary and unstable, largely confining generated videos to single-shot sequences. In this work, we introduce CineTrans, a novel framework for generating coherent multi-shot videos with cinematic, film-style transitions. To facilitate insights into the film editing style, we construct a multi-shot video-text dataset Cine250K with detailed shot annotations. Furthermore, our analysis of existing video diffusion models uncovers a correspondence between attention maps in the diffusion model and shot boundaries, which we leverage to design a mask-based control mechanism that enables transitions at arbitrary positions and transfers effectively in a training-free setting. After fine-tuning on our dataset with the mask mechanism, CineTrans produces cinematic multi-shot sequences while adhering to the film editing style, avoiding unstable transitions or naive concatenations. Finally, we propose specialized evaluation metrics for transition control, temporal consistency and overall quality, and demonstrate through extensive experiments that CineTrans significantly outperforms existing baselines across all criteria.</li>
<li><strong>摘要：</strong>尽管视频综合方面取得了重大进展，但对多拍视频生成的研究仍处于起步阶段。即使使用扩展模型和大量数据集，射击过渡功能仍然是基本的和不稳定的，在很大程度上将生成的视频限制在单次序列中。在这项工作中，我们介绍了Cinetrans，这是一个新颖的框架，用于通过电影，电影风格的过渡生成连贯的多拍视频。为了促进对电影编辑风格的见解，我们构建了带有详细的镜头注释的多拍视频数据集Cine250k。此外，我们对现有视频扩散模型的分析发现了扩散模型和射击边界中的注意图之间的对应关系，我们利用这些对应图在设计基于掩模的控制机制上，该机制可以在任意位置和在无训练环境中有效地转移的过渡。在使用掩模机制对数据集进行了微调后，Cinetrans在粘附胶片编辑样式的同时产生了电影的多拍序列，避免了不稳定的过渡或幼稚的串联。最后，我们提出了用于过渡控制，时间一致性和整体质量的专门评估指标，并通过广泛的实验证明，在所有标准上，Cinetrans明显优于现有基线。</li>
</ul>

<h3>Title: DiCriTest: Testing Scenario Generation for Decision-Making Agents Considering Diversity and Criticality</h3>
<ul>
<li><strong>Authors: </strong>Qitong Chu, Yufeng Yue, Danya Yao, Huaxin Pei</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11514">https://arxiv.org/abs/2508.11514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11514">https://arxiv.org/pdf/2508.11514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11514]] DiCriTest: Testing Scenario Generation for Decision-Making Agents Considering Diversity and Criticality(https://arxiv.org/abs/2508.11514)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The growing deployment of decision-making agents in dynamic environments increases the demand for safety verification. While critical testing scenario generation has emerged as an appealing verification methodology, effectively balancing diversity and criticality remains a key challenge for existing methods, particularly due to local optima entrapment in high-dimensional scenario spaces. To address this limitation, we propose a dual-space guided testing framework that coordinates scenario parameter space and agent behavior space, aiming to generate testing scenarios considering diversity and criticality. Specifically, in the scenario parameter space, a hierarchical representation framework combines dimensionality reduction and multi-dimensional subspace evaluation to efficiently localize diverse and critical subspaces. This guides dynamic coordination between two generation modes: local perturbation and global exploration, optimizing critical scenario quantity and diversity. Complementarily, in the agent behavior space, agent-environment interaction data are leveraged to quantify behavioral criticality/diversity and adaptively support generation mode switching, forming a closed feedback loop that continuously enhances scenario characterization and exploration within the parameter space. Experiments show our framework improves critical scenario generation by an average of 56.23\% and demonstrates greater diversity under novel parameter-behavior co-driven metrics when tested on five decision-making agents, outperforming state-of-the-art baselines.</li>
<li><strong>摘要：</strong>在动态环境中，决策代理的部署日益增长会增加对安全验证的需求。尽管关键的测试场景产生已成为一种吸引人的验证方法，但有效地平衡多样性和批判性仍然是现有方法的关键挑战，尤其是由于本地Optima在高维场景中的陷入困境。为了解决这一限制，我们提出了一个双空间指导测试框架，该框架协调方案参数空间和代理行为空间，旨在考虑考虑多样性和关键性的测试场景。具体而言，在场景参数空间中，层次表示框架结合了维度降低和多维子空间评估，从而有效地定位了多样化和关键的子空间。这指导了两代模式之间的动态协调：局部扰动和全球探索，优化了关键场景数量和多样性。在互补的情况下，在代理行为空间中，借用了代理 - 环境相互作用数据来量化行为临界/多样性和自适应支持生成模式切换，形成了封闭的反馈回路，从而不断增强参数空间内的场景表征和探索。实验表明，我们的框架平均提高了56.23 \％的关键场景生成，并在新的参数行为共同驱动的指标下表现出更大的多样性，当时对五个决策代理进行测试，表现优于最先进的盆地。</li>
</ul>

<h3>Title: Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series</h3>
<ul>
<li><strong>Authors: </strong>Juhi Soni, Markus Lange-Hegermann, Stefan Windmann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11528">https://arxiv.org/abs/2508.11528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11528">https://arxiv.org/pdf/2508.11528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11528]] Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series(https://arxiv.org/abs/2508.11528)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose an unsupervised anomaly detection approach based on a physics-informed diffusion model for multivariate time series data. Over the past years, diffusion model has demonstrated its effectiveness in forecasting, imputation, generation, and anomaly detection in the time series domain. In this paper, we present a new approach for learning the physics-dependent temporal distribution of multivariate time series data using a weighted physics-informed loss during diffusion model training. A weighted physics-informed loss is constructed using a static weight schedule. This approach enables a diffusion model to accurately approximate underlying data distribution, which can influence the unsupervised anomaly detection performance. Our experiments on synthetic and real-world datasets show that physics-informed training improves the F1 score in anomaly detection; it generates better data diversity and log-likelihood. Our model outperforms baseline approaches, additionally, it surpasses prior physics-informed work and purely data-driven diffusion models on a synthetic dataset and one real-world dataset while remaining competitive on others.</li>
<li><strong>摘要：</strong>我们提出了一种基于多元时间序列数据的物理信息扩散模型的无监督异常检测方法。在过去的几年中，扩散模型在时间序列域中的预测，归因，产生和异常检测中证明了其有效性。在本文中，我们提出了一种新的方法，用于在扩散模型训练中使用加权物理信息损失来学习多元时间序列数据的物理时间分布。使用静态体重计划构建加权物理信息的损失。这种方法使扩散模型能够准确近似潜在的数据分布，从而影响无监督的异常检测性能。我们对合成和现实世界数据集的实验表明，物理知识的训练可改善异常检测中的F1分数。它产生了更好的数据多样性和对数。此外，我们的模型超过了基线方法，它超过了物理知识的工作，并且在合成数据集和一个现实世界数据集上纯粹是数据驱动的扩散模型，同时又保持了对他人的竞争力。</li>
</ul>

<h3>Title: Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction</h3>
<ul>
<li><strong>Authors: </strong>Shilei Wang, Gong Cheng, Pujian Lai, Dong Gao, Junwei Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11531">https://arxiv.org/abs/2508.11531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11531">https://arxiv.org/pdf/2508.11531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11531]] Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction(https://arxiv.org/abs/2508.11531)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Efficient trackers achieve faster runtime by reducing computational complexity and model parameters. However, this efficiency often compromises the expense of weakened feature representation capacity, thus limiting their ability to accurately capture target states using single-layer features. To overcome this limitation, we propose Multi-State Tracker (MST), which utilizes highly lightweight state-specific enhancement (SSE) to perform specialized enhancement on multi-state features produced by multi-state generation (MSG) and aggregates them in an interactive and adaptive manner using cross-state interaction (CSI). This design greatly enhances feature representation while incurring minimal computational overhead, leading to improved tracking robustness in complex environments. Specifically, the MSG generates multiple state representations at multiple stages during feature extraction, while SSE refines them to highlight target-specific features. The CSI module facilitates information exchange between these states and ensures the integration of complementary features. Notably, the introduced SSE and CSI modules adopt a highly lightweight hidden state adaptation-based state space duality (HSA-SSD) design, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters. Experimental results demonstrate that MST outperforms all previous efficient trackers across multiple datasets, significantly improving tracking accuracy and robustness. In particular, it shows excellent runtime performance, with an AO score improvement of 4.5\% over the previous SOTA efficient tracker HCAT on the GOT-10K dataset. The code is available at this https URL.</li>
<li><strong>摘要：</strong>有效的跟踪器通过降低计算复杂性和模型参数来实现更快的运行时。但是，这种效率通常会损害弱特征表示能力的费用，从而限制了其使用单层特征准确捕获目标状态的能力。为了克服这一限制，我们提出了多州跟踪器（MST），该跟踪器利用高度轻巧的州特异性增强（SSE）对多状态生成（MSG）产生的多状态特征进行专门增强，并使用跨州相互作用（CSI）以交互式和适应性方式聚集它们。这种设计大大增强了特征表示，同时产生了最小的计算开销，从而改善了在复杂环境中的跟踪鲁棒性。具体而言，MSG在功能提取过程中在多个阶段生成多个状态表示，而SSE则完善它们以突出特定于目标的特征。 CSI模块促进了这些状态之间的信息交换，并确保互补特征的整合。值得注意的是，引入的SSE和CSI模块采用了高度轻巧的基于状态适应状态的状态空间双重性（HSA-SSD）设计，在计算中仅产生0.1 GFLOPS，参数为0.66 m。实验结果表明，MST的表现优于多个数据集的所有有效跟踪器，从而显着提高了跟踪准确性和鲁棒性。特别是，它显示出出色的运行时性能，与以前的SOTA有效跟踪器HCAT相比，AO得分提高了4.5％。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Zuo Zuo, Jiahao Dong, Yanyun Qu, Zongze Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11550">https://arxiv.org/abs/2508.11550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11550">https://arxiv.org/pdf/2508.11550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11550]] Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model(https://arxiv.org/abs/2508.11550)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Industrial anomaly detection (AD) plays a significant role in manufacturing where a long-standing challenge is data scarcity. A growing body of works have emerged to address insufficient anomaly data via anomaly generation. However, these anomaly generation methods suffer from lack of fidelity or need to be trained with extra data. To this end, we propose a training-free anomaly generation framework dubbed AAG, which is based on Stable Diffusion (SD)'s strong generation ability for effective anomaly image generation. Given a normal image, mask and a simple text prompt, AAG can generate realistic and natural anomalies in the specific regions and simultaneously keep contents in other regions unchanged. In particular, we propose Cross-Attention Enhancement (CAE) to re-engineer the cross-attention mechanism within Stable Diffusion based on the given mask. CAE increases the similarity between visual tokens in specific regions and text embeddings, which guides these generated visual tokens in accordance with the text description. Besides, generated anomalies need to be more natural and plausible with object in given image. We propose Self-Attention Enhancement (SAE) which improves similarity between each normal visual token and anomaly visual tokens. SAE ensures that generated anomalies are coherent with original pattern. Extensive experiments on MVTec AD and VisA datasets demonstrate effectiveness of AAG in anomaly generation and its utility. Furthermore, anomaly images generated by AAG can bolster performance of various downstream anomaly inspection tasks.</li>
<li><strong>摘要：</strong>工业异常检测（AD）在制造中起着重要的作用，而长期存在的挑战是数据稀缺。越来越多的作品已经出现，可以通过异常产生解决异常数据不足。但是，这些异常生成方法缺乏忠诚或需要接受额外数据的培训。为此，我们提出了一个称为AAG的无训练异常生成框架，该框架基于稳定扩散（SD）的强大生成能力，用于有效的异常图像产生。鉴于正常的图像，掩码和简单的文本提示，AAG可以在特定区域产生逼真而自然的异常，并同时将内容保留在其他不变的区域中。特别是，我们提出了交叉注意增强（CAE），以根据给定的掩码重新设计稳定扩散内的交叉注意机制。 CAE增加了特定区域和文本嵌入中的视觉令牌之间的相似性，该图形根据文本描述指导这些生成的视觉令牌。此外，在给定图像中，生成的异常需要与对象更自然和合理。我们提出了自我注意力增强（SAE），以提高每个正常视觉令牌和异常视觉令牌之间的相似性。 SAE确保生成异常与原始图案相干。 MVTEC AD和VISA数据集的广泛实验表明AAG在异常产生及其效用中的有效性。此外，AAG产生的异常图像可以增强各种下游异常检查任务的性能。</li>
</ul>

<h3>Title: Thyme: Think Beyond Images</h3>
<ul>
<li><strong>Authors: </strong>Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, Haonan Fan, Kaibing Chen, Jiankang Chen, Haojie Ding, Kaiyu Tang, Zhang Zhang, Liang Wang, Fan Yang, Tingting Gao, Guorui Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.11630">https://arxiv.org/abs/2508.11630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.11630">https://arxiv.org/pdf/2508.11630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.11630]] Thyme: Think Beyond Images(https://arxiv.org/abs/2508.11630)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Following OpenAI's introduction of the ``thinking with images'' concept, recent efforts have explored stimulating the use of visual information in the reasoning process to enhance model performance in perception and reasoning tasks. However, to the best of our knowledge, no open-source work currently offers a feature set as rich as proprietary models (O3), which can perform diverse image manipulations and simultaneously enhance logical reasoning capabilities through code. In this paper, we make a preliminary attempt in this direction by introducing Thyme (Think Beyond Images), a novel paradigm for enabling MLLMs to transcend existing ``think with images'' approaches by autonomously generating and executing diverse image processing and computational operations via executable code. This approach not only facilitates a rich, on-the-fly set of image manipulations (e.g., cropping, rotation, contrast enhancement) but also allows for mathematical computations, all while maintaining high autonomy in deciding when and how to apply these operations. We activate this capability through a two-stage training strategy: an initial SFT on a curated dataset of 500K samples to teach code generation, followed by a RL phase to refine decision-making. For the RL stage, we manually collect and design high-resolution question-answer pairs to increase the learning difficulty, and we propose GRPO-ATS (Group Relative Policy Optimization with Adaptive Temperature Sampling), an algorithm that applies distinct temperatures to text and code generation to balance reasoning exploration with code execution precision. We conduct extensive experimental analysis and ablation studies. Comprehensive evaluations on nearly 20 benchmarks show that Thyme yields significant and consistent performance gains, particularly in challenging high-resolution perception and complex reasoning tasks.</li>
<li><strong>摘要：</strong>在Openai介绍了``用图像的思考''概念的介绍之后，最近的努力探索了在推理过程中使用视觉信息的使用，以增强感知和推理任务中的模型性能。但是，据我们所知，目前尚无开源工作提供的功能集，如专有模型（O3），可以执行多种图像操作，并同时通过代码增强逻辑推理能力。在本文中，我们通过引入百里香（思考超出图像）来朝这个方向进行初步尝试，这是一种新颖的范式，它是使MLLM能够通过自动生成和执行可执行的代码来自主产生和执行多样化的图像处理和计算操作来超越现有的``图像''方法。这种方法不仅有助于一组丰富的图像操作（例如，裁剪，旋转，对比度增强），而且还允许进行数学计算，同时在决定何时以及如何应用这些操作方面保持了高度自主权。我们通过两阶段的培训策略来激活此功能：在500K样品的策划数据集上的初始SFT来教授代码生成，然后进行RL阶段以完善决策。对于RL阶段，我们手动收集和设计高分辨率的问题解答以增加学习难度，并提出了GRPO-ATS（组相对策略优化，使用自适应温度采样），该算法将不同的温度应用于文本和代码生成，以平衡推理探索的代码探索，并具有代码执行精确。我们进行广泛的实验分析和消融研究。对近20个基准的全面评估表明，百里香可以带来显着且一致的性能提高，尤其是在挑战高分辨率感知和复杂的推理任务方面。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
