<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-15</h1>
<h3>Title: Bias Detection and Rotation-Robustness Mitigation in Vision-Language Models and Generative Image Models</h3>
<ul>
<li><strong>Authors: </strong>Tarannum Mithila</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08860">https://arxiv.org/abs/2601.08860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08860">https://arxiv.org/pdf/2601.08860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08860]] Bias Detection and Rotation-Robustness Mitigation in Vision-Language Models and Generative Image Models(https://arxiv.org/abs/2601.08860)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) and generative image models have achieved remarkable performance across multimodal tasks, yet their robustness and fairness under input transformations remain insufficiently explored. This work investigates bias propagation and robustness degradation in state-of-the-art vision-language and generative models, with a particular focus on image rotation and distributional shifts. We analyze how rotation-induced perturbations affect model predictions, confidence calibration, and demographic bias patterns. To address these issues, we propose rotation-robust mitigation strategies that combine data augmentation, representation alignment, and model-level regularization. Experimental results across multiple datasets demonstrate that the proposed methods significantly improve robustness while reducing bias amplification without sacrificing overall performance. This study highlights critical limitations of current multimodal systems and provides practical mitigation techniques for building more reliable and fair AI models.</li>
<li><strong>摘要：</strong>视觉语言模型（VLM）和生成图像模型在多模态任务中取得了显着的性能，但它们在输入转换下的鲁棒性和公平性尚未得到充分探索。这项工作研究了最先进的视觉语言和生成模型中的偏差传播和鲁棒性退化，特别关注图像旋转和分布变化。我们分析旋转引起的扰动如何影响模型预测、置信度校准和人口统计偏差模式。为了解决这些问题，我们提出了结合数据增强、表示对齐和模型级正则化的旋转稳健缓解策略。多个数据集的实验结果表明，所提出的方法显着提高了鲁棒性，同时减少了偏差放大，而不牺牲整体性能。这项研究强调了当前多模式系统的关键局限性，并为构建更可靠和公平的人工智能模型提供了实用的缓解技术。</li>
</ul>

<h3>Title: R$^2$BD: A Reconstruction-Based Method for Generalizable and Efficient Detection of Fake Images</h3>
<ul>
<li><strong>Authors: </strong>Qingyu Liu, Zhongjie Ba, Jianmin Guo, Qiu Wang, Zhibo Wang, Jie Shi, Kui Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08867">https://arxiv.org/abs/2601.08867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08867">https://arxiv.org/pdf/2601.08867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08867]] R$^2$BD: A Reconstruction-Based Method for Generalizable and Efficient Detection of Fake Images(https://arxiv.org/abs/2601.08867)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recently, reconstruction-based methods have gained attention for AIGC image detection. These methods leverage pre-trained diffusion models to reconstruct inputs and measure residuals for distinguishing real from fake images. Their key advantage lies in reducing reliance on dataset-specific artifacts and improving generalization under distribution shifts. However, they are limited by significant inefficiency due to multi-step inversion and reconstruction, and their reliance on diffusion backbones further limits generalization to other generative paradigms such as GANs. In this paper, we propose a novel fake image detection framework, called R$^2$BD, built upon two key designs: (1) G-LDM, a unified reconstruction model that simulates the generation behaviors of VAEs, GANs, and diffusion models, thereby broadening the detection scope beyond prior diffusion-only approaches; and (2) a residual bias calculation module that distinguishes real and fake images in a single inference step, which is a significant efficiency improvement over existing methods that typically require 20$+$ steps. Extensive experiments on the benchmark from 10 public datasets demonstrate that R$^2$BD is over 22$\times$ faster than existing reconstruction-based methods while achieving superior detection accuracy. In cross-dataset evaluations, it outperforms state-of-the-art methods by an average of 13.87\%, showing strong efficiency and generalization across diverse generative methods. The code and dataset used for evaluation are available at this https URL.</li>
<li><strong>摘要：</strong>最近，基于重建的方法在 AIGC 图像检测中引起了人们的关注。这些方法利用预先训练的扩散模型来重建输入并测量残差以区分真假图像。它们的主要优势在于减少对数据集特定工件的依赖并提高分布变化下的泛化能力。然而，由于多步反转和重建，它们受到显着低效的限制，并且它们对扩散主干的依赖进一步限制了对其他生成范式（例如 GAN）的泛化。在本文中，我们提出了一种新颖的假图像检测框架，称为 R$^2$BD，它基于两个关键设计：（1）G-LDM，一个统一的重建模型，模拟 VAE、GAN 和扩散模型的生成行为，从而扩大了检测范围，超越了之前的仅扩散方法； (2) 残余偏差计算模块，可在单个推理步骤中区分真假图像，这比通常需要 20 美元以上步骤的现有方法显着提高了效率。对 10 个公共数据集的基准进行的广泛实验表明，R$^2$BD 比现有基于重建的方法快超过 22$\times$，同时实现了卓越的检测精度。在跨数据集评估中，它平均优于最先进的方法 13.87%，在不同的生成方法中显示出强大的效率和泛化能力。用于评估的代码和数据集可从此 https URL 获取。</li>
</ul>

<h3>Title: TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts</h3>
<ul>
<li><strong>Authors: </strong>Yu Xu, Hongbin Yan, Juan Cao, Yiji Cheng, Tiankai Hang, Runze He, Zijin Yin, Shiyi Zhang, Yuxin Zhang, Jintao Li, Chunyu Wang, Qinglin Lu, Tong-Yee Lee, Fan Tang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08881">https://arxiv.org/abs/2601.08881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08881">https://arxiv.org/pdf/2601.08881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08881]] TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts(https://arxiv.org/abs/2601.08881)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Unified image generation and editing models suffer from severe task interference in dense diffusion transformers architectures, where a shared parameter space must compromise between conflicting objectives (e.g., local editing v.s. subject-driven generation). While the sparse Mixture-of-Experts (MoE) paradigm is a promising solution, its gating networks remain task-agnostic, operating based on local features, unaware of global task intent. This task-agnostic nature prevents meaningful specialization and fails to resolve the underlying task interference. In this paper, we propose a novel framework to inject semantic intent into MoE routing. We introduce a Hierarchical Task Semantic Annotation scheme to create structured task descriptors (e.g., scope, type, preservation). We then design Predictive Alignment Regularization to align internal routing decisions with the task's high-level semantics. This regularization evolves the gating network from a task-agnostic executor to a dispatch center. Our model effectively mitigates task interference, outperforming dense baselines in fidelity and quality, and our analysis shows that experts naturally develop clear and semantically correlated specializations.</li>
<li><strong>摘要：</strong>统一图像生成和编辑模型在密集扩散变压器架构中受到严重的任务干扰，其中共享参数空间必须在相互冲突的目标之间进行妥协（例如，本地编辑与主题驱动生成）。虽然稀疏专家混合（MoE）范式是一个有前途的解决方案，但其门控网络仍然与任务无关，基于局部特征进行操作，不知道全局任务意图。这种与任务无关的性质阻碍了有意义的专业化，并且无法解决潜在的任务干扰。在本文中，我们提出了一种新颖的框架，将语义意图注入 MoE 路由中。我们引入了分层任务语义注释方案来创建结构化任务描述符（例如范围、类型、保存）。然后，我们设计预测对齐正则化，以使内部路由决策与任务的高级语义保持一致。这种正则化将门控网络从任务无关的执行器发展为调度中心。我们的模型有效地减轻了任务干扰，在保真度和质量方面优于密集基线，并且我们的分析表明专家自然会发展出清晰且语义相关的专业知识。</li>
</ul>

<h3>Title: Spectral Generative Flow Models: A Physics-Inspired Replacement for Vectorized Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Andrew Kiruluta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08893">https://arxiv.org/abs/2601.08893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08893">https://arxiv.org/pdf/2601.08893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08893]] Spectral Generative Flow Models: A Physics-Inspired Replacement for Vectorized Large Language Models(https://arxiv.org/abs/2601.08893)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce Spectral Generative Flow Models (SGFMs), a physics-inspired alternative to transformer-based large language models. Instead of representing text or video as sequences of discrete tokens processed by attention, SGFMs treat generation as the evolution of a continuous field governed by constrained stochastic dynamics in a multiscale wavelet basis. This formulation replaces global attention with local operators, spectral projections, and Navier--Stokes-like transport, yielding a generative mechanism grounded in continuity, geometry, and physical structure. Our framework provides three key innovations: (i) a field-theoretic ontology in which text and video are unified as trajectories of a stochastic partial differential equation; (ii) a wavelet-domain representation that induces sparsity, scale separation, and computational efficiency; and (iii) a constrained stochastic flow that enforces stability, coherence, and uncertainty propagation. Together, these components define a generative architecture that departs fundamentally from autoregressive modeling and diffusion-based approaches. SGFMs offer a principled path toward long-range coherence, multimodal generality, and physically structured inductive bias in next-generation generative models.</li>
<li><strong>摘要：</strong>我们引入了谱生成流模型（SGFM），这是一种受物理学启发的基于 Transformer 的大型语言模型的替代方案。 SGFM 不是将文本或视频表示为经过注意力处理的离散标记序列，而是将生成视为由多尺度小波基中的约束随机动力学控制的连续场的演化。这种表述用局部算子、光谱投影和类纳维-斯托克斯输运取代了全局注意力，产生了一种基于连续性、几何和物理结构的生成机制。我们的框架提供了三个关键创新：（i）场论本体，其中文本和视频统一为随机偏微分方程的轨迹； (ii) 引起稀疏性、尺度分离和计算效率的小波域表示； (iii) 约束随机流，增强稳定性、连贯性和不确定性传播。这些组件共同定义了一个生成架构，该架构从根本上不同于自回归建模和基于扩散的方法。 SGFM 为下一代生成模型中的远程一致性、多模态通用性和物理结构归纳偏差提供了一条原则性路径。</li>
</ul>

<h3>Title: Breaking the Bottlenecks: Scalable Diffusion Models for 3D Molecular Generation</h3>
<ul>
<li><strong>Authors: </strong>Adrita Das, Peiran Jiang, Dantong Zhu, Barnabas Poczos, Jose Lugo-Martinez</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08963">https://arxiv.org/abs/2601.08963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08963">https://arxiv.org/pdf/2601.08963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08963]] Breaking the Bottlenecks: Scalable Diffusion Models for 3D Molecular Generation(https://arxiv.org/abs/2601.08963)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as a powerful class of generative models for molecular design, capable of capturing complex structural distributions and achieving high fidelity in 3D molecule generation. However, their widespread use remains constrained by long sampling trajectories, stochastic variance in the reverse process, and limited structural awareness in denoising dynamics. The Directly Denoising Diffusion Model (DDDM) mitigates these inefficiencies by replacing stochastic reverse MCMC updates with deterministic denoising step, substantially reducing inference time. Yet, the theoretical underpinnings of such deterministic updates have remained opaque. In this work, we provide a principled reinterpretation of DDDM through the lens of the Reverse Transition Kernel (RTK) framework by Huang et al. 2024, unifying deterministic and stochastic diffusion under a shared probabilistic formalism. By expressing the DDDM reverse process as an approximate kernel operator, we show that the direct denoising process implicitly optimizes a structured transport map between noisy and clean samples. This perspective elucidates why deterministic denoising achieves efficient inference. Beyond theoretical clarity, this reframing resolves several long-standing bottlenecks in molecular diffusion. The RTK view ensures numerical stability by enforcing well-conditioned reverse kernels, improves sample consistency by eliminating stochastic variance, and enables scalable and symmetry-preserving denoisers that respect SE(3) equivariance. Empirically, we demonstrate that RTK-guided deterministic denoising achieves faster convergence and higher structural fidelity than stochastic diffusion models, while preserving chemical validity across GEOM-DRUGS dataset. Code, models, and datasets are publicly available in our project repository.</li>
<li><strong>摘要：</strong>扩散模型已成为一类强大的分子设计生成模型，能够捕获复杂的结构分布并在 3D 分子生成中实现高保真度。然而，它们的广泛使用仍然受到长采样轨迹、反向过程中的随机方差以及去噪动力学中有限的结构意识的限制。直接去噪扩散模型 (DDDM) 通过用确定性去噪步骤替换随机反向 MCMC 更新来缓解这些低效率问题，从而显着缩短推理时间。然而，这种确定性更新的理论基础仍然不透明。在这项工作中，我们通过 Huang 等人的反向转换内核 (RTK) 框架的视角对 DDDM 进行了原则性的重新解释。 2024 年，在共享的概率形式主义下统一确定性和随机扩散。通过将 DDDM 逆向过程表示为近似核算子，我们表明直接去噪过程隐式优化了噪声样本和干净样本之间的结构化传输图。这个观点阐明了为什么确定性去噪能够实现有效的推理。除了理论上的清晰度之外，这种重新构建还解决了分子扩散中几个长期存在的瓶颈。 RTK 视图通过强制执行条件良好的反向核来确保数值稳定性，通过消除随机方差来提高样本一致性，并实现尊重 SE(3) 等方差的可扩展且保持对称的降噪器。根据经验，我们证明 RTK 引导的确定性去噪比随机扩散模型实现更快的收敛和更高的结构保真度，同时在 GEOM-DRUGS 数据集中保持化学有效性。代码、模型和数据集在我们的项目存储库中公开可用。</li>
</ul>

<h3>Title: Optimising for Energy Efficiency and Performance in Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Emile Dos Santos Ferreira, Neil D. Lawrence, Andrei Paleyes</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08991">https://arxiv.org/abs/2601.08991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08991">https://arxiv.org/pdf/2601.08991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08991]] Optimising for Energy Efficiency and Performance in Machine Learning(https://arxiv.org/abs/2601.08991)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The ubiquity of machine learning (ML) and the demand for ever-larger models bring an increase in energy consumption and environmental impact. However, little is known about the energy scaling laws in ML, and existing research focuses on training cost -- ignoring the larger cost of inference. Furthermore, tools for measuring the energy consumption of ML do not provide actionable feedback. To address these gaps, we developed Energy Consumption Optimiser (ECOpt): a hyperparameter tuner that optimises for energy efficiency and model performance. ECOpt quantifies the trade-off between these metrics as an interpretable Pareto frontier. This enables ML practitioners to make informed decisions about energy cost and environmental impact, while maximising the benefit of their models and complying with new regulations. Using ECOpt, we show that parameter and floating-point operation counts can be unreliable proxies for energy consumption, and observe that the energy efficiency of Transformer models for text generation is relatively consistent across hardware. These findings motivate measuring and publishing the energy metrics of ML models. We further show that ECOpt can have a net positive environmental impact and use it to uncover seven models for CIFAR-10 that improve upon the state of the art, when considering accuracy and energy efficiency together.</li>
<li><strong>摘要：</strong>机器学习 (ML) 的普及和对更大模型的需求带来了能源消耗和环境影响的增加。然而，人们对机器学习中的能量缩放定律知之甚少，现有的研究主要集中在训练成本上——忽略了更大的推理成本。此外，用于测量机器学习能耗的工具不提供可操作的反馈。为了解决这些差距，我们开发了能耗优化器 (ECOpt)：一种可优化能源效率和模型性能的超参数调节器。 ECOpt 将这些指标之间的权衡量化为可解释的帕累托前沿。这使得机器学习从业者能够就能源成本和环境影响做出明智的决策，同时最大限度地提高模型的效益并遵守新法规。使用 ECOpt，我们表明参数和浮点运算计数可能是能源消耗的不可靠代理，并观察到用于文本生成的 Transformer 模型的能源效率在各个硬件上相对一致。这些发现激励测量和发布机器学习模型的能量指标。我们进一步表明，ECOpt 可以对环境产生净积极影响，并用它来发现 CIFAR-10 的七个模型，在同时考虑准确性和能源效率时，这些模型改进了现有技术。</li>
</ul>

<h3>Title: Physics-Guided Counterfactual Explanations for Large-Scale Multivariate Time Series: Application in Scalable and Interpretable SEP Event Prediction</h3>
<ul>
<li><strong>Authors: </strong>Pranjal Patil, Anli Ji, Berkay Aydin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.08999">https://arxiv.org/abs/2601.08999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.08999">https://arxiv.org/pdf/2601.08999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.08999]] Physics-Guided Counterfactual Explanations for Large-Scale Multivariate Time Series: Application in Scalable and Interpretable SEP Event Prediction(https://arxiv.org/abs/2601.08999)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accurate prediction of solar energetic particle events is vital for safeguarding satellites, astronauts, and space-based infrastructure. Modern space weather monitoring generates massive volumes of high-frequency, multivariate time series (MVTS) data from sources such as the Geostationary perational Environmental Satellites (GOES). Machine learning (ML) models trained on this data show strong predictive power, but most existing methods overlook domain-specific feasibility constraints. Counterfactual explanations have emerged as a key tool for improving model interpretability, yet existing approaches rarely enforce physical plausibility. This work introduces a Physics-Guided Counterfactual Explanation framework, a novel method for generating counterfactual explanations in time series classification tasks that remain consistent with underlying physical principles. Applied to solar energetic particles (SEP) forecasting, this framework achieves over 80% reduction in Dynamic Time Warping (DTW) distance increasing the proximity, produces counterfactual explanations with higher sparsity, and reduces runtime by nearly 50% compared to state-of-the-art baselines such as DiCE. Beyond numerical improvements, this framework ensures that generated counterfactual explanations are physically plausible and actionable in scientific domains. In summary, the framework generates counterfactual explanations that are both valid and physically consistent, while laying the foundation for scalable counterfactual generation in big data environments.</li>
<li><strong>摘要：</strong>准确预测太阳高能粒子事件对于保护卫星、宇航员和天基基础设施至关重要。现代空间天气监测通过对地静止环境卫星 (GOES) 等来源生成大量高频、多元时间序列 (MVTS) 数据。根据这些数据训练的机器学习 (ML) 模型显示出强大的预测能力，但大多数现有方法忽略了特定领域的可行性限制。反事实解释已成为提高模型可解释性的关键工具，但现有方法很少强化物理合理性。这项工作引入了物理引导的反事实解释框架，这是一种在时间序列分类任务中生成反事实解释的新方法，该方法与基本物理原理保持一致。应用于太阳高能粒子 (SEP) 预测时，该框架实现了动态时间扭曲 (DTW) 距离减少 80% 以上，增加了接近度，产生了具有更高稀疏性的反事实解释，并且与 DiCE 等最先进的基线相比，运行时间减少了近 50%。除了数值上的改进之外，该框架还确保生成的反事实解释在科学领域中在物理上是合理且可操作的。总之，该框架生成既有效又物理一致的反事实解释，同时为大数据环境中可扩展的反事实生成奠定了基础。</li>
</ul>

<h3>Title: SRT: Accelerating Reinforcement Learning via Speculative Rollout with Tree-Structured Cache</h3>
<ul>
<li><strong>Authors: </strong>Chi-Chih Chang, Siqi Zhu, Zhichen Zeng, Haibin Lin, Jiaxuan You, Mohamed S. Abdelfattah, Ziheng Jiang, Xuehai Qian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09083">https://arxiv.org/abs/2601.09083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09083">https://arxiv.org/pdf/2601.09083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09083]] SRT: Accelerating Reinforcement Learning via Speculative Rollout with Tree-Structured Cache(https://arxiv.org/abs/2601.09083)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present Speculative Rollout with Tree-Structured Cache (SRT), a simple, model-free approach to accelerate on-policy reinforcement learning (RL) for language models without sacrificing distributional correctness. SRT exploits the empirical similarity of rollouts for the same prompt across training steps by storing previously generated continuations in a per-prompt tree-structured cache. During generation, the current policy uses this tree as the draft model for performing speculative decoding. To keep the cache fresh and improve draft model quality, SRT updates trees online from ongoing rollouts and proactively performs run-ahead generation during idle GPU bubbles. Integrated into standard RL pipelines (\textit{e.g.}, PPO, GRPO and DAPO) and multi-turn settings, SRT consistently reduces generation and step latency and lowers per-token inference cost, achieving up to 2.08x wall-clock time speedup during rollout.</li>
<li><strong>摘要：</strong>我们提出了带有树结构缓存（SRT）的推测性推出，这是一种简单的、无模型的方法，可以在不牺牲分布正确性的情况下加速语言模型的策略强化学习（RL）。 SRT 通过将先前生成的延续存储在每个提示的树形结构缓存中，利用跨训练步骤相同提示的推出的经验相似性。在生成过程中，当前策略使用该树作为执行推测解码的草稿模型。为了保持缓存新鲜并提高草稿模型质量，SRT 通过持续推出在线更新树，并在空闲 GPU 气泡期间主动执行预运行生成。 SRT 集成到标准 RL 管道（\textit{e.g.}、PPO、GRPO 和 DAPO）和多轮设置中，持续减少生成和步骤延迟，并降低每个令牌的推理成本，在推出期间实现高达 2.08 倍的挂钟时间加速。</li>
</ul>

<h3>Title: Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Shaotian Yan, Kaiyuan Liu, Chen Shen, Bing Wang, Sinan Fan, Jun Zhang, Yue Wu, Zheng Wang, Jieping Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09088">https://arxiv.org/abs/2601.09088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09088">https://arxiv.org/pdf/2601.09088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09088]] Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning(https://arxiv.org/abs/2601.09088)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.</li>
<li><strong>摘要：</strong>在本报告中，我们介绍了 DASD-4B-Thinking，这是一种轻量级但功能强大、完全开源的推理模型。它在数学、科学推理和代码生成方面具有挑战性的基准上，在规模相当的开源模型中实现了 SOTA 性能，甚至超越了几个更大的模型。我们首先批判性地重新审视社区中广泛采用的蒸馏范式：针对教师生成的响应的 SFT，也称为序列级蒸馏。尽管最近一系列遵循该方案的工作已经表现出显着的效率和强大的实证表现，但它们主要基于 SFT 的视角。因此，这些方法主要侧重于设计 SFT 数据过滤的启发式规则，而在很大程度上忽略了蒸馏本身的核心原则——使学生模型能够学习教师的完整输出分布，从而继承其泛化能力。具体来说，我们确定了当前实践中的三个关键局限性：i）教师序列级别分布的代表性不足； ii) 教师的产出分布与学生的学习能力之间的错位； iii) 教师强制训练与自回归推理产生的暴露偏差。综上所述，这些缺陷反映出整个蒸馏过程中系统性地缺乏明确的师生互动，使得蒸馏的本质没有得到充分利用。为了解决这些问题，我们提出了几种方法创新，这些创新共同形成了增强的序列级蒸馏训练管道。值得注意的是，DASD-4B-Thinking 仅使用 448K 训练样本就获得了有竞争力的结果，比大多数现有开源项目所使用的样本要少一个数量级。为了支持社区研究，我们公开发布我们的模型和训练数据集。</li>
</ul>

<h3>Title: Hidden States as Early Signals: Step-level Trace Evaluation and Pruning for Efficient Test-Time Scaling</h3>
<ul>
<li><strong>Authors: </strong>Zhixiang Liang, Beichen Huang, Zheng Wang, Minjia Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09093">https://arxiv.org/abs/2601.09093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09093">https://arxiv.org/pdf/2601.09093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09093]] Hidden States as Early Signals: Step-level Trace Evaluation and Pruning for Efficient Test-Time Scaling(https://arxiv.org/abs/2601.09093)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can enhance reasoning capabilities through test-time scaling by generating multiple traces. However, the combination of lengthy reasoning traces with multiple sampling introduces substantial computation and high end-to-end latency. Prior work on accelerating this process has relied on similarity-based or confidence-based pruning, but these signals do not reliably indicate trace quality. To address these limitations, we propose STEP: Step-level Trace Evaluation and Pruning, a novel pruning framework that evaluates reasoning steps using hidden states and dynamically prunes unpromising traces during generation. We train a lightweight step scorer to estimate trace quality, and design a GPU memory-aware pruning strategy that triggers pruning as the GPU memory is saturated by KV cache to reduce end-to-end latency. Experiments across challenging reasoning benchmarks demonstrate that STEP reduces end-to-end inference latency by 45%-70% on average compared to self-consistency while also improving reasoning accuracy. Our code is released at: this https URL</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 可以通过生成多个跟踪来扩展测试时间，从而增强推理能力。然而，冗长的推理轨迹与多次采样的结合会带来大量的计算和较高的端到端延迟。之前加速这一过程的工作依赖于基于相似性或基于置信度的修剪，但这些信号并不能可靠地指示跟踪质量。为了解决这些限制，我们提出了STEP：步骤级跟踪评估和修剪，这是一种新颖的修剪框架，它使用隐藏状态评估推理步骤，并在生成过程中动态修剪无希望的跟踪。我们训练一个轻量级的步进评分器来估计跟踪质量，并设计一种 GPU 内存感知修剪策略，当 GPU 内存被 KV 缓存饱和时触发修剪，以减少端到端延迟。跨具有挑战性的推理基准的实验表明，与自我一致性相比，STEP 平均减少了 45%-70% 的端到端推理延迟，同时还提高了推理准确性。我们的代码发布于：此 https URL</li>
</ul>

<h3>Title: Enhancing Imbalanced Electrocardiogram Classification: A Novel Approach Integrating Data Augmentation through Wavelet Transform and Interclass Fusion</h3>
<ul>
<li><strong>Authors: </strong>Haijian Shao, Wei Liu, Xing Deng, Daze Lu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09103">https://arxiv.org/abs/2601.09103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09103">https://arxiv.org/pdf/2601.09103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09103]] Enhancing Imbalanced Electrocardiogram Classification: A Novel Approach Integrating Data Augmentation through Wavelet Transform and Interclass Fusion(https://arxiv.org/abs/2601.09103)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Imbalanced electrocardiogram (ECG) data hampers the efficacy and resilience of algorithms in the automated processing and interpretation of cardiovascular diagnostic information, which in turn impedes deep learning-based ECG classification. Notably, certain cardiac conditions that are infrequently encountered are disproportionately underrepresented in these datasets. Although algorithmic generation and oversampling of specific ECG signal types can mitigate class skew, there is a lack of consensus regarding the effectiveness of such techniques in ECG classification. Furthermore, the methodologies and scenarios of ECG acquisition introduce noise, further complicating the processing of ECG data. This paper presents a significantly enhanced ECG classifier that simultaneously addresses both class imbalance and noise-related challenges in ECG analysis, as observed in the CPSC 2018 dataset. Specifically, we propose the application of feature fusion based on the wavelet transform, with a focus on wavelet transform-based interclass fusion, to generate the training feature library and the test set feature library. Subsequently, the original training and test data are amalgamated with their respective feature databases, resulting in more balanced training and test datasets. Employing this approach, our ECG model achieves recognition accuracies of up to 99%, 98%, 97%, 98%, 96%, 92%, and 93% for Normal, AF, I-AVB, LBBB, RBBB, PAC, PVC, STD, and STE, respectively. Furthermore, the average recognition accuracy for these categories ranges between 92\% and 98\%. Notably, our proposed data fusion methodology surpasses any known algorithms in terms of ECG classification accuracy in the CPSC 2018 dataset.</li>
<li><strong>摘要：</strong>不平衡的心电图 (ECG) 数据会妨碍算法在心血管诊断信息自动处理和解释方面的有效性和弹性，进而阻碍基于深度学习的心电图分类。值得注意的是，某些不常遇到的心脏疾病在这些数据集中的代表性不足。尽管特定心电图信号类型的算法生成和过采样可以减轻类别偏差，但对于此类技术在心电图分类中的有效性缺乏共识。此外，心电图采集的方法和场景引入了噪声，使心电图数据的处理进一步复杂化。本文提出了一种显着增强的心电图分类器，可同时解决心电图分析中的类别不平衡和噪声相关挑战，正如 CPSC 2018 数据集中观察到的那样。具体来说，我们提出应用基于小波变换的特征融合，重点是基于小波变换的类间融合，生成训练特征库和测试集特征库。随后，原始训练和测试数据与各自的特征数据库合并，从而产生更加平衡的训练和测试数据集。采用这种方法，我们的心电图模型对正常、AF、I-AVB、LBBB、RBBB、PAC、PVC、STD 和 STE 的识别准确率分别高达 99%、98%、97%、98%、96%、92% 和 93%。此外，这些类别的平均识别准确率在 92% 到 98% 之间。值得注意的是，我们提出的数据融合方法在 CPSC 2018 数据集中的心电图分类准确性方面超越了任何已知算法。</li>
</ul>

<h3>Title: LP-LLM: End-to-End Real-World Degraded License Plate Text Recognition via Large Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Haoyan Gong, Hongbin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09116">https://arxiv.org/abs/2601.09116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09116">https://arxiv.org/pdf/2601.09116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09116]] LP-LLM: End-to-End Real-World Degraded License Plate Text Recognition via Large Multimodal Models(https://arxiv.org/abs/2601.09116)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation</a></li>
<li><strong>Abstract: </strong>Real-world License Plate Recognition (LPR) faces significant challenges from severe degradations such as motion blur, low resolution, and complex illumination. The prevailing "restoration-then-recognition" two-stage paradigm suffers from a fundamental flaw: the pixel-level optimization objectives of image restoration models are misaligned with the semantic goals of character recognition, leading to artifact interference and error accumulation. While Vision-Language Models (VLMs) have demonstrated powerful general capabilities, they lack explicit structural modeling for license plate character sequences (e.g., fixed length, specific order). To address this, we propose an end-to-end structure-aware multimodal reasoning framework based on Qwen3-VL. The core innovation lies in the Character-Aware Multimodal Reasoning Module (CMRM), which introduces a set of learnable Character Slot Queries. Through a cross-attention mechanism, these queries actively retrieve fine-grained evidence corresponding to character positions from visual features. Subsequently, we inject these character-aware representations back into the visual tokens via residual modulation, enabling the language model to perform autoregressive generation based on explicit structural priors. Furthermore, combined with the LoRA parameter-efficient fine-tuning strategy, the model achieves domain adaptation while retaining the generalization capabilities of the large model. Extensive experiments on both synthetic and real-world severely degraded datasets demonstrate that our method significantly outperforms existing restoration-recognition combinations and general VLMs, validating the superiority of incorporating structured reasoning into large models for low-quality text recognition tasks.</li>
<li><strong>摘要：</strong>现实世界的车牌识别 (LPR) 面临着运动模糊、低分辨率和复杂照明等严重退化带来的重大挑战。流行的“恢复然后识别”两阶段范式存在一个根本缺陷：图像恢复模型的像素级优化目标与字符识别的语义目标不一致，导致伪影干扰和错误累积。虽然视觉语言模型（VLM）已经展示了强大的通用功能，但它们缺乏针对车牌字符序列（例如固定长度、特定顺序）的显式结构建模。为了解决这个问题，我们提出了一种基于 Qwen3-VL 的端到端结构感知多模态推理框架。核心创新在于字符感知多模态推理模块（CMRM），它引入了一组可学习的字符槽查询。通过交叉注意力机制，这些查询主动从视觉特征中检索与字符位置相对应的细粒度证据。随后，我们通过残差调制将这些字符感知表示重新注入到视觉标记中，使语言模型能够基于显式结构先验执行自回归生成。此外，结合LoRA参数高效的微调策略，模型在保留大模型泛化能力的同时实现了领域自适应。对合成数据集和真实世界严重退化数据集的大量实验表明，我们的方法显着优于现有的恢复识别组合和通用 VLM，验证了将结构化推理纳入大型模型以执行低质量文本识别任务的优越性。</li>
</ul>

<h3>Title: SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Chenhao Fu, Han Fang, Xiuzheng Zheng, Wenbo Wei, Yonghua Li, Hao Sun, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09147">https://arxiv.org/abs/2601.09147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09147">https://arxiv.org/pdf/2601.09147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09147]] SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection(https://arxiv.org/abs/2601.09147)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Zero-Shot Anomaly Detection (ZSAD) leverages Vision-Language Models (VLMs) to enable supervision-free industrial inspection. However, existing ZSAD paradigms are constrained by single visual backbones, which struggle to balance global semantic generalization with fine-grained structural discriminability. To bridge this gap, we propose Synergistic Semantic-Visual Prompting (SSVP), that efficiently fuses diverse visual encodings to elevate model's fine-grained perception. Specifically, SSVP introduces the Hierarchical Semantic-Visual Synergy (HSVS) mechanism, which deeply integrates DINOv3's multi-scale structural priors into the CLIP semantic space. Subsequently, the Vision-Conditioned Prompt Generator (VCPG) employs cross-modal attention to guide dynamic prompt generation, enabling linguistic queries to precisely anchor to specific anomaly patterns. Furthermore, to address the discrepancy between global scoring and local evidence, the Visual-Text Anomaly Mapper (VTAM) establishes a dual-gated calibration paradigm. Extensive evaluations on seven industrial benchmarks validate the robustness of our method; SSVP achieves state-of-the-art performance with 93.0\% Image-AUROC and 92.2\% Pixel-AUROC on MVTec-AD, significantly outperforming existing zero-shot approaches.</li>
<li><strong>摘要：</strong>零样本异常检测 (ZSAD) 利用视觉语言模型 (VLM) 来实现无监督的工业检查。然而，现有的 ZSAD 范式受到单一视觉主干的限制，难以平衡全局语义泛化与细粒度结构区分性。为了弥补这一差距，我们提出了协同语义视觉提示（SSVP），它有效地融合了不同的视觉编码，以提升模型的细粒度感知。具体来说，SSVP引入了分层语义视觉协同（HSVS）机制，将DINOv3的多尺度结构先验深度集成到CLIP语义空间中。随后，视觉条件提示生成器（VCPG）采用跨模式注意来指导动态提示生成，使语言查询能够精确锚定到特定的异常模式。此外，为了解决全局评分和本地证据之间的差异，视觉文本异常映射器（VTAM）建立了双门校准范例。对七个工业基准的广泛评估验证了我们方法的稳健性； SSVP 在 MVTec-AD 上实现了最先进的性能，图像 AUROC 为 93.0%，像素 AUROC 为 92.2%，显着优于现有的零样本方法。</li>
</ul>

<h3>Title: KTCF: Actionable Recourse in Knowledge Tracing via Counterfactual Explanations for Education</h3>
<ul>
<li><strong>Authors: </strong>Woojin Kim, Changkwon Lee, Hyeoncheol Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09156">https://arxiv.org/abs/2601.09156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09156">https://arxiv.org/pdf/2601.09156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09156]] KTCF: Actionable Recourse in Knowledge Tracing via Counterfactual Explanations for Education(https://arxiv.org/abs/2601.09156)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Using Artificial Intelligence to improve teaching and learning benefits greater adaptivity and scalability in education. Knowledge Tracing (KT) is recognized for student modeling task due to its superior performance and application potential in education. To this end, we conceptualize and investigate counterfactual explanation as the connection from XAI for KT to education. Counterfactual explanations offer actionable recourse, are inherently causal and local, and easy for educational stakeholders to understand who are often non-experts. We propose KTCF, a counterfactual explanation generation method for KT that accounts for knowledge concept relationships, and a post-processing scheme that converts a counterfactual explanation into a sequence of educational instructions. We experiment on a large-scale educational dataset and show our KTCF method achieves superior and robust performance over existing methods, with improvements ranging from 5.7% to 34% across metrics. Additionally, we provide a qualitative evaluation of our post-processing scheme, demonstrating that the resulting educational instructions help in reducing large study burden. We show that counterfactuals have the potential to advance the responsible and practical use of AI in education. Future works on XAI for KT may benefit from educationally grounded conceptualization and developing stakeholder-centered methods.</li>
<li><strong>摘要：</strong>使用人工智能改善教学有利于提高教育的适应性和可扩展性。知识追踪（KT）因其卓越的性能和在教育中的应用潜力而被认可用于学生建模任务。为此，我们将反事实解释概念化并研究为 KT 的 XAI 与教育的联系。反事实解释提供了可行的资源，本质上是因果性的和局部性的，并且教育利益相关者很容易理解谁通常是非专家。我们提出了 KTCF，一种用于解释知识概念关系的 KT 反事实解释生成方法，以及一种将反事实解释转换为一系列教育指令的后处理方案。我们在大规模教育数据集上进行了实验，结果表明我们的 KTCF 方法比现有方法实现了卓越且稳健的性能，各项指标的改进范围为 5.7% 到 34%。此外，我们对后处理方案进行了定性评估，表明由此产生的教育指导有助于减轻大量的学习负担。我们证明，反事实有可能促进人工智能在教育中负责任和实际的应用。 KT 的 XAI 未来工作可能会受益于基于教育的概念化和开发以利益相关者为中心的方法。</li>
</ul>

<h3>Title: Architecture inside the mirage: evaluating generative image models on architectural style, elements, and typologies</h3>
<ul>
<li><strong>Authors: </strong>Jamie Magrill (1), Leah Gornstein (1), Sandra Seekins (2), Barry Magrill (2) ((1) McGill University, Montreal, Canada, (2) Capilano University, North Vancouver, Canada)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09169">https://arxiv.org/abs/2601.09169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09169">https://arxiv.org/pdf/2601.09169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09169]] Architecture inside the mirage: evaluating generative image models on architectural style, elements, and typologies(https://arxiv.org/abs/2601.09169)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative artificial intelligence (GenAI) text-to-image systems are increasingly used to generate architectural imagery, yet their capacity to reproduce accurate images in a historically rule-bound field remains poorly characterized. We evaluated five widely used GenAI image platforms (Adobe Firefly, DALL-E 3, Google Imagen 3, Microsoft Image Generator, and Midjourney) using 30 architectural prompts spanning styles, typologies, and codified elements. Each prompt-generator pair produced four images (n = 600 images total). Two architectural historians independently scored each image for accuracy against predefined criteria, resolving disagreements by consensus. Set-level performance was summarized as zero to four accurate images per four-image set. Image output from Common prompts was 2.7-fold more accurate than from Rare prompts (p < 0.05). Across platforms, overall accuracy was limited (highest accuracy score 52 percent; lowest 32 percent; mean 42 percent). All-correct (4 out of 4) outcomes were similar across platforms. By contrast, all-incorrect (0 out of 4) outcomes varied substantially, with Imagen 3 exhibiting the fewest failures and Microsoft Image Generator exhibiting the highest number of failures. Qualitative review of the image dataset identified recurring patterns including over-embellishment, confusion between medieval styles and their later revivals, and misrepresentation of descriptive prompts (for example, egg-and-dart, banded column, pendentive). These findings support the need for visible labeling of GenAI synthetic content, provenance standards for future training datasets, and cautious educational use of GenAI architectural imagery.</li>
<li><strong>摘要：</strong>生成人工智能（GenAI）文本到图像系统越来越多地用于生成建筑图像，但它们在历史上受规则约束的领域中再现准确图像的能力仍然缺乏表征。我们使用涵盖样式、类型和编码元素的 30 种架构提示评估了 5 个广泛使用的 GenAI 图像平台（Adobe Firefly、DALL-E 3、Google Imagen 3、Microsoft Image Generator 和 Midjourney）。每个提示生成器对生成四张图像（总共 n = 600 张图像）。两名建筑历史学家根据预先定义的标准独立对每幅图像的准确性进行评分，以协商一致的方式解决分歧。集级性能概括为每四图像集零到四幅准确图像。常见提示的图像输出比罕见提示的图像输出准确度高 2.7 倍 (p < 0.05)。在各个平台上，总体准确度都很有限（最高准确度分数为 52%；最低准确度分数为 32%；平均分数为 42%）。所有平台上的全部正确（四分之四）结果相似。相比之下，全部错误（4 分之 0 分）的结果差别很大，Imagen 3 的失败次数最少，而 Microsoft Image Generator 的失败次数最多。对图像数据集的定性审查发现了重复出现的模式，包括过度修饰、中世纪风格与其后来的复兴之间的混淆以及描述性提示的错误表述（例如，鸡蛋和飞镖、带状柱、悬垂）。这些发现支持对 GenAI 合成内容进行可见标记、未来训练数据集的出处标准以及对 GenAI 建筑图像的谨慎教育使用的需求。</li>
</ul>

<h3>Title: Affostruction: 3D Affordance Grounding with Generative Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Chunghyun Park, Seunghyeon Lee, Minsu Cho</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09211">https://arxiv.org/abs/2601.09211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09211">https://arxiv.org/pdf/2601.09211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09211]] Affostruction: 3D Affordance Grounding with Generative Reconstruction(https://arxiv.org/abs/2601.09211)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper addresses the problem of affordance grounding from RGBD images of an object, which aims to localize surface regions corresponding to a text query that describes an action on the object. While existing methods predict affordance regions only on visible surfaces, we propose Affostruction, a generative framework that reconstructs complete geometry from partial observations and grounds affordances on the full shape including unobserved regions. We make three core contributions: generative multi-view reconstruction via sparse voxel fusion that extrapolates unseen geometry while maintaining constant token complexity, flow-based affordance grounding that captures inherent ambiguity in affordance distributions, and affordance-driven active view selection that leverages predicted affordances for intelligent viewpoint sampling. Affostruction achieves 19.1 aIoU on affordance grounding (40.4\% improvement) and 32.67 IoU for 3D reconstruction (67.7\% improvement), enabling accurate affordance prediction on complete shapes.</li>
<li><strong>摘要：</strong>本文解决了物体 RGBD 图像的可供性基础问题，其目的是定位与描述物体上的动作的文本查询相对应的表面区域。虽然现有方法仅预测可见表面上的可供性区域，但我们提出了 Affostruction，这是一种生成框架，可以根据部分观察重建完整的几何形状，并在包括未观察到的区域在内的完整形状上提供可供性。我们做出了三个核心贡献：通过稀疏体素融合进行生成多视图重建，推断出不可见的几何形状，同时保持恒定的标记复杂性；基于流的可供性基础，捕获可供性分布中固有的模糊性；可供性驱动的主动视图选择，利用预测可供性进行智能视点采样。 Affostruction 在可供性基础上达到 19.1 aIoU（提高 40.4%），在 3D 重建方面达到 32.67 IoU（提高 67.7%），从而能够对完整形状进行准确的可供性预测。</li>
</ul>

<h3>Title: Annealed Relaxation of Speculative Decoding for Faster Autoregressive Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Xingyao Li, Fengzhuo Zhang, Cunxiao Du, Hui Ji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09212">https://arxiv.org/abs/2601.09212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09212">https://arxiv.org/pdf/2601.09212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09212]] Annealed Relaxation of Speculative Decoding for Faster Autoregressive Image Generation(https://arxiv.org/abs/2601.09212)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite significant progress in autoregressive image generation, inference remains slow due to the sequential nature of AR models and the ambiguity of image tokens, even when using speculative decoding. Recent works attempt to address this with relaxed speculative decoding but lack theoretical grounding. In this paper, we establish the theoretical basis of relaxed SD and propose COOL-SD, an annealed relaxation of speculative decoding built on two key insights. The first analyzes the total variation (TV) distance between the target model and relaxed speculative decoding and yields an optimal resampling distribution that minimizes an upper bound of the distance. The second uses perturbation analysis to reveal an annealing behaviour in relaxed speculative decoding, motivating our annealed design. Together, these insights enable COOL-SD to generate images faster with comparable quality, or achieve better quality at similar latency. Experiments validate the effectiveness of COOL-SD, showing consistent improvements over prior methods in speed-quality trade-offs.</li>
<li><strong>摘要：</strong>尽管自回归图像生成取得了重大进展，但由于 AR 模型的顺序性质和图像标记的模糊性，即使使用推测解码，推理仍然缓慢。最近的工作试图通过宽松的推测解码来解决这个问题，但缺乏理论基础。在本文中，我们建立了松弛 SD 的理论基础，并提出了 COOL-SD，这是一种基于两个关键见解的推测解码的退火松弛。第一个分析目标模型和宽松推测解码之间的总变异 (TV) 距离，并产生最小化距离上限的最佳重采样分布。第二个使用扰动分析来揭示宽松推测解码中的退火行为，激发我们的退火设计。总之，这些见解使 COOL-SD 能够更快地生成具有相当质量的图像，或者在相似的延迟下实现更好的质量。实验验证了 COOL-SD 的有效性，显示出在速度与质量权衡方面相对于先前方法的持续改进。</li>
</ul>

<h3>Title: SpikeVAEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-VAE and Versatile Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Jialu Li, Taiyan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09213">https://arxiv.org/abs/2601.09213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09213">https://arxiv.org/pdf/2601.09213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09213]] SpikeVAEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-VAE and Versatile Diffusion(https://arxiv.org/abs/2601.09213)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Reconstructing natural visual scenes from neural activity is a key challenge in neuroscience and computer vision. We present SpikeVAEDiff, a novel two-stage framework that combines a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to generate high-resolution and semantically meaningful image reconstructions from neural spike data. In the first stage, VDVAE produces low-resolution preliminary reconstructions by mapping neural spike signals to latent representations. In the second stage, regression models map neural spike signals to CLIP-Vision and CLIP-Text features, enabling Versatile Diffusion to refine the images via image-to-image generation. We evaluate our approach on the Allen Visual Coding-Neuropixels dataset and analyze different brain regions. Our results show that the VISI region exhibits the most prominent activation and plays a key role in reconstruction quality. We present both successful and unsuccessful reconstruction examples, reflecting the challenges of decoding neural activity. Compared with fMRI-based approaches, spike data provides superior temporal and spatial resolution. We further validate the effectiveness of the VDVAE model and conduct ablation studies demonstrating that data from specific brain regions significantly enhances reconstruction performance.</li>
<li><strong>摘要：</strong>从神经活动重建自然视觉场景是神经科学和计算机视觉的一个关键挑战。我们提出了 SpikeVAEDiff，这是一种新颖的两阶段框架，它结合了非常深的变分自动编码器（VDVAE）和多功能扩散模型，可以从神经尖峰数据生成高分辨率且具有语义意义的图像重建。在第一阶段，VDVAE 通过将神经尖峰信号映射到潜在表示来产生低分辨率初步重建。在第二阶段，回归模型将神经尖峰信号映射到 CLIP-Vision 和 CLIP-Text 特征，从而使 Versatile Diffusion 能够通过图像到图像的生成来细化图像。我们在 Allen Visual Coding-Neuropixels 数据集上评估我们的方法并分析不同的大脑区域。我们的结果表明，VISI 区域表现出最显着的激活，并且在重建质量中起着关键作用。我们提供了成功和不成功的重建示例，反映了解码神经活动的挑战。与基于功能磁共振成像的方法相比，尖峰数据提供了卓越的时间和空间分辨率。我们进一步验证了 VDVAE 模型的有效性，并进行了消融研究，证明来自特定大脑区域的数据可显着提高重建性能。</li>
</ul>

<h3>Title: Knowledge-Embedded and Hypernetwork-Guided Few-Shot Substation Meter Defect Image Generation Method</h3>
<ul>
<li><strong>Authors: </strong>Jackie Alex, Justin Petter</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09238">https://arxiv.org/abs/2601.09238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09238">https://arxiv.org/pdf/2601.09238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09238]] Knowledge-Embedded and Hypernetwork-Guided Few-Shot Substation Meter Defect Image Generation Method(https://arxiv.org/abs/2601.09238)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Substation meters play a critical role in monitoring and ensuring the stable operation of power grids, yet their detection of cracks and other physical defects is often hampered by a severe scarcity of annotated samples. To address this few-shot generation challenge, we propose a novel framework that integrates Knowledge Embedding and Hypernetwork-Guided Conditional Control into a Stable Diffusion pipeline, enabling realistic and controllable synthesis of defect images from limited data. First, we bridge the substantial domain gap between natural-image pre-trained models and industrial equipment by fine-tuning a Stable Diffusion backbone using DreamBooth-style knowledge embedding. This process encodes the unique structural and textural priors of substation meters, ensuring generated images retain authentic meter characteristics. Second, we introduce a geometric crack modeling module that parameterizes defect attributes--such as location, length, curvature, and branching pattern--to produce spatially constrained control maps. These maps provide precise, pixel-level guidance during generation. Third, we design a lightweight hypernetwork that dynamically modulates the denoising process of the diffusion model in response to the control maps and high-level defect descriptors, achieving a flexible balance between generation fidelity and controllability. Extensive experiments on a real-world substation meter dataset demonstrate that our method substantially outperforms existing augmentation and generation baselines. It reduces Frechet Inception Distance (FID) by 32.7%, increases diversity metrics, and--most importantly--boosts the mAP of a downstream defect detector by 15.3% when trained on augmented data. The framework offers a practical, high-quality data synthesis solution for industrial inspection systems where defect samples are rare.</li>
<li><strong>摘要：</strong>变电站电表在监测和确保电网稳定运行方面发挥着关键作用，但其对裂缝和其他物理缺陷的检测往往因注释样本的严重缺乏而受到阻碍。为了解决这种少数镜头生成的挑战，我们提出了一种新颖的框架，它将知识嵌入和超网络引导条件控制集成到稳定扩散管道中，从而能够从有限的数据中现实且可控地合成缺陷图像。首先，我们通过使用 DreamBooth 式知识嵌入微调稳定扩散主干，弥合自然图像预训练模型和工业设备之间的巨大领域差距。此过程对变电站仪表的独特结构和纹理先验进行编码，确保生成的图像保留真实的仪表特征。其次，我们引入了一个几何裂纹建模模块，该模块对缺陷属性（例如位置、长度、曲率和分支模式）进行参数化，以生成空间约束的控制图。这些地图在生成过程中提供精确的像素级指导。第三，我们设计了一个轻量级超网络，根据控制图和高级缺陷描述符动态调节扩散模型的去噪过程，实现生成保真度和可控性之间的灵活平衡。对现实世界变电站仪表数据集的大量实验表明，我们的方法大大优于现有的增强和发电基线。它将 Frechet 起始距离 (FID) 减少了 32.7%，增加了多样性指标，最重要的是，在使用增强数据进行训练时，将下游缺陷检测器的 mAP 提高了 15.3%。该框架为缺陷样本很少的工业检测系统提供了实用、高质量的数据合成解决方案。</li>
</ul>

<h3>Title: PhyRPR: Training-Free Physics-Constrained Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yibo Zhao, Hengjia Li, Xiaofei He, Boxi Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09255">https://arxiv.org/abs/2601.09255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09255">https://arxiv.org/pdf/2601.09255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09255]] PhyRPR: Training-Free Physics-Constrained Video Generation(https://arxiv.org/abs/2601.09255)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline,\textit{PhyRPR}:\textit{Phy\uline{R}eason}--\textit{Phy\uline{P}lan}--\textit{Phy\uline{R}efine}, which decouples physical understanding from visual synthesis. Specifically, \textit{PhyReason} uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; \textit{PhyPlan} deterministically synthesizes a controllable coarse motion scaffold; and \textit{PhyRefine} injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability.</li>
<li><strong>摘要：</strong>最近基于扩散的视频生成模型可以合成视觉上合理的视频，但它们往往难以满足物理限制。一个关键原因是大多数现有方法仍然是单阶段的：它们将高级物理理解与低级视觉合成纠缠在一起，使得很难生成需要显式物理推理的内容。为了解决这个限制，我们提出了一种免训练的三阶段管道，\textit{PhyRPR}:\textit{Phy\uline{R}eason}--\textit{Phy\uline{P}lan}--\textit{Phy\uline{R}efine}，它将物理理解与视觉合成分离。具体来说，\textit{PhyReason} 使用大型多模态模型进行物理状态推理，并使用图像生成器进行关键帧合成； \textit{PhyPlan} 确定性地合成一个可控的粗运动支架； \textit{PhyRefine} 通过潜在融合策略将该支架注入扩散采样中，以细化外观，同时保留计划的动态。这种分阶段设计可以在生成过程中实现明确的物理控制。在物理限制下的大量实验表明，我们的方法不断提高物理合理性和运动可控性。</li>
</ul>

<h3>Title: GaussianFluent: Gaussian Simulation for Dynamic Scenes with Mixed Materials</h3>
<ul>
<li><strong>Authors: </strong>Bei Huang, Yixin Chen, Ruijie Lu, Gang Zeng, Hongbin Zha, Yuru Pei, Siyuan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09265">https://arxiv.org/abs/2601.09265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09265">https://arxiv.org/pdf/2601.09265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09265]] GaussianFluent: Gaussian Simulation for Dynamic Scenes with Mixed Materials(https://arxiv.org/abs/2601.09265)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting (3DGS) has emerged as a prominent 3D representation for high-fidelity and real-time rendering. Prior work has coupled physics simulation with Gaussians, but predominantly targets soft, deformable materials, leaving brittle fracture largely unresolved. This stems from two key obstacles: the lack of volumetric interiors with coherent textures in GS representation, and the absence of fracture-aware simulation methods for Gaussians. To address these challenges, we introduce GaussianFluent, a unified framework for realistic simulation and rendering of dynamic object states. First, it synthesizes photorealistic interiors by densifying internal Gaussians guided by generative models. Second, it integrates an optimized Continuum Damage Material Point Method (CD-MPM) to enable brittle fracture simulation at remarkably high speed. Our approach handles complex scenarios including mixed-material objects and multi-stage fracture propagation, achieving results infeasible with previous methods. Experiments clearly demonstrate GaussianFluent's capability for photo-realistic, real-time rendering with structurally consistent interiors, highlighting its potential for downstream application, such as VR and Robotics.</li>
<li><strong>摘要：</strong>3D 高斯泼溅 (3DGS) 已成为高保真和实时渲染的重要 3D 表示形式。先前的工作将物理模拟与高斯耦合，但主要针对柔软的可变形材料，脆性断裂在很大程度上尚未得到解决。这源于两个关键障碍：GS 表示中缺乏具有连贯纹理的体积内部，以及缺乏高斯的断裂感知模拟方法。为了解决这些挑战，我们引入了 GaussianFluent，一个用于动态对象状态的真实模拟和渲染的统一框架。首先，它通过在生成模型的指导下致密内部高斯来合成照片级真实感的内部空间。其次，它集成了优化的连续损伤材料点法 (CD-MPM)，能够以非常高的速度进行脆性断裂模拟。我们的方法可以处理复杂的场景，包括混合材料物体和多级断裂扩展，实现了以前方法无法实现的结果。实验清楚地证明了 GaussianFluent 具有照片级真实感、实时渲染和内部结构一致的能力，凸显了其在 VR 和机器人等下游应用中的潜力。</li>
</ul>

<h3>Title: Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks Structure Prediction</h3>
<ul>
<li><strong>Authors: </strong>Mianzhi Pan, JianFei Li, Peishuo Liu, Botian Wang, Yawen Ouyang, Yiming Rong, Hao Zhou, Jianbing Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09285">https://arxiv.org/abs/2601.09285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09285">https://arxiv.org/pdf/2601.09285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09285]] Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks Structure Prediction(https://arxiv.org/abs/2601.09285)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Metal-organic frameworks (MOFs) are porous crystalline materials with broad applications such as carbon capture and drug delivery, yet accurately predicting their 3D structures remains a significant challenge. While Large Language Models (LLMs) have shown promise in generating crystals, their application to MOFs is hindered by MOFs' high atomic complexity. Inspired by the success of block-wise paradigms in deep generative models, we pioneer the use of LLMs in this domain by introducing MOF-LLM, the first LLM framework specifically adapted for block-level MOF structure prediction. To effectively harness LLMs for this modular assembly task, our training paradigm integrates spatial-aware continual pre-training (CPT), structural supervised fine-tuning (SFT), and matching-driven reinforcement learning (RL). By incorporating explicit spatial priors and optimizing structural stability via Soft Adaptive Policy Optimization (SAPO), our approach substantially enhances the spatial reasoning capability of a Qwen-3 8B model for accurate MOF structure prediction. Comprehensive experiments demonstrate that MOF-LLM outperforms state-of-the-art denoising-based and LLM-based methods while exhibiting superior sampling efficiency.</li>
<li><strong>摘要：</strong>金属有机框架 (MOF) 是多孔晶体材料，具有广泛的应用，例如碳捕获和药物输送，但准确预测其 3D 结构仍然是一个重大挑战。虽然大型语言模型 (LLM) 在生成晶体方面表现出了良好的前景，但 MOF 的高原子复杂性阻碍了它们在 MOF 中的应用。受到深度生成模型中块级范例成功的启发，我们通过引入 MOF-LLM 率先在该领域使用 LLM，MOF-LLM 是第一个专门适用于块级 MOF 结构预测的 LLM 框架。为了有效利用 LLM 来完成此模块化组装任务，我们的训练范例集成了空间感知持续预训练 (CPT)、结构监督微调 (SFT) 和匹配驱动的强化学习 (RL)。通过结合显式空间先验并通过软自适应策略优化 (SAPO) 优化结构稳定性，我们的方法大大增强了 Qwen-3 8B 模型的空间推理能力，以实现准确的 MOF 结构预测。综合实验表明，MOF-LLM 优于最先进的基于去噪和基于 LLM 的方法，同时表现出卓越的采样效率。</li>
</ul>

<h3>Title: Multi-Modal LLM based Image Captioning in ICT: Bridging the Gap Between General and Industry Domain</h3>
<ul>
<li><strong>Authors: </strong>Lianying Chao, Haoran Cai, Xubin Li, Kai Zhang, Sijie Wu, Rui Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09298">https://arxiv.org/abs/2601.09298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09298">https://arxiv.org/pdf/2601.09298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09298]] Multi-Modal LLM based Image Captioning in ICT: Bridging the Gap Between General and Industry Domain(https://arxiv.org/abs/2601.09298)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In the information and communications technology (ICT) industry, training a domain-specific large language model (LLM) or constructing a retrieval-augmented generation system requires a substantial amount of high-value domain knowledge. However, the knowledge is not only hidden in the textual modality but also in the image modality. Traditional methods can parse text from domain documents but dont have image captioning ability. Multi-modal LLM (MLLM) can understand images, but they do not have sufficient domain knowledge. To address the above issues, this paper proposes a multi-stage progressive training strategy to train a Domain-specific Image Captioning Model (DICModel) in ICT, and constructs a standard evaluation system to validate the performance of DICModel. Specifically, this work first synthesizes about 7K image-text pairs by combining the Mermaid tool and LLMs, which are used for the first-stage supervised-fine-tuning (SFT) of DICModel. Then, ICT-domain experts manually annotate about 2K image-text pairs for the second-stage SFT of DICModel. Finally, experts and LLMs jointly synthesize about 1.5K visual question answering data for the instruction-based SFT. Experimental results indicate that our DICModel with only 7B parameters performs better than other state-of-the-art models with 32B parameters. Compared to the SOTA models with 7B and 32B parameters, our DICModel increases the BLEU metric by approximately 56.8% and 20.8%, respectively. On the objective questions constructed by ICT domain experts, our DICModel outperforms Qwen2.5-VL 32B by 1% in terms of accuracy rate. In summary, this work can efficiently and accurately extract the logical text from images, which is expected to promote the development of multimodal models in the ICT domain.</li>
<li><strong>摘要：</strong>在信息通信技术（ICT）行业中，训练特定领域的大语言模型（LLM）或构建检索增强生成系统需要大量高价值的领域知识。然而，知识不仅隐藏在文本模态中，也隐藏在图像模态中。传统方法可以解析领域文档中的文本，但不具备图像字幕功能。多模态法学硕士（MLLM）可以理解图像，但他们没有足够的领域知识。针对上述问题，本文提出了一种多阶段渐进训练策略来训练ICT中的特定领域图像描述模型（DICModel），并构建了标准评估体系来验证DICModel的性能。具体来说，这项工作首先通过结合 Mermaid 工具和 LLM 合成了大约 7K 个图像文本对，用于 DICModel 的第一阶段监督微调（SFT）。然后，ICT 领域专家手动注释大约 2K 个图像文本对，用于 DICModel 的第二阶段 SFT。最后，专家和法学硕士共同合成了约 1.5K 视觉问答数据，用于基于指令的 SFT。实验结果表明，我们的 DIC 模型仅具有 7B 参数，其性能优于其他具有 32B 参数的最先进模型。与具有 7B 和 32B 参数的 SOTA 模型相比，我们的 DICModel 将 BLEU 指标分别提高了约 56.8% 和 20.8%。在ICT领域专家构建的客观问题上，我们的DIC模型在准确率方面比Qwen2.5-VL 32B高出1%。综上所述，这项工作可以高效、准确地从图像中提取逻辑文本，有望促进ICT领域多模态模型的发展。</li>
</ul>

<h3>Title: Detail Loss in Super-Resolution Models Based on the Laplacian Pyramid and Repeated Upscaling and Downscaling Process</h3>
<ul>
<li><strong>Authors: </strong>Sangjun Han, Youngmi Hur</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09410">https://arxiv.org/abs/2601.09410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09410">https://arxiv.org/pdf/2601.09410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09410]] Detail Loss in Super-Resolution Models Based on the Laplacian Pyramid and Repeated Upscaling and Downscaling Process(https://arxiv.org/abs/2601.09410)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>With advances in artificial intelligence, image processing has gained significant interest. Image super-resolution is a vital technology closely related to real-world applications, as it enhances the quality of existing images. Since enhancing fine details is crucial for the super-resolution task, pixels that contribute to high-frequency information should be emphasized. This paper proposes two methods to enhance high-frequency details in super-resolution images: a Laplacian pyramid-based detail loss and a repeated upscaling and downscaling process. Total loss with our detail loss guides a model by separately generating and controlling super-resolution and detail images. This approach allows the model to focus more effectively on high-frequency components, resulting in improved super-resolution images. Additionally, repeated upscaling and downscaling amplify the effectiveness of the detail loss by extracting diverse information from multiple low-resolution features. We conduct two types of experiments. First, we design a CNN-based model incorporating our methods. This model achieves state-of-the-art results, surpassing all currently available CNN-based and even some attention-based models. Second, we apply our methods to existing attention-based models on a small scale. In all our experiments, attention-based models adding our detail loss show improvements compared to the originals. These results demonstrate our approaches effectively enhance super-resolution images across different model structures.</li>
<li><strong>摘要：</strong>随着人工智能的进步，图像处理引起了人们的极大兴趣。图像超分辨率是一项与实际应用密切相关的重要技术，因为它提高了现有图像的质量。由于增强精细细节对于超分辨率任务至关重要，因此应强调有助于高频信息的像素。本文提出了两种增强超分辨率图像中高频细节的方法：基于拉普拉斯金字塔的细节损失和重复的放大和缩小过程。完全损失和细节损失通过单独生成和控制超分辨率和细节图像来指导模型。这种方法使模型能够更有效地关注高频成分，从而改善超分辨率图像。此外，重复的放大和缩小通过从多个低分辨率特征中提取不同的信息来放大细节损失的有效性。我们进行两种类型的实验。首先，我们结合我们的方法设计了一个基于 CNN 的模型。该模型取得了最先进的结果，超越了目前所有可用的基于 CNN 的模型，甚至超越了一些基于注意力的模型。其次，我们将我们的方法小规模地应用于现有的基于注意力的模型。在我们所有的实验中，基于注意力的模型添加了细节损失，与原始模型相比显示出改进。这些结果表明我们的方法有效地增强了不同模型结构的超分辨率图像。</li>
</ul>

<h3>Title: Draw it like Euclid: Teaching transformer models to generate CAD profiles using ruler and compass construction steps</h3>
<ul>
<li><strong>Authors: </strong>Siyi Li, Joseph G. Lambourne, Longfei Zhang, Pradeep Kumar Jayaraman, Karl. D.D. Willis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09428">https://arxiv.org/abs/2601.09428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09428">https://arxiv.org/pdf/2601.09428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09428]] Draw it like Euclid: Teaching transformer models to generate CAD profiles using ruler and compass construction steps(https://arxiv.org/abs/2601.09428)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce a new method of generating Computer Aided Design (CAD) profiles via a sequence of simple geometric constructions including curve offsetting, rotations and intersections. These sequences start with geometry provided by a designer and build up the points and curves of the final profile step by step. We demonstrate that adding construction steps between the designer's input geometry and the final profile improves generation quality in a similar way to the introduction of a chain of thought in language models. Similar to the constraints in a parametric CAD model, the construction sequences reduce the degrees of freedom in the modeled shape to a small set of parameter values which can be adjusted by the designer, allowing parametric editing with the constructed geometry evaluated to floating point precision. In addition we show that applying reinforcement learning to the construction sequences gives further improvements over a wide range of metrics, including some which were not explicitly optimized.</li>
<li><strong>摘要：</strong>我们引入了一种通过一系列简单的几何构造（包括曲线偏移、旋转和交叉）生成计算机辅助设计 (CAD) 轮廓的新方法。这些序列从设计师提供的几何形状开始，并逐步构建最终轮廓的点和曲线。我们证明，在设计者的输入几何图形和最终配置文件之间添加构造步骤可以提高生成质量，其方式类似于在语言模型中引入思维链。与参数化 CAD 模型中的约束类似，构造序列将建模形状的自由度减少到可由设计者调整的一小组参数值，从而允许对构建的几何体进行参数化编辑，评估为浮点精度。此外，我们还表明，将强化学习应用于构建序列可以进一步改进各种指标，包括一些未明确优化的指标。</li>
</ul>

<h3>Title: Video-MSR: Benchmarking Multi-hop Spatial Reasoning Capabilities of MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Rui Zhu, Xin Shen, Shuchen Wu, Chenxi Miao, Xin Yu, Yang Li, Weikang Li, Deguo Xia, Jizhou Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09430">https://arxiv.org/abs/2601.09430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09430">https://arxiv.org/pdf/2601.09430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09430]] Video-MSR: Benchmarking Multi-hop Spatial Reasoning Capabilities of MLLMs(https://arxiv.org/abs/2601.09430)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Spatial reasoning has emerged as a critical capability for Multimodal Large Language Models (MLLMs), drawing increasing attention and rapid advancement. However, existing benchmarks primarily focus on single-step perception-to-judgment tasks, leaving scenarios requiring complex visual-spatial logical chains significantly underexplored. To bridge this gap, we introduce Video-MSR, the first benchmark specifically designed to evaluate Multi-hop Spatial Reasoning (MSR) in dynamic video scenarios. Video-MSR systematically probes MSR capabilities through four distinct tasks: Constrained Localization, Chain-based Reference Retrieval, Route Planning, and Counterfactual Physical Deduction. Our benchmark comprises 3,052 high-quality video instances with 4,993 question-answer pairs, constructed via a scalable, visually-grounded pipeline combining advanced model generation with rigorous human verification. Through a comprehensive evaluation of 20 state-of-the-art MLLMs, we uncover significant limitations, revealing that while models demonstrate proficiency in surface-level perception, they exhibit distinct performance drops in MSR tasks, frequently suffering from spatial disorientation and hallucination during multi-step deductions. To mitigate these shortcomings and empower models with stronger MSR capabilities, we further curate MSR-9K, a specialized instruction-tuning dataset, and fine-tune Qwen-VL, achieving a +7.82% absolute improvement on Video-MSR. Our results underscore the efficacy of multi-hop spatial instruction data and establish Video-MSR as a vital foundation for future research. The code and data will be available at this https URL.</li>
<li><strong>摘要：</strong>空间推理已成为多模态大型语言模型 (MLLM) 的一项关键功能，引起越来越多的关注并得到快速发展。然而，现有的基准主要关注单步感知到判断的任务，对需要复杂视觉空间逻辑链的场景的探索明显不足。为了弥补这一差距，我们引入了 Video-MSR，这是第一个专门用于评估动态视频场景中的多跳空间推理 (MSR) 的基准测试。 Video-MSR 通过四个不同的任务系统地探索 MSR 功能：约束定位、基于链的参考检索、路线规划和反事实物理推导。我们的基准测试包括 3,052 个高质量视频实例和 4,993 个问答对，通过可扩展、基于视觉的管道构建，将高级模型生成与严格的人工验证相结合。通过对 20 个最先进的 MLLM 的综合评估，我们发现了显着的局限性，揭示了虽然模型在表面感知方面表现出熟练程度，但它们在 MSR 任务中表现出明显的性能下降，在多步推论过程中经常出现空间迷失方向和幻觉。为了减轻这些缺点并赋予模型更强的 MSR 功能，我们进一步策划了专门的指令调优数据集 MSR-9K，并对 Qwen-VL 进行了微调，在 Video-MSR 上实现了 +7.82% 的绝对改进。我们的结果强调了多跳空间指令数据的有效性，并将视频 MSR 确立为未来研究的重要基础。代码和数据将在此 https URL 中提供。</li>
</ul>

<h3>Title: FairGU: Fairness-aware Graph Unlearning in Social Network</h3>
<ul>
<li><strong>Authors: </strong>Renqiang Luo, Yongshuai Yang, Huafei Huang, Qing Qing, Mingliang Hou, Ziqi Xu, Yi Yu, Jingjing Zhou, Feng Xia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09469">https://arxiv.org/abs/2601.09469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09469">https://arxiv.org/pdf/2601.09469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09469]] FairGU: Fairness-aware Graph Unlearning in Social Network(https://arxiv.org/abs/2601.09469)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Graph unlearning has emerged as a critical mechanism for supporting sustainable and privacy-preserving social networks, enabling models to remove the influence of deleted nodes and thereby better safeguard user information. However, we observe that existing graph unlearning techniques insufficiently protect sensitive attributes, often leading to degraded algorithmic fairness compared with traditional graph learning methods. To address this gap, we introduce FairGU, a fairness-aware graph unlearning framework designed to preserve both utility and fairness during the unlearning process. FairGU integrates a dedicated fairness-aware module with effective data protection strategies, ensuring that sensitive attributes are neither inadvertently amplified nor structurally exposed when nodes are removed. Through extensive experiments on multiple real-world datasets, we demonstrate that FairGU consistently outperforms state-of-the-art graph unlearning methods and fairness-enhanced graph learning baselines in terms of both accuracy and fairness metrics. Our findings highlight a previously overlooked risk in current unlearning practices and establish FairGU as a robust and equitable solution for the next generation of socially sustainable networked systems. The codes are available at this https URL.</li>
<li><strong>摘要：</strong>图解学习已成为支持可持续和保护隐私的社交网络的关键机制，使模型能够消除已删除节点的影响，从而更好地保护用户信息。然而，我们观察到现有的图去学习技术不足以保护敏感属性，与传统的图学习方法相比，通常会导致算法公平性下降。为了解决这一差距，我们引入了 FairGU，这是一种具有公平意识的图取消学习框架，旨在在取消学习过程中保持实用性和公平性。 FairGU将专用的公平感知模块与有效的数据保护策略集成在一起，确保敏感属性不会在节点被删除时被无意中放大或结构性暴露。通过对多个真实世界数据集的广泛实验，我们证明 FairGU 在准确性和公平性指标方面始终优于最先进的图取消学习方法和公平性增强的图学习基线。我们的研究结果强调了当前遗忘实践中先前被忽视的风险，并将 FairGU 确立为下一代社会可持续网络系统的稳健且公平的解决方案。这些代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Terminally constrained flow-based generative models from an optimal control perspective</h3>
<ul>
<li><strong>Authors: </strong>Weiguo Gao, Ming Li, Qianxiao Li</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09474">https://arxiv.org/abs/2601.09474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09474">https://arxiv.org/pdf/2601.09474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09474]] Terminally constrained flow-based generative models from an optimal control perspective(https://arxiv.org/abs/2601.09474)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We address the problem of sampling from terminally constrained distributions with pre-trained flow-based generative models through an optimal control formulation. Theoretically, we characterize the value function by a Hamilton-Jacobi-Bellman equation and derive the optimal feedback control as the minimizer of the associated Hamiltonian. We show that as the control penalty increases, the controlled process recovers the reference distribution, while as the penalty vanishes, the terminal law converges to a generalized Wasserstein projection onto the constraint manifold. Algorithmically, we introduce Terminal Optimal Control with Flow-based models (TOCFlow), a geometry-aware sampling-time guidance method for pre-trained flows. Solving the control problem in a terminal co-moving frame that tracks reference trajectories yields a closed-form scalar damping factor along the Riemannian gradient, capturing second-order curvature effects without matrix inversions. TOCFlow therefore matches the geometric consistency of Gauss-Newton updates at the computational cost of standard gradient guidance. We evaluate TOCFlow on three high-dimensional scientific tasks spanning equality, inequality, and global statistical constraints, namely Darcy flow, constrained trajectory planning, and turbulence snapshot generation with Kolmogorov spectral scaling. Across all settings, TOCFlow improves constraint satisfaction over Euclidean guidance and projection baselines while preserving the reference model's generative quality.</li>
<li><strong>摘要：</strong>我们通过最优控制公式解决了使用预先训练的基于流的生成模型从终端约束分布中采样的问题。理论上，我们通过 Hamilton-Jacobi-Bellman 方程来表征价值函数，并导出最优反馈控制作为相关哈密顿量的最小值。我们表明，随着控制惩罚的增加，受控过程恢复参考分布，而当惩罚消失时，终端定律收敛到约束流形上的广义 Wasserstein 投影。在算法上，我们引入了基于流的模型的终端最优控制（TOCFlow），这是一种用于预训练流的几何感知采样时间指导方法。解决跟踪参考轨迹的终端联动坐标系中的控制问题会产生沿黎曼梯度的封闭式标量阻尼因子，无需矩阵求逆即可捕获二阶曲率效应。因此，TOCFlow 以标准梯度引导的计算成本来匹配高斯-牛顿更新的几何一致性。我们在涵盖等式、不等式和全局统计约束的三个高维科学任务上评估 TOCFlow，即达西流、约束轨迹规划和带有 Kolmogorov 谱缩放的湍流快照生成。在所有设置中，TOCFlow 提高了欧几里德指导和投影基线的约束满意度，同时保留了参考模型的生成质量。</li>
</ul>

<h3>Title: Parallelizable memory recurrent units</h3>
<ul>
<li><strong>Authors: </strong>Florent De Geeter, Gaspard Lambrechts, Damien Ernst, Guillaume Drion</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09495">https://arxiv.org/abs/2601.09495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09495">https://arxiv.org/pdf/2601.09495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09495]] Parallelizable memory recurrent units(https://arxiv.org/abs/2601.09495)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the emergence of massively parallel processing units, parallelization has become a desirable property for new sequence models. The ability to parallelize the processing of sequences with respect to the sequence length during training is one of the main factors behind the uprising of the Transformer architecture. However, Transformers lack efficiency at sequence generation, as they need to reprocess all past timesteps at every generation step. Recently, state-space models (SSMs) emerged as a more efficient alternative. These new kinds of recurrent neural networks (RNNs) keep the efficient update of the RNNs while gaining parallelization by getting rid of nonlinear dynamics (or recurrence). SSMs can reach state-of-the art performance through the efficient training of potentially very large networks, but still suffer from limited representation capabilities. In particular, SSMs cannot exhibit persistent memory, or the capacity of retaining information for an infinite duration, because of their monostability. In this paper, we introduce a new family of RNNs, the memory recurrent units (MRUs), that combine the persistent memory capabilities of nonlinear RNNs with the parallelizable computations of SSMs. These units leverage multistability as a source of persistent memory, while getting rid of transient dynamics for efficient computations. We then derive a specific implementation as proof-of-concept: the bistable memory recurrent unit (BMRU). This new RNN is compatible with the parallel scan algorithm. We show that BMRU achieves good results in tasks with long-term dependencies, and can be combined with state-space models to create hybrid networks that are parallelizable and have transient dynamics as well as persistent memory.</li>
<li><strong>摘要：</strong>随着大规模并行处理单元的出现，并行化已成为新序列模型的理想属性。在训练期间根据序列长度并行处理序列的能力是 Transformer 架构兴起的主要因素之一。然而，Transformers 在序列生成方面缺乏效率，因为它们需要在每个生成步骤中重新处理所有过去的时间步。最近，状态空间模型（SSM）作为一种更有效的替代方案出现。这些新型循环神经网络 (RNN) 保持了 RNN 的高效更新，同时通过消除非线性动力学（或循环）获得并行化。 SSM 可以通过对可能非常大的网络进行有效训练来达到最先进的性能，但仍然受到表示能力有限的影响。特别是，由于 SSM 的单稳定性，它无法表现出持久记忆或无限期保留信息的能力。在本文中，我们介绍了一个新的 RNN 系列，即记忆循环单元 (MRU)，它将非线性 RNN 的持久记忆功能与 SSM 的可并行计算相结合。这些单元利用多稳定性作为持久内存的来源，同时消除瞬态动态以实现高效计算。然后，我们推导出一个具体的实现作为概念验证：双稳态内存循环单元（BMRU）。这个新的 RNN 与并行扫描算法兼容。我们证明 BMRU 在具有长期依赖关系的任务中取得了良好的结果，并且可以与状态空间模型相结合来创建可并行且具有瞬态动态和持久内存的混合网络。</li>
</ul>

<h3>Title: GlovEgo-HOI: Bridging the Synthetic-to-Real Gap for Industrial Egocentric Human-Object Interaction Detection</h3>
<ul>
<li><strong>Authors: </strong>Alfio Spoto, Rosario Leonardi, Francesco Ragusa, Giovanni Maria Farinella</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09528">https://arxiv.org/abs/2601.09528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09528">https://arxiv.org/pdf/2601.09528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09528]] GlovEgo-HOI: Bridging the Synthetic-to-Real Gap for Industrial Egocentric Human-Object Interaction Detection(https://arxiv.org/abs/2601.09528)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Egocentric Human-Object Interaction (EHOI) analysis is crucial for industrial safety, yet the development of robust models is hindered by the scarcity of annotated domain-specific data. We address this challenge by introducing a data generation framework that combines synthetic data with a diffusion-based process to augment real-world images with realistic Personal Protective Equipment (PPE). We present GlovEgo-HOI, a new benchmark dataset for industrial EHOI, and GlovEgo-Net, a model integrating Glove-Head and Keypoint- Head modules to leverage hand pose information for enhanced interaction detection. Extensive experiments demonstrate the effectiveness of the proposed data generation framework and GlovEgo-Net. To foster further research, we release the GlovEgo-HOI dataset, augmentation pipeline, and pre-trained models at: GitHub project.</li>
<li><strong>摘要：</strong>以自我为中心的人机交互 (EHOI) 分析对于工业安全至关重要，但鲁棒模型的开发因缺乏带注释的特定领域数据而受到阻碍。我们通过引入一个数据生成框架来应对这一挑战，该框架将合成数据与基于扩散的过程相结合，以使用真实的个人防护装备 (PPE) 来增强真实世界的图像。我们推出了 GlovEgo-HOI（工业 EHOI 的新基准数据集）和 GlovEgo-Net（集成 Glove-Head 和 Keypoint-Head 模块以利用手势信息来增强交互检测的模型）。大量实验证明了所提出的数据生成框架和 GlovEgo-Net 的有效性。为了促进进一步的研究，我们在 GitHub 项目上发布了 GlovEgo-HOI 数据集、增强管道和预训练模型。</li>
</ul>

<h3>Title: Trustworthy Longitudinal Brain MRI Completion: A Deformation-Based Approach with KAN-Enhanced Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Tianli Tao, Ziyang Wang, Delong Yang, Han Zhang, Le Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09572">https://arxiv.org/abs/2601.09572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09572">https://arxiv.org/pdf/2601.09572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09572]] Trustworthy Longitudinal Brain MRI Completion: A Deformation-Based Approach with KAN-Enhanced Diffusion Model(https://arxiv.org/abs/2601.09572)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Longitudinal brain MRI is essential for lifespan study, yet high attrition rates often lead to missing data, complicating analysis. Deep generative models have been explored, but most rely solely on image intensity, leading to two key limitations: 1) the fidelity or trustworthiness of the generated brain images are limited, making downstream studies questionable; 2) the usage flexibility is restricted due to fixed guidance rooted in the model structure, restricting full ability to versatile application scenarios. To address these challenges, we introduce DF-DiffCom, a Kolmogorov-Arnold Networks (KAN)-enhanced diffusion model that smartly leverages deformation fields for trustworthy longitudinal brain image completion. Trained on OASIS-3, DF-DiffCom outperforms state-of-the-art methods, improving PSNR by 5.6% and SSIM by 0.12. More importantly, its modality-agnostic nature allows smooth extension to varied MRI modalities, even to attribute maps such as brain tissue segmentation results.</li>
<li><strong>摘要：</strong>纵向脑部 MRI 对于寿命研究至关重要，但高损耗率通常会导致数据丢失，使分析变得复杂。深度生成模型已经被探索过，但大多数仅依赖于图像强度，导致两个关键限制：1）生成的大脑图像的保真度或可信度有限，使得下游研究受到质疑； 2）由于模型结构中的固定指导，使用灵活性受到限制，限制了对多种应用场景的全面能力。为了应对这些挑战，我们引入了 DF-DiffCom，这是一种柯尔莫哥洛夫-阿诺德网络 (KAN) 增强的扩散模型，它巧妙地利用变形场来完成值得信赖的纵向大脑图像。在 OASIS-3 上训练后，DF-DiffCom 的性能优于最先进的方法，PSNR 提高了 5.6%，SSIM 提高了 0.12。更重要的是，其与模态无关的性质允许平滑扩展到各种 MRI 模态，甚至可以扩展到诸如脑组织分割结果之类的属性图。</li>
</ul>

<h3>Title: Identifying Models Behind Text-to-Image Leaderboards</h3>
<ul>
<li><strong>Authors: </strong>Ali Naseh, Yuefeng Peng, Anshuman Suri, Harsh Chaudhari, Alina Oprea, Amir Houmansadr</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09647">https://arxiv.org/abs/2601.09647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09647">https://arxiv.org/pdf/2601.09647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09647]] Identifying Models Behind Text-to-Image Leaderboards(https://arxiv.org/abs/2601.09647)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) models are increasingly popular, producing a large share of AI-generated images online. To compare model quality, voting-based leaderboards have become the standard, relying on anonymized model outputs for fairness. In this work, we show that such anonymity can be easily broken. We find that generations from each T2I model form distinctive clusters in the image embedding space, enabling accurate deanonymization without prompt control or training data. Using 22 models and 280 prompts (150K images), our centroid-based method achieves high accuracy and reveals systematic model-specific signatures. We further introduce a prompt-level distinguishability metric and conduct large-scale analyses showing how certain prompts can lead to near-perfect distinguishability. Our findings expose fundamental security flaws in T2I leaderboards and motivate stronger anonymization defenses.</li>
<li><strong>摘要：</strong>文本到图像 (T2I) 模型越来越受欢迎，在线生成了大量人工智能生成的图像。为了比较模型质量，基于投票的排行榜已成为标准，依靠匿名模型输出来实现公平性。在这项工作中，我们表明这种匿名性很容易被打破。我们发现，每个 T2I 模型的世代在图像嵌入空间中形成独特的簇，无需即时控制或训练数据即可实现准确的去匿名化。我们的基于质心的方法使用 22 个模型和 280 个提示（150K 图像），实现了高精度并揭示了系统的模型特定特征。我们进一步引入了提示级别的可区分性指标，并进行大规模分析，显示某些提示如何导致近乎完美的可区分性。我们的发现暴露了 T2I 排行榜中的基本安全缺陷，并激发了更强大的匿名防御。</li>
</ul>

<h3>Title: Image2Garment: Simulation-ready Garment Generation from a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Selim Emir Can, Jan Ackermann, Kiyohiro Nakayama, Ruofan Liu, Tong Wu, Yang Zheng, Hugo Bertiche, Menglei Chai, Thabo Beeler, Gordon Wetzstein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09658">https://arxiv.org/abs/2601.09658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09658">https://arxiv.org/pdf/2601.09658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09658]] Image2Garment: Simulation-ready Garment Generation from a Single Image(https://arxiv.org/abs/2601.09658)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods.</li>
<li><strong>摘要：</strong>由于缺乏图像到物理数据集以及该问题的不适定性质，从单个图像估计物理准确、可用于模拟的服装具有挑战性。现有方法要么需要多视图捕获和昂贵的可微分模拟，要么仅预测服装几何形状，而无需实际模拟所需的材料属性。我们提出了一个前馈框架，该框架首先微调视觉语言模型以从真实图像推断材料成分和织物属性，然后训练一个轻量级预测器，使用材料物理测量的小型数据集将这些属性映射到相应的物理织物参数，从而规避这些限制。我们的方法引入了两个新的数据集（FTAG 和 T2P），并从单个图像提供可模拟的服装，而无需迭代优化。实验表明，我们的估计器在材料成分估计和织物属性预测方面实现了卓越的准确性，并且通过将它们传递给我们的物理参数估计器，与最先进的图像到服装方法相比，我们进一步实现了更高保真度的模拟。</li>
</ul>

<h3>Title: Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering</h3>
<ul>
<li><strong>Authors: </strong>Jieying Chen, Jeffrey Hu, Joan Lasenby, Ayush Tewari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.09697">https://arxiv.org/abs/2601.09697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.09697">https://arxiv.org/pdf/2601.09697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.09697]] Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering(https://arxiv.org/abs/2601.09697)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex camera motion. This results in video generation that is more than 40 times faster than the diffusion-based baseline in generating 20 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.</li>
<li><strong>摘要：</strong>基于扩散模型的现代视频生成模型可以生成非常逼真的剪辑，但它们的计算效率较低，通常需要几分钟的 GPU 时间来处理几秒钟的视频。这种低效率对在需要实时交互的应用程序（例如嵌入式 AI 和 VR/AR）中部署生成视频构成了关键障碍。本文探索了一种静态场景相机条件视频生成的新策略：使用基于扩散的生成模型生成一组稀疏关键帧，然后通过 3D 重建和渲染合成完整视频。通过将关键帧提升为 3D 表示并渲染中间视图，我们的方法可以在数百个帧之间分摊生成成本，同时强制几何一致性。我们进一步引入了一个模型，可以预测给定相机轨迹的最佳关键帧数量，从而允许系统自适应地分配计算。我们的最终方法 SRENDER 使用非常稀疏的关键帧来表示简单的轨迹，使用更密集的关键帧来表示复杂的相机运动。这使得视频生成速度比基于扩散的基线快 40 倍以上，生成 20 秒的视频，同时保持高视觉保真度和时间稳定性，为高效、可控的视频合成提供了一条实用途径。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
