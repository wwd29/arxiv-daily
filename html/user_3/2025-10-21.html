<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-21</h1>
<h3>Title: BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling</h3>
<ul>
<li><strong>Authors: </strong>Guangya Wan, Zixin Stephen Xu, Sasa Zorc, Manel Baucells, Mengxuan Hu, Hao Wang, Sheng Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15945">https://arxiv.org/abs/2510.15945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15945">https://arxiv.org/pdf/2510.15945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15945]] BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling(https://arxiv.org/abs/2510.15945)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Sampling multiple responses is a common way to improve LLM output quality, but it comes at the cost of additional computation. The key challenge is deciding when to stop generating new samples to balance accuracy gains against efficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive Criterion for Optimal N-stopping), a principled adaptive sampling framework grounded in Sequential Search with Bayesian Learning. BEACON sequentially generates responses from the policy LLM, updates posterior belief over reward distributions in real time without further training, and determines when to stop by weighing expected gains against computational cost. Sampling terminates once the marginal utility of further exploration no longer justifies the expense. We establish both theoretical optimality guarantees and practical tractability, and show empirically that BEACON reduces average sampling by up to 80% while maintaining response quality. We further demonstrate BEACON's utility for cost-efficient preference data generation and outline practical extensions, offering actionable insights for future researchers.</li>
<li><strong>摘要：</strong>对多个响应进行采样是提高 LLM 输出质量的常见方法，但它是以额外计算为代价的。关键的挑战是决定何时停止生成新样本以平衡准确性增益和效率。为了解决这个问题，我们引入了 BEACON（最佳 N 停止的贝叶斯高效自适应标准），这是一种基于贝叶斯学习的顺序搜索的有原则的自适应采样框架。 BEACON 按顺序生成来自策略 LLM 的响应，无需进一步训练即可实时更新奖励分布的后验信念，并通过权衡预期收益与计算成本来确定何时停止。一旦进一步勘探的边际效用不再证明支出合理，抽样就会终止。我们建立了理论最优性保证和实际可处理性，并根据经验表明，BEACON 在保持响应质量的同时将平均采样减少了高达 80%。我们进一步展示了 BEACON 在经济高效的偏好数据生成方面的实用性，并概述了实用的扩展，为未来的研究人员提供了可行的见解。</li>
</ul>

<h3>Title: Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter</h3>
<ul>
<li><strong>Authors: </strong>Hongzheng Shi, Yuhang Wang, Xiao Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15954">https://arxiv.org/abs/2510.15954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15954">https://arxiv.org/pdf/2510.15954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15954]] Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter(https://arxiv.org/abs/2510.15954)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As wildfires become increasingly destructive and expensive to control, effective management of active wildfires requires accurate, real-time fire spread predictions. To enhance the forecasting accuracy of active fires, data assimilation plays a vital role by integrating observations (such as remote-sensing data) and fire predictions generated from numerical models. This paper provides a comprehensive investigation on the application of a recently proposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter (EnSF) -- to the data assimilation problem for real-time active wildfire spread predictions. Leveraging a score-based generative diffusion model, EnSF has been shown to have superior accuracy for high-dimensional nonlinear filtering problems, making it an ideal candidate for the filtering problems of wildfire spread models. Technical details are provided, and our numerical investigations demonstrate that EnSF provides superior accuracy, stability, and computational efficiency, establishing it as a robust and practical method for wildfire data assimilation. Our code has been made publicly available.</li>
<li><strong>摘要：</strong>随着野火的破坏性越来越大，控制成本也越来越高，有效管理活跃的野火需要准确、实时的火势蔓延预测。为了提高活动火灾的预报精度，通过整合观测数据（例如遥感数据）和数值模型生成的火灾预测，数据同化发挥着至关重要的作用。本文对最近提出的基于扩散模型的过滤算法（Ensemble Score Filter (EnSF)）在实时主动野火蔓延预测的数据同化问题中的应用进行了全面的研究。利用基于分数的生成扩散模型，EnSF 已被证明对高维非线性过滤问题具有卓越的准确性，使其成为野火蔓延模型过滤问题的理想候选者。提供了技术细节，我们的数值研究表明 EnSF 提供了卓越的准确性、稳定性和计算效率，使其成为野火数据同化的稳健且实用的方法。我们的代码已公开。</li>
</ul>

<h3>Title: ESCA: Contextualizing Embodied Agents via Scene-Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiani Huang, Amish Sethi, Matthew Kuo, Mayank Keoliya, Neelay Velingker, JungHo Jung, Ser-Nam Lim, Ziyang Li, Mayur Naik</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15963">https://arxiv.org/abs/2510.15963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15963">https://arxiv.org/pdf/2510.15963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15963]] ESCA: Contextualizing Embodied Agents via Scene-Graph Generation(https://arxiv.org/abs/2510.15963)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multi-modal large language models (MLLMs) are making rapid progress toward general-purpose embodied agents. However, current training pipelines primarily rely on high-level vision-sound-text pairs and lack fine-grained, structured alignment between pixel-level visual content and textual semantics. To overcome this challenge, we propose ESCA, a new framework for contextualizing embodied agents through structured spatial-temporal understanding. At its core is SGClip, a novel CLIP-based, open-domain, and promptable model for generating scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic learning pipeline, which harnesses model-driven self-supervision from video-caption pairs and structured reasoning, thereby eliminating the need for human-labeled scene graph annotations. We demonstrate that SGClip supports both prompt-based inference and task-specific fine-tuning, excelling in scene graph generation and action localization benchmarks. ESCA with SGClip consistently improves both open-source and commercial MLLMs, achieving state-of-the-art performance across two embodied environments. Notably, it significantly reduces agent perception errors and enables open-source models to surpass proprietary baselines.</li>
<li><strong>摘要：</strong>多模态大语言模型（MLLM）正在朝着通用具体代理的方向快速进步。然而，当前的训练管道主要依赖于高级视觉-声音-文本对，并且缺乏像素级视觉内容和文本语义之间的细粒度、结构化对齐。为了克服这一挑战，我们提出了 ESCA，这是一种通过结构化时空理解将具体主体置于情境中的新框架。其核心是 SGClip，这是一种基于 CLIP 的新型开放域、可提示的场景图生成模型。 SGClip 通过神经符号学习管道对超过 87K 个开放域视频进行训练，该管道利用来自视频字幕对和结构化推理的模型驱动的自我监督，从而消除了对人工标记的场景图注释的需要。我们证明 SGClip 支持基于提示的推理和特定于任务的微调，在场景图生成和动作本地化基准测试中表现出色。 ESCA 与 SGClip 不断改进开源和商业 MLLM，在两个具体环境中实现最先进的性能。值得注意的是，它显着减少了代理感知错误，并使开源模型能够超越专有基线。</li>
</ul>

<h3>Title: One Token Embedding Is Enough to Deadlock Your Large Reasoning Model</h3>
<ul>
<li><strong>Authors: </strong>Mohan Zhang, Yihua Zhang, Jinghan Jia, Zhangyang Wang, Sijia Liu, Tianlong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15965">https://arxiv.org/abs/2510.15965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15965">https://arxiv.org/pdf/2510.15965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15965]] One Token Embedding Is Enough to Deadlock Your Large Reasoning Model(https://arxiv.org/abs/2510.15965)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modern large reasoning models (LRMs) exhibit impressive multi-step problem-solving via chain-of-thought (CoT) reasoning. However, this iterative thinking mechanism introduces a new vulnerability surface. We present the Deadlock Attack, a resource exhaustion method that hijacks an LRM's generative control flow by training a malicious adversarial embedding to induce perpetual reasoning loops. Specifically, the optimized embedding encourages transitional tokens (e.g., "Wait", "But") after reasoning steps, preventing the model from concluding its answer. A key challenge we identify is the continuous-to-discrete projection gap: naïve projections of adversarial embeddings to token sequences nullify the attack. To overcome this, we introduce a backdoor implantation strategy, enabling reliable activation through specific trigger tokens. Our method achieves a 100% attack success rate across four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three math reasoning benchmarks, forcing models to generate up to their maximum token limits. The attack is also stealthy (in terms of causing negligible utility loss on benign user inputs) and remains robust against existing strategies trying to mitigate the overthinking issue. Our findings expose a critical and underexplored security vulnerability in LRMs from the perspective of reasoning (in)efficiency.</li>
<li><strong>摘要：</strong>现代大型推理模型 (LRM) 通过思想链 (CoT) 推理展现出令人印象深刻的多步骤问题解决能力。然而，这种迭代思维机制引入了新的脆弱面。我们提出了死锁攻击，这是一种资源耗尽方法，通过训练恶意对抗性嵌入来诱导永久推理循环来劫持 LRM 的生成控制流。具体来说，优化的嵌入鼓励在推理步骤之后使用过渡标记（例如“等待”、“但是”），从而阻止模型得出答案。我们发现的一个关键挑战是连续到离散的投影差距：对抗性嵌入到令牌序列的天真投影会使攻击无效。为了克服这个问题，我们引入了后门植入策略，通过特定的触发令牌实现可靠的激活。我们的方法在四个高级 LRM（Phi-RM、Nemotron-Nano、R1-Qwen、R1-Llama）和三个数学推理基准上实现了 100% 的攻击成功率，迫使模型生成最大令牌限制。这种攻击也是隐秘的（就对良性用户输入造成的效用损失而言可以忽略不计），并且对于试图缓解过度思考问题的现有策略来说仍然具有鲁棒性。我们的研究结果从推理（低）效率的角度揭示了 LRM 中一个关键且尚未充分研究的安全漏洞。</li>
</ul>

<h3>Title: Bolster Hallucination Detection via Prompt-Guided Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Wenyun Li, Zheng Zhang, Dongmei Jiang, Xiangyuan Lan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15977">https://arxiv.org/abs/2510.15977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15977">https://arxiv.org/pdf/2510.15977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15977]] Bolster Hallucination Detection via Prompt-Guided Data Augmentation(https://arxiv.org/abs/2510.15977)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have garnered significant interest in AI community. Despite their impressive generation capabilities, they have been found to produce misleading or fabricated information, a phenomenon known as hallucinations. Consequently, hallucination detection has become critical to ensure the reliability of LLM-generated content. One primary challenge in hallucination detection is the scarcity of well-labeled datasets containing both truthful and hallucinated outputs. To address this issue, we introduce Prompt-guided data Augmented haLlucination dEtection (PALE), a novel framework that leverages prompt-guided responses from LLMs as data augmentation for hallucination detection. This strategy can generate both truthful and hallucinated data under prompt guidance at a relatively low cost. To more effectively evaluate the truthfulness of the sparse intermediate embeddings produced by LLMs, we introduce an estimation metric called the Contrastive Mahalanobis Score (CM Score). This score is based on modeling the distributions of truthful and hallucinated data in the activation space. CM Score employs a matrix decomposition approach to more accurately capture the underlying structure of these distributions. Importantly, our framework does not require additional human annotations, offering strong generalizability and practicality for real-world applications. Extensive experiments demonstrate that PALE achieves superior hallucination detection performance, outperforming the competitive baseline by a significant margin of 6.55%.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）引起了人工智能社区的极大兴趣。尽管它们的生成能力令人印象深刻，但人们发现它们会产生误导或捏造的信息，这种现象被称为幻觉。因此，幻觉检测对于确保法学硕士生成内容的可靠性变得至关重要。幻觉检测的一个主要挑战是缺乏包含真实输出和幻觉输出的标记良好的数据集。为了解决这个问题，我们引入了即时引导数据增强幻觉检测（PALE），这是一种新颖的框架，利用法学硕士的即时引导响应作为幻觉检测的数据增强。该策略可以在及时指导下以相对较低的成本生成真实数据和幻觉数据。为了更有效地评估 LLM 生成的稀疏中间嵌入的真实性，我们引入了一种称为对比马氏分数（CM 分数）的估计指标。该分数基于对激活空间中真实数据和幻觉数据的分布进行建模。 CM Score 采用矩阵分解方法来更准确地捕获这些分布的基础结构。重要的是，我们的框架不需要额外的人工注释，为实际应用提供了强大的通用性和实用性。大量实验表明，PALE 实现了卓越的幻觉检测性能，明显优于竞争基线 6.55%。</li>
</ul>

<h3>Title: Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Samuel Lippl, Thomas McGee, Kimberly Lopez, Ziwen Pan, Pierce Zhang, Salma Ziadi, Oliver Eberle, Ida Momennejad</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.15987">https://arxiv.org/abs/2510.15987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.15987">https://arxiv.org/pdf/2510.15987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.15987]] Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models(https://arxiv.org/abs/2510.15987)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>How do latent and inference time computations enable large language models (LLMs) to solve multi-step reasoning? We introduce a framework for tracing and steering algorithmic primitives that underlie model reasoning. Our approach links reasoning traces to internal activation patterns and evaluates algorithmic primitives by injecting them into residual streams and measuring their effect on reasoning steps and task performance. We consider four benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph navigation. We operationalize primitives by clustering neural activations and labeling their matched reasoning traces. We then apply function vector methods to derive primitive vectors as reusable compositional building blocks of reasoning. Primitive vectors can be combined through addition, subtraction, and scalar operations, revealing a geometric logic in activation space. Cross-task and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both shared and task-specific primitives. Notably, comparing Phi-4 with its reasoning-finetuned variant highlights compositional generalization after finetuning: Phi-4-Reasoning exhibits more systematic use of verification and path-generation primitives. Injecting the associated primitive vectors in Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning. Together, these findings demonstrate that reasoning in LLMs may be supported by a compositional geometry of algorithmic primitives, that primitives transfer cross-task and cross-model, and that reasoning finetuning strengthens algorithmic generalization across domains.</li>
<li><strong>摘要：</strong>潜在时间和推理时间计算如何使大型语言模型 (LLM) 能够解决多步骤推理？我们引入了一个用于跟踪和引导作为模型推理基础的算法原语的框架。我们的方法将推理轨迹与内部激活模式联系起来，并通过将算法原语注入残余流并测量它们对推理步骤和任务性能的影响来评估算法原语。我们考虑四个基准：旅行推销员问题 (TSP)、3SAT、AIME 和图形导航。我们通过对神经激活进行聚类并标记其匹配的推理轨迹来操作原语。然后，我们应用函数向量方法来导出原始向量作为可重用的推理组合构建块。原始向量可以通过加法、减法和标量运算进行组合，揭示激活空间中的几何逻辑。跨任务和跨模型评估（Phi-4、Phi-4-Reasoning、Llama-3-8B）显示共享原语和特定于任务的原语。值得注意的是，将 Phi-4 与其推理微调变体进行比较，突出了微调后的组合泛化：Phi-4-Reasoning 展示了对验证和路径生成原语的更系统的使用。在 Phi-4-Base 中注入相关的原始向量会诱发与 Phi-4-Reasoning 相关的行为特征。总之，这些发现表明，法学硕士中的推理可能得到算法原语的组合几何的支持，原语跨任务和跨模型传输，推理微调增强了跨领域的算法泛化。</li>
</ul>

<h3>Title: Feature-driven reinforcement learning for photovoltaic in continuous intraday trading</h3>
<ul>
<li><strong>Authors: </strong>Arega Getaneh Abate, Xiufeng Liu, Ruyu Liu, Xiaobing Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16021">https://arxiv.org/abs/2510.16021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16021">https://arxiv.org/pdf/2510.16021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16021]] Feature-driven reinforcement learning for photovoltaic in continuous intraday trading(https://arxiv.org/abs/2510.16021)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Photovoltaic (PV) operators face substantial uncertainty in generation and short-term electricity prices. Continuous intraday markets enable producers to adjust their positions in real time, potentially improving revenues and reducing imbalance costs. We propose a feature-driven reinforcement learning (RL) approach for PV intraday trading that integrates data-driven features into the state and learns bidding policies in a sequential decision framework. The problem is cast as a Markov Decision Process with a reward that balances trading profit and imbalance penalties and is solved with Proximal Policy Optimization (PPO) using a predominantly linear, interpretable policy. Trained on historical market data and evaluated out-of-sample, the strategy consistently outperforms benchmark baselines across diverse scenarios. Extensive validation shows rapid convergence, real-time inference, and transparent decision rules. Learned weights highlight the central role of market microstructure and historical features. Taken together, these results indicate that feature-driven RL offers a practical, data-efficient, and operationally deployable pathway for active intraday participation by PV producers.</li>
<li><strong>摘要：</strong>光伏（PV）运营商在发电和短期电价方面面临巨大的不确定性。连续的日内市场使生产商能够实时调整头寸，从而有可能提高收入并降低失衡成本。我们提出了一种用于光伏日内交易的特征驱动强化学习（RL）方法，该方法将数据驱动特征集成到状态中，并在顺序决策框架中学习出价策略。该问题被视为马尔可夫决策过程，其奖励平衡交易利润和不平衡惩罚，并通过使用主要是线性的、可解释的策略的近端策略优化（PPO）来解决。该策略根据历史市场数据进行训练并进行样本外评估，在不同场景中始终优于基准基线。广泛的验证显示出快速收敛、实时推理和透明的决策规则。学习权重凸显了市场微观结构和历史特征的核心作用。总而言之，这些结果表明，特征驱动的强化学习为光伏生产商的积极日内参与提供了实用、数据高效且可操作部署的途径。</li>
</ul>

<h3>Title: Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization</h3>
<ul>
<li><strong>Authors: </strong>Changsheng Wang, Xin Chen, Sijia Liu, Ke Ding</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16022">https://arxiv.org/abs/2510.16022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16022">https://arxiv.org/pdf/2510.16022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16022]] Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization(https://arxiv.org/abs/2510.16022)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Adapting pretrained large language models (LLMs) to code domains via supervised fine-tuning (FT) has been commonly used for code generation. However, we identify a previously underappreciated failure mode, the memorization barrier, where strong memorization of downstream code data in the base model could trap optimization and prevent the standard FT from effectively acquiring new, generalizable code knowledge. To overcome this barrier, we propose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which applies an IB penalty on hidden representations of the code data to compress spurious, memorized features while preserving task-relevant information. Extensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1) show that IB-FT substantially alleviates the memorization barrier, improves top-1 performance (Pass@$1$), and yields far more stable gains under the stricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if at least $m$ of $k$ samples pass unit tests) compared with conventional FT.</li>
<li><strong>摘要：</strong>通过监督微调 (FT) 将预训练的大型语言模型 (LLM) 应用于代码域已广泛用于代码生成。然而，我们发现了一种先前未被充分认识的故障模式，即记忆障碍，其中基础模型中下游代码数据的强记忆可能会陷入优化并阻止标准 FT 有效获取新的、可概括的代码知识。为了克服这一障碍，我们提出了信息瓶颈（IB）引导的微调，称为 IB-FT，它对代码数据的隐藏表示应用 IB 惩罚，以压缩虚假的记忆特征，同时保留任务相关信息。对两个代码基准测试（OriGen 和 Evol-CodeAlpaca-V1）的大量实验表明，IB-FT 大大缓解了记忆障碍，提高了 top-1 性能 (Pass@$1$)，并在更严格的多样本指标 Pass@$k^{(m)}$ 下产生更稳定的收益（只有当 $k$ 样本中至少 $m$ 通过单位时，问题才算已解决 测试）与传统 FT 进行比较。</li>
</ul>

<h3>Title: Unifying Polymer Modeling and Design via a Conformation-Centric Generative Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Fanmeng Wang, Shan Mei, Wentao Guo, Hongshuai Wang, Qi Ou, Zhifeng Gao, Hongteng Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16023">https://arxiv.org/abs/2510.16023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16023">https://arxiv.org/pdf/2510.16023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16023]] Unifying Polymer Modeling and Design via a Conformation-Centric Generative Foundation Model(https://arxiv.org/abs/2510.16023)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Polymers, macromolecules formed from covalently bonded monomers, underpin countless technologies and are indispensable to modern life. While deep learning is advancing polymer science, existing methods typically represent the whole polymer solely through monomer-level descriptors, overlooking the global structural information inherent in polymer conformations, which ultimately limits their practical performance. Moreover, this field still lacks a universal foundation model that can effectively support diverse downstream tasks, thereby severely constraining progress. To address these challenges, we introduce PolyConFM, the first polymer foundation model that unifies polymer modeling and design through conformation-centric generative pretraining. Recognizing that each polymer conformation can be decomposed into a sequence of local conformations (i.e., those of its repeating units), we pretrain PolyConFM under the conditional generation paradigm, reconstructing these local conformations via masked autoregressive (MAR) modeling and further generating their orientation transformations to recover the corresponding polymer conformation. Besides, we construct the first high-quality polymer conformation dataset via molecular dynamics simulations to mitigate data sparsity, thereby enabling conformation-centric pretraining. Experiments demonstrate that PolyConFM consistently outperforms representative task-specific methods on diverse downstream tasks, equipping polymer science with a universal and powerful tool.</li>
<li><strong>摘要：</strong>聚合物是由共价键合单体形成的大分子，支撑着无数技术，是现代生活不可或缺的。虽然深度学习正在推动聚合物科学的发展，但现有的方法通常仅通过单体级描述符来表示整个聚合物，忽略了聚合物构象固有的全局结构信息，这最终限制了它们的实际性能。此外，该领域仍然缺乏能够有效支持多样化下游任务的通用基础模型，从而严重制约了进展。为了应对这些挑战，我们推出了 PolyConFM，这是第一个通过以构象为中心的生成预训练统一聚合物建模和设计的聚合物基础模型。认识到每个聚合物构象可以分解为一系列局部构象（即其重复单元的构象），我们在条件生成范例下预训练 PolyConFM，通过掩蔽自回归（MAR）建模重建这些局部构象，并进一步生成它们的方向变换以恢复相应的聚合物构象。此外，我们通过分子动力学模拟构建了第一个高质量的聚合物构象数据集，以减轻数据稀疏性，从而实现以构象为中心的预训练。实验表明，PolyConFM 在各种下游任务上始终优于代表性的特定任务方法，为聚合物科学配备了通用且强大的工具。</li>
</ul>

<h3>Title: GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer</h3>
<ul>
<li><strong>Authors: </strong>Sayan Deb Sarkar, Sinisa Stekovic, Vincent Lepetit, Iro Armeni</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16136">https://arxiv.org/abs/2510.16136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16136">https://arxiv.org/pdf/2510.16136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16136]] GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer(https://arxiv.org/abs/2510.16136)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Transferring appearance to 3D assets using different representations of the appearance object - such as images or text - has garnered interest due to its wide range of applications in industries like gaming, augmented reality, and digital content creation. However, state-of-the-art methods still fail when the geometry between the input and appearance objects is significantly different. A straightforward approach is to directly apply a 3D generative model, but we show that this ultimately fails to produce appealing results. Instead, we propose a principled approach inspired by universal guidance. Given a pretrained rectified flow model conditioned on image or text, our training-free method interacts with the sampling process by periodically adding guidance. This guidance can be modeled as a differentiable loss function, and we experiment with two different types of guidance including part-aware losses for appearance and self-similarity. Our experiments show that our approach successfully transfers texture and geometric details to the input 3D asset, outperforming baselines both qualitatively and quantitatively. We also show that traditional metrics are not suitable for evaluating the task due to their inability of focusing on local details and comparing dissimilar inputs, in absence of ground truth data. We thus evaluate appearance transfer quality with a GPT-based system objectively ranking outputs, ensuring robust and human-like assessment, as further confirmed by our user study. Beyond showcased scenarios, our method is general and could be extended to different types of diffusion models and guidance functions.</li>
<li><strong>摘要：</strong>使用外观对象的不同表示（例如图像或文本）将外观转移到 3D 资产已引起人们的兴趣，因为它在游戏、增强现实和数字内容创建等行业中具有广泛的应用。然而，当输入对象和外观对象之间的几何形状显着不同时，最先进的方法仍然会失败。一种直接的方法是直接应用 3D 生成模型，但我们表明这最终无法产生有吸引力的结果。相反，我们提出了一种受普遍指导启发的原则性方法。给定以图像或文本为条件的预训练整流流模型，我们的免训练方法通过定期添加指导来与采样过程交互。该指导可以建模为可微损失函数，我们尝试了两种不同类型的指导，包括外观和自相似性的部分感知损失。我们的实验表明，我们的方法成功地将纹理和几何细节传输到输入 3D 资产，在质量和数量上都优于基线。我们还表明，传统指标不适合评估任务，因为在缺乏地面实况数据的情况下，传统指标无法关注局部细节并比较不同的输入。因此，我们使用基于 GPT 的系统客观地对输出进行排名来评估外观传输质量，确保稳健且类似于人类的评估，正如我们的用户研究进一步证实的那样。除了展示的场景之外，我们的方法是通用的，可以扩展到不同类型的扩散模型和指导函数。</li>
</ul>

<h3>Title: Zeroth-Order Sharpness-Aware Learning with Exponential Tilting</h3>
<ul>
<li><strong>Authors: </strong>Xuchen Gong, Tian Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16157">https://arxiv.org/abs/2510.16157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16157">https://arxiv.org/pdf/2510.16157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16157]] Zeroth-Order Sharpness-Aware Learning with Exponential Tilting(https://arxiv.org/abs/2510.16157)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Classic zeroth-order optimization approaches typically optimize for a smoothed version of the original function, i.e., the expected objective under randomly perturbed model parameters. This can be interpreted as encouraging the loss values in the perturbation set to be small on average. Popular sharpness-aware minimization (SAM) objectives, however, typically focus on the largest loss within the neighborhood to arrive at flat minima more effectively. In this work, we connect zeroth-order optimization (and its corresponding objectives) with SAM approaches explicitly, through an exponential tilting objective that provides a smooth transition between the average- and the max-loss formulations. We explore new zeroth-order algorithms to solve a soft SAM objective parameterized by a tilting parameter $t$. We provide precise characterizations of the sharpness notions of the tilted SAM framework. Practically, our approach can be used as a gradient-free and memory-efficient alternative to SAM variants, and it achieves better generalization compared to vanilla zeroth-order baselines on a wide range of downstream tasks, including classification, multiple choice QA, and language generation.</li>
<li><strong>摘要：</strong>经典的零阶优化方法通常针对原始函数的平滑版本进行优化，即随机扰动模型参数下的预期目标。这可以解释为鼓励扰动集中的损失值平均较小。然而，流行的锐度感知最小化（SAM）目标通常关注邻域内的最大损失，以更有效地达到平坦最小值。在这项工作中，我们通过指数倾斜目标明确地将零阶优化（及其相应的目标）与 SAM 方法联系起来，该目标提供平均损失公式和最大损失公式之间的平滑过渡。我们探索新的零阶算法来求解由倾斜参数 $t$ 参数化的软 SAM 目标。我们提供倾斜 SAM 框架锐度概念的精确表征。实际上，我们的方法可以用作 SAM 变体的无梯度且内存高效的替代方案，并且与普通零阶基线相比，它在各种下游任务（包括分类、多项选择 QA 和语言生成）上实现了更好的泛化。</li>
</ul>

<h3>Title: AtomBench: A Benchmark for Generative Atomic Structure Models using GPT, Diffusion, and Flow Architectures</h3>
<ul>
<li><strong>Authors: </strong>Charles Rhys Campbell, Aldo H. Romero, Kamal Choudhary</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.supr-con</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16165">https://arxiv.org/abs/2510.16165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16165">https://arxiv.org/pdf/2510.16165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16165]] AtomBench: A Benchmark for Generative Atomic Structure Models using GPT, Diffusion, and Flow Architectures(https://arxiv.org/abs/2510.16165)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models have become significant assets in the exploration and identification of new materials, enabling the rapid proposal of candidate crystal structures that satisfy target properties. Despite the increasing adoption of diverse architectures, a rigorous comparative evaluation of their performance on materials datasets is lacking. In this work, we present a systematic benchmark of three representative generative models- AtomGPT (a transformer-based model), Crystal Diffusion Variational Autoencoder (CDVAE), and FlowMM (a Riemannian flow matching model). These models were trained to reconstruct crystal structures from subsets of two publicly available superconductivity datasets- JARVIS Supercon 3D and DS A/B from the Alexandria database. Performance was assessed using the Kullback-Leibler (KL) divergence between predicted and reference distributions of lattice parameters, as well as the mean absolute error (MAE) of individual lattice constants. For the computed KLD and MAE scores, CDVAE performs most favorably, followed by AtomGPT, and then FlowMM. All benchmarking code and model configurations will be made publicly available at this https URL.</li>
<li><strong>摘要：</strong>生成模型已成为探索和识别新材料的重要资产，能够快速提出满足目标特性的候选晶体结构。尽管越来越多地采用不同的架构，但仍缺乏对其在材料数据集上的性能进行严格的比较评估。在这项工作中，我们提出了三种代表性生成模型的系统基准——AtomGPT（基于变压器的模型）、晶体扩散变分自动编码器（CDVAE）和 FlowMM（黎曼流匹配模型）。这些模型经过训练，可以根据两个公开可用的超导数据集（来自 Alexandria 数据库的 JARVIS Supercon 3D 和 DS A/B）的子集重建晶体结构。使用晶格参数的预测分布和参考分布之间的 Kullback-Leibler (KL) 散度以及各个晶格常数的平均绝对误差 (MAE) 来评估性能。对于计算的 KLD 和 MAE 分数，CDVAE 表现最好，其次是 AtomGPT，然后是 FlowMM。所有基准测试代码和模型配置都将在此 https URL 上公开提供。</li>
</ul>

<h3>Title: Cost Savings from Automatic Quality Assessment of Generated Images</h3>
<ul>
<li><strong>Authors: </strong>Xavier Giro-i-Nieto, Nefeli Andreou, Anqi Liang, Manel Baradad, Francesc Moreno-Noguer, Aleix Martinez</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16179">https://arxiv.org/abs/2510.16179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16179">https://arxiv.org/pdf/2510.16179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16179]] Cost Savings from Automatic Quality Assessment of Generated Images(https://arxiv.org/abs/2510.16179)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, quality assessment</a></li>
<li><strong>Abstract: </strong>Deep generative models have shown impressive progress in recent years, making it possible to produce high quality images with a simple text prompt or a reference image. However, state of the art technology does not yet meet the quality standards offered by traditional photographic methods. For this reason, production pipelines that use generated images often include a manual stage of image quality assessment (IQA). This process is slow and expensive, especially because of the low yield of automatically generated images that pass the quality bar. The IQA workload can be reduced by introducing an automatic pre-filtering stage, that will increase the overall quality of the images sent to review and, therefore, reduce the average cost required to obtain a high quality image. We present a formula that estimates the cost savings depending on the precision and pass yield of a generic IQA engine. This formula is applied in a use case of background inpainting, showcasing a significant cost saving of 51.61% obtained with a simple AutoML solution.</li>
<li><strong>摘要：</strong>近年来，深度生成模型取得了令人瞩目的进展，使得通过简单的文本提示或参考图像生成高质量图像成为可能。然而，最先进的技术尚未达到传统摄影方法提供的质量标准。因此，使用生成图像的生产流程通常包括图像质量评估 (IQA) 的手动阶段。这个过程缓慢且昂贵，特别是因为自动生成的通过质量标准的图像产量较低。通过引入自动预过滤阶段可以减少 IQA 工作量，这将提高发送审查的图像的整体质量，从而降低获得高质量图像所需的平均成本。我们提出了一个公式，根据通用 IQA 引擎的精度和通过率来估算成本节省。该公式应用于背景修复用例，显示通过简单的 AutoML 解决方案可显着节省 51.61% 的成本。</li>
</ul>

<h3>Title: Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI</h3>
<ul>
<li><strong>Authors: </strong>Zheng Huang, Enpei Zhang, Yinghao Cai, Weikang Qiu, Carl Yang, Elynn Chen, Xiang Zhang, Rex Ying, Dawei Zhou, Yujun Yan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16196">https://arxiv.org/abs/2510.16196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16196">https://arxiv.org/pdf/2510.16196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16196]] Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI(https://arxiv.org/abs/2510.16196)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Understanding how the brain encodes visual information is a central challenge in neuroscience and machine learning. A promising approach is to reconstruct visual stimuli, essentially images, from functional Magnetic Resonance Imaging (fMRI) signals. This involves two stages: transforming fMRI signals into a latent space and then using a pretrained generative model to reconstruct images. The reconstruction quality depends on how similar the latent space is to the structure of neural activity and how well the generative model produces images from that space. Yet, it remains unclear which type of latent space best supports this transformation and how it should be organized to represent visual stimuli effectively. We present two key findings. First, fMRI signals are more similar to the text space of a language model than to either a vision based space or a joint text image space. Second, text representations and the generative model should be adapted to capture the compositional nature of visual stimuli, including objects, their detailed attributes, and relationships. Building on these insights, we propose PRISM, a model that Projects fMRI sIgnals into a Structured text space as an interMediate representation for visual stimuli reconstruction. It includes an object centric diffusion module that generates images by composing individual objects to reduce object detection errors, and an attribute relationship search module that automatically identifies key attributes and relationships that best align with the neural activity. Extensive experiments on real world datasets demonstrate that our framework outperforms existing methods, achieving up to an 8% reduction in perceptual loss. These results highlight the importance of using structured text as the intermediate space to bridge fMRI signals and image reconstruction.</li>
<li><strong>摘要：</strong>了解大脑如何编码视觉信息是神经科学和机器学习的核心挑战。一种有前途的方法是从功能磁共振成像（fMRI）信号中重建视觉刺激，本质上是图像。这涉及两个阶段：将功能磁共振成像信号转换为潜在空间，然后使用预训练的生成模型来重建图像。重建质量取决于潜在空间与神经活动结构的相似程度以及生成模型从该空间生成图像的效果。然而，目前尚不清楚哪种类型的潜在空间最能支持这种转变，以及如何组织它以有效地表示视觉刺激。我们提出了两个关键发现。首先，fMRI 信号与语言模型的文本空间比基于视觉的空间或联合文本图像空间更相似。其次，文本表示和生成模型应该适应捕捉视觉刺激的组成性质，包括对象、它们的详细属性和关系。基于这些见解，我们提出了 PRISM，这是一种将功能磁共振成像信号投射到结构化文本空间中的模型，作为视觉刺激重建的中间表示。它包括一个以对象为中心的扩散模块，该模块通过组合单个对象来生成图像以减少对象检测错误，以及一个属性关系搜索模块，该模块可自动识别与神经活动最相符的关键属性和关系。对现实世界数据集的大量实验表明，我们的框架优于现有方法，感知损失最多减少 8%。这些结果凸显了使用结构化文本作为中间空间来连接 fMRI 信号和图像重建的重要性。</li>
</ul>

<h3>Title: Scaffold-Aware Generative Augmentation and Reranking for Enhanced Virtual Screening</h3>
<ul>
<li><strong>Authors: </strong>Xin Wang, Yu Wang, Yunchao Liu, Jens Meiler, Tyler Derr</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16306">https://arxiv.org/abs/2510.16306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16306">https://arxiv.org/pdf/2510.16306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16306]] Scaffold-Aware Generative Augmentation and Reranking for Enhanced Virtual Screening(https://arxiv.org/abs/2510.16306)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Ligand-based virtual screening (VS) is an essential step in drug discovery that evaluates large chemical libraries to identify compounds that potentially bind to a therapeutic target. However, VS faces three major challenges: class imbalance due to the low active rate, structural imbalance among active molecules where certain scaffolds dominate, and the need to identify structurally diverse active compounds for novel drug development. We introduce ScaffAug, a scaffold-aware VS framework that addresses these challenges through three modules. The augmentation module first generates synthetic data conditioned on scaffolds of actual hits using generative AI, specifically a graph diffusion model. This helps mitigate the class imbalance and furthermore the structural imbalance, due to our proposed scaffold-aware sampling algorithm, designed to produce more samples for active molecules with underrepresented scaffolds. A model-agnostic self-training module is then used to safely integrate the generated synthetic data from our augmentation module with the original labeled data. Lastly, we introduce a reranking module that improves VS by enhancing scaffold diversity in the top recommended set of molecules, while still maintaining and even enhancing the overall general performance of identifying novel, active compounds. We conduct comprehensive computational experiments across five target classes, comparing ScaffAug against existing baseline methods by reporting the performance of multiple evaluation metrics and performing ablation studies on ScaffAug. Overall, this work introduces novel perspectives on effectively enhancing VS by leveraging generative augmentations, reranking, and general scaffold-awareness.</li>
<li><strong>摘要：</strong>基于配体的虚拟筛选（VS）是药物发现中的一个重要步骤，它评估大型化学库以识别可能与治疗靶点结合的化合物。然而，VS 面临三大挑战：活性率低导致的类别不平衡、活性分子之间的结构不平衡（其中某些支架占主导地位）以及需要识别结构多样的活性化合物以用于新药开发。我们引入了 ScaffAug，这是一个脚手架感知的 VS 框架，它通过三个模块解决这些挑战。增强模块首先使用生成式人工智能（特别是图扩散模型）生成以实际命中的支架为条件的合成数据。由于我们提出的支架感知采样算法，这有助于减轻类别不平衡以及结构不平衡，该算法旨在为具有代表性不足的支架的活性分子产生更多样本。然后使用与模型无关的自训练模块将增强模块生成的合成数据与原始标记数据安全地集成。最后，我们引入了一个重新排序模块，该模块通过增强最推荐的分子组中的支架多样性来提高 VS，同时仍然保持甚至增强识别新型活性化合物的整体一般性能。我们在五个目标类别中进行了全面的计算实验，通过报告多个评估指标的性能并在 ScaffAug 上进行消融研究，将 ScaffAug 与现有基线方法进行比较。总的来说，这项工作引入了通过利用生成增强、重新排名和一般脚手架意识来有效增强 VS 的新颖视角。</li>
</ul>

<h3>Title: Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation</h3>
<ul>
<li><strong>Authors: </strong>Rui Yang, Huining Li, Yiyi Long, Xiaojun Wu, Shengfeng He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16319">https://arxiv.org/abs/2510.16319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16319">https://arxiv.org/pdf/2510.16319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16319]] Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation(https://arxiv.org/abs/2510.16319)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating sketches guided by reference styles requires precise transfer of stroke attributes, such as line thickness, deformation, and texture sparsity, while preserving semantic structure and content fidelity. To this end, we propose Stroke2Sketch, a novel training-free framework that introduces cross-image stroke attention, a mechanism embedded within self-attention layers to establish fine-grained semantic correspondences and enable accurate stroke attribute transfer. This allows our method to adaptively integrate reference stroke characteristics into content images while maintaining structural integrity. Additionally, we develop adaptive contrast enhancement and semantic-focused attention to reinforce content preservation and foreground emphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches that closely resemble handcrafted results, outperforming existing methods in expressive stroke control and semantic coherence. Codes are available at this https URL.</li>
<li><strong>摘要：</strong>根据参考样式生成草图需要精确传输笔画属性，例如线条粗细、变形和纹理稀疏性，同时保留语义结构和内容保真度。为此，我们提出了 Stroke2Sketch，这是一种新颖的免训练框架，它引入了跨图像笔划注意力，这是一种嵌入自注意力层中的机制，用于建立细粒度的语义对应并实现准确的笔划属性转移。这使得我们的方法能够自适应地将参考笔画特征集成到内容图像中，同时保持结构完整性。此外，我们开发了自适应对比度增强和语义关注，以加强内容保留和前景强调。 Stroke2Sketch 有效地合成了风格上忠实的草图，与手工制作的结果非常相似，在表现力笔画控制和语义连贯性方面优于现有方法。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Scaling Laws for Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Wang, Longqi Cai, Taihong Xiao, Yuxiao Wang, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16320">https://arxiv.org/abs/2510.16320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16320">https://arxiv.org/pdf/2510.16320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16320]] Scaling Laws for Deepfake Detection(https://arxiv.org/abs/2510.16320)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper presents a systematic study of scaling laws for the deepfake detection task. Specifically, we analyze the model performance against the number of real image domains, deepfake generation methods, and training images. Since no existing dataset meets the scale requirements for this research, we construct ScaleDF, the largest dataset to date in this field, which contains over 5.8 million real images from 51 different datasets (domains) and more than 8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we observe power-law scaling similar to that shown in large language models (LLMs). Specifically, the average detection error follows a predictable power-law decay as either the number of real domains or the number of deepfake methods increases. This key observation not only allows us to forecast the number of additional real domains or deepfake methods required to reach a target performance, but also inspires us to counter the evolving deepfake technology in a data-centric manner. Beyond this, we examine the role of pre-training and data augmentations in deepfake detection under scaling, as well as the limitations of scaling itself.</li>
<li><strong>摘要：</strong>本文对深度伪造检测任务的缩放定律进行了系统研究。具体来说，我们根据真实图像域的数量、深度伪造生成方法和训练图像来分析模型性能。由于现有数据集无法满足本研究的规模要求，因此我们构建了该领域迄今为止最大的数据集 ScaleDF，其中包含来自 51 个不同数据集（域）的超过 580 万张真实图像以及由 102 种 Deepfake 方法生成的超过 880 万张假图像。使用 ScaleDF，我们观察到类似于大型语言模型 (LLM) 中显示的幂律缩放。具体来说，随着真实域数量或深度伪造方法数量的增加，平均检测误差遵循可预测的幂律衰减。这一关键观察不仅使我们能够预测达到目标性能所需的额外真实域或深度伪造方法的数量，而且激励我们以数据为中心的方式对抗不断发展的深度伪造技术。除此之外，我们还研究了预训练和数据增强在缩放下的深度伪造检测中的作用，以及缩放本身的局限性。</li>
</ul>

<h3>Title: Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention</h3>
<ul>
<li><strong>Authors: </strong>Yuyao Zhang, Yu-Wing Tai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16325">https://arxiv.org/abs/2510.16325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16325">https://arxiv.org/pdf/2510.16325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16325]] Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention(https://arxiv.org/abs/2510.16325)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Ultra-high-resolution text-to-image generation demands both fine-grained texture synthesis and globally coherent structure, yet current diffusion models remain constrained to sub-$1K \times 1K$ resolutions due to the prohibitive quadratic complexity of attention and the scarcity of native $4K$ training data. We present \textbf{Scale-DiT}, a new diffusion framework that introduces hierarchical local attention with low-resolution global guidance, enabling efficient, scalable, and semantically coherent image synthesis at ultra-high resolutions. Specifically, high-resolution latents are divided into fixed-size local windows to reduce attention complexity from quadratic to near-linear, while a low-resolution latent equipped with scaled positional anchors injects global semantics. A lightweight LoRA adaptation bridges global and local pathways during denoising, ensuring consistency across structure and detail. To maximize inference efficiency, we repermute token sequence in Hilbert curve order and implement a fused-kernel for skipping masked operations, resulting in a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT achieves more than $2\times$ faster inference and lower memory usage compared to dense attention baselines, while reliably scaling to $4K \times 4K$ resolution without requiring additional high-resolution training data. On both quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons, Scale-DiT delivers superior global coherence and sharper local detail, matching or outperforming state-of-the-art methods that rely on native 4K training. Taken together, these results highlight hierarchical local attention with guided low-resolution anchors as a promising and effective approach for advancing ultra-high-resolution image generation.</li>
<li><strong>摘要：</strong>超高分辨率文本到图像的生成需要细粒度的纹理合成和全局连贯的结构，但由于注意力的二次复杂度过高以及原生 4K 美元训练数据的稀缺，当前的扩散模型仍然局限于低于 1K 乘以 1K 的分辨率。我们提出了 \textbf{Scale-DiT}，一种新的扩散框架，它引入了具有低分辨率全局指导的分层局部注意力，从而实现了超高分辨率下的高效、可扩展和语义连贯的图像合成。具体来说，高分辨率潜在模型被划分为固定大小的局部窗口，以将注意力复杂度从二次降低到近线性，而配备缩放位置锚的低分辨率潜在模型则注入全局语义。轻量级 LoRA 适配在去噪过程中连接全局和局部路径，确保结构和细节的一致性。为了最大限度地提高推理效率，我们按照希尔伯特曲线顺序重新排列标记序列，并实现融合内核来跳过屏蔽操作，从而实现 GPU 友好的设计。大量实验表明，与密集注意力基线相比，Scale-DiT 的推理速度提高了 2 倍以上，内存使用量更低，同时可靠地扩展到 4K 倍 4K 分辨率，无需额外的高分辨率训练数据。在定量基准（FID、IS、CLIP 分数）和定性比较方面，Scale-DiT 提供了卓越的全局一致性和更清晰的局部细节，匹配或优于依赖于原生 4K 训练的最先进方法。总而言之，这些结果强调了引导低分辨率锚点的分层局部注意力是推进超高分辨率图像生成的一种有前途且有效的方法。</li>
</ul>

<h3>Title: DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution</h3>
<ul>
<li><strong>Authors: </strong>Yi Wei, Shunpu Tang, Liang Zhao, Qiangian Yang (College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16326">https://arxiv.org/abs/2510.16326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16326">https://arxiv.org/pdf/2510.16326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16326]] DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution(https://arxiv.org/abs/2510.16326)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models have driven remarkable progress in image generation. However, the generation process remains computationally intensive, and users often need to iteratively refine prompts to achieve the desired results, further increasing latency and placing a heavy burden on cloud resources. To address this challenge, we propose DiffusionX, a cloud-edge collaborative framework for efficient multi-round, prompt-based generation. In this system, a lightweight on-device diffusion model interacts with users by rapidly producing preview images, while a high-capacity cloud model performs final refinements after the prompt is finalized. We further introduce a noise level predictor that dynamically balances the computation load, optimizing the trade-off between latency and cloud workload. Experiments show that DiffusionX reduces average generation time by 15.8% compared with Stable Diffusion v1.5, while maintaining comparable image quality. Moreover, it is only 0.9% slower than Tiny-SD with significantly improved image quality, thereby demonstrating efficiency and scalability with minimal overhead.</li>
<li><strong>摘要：</strong>扩散模型的最新进展推动了图像生成的显着进步。然而，生成过程仍然是计算密集型的，用户常常需要迭代地完善提示才能达到预期的结果，这进一步增加了延迟，给云资源带来了沉重的负担。为了应对这一挑战，我们提出了 DiffusionX，这是一种云边缘协作框架，用于高效的多轮、基于提示的生成。在该系统中，轻量级的设备端扩散模型通过快速生成预览图像与用户交互，而大容量的云模型在提示完成后执行最终的细化。我们进一步引入了一个噪声水平预测器，可以动态平衡计算负载，优化延迟和云工作负载之间的权衡。实验表明，与 Stable Diffusion v1.5 相比，DiffusionX 平均生成时间减少了 15.8%，同时保持了相当的图像质量。此外，它仅比 Tiny-SD 慢 0.9%，并且图像质量显着提高，从而以最小的开销展示了效率和可扩展性。</li>
</ul>

<h3>Title: TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement</h3>
<ul>
<li><strong>Authors: </strong>Haiyue Sun, Qingdong He, Jinlong Peng, Peng Tang, Jiangning Zhang, Junwei Zhu, Xiaobin Hu, Shuicheng Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16332">https://arxiv.org/abs/2510.16332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16332">https://arxiv.org/pdf/2510.16332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16332]] TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement(https://arxiv.org/abs/2510.16332)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autoregressive Model (AR) has shown remarkable success in conditional image generation. However, these approaches for multiple reference generation struggle with decoupling different reference identities. In this work, we propose the TokenAR framework, specifically focused on a simple but effective token-level enhancement mechanism to address reference identity confusion problem. Such token-level enhancement consists of three parts, 1). Token Index Embedding clusters the tokens index for better representing the same reference images; 2). Instruct Token Injection plays as a role of extra visual feature container to inject detailed and complementary priors for reference tokens; 3). The identity-token disentanglement strategy (ITD) explicitly guides the token representations toward independently representing the features of each this http URL token-enhancement framework significantly augments the capabilities of existing AR based methods in conditional image generation, enabling good identity consistency while preserving high quality background reconstruction. Driven by the goal of high-quality and high-diversity in multi-subject generation, we introduce the InstructAR Dataset, the first open-source, large-scale, multi-reference input, open domain image generation dataset that includes 28K training pairs, each example has two reference subjects, a relative prompt and a background with mask annotation, curated for multiple reference image generation training and evaluating. Comprehensive experiments validate that our approach surpasses current state-of-the-art models in multiple reference image generation task. The implementation code and datasets will be made publicly. Codes are available, see this https URL</li>
<li><strong>摘要：</strong>自回归模型（AR）在条件图像生成方面取得了显着的成功。然而，这些用于多参考生成的方法很难解耦不同的参考身份。在这项工作中，我们提出了 TokenAR 框架，特别关注一种简单但有效的令牌级增强机制来解决参考身份混淆问题。这种令牌级别的增强由三部分组成，1)。令牌索引嵌入对令牌索引进行聚类，以更好地表示相同的参考图像； 2）。指示令牌注入充当额外的视觉特征容器的角色，为参考令牌注入详细和补充的先验； 3）。身份令牌解缠策略 (ITD) 明确引导令牌表示独立地表示每个 http URL 令牌增强框架的特征，显着增强了现有基于 AR 的方法在条件图像生成方面的能力，实现了良好的身份一致性，同时保留了高质量的背景重建。在多主题生成的高质量和高多样性目标的驱动下，我们引入了 InstructAR 数据集，这是第一个开源、大规模、多参考输入、开放域图像生成数据集，其中包括 28K 训练对，每个示例都有两个参考主题、一个相对提示和一个带掩模注释的背景，专为多参考图像生成训练和评估而设计。综合实验验证了我们的方法在多参考图像生成任务中超越了当前最先进的模型。实施代码和数据集将公开。代码可用，请参阅此 https URL</li>
</ul>

<h3>Title: Sparse Transformer Architectures via Regularized Wasserstein Proximal Operator with $L_1$ Prior</h3>
<ul>
<li><strong>Authors: </strong>Fuqun Han, Stanley Osher, Wuchen Li</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16356">https://arxiv.org/abs/2510.16356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16356">https://arxiv.org/pdf/2510.16356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16356]] Sparse Transformer Architectures via Regularized Wasserstein Proximal Operator with $L_1$ Prior(https://arxiv.org/abs/2510.16356)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this work, we propose a sparse transformer architecture that incorporates prior information about the underlying data distribution directly into the transformer structure of the neural network. The design of the model is motivated by a special optimal transport problem, namely the regularized Wasserstein proximal operator, which admits a closed-form solution and turns out to be a special representation of transformer architectures. Compared with classical flow-based models, the proposed approach improves the convexity properties of the optimization problem and promotes sparsity in the generated samples. Through both theoretical analysis and numerical experiments, including applications in generative modeling and Bayesian inverse problems, we demonstrate that the sparse transformer achieves higher accuracy and faster convergence to the target distribution than classical neural ODE-based methods.</li>
<li><strong>摘要：</strong>在这项工作中，我们提出了一种稀疏变压器架构，它将有关底层数据分布的先验信息直接合并到神经网络的变压器结构中。该模型的设计是由一个特殊的最优传输问题驱动的，即正则化 Wasserstein 近端算子，它允许封闭形式的解决方案，并且结果是变压器架构的特殊表示。与经典的基于流的模型相比，所提出的方法改善了优化问题的凸性特性并促进了生成样本的稀疏性。通过理论分析和数值实验，包括在生成建模和贝叶斯反问题中的应用，我们证明稀疏变换器比基于经典神经常微分方程的方法具有更高的精度和更快地收敛到目标分布。</li>
</ul>

<h3>Title: iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance</h3>
<ul>
<li><strong>Authors: </strong>Rishi Raj Sahoo, Surbhi Saswati Mohanty, Subhankar Mishra</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16375">https://arxiv.org/abs/2510.16375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16375">https://arxiv.org/pdf/2510.16375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16375]] iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance(https://arxiv.org/abs/2510.16375)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Road potholes pose significant safety hazards and maintenance challenges, particularly on India's diverse and under-maintained road networks. This paper presents iWatchRoadv2, a fully automated end-to-end platform for real-time pothole detection, GPS-based geotagging, and dynamic road health visualization using OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000 dashcam frames capturing diverse Indian road conditions, weather patterns, and lighting scenarios, which we used to fine-tune the Ultralytics YOLO model for accurate pothole detection. The system synchronizes OCR-extracted video timestamps with external GPS logs to precisely geolocate each detected pothole, enriching detections with comprehensive metadata, including road segment attribution and contractor information managed through an optimized backend database. iWatchRoadv2 introduces intelligent governance features that enable authorities to link road segments with contract metadata through a secure login interface. The system automatically sends alerts to contractors and officials when road health deteriorates, supporting automated accountability and warranty enforcement. The intuitive web interface delivers actionable analytics to stakeholders and the public, facilitating evidence-driven repair planning, budget allocation, and quality assessment. Our cost-effective and scalable solution streamlines frame processing and storage while supporting seamless public engagement for urban and rural deployments. By automating the complete pothole monitoring lifecycle, from detection to repair verification, iWatchRoadv2 enables data-driven smart city management, transparent governance, and sustainable improvements in road infrastructure maintenance. The platform and live demonstration are accessible at this https URL.</li>
<li><strong>摘要：</strong>道路坑洼带来了重大的安全隐患和维护挑战，特别是在印度多样化且维护不足的道路网络上。本文介绍了 iWatchRoadv2，这是一个全自动端到端平台，用于使用 OpenStreetMap (OSM) 进行实时坑洞检测、基于 GPS 的地理标记和动态道路健康可视化。我们整理了一个由 7,000 多个行车记录仪帧组成的自注释数据集，捕获了不同的印度路况、天气模式和照明场景，我们用它来微调 Ultralytics YOLO 模型，以实现准确的坑洞检测。该系统将 OCR 提取的视频时间戳与外部 GPS 日志同步，以精确定位每个检测到的坑洼，通过全面的元数据丰富检测，包括通过优化的后端数据库管理的路段属性和承包商信息。 iWatchRoadv2 引入了智能治理功能，使当局能够通过安全登录界面将路段与合同元数据链接起来。当道路健康状况恶化时，系统会自动向承包商和官员发送警报，支持自动问责和保修执行。直观的网络界面为利益相关者和公众提供可操作的分析，促进证据驱动的维修计划、预算分配和质量评估。我们经济高效且可扩展的解决方案简化了帧处理和存储，同时支持城市和农村部署的无缝公众参与。通过自动化从检测到修复验证的整个坑洞监测生命周期，iWatchRoadv2 实现了数据驱动的智能城市管理、透明治理以及道路基础设施维护的可持续改进。可通过此 https URL 访问该平台和现场演示。</li>
</ul>

<h3>Title: Demeter: A Parametric Model of Crop Plant Morphology from the Real World</h3>
<ul>
<li><strong>Authors: </strong>Tianhang Cheng, Albert J. Zhai, Evan Z. Chen, Rui Zhou, Yawen Deng, Zitong Li, Kejie Zhao, Janice Shiu, Qianyu Zhao, Yide Xu, Xinlei Wang, Yuan Shen, Sheng Wang, Lisa Ainsworth, Kaiyu Guan, Shenlong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16377">https://arxiv.org/abs/2510.16377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16377">https://arxiv.org/pdf/2510.16377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16377]] Demeter: A Parametric Model of Crop Plant Morphology from the Real World(https://arxiv.org/abs/2510.16377)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Learning 3D parametric shape models of objects has gained popularity in vision and graphics and has showed broad utility in 3D reconstruction, generation, understanding, and simulation. While powerful models exist for humans and animals, equally expressive approaches for modeling plants are lacking. In this work, we present Demeter, a data-driven parametric model that encodes key factors of a plant morphology, including topology, shape, articulation, and deformation into a compact learned representation. Unlike previous parametric models, Demeter handles varying shape topology across various species and models three sources of shape variation: articulation, subcomponent shape variation, and non-rigid deformation. To advance crop plant modeling, we collected a large-scale, ground-truthed dataset from a soybean farm as a testbed. Experiments show that Demeter effectively synthesizes shapes, reconstructs structures, and simulates biophysical processes. Code and data is available at this https URL.</li>
<li><strong>摘要：</strong>学习物体的 3D 参数化形状模型在视觉和图形领域越来越受欢迎，并在 3D 重建、生成、理解和模拟中显示出广泛的实用性。尽管存在针对人类和动物的强大模型，但缺乏同样具有表现力的植物建模方法。在这项工作中，我们提出了 Demeter，一种数据驱动的参数模型，它将植物形态的关键因素（包括拓扑、形状、关节和变形）编码为紧凑的学习表示。与以前的参数化模型不同，Demeter 处理不同物种的不同形状拓扑，并对形状变化的三种来源进行建模：关节、子组件形状变化和非刚性变形。为了推进农作物建模，我们从大豆农场收集了大规模的真实数据集作为测试平台。实验表明，Demeter 可以有效地合成形状、重建结构和模拟生物物理过程。代码和数据可从此 https URL 获取。</li>
</ul>

<h3>Title: Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution</h3>
<ul>
<li><strong>Authors: </strong>Dimitris Stefanopoulos, Andreas Voskou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16443">https://arxiv.org/abs/2510.16443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16443">https://arxiv.org/pdf/2510.16443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16443]] Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution(https://arxiv.org/abs/2510.16443)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This report presents the winning solution for Task 2 of Colliding with Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at ECML-PKDD 2025. The goal of the challenge was to design and train a robust ANN-based model capable of achieving high accuracy in a binary classification task on both clean and adversarial data generated with the Random Distribution Shuffle Attack (RDSA). Our solution consists of two components: a data generation phase and a robust model training phase. In the first phase, we produced 15 million artificial training samples using a custom methodology derived from Random Distribution Shuffle Attack (RDSA). In the second phase, we introduced a robust architecture comprising (i)a Feature Embedding Block with shared weights among features of the same type and (ii)a Dense Fusion Tail responsible for the final prediction. Training this architecture on our adversarial dataset achieved a mixed accuracy score of 80\%, exceeding the second-place solution by two percentage points.</li>
<li><strong>摘要：</strong>本报告介绍了“与对手碰撞”任务 2 的获胜解决方案：ECML-PKDD 2025 上高能物理发现中的鲁棒学习挑战。该挑战的目标是设计和训练一个鲁棒的基于 ANN 的模型，该模型能够在随机分布洗牌攻击 (RDSA) 生成的干净数据和对抗性数据的二元分类任务中实现高精度。我们的解决方案由两个部分组成：数据生成阶段和稳健的模型训练阶段。在第一阶段，我们使用源自随机分布洗牌攻击 (RDSA) 的自定义方法生成了 1500 万个人工训练样本。在第二阶段，我们引入了一个鲁棒的架构，包括（i）在相同类型的特征之间具有共享权重的特征嵌入块和（ii）负责最终预测的密集融合尾部。在我们的对抗数据集上训练该架构，混合准确度得分达到 80%，比第二名的解决方案高出两个百分点。</li>
</ul>

<h3>Title: Fit for Purpose? Deepfake Detection in the Real World</h3>
<ul>
<li><strong>Authors: </strong>Guangyu Lin, Li Lin, Christina P. Walker, Daniel S. Schiff, Shu Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16556">https://arxiv.org/abs/2510.16556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16556">https://arxiv.org/pdf/2510.16556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16556]] Fit for Purpose? Deepfake Detection in the Real World(https://arxiv.org/abs/2510.16556)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid proliferation of AI-generated content, driven by advances in generative adversarial networks, diffusion models, and multimodal large language models, has made the creation and dissemination of synthetic media effortless, heightening the risks of misinformation, particularly political deepfakes that distort truth and undermine trust in political institutions. In turn, governments, research institutions, and industry have strongly promoted deepfake detection initiatives as solutions. Yet, most existing models are trained and validated on synthetic, laboratory-controlled datasets, limiting their generalizability to the kinds of real-world political deepfakes circulating on social platforms that affect the public. In this work, we introduce the first systematic benchmark based on the Political Deepfakes Incident Database, a curated collection of real-world political deepfakes shared on social media since 2018. Our study includes a systematic evaluation of state-of-the-art deepfake detectors across academia, government, and industry. We find that the detectors from academia and government perform relatively poorly. While paid detection tools achieve relatively higher performance than free-access models, all evaluated detectors struggle to generalize effectively to authentic political deepfakes, and are vulnerable to simple manipulations, especially in the video domain. Results urge the need for politically contextualized deepfake detection frameworks to better safeguard the public in real-world settings.</li>
<li><strong>摘要：</strong>在生成对抗网络、传播模型和多模态大语言模型的进步推动下，人工智能生成的内容迅速扩散，使得合成媒体的创建和传播变得毫不费力，从而增加了错误信息的风险，特别是歪曲真相并破坏对政治机构信任的政治深度造假的风险。反过来，政府、研究机构和行业也大力推广深度换脸检测举措作为解决方案。然而，大多数现有模型都是在实验室控制的合成数据集上进行训练和验证的，这限制了它们对社交平台上传播的影响公众的现实世界政治深度造假的普遍性。在这项工作中，我们引入了第一个基于政治 Deepfakes 事件数据库的系统基准，该数据库是自 2018 年以来在社交媒体上分享的现实世界政治 Deepfakes 的精选集合。我们的研究包括对学术界、政府和行业最先进的 Deepfake 探测器的系统评估。我们发现学术界和政府的探测器表现相对较差。虽然付费检测工具比免费模型获得了相对更高的性能，但所有经过评估的检测器都难以有效地推广到真实的政治深度伪造品，并且容易受到简单的操纵，尤其是在视频领域。研究结果表明，需要建立政治背景下的深度伪造检测框架，以更好地保护现实世界中的公众。</li>
</ul>

<h3>Title: Copy-Augmented Representation for Structure Invariant Template-Free Retrosynthesis</h3>
<ul>
<li><strong>Authors: </strong>Jiaxi Zhuang, Yu Zhang, Aimin Zhou, Ying Qian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16588">https://arxiv.org/abs/2510.16588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16588">https://arxiv.org/pdf/2510.16588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16588]] Copy-Augmented Representation for Structure Invariant Template-Free Retrosynthesis(https://arxiv.org/abs/2510.16588)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Retrosynthesis prediction is fundamental to drug discovery and chemical synthesis, requiring the identification of reactants that can produce a target molecule. Current template-free methods struggle to capture the structural invariance inherent in chemical reactions, where substantial molecular scaffolds remain unchanged, leading to unnecessarily large search spaces and reduced prediction accuracy. We introduce C-SMILES, a novel molecular representation that decomposes traditional SMILES into element-token pairs with five special tokens, effectively minimizing editing distance between reactants and products. Building upon this representation, we incorporate a copy-augmented mechanism that dynamically determines whether to generate new tokens or preserve unchanged molecular fragments from the product. Our approach integrates SMILES alignment guidance to enhance attention consistency with ground-truth atom mappings, enabling more chemically coherent predictions. Comprehensive evaluation on USPTO-50K and large-scale USPTO-FULL datasets demonstrates significant improvements: 67.2% top-1 accuracy on USPTO-50K and 50.8% on USPTO-FULL, with 99.9% validity in generated molecules. This work establishes a new paradigm for structure-aware molecular generation with direct applications in computational drug discovery.</li>
<li><strong>摘要：</strong>逆合成预测是药物发现和化学合成的基础，需要识别可以产生目标分子的反应物。目前的无模板方法很难捕获化学反应中固有的结构不变性，其中大量分子支架保持不变，导致不必要的大搜索空间并降低预测准确性。我们引入了 C-SMILES，一种新颖的分子表示形式，它将传统的 SMILES 分解为具有五个特殊标记的元素标记对，有效地最小化了反应物和产物之间的编辑距离。在此表示的基础上，我们采用了复制增强机制，该机制可以动态确定是否生成新令牌或保留产品中未更改的分子片段。我们的方法集成了 SMILES 对齐指导，以增强注意力与真实原子映射的一致性，从而实现化学上更一致的预测。对 USPTO-50K 和大规模 USPTO-FULL 数据集的综合评估显示出显着的改进：USPTO-50K 上的 top-1 准确度为 67.2%，USPTO-FULL 上的 top-1 准确度为 50.8%，生成的分子的有效性为 99.9%。这项工作为结构感知分子生成建立了一个新的范例，可直接应用于计算药物发现。</li>
</ul>

<h3>Title: Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations</h3>
<ul>
<li><strong>Authors: </strong>Cassidy Ashworth, Pietro Liò, Francesco Caso</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.stat-mech, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16591">https://arxiv.org/abs/2510.16591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16591">https://arxiv.org/pdf/2510.16591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16591]] Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations(https://arxiv.org/abs/2510.16591)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Deep learning models have proven enormously successful at using multiple layers of representation to learn relevant features of structured data. Encoding physical symmetries into these models can improve performance on difficult tasks, and recent work has motivated the principle of parameter symmetry breaking and restoration as a unifying mechanism underlying their hierarchical learning dynamics. We evaluate the role of parameter symmetry and network expressivity in the generalisation behaviour of neural networks when learning a real-space renormalisation group (RG) transformation, using the central limit theorem (CLT) as a test case map. We consider simple multilayer perceptrons (MLPs) and graph neural networks (GNNs), and vary weight symmetries and activation functions across architectures. Our results reveal a competition between symmetry constraints and expressivity, with overly complex or overconstrained models generalising poorly. We analytically demonstrate this poor generalisation behaviour for certain constrained MLP architectures by recasting the CLT as a cumulant recursion relation and making use of an established framework to propagate cumulants through MLPs. We also empirically validate an extension of this framework from MLPs to GNNs, elucidating the internal information processing performed by these more complex models. These findings offer new insight into the learning dynamics of symmetric networks and their limitations in modelling structured physical transformations.</li>
<li><strong>摘要：</strong>事实证明，深度学习模型在使用多层表示来学习结构化数据的相关特征方面取得了巨大成功。将物理对称性编码到这些模型中可以提高困难任务的性能，最近的工作激发了参数对称性破坏和恢复的原理作为分层学习动态背后的统一机制。当学习实空间重整化群（RG）变换时，我们使用中心极限定理（CLT）作为测试用例图来评估参数对称性和网络表达性在神经网络泛化行为中的作用。我们考虑简单的多层感知器（MLP）和图神经网络（GNN），并跨架构改变权重对称性和激活函数。我们的结果揭示了对称约束和表现力之间的竞争，过于复杂或过度约束的模型泛化能力很差。我们通过将 CLT 重新转换为累积量递归关系并利用已建立的框架通过 MLP 传播累积量，分析证明了某些受限 MLP 架构的这种较差的泛化行为。我们还凭经验验证了该框架从 MLP 到 GNN 的扩展，阐明了这些更复杂的模型执行的内部信息处理。这些发现为对称网络的学习动态及其在结构化物理变换建模中的局限性提供了新的见解。</li>
</ul>

<h3>Title: Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods</h3>
<ul>
<li><strong>Authors: </strong>Avrim Blum, Daniel Hsu, Cyrus Rashtchian, Donya Saless</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CC, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16609">https://arxiv.org/abs/2510.16609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16609">https://arxiv.org/pdf/2510.16609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16609]] Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods(https://arxiv.org/abs/2510.16609)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Test-time augmentation, such as Retrieval-Augmented Generation (RAG) or tool use, critically depends on an interplay between a model's parametric knowledge and externally retrieved information. However, the theoretical underpinnings of this relationship remain poorly understood. Specifically, it is not clear how much pre-training knowledge is required to answer queries with a small number of augmentation steps, which is a desirable property in practice. To address this question, we formulate multi-step reasoning as an $s$-$t$ connectivity problem on a knowledge graph. We represent a model's pre-training parametric knowledge as a partial, potentially noisy subgraph. We view augmentation as querying an oracle for true edges that augment the model's knowledge. Then, we characterize the necessary and sufficient number of augmentation steps for the model to generate an accurate answer given partial prior knowledge. One key result shows a phase transition: if the prior knowledge graph over $n$ vertices is disconnected into small components, then finding a path via augmentation is inefficient and requires $\Omega(\sqrt{n})$ queries. On the other hand, once the density of correct knowledge surpasses a threshold, forming a giant component, we can find paths with an expected constant number of queries.</li>
<li><strong>摘要：</strong>测试时的增强，例如检索增强生成（RAG）或工具的使用，很大程度上取决于模型的参数知识和外部检索的信息之间的相互作用。然而，人们对这种关系的理论基础仍然知之甚少。具体来说，尚不清楚需要多少预训练知识才能用少量的增强步骤回答查询，这在实践中是一个理想的属性。为了解决这个问题，我们将多步推理表述为知识图上的 $s$-$t$ 连接问题。我们将模型的预训练参数知识表示为部分的、可能有噪声的子图。我们将增强视为查询预言机以获取增强模型知识的真实边缘。然后，我们描述模型所需且足够数量的增强步骤，以便在给定部分先验知识的情况下生成准确的答案。一个关键结果显示了一种相变：如果 $n$ 个顶点上的先验知识图被断开成小组件，那么通过增强查找路径的效率很低，并且需要 $\Omega(\sqrt{n})$ 查询。另一方面，一旦正确知识的密度超过阈值，形成一个巨大的组件，我们就可以找到具有预期恒定查询数量的路径。</li>
</ul>

<h3>Title: Structured Interfaces for Automated Reasoning with 3D Scene Graphs</h3>
<ul>
<li><strong>Authors: </strong>Aaron Ray, Jacob Arkin, Harel Biggie, Chuchu Fan, Luca Carlone, Nicholas Roy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16643">https://arxiv.org/abs/2510.16643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16643">https://arxiv.org/pdf/2510.16643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16643]] Structured Interfaces for Automated Reasoning with 3D Scene Graphs(https://arxiv.org/abs/2510.16643)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In order to provide a robot with the ability to understand and react to a user's natural language inputs, the natural language must be connected to the robot's underlying representations of the world. Recently, large language models (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for grounding natural language and representing the world. In this work, we address the challenge of using LLMs with 3DSGs to ground natural language. Existing methods encode the scene graph as serialized text within the LLM's context window, but this encoding does not scale to large or rich 3DSGs. Instead, we propose to use a form of Retrieval Augmented Generation to select a subset of the 3DSG relevant to the task. We encode a 3DSG in a graph database and provide a query language interface (Cypher) as a tool to the LLM with which it can retrieve relevant data for language grounding. We evaluate our approach on instruction following and scene question-answering tasks and compare against baseline context window and code generation methods. Our results show that using Cypher as an interface to 3D scene graphs scales significantly better to large, rich graphs on both local and cloud-based models. This leads to large performance improvements in grounded language tasks while also substantially reducing the token count of the scene graph content. A video supplement is available at this https URL.</li>
<li><strong>摘要：</strong>为了使机器人能够理解用户的自然语言输入并做出反应，自然语言必须与机器人对世界的底层表示连接起来。最近，大语言模型 (LLM) 和 3D 场景图 (3DSG) 已成为基础自然语言和代表世界的流行选择。在这项工作中，我们解决了使用法学硕士和 3DSG 来奠定自然语言基础的挑战。现有方法将场景图编码为 LLM 上下文窗口内的序列化文本，但这种编码无法扩展到大型或丰富的 3DSG。相反，我们建议使用检索增强生成的形式来选择与任务相关的 3DSG 子集。我们在图形数据库中对 3DSG 进行编码，并提供查询语言接口 (Cypher) 作为 LLM 的工具，LLM 可以使用它检索相关数据以进行语言基础。我们评估我们的指令跟踪和场景问答任务方法，并与基线上下文窗口和代码生成方法进行比较。我们的结果表明，使用 Cypher 作为 3D 场景图的接口，可以在本地和基于云的模型上更好地扩展大型、丰富的图。这极大地提高了基础语言任务的性能，同时也大大减少了场景图内容的标记数量。此 https URL 提供了视频补充。</li>
</ul>

<h3>Title: Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory</h3>
<ul>
<li><strong>Authors: </strong>Anindya Sarkar, Binglin Ji, Yevgeniy Vorobeychik</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16676">https://arxiv.org/abs/2510.16676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16676">https://arxiv.org/pdf/2510.16676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16676]] Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory(https://arxiv.org/abs/2510.16676)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In many scientific and engineering fields, where acquiring high-quality data is expensive--such as medical imaging, environmental monitoring, and remote sensing--strategic sampling of unobserved regions based on prior observations is crucial for maximizing discovery rates within a constrained budget. The rise of powerful generative models, such as diffusion models, has enabled active target discovery in partially observable environments by leveraging learned priors--probabilistic representations that capture underlying structure from data. With guidance from sequentially gathered task-specific observations, these models can progressively refine exploration and efficiently direct queries toward promising regions. However, in domains where learning a strong prior is infeasible due to extremely limited data or high sampling cost (such as rare species discovery, diagnostics for emerging diseases, etc.), these methods struggle to generalize. To overcome this limitation, we propose a novel approach that enables effective active target discovery even in settings with uninformative priors, ensuring robust exploration and adaptability in complex real-world scenarios. Our framework is theoretically principled and draws inspiration from neuroscience to guide its design. Unlike black-box policies, our approach is inherently interpretable, providing clear insights into decision-making. Furthermore, it guarantees a strong, monotonic improvement in prior estimates with each new observation, leading to increasingly accurate sampling and reinforcing both reliability and adaptability in dynamic settings. Through comprehensive experiments and ablation studies across various domains, including species distribution modeling and remote sensing, we demonstrate that our method substantially outperforms baseline approaches.</li>
<li><strong>摘要：</strong>在许多科学和工程领域，获取高质量数据的成本很高，例如医学成像、环境监测和遥感，基于先前的观测对未观测区域进行战略采样对于在有限的预算内最大化发现率至关重要。强大的生成模型（例如扩散模型）的兴起，通过利用学习的先验（从数据中捕获底层结构的概率表示），在部分可观察的环境中实现了主动目标发现。在顺序收集的特定任务观察的指导下，这些模型可以逐步完善探索并有效地将查询定向到有希望的区域。然而，在由于数据极其有限或采样成本高昂而无法学习强先验的领域（例如稀有物种发现、新出现疾病的诊断等），这些方法很难推广。为了克服这一限制，我们提出了一种新方法，即使在先验信息不足的情况下也能实现有效的主动目标发现，从而确保在复杂的现实场景中进行稳健的探索和适应性。我们的框架具有理论原则，并从神经科学中汲取灵感来指导其设计。与黑盒政策不同，我们的方法本质上是可解释的，可以为决策提供清晰的见解。此外，它保证了每次新观测的先前估计的强大、单调改进，从而导致采样越来越准确，并增强动态设置中的可靠性和适应性。通过跨各个领域（包括物种分布建模和遥感）的综合实验和消融研究，我们证明我们的方法大大优于基线方法。</li>
</ul>

<h3>Title: Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Tianxin Wei, Yifan Chen, Xinrui He, Wenxuan Bao, Jingrui He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16704">https://arxiv.org/abs/2510.16704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16704">https://arxiv.org/pdf/2510.16704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16704]] Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization(https://arxiv.org/abs/2510.16704)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Distribution shifts between training and testing samples frequently occur in practice and impede model generalization performance. This crucial challenge thereby motivates studies on domain generalization (DG), which aim to predict the label on unseen target domain data by solely using data from source domains. It is intuitive to conceive the class-separated representations learned in contrastive learning (CL) are able to improve DG, while the reality is quite the opposite: users observe directly applying CL deteriorates the performance. We analyze the phenomenon with the insights from CL theory and discover lack of intra-class connectivity in the DG setting causes the deficiency. We thus propose a new paradigm, domain-connecting contrastive learning (DCCL), to enhance the conceptual connectivity across domains and obtain generalizable representations for DG. On the data side, more aggressive data augmentation and cross-domain positive samples are introduced to improve intra-class connectivity. On the model side, to better embed the unseen test domains, we propose model anchoring to exploit the intra-class connectivity in pre-trained representations and complement the anchoring with generative transformation loss. Extensive experiments on five standard DG benchmarks are performed. The results verify that DCCL outperforms state-of-the-art baselines even without domain supervision. The detailed model implementation and the code are provided through this https URL</li>
<li><strong>摘要：</strong>在实践中，训练样本和测试样本之间的分布变化经常发生，并阻碍模型的泛化性能。因此，这一关键挑战激发了对域泛化（DG）的研究，其目的是仅使用源域中的数据来预测未见过的目标域数据的标签。直观地认为，在对比学习（CL）中学习的类分离表示能够改善 DG，但现实却恰恰相反：用户观察到直接应用 CL 会降低性能。我们根据 CL 理论的见解分析了这一现象，发现 DG 设置中缺乏类内连接导致了这一缺陷。因此，我们提出了一种新的范式，即域连接对比学习（DCCL），以增强跨域的概念连接性并获得 DG 的可推广表示。在数据方面，引入了更积极的数据增强和跨域正样本，以提高类内连接性。在模型方面，为了更好地嵌入未见过的测试域，我们提出模型锚定来利用预训练表示中的类内连接性，并通过生成转换损失来补充锚定。对五个标准 DG 基准进行了广泛的实验。结果证明，即使没有域监督，DCCL 的性能也优于最先进的基线。详细的模型实现和代码通过此 https URL 提供</li>
</ul>

<h3>Title: HumanCM: One Step Human Motion Prediction</h3>
<ul>
<li><strong>Authors: </strong>Liu Haojie, Gao Suixiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16709">https://arxiv.org/abs/2510.16709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16709">https://arxiv.org/pdf/2510.16709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16709]] HumanCM: One Step Human Motion Prediction(https://arxiv.org/abs/2510.16709)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present HumanCM, a one-step human motion prediction framework built upon consistency models. Instead of relying on multi-step denoising as in diffusion-based methods, HumanCM performs efficient single-step generation by learning a self-consistent mapping between noisy and clean motion states. The framework adopts a Transformer-based spatiotemporal architecture with temporal embeddings to model long-range dependencies and preserve motion coherence. Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves comparable or superior accuracy to state-of-the-art diffusion models while reducing inference steps by up to two orders of magnitude.</li>
<li><strong>摘要：</strong>我们提出了 HumanCM，一种基于一致性模型的一步式人体运动预测框架。 HumanCM 不是像基于扩散的方法那样依赖多步去噪，而是通过学习噪声和干净运动状态之间的自洽映射来执行高效的单步生成。该框架采用基于 Transformer 的时空架构和时间嵌入来模拟远程依赖性并保持运动连贯性。 Human3.6M 和 HumanEva-I 上的实验表明，HumanCM 实现了与最先进的扩散模型相当或更高的精度，同时将推理步骤减少了两个数量级。</li>
</ul>

<h3>Title: Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling</h3>
<ul>
<li><strong>Authors: </strong>Erik Riise, Mehmet Onurcan Kaya, Dim P. Papadopoulos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16751">https://arxiv.org/abs/2510.16751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16751">https://arxiv.org/pdf/2510.16751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16751]] Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling(https://arxiv.org/abs/2510.16751)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While inference-time scaling through search has revolutionized Large Language Models, translating these gains to image generation has proven difficult. Recent attempts to apply search strategies to continuous diffusion models show limited benefits, with simple random sampling often performing best. We demonstrate that the discrete, sequential nature of visual autoregressive models enables effective search for image generation. We show that beam search substantially improves text-to-image generation, enabling a 2B parameter autoregressive model to outperform a 12B parameter diffusion model across benchmarks. Systematic ablations show that this advantage comes from the discrete token space, which allows early pruning and computational reuse, and our verifier analysis highlights trade-offs between speed and reasoning capability. These findings suggest that model architecture, not just scale, is critical for inference-time optimization in visual generation.</li>
<li><strong>摘要：</strong>虽然通过搜索进行的推理时间扩展彻底改变了大型语言模型，但将这些成果转化为图像生成已被证明是困难的。最近将搜索策略应用于连续扩散模型的尝试显示出有限的好处，简单的随机抽样通常表现最好。我们证明了视觉自回归模型的离散、顺序性质可以实现图像生成的有效搜索。我们表明，集束搜索极大地改进了文本到图像的生成，使 2B 参数自回归模型在基准测试中优于 12B 参数扩散模型。系统消融表明，这种优势来自离散的令牌空间，它允许早期修剪和计算重用，并且我们的验证者分析强调了速度和推理能力之间的权衡。这些发现表明，模型架构（而不仅仅是规模）对于视觉生成中的推理时间优化至关重要。</li>
</ul>

<h3>Title: Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Ivan Molodetskikh, Kirill Malyshev, Mark Mirgaleev, Nikita Zagainov, Evgeney Bogatyrev, Dmitriy Vatolin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16752">https://arxiv.org/abs/2510.16752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16752">https://arxiv.org/pdf/2510.16752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16752]] Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution(https://arxiv.org/abs/2510.16752)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution, generative</a></li>
<li><strong>Abstract: </strong>Generative image super-resolution (SR) is rapidly advancing in visual quality and detail restoration. As the capacity of SR models expands, however, so does their tendency to produce artifacts: incorrect, visually disturbing details that reduce perceived quality. Crucially, their perceptual impact varies: some artifacts are barely noticeable while others strongly degrade the image. We argue that artifacts should be characterized by their prominence to human observers rather than treated as uniform binary defects. Motivated by this, we present a novel dataset of 1302 artifact examples from 11 contemporary image-SR methods, where each artifact is paired with a crowdsourced prominence score. Building on this dataset, we train a lightweight regressor that produces spatial prominence heatmaps and outperforms existing methods at detecting prominent artifacts. We release the dataset and code to facilitate prominence-aware evaluation and mitigation of SR artifacts.</li>
<li><strong>摘要：</strong>生成图像超分辨率（SR）在视觉质量和细节恢复方面正在迅速进步。然而，随着 SR 模型容量的扩大，它们产生伪像的倾向也在增加：不正确的、视觉上令人不安的细节，从而降低了感知质量。至关重要的是，它们的感知影响各不相同：有些伪影几乎无法察觉，而另一些则严重降低了图像质量。我们认为，人工制品的特征应该是它们对人类观察者的重要性，而不是被视为统一的二元缺陷。受此启发，我们提出了一个新颖的数据集，其中包含来自 11 种当代图像 SR 方法的 1302 个伪影示例，其中每个伪影都与众包的突出分数配对。在此数据集的基础上，我们训练了一个轻量级回归器，它可以生成空间突出热图，并在检测突出伪影方面优于现有方法。我们发布数据集和代码，以促进 SR 工件的显着性评估和缓解。</li>
</ul>

<h3>Title: WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Shengyu Zhu, Fan, Fuxuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16765">https://arxiv.org/abs/2510.16765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16765">https://arxiv.org/pdf/2510.16765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16765]] WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement(https://arxiv.org/abs/2510.16765)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Image restoration is a fundamental and challenging task in computer vision, where CNN-based frameworks demonstrate significant computational efficiency. However, previous CNN-based methods often face challenges in adequately restoring fine texture details, which are limited by the small receptive field of CNN structures and the lack of channel feature modeling. In this paper, we propose WaMaIR, which is a novel framework with a large receptive field for image perception and improves the reconstruction of texture details in restored images. Specifically, we introduce the Global Multiscale Wavelet Transform Convolutions (GMWTConvs) for expandding the receptive field to extract image features, preserving and enriching texture features in model inputs. Meanwhile, we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to capture long-range dependencies within feature channels, which enhancing the model sensitivity to color, edges, and texture information. Additionally, we propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to guide the model in preserving detailed texture structures effectively. Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods, achieving better image restoration and efficient computational performance of the model.</li>
<li><strong>摘要：</strong>图像恢复是计算机视觉中一项基本且具有挑战性的任务，其中基于 CNN 的框架展示了显着的计算效率。然而，先前基于 CNN 的方法在充分恢复精细纹理细节方面经常面临挑战，这受到 CNN 结构的小感受野和缺乏通道特征建模的限制。在本文中，我们提出了 WaMaIR，这是一种新颖的框架，具有较大的图像感知感受野，并改进了恢复图像中纹理细节的重建。具体来说，我们引入了全局多尺度小波变换卷积（GMWTConvs），用于扩展感受野以提取图像特征，保留和丰富模型输入中的纹理特征。同时，我们提出了基于 Mamba 的通道感知模块（MCAM），明确设计用于捕获特征通道内的远程依赖性，从而增强模型对颜色、边缘和纹理信息的敏感性。此外，我们提出了用于图像恢复的多尺度纹理增强损失（MTELoss），以指导模型有效地保留详细的纹理结构。大量实验证实，WaMaIR 优于最先进的方法，实现了更好的图像恢复和模型的高效计算性能。</li>
</ul>

<h3>Title: EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Mingzheng Zhang, Jinfeng Gao, Dan Xu, Jiangrui Yu, Yuhan Qiao, Lan Chen, Jin Tang, Xiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16776">https://arxiv.org/abs/2510.16776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16776">https://arxiv.org/pdf/2510.16776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16776]] EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation(https://arxiv.org/abs/2510.16776)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>X-ray image-based medical report generation (MRG) is a pivotal area in artificial intelligence that can significantly reduce diagnostic burdens for clinicians and patient wait times. Existing MRG models predominantly rely on Large Language Models (LLMs) to improve report generation, with limited exploration of pre-trained vision foundation models or advanced fine-tuning techniques. Mainstream frameworks either avoid fine-tuning or utilize simplistic methods like LoRA, often neglecting the potential of enhancing cross-attention mechanisms. Additionally, while Transformer-based models dominate vision-language tasks, non-Transformer architectures, such as the Mamba network, remain underexplored for medical report generation, presenting a promising avenue for future research. In this paper, we propose EMRRG, a novel X-ray report generation framework that fine-tunes pre-trained Mamba networks using parameter-efficient methods. Specifically, X-ray images are divided into patches, tokenized, and processed by an SSM-based vision backbone for feature extraction, with Partial LoRA yielding optimal performance. An LLM with a hybrid decoder generates the medical report, enabling end-to-end training and achieving strong results on benchmark datasets. Extensive experiments on three widely used benchmark datasets fully validated the effectiveness of our proposed strategies for the X-ray MRG. The source code of this paper will be released on this https URL.</li>
<li><strong>摘要：</strong>基于 X 射线图像的医疗报告生成 (MRG) 是人工智能的关键领域，可以显着减少临床医生的诊断负担和患者的等待时间。现有的 MRG 模型主要依靠大型语言模型 (LLM) 来改进报告生成，对预先训练的视觉基础模型或高级微调技术的探索有限。主流框架要么避免微调，要么利用 LoRA 等简单方法，往往忽略了增强交叉注意力机制的潜力。此外，虽然基于 Transformer 的模型在视觉语言任务中占主导地位，但非 Transformer 架构（例如 Mamba 网络）在医疗报告生成方面的探索仍未充分，这为未来的研究提供了一条有希望的途径。在本文中，我们提出了 EMRRG，这是一种新颖的 X 射线报告生成框架，它使用参数有效的方法对预训练的 Mamba 网络进行微调。具体来说，X 射线图像被分成块、标记化，并由基于 SSM 的视觉主干进行处理以进行特征提取，部分 LoRA 会产生最佳性能。具有混合解码器的法学硕士生成医学报告，实现端到端训练并在基准数据集上取得出色的结果。对三个广泛使用的基准数据集进行的广泛实验充分验证了我们提出的 X 射线 MRG 策略的有效性。本文的源代码将在此 https URL 上发布。</li>
</ul>

<h3>Title: Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features</h3>
<ul>
<li><strong>Authors: </strong>Shihao Ji, Zihui Song</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16781">https://arxiv.org/abs/2510.16781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16781">https://arxiv.org/pdf/2510.16781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16781]] Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features(https://arxiv.org/abs/2510.16781)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The remarkable zero-shot reasoning capabilities of large-scale Visual Language Models (VLMs) on static images have yet to be fully translated to the video domain. Conventional video understanding models often rely on extensive, task-specific training on annotated datasets, a process that is both costly and limited in scalability. This paper introduces a novel, training-free framework for video understanding that circumvents end-to-end training by synergistically combining the rich semantic priors of pre-trained VLMs with classic machine learning algorithms for pattern discovery. Our core idea is to reframe video understanding as a self-supervised spatio-temporal clustering problem within a high-dimensional semantic feature space. The proposed pipeline first transforms a video stream into a semantic feature trajectory using the frozen visual encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal Segmentation (KTS), a robust machine learning technique, to partition the continuous feature stream into discrete, semantically coherent event segments. These segments are then subjected to unsupervised density-based clustering to identify recurring macroscopic scenes and themes throughout the video. By selecting representative keyframes from each discovered cluster and leveraging the VLM's generative capabilities for textual description, our framework automatically produces a structured, multi-modal summary of the video content. This approach provides an effective, interpretable, and model-agnostic pathway for zero-shot, automated structural analysis of video content.</li>
<li><strong>摘要：</strong>大规模视觉语言模型（VLM）在静态图像上卓越的零样本推理能力尚未完全转化到视频领域。传统的视频理解模型通常依赖于对带注释的数据集进行广泛的、特定于任务的训练，这一过程既昂贵又可扩展性有限。本文介绍了一种新颖的、免训练的视频理解框架，该框架通过将预训练 VLM 的丰富语义先验与用于模式发现的经典机器学习算法协同结合来规避端到端训练。我们的核心思想是将视频理解重新定义为高维语义特征空间内的自监督时空聚类问题。所提出的管道首先使用预训练 VLM 的冻结视觉编码器将视频流转换为语义特征轨迹。随后，我们采用内核时间分割（KTS）这种强大的机器学习技术，将连续特征流划分为离散的、语义连贯的事件片段。然后对这些片段进行无监督的基于密度的聚类，以识别整个视频中重复出现的宏观场景和主题。通过从每个发现的集群中选择代表性关键帧并利用 VLM 的文本描述生成功能，我们的框架自动生成视频内容的结构化、多模式摘要。这种方法为视频内容的零样本、自动化结构分析提供了一种有效、可解释且与模型无关的途径。</li>
</ul>

<h3>Title: Personalized Image Filter: Mastering Your Photographic Style</h3>
<ul>
<li><strong>Authors: </strong>Chengxuan Zhu, Shuchen Weng, Jiacong Fang, Peixuan Zhang, Si Li, Chao Xu, Boxin Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16791">https://arxiv.org/abs/2510.16791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16791">https://arxiv.org/pdf/2510.16791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16791]] Personalized Image Filter: Mastering Your Photographic Style(https://arxiv.org/abs/2510.16791)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Photographic style, as a composition of certain photographic concepts, is the charm behind renowned photographers. But learning and transferring photographic style need a profound understanding of how the photo is edited from the unknown original appearance. Previous works either fail to learn meaningful photographic concepts from reference images, or cannot preserve the content of the content image. To tackle these issues, we proposed a Personalized Image Filter (PIF). Based on a pretrained text-to-image diffusion model, the generative prior enables PIF to learn the average appearance of photographic concepts, as well as how to adjust them according to text prompts. PIF then learns the photographic style of reference images with the textual inversion technique, by optimizing the prompts for the photographic concepts. PIF shows outstanding performance in extracting and transferring various kinds of photographic style. Project page: this https URL</li>
<li><strong>摘要：</strong>摄影风格作为一定摄影理念的组合，是知名摄影师背后的魅力所在。但学习和迁移摄影风格需要深刻理解照片是如何从未知的原貌中剪辑出来的。以前的作品要么无法从参考图像中学习有意义的摄影概念，要么无法保留内容图像的内容。为了解决这些问题，我们提出了个性化图像过滤器（PIF）。基于预训练的文本到图像扩散模型，生成先验使 PIF 能够学习摄影概念的平均外观，以及如何根据文本提示进行调整。然后，PIF 通过优化摄影概念的提示，使用文本反转技术来学习参考图像的摄影风格。 PIF在提取和转移各种摄影风格方面表现出了出色的表现。项目页面：此 https URL</li>
</ul>

<h3>Title: From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display</h3>
<ul>
<li><strong>Authors: </strong>Xiangyu Mu, Dongliang Zhou, Jie Hou, Haijun Zhang, Weili Guan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16833">https://arxiv.org/abs/2510.16833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16833">https://arxiv.org/pdf/2510.16833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16833]] From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display(https://arxiv.org/abs/2510.16833)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Mannequin-based clothing displays offer a cost-effective alternative to real-model showcases for online fashion presentation, but lack realism and expressive detail. To overcome this limitation, we introduce a new task called mannequin-to-human (M2H) video generation, which aims to synthesize identity-controllable, photorealistic human videos from footage of mannequins. We propose M2HVideo, a pose-aware and identity-preserving video generation framework that addresses two key challenges: the misalignment between head and body motion, and identity drift caused by temporal modeling. In particular, M2HVideo incorporates a dynamic pose-aware head encoder that fuses facial semantics with body pose to produce consistent identity embeddings across frames. To address the loss of fine facial details due to latent space compression, we introduce a mirror loss applied in pixel space through a denoising diffusion implicit model (DDIM)-based one-step denoising. Additionally, we design a distribution-aware adapter that aligns statistical distributions of identity and clothing features to enhance temporal coherence. Extensive experiments on the UBC fashion dataset, our self-constructed ASOS dataset, and the newly collected MannequinVideos dataset captured on-site demonstrate that M2HVideo achieves superior performance in terms of clothing consistency, identity preservation, and video fidelity in comparison to state-of-the-art methods.</li>
<li><strong>摘要：</strong>基于人体模型的服装展示为在线时尚展示提供了一种比真人模特展示更经济高效的替代方案，但缺乏真实性和表现力细节。为了克服这一限制，我们引入了一项名为人体模型到人类（M2H）视频生成的新任务，该任务旨在从人体模型的镜头中合成身份可控、逼真的人类视频。我们提出了 M2HVideo，一种姿势感知和身份保护的视频生成框架，它解决了两个关键挑战：头部和身体运动之间的错位，以及时间建模引起的身份漂移。特别是，M2HVideo 结合了动态姿势感知头部编码器，将面部语义与身体姿势融合，以在帧之间产生一致的身份嵌入。为了解决由于潜在空间压缩导致的精细面部细节的损失，我们通过基于去噪扩散隐式模型（DDIM）的一步去噪引入了一种应用于像素空间的镜像损失。此外，我们设计了一个分布感知适配器，可以调整身份和服装特征的统计分布，以增强时间一致性。对 UBC 时尚数据集、我们自行构建的 ASOS 数据集以及现场捕获的新收集的 MannequinVideos 数据集进行的大量实验表明，与最先进的方法相比，M2HVideo 在服装一致性、身份保存和视频保真度方面实现了卓越的性能。</li>
</ul>

<h3>Title: Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Nusrat Munia, Abdullah Imran</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16887">https://arxiv.org/abs/2510.16887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16887">https://arxiv.org/pdf/2510.16887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16887]] Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis(https://arxiv.org/abs/2510.16887)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative models, especially Diffusion Models, have demonstrated remarkable capability in generating high-quality synthetic data, including medical images. However, traditional class-conditioned generative models often struggle to generate images that accurately represent specific medical categories, limiting their usefulness for applications such as skin cancer diagnosis. To address this problem, we propose a classification-induced diffusion model, namely, Class-N-Diff, to simultaneously generate and classify dermoscopic images. Our Class-N-Diff model integrates a classifier within a diffusion model to guide image generation based on its class conditions. Thus, the model has better control over class-conditioned image synthesis, resulting in more realistic and diverse images. Additionally, the classifier demonstrates improved performance, highlighting its effectiveness for downstream diagnostic tasks. This unique integration in our Class-N-Diff makes it a robust tool for enhancing the quality and utility of diffusion model-based synthetic dermoscopic image generation. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>生成模型，特别是扩散模型，在生成高质量合成数据（包括医学图像）方面表现出了卓越的能力。然而，传统的类条件生成模型通常难以生成准确代表特定医学类别的图像，从而限制了它们在皮肤癌诊断等应用中的有用性。为了解决这个问题，我们提出了一种分类诱导扩散模型，即 Class-N-Diff，以同时生成和分类皮肤镜图像。我们的 Class-N-Diff 模型在扩散模型中集成了分类器，以根据其类别条件指导图像生成。因此，该模型可以更好地控制类条件图像合成，从而产生更真实和多样化的图像。此外，分类器还展示了改进的性能，突出了其对下游诊断任务的有效性。 Class-N-Diff 中的这种独特集成使其成为增强基于扩散模型的合成皮肤镜图像生成的质量和实用性的强大工具。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: Adaptive Online Learning with LSTM Networks for Energy Price Prediction</h3>
<ul>
<li><strong>Authors: </strong>Salih Salihoglu, Ibrahim Ahmed, Afshin Asadi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16898">https://arxiv.org/abs/2510.16898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16898">https://arxiv.org/pdf/2510.16898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16898]] Adaptive Online Learning with LSTM Networks for Energy Price Prediction(https://arxiv.org/abs/2510.16898)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accurate prediction of electricity prices is crucial for stakeholders in the energy market, particularly for grid operators, energy producers, and consumers. This study focuses on developing a predictive model leveraging Long Short-Term Memory (LSTM) networks to forecast day-ahead electricity prices in the California energy market. The model incorporates a variety of features, including historical price data, weather conditions, and the energy generation mix. A novel custom loss function that integrates Mean Absolute Error (MAE), Jensen-Shannon Divergence (JSD), and a smoothness penalty is introduced to enhance the prediction accuracy and interpretability. Additionally, an online learning approach is implemented to allow the model to adapt to new data incrementally, ensuring continuous relevance and accuracy. The results demonstrate that the custom loss function can improve the model's performance, aligning predicted prices more closely with actual values, particularly during peak intervals. Also, the online learning model outperforms other models by effectively incorporating real-time data, resulting in lower prediction error and variability. The inclusion of the energy generation mix further enhances the model's predictive capabilities, highlighting the importance of comprehensive feature integration. This research provides a robust framework for electricity price forecasting, offering valuable insights and tools for better decision-making in dynamic electricity markets.</li>
<li><strong>摘要：</strong>准确预测电价对于能源市场的利益相关者，特别是电网运营商、能源生产商和消费者至关重要。本研究的重点是开发一个利用长短期记忆 (LSTM) 网络的预测模型来预测加州能源市场的日前电价。该模型融合了多种功能，包括历史价格数据、天气状况和能源发电组合。引入了一种新颖的自定义损失函数，该函数集成了平均绝对误差（MAE）、詹森-香农散度（JSD）和平滑度惩罚，以提高预测准确性和可解释性。此外，还实施了在线学习方法，使模型能够逐步适应新数据，确保持续的相关性和准确性。结果表明，自定义损失函数可以提高模型的性能，使预测价格与实际值更接近，特别是在高峰时段。此外，在线学习模型通过有效地结合实时数据来优于其他模型，从而降低预测误差和变异性。能源发电组合的纳入进一步增强了模型的预测能力，凸显了全面特征集成的重要性。这项研究为电价预测提供了一个强大的框架，为动态电力市场中更好的决策提供了宝贵的见解和工具。</li>
</ul>

<h3>Title: SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search</h3>
<ul>
<li><strong>Authors: </strong>Dong Li, Xujiang Zhao, Linlin Yu, Yanchi Liu, Wei Cheng, Zhengzhang Chen, Zhong Chen, Feng Chen, Chen Zhao, Haifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16916">https://arxiv.org/abs/2510.16916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16916">https://arxiv.org/pdf/2510.16916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16916]] SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search(https://arxiv.org/abs/2510.16916)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) offer promising capabilities for tackling complex reasoning tasks, including optimization problems. However, existing methods either rely on prompt engineering, which leads to poor generalization across problem types, or require costly supervised training. We introduce SolverLLM, a training-free framework that leverages test-time scaling to solve diverse optimization problems. Rather than solving directly, SolverLLM generates mathematical formulations and translates them into solver-ready code, guided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the search process, we modify classical MCTS with (1) dynamic expansion for adaptive formulation generation, (2) prompt backpropagation to guide exploration via outcome-driven feedback, and (3) uncertainty backpropagation to incorporate reward reliability into decision-making. Experiments on six standard benchmark datasets demonstrate that SolverLLM outperforms both prompt-based and learning-based baselines, achieving strong generalization without additional training.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 为处理复杂的推理任务（包括优化问题）提供了有前景的功能。然而，现有的方法要么依赖于即时工程，这导致跨问题类型的泛化能力较差，要么需要昂贵的监督培训。我们引入 SolverLLM，这是一个免训练框架，利用测试时间扩展来解决各种优化问题。 SolverLLM 不是直接求解，而是在新颖的蒙特卡罗树搜索 (MCTS) 策略的指导下生成数学公式并将其转换为求解器就绪代码。为了增强搜索过程，我们通过以下方式修改经典 MCTS：（1）用于自适应公式生成的动态扩展，（2）提示反向传播以通过结果驱动的反馈来指导探索，以及（3）不确定性反向传播以将奖励可靠性纳入决策中。对六个标准基准数据集的实验表明，SolverLLM 的性能优于基于提示和基于学习的基线，无需额外训练即可实现强大的泛化能力。</li>
</ul>

<h3>Title: Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input</h3>
<ul>
<li><strong>Authors: </strong>Chenxu Li, Zhicai Wang, Yuan Sheng, Xingyu Zhu, Yanbin Hao, Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16926">https://arxiv.org/abs/2510.16926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16926">https://arxiv.org/pdf/2510.16926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16926]] Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input(https://arxiv.org/abs/2510.16926)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) increasingly support dynamic image resolutions. However, current evaluation paradigms primarily assess semantic performance, overlooking the critical question of resolution robustness - whether performance remains stable across varying input resolutions. To address this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising 14,400 samples across 12 resolution levels and six core capability dimensions. We designed a novel evaluation framework that goes beyond traditional accuracy metrics to capture performance stability. This framework introduces multiple robustness metrics: Spearman's correlation for assessing resolution-performance trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring performance volatility. Using these metrics, we conducted a large-scale evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and task-centric robustness examination, (2) investigation of preprocessing strategies including padding and super-resolution, and (3) exploration of fine-tuning for stability enhancement.</li>
<li><strong>摘要：</strong>多模态大语言模型 (MLLM) 越来越多地支持动态图像分辨率。然而，当前的评估范式主要评估语义性能，忽略了分辨率鲁棒性的关键问题——性能是否在不同的输入分辨率下保持稳定。为了解决这一差距，我们引入了 \textbf{Res-Bench}，这是一个综合基准测试，包含 12 个分辨率级别和 6 个核心能力维度的 14,400 个样本。我们设计了一种新颖的评估框架，超越了传统的准确性指标，以捕获性能稳定性。该框架引入了多个稳健性指标：用于评估分辨率-性能趋势的 Spearman 相关性，以及用于测量性能波动性的绝对/相对连续误差 (ACE/RCE)。使用这些指标，我们对领先的 MLLM 进行了大规模评估。我们的分析包括：（1）以模型为中心和以任务为中心的稳健性检查，（2）对包括填充和超分辨率在内的预处理策略的研究，以及（3）对增强稳定性的微调的探索。</li>
</ul>

<h3>Title: Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis</h3>
<ul>
<li><strong>Authors: </strong>Praveenbalaji Rajendran, Mojtaba Safari, Wenfeng He, Mingzhe Hu, Shansong Wang, Jun Zhou, Xiaofeng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, physics.med-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16973">https://arxiv.org/abs/2510.16973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16973">https://arxiv.org/pdf/2510.16973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16973]] Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis(https://arxiv.org/abs/2510.16973)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in artificial intelligence (AI), particularly foundation models (FMs), have revolutionized medical image analysis, demonstrating strong zero- and few-shot performance across diverse medical imaging tasks, from segmentation to report generation. Unlike traditional task-specific AI models, FMs leverage large corpora of labeled and unlabeled multimodal datasets to learn generalized representations that can be adapted to various downstream clinical applications with minimal fine-tuning. However, despite the rapid proliferation of FM research in medical imaging, the field remains fragmented, lacking a unified synthesis that systematically maps the evolution of architectures, training paradigms, and clinical applications across modalities. To address this gap, this review article provides a comprehensive and structured analysis of FMs in medical image analysis. We systematically categorize studies into vision-only and vision-language FMs based on their architectural foundations, training strategies, and downstream clinical tasks. Additionally, a quantitative meta-analysis of the studies was conducted to characterize temporal trends in dataset utilization and application domains. We also critically discuss persistent challenges, including domain adaptation, efficient fine-tuning, computational constraints, and interpretability along with emerging solutions such as federated learning, knowledge distillation, and advanced prompting. Finally, we identify key future research directions aimed at enhancing the robustness, explainability, and clinical integration of FMs, thereby accelerating their translation into real-world medical practice.</li>
<li><strong>摘要：</strong>人工智能 (AI)，特别是基础模型 (FM) 的最新进展，彻底改变了医学图像分析，在从分割到报告生成的各种医学成像任务中展示了强大的零镜头和少镜头性能。与传统的特定任务人工智能模型不同，FM 利用大量标记和未标记多模态数据集来学习通用表示，这些表示可以通过最少的微调适应各种下游临床应用。然而，尽管医学成像领域的 FM 研究迅速扩散，但该领域仍然分散，缺乏统一的综合来系统地描绘跨模式的架构、训练范式和临床应用的演变。为了弥补这一差距，这篇综述文章对医学图像分析中的 FM 进行了全面、结构化的分析。我们根据其架构基础、训练策略和下游临床任务将研究系统地分类为仅视觉 FM 和视觉语言 FM。此外，还对这些研究进行了定量荟萃分析，以表征数据集利用和应用领域的时间趋势。我们还批判性地讨论了持续存在的挑战，包括领域适应、高效微调、计算约束和可解释性，以及联邦学习、知识蒸馏和高级提示等新兴解决方案。最后，我们确定了未来的关键研究方向，旨在增强 FM 的稳健性、可解释性和临床整合性，从而加速其转化为现实世界的医疗实践。</li>
</ul>

<h3>Title: Differentially Private Linear Regression and Synthetic Data Generation with Statistical Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Shurong Lin, Aleksandra Slavković, Deekshith Reddy Bhoomireddy</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16974">https://arxiv.org/abs/2510.16974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16974">https://arxiv.org/pdf/2510.16974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16974]] Differentially Private Linear Regression and Synthetic Data Generation with Statistical Guarantees(https://arxiv.org/abs/2510.16974)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In social sciences, small- to medium-scale datasets are common and linear regression (LR) is canonical. In privacy-aware settings, much work has focused on differentially private (DP) LR, but mostly on point estimation with limited attention to uncertainty quantification. Meanwhile, synthetic data generation (SDG) is increasingly important for reproducibility studies, yet current DP LR methods do not readily support it. Mainstream SDG approaches are either tailored to discretized data, making them less suitable for continuous regression, or rely on deep models that require large datasets, limiting their use for the smaller, continuous data typical in social science. We propose a method for LR with valid inference under Gaussian DP: a DP bias-corrected estimator with asymptotic confidence intervals (CIs) and a general SDG procedure in which regression on the synthetic data matches our DP regression. Our binning-aggregation strategy is effective in small- to moderate-dimensional settings. Experiments show our method (1) improves accuracy over existing methods, (2) provides valid CIs, and (3) produces more reliable synthetic data for downstream ML tasks than current DP SDGs.</li>
<li><strong>摘要：</strong>在社会科学中，中小型数据集很常见，线性回归（LR）是规范的。在隐私意识环境中，许多工作都集中在差分隐私（DP）LR上，但主要集中在点估计上，而对不确定性量化的关注有限。与此同时，合成数据生成 (SDG) 对于再现性研究越来越重要，但当前的 DP LR 方法并不容易支持它。主流 SDG 方法要么是针对离散数据量身定制的，这使得它们不太适合连续回归，要么依赖于需要大型数据集的深度模型，限制了它们对社会科学中典型的较小连续数据的使用。我们提出了一种在高斯 DP 下进行有效推理的 LR 方法：具有渐进置信区间 (CI) 的 DP 偏差校正估计器和通用 SDG 程序，其中合成数据的回归与我们的 DP 回归相匹配。我们的分箱聚合策略在小到中等维度的设置中是有效的。实验表明，我们的方法 (1) 比现有方法提高了准确性，(2) 提供了有效的 CI，(3) 为下游 ML 任务生成比当前 DP SDG 更可靠的合成数据。</li>
</ul>

<h3>Title: One-step Diffusion Models with Bregman Density Ratio Matching</h3>
<ul>
<li><strong>Authors: </strong>Yuanzhi Zhu, Eleftherios Tsonis, Lucas Degeorge, Vicky Kalogeiton</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16983">https://arxiv.org/abs/2510.16983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16983">https://arxiv.org/pdf/2510.16983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16983]] One-step Diffusion Models with Bregman Density Ratio Matching(https://arxiv.org/abs/2510.16983)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion and flow models achieve high generative quality but remain computationally expensive due to slow multi-step sampling. Distillation methods accelerate them by training fast student generators, yet most existing objectives lack a unified theoretical foundation. In this work, we propose Di-Bregman, a compact framework that formulates diffusion distillation as Bregman divergence-based density-ratio matching. This convex-analytic view connects several existing objectives through a common lens. Experiments on CIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves improved one-step FID over reverse-KL distillation and maintains high visual fidelity compared to the teacher model. Our results highlight Bregman density-ratio matching as a practical and theoretically-grounded route toward efficient one-step diffusion generation.</li>
<li><strong>摘要：</strong>扩散和流动模型实现了较高的生成质量，但由于多步采样速度较慢，计算成本仍然很高。蒸馏方法通过训练快速学生生成器来加速它们，但大多数现有目标缺乏统一的理论基础。在这项工作中，我们提出了 Di-Bregman，一个紧凑的框架，它将扩散蒸馏表述为基于 Bregman 散度的密度比匹配。这种凸解析视图通过一个共同的镜头连接了几个现有的物镜。 CIFAR-10 和文本到图像生成的实验表明，Di-Bregman 比反向 KL 蒸馏实现了改进的一步 FID，并且与教师模型相比保持了高视觉保真度。我们的结果强调了布雷格曼密度比匹配是一种实现高效单步扩散生成的实用且有理论依据的途径。</li>
</ul>

<h3>Title: Graph4MM: Weaving Multimodal Learning with Structural Information</h3>
<ul>
<li><strong>Authors: </strong>Xuying Ning, Dongqi Fu, Tianxin Wei, Wujiang Xu, Jingrui He</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.16990">https://arxiv.org/abs/2510.16990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.16990">https://arxiv.org/pdf/2510.16990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.16990]] Graph4MM: Weaving Multimodal Learning with Structural Information(https://arxiv.org/abs/2510.16990)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Real-world multimodal data usually exhibit complex structural relationships beyond traditional one-to-one mappings like image-caption pairs. Entities across modalities interact in intricate ways, with images and text forming diverse interconnections through contextual dependencies and co-references. Graphs provide powerful structural information for modeling intra-modal and inter-modal relationships. However, previous works fail to distinguish multi-hop neighbors and treat the graph as a standalone modality, which fragments the overall understanding. This limitation presents two key challenges in multimodal learning: (1) integrating structural information from multi-hop neighbors into foundational models, and (2) fusing modality-specific information in a principled manner. To address these challenges, we revisit the role of graphs in multimodal learning within the era of foundation models and propose Graph4MM, a graph-based multimodal learning framework. To be specific, we introduce Hop-Diffused Attention, which integrates multi-hop structural information into self-attention through causal masking and hop diffusion. Furthermore, we design MM-QFormer, a multi-mapping querying transformer for cross-modal fusion. Through theoretical and empirical analysis, we show that leveraging structures to integrate both intra- and inter-modal interactions improves multimodal understanding beyond treating them as a standalone modality. Experiments on both generative and discriminative tasks show that Graph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines, achieving a 6.93% average improvement.</li>
<li><strong>摘要：</strong>现实世界的多模态数据通常表现出复杂的结构关系，超出了传统的一对一映射（例如图像标题对）。跨模式的实体以复杂的方式交互，图像和文本通过上下文依赖和共同引用形成不同的互连。图为建模模态内和模态间关系提供了强大的结构信息。然而，以前的工作未能区分多跳邻居，并将图视为独立的模态，这破坏了整体理解。这种限制在多模态学习中提出了两个关键挑战：（1）将来自多跳邻居的结构信息集成到基础模型中，以及（2）以有原则的方式融合特定于模态的信息。为了应对这些挑战，我们重新审视了基础模型时代图在多模态学习中的作用，并提出了 Graph4MM，一种基于图的多模态学习框架。具体来说，我们引入了Hop-Diffused Attention，它通过因果掩蔽和跳跃扩散将多跳结构信息集成到自注意力中。此外，我们设计了 MM-QFormer，一种用于跨模式融合的多映射查询变压器。通过理论和实证分析，我们表明，利用结构来整合模式内和模式间的相互作用可以提高多模式理解，而不仅仅是将它们视为独立的模式。生成任务和判别任务的实验表明，Graph4MM 的性能优于较大的 VLM、LLM 和多模态图基线，平均提高了 6.93%。</li>
</ul>

<h3>Title: EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of Analog Circuit</h3>
<ul>
<li><strong>Authors: </strong>Chang Liu, Danial Chitnis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17002">https://arxiv.org/abs/2510.17002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17002">https://arxiv.org/pdf/2510.17002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17002]] EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of Analog Circuit(https://arxiv.org/abs/2510.17002)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Circuit schematics play a crucial role in analog integrated circuit design, serving as the primary medium for human understanding and verification of circuit functionality. While recent large language model (LLM)-based approaches have shown promise in circuit topology generation and device sizing, most rely solely on textual representations such as SPICE netlists, which lack visual interpretability for circuit designers. To address this limitation, we propose EEschematic, an AI agent for automatic analog schematic generation based on a Multimodal Large Language Model (MLLM). EEschematic integrates textual, visual, and symbolic modalities to translate SPICE netlists into schematic diagrams represented in a human-editable format. The framework uses six analog substructure examples for few-shot placement and a Visual Chain-of-Thought (VCoT) strategy to iteratively refine placement and wiring, enhancing schematic clarity and symmetry. Experimental results on representative analog circuits, including a CMOS inverter, a five-transistor operational transconductance amplifier (5T-OTA), and a telescopic cascode amplifier, demonstrate that EEschematic produces schematics with high visual quality and structural correctness.</li>
<li><strong>摘要：</strong>电路原理图在模拟集成电路设计中发挥着至关重要的作用，是人类理解和验证电路功能的主要媒介。虽然最近基于大语言模型 (LLM) 的方法在电路拓扑生成和器件尺寸调整方面显示出了前景，但大多数方法仅依赖于 SPICE 网表等文本表示，而这些表示对于电路设计人员来说缺乏视觉可解释性。为了解决这一限制，我们提出了 EEschematic，这是一种基于多模态大语言模型 (MLLM) 自动模拟原理图生成的 AI 代理。 EEschematic 集成了文本、视觉和符号模式，将 SPICE 网表转换为以人类可编辑格式表示的原理图。该框架使用六个模拟子结构示例进行几次布局，并使用视觉思维链 (VCoT) 策略来迭代优化布局和布线，从而增强原理图清晰度和对称性。代表性模拟电路（包括 CMOS 反相器、五晶体管运算跨导放大器 (5T-OTA) 和伸缩共源共栅放大器）的实验结果表明，EEschematic 生成的原理图具有高视觉质量和结构正确性。</li>
</ul>

<h3>Title: Conditional Synthetic Live and Spoof Fingerprint Generation</h3>
<ul>
<li><strong>Authors: </strong>Syed Konain Abbas, Sandip Purnapatra, M. G. Sarwar Murshed, Conor Miller-Lynch, Lambert Igene, Soumyabrata Dey, Stephanie Schuckers, Faraz Hussain</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17035">https://arxiv.org/abs/2510.17035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17035">https://arxiv.org/pdf/2510.17035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17035]] Conditional Synthetic Live and Spoof Fingerprint Generation(https://arxiv.org/abs/2510.17035)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Large fingerprint datasets, while important for training and evaluation, are time-consuming and expensive to collect and require strict privacy measures. Researchers are exploring the use of synthetic fingerprint data to address these issues. This paper presents a novel approach for generating synthetic fingerprint images (both spoof and live), addressing concerns related to privacy, cost, and accessibility in biometric data collection. Our approach utilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce high-resolution synthetic live fingerprints, conditioned on specific finger identities (thumb through little finger). Additionally, we employ CycleGANs to translate these into realistic spoof fingerprints, simulating a variety of presentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof fingerprints are crucial for developing robust spoof detection systems. Through these generative models, we created two synthetic datasets (DB2 and DB3), each containing 1,500 fingerprint images of all ten fingers with multiple impressions per finger, and including corresponding spoofs in eight material types. The results indicate robust performance: our StyleGAN3 model achieves a Fréchet Inception Distance (FID) as low as 5, and the generated fingerprints achieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The StyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess fingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably, matching experiments confirm strong privacy preservation, with no significant evidence of identity leakage, confirming the strong privacy-preserving properties of our synthetic datasets.</li>
<li><strong>摘要：</strong>大型指纹数据集虽然对于训练和评估很重要，但收集起来既耗时又昂贵，并且需要严格的隐私措施。研究人员正在探索使用合成指纹数据来解决这些问题。本文提出了一种生成合成指纹图像（欺骗和实时）的新方法，解决了生物识别数据收集中与隐私、成本和可访问性相关的问题。我们的方法利用条件 StyleGAN2-ADA 和 StyleGAN3 架构来生成高分辨率合成活体指纹，以特定手指身份（拇指到小指）为条件。此外，我们使用 CycleGAN 将这些转化为真实的欺骗指纹，模拟各种演示攻击材料（例如 EcoFlex、Play-Doh）。这些合成的欺骗指纹对于开发强大的欺骗检测系统至关重要。通过这些生成模型，我们创建了两个合成数据集（DB2 和 DB3），每个数据集包含所有 10 个手指的 1,500 个指纹图像，每个手指有多个印象，并包括八种材料类型的相应恶搞。结果显示出强大的性能：我们的 StyleGAN3 模型的 Fréchet 起始距离 (FID) 低至 5，生成的指纹的真实接受率达到 99.47%，错误接受率为 0.01%。 StyleGAN2-ADA 模型在 FAR 为 0.01% 的情况下实现了 98.67% 的 TAR。我们使用标准指标（NFIQ2、MINDTCT）评估指纹质量，值得注意的是，匹配实验证实了强大的隐私保护，没有明显的身份泄露证据，证实了我们的合成数据集的强大隐私保护特性。</li>
</ul>

<h3>Title: Hephaestus: Mixture Generative Modeling with Energy Guidance for Large-scale QoS Degradation</h3>
<ul>
<li><strong>Authors: </strong>Nguyen Do, Bach Ngo, Youval Kashuv, Canh V. Pham, Hanghang Tong, My T. Thai</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17036">https://arxiv.org/abs/2510.17036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17036">https://arxiv.org/pdf/2510.17036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17036]] Hephaestus: Mixture Generative Modeling with Energy Guidance for Large-scale QoS Degradation(https://arxiv.org/abs/2510.17036)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We study the Quality of Service Degradation (QoSD) problem, in which an adversary perturbs edge weights to degrade network performance. This setting arises in both network infrastructures and distributed ML systems, where communication quality, not just connectivity, determines functionality. While classical methods rely on combinatorial optimization, and recent ML approaches address only restricted linear variants with small-size networks, no prior model directly tackles the QoSD problem under nonlinear edge-weight functions. This work proposes \PIMMA, a self-reinforcing generative framework that synthesizes feasible solutions in latent space, to fill this gap. Our method includes three phases: (1) Forge: a Predictive Path-Stressing (PPS) algorithm that uses graph learning and approximation to produce feasible solutions with performance guarantee, (2) Morph: a new theoretically grounded training paradigm for Mixture of Conditional VAEs guided by an energy-based model to capture solution feature distributions, and (3) Refine: a reinforcement learning agent that explores this space to generate progressively near-optimal solutions using our designed differentiable reward function. Experiments on both synthetic and real-world networks show that our approach consistently outperforms classical and ML baselines, particularly in scenarios with nonlinear cost functions where traditional methods fail to generalize.</li>
<li><strong>摘要：</strong>我们研究服务质量下降（QoSD）问题，其中对手扰乱边缘权重以降低网络性能。这种设置出现在网络基础设施和分布式机器学习系统中，其中通信质量（而不仅仅是连接性）决定了功能。虽然经典方法依赖于组合优化，而最近的 ML 方法仅解决小规模网络的受限线性变体，但现有模型没有直接解决非线性边权重函数下的 QoSD 问题。这项工作提出了 \PIMMA，一种自我强化的生成框架，可以在潜在空间中综合可行的解决方案，以填补这一空白。我们的方法包括三个阶段：（1）Forge：一种预测路径压力（PPS）算法，使用图学习和近似来产生具有性能保证的可行解决方案，（2）Morph：一种新的理论基础训练范式，用于条件 VAE 混合，由基于能量的模型引导以捕获解决方案特征分布，以及（3）Refine：一种强化学习代理，探索该空间以逐步生成 使用我们设计的可微奖励函数的接近最优的解决方案。对合成网络和现实世界网络的实验表明，我们的方法始终优于经典和机器学习基线，特别是在传统方法无法泛化的非线性成本函数场景中。</li>
</ul>

<h3>Title: Consistent Zero-Shot Imitation with Contrastive Goal Inference</h3>
<ul>
<li><strong>Authors: </strong>Kathryn Wantlin, Chongyi Zheng, Benjamin Eysenbach</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17059">https://arxiv.org/abs/2510.17059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17059">https://arxiv.org/pdf/2510.17059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17059]] Consistent Zero-Shot Imitation with Contrastive Goal Inference(https://arxiv.org/abs/2510.17059)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the same way that generative models today conduct most of their training in a self-supervised fashion, how can agentic models conduct their training in a self-supervised fashion, interactively exploring, learning, and preparing to quickly adapt to new tasks? A prerequisite for embodied agents deployed in real world interactions ought to be training with interaction, yet today's most successful AI models (e.g., VLMs, LLMs) are trained without an explicit notion of action. The problem of pure exploration (which assumes no data as input) is well studied in the reinforcement learning literature and provides agents with a wide array of experiences, yet it fails to prepare them for rapid adaptation to new tasks. Today's language and vision models are trained on data provided by humans, which provides a strong inductive bias for the sorts of tasks that the model will have to solve (e.g., modeling chords in a song, phrases in a sonnet, sentences in a medical record). However, when they are prompted to solve a new task, there is a faulty tacit assumption that humans spend most of their time in the most rewarding states. The key contribution of our paper is a method for pre-training interactive agents in a self-supervised fashion, so that they can instantly mimic human demonstrations. Our method treats goals (i.e., observations) as the atomic construct. During training, our method automatically proposes goals and practices reaching them, building off prior work in reinforcement learning exploration. During evaluation, our method solves an (amortized) inverse reinforcement learning problem to explain demonstrations as optimal goal-reaching behavior. Experiments on standard benchmarks (not designed for goal-reaching) show that our approach outperforms prior methods for zero-shot imitation.</li>
<li><strong>摘要：</strong>就像今天的生成模型以自我监督的方式进行大部分训练一样，代理模型如何以自我监督的方式进行训练，交互式探索、学习并准备快速适应新任务？在现实世界交互中部署实体代理的先决条件应该是通过交互进行训练，但当今最成功的人工智能模型（例如 VLM、LLM）是在没有明确的动作概念的情况下进行训练的。纯粹探索的问题（假设没有数据作为输入）在强化学习文献中得到了很好的研究，并为代理提供了广泛的经验，但它无法让他们为快速适应新任务做好准备。今天的语言和视觉模型是根据人类提供的数据进行训练的，这为模型必须解决的各种任务提供了很强的归纳偏差（例如，对歌曲中的和弦、十四行诗中的短语、医疗记录中的句子进行建模）。然而，当他们被提示解决一项新任务时，存在一个错误的默认假设，即人类将大部分时间花在最有价值的状态上。我们论文的关键贡献是一种以自我监督的方式预训练交互式代理的方法，以便它们可以立即模仿人类演示。我们的方法将目标（即观察）视为原子构造。在训练过程中，我们的方法会自动提出目标和实现目标的实践，以强化学习探索中的先前工作为基础。在评估过程中，我们的方法解决了（摊销的）逆强化学习问题，以将演示解释为最佳的目标达成行为。标准基准测试（不是为实现目标而设计）的实验表明，我们的方法优于先前的零样本模仿方法。</li>
</ul>

<h3>Title: Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement</h3>
<ul>
<li><strong>Authors: </strong>Xiaogang Xu, Jian Wang, Yunfan Lu, Ruihang Chu, Ruixing Wang, Jiafei Wu, Bei Yu, Liang Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17105">https://arxiv.org/abs/2510.17105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17105">https://arxiv.org/pdf/2510.17105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17105]] Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement(https://arxiv.org/abs/2510.17105)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based methods, leveraging pre-trained large models like Stable Diffusion via ControlNet, have achieved remarkable performance in several low-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods often sacrifice content fidelity to attain higher perceptual realism. This issue is exacerbated in low-light scenarios, where severely degraded information caused by the darkness limits effective control. We identify two primary causes of fidelity loss: the absence of suitable conditional latent modeling and the lack of bidirectional interaction between the conditional latent and noisy latent in the diffusion process. To address this, we propose a novel optimization strategy for conditioning in pre-trained diffusion models, enhancing fidelity while preserving realism and aesthetics. Our method introduces a mechanism to recover spatial details lost during VAE encoding, i.e., a latent refinement pipeline incorporating generative priors. Additionally, the refined latent condition interacts dynamically with the noisy latent, leading to improved restoration performance. Our approach is plug-and-play, seamlessly integrating into existing diffusion networks to provide more effective control. Extensive experiments demonstrate significant fidelity improvements in PTDB methods.</li>
<li><strong>摘要：</strong>基于扩散的方法利用预先训练的大型模型（例如通过 ControlNet 实现的稳定扩散），在多个低级视觉任务中取得了显着的性能。然而，基于预训练扩散（PTDB）的方法通常会牺牲内容保真度来获得更高的感知真实感。在弱光场景中，这个问题会更加严重，因为黑暗导致信息严重退化，限制了有效控制。我们确定了保真度损失的两个主要原因：缺乏合适的条件潜在模型以及扩散过程中条件潜在和噪声潜在之间缺乏双向相互作用。为了解决这个问题，我们提出了一种新颖的优化策略，用于在预先训练的扩散模型中进行调节，在提高保真度的同时保留真实性和美观性。我们的方法引入了一种机制来恢复 VAE 编码期间丢失的空间细节，即结合生成先验的潜在细化管道。此外，经过改进的潜在条件与噪声潜在条件动态地相互作用，从而提高了恢复性能。我们的方法是即插即用的，无缝集成到现有的扩散网络中，以提供更有效的控制。大量实验证明 PTDB 方法的保真度显着提高。</li>
</ul>

<h3>Title: Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Rishi Sonthalia, Raj Rao Nadakuditi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17120">https://arxiv.org/abs/2510.17120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17120">https://arxiv.org/pdf/2510.17120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17120]] Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation(https://arxiv.org/abs/2510.17120)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce a novel regularization scheme for autoencoders based on matricial free energy. Our approach defines a differentiable loss function in terms of the singular values of the code matrix (code dimension x batch size). From the standpoint of free probability an d random matrix theory, this loss achieves its minimum when the singular value distribution of the code matrix coincides with that of an appropriately sculpted random metric with i.i.d. Gaussian entries. Empirical simulations demonstrate that minimizing the negative matricial free energy through standard stochastic gradient-based training yields Gaussian-like codes that generalize across training and test sets. Building on this foundation, we propose a matricidal free energy maximizing autoencoder that reliably produces Gaussian codes and show its application to underdetermined inverse problems.</li>
<li><strong>摘要：</strong>我们引入了一种基于矩阵自由能的自动编码器的新颖正则化方案。我们的方法根据代码矩阵的奇异值（代码维度 x 批量大小）定义了可微的损失函数。从自由概率和随机矩阵理论的角度来看，当代码矩阵的奇异值分布与具有 i.i.d 的适当雕刻的随机度量的奇异值分布一致时，这种损失达到最小值。高斯条目。经验模拟表明，通过基于标准随机梯度的训练最小化负矩阵自由能会产生可在训练和测试集上推广的类高斯代码。在此基础上，我们提出了一种杀母自由能最大化自动编码器，它能够可靠地生成高斯码，并展示其在欠定逆问题中的应用。</li>
</ul>

<h3>Title: GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection</h3>
<ul>
<li><strong>Authors: </strong>Xin Gao, Jiyao Liu, Guanghao Li, Yueming Lyu, Jianxiong Gao, Weichen Yu, Ningsheng Xu, Liang Wang, Caifeng Shan, Ziwei Liu, Chenyang Si</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17131">https://arxiv.org/abs/2510.17131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17131">https://arxiv.org/pdf/2510.17131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17131]] GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection(https://arxiv.org/abs/2510.17131)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements have explored text-to-image diffusion models for synthesizing out-of-distribution (OOD) samples, substantially enhancing the performance of OOD detection. However, existing approaches typically rely on perturbing text-conditioned embeddings, resulting in semantic instability and insufficient shift diversity, which limit generalization to realistic OOD. To address these challenges, we propose GOOD, a novel and flexible framework that directly guides diffusion sampling trajectories towards OOD regions using off-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level guidance: (1) Image-level guidance based on the gradient of log partition to reduce input likelihood, drives samples toward low-density regions in pixel space. (2) Feature-level guidance, derived from k-NN distance in the classifier's latent space, promotes sampling in feature-sparse regions. Hence, this dual-guidance design enables more controllable and diverse OOD sample generation. Additionally, we introduce a unified OOD score that adaptively combines image and feature discrepancies, enhancing detection robustness. We perform thorough quantitative and qualitative analyses to evaluate the effectiveness of GOOD, demonstrating that training with samples generated by GOOD can notably enhance OOD detection performance.</li>
<li><strong>摘要：</strong>最近的进展探索了用于合成分布外 (OOD) 样本的文本到图像扩散模型，大大增强了 OOD 检测的性能。然而，现有方法通常依赖于扰动文本条件嵌入，导致语义不稳定和转移多样性不足，从而限制了对现实 OOD 的泛化。为了应对这些挑战，我们提出了 GOOD，这是一种新颖且灵活的框架，它使用现成的分布内 (ID) 分类器直接引导扩散采样轨迹朝向 OOD 区域。 GOOD 结合了双级引导：（1）基于对数分区梯度的图像级引导，以减少输入可能性，将样本驱动到像素空间中的低密度区域。 (2) 特征级指导，源自分类器潜在空间中的 k-NN 距离，促进特征稀疏区域中的采样。因此，这种双引导设计可以实现更加可控和多样化的 OOD 样本生成。此外，我们引入了统一的 OOD 评分，可以自适应地结合图像和特征差异，从而增强检测的鲁棒性。我们进行了彻底的定量和定性分析来评估 GOOD 的有效性，证明使用 GOOD 生成的样本进行训练可以显着提高 OOD 检测性能。</li>
</ul>

<h3>Title: In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Enhao Gu, Haolin Hou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17136">https://arxiv.org/abs/2510.17136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17136">https://arxiv.org/pdf/2510.17136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17136]] In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models(https://arxiv.org/abs/2510.17136)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The generation of high-quality, diverse, and prompt-aligned images is a central goal in image-generating diffusion models. The popular classifier-free guidance (CFG) approach improves quality and alignment at the cost of reduced variation, creating an inherent entanglement of these effects. Recent work has successfully disentangled these properties by guiding a model with a separately trained, inferior counterpart; however, this solution introduces the considerable overhead of requiring an auxiliary model. We challenge this prerequisite by introducing In-situ Autoguidance, a method that elicits guidance from the model itself without any auxiliary components. Our approach dynamically generates an inferior prediction on the fly using a stochastic forward pass, reframing guidance as a form of inference-time self-correction. We demonstrate that this zero-cost approach is not only viable but also establishes a powerful new baseline for cost-efficient guidance, proving that the benefits of self-guidance can be achieved without external models.</li>
<li><strong>摘要：</strong>生成高质量、多样化且及时对齐的图像是图像生成扩散模型的中心目标。流行的无分类器引导（CFG）方法以减少变化为代价提高了质量和对齐，从而产生了这些效应的固有纠缠。最近的工作通过用单独训练的劣质对应模型来指导模型，成功地解开了这些属性。然而，该解决方案引入了需要辅助模型的相当大的开销。我们通过引入原位自动引导来挑战这一先决条件，这种方法无需任何辅助组件即可从模型本身获得引导。我们的方法使用随机前向传递动态生成较差的预测，将指导重新构建为推理时间自我校正的一种形式。我们证明这种零成本方法不仅可行，而且还为具有成本效益的指导建立了强大的新基准，证明无需外部模型即可实现自我指导的好处。</li>
</ul>

<h3>Title: KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation</h3>
<ul>
<li><strong>Authors: </strong>WenBo Xu, Liu Liu, Li Zhang, Ran Zhang, Hao Wu, Dan Guo, Meng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17137">https://arxiv.org/abs/2510.17137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17137">https://arxiv.org/pdf/2510.17137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17137]] KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation(https://arxiv.org/abs/2510.17137)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Articulated objects, such as laptops and drawers, exhibit significant challenges for 3D reconstruction and pose estimation due to their multi-part geometries and variable joint configurations, which introduce structural diversity across different states. To address these challenges, we propose KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation, a unified framework for reconstructing diverse articulated instances and pose estimation from single view input. Specifically, we first encode complete geometry (SDFs), joint angles, and part segmentation into a structured latent space via a novel Kinematic-Aware VAE (KA-VAE). In addition, we employ two conditional diffusion models: one for regressing global pose (SE(3)) and joint parameters, and another for generating the kinematic-aware latent code from partial observations. Finally, we produce an iterative optimization module that bidirectionally refines reconstruction accuracy and kinematic parameters via Chamfer-distance minimization while preserving articulation constraints. Experimental results on synthetic, semi-synthetic, and real-world datasets demonstrate the effectiveness of our approach in accurately reconstructing articulated objects and estimating their kinematic properties.</li>
<li><strong>摘要：</strong>笔记本电脑和抽屉等铰接物体由于其多部分几何形状和可变的关节配置而在 3D 重建和姿态估计方面面临重大挑战，这在不同状态下引入了结构多样性。为了应对这些挑战，我们提出了 KineDiff3D：用于类别级铰接对象形状重建和生成的运动学感知扩散，这是一个用于重建不同铰接实例并根据单视图输入进行姿态估计的统一框架。具体来说，我们首先通过新颖的运动学感知 VAE (KA-VAE) 将完整几何 (SDF)、关节角度和零件分割编码到结构化潜在空间中。此外，我们采用两种条件扩散模型：一种用于回归全局姿态（SE（3））和关节参数，另一种用于从部分观察中生成运动学感知的潜在代码。最后，我们生成一个迭代优化模块，通过倒角距离最小化双向细化重建精度和运动学参数，同时保留关节约束。合成、半合成和真实数据集的实验结果证明了我们的方法在准确重建铰接物体并估计其运动学特性方面的有效性。</li>
</ul>

<h3>Title: GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image</h3>
<ul>
<li><strong>Authors: </strong>Yinghui Wang, Xinyu Zhang, Peng Du</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17157">https://arxiv.org/abs/2510.17157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17157">https://arxiv.org/pdf/2510.17157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17157]] GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image(https://arxiv.org/abs/2510.17157)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating editable, parametric CAD models from a single image holds great potential to lower the barriers of industrial concept design. However, current multi-modal large language models (MLLMs) still struggle with accurately inferring 3D geometry from 2D images due to limited spatial reasoning capabilities. We address this limitation by introducing GACO-CAD, a novel two-stage post-training framework. It is designed to achieve a joint objective: simultaneously improving the geometric accuracy of the generated CAD models and encouraging the use of more concise modeling procedures. First, during supervised fine-tuning, we leverage depth and surface normal maps as dense geometric priors, combining them with the RGB image to form a multi-channel input. In the context of single-view reconstruction, these priors provide complementary spatial cues that help the MLLM more reliably recover 3D geometry from 2D observations. Second, during reinforcement learning, we introduce a group length reward that, while preserving high geometric fidelity, promotes the generation of more compact and less redundant parametric modeling sequences. A simple dynamic weighting strategy is adopted to stabilize training. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD achieves state-of-the-art performance under the same MLLM backbone, consistently outperforming existing methods in terms of code validity, geometric accuracy, and modeling conciseness.</li>
<li><strong>摘要：</strong>从单个图像生成可编辑的参数化 CAD 模型对于降低工业概念设计的障碍具有巨大的潜力。然而，由于空间推理能力有限，当前的多模态大语言模型 (MLLM) 仍然难以从 2D 图像准确推断 3D 几何。我们通过引入 GACO-CAD（一种新颖的两阶段后训练框架）来解决这一限制。它旨在实现一个共同目标：同时提高生成的 CAD 模型的几何精度，并鼓励使用更简洁的建模程序。首先，在监督微调过程中，我们利用深度和表面法线贴图作为密集几何先验，将它们与 RGB 图像相结合以形成多通道输入。在单视图重建的背景下，这些先验提供了互补的空间线索，帮助 MLLM 更可靠地从 2D 观测中恢复 3D 几何形状。其次，在强化学习过程中，我们引入了组长度奖励，在保持高几何保真度的同时，促进生成更紧凑和更少冗余的参数化建模序列。采用简单的动态加权策略来稳定训练。 DeepCAD 和 Fusion360 数据集上的实验表明，GACO-CAD 在相同的 MLLM 主干下实现了最先进的性能，在代码有效性、几何精度和建模简洁性方面始终优于现有方法。</li>
</ul>

<h3>Title: Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling</h3>
<ul>
<li><strong>Authors: </strong>Feihong Yan, Peiru Wang, Yao Zhu, Kaiyu Pang, Qingyan Wei, Huiqi Li, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17171">https://arxiv.org/abs/2510.17171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17171">https://arxiv.org/pdf/2510.17171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17171]] Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling(https://arxiv.org/abs/2510.17171)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Masked Autoregressive (MAR) models promise better efficiency in visual generation than autoregressive (AR) models for the ability of parallel generation, yet their acceleration potential remains constrained by the modeling complexity of spatially correlated visual tokens in a single step. To address this limitation, we introduce Generation then Reconstruction (GtR), a training-free hierarchical sampling strategy that decomposes generation into two stages: structure generation establishing global semantic scaffolding, followed by detail reconstruction efficiently completing remaining tokens. Assuming that it is more difficult to create an image from scratch than to complement images based on a basic image framework, GtR is designed to achieve acceleration by computing the reconstruction stage quickly while maintaining the generation quality by computing the generation stage slowly. Moreover, observing that tokens on the details of an image often carry more semantic information than tokens in the salient regions, we further propose Frequency-Weighted Token Selection (FTS) to offer more computation budget to tokens on image details, which are localized based on the energy of high frequency information. Extensive experiments on ImageNet class-conditional and text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1), substantially outperforming existing acceleration methods across various model scales and generation tasks. Our codes will be released in this https URL.</li>
<li><strong>摘要：</strong>蒙蔽自回归 (MAR) 模型在并行生成能力方面比自回归 (AR) 模型在视觉生成方面具有更高的效率，但其加速潜力仍然受到一步中空间相关视觉标记的建模复杂性的限制。为了解决这个限制，我们引入了生成然后重建（GtR），这是一种免训练的分层采样策略，它将生成分解为两个阶段：结构生成建立全局语义脚手架，然后有效地完成剩余标记的细节重建。假设从头开始创建图像比基于基本图像框架补充图像更困难，GtR 的设计目的是通过快速计算重建阶段来实现加速，同时通过缓慢计算生成阶段来保持生成质量。此外，观察到图像细节上的标记通常比显着区域中的标记携带更多语义信息，我们进一步提出频率加权标记选择（FTS），为图像细节上的标记提供更多的计算预算，这些标记是基于高频信息的能量进行局部化的。对 ImageNet 类条件和文本到图像生成的大量实验表明，MAR-H 的加速速度提高了 3.72 倍，同时保持了相当的质量（例如，FID：1.59、IS：304.4 与原始 1.59、299.1），在各种模型规模和生成任务中大大优于现有的加速方法。我们的代码将在此 https URL 中发布。</li>
</ul>

<h3>Title: Soft-Masked Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Michael Hersche, Samuel Moor-Smith, Thomas Hofmann, Abbas Rahimi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17206">https://arxiv.org/abs/2510.17206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17206">https://arxiv.org/pdf/2510.17206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17206]] Soft-Masked Diffusion Language Models(https://arxiv.org/abs/2510.17206)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated strong potential in language modeling, offering various advantages over traditional autoregressive approaches. Their ability to generate and revise entire responses in parallel enables faster generation and built-in self-correction mechanisms. Most modern diffusion-based language models employ masked diffusion, where decoding involves iteratively processing masked tokens based on a binary decision: either retaining the mask or replacing it with the predicted token. However, this binary choice discards valuable predictive information when the mask is retained. To address this limitation, we introduce soft-masking (SM), a novel method that dynamically blends the embedding of the mask token with the embeddings of the top-$k$ predicted tokens from the previous decoding step, for each retained mask. This provides the model with a more informative prior, preserving context from earlier computations and allowing partial information about masked tokens to propagate beyond a single step. We propose a training methodology that adapts a pretrained masked diffusion language model to incorporate SM. We demonstrate that continuing pretraining a 169M parameter model with SM leads to improved perplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art diffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently improves performance across multiple coding benchmarks, particularly in high-throughput settings.</li>
<li><strong>摘要：</strong>扩散模型在语言建模方面表现出了强大的潜力，与传统的自回归方法相比具有多种优势。它们能够并行生成和修改整个响应，从而实现更快的生成和内置的自我纠正机制。大多数现代基于扩散的语言模型都采用掩码扩散，其中解码涉及基于二元决策迭代处理掩码标记：要么保留掩码，要么用预测的标记替换它。然而，当保留掩模时，这种二元选择会丢弃有价值的预测信息。为了解决这个限制，我们引入了软掩码（SM），这是一种新颖的方法，对于每个保留的掩码，动态地将掩码令牌的嵌入与来自先前解码步骤的 top-$k$ 预测令牌的嵌入混合。这为模型提供了更丰富的先验信息，保留了早期计算的上下文，并允许有关屏蔽标记的部分信息传播到单个步骤之外。我们提出了一种训练方法，该方法采用预训练的掩蔽扩散语言模型来合并 SM。我们证明，继续使用 SM 预训练 169M 参数模型可以提高困惑度和 MUVE 分数。此外，我们还使用 SM 微调了两个最先进的扩散模型 Dream-7B 和 Dream-Coder-7B。 SM 持续提高多个编码基准的性能，特别是在高吞吐量设置中。</li>
</ul>

<h3>Title: Adaptive Discretization for Consistency Models</h3>
<ul>
<li><strong>Authors: </strong>Jiayu Bai, Zhanbo Feng, Zhijie Deng, Tianqi Hou, Robert C. Qiu, Zenan Ling</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17266">https://arxiv.org/abs/2510.17266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17266">https://arxiv.org/pdf/2510.17266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17266]] Adaptive Discretization for Consistency Models(https://arxiv.org/abs/2510.17266)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Consistency Models (CMs) have shown promise for efficient one-step generation. However, most existing CMs rely on manually designed discretization schemes, which can cause repeated adjustments for different noise schedules and datasets. To address this, we propose a unified framework for the automatic and adaptive discretization of CMs, formulating it as an optimization problem with respect to the discretization step. Concretely, during the consistency training process, we propose using local consistency as the optimization objective to ensure trainability by avoiding excessive discretization, and taking global consistency as a constraint to ensure stability by controlling the denoising error in the training target. We establish the trade-off between local and global consistency with a Lagrange multiplier. Building on this framework, we achieve adaptive discretization for CMs using the Gauss-Newton method. We refer to our approach as ADCMs. Experiments demonstrate that ADCMs significantly improve the training efficiency of CMs, achieving superior generative performance with minimal training overhead on both CIFAR-10 and ImageNet. Moreover, ADCMs exhibit strong adaptability to more advanced DM variants. Code is available at this https URL.</li>
<li><strong>摘要：</strong>一致性模型 (CM) 已显示出高效一步生成的前景。然而，大多数现有的 CM 依赖于手动设计的离散化方案，这可能会导致对不同的噪声表和数据集进行重复调整。为了解决这个问题，我们提出了一个用于 CM 自动和自适应离散化的统一框架，将其表述为关于离散化步骤的优化问题。具体来说，在一致性训练过程中，我们建议使用局部一致性作为优化目标，通过避免过度离散化来确保可训练性，并以全局一致性作为约束，通过控制训练目标中的去噪误差来确保稳定性。我们使用拉格朗日乘子建立局部一致性和全局一致性之间的权衡。在此框架的基础上，我们使用高斯-牛顿方法实现了 CM 的自适应离散化。我们将我们的方法称为 ADCM。实验表明，ADCM 显着提高了 CM 的训练效率，在 CIFAR-10 和 ImageNet 上以最小的训练开销实现了卓越的生成性能。此外，ADCM 对更先进的 DM 变体表现出强大的适应性。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Gyuhwan Park, Kihyun Na, Injung Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17330">https://arxiv.org/abs/2510.17330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17330">https://arxiv.org/pdf/2510.17330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17330]] CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration(https://arxiv.org/abs/2510.17330)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>The significance of license plate image restoration goes beyond the preprocessing stage of License Plate Recognition (LPR) systems, as it also serves various purposes, including increasing evidential value, enhancing the clarity of visual interface, and facilitating further utilization of license plate images. We propose a novel diffusion-based framework with character-level guidance, CharDiff, which effectively restores and recognizes severely degraded license plate images captured under realistic conditions. CharDiff leverages fine-grained character-level priors extracted through external segmentation and Optical Character Recognition (OCR) modules tailored for low-quality license plate images. For precise and focused guidance, CharDiff incorporates a novel Character-guided Attention through Region-wise Masking (CHARM) module, which ensures that each character's guidance is restricted to its own region, thereby avoiding interference with other regions. In experiments, CharDiff significantly outperformed the baseline restoration models in both restoration quality and recognition accuracy, achieving a 28% relative reduction in CER on the Roboflow-LP dataset, compared to the best-performing baseline model. These results indicate that the structured character-guided conditioning effectively enhances the robustness of diffusion-based license plate restoration and recognition in practical deployment scenarios.</li>
<li><strong>摘要：</strong>车牌图像恢复的重要性超出了车牌识别（LPR）系统的预处理阶段，因为它还具有多种目的，包括增加证据价值、增强视觉界面的清晰度以及促进车牌图像的进一步利用。我们提出了一种具有字符级指导的新型基于扩散的框架 CharDiff，它可以有效地恢复和识别在现实条件下捕获的严重退化的车牌图像。 CharDiff 利用通过外部分割提取的细粒度字符级先验和专为低质量车牌图像定制的光学字符识别 (OCR) 模块。为了实现精确和集中的引导，CharDiff 结合了一种新颖的通过区域屏蔽进行角色引导注意 (CHARM) 模块，该模块可确保每个角色的引导仅限于其自己的区域，从而避免与其他区域的干扰。在实验中，CharDiff 在恢复质量和识别精度方面均显着优于基线恢复模型，与性能最佳的基线模型相比，在 Roboflow-LP 数据集上实现了 28% 的 CER 相对降低。这些结果表明，结构化字符引导条件有效增强了实际部署场景中基于扩散的车牌恢复和识别的鲁棒性。</li>
</ul>

<h3>Title: iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA</h3>
<ul>
<li><strong>Authors: </strong>Zhaoran Zhao, Xinli Yue, Jianhui Sun, Yuhao Xie, Tao Shao, Liangchao Yao, Fan Xia, Yuetang Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17332">https://arxiv.org/abs/2510.17332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17332">https://arxiv.org/pdf/2510.17332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17332]] iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA(https://arxiv.org/abs/2510.17332)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Image Quality Assessment (IQA) has progressed from scalar quality prediction to more interpretable, human-aligned evaluation paradigms. In this work, we address the emerging challenge of detailed and explainable IQA by proposing iDETEX-a unified multimodal large language model (MLLM) capable of simultaneously performing three key tasks: quality grounding, perception, and description. To facilitate efficient and generalizable training across these heterogeneous subtasks, we design a suite of task-specific offline augmentation modules and a data mixing strategy. These are further complemented by online enhancement strategies to fully exploit multi-sourced supervision. We validate our approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves state-of-the-art performance across all subtasks. Our model ranks first in the ICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its effectiveness and robustness in delivering accurate and interpretable quality assessments.</li>
<li><strong>摘要：</strong>图像质量评估 (IQA) 已从标量质量预测发展为更可解释、更符合人类需求的评估范例。在这项工作中，我们通过提出 iDETEX——一种统一的多模态大语言模型 (MLLM)，能够同时执行三个关键任务：质量基础、感知和描述来解决详细和可解释的 IQA 的新挑战。为了促进跨这些异构子任务的高效和通用训练，我们设计了一套特定于任务的离线增强模块和数据混合策略。在线增强策略进一步补充了这些策略，以充分利用多源监督。我们在大规模 ViDA-UGC 基准测试上验证了我们的方法，其中 iDETEX 在所有子任务中都实现了最先进的性能。我们的模型在 ICCV MIPI 2025 详细图像质量评估挑战赛中排名第一，证明了其在提供准确且可解释的质量评估方面的有效性和稳健性。</li>
</ul>

<h3>Title: Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise</h3>
<ul>
<li><strong>Authors: </strong>Paweł Borsukiewicz, Fadi Boutros, Iyiola E. Olatunji, Charles Beumier, Wendkûuni C. Ouedraogo, Jacques Klein, Tegawendé F. Bissyandé</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17372">https://arxiv.org/abs/2510.17372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17372">https://arxiv.org/pdf/2510.17372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17372]] Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise(https://arxiv.org/abs/2510.17372)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The deployment of facial recognition systems has created an ethical dilemma: achieving high accuracy requires massive datasets of real faces collected without consent, leading to dataset retractions and potential legal liabilities under regulations like GDPR. While synthetic facial data presents a promising privacy-preserving alternative, the field lacks comprehensive empirical evidence of its viability. This study addresses this critical gap through extensive evaluation of synthetic facial recognition datasets. We present a systematic literature review identifying 25 synthetic facial recognition datasets (2018-2025), combined with rigorous experimental validation. Our methodology examines seven key requirements for privacy-preserving synthetic data: identity leakage prevention, intra-class variability, identity separability, dataset scale, ethical data sourcing, bias mitigation, and benchmark reliability. Through experiments involving over 10 million synthetic samples, extended by a comparison of results reported on five standard benchmarks, we provide the first comprehensive empirical assessment of synthetic data's capability to replace real datasets. Best-performing synthetic datasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and 94.91% respectively, surpassing established real datasets including CASIA-WebFace (94.70%). While those images remain private, publicly available alternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our findings reveal that they ensure proper intra-class variability while maintaining identity separability. Demographic bias analysis shows that, even though synthetic data inherits limited biases, it offers unprecedented control for bias mitigation through generation parameters. These results establish synthetic facial data as a scientifically viable and ethically imperative alternative for facial recognition research.</li>
<li><strong>摘要：</strong>面部识别系统的部署造成了道德困境：要实现高精度，需要在未经同意的情况下收集大量真实面部数据集，从而导致数据集被撤回，并根据 GDPR 等法规承担潜在的法律责任。虽然合成面部数据提供了一种有前景的隐私保护替代方案，但该领域缺乏其可行性的全面经验证据。这项研究通过对合成面部识别数据集的广泛评估来解决这一关键差距。我们提出了系统的文献综述，识别了 25 个合成面部识别数据集（2018-2025 年），并结合严格的实验验证。我们的方法研究了保护隐私的合成数据的七个关键要求：身份泄露预防、类内可变性、身份可分离性、数据集规模、道德数据源、偏见缓解和基准可靠性。通过涉及超过 1000 万个合成样本的实验，并通过比较五个标准基准报告的结果进行扩展，我们首次对合成数据替代真实数据集的能力进行了全面的实证评估。性能最佳的合成数据集（VariFace、VIGFace）分别实现了 95.67% 和 94.91% 的识别准确率，超过了包括 CASIA-WebFace 在内的现有真实数据集（94.70%）。虽然这些图像仍然是私有的，但公开可用的替代品 Vec2Face (93.52%) 和 CemiFace (93.22%) 紧随其后。我们的研究结果表明，它们确保了适当的类内变异性，同时保持了身份可分离性。人口统计偏差分析表明，尽管合成数据继承了有限的偏差，但它通过生成参数为减轻偏差提供了前所未有的控制。这些结果确立了合成面部数据作为面部识别研究的科学上可行且道德上必要的替代方案。</li>
</ul>

<h3>Title: Model Metamers Reveal Invariances in Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Wei Xu, Xiaoyi Jiang, Lixiang Xu, Dechao Tang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17378">https://arxiv.org/abs/2510.17378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17378">https://arxiv.org/pdf/2510.17378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17378]] Model Metamers Reveal Invariances in Graph Neural Networks(https://arxiv.org/abs/2510.17378)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In recent years, deep neural networks have been extensively employed in perceptual systems to learn representations endowed with invariances, aiming to emulate the invariance mechanisms observed in the human brain. However, studies in the visual and auditory domains have confirmed that significant gaps remain between the invariance properties of artificial neural networks and those of humans. To investigate the invariance behavior within graph neural networks (GNNs), we introduce a model ``metamers'' generation technique. By optimizing input graphs such that their internal node activations match those of a reference graph, we obtain graphs that are equivalent in the model's representation space, yet differ significantly in both structure and node features. Our theoretical analysis focuses on two aspects: the local metamer dimension for a single node and the activation-induced volume change of the metamer manifold. Utilizing this approach, we uncover extreme levels of representational invariance across several classic GNN architectures. Although targeted modifications to model architecture and training strategies can partially mitigate this excessive invariance, they fail to fundamentally bridge the gap to human-like invariance. Finally, we quantify the deviation between metamer graphs and their original counterparts, revealing unique failure modes of current GNNs and providing a complementary benchmark for model evaluation.</li>
<li><strong>摘要：</strong>近年来，深度神经网络已广泛应用于感知系统中，以学习具有不变性的表示，旨在模拟人脑中观察到的不变性机制。然而，视觉和听觉领域的研究已经证实，人工神经网络的不变性与人类的不变性之间仍然存在显着差距。为了研究图神经网络（GNN）内的不变性行为，我们引入了模型“同元”生成技术。通过优化输入图，使其内部节点激活与参考图的内部节点激活相匹配，我们获得了在模型表示空间中等效的图，但在结构和节点特征上存在显着差异。我们的理论分析集中在两个方面：单个节点的局部同色异构体尺寸和激活引起的同色异构体流形的体积变化。利用这种方法，我们发现了几种经典 GNN 架构的极端表示不变性。尽管对模型架构和训练策略进行有针对性的修改可以部分缓解这种过度的不变性，但它们无法从根本上弥合与类人不变性的差距。最后，我们量化了同色异谱图与其原始对应图之间的偏差，揭示了当前 GNN 的独特故障模式，并为模型评估提供了补充基准。</li>
</ul>

<h3>Title: Latent Spaces Beyond Synthesis: From GANs to Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ludovica Schaerf</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17383">https://arxiv.org/abs/2510.17383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17383">https://arxiv.org/pdf/2510.17383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17383]] Latent Spaces Beyond Synthesis: From GANs to Diffusion Models(https://arxiv.org/abs/2510.17383)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper examines the evolving nature of internal representations in generative visual models, focusing on the conceptual and technical shift from GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's account of synthesis as the amalgamation of distributed representations, we propose a distinction between "synthesis in a strict sense", where a compact latent space wholly determines the generative process, and "synthesis in a broad sense," which characterizes models whose representational labor is distributed across layers. Through close readings of model architectures and a targeted experimental setup that intervenes in layerwise representations, we show how diffusion models fragment the burden of representation and thereby challenge assumptions of unified internal space. By situating these findings within media theoretical frameworks and critically engaging with metaphors such as the latent space and the Platonic Representation Hypothesis, we argue for a reorientation of how generative AI is understood: not as a direct synthesis of content, but as an emergent configuration of specialized processes.</li>
<li><strong>摘要：</strong>本文研究了生成视觉模型中内部表示的演变性质，重点关注从 GAN 和 VAE 到基于扩散的架构的概念和技术转变。借鉴Beatrice Fazi对综合作​​为分布式表征的融合的描述，我们提出了“严格意义上的综合”和“广义综合”之间的区别，其中紧凑的潜在空间完全决定了生成过程，而广义的综合则描述了其表征劳动分布于各层的模型。通过仔细阅读模型架构和干预分层表示的有针对性的实验设置，我们展示了扩散模型如何分散表示的负担，从而挑战统一内部空间的假设。通过将这些发现置于媒体理论框架内，并批判性地运用潜在空间和柏拉图表征假说等隐喻，我们主张重新定位生成式人工智能的理解方式：不是内容的直接合成，而是专业过程的紧急配置。</li>
</ul>

<h3>Title: A Conditional Diffusion Model for Probabilistic Prediction of Battery Capacity Degradation</h3>
<ul>
<li><strong>Authors: </strong>Hequn Li, Zhongwei Deng, Chunlin Jiang, Yvxin He andZhansheng Ning</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17414">https://arxiv.org/abs/2510.17414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17414">https://arxiv.org/pdf/2510.17414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17414]] A Conditional Diffusion Model for Probabilistic Prediction of Battery Capacity Degradation(https://arxiv.org/abs/2510.17414)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate prediction of lithium-ion battery capacity and its associated uncertainty is essential for reliable battery management but remains challenging due to the stochastic nature of aging. This paper presents a novel method, termed the Condition Diffusion U-Net with Attention (CDUA), which integrates feature engineering and deep learning to address this challenge. The proposed approach employs a diffusion-based generative model for time-series forecasting and incorporates attention mechanisms to enhance predictive performance. Battery capacity is first derived from real-world vehicle operation data. The most relevant features are then identified using the Pearson correlation coefficient and the XGBoost algorithm. These features are used to train the CDUA model, which comprises two core components: (1) a contextual U-Net with self-attention to capture complex temporal dependencies, and (2) a denoising network to reconstruct accurate capacity values from noisy observations. Experimental validation on the real-world vehicle data demonstrates that the proposed CDUA model achieves a relative Mean Absolute Error (MAE) of 0.94% and a relative Root Mean Square Error (RMSE) of 1.14%, with a narrow 95% confidence interval of 3.74% in relative width. These results confirm that CDUA provides both accurate capacity estimation and reliable uncertainty quantification. Comparative experiments further verify its robustness and superior performance over existing mainstream approaches.</li>
<li><strong>摘要：</strong>准确预测锂离子电池容量及其相关的不确定性对于可靠的电池管理至关重要，但由于老化的随机性，仍然具有挑战性。本文提出了一种新颖的方法，称为带有注意力的条件扩散 U-Net (CDUA)，它集成了特征工程和深度学习来应对这一挑战。所提出的方法采用基于扩散的生成模型进行时间序列预测，并结合注意力机制来增强预测性能。电池容量首先来自真实的车辆运行数据。然后使用 Pearson 相关系数和 XGBoost 算法识别最相关的特征。这些特征用于训练 CDUA 模型，该模型包含两个核心组件：(1) 具有自注意力的上下文 U-Net，用于捕获复杂的时间依赖性；(2) 去噪网络，用于从噪声观测中重建准确的容量值。对真实车辆数据的实验验证表明，所提出的 CDUA 模型的相对平均绝对误差 (MAE) 为 0.94%，相对均方根误差 (RMSE) 为 1.14%，相对宽度的 95% 置信区间较窄，为 3.74%。这些结果证实 CDUA 提供了准确的容量估计和可靠的不确定性量化。对比实验进一步验证了其相对于现有主流方法的鲁棒性和优越性能。</li>
</ul>

<h3>Title: Diffusion Models as Dataset Distillation Priors</h3>
<ul>
<li><strong>Authors: </strong>Duo Su, Huyu Wu, Huanran Chen, Yiming Shi, Yuzhu Wang, Xi Ye, Jun Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17421">https://arxiv.org/abs/2510.17421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17421">https://arxiv.org/pdf/2510.17421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17421]] Diffusion Models as Dataset Distillation Priors(https://arxiv.org/abs/2510.17421)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Dataset distillation aims to synthesize compact yet informative datasets from large ones. A significant challenge in this field is achieving a trifecta of diversity, generalization, and representativeness in a single distilled dataset. Although recent generative dataset distillation methods adopt powerful diffusion models as their foundation models, the inherent representativeness prior in diffusion models is overlooked. Consequently, these approaches often necessitate the integration of external constraints to enhance data quality. To address this, we propose Diffusion As Priors (DAP), which formalizes representativeness by quantifying the similarity between synthetic and real data in feature space using a Mercer kernel. We then introduce this prior as guidance to steer the reverse diffusion process, enhancing the representativeness of distilled samples without any retraining. Extensive experiments on large-scale datasets, such as ImageNet-1K and its subsets, demonstrate that DAP outperforms state-of-the-art methods in generating high-fidelity datasets while achieving superior cross-architecture generalization. Our work not only establishes a theoretical connection between diffusion priors and the objectives of dataset distillation but also provides a practical, training-free framework for improving the quality of the distilled dataset.</li>
<li><strong>摘要：</strong>数据集蒸馏的目的是从大型数据集中合成紧凑但信息丰富的数据集。该领域的一个重大挑战是在单个精炼数据集中实现多样性、泛化性和代表性的三重效果。尽管最近的生成数据集蒸馏方法采用强大的扩散模型作为其基础模型，但忽略了扩散模型先验的固有代表性。因此，这些方法通常需要整合外部约束以提高数据质量。为了解决这个问题，我们提出了扩散先验（DAP），它通过使用 Mercer 内核量化特征空间中合成数据和真实数据之间的相似性来形式化代表性。然后，我们引入此先验作为引导反向扩散过程的指导，从而在无需任何再训练的情况下增强蒸馏样品的代表性。对大规模数据集（例如 ImageNet-1K 及其子集）的大量实验表明，DAP 在生成高保真数据集方面优于最先进的方法，同时实现了卓越的跨架构泛化。我们的工作不仅在扩散先验和数据集蒸馏的目标之间建立了理论联系，而且还提供了一个实用的、免训练的框架来提高蒸馏数据集的质量。</li>
</ul>

<h3>Title: Towards geological inference with process-based and deep generative modeling, part 2: inversion of fluvial deposits and latent-space disentanglement</h3>
<ul>
<li><strong>Authors: </strong>Guillaume Rongier, Luk Peeters</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17478">https://arxiv.org/abs/2510.17478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17478">https://arxiv.org/pdf/2510.17478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17478]] Towards geological inference with process-based and deep generative modeling, part 2: inversion of fluvial deposits and latent-space disentanglement(https://arxiv.org/abs/2510.17478)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>High costs and uncertainties make subsurface decision-making challenging, as acquiring new data is rarely scalable. Embedding geological knowledge directly into predictive models offers a valuable alternative. A joint approach enables just that: process-based models that mimic geological processes can help train generative models that make predictions more efficiently. This study explores whether a generative adversarial network (GAN) - a type of deep-learning algorithm for generative modeling - trained to produce fluvial deposits can be inverted to match well and seismic data. Four inversion approaches applied to three test samples with 4, 8, and 20 wells struggled to match these well data, especially as the well number increased or as the test sample diverged from the training data. The key bottleneck lies in the GAN's latent representation: it is entangled, so samples with similar sedimentological features are not necessarily close in the latent space. Label conditioning or latent overparameterization can partially disentangle the latent space during training, although not yet sufficiently for a successful inversion. Fine-tuning the GAN to restructure the latent space locally reduces mismatches to acceptable levels for all test cases, with and without seismic data. But this approach depends on an initial, partially successful inversion step, which influences the quality and diversity of the final samples. Overall, GANs can already handle the tasks required for their integration into geomodeling workflows. We still need to further assess their robustness, and how to best leverage them in support of geological interpretation.</li>
<li><strong>摘要：</strong>高成本和不确定性使得地下决策具有挑战性，因为获取新数据很少具有可扩展性。将地质知识直接嵌入到预测模型中提供了一种有价值的替代方案。联合方法可以实现这一点：模仿地质过程的基于过程的模型可以帮助训练生成模型，从而更有效地进行预测。本研究探讨了生成对抗网络（GAN）——一种用于生成建模的深度学习算法——经过训练以产生河流沉积物，是否可以反演以匹配油井和地震数据。应用于 4、8 和 20 口井的三个测试样本的四种反演方法很难匹配这些井数据，特别是当井数增加或测试样本偏离训练数据时。关键瓶颈在于 GAN 的潜在表示：它是纠缠的，因此具有相似沉积学特征的样本在潜在空间中不一定很接近。标签调节或潜在超参数化可以在训练期间部分解开潜在空间，尽管还不足以成功进行反演。微调 GAN 以在局部重构潜在空间，将所有测试用例的不匹配降低到可接受的水平，无论有或没有地震数据。但这种方法依赖于初始的、部分成功的反演步骤，这会影响最终样本的质量和多样性。总体而言，GAN 已经可以处理集成到地理建模工作流程中所需的任务。我们仍然需要进一步评估它们的稳健性，以及如何最好地利用它们来支持地质解释。</li>
</ul>

<h3>Title: MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Yongshun Zhang, Zhongyi Fan, Yonghang Zhang, Zhangzikang Li, Weifeng Chen, Zhongwei Feng, Chaoyue Wang, Peng Hou, Anxiang Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17519">https://arxiv.org/abs/2510.17519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17519">https://arxiv.org/pdf/2510.17519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17519]] MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models(https://arxiv.org/abs/2510.17519)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In recent years, large-scale generative models for visual content (\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable progress. However, training large-scale video generation models remains particularly challenging and resource-intensive due to cross-modal text-video alignment, the long sequences involved, and the complex spatiotemporal dependencies. To address these challenges, we present a training framework that optimizes four pillars: (i) data processing, (ii) model architecture, (iii) training strategy, and (iv) infrastructure for large-scale video generation models. These optimizations delivered significant efficiency gains and performance improvements across all stages of data preprocessing, video compression, parameter scaling, curriculum-based pretraining, and alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent state-of-the-art video generators overall and, on e-commerce-oriented video generation tasks, surpasses leading open-source baselines in human evaluations. More importantly, we open-source the complete stack, including model weights, Megatron-Core-based large-scale training code, and inference pipelines for video generation and enhancement. To our knowledge, this is the first public release of large-scale video generation training code that exploits Megatron-Core to achieve high training efficiency and near-linear multi-node scaling, details are available in \href{this https URL}{our webpage}.</li>
<li><strong>摘要：</strong>近年来，视觉内容（\textit{例如}图像、视频和 3D 对象/场景）的大规模生成模型取得了显着进展。然而，由于跨模式文本视频对齐、涉及的长序列以及复杂的时空依赖性，训练大规模视频生成模型仍然特别具有挑战性和资源密集型。为了应对这些挑战，我们提出了一个优化四个支柱的训练框架：（i）数据处理，（ii）模型架构，（iii）训练策略，以及（iv）大规模视频生成模型的基础设施。这些优化在数据预处理、视频压缩、参数缩放、基于课程的预训练和以对齐为中心的后期训练的所有阶段都带来了显着的效率提升和性能改进。我们生成的模型 MUG-V 10B 总体上与最新最先进的视频生成器相匹配，并且在面向电子商务的视频生成任务上，超越了人类评估中领先的开源基线。更重要的是，我们开源了完整的堆栈，包括模型权重、基于 Megatron-Core 的大规模训练代码以及用于视频生成和增强的推理管道。据我们所知，这是第一个公开发布的大规模视频生成训练代码，利用 Megatron-Core 实现高训练效率和近线性多节点缩放，详细信息请参见 \href{此 https URL}{我们的网页}。</li>
</ul>

<h3>Title: The Free Transformer</h3>
<ul>
<li><strong>Authors: </strong>François Fleuret</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17558">https://arxiv.org/abs/2510.17558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17558">https://arxiv.org/pdf/2510.17558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17558]] The Free Transformer(https://arxiv.org/abs/2510.17558)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose an extension of the decoder Transformer that conditions its generative process on random latent variables which are learned without supervision thanks to a variational procedure. Experimental evaluations show that allowing such a conditioning translates into substantial improvements on downstream tasks.</li>
<li><strong>摘要：</strong>我们提出了解码器 Transformer 的扩展，其生成过程以随机潜在变量为条件，这些变量是通过变分过程在没有监督的情况下学习的。实验评估表明，允许这种调节可以转化为下游任务的实质性改进。</li>
</ul>

<h3>Title: WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection</h3>
<ul>
<li><strong>Authors: </strong>Nachuan Ma, Zhengfei Song, Qiang Hu, Xiaoyu Tang, Chengxi Zhang, Rui Fan, Lihua Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17566">https://arxiv.org/abs/2510.17566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17566">https://arxiv.org/pdf/2510.17566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17566]] WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection(https://arxiv.org/abs/2510.17566)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Road crack detection is essential for intelligent infrastructure maintenance in smart cities. To reduce reliance on costly pixel-level annotations, we propose WP-CrackNet, an end-to-end weakly-supervised method that trains with only image-level labels for pixel-wise crack detection. WP-CrackNet integrates three components: a classifier generating class activation maps (CAMs), a reconstructor measuring feature inferability, and a detector producing pixel-wise road crack detection results. During training, the classifier and reconstructor alternate in adversarial learning to encourage crack CAMs to cover complete crack regions, while the detector learns from pseudo labels derived from post-processed crack CAMs. This mutual feedback among the three components improves learning stability and detection accuracy. To further boost detection performance, we design a path-aware attention module (PAAM) that fuses high-level semantics from the classifier with low-level structural cues from the reconstructor by modeling spatial and channel-wise dependencies. Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to refine crack CAMs using center Gaussian weighting and consistency constraints, enabling better pseudo-label generation. We create three image-level datasets and extensive experiments show that WP-CrackNet achieves comparable results to supervised methods and outperforms existing weakly-supervised methods, significantly advancing scalable road inspection. The source code package and datasets are available at this https URL.</li>
<li><strong>摘要：</strong>道路裂缝检测对于智慧城市的智能基础设施维护至关重要。为了减少对昂贵的像素级注释的依赖，我们提出了 WP-CrackNet，这是一种端到端的弱监督方法，仅使用图像级标签进行训练以进行像素级裂纹检测。 WP-CrackNet 集成了三个组件：生成类激活图 (CAM) 的分类器、测量特征可推断性的重建器以及生成像素级道路裂缝检测结果的检测器。在训练过程中，分类器和重建器交替进行对抗性学习，以鼓励裂纹 CAM 覆盖完整的裂纹区域，而检测器则从后处理裂纹 CAM 派生的伪标签中学习。这三个组件之间的相互反馈提高了学习稳定性和检测准确性。为了进一步提高检测性能，我们设计了一个路径感知注意模块（PAAM），通过对空间和通道依赖性进行建模，将分类器的高级语义与重建器的低级结构线索融合在一起。此外，还提出了中心增强 CAM 一致性模块 (CECCM)，使用中心高斯加权和一致性约束来细化裂纹 CAM，从而实现更好的伪标签生成。我们创建了三个图像级数据集，大量实验表明 WP-CrackNet 取得了与监督方法相当的结果，并且优于现有的弱监督方法，显着推进了可扩展的道路检查。源代码包和数据集可从此 https URL 获取。</li>
</ul>

<h3>Title: Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides</h3>
<ul>
<li><strong>Authors: </strong>Jyler Menard, R. A. Mansbach</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17569">https://arxiv.org/abs/2510.17569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17569">https://arxiv.org/pdf/2510.17569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17569]] Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides(https://arxiv.org/abs/2510.17569)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat bacterial infections. Discovering and designing such peptides is difficult because of the vast number of possible sequences of amino acids. Deep generative models, such as variational autoencoders, have shown value in peptide design due to their ability to model sequence space with a continuous-valued latent space. Although such models have already been used to great effect in biomolecular design, they still suffer from a lack of interpretability and rigorous quantification of latent space quality as a search space. We investigate (1) whether further compression of the design space via dimensionality reduction may facilitate optimization, (2) the interpretability of the spaces, and (3) how organizing latent spaces with physicochemical properties may improve the efficiency of optimizing antimicrobial activity. We find that further reduction of the latent space via dimensionality reduction can be advantageous when organizing the space with more relevant information at data availability, that using the dimensionality reduction search space can be more interpretable, and that we can organize the latent space with different physicochemical properties even at different percentages of available labels.</li>
<li><strong>摘要：</strong>抗菌肽（AMP）是一类很有前途的治疗细菌感染的疗法。由于氨基酸序列的可能数量巨大，发现和设计此类肽非常困难。深度生成模型（例如变分自动编码器）由于能够使用连续值的潜在空间对序列空间进行建模，因此在肽设计中显示出了价值。尽管此类模型已经在生物分子设计中发挥了巨大作用，但它们仍然缺乏对作为搜索空间的潜在空间质量的可解释性和严格量化。我们研究（1）通过降维进一步压缩设计空间是否可以促进优化，（2）空间的可解释性，以及（3）如何组织具有物理化学特性的潜在空间可以提高优化抗菌活性的效率。我们发现，当在数据可用性方面用更多相关信息组织空间时，通过降维进一步减少潜在空间可能是有利的，使用降维搜索空间可以更具可解释性，并且即使在不同百分比的可用标签下，我们也可以组织具有不同物理化学属性的潜在空间。</li>
</ul>

<h3>Title: ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling</h3>
<ul>
<li><strong>Authors: </strong>Shuyuan Zhang, Chenhan Jiang, Zuoou Li, Jiankang Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17603">https://arxiv.org/abs/2510.17603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17603">https://arxiv.org/pdf/2510.17603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17603]] ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling(https://arxiv.org/abs/2510.17603)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D generation from natural language offers significant potential to reduce expert manual modeling efforts and enhance accessibility to 3D assets. However, existing methods often yield unstructured meshes and exhibit poor interactivity, making them impractical for artistic workflows. To address these limitations, we represent 3D assets as shape programs and introduce ShapeCraft, a novel multi-agent framework for text-to-3D generation. At its core, we propose a Graph-based Procedural Shape (GPS) representation that decomposes complex natural language into a structured graph of sub-tasks, thereby facilitating accurate LLM comprehension and interpretation of spatial relationships and semantic shape details. Specifically, LLM agents hierarchically parse user input to initialize GPS, then iteratively refine procedural modeling and painting to produce structured, textured, and interactive 3D assets. Qualitative and quantitative experiments demonstrate ShapeCraft's superior performance in generating geometrically accurate and semantically rich 3D assets compared to existing LLM-based agents. We further show the versatility of ShapeCraft through examples of animated and user-customized editing, highlighting its potential for broader interactive applications.</li>
<li><strong>摘要：</strong>通过自然语言生成 3D 具有巨大的潜力，可以减少专家手动建模工作并增强 3D 资产的可访问性。然而，现有的方法通常会产生非结构化网格并且交互性较差，这使得它们对于艺术工作流程来说不切实际。为了解决这些限制，我们将 3D 资产表示为形状程序，并引入 ShapeCraft，这是一种用于文本到 3D 生成的新型多代理框架。其核心是，我们提出了一种基于图的过程形状（GPS）表示，将复杂的自然语言分解为子任务的结构化图，从而促进LLM对空间关系和语义形状细节的准确理解和解释。具体来说，LLM 代理分层解析用户输入以初始化 GPS，然后迭代地完善程序建模和绘画以生成结构化、纹理化和交互式 3D 资产。定性和定量实验证明，与现有的基于 LLM 的代理相比，ShapeCraft 在生成几何精确和语义丰富的 3D 资产方面具有卓越的性能。我们通过动画和用户定制编辑的示例进一步展示了 ShapeCraft 的多功能性，突出了其在更广泛的交互式应用中的潜力。</li>
</ul>

<h3>Title: CaMiT: A Time-Aware Car Model Dataset for Classification and Generation</h3>
<ul>
<li><strong>Authors: </strong>Frédéric LIN, Biruk Abere Ambaw, Adrian Popescu, Hejer Ammar, Romaric Audigier, Hervé Le Borgne (Université Paris-Saclay, CEA, List, F-91120, Palaiseau, France)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17626">https://arxiv.org/abs/2510.17626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17626">https://arxiv.org/pdf/2510.17626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17626]] CaMiT: A Time-Aware Car Model Dataset for Classification and Generation(https://arxiv.org/abs/2510.17626)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>AI systems must adapt to evolving visual environments, especially in domains where object appearances change over time. We introduce Car Models in Time (CaMiT), a fine-grained dataset capturing the temporal evolution of car models, a representative class of technological artifacts. CaMiT includes 787K labeled samples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023), supporting both supervised and self-supervised learning. Static pretraining on in-domain data achieves competitive performance with large-scale generalist models while being more resource-efficient, yet accuracy declines when models are tested across years. To address this, we propose a time-incremental classification setting, a realistic continual learning scenario with emerging, evolving, and disappearing classes. We evaluate two strategies: time-incremental pretraining, which updates the backbone, and time-incremental classifier learning, which updates only the final layer, both improving temporal robustness. Finally, we explore time-aware image generation that leverages temporal metadata during training, yielding more realistic outputs. CaMiT offers a rich benchmark for studying temporal adaptation in fine-grained visual recognition and generation.</li>
<li><strong>摘要：</strong>人工智能系统必须适应不断变化的视觉环境，尤其是在对象外观随时间变化的领域。我们引入了汽车模型时间（CaMiT），这是一个细粒度的数据集，捕获汽车模型的时间演变，这是技术工件的代表性类别。 CaMiT 包括 190 个车型的 787K 个标记样本（2007-2023 年）和 510 万个未标记样本（2005-2023 年），支持监督学习和自监督学习。对域内数据进行静态预训练可实现与大规模通用模型的竞争性能，同时提高资源效率，但当模型经过多年测试时，准确性会下降。为了解决这个问题，我们提出了一种时间增量分类设置，一个具有新兴、演变和消失类别的现实持续学习场景。我们评估了两种策略：更新主干的时间增量预训练和仅更新最后一层的时间增量分类器学习，两者都提高了时间鲁棒性。最后，我们探索时间感知图像生成，在训练期间利用时间元数据，产生更真实的输出。 CaMiT 为研究细粒度视觉识别和生成中的时间适应提供了丰富的基准。</li>
</ul>

<h3>Title: Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction</h3>
<ul>
<li><strong>Authors: </strong>Vaishnavi Visweswaraiah, Tanvi Banerjee, William Romine</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17661">https://arxiv.org/abs/2510.17661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17661">https://arxiv.org/pdf/2510.17661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17661]] Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction(https://arxiv.org/abs/2510.17661)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Suicide prediction is the key for prevention, but real data with sufficient positive samples is rare and causes extreme class imbalance. We utilized machine learning (ML) to build the model and deep learning (DL) techniques, like Generative Adversarial Networks (GAN), to generate synthetic data samples to enhance the dataset. The initial dataset contained 656 samples, with only four positive cases, prompting the need for data augmentation. A variety of machine learning models, ranging from interpretable data models to black box algorithmic models, were used. On real test data, Logistic Regression (LR) achieved a weighted precision of 0.99, a weighted recall of 0.85, and a weighted F1 score of 0.91; Random Forest (RF) showed 0.98, 0.99, and 0.99, respectively; and Support Vector Machine (SVM) achieved 0.99, 0.76, and 0.86. LR and SVM correctly identified one suicide attempt case (sensitivity:1.0) and misclassified LR(20) and SVM (31) non-attempts as attempts (specificity: 0.85 & 0.76, respectively). RF identified 0 suicide attempt cases (sensitivity: 0.0) with 0 false positives (specificity: 1.0). These results highlight the models' effectiveness, with GAN playing a key role in generating synthetic data to support suicide prevention modeling efforts.</li>
<li><strong>摘要：</strong>自杀预测是预防的关键，但具有足够阳性样本的真实数据很少，导致极端的类别不平衡。我们利用机器学习 (ML) 来构建模型，并利用生成对抗网络 (GAN) 等深度学习 (DL) 技术来生成合成数据样本以增强数据集。初始数据集包含 656 个样本，其中只有 4 个阳性案例，提示需要进行数据增强。使用了各种机器学习模型，从可解释的数据模型到黑盒算法模型。在真实测试数据上，Logistic回归（LR）取得了0.99的加权精度、0.85的加权召回率、0.91的加权F1分数；随机森林（RF）分别显示 0.98、0.99 和 0.99；支持向量机 (SVM) 达到 0.99、0.76 和 0.86。 LR 和 SVM 正确识别了 1 起自杀未遂案例（灵敏度：1.0），并将 LR(20) 和 SVM (31) 未尝试错误分类为企图（特异性：分别为 0.85 和 0.76）。 RF 识别出 0 起自杀未遂案件（敏感性：0.0），误报 0 例（特异性：1.0）。这些结果凸显了模型的有效性，GAN 在生成综合数据以支持自杀预防建模工作方面发挥着关键作用。</li>
</ul>

<h3>Title: PICABench: How Far Are We from Physically Realistic Image Editing?</h3>
<ul>
<li><strong>Authors: </strong>Yuandong Pu, Le Zhuo, Songhao Han, Jinbo Xing, Kaiwen Zhu, Shuo Cao, Bin Fu, Si Liu, Hongsheng Li, Yu Qiao, Wenlong Zhang, Xi Chen, Yihao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17681">https://arxiv.org/abs/2510.17681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17681">https://arxiv.org/pdf/2510.17681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17681]] PICABench: How Far Are We from Physically Realistic Image Editing?(https://arxiv.org/abs/2510.17681)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Image editing has achieved remarkable progress recently. Modern editing models could already follow complex instructions to manipulate the original content. However, beyond completing the editing instructions, the accompanying physical effects are the key to the generation realism. For example, removing an object should also remove its shadow, reflections, and interactions with nearby objects. Unfortunately, existing models and benchmarks mainly focus on instruction completion but overlook these physical effects. So, at this moment, how far are we from physically realistic image editing? To answer this, we introduce PICABench, which systematically evaluates physical realism across eight sub-dimension (spanning optics, mechanics, and state transitions) for most of the common editing operations (add, remove, attribute change, etc). We further propose the PICAEval, a reliable evaluation protocol that uses VLM-as-a-judge with per-case, region-level human annotations and questions. Beyond benchmarking, we also explore effective solutions by learning physics from videos and construct a training dataset PICA-100K. After evaluating most of the mainstream models, we observe that physical realism remains a challenging problem with large rooms to explore. We hope that our benchmark and proposed solutions can serve as a foundation for future work moving from naive content editing toward physically consistent realism.</li>
<li><strong>摘要：</strong>图像编辑最近取得了显着的进步。现代编辑模型已经可以遵循复杂的指令来操纵原始内容。然而，除了完成编辑指令之外，附带的物理效果也是生成真实感的关键。例如，删除一个对象还应该删除它的阴影、反射以及与附近对象的交互。不幸的是，现有的模型和基准主要关注指令完成，而忽略了这些物理影响。那么，目前我们离物理真实的图像编辑还有多远？为了回答这个问题，我们引入了 PICABench，它系统地评估了大多数常见编辑操作（添加、删除、属性更改等）的八个子维度（跨越光学、力学和状态转换）的物理真实感。我们进一步提出了 PICAEval，这是一种可靠的评估协议，它使用 VLM 作为法官，并针对每个案例、区域级别的人工注释和问题进行评估。除了基准测试之外，我们还通过从视频中学习物理并构建训练数据集 PICA-100K 来探索有效的解决方案。在评估了大多数主流模型后，我们发现物理现实主义仍然是一个具有挑战性的问题，有很大的探索空间。我们希望我们的基准和提出的解决方案可以作为未来工作从简单的内容编辑转向物理一致的现实主义的基础。</li>
</ul>

<h3>Title: GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver</h3>
<ul>
<li><strong>Authors: </strong>Aleksandr Oganov, Ilya Bykov, Eva Neudachina, Mishan Aliev, Alexander Tolmachev, Alexander Sidorov, Aleksandr Zuev, Andrey Okhotin, Denis Rakitin, Aibek Alanov</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17699">https://arxiv.org/abs/2510.17699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17699">https://arxiv.org/pdf/2510.17699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17699]] GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver(https://arxiv.org/abs/2510.17699)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While diffusion models achieve state-of-the-art generation quality, they still suffer from computationally expensive sampling. Recent works address this issue with gradient-based optimization methods that distill a few-step ODE diffusion solver from the full sampling process, reducing the number of function evaluations from dozens to just a few. However, these approaches often rely on intricate training techniques and do not explicitly focus on preserving fine-grained details. In this paper, we introduce the Generalized Solver: a simple parameterization of the ODE sampler that does not require additional training tricks and improves quality over existing approaches. We further combine the original distillation loss with adversarial training, which mitigates artifacts and enhances detail fidelity. We call the resulting method the Generalized Adversarial Solver and demonstrate its superior performance compared to existing solver training methods under similar resource constraints. Code is available at this https URL.</li>
<li><strong>摘要：</strong>虽然扩散模型实现了最先进的生成质量，但它们仍然受到计算成本高昂的采样的影响。最近的工作使用基于梯度的优化方法解决了这个问题，该方法从完整的采样过程中提取了几个步骤的 ODE 扩散求解器，将函数评估的数量从几十个减少到几个。然而，这些方法通常依赖于复杂的训练技术，并且没有明确关注保留细粒度的细节。在本文中，我们介绍了广义求解器：ODE 采样器的简单参数化，不需要额外的训练技巧，并且比现有方法提高了质量。我们进一步将原始蒸馏损失与对抗性训练相结合，从而减少伪影并提高细节保真度。我们将由此产生的方法称为广义对抗求解器，并在类似的资源限制下与现有的求解器训练方法相比，展示了其优越的性能。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns</h3>
<ul>
<li><strong>Authors: </strong>Mhd Adnan Albani, Riad Sonbol</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17703">https://arxiv.org/abs/2510.17703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17703">https://arxiv.org/pdf/2510.17703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17703]] Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns(https://arxiv.org/abs/2510.17703)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of people over the age of 60, causing motor impairments that impede hand coordination activities such as writing and drawing. Many approaches have tried to support early detection of Parkinson's disease based on hand-drawn images; however, we identified two major limitations in the related works: (1) the lack of sufficient datasets, (2) the robustness when dealing with unseen patient data. In this paper, we propose a new approach to detect Parkinson's disease that consists of two stages: The first stage classifies based on their drawing type(circle, meander, spiral), and the second stage extracts the required features from the images and detects Parkinson's disease. We overcame the previous two limitations by applying a chunking strategy where we divide each image into 2x2 chunks. Each chunk is processed separately when extracting features and recognizing Parkinson's disease indicators. To make the final classification, an ensemble method is used to merge the decisions made from each chunk. Our evaluation shows that our proposed approach outperforms the top performing state-of-the-art approaches, in particular on unseen patients. On the NewHandPD dataset our approach, it achieved 97.08% accuracy for seen patients and 94.91% for unseen patients, our proposed approach maintained a gap of only 2.17 percentage points, compared to the 4.76-point drop observed in prior work.</li>
<li><strong>摘要：</strong>帕金森病 (PD) 是一种神经退行性疾病，影响约 1% 的 60 岁以上人群，导致运动障碍，妨碍书写和绘画等手部协调活动。许多方法都试图支持基于手绘图像的帕金森病的早期检测；然而，我们发现相关工作存在两个主要局限性：（1）缺乏足够的数据集，（2）处理看不见的患者数据时的鲁棒性。在本文中，我们提出了一种检测帕金森病的新方法，该方法包括两个阶段：第一阶段根据绘图类型（圆形、曲折、螺旋）进行分类，第二阶段从图像中提取所需的特征并检测帕金森病。我们通过应用分块策略克服了前两个限制，将每个图像分为 2x2 块。在提取特征和识别帕金森病指标时，每个块都会被单独处理。为了进行最终分类，使用集成方法来合并每个块做出的决策。我们的评估表明，我们提出的方法优于表现最佳的最先进方法，特别是对于看不见的患者。在 NewHandPD 数据集上，我们的方法对于已见过的患者实现了 97.08% 的准确率，对于未见过的患者实现了 94.91% 的准确率，与之前工作中观察到的 4.76 个百分点的下降相比，我们提出的方法仅保持了 2.17 个百分点的差距。</li>
</ul>

<h3>Title: Inference-Time Compute Scaling For Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Adam Stecklov, Noah El Rimawi-Fine, Mathieu Blanchette</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17786">https://arxiv.org/abs/2510.17786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17786">https://arxiv.org/pdf/2510.17786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17786]] Inference-Time Compute Scaling For Flow Matching(https://arxiv.org/abs/2510.17786)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Allocating extra computation at inference time has recently improved sample quality in large language models and diffusion-based image generation. In parallel, Flow Matching (FM) has gained traction in language, vision, and scientific domains, but inference-time scaling methods for it remain under-explored. Concurrently, Kim et al., 2025 approach this problem but replace the linear interpolant with a non-linear variance-preserving (VP) interpolant at inference, sacrificing FM's efficient and straight sampling. Additionally, inference-time compute scaling for flow matching has only been applied to visual tasks, like image generation. We introduce novel inference-time scaling procedures for FM that preserve the linear interpolant during sampling. Evaluations of our method on image generation, and for the first time (to the best of our knowledge), unconditional protein generation, show that I) sample quality consistently improves as inference compute increases, and II) flow matching inference-time scaling can be applied to scientific domains.</li>
<li><strong>摘要：</strong>最近，在推理时分配额外的计算提高了大型语言模型和基于扩散的图像生成中的样本质量。与此同时，流程匹配 (FM) 在语言、视觉和科学领域获得了关注，但其推理时间缩放方法仍未得到充分探索。与此同时，Kim et al., 2025 解决了这个问题，但在推理时用非线性保方差 (VP) 插值代替了线性插值，牺牲了 FM 的高效和直接采样。此外，流匹配的推理时间计算缩放仅应用于视觉任务，例如图像生成。我们引入了新颖的 FM 推理时间缩放程序，可在采样期间保留线性插值。对我们的图像生成方法以及首次（据我们所知）无条件蛋白质生成的评估表明，I) 样本质量随着推理计算的增加而不断提高，II) 流匹配推理时间缩放可以应用于科学领域。</li>
</ul>

<h3>Title: UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Yang, Zhen Yang, Zi-Yi Dou, Anh Nguyen, Keen You, Omar Attia, Andrew Szot, Michael Feng, Ram Ramrakhya, Alexander Toshev, Chao Huang, Yinfei Yang, Zhe Gan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17790">https://arxiv.org/abs/2510.17790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17790">https://arxiv.org/pdf/2510.17790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17790]] UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action(https://arxiv.org/abs/2510.17790)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal agents for computer use rely exclusively on primitive actions (click, type, scroll) that require accurate visual grounding and lengthy execution chains, leading to cascading failures and performance bottlenecks. While other agents leverage rich programmatic interfaces (APIs, MCP servers, tools), computer-use agents (CUAs) remain isolated from these capabilities. We present UltraCUA, a foundation model that bridges this gap through hybrid action -- seamlessly integrating GUI primitives with high-level programmatic tool calls. To achieve this, our approach comprises four key components: (1) an automated pipeline that scales programmatic tools from software documentation, open-source repositories, and code generation; (2) a synthetic data engine producing over 17,000 verifiable tasks spanning real-world computer-use scenarios; (3) a large-scale high-quality hybrid action trajectory collection with both low-level GUI actions and high-level programmatic tool calls; and (4) a two-stage training pipeline combining supervised fine-tuning with online reinforcement learning, enabling strategic alternation between low-level and high-level actions. Experiments with our 7B and 32B models demonstrate substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA models achieve an average 22% relative improvement over base models, while being 11% faster in terms of steps. Out-of-domain evaluation on WindowsAgentArena shows our model reaches 21.7% success rate, outperforming baselines trained on Windows data. The hybrid action mechanism proves critical, reducing error propagation while maintaining execution efficiency.</li>
<li><strong>摘要：</strong>用于计算机使用的多模式代理完全依赖于原始操作（单击、键入、滚动），这些操作需要准确的视觉基础和冗长的执行链，从而导致级联故障和性能瓶颈。虽然其他代理利用丰富的编程接口（API、MCP 服务器、工具），但计算机使用代理 (CUA) 仍然与这些功能隔离。我们推出了 UltraCUA，这是一个通过混合操作弥补这一差距的基础模型——将 GUI 原语与高级编程工具调用无缝集成。为了实现这一目标，我们的方法包括四个关键组成部分：（1）自动化管道，可扩展软件文档、开源存储库和代码生成中的编程工具； (2) 一个合成数据引擎，可生成超过 17,000 个可验证的任务，涵盖现实世界的计算机使用场景； (3) 具有低级GUI动作和高级编程工具调用的大规模高质量混合动作轨迹集合； （4）将监督微调与在线强化学习相结合的两阶段训练管道，实现低级和高级动作之间的战略交替。我们的 7B 和 32B 模型的实验表明，与最先进的代理相比，有显着的改进。在 OSWorld 上，UltraCUA 模型比基本模型平均相对提高了 22%，同时在步骤方面快了 11%。 WindowsAgentArena 上的域外评估显示我们的模型成功率达到 21.7%，优于在 Windows 数据上训练的基线。事实证明，混合操作机制至关重要，可以在保持执行效率的同时减少错误传播。</li>
</ul>

<h3>Title: ConsistEdit: Highly Consistent and Precise Training-free Visual Editing</h3>
<ul>
<li><strong>Authors: </strong>Zixin Yin, Ling-Hao Chen, Lionel Ni, Xili Dai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17803">https://arxiv.org/abs/2510.17803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17803">https://arxiv.org/pdf/2510.17803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17803]] ConsistEdit: Highly Consistent and Precise Training-free Visual Editing(https://arxiv.org/abs/2510.17803)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in training-free attention control methods have enabled flexible and efficient text-guided editing capabilities for existing generation models. However, current approaches struggle to simultaneously deliver strong editing strength while preserving consistency with the source. This limitation becomes particularly critical in multi-round and video editing, where visual errors can accumulate over time. Moreover, most existing methods enforce global consistency, which limits their ability to modify individual attributes such as texture while preserving others, thereby hindering fine-grained editing. Recently, the architectural shift from U-Net to MM-DiT has brought significant improvements in generative performance and introduced a novel mechanism for integrating text and vision modalities. These advancements pave the way for overcoming challenges that previous methods failed to resolve. Through an in-depth analysis of MM-DiT, we identify three key insights into its attention mechanisms. Building on these, we propose ConsistEdit, a novel attention control method specifically tailored for MM-DiT. ConsistEdit incorporates vision-only attention control, mask-guided pre-attention fusion, and differentiated manipulation of the query, key, and value tokens to produce consistent, prompt-aligned edits. Extensive experiments demonstrate that ConsistEdit achieves state-of-the-art performance across a wide range of image and video editing tasks, including both structure-consistent and structure-inconsistent scenarios. Unlike prior methods, it is the first approach to perform editing across all inference steps and attention layers without handcraft, significantly enhancing reliability and consistency, which enables robust multi-round and multi-region editing. Furthermore, it supports progressive adjustment of structural consistency, enabling finer control.</li>
<li><strong>摘要：</strong>免训练注意力控制方法的最新进展为现有生成模型提供了灵活高效的文本引导编辑功能。然而，当前的方法很难同时提供强大的编辑能力，同时保持与源的一致性。这种限制在多轮编辑和视频编辑中变得尤为重要，因为视觉错误会随着时间的推移而累积。此外，大多数现有方法都强制执行全局一致性，这限制了它们在保留其他属性的同时修改纹理等单个属性的能力，从而阻碍了细粒度编辑。最近，从 U-Net 到 MM-DiT 的架构转变带来了生成性能的显着改进，并引入了一种集成文本和视觉模式的新颖机制。这些进步为克服以前方法未能解决的挑战铺平了道路。通过对 MM-DiT 的深入分析，我们确定了对其注意力机制的三个关键见解。在此基础上，我们提出了 ConsistEdit，一种专门为 MM-DiT 量身定制的新型注意力控制方法。 ConsistEdit 结合了仅视觉注意力控制、掩模引导的预注意力融合以及对查询、键和值标记的差异化操作，以产生一致的、提示对齐的编辑。大量实验表明，ConsistEdit 在各种图像和视频编辑任务中实现了最先进的性能，包括结构一致和结构​​不一致的场景。与之前的方法不同，它是第一种无需手工即可在所有推理步骤和注意力层上执行编辑的方法，显着增强了可靠性和一致性，从而实现了强大的多轮和多区域编辑。此外，它还支持结构一致性的渐进式调整，从而实现更精细的控制。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
