<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-08</h1>
<h3>Title: Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model</h3>
<ul>
<li><strong>Authors: </strong>Hongyang Wei, Baixin Xu, Hongbo Liu, Cyrus Wu, Jie Liu, Yi Peng, Peiyu Wang, Zexiang Liu, Jingwen He, Yidan Xietian, Chuanxin Tang, Zidong Wang, Yichen Wei, Liang Hu, Boyi Jiang, William Li, Ying He, Yang Liu, Xuchen Song, Eric Li, Yahui Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04548">https://arxiv.org/abs/2509.04548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04548">https://arxiv.org/pdf/2509.04548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04548]] Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model(https://arxiv.org/abs/2509.04548)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in multimodal models have demonstrated impressive capabilities in unified image generation and editing. However, many prominent open-source models prioritize scaling model parameters over optimizing training strategies, limiting their efficiency and performance. In this work, we present UniPic2-SD3.5M-Kontext, a 2B-parameter DiT model based on SD3.5-Medium, which achieves state-of-the-art image generation and editing while extending seamlessly into a unified multimodal framework. Our approach begins with architectural modifications to SD3.5-Medium and large-scale pre-training on high-quality data, enabling joint text-to-image generation and editing capabilities. To enhance instruction following and editing consistency, we propose a novel Progressive Dual-Task Reinforcement strategy (PDTR), which effectively strengthens both tasks in a staged manner. We empirically validate that the reinforcement phases for different tasks are mutually beneficial and do not induce negative interference. After pre-training and reinforcement strategies, UniPic2-SD3.5M-Kontext demonstrates stronger image generation and editing capabilities than models with significantly larger generation parameters-including BAGEL (7B) and Flux-Kontext (12B). Furthermore, following the MetaQuery, we connect the UniPic2-SD3.5M-Kontext and Qwen2.5-VL-7B via a connector and perform joint training to launch a unified multimodal model UniPic2-Metaquery. UniPic2-Metaquery integrates understanding, generation, and editing, achieving top-tier performance across diverse tasks with a simple and scalable training paradigm. This consistently validates the effectiveness and generalizability of our proposed training paradigm, which we formalize as Skywork UniPic 2.0.</li>
<li><strong>摘要：</strong>多模型模型的最新进展表明，在统一的图像生成和编辑中表现出了令人印象深刻的功能。但是，许多著名的开源模型将缩放模型参数优先于优化培训策略，从而限制其效率和性能。在这项工作中，我们提出了基于SD3.5-米形的2B参数DIT模型Unipic2-SD35M-Kontext，该模型可实现最新的图像生成和编辑，同时无缝扩展到统一的多模式框架。我们的方法始于对高质量数据的SD3.5中的建筑修改和大规模预训练，从而实现了联合文本到图像生成和编辑功能。为了增强遵循和编辑一致性的指导，我们提出了一种新型的渐进性双任务加强策略（PDTR），该策略有效地以分阶段的方式加强了这两个任务。我们从经验上验证了不同任务的加固阶段是互惠互利的，不会引起负干扰。经过预训练和增强策略，Unipic2-SD35M-Kontext比具有明显更大的生成参数的模型（包括Bagel（7b）（7b）和Flux-Kontext（12B）表现出更强的图像生成和编辑功能。此外，在元夸克之后，我们将Unipic2-SD35M-Kontext和QWEN2.5-VL-7B连接到连接器，并执行联合培训以启动统一的多模型unipic2-metaquery。 Unipic2-Metaquery通过简单可扩展的培训范式整合了理解，生成和编辑，在各种任务中实现了顶级性能。这始终如一地验证了我们提出的培训范式的有效性和概括性，我们将其正式为Skywork Unipic 2.0。</li>
</ul>

<h3>Title: Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping</h3>
<ul>
<li><strong>Authors: </strong>Jingyi Lu, Kai Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04582">https://arxiv.org/abs/2509.04582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04582">https://arxiv.org/pdf/2509.04582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04582]] Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping(https://arxiv.org/abs/2509.04582)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Drag-based image editing has emerged as a powerful paradigm for intuitive image manipulation. However, existing approaches predominantly rely on manipulating the latent space of generative models, leading to limited precision, delayed feedback, and model-specific constraints. Accordingly, we present Inpaint4Drag, a novel framework that decomposes drag-based editing into pixel-space bidirectional warping and image inpainting. Inspired by elastic object deformation in the physical world, we treat image regions as deformable materials that maintain natural shape under user manipulation. Our method achieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at 512x512 resolution, significantly improving the interaction experience compared to existing methods that require minutes per edit. By transforming drag inputs directly into standard inpainting formats, our approach serves as a universal adapter for any inpainting model without architecture modification, automatically inheriting all future improvements in inpainting technology. Extensive experiments demonstrate that our method achieves superior visual quality and precise control while maintaining real-time performance. Project page: this https URL</li>
<li><strong>摘要：</strong>基于阻力的图像编辑已成为直观图像操纵的强大范例。但是，现有方法主要依赖于操纵生成模型的潜在空间，从而导致精度有限，反馈延迟和特定于模型的约束。因此，我们介绍了intpaint4drag，这是一个新颖的框架，将基于阻力的编辑分解为像素空间双向翘曲和图像插入。受物理世界中弹性对象变形的启发，我们将图像区域视为可变形的材料，这些材料在用户操作下保持自然形状。我们的方法在512x512分辨率下实现了实时翘曲预览（0.01）和有效的涂漆（0.3s），与现有的每次编辑需要分钟的现有方法相比，相互作用的经验显着改善了相互作用的经验。通过将阻力输入直接转换为标准的覆盖格式，我们的方法是任何没有体系结构修改的介入模型的通用适配器，自动继承了未来在介入技术中的所有改进。广泛的实验表明，我们的方法在保持实时性能的同时，可以达到卓越的视觉质量和精确的控制。项目页面：此HTTPS URL</li>
</ul>

<h3>Title: DisPatch: Disarming Adversarial Patches in Object Detection with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jin Ma, Mohammed Aldeen, Christopher Salas, Feng Luo, Mashrur Chowdhury, Mert Pesé, Long Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04597">https://arxiv.org/abs/2509.04597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04597">https://arxiv.org/pdf/2509.04597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04597]] DisPatch: Disarming Adversarial Patches in Object Detection with Diffusion Models(https://arxiv.org/abs/2509.04597)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Object detection is fundamental to various real-world applications, such as security monitoring and surveillance video analysis. Despite their advancements, state-of-theart object detectors are still vulnerable to adversarial patch attacks, which can be easily applied to real-world objects to either conceal actual items or create non-existent ones, leading to severe consequences. Given the current diversity of adversarial patch attacks and potential unknown threats, an ideal defense method should be effective, generalizable, and robust against adaptive attacks. In this work, we introduce DISPATCH, the first diffusion-based defense framework for object detection. Unlike previous works that aim to "detect and remove" adversarial patches, DISPATCH adopts a "regenerate and rectify" strategy, leveraging generative models to disarm attack effects while preserving the integrity of the input image. Specifically, we utilize the in-distribution generative power of diffusion models to regenerate the entire image, aligning it with benign data. A rectification process is then employed to identify and replace adversarial regions with their regenerated benign counterparts. DISPATCH is attack-agnostic and requires no prior knowledge of the existing patches. Extensive experiments across multiple detectors and attacks demonstrate that DISPATCH consistently outperforms state-of-the-art defenses on both hiding attacks and creating attacks, achieving the best overall mAP.5 score of 89.3% on hiding attacks, and lowering the attack success rate to 24.8% on untargeted creating attacks. Moreover, it maintains strong robustness against adaptive attacks, making it a practical and reliable defense for object detection systems.</li>
<li><strong>摘要：</strong>对象检测是各种现实世界应用的基础，例如安全监视和监视视频分析。尽管有进步，但最先进的对象探测器仍然容易受到对抗贴片攻击的影响，这些探测器可以轻松地应用于现实世界中的对象，以掩盖实际项目或创建不存在的物品，从而造成严重的后果。鉴于当前的对抗斑块攻击和潜在的未知威胁的多样性，理想的防御方法应有效，可推广和强大，以抵抗适应性攻击。在这项工作中，我们引入了Dispatch，这是第一个基于扩散的对象检测的防御框架。与旨在“检测和删除”对抗贴片的以前的作品不同，调度采用了“再生和纠正”策略，利用生成模型来解除攻击效果，同时保留输入图像的完整性。具体而言，我们利用扩散模型的分布生成力来再生整个图像，并将其与良性数据保持一致。然后，采用纠正过程来识别和替换对抗区域的再生良性对应物。调度是攻击性不足的，不需要事先了解现有补丁。跨多个检测器和攻击的广泛实验表明，派遣在隐藏攻击和造成攻击方面始终优于最先进的防御能力，达到了最佳的整体地图。5在隐藏攻击方面的得分为89.3％，并将攻击成功率降低到24.8％，以降低到24.8％，以创造攻击攻击。此外，它保持了针对自适应攻击的强大鲁棒性，使其成为对象检测系统的实用和可靠的防御。</li>
</ul>

<h3>Title: Split Conformal Prediction in the Function Space with Neural Operators</h3>
<ul>
<li><strong>Authors: </strong>David Millard, Lars Lindemann, Ali Baheri</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04623">https://arxiv.org/abs/2509.04623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04623">https://arxiv.org/pdf/2509.04623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04623]] Split Conformal Prediction in the Function Space with Neural Operators(https://arxiv.org/abs/2509.04623)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Uncertainty quantification for neural operators remains an open problem in the infinite-dimensional setting due to the lack of finite-sample coverage guarantees over functional outputs. While conformal prediction offers finite-sample guarantees in finite-dimensional spaces, it does not directly extend to function-valued outputs. Existing approaches (Gaussian processes, Bayesian neural networks, and quantile-based operators) require strong distributional assumptions or yield conservative coverage. This work extends split conformal prediction to function spaces following a two step method. We first establish finite-sample coverage guarantees in a finite-dimensional space using a discretization map in the output function space. Then these guarantees are lifted to the function-space by considering the asymptotic convergence as the discretization is refined. To characterize the effect of resolution, we decompose the conformal radius into discretization, calibration, and misspecification components. This decomposition motivates a regression-based correction to transfer calibration across resolutions. Additionally, we propose two diagnostic metrics (conformal ensemble score and internal agreement) to quantify forecast degradation in autoregressive settings. Empirical results show that our method maintains calibrated coverage with less variation under resolution shifts and achieves better coverage in super-resolution tasks.</li>
<li><strong>摘要：</strong>由于缺乏有限样本的覆盖范围，在功能输出中保证了无限样本的覆盖范围，因此在无限维度环境中，神经操作员的不确定性定量仍然是一个空旷的问题。尽管共形预测在有限维空间中提供有限样本保证，但它并未直接扩展到功能值值输出。现有方法（高斯过程，贝叶斯神经网络和基于分位数的操作员）需要强大的分配假设或产生保守的覆盖范围。这项工作将拆分的共形预测扩展到遵循两步方法的功能空间。我们首先使用输出函数空间中的离散化映射在有限维空间中建立有限样本覆盖范围。然后通过考虑离散化的渐近收敛来提高这些保证，从而将这些保证提升为功能空间。为了表征分辨率的效果，我们将共形半径分解为离散，校准和指定组件。这种分解激发了基于回归的校正以转移跨分辨率的校准。此外，我们提出了两个诊断指标（保形集合得分和内部协议），以量化自回旋环境中的预测退化。经验结果表明，我们的方法保持校准的覆盖范围，在分辨率偏移下变化较小，并且在超分辨率任务中获得更好的覆盖率。</li>
</ul>

<h3>Title: UAV-Based Intelligent Traffic Surveillance System: Real-Time Vehicle Detection, Classification, Tracking, and Behavioral Analysis</h3>
<ul>
<li><strong>Authors: </strong>Ali Khanpour, Tianyi Wang, Afra Vahidi-Shams, Wim Ectors, Farzam Nakhaie, Amirhossein Taheri, Christian Claudel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.ET, cs.RO, eess.IV, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04624">https://arxiv.org/abs/2509.04624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04624">https://arxiv.org/pdf/2509.04624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04624]] UAV-Based Intelligent Traffic Surveillance System: Real-Time Vehicle Detection, Classification, Tracking, and Behavioral Analysis(https://arxiv.org/abs/2509.04624)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Traffic congestion and violations pose significant challenges for urban mobility and road safety. Traditional traffic monitoring systems, such as fixed cameras and sensor-based methods, are often constrained by limited coverage, low adaptability, and poor scalability. To address these challenges, this paper introduces an advanced unmanned aerial vehicle (UAV)-based traffic surveillance system capable of accurate vehicle detection, classification, tracking, and behavioral analysis in real-world, unconstrained urban environments. The system leverages multi-scale and multi-angle template matching, Kalman filtering, and homography-based calibration to process aerial video data collected from altitudes of approximately 200 meters. A case study in urban area demonstrates robust performance, achieving a detection precision of 91.8%, an F1-score of 90.5%, and tracking metrics (MOTA/MOTP) of 92.1% and 93.7%, respectively. Beyond precise detection, the system classifies five vehicle types and automatically detects critical traffic violations, including unsafe lane changes, illegal double parking, and crosswalk obstructions, through the fusion of geofencing, motion filtering, and trajectory deviation analysis. The integrated analytics module supports origin-destination tracking, vehicle count visualization, inter-class correlation analysis, and heatmap-based congestion modeling. Additionally, the system enables entry-exit trajectory profiling, vehicle density estimation across road segments, and movement direction logging, supporting comprehensive multi-scale urban mobility analytics. Experimental results confirms the system's scalability, accuracy, and practical relevance, highlighting its potential as an enforcement-aware, infrastructure-independent traffic monitoring solution for next-generation smart cities.</li>
<li><strong>摘要：</strong>交通拥堵和违规对城市流动性和道路安全构成了重大挑战。传统的交通监控系统，例如固定摄像机和基于传感器的方法，通常受到有限的覆盖范围，低适应性和可扩展性差的限制。为了应对这些挑战，本文介绍了一个基于高级的无人机（UAV）的交通监视系统，能够在现实世界中无限制的Urban环境中进行准确的车辆检测，分类，跟踪和行为分析。该系统利用多尺度和多角模板匹配，Kalman过滤和基于同型的校准来处理从约200米高度收集的空中视频数据。在市区的一个案例研究表明，表现出色的表现，其检测精度为91.8％，F1得分为90.5％，跟踪指标（MOTA/MOTP）分别为92.1％和93.7％。除了精确检测外，该系统还对五种车辆进行了分类，并自动检测到严重的交通违规行为，包括不安全的车道变化，非法的双重停车场和人行横道障碍物，通过融合地理载体，运动过滤和轨迹偏差分析。综合分析模块支持原点预测跟踪，车辆数量可视化，类间相关分析和基于热图的拥塞建模。此外，该系统可以实现入门轨迹轨迹分析，跨路段的车辆密度估算以及运动方向记录，从而支持全面的多尺度城市流动性分析。实验结果证实了该系统的可扩展性，准确性和实际相关性，强调了其作为执法意识的潜力，独立于基础架构的交通监控解决方案对于下一代智能城市。</li>
</ul>

<h3>Title: Hybrid-Tower: Fine-grained Pseudo-query Interaction and Generation for Text-to-Video Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Bangxiang Lan, Ruobing Xie, Ruixiang Zhao, Xingwu Sun, Zhanhui Kang, Gang Yang, Xirong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04773">https://arxiv.org/abs/2509.04773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04773">https://arxiv.org/pdf/2509.04773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04773]] Hybrid-Tower: Fine-grained Pseudo-query Interaction and Generation for Text-to-Video Retrieval(https://arxiv.org/abs/2509.04773)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The Text-to-Video Retrieval (T2VR) task aims to retrieve unlabeled videos by textual queries with the same semantic meanings. Recent CLIP-based approaches have explored two frameworks: Two-Tower versus Single-Tower framework, yet the former suffers from low effectiveness, while the latter suffers from low efficiency. In this study, we explore a new Hybrid-Tower framework that can hybridize the advantages of the Two-Tower and Single-Tower framework, achieving high effectiveness and efficiency simultaneously. We propose a novel hybrid method, Fine-grained Pseudo-query Interaction and Generation for T2VR, ie, PIG, which includes a new pseudo-query generator designed to generate a pseudo-query for each video. This enables the video feature and the textual features of pseudo-query to interact in a fine-grained manner, similar to the Single-Tower approaches to hold high effectiveness, even before the real textual query is received. Simultaneously, our method introduces no additional storage or computational overhead compared to the Two-Tower framework during the inference stage, thus maintaining high efficiency. Extensive experiments on five commonly used text-video retrieval benchmarks demonstrate that our method achieves a significant improvement over the baseline, with an increase of $1.6\% \sim 3.9\%$ in R@1. Furthermore, our method matches the efficiency of Two-Tower models while achieving near state-of-the-art performance, highlighting the advantages of the Hybrid-Tower framework.</li>
<li><strong>摘要：</strong>文本到视频检索（T2VR）任务旨在通过具有相同语义含义的文本查询来检索未标记的视频。最近的基于夹子的方法探索了两个框架：两脚架与单身框架，但前者的效率低，而后者的效率低。在这项研究中，我们探索了一个新的混合框架，该框架可以杂交两个较高和单塔框架的优势，同时达到高效和效率。我们提出了一种新型的混合方法，T2VR，即猪的细粒度伪Query相互作用和生成，其中包括一个新的伪Query Generator，旨在为每个视频生成伪Query。这使视频功能和伪Query的文本功能也能够以细粒度的方式进行交互，类似于在收到真实的文本查询之前，类似于具有高效的单塔方法。同时，与推理阶段的两个较高框架相比，我们的方法没有引入其他额外的存储或计算开销，从而保持了高效率。对五个常用的文本视频检索基准进行了广泛的实验表明，我们的方法比基线取得了重大改进，$ 1.6 \％\％\ sim 3.9 \％$ in r@in@1中。此外，我们的方法与两个塔模型的效率相匹配，同时达到了几乎最先进的性能，突出了混合式框架的优势。</li>
</ul>

<h3>Title: Exploring Non-Local Spatial-Angular Correlations with a Hybrid Mamba-Transformer Framework for Light Field Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Haosong Liu, Xiancheng Zhu, Huanqiang Zeng, Jianqing Zhu, Jiuwen Cao, Junhui Hou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04824">https://arxiv.org/abs/2509.04824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04824">https://arxiv.org/pdf/2509.04824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04824]] Exploring Non-Local Spatial-Angular Correlations with a Hybrid Mamba-Transformer Framework for Light Field Super-Resolution(https://arxiv.org/abs/2509.04824)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Recently, Mamba-based methods, with its advantage in long-range information modeling and linear complexity, have shown great potential in optimizing both computational cost and performance of light field image super-resolution (LFSR). However, current multi-directional scanning strategies lead to inefficient and redundant feature extraction when applied to complex LF data. To overcome this challenge, we propose a Subspace Simple Scanning (Sub-SS) strategy, based on which we design the Subspace Simple Mamba Block (SSMB) to achieve more efficient and precise feature extraction. Furthermore, we propose a dual-stage modeling strategy to address the limitation of state space in preserving spatial-angular and disparity information, thereby enabling a more comprehensive exploration of non-local spatial-angular correlations. Specifically, in stage I, we introduce the Spatial-Angular Residual Subspace Mamba Block (SA-RSMB) for shallow spatial-angular feature extraction; in stage II, we use a dual-branch parallel structure combining the Epipolar Plane Mamba Block (EPMB) and Epipolar Plane Transformer Block (EPTB) for deep epipolar feature refinement. Building upon meticulously designed modules and strategies, we introduce a hybrid Mamba-Transformer framework, termed LFMT. LFMT integrates the strengths of Mamba and Transformer models for LFSR, enabling comprehensive information exploration across spatial, angular, and epipolar-plane domains. Experimental results demonstrate that LFMT significantly outperforms current state-of-the-art methods in LFSR, achieving substantial improvements in performance while maintaining low computational complexity on both real-word and synthetic LF datasets.</li>
<li><strong>摘要：</strong>最近，基于MAMBA的方法在远程信息建模和线性复杂性方面具有优势，在优化光场图像超分辨率（LFSR）的计算成本和性能方面具有巨大的潜力。但是，当当前的多向扫描策略将应用于复杂的LF数据时会导致效率低下和冗余的提取。为了克服这一挑战，我们提出了一个子空间简单扫描（子-SS）策略，基于该策略，我们设计了Subspace Simple Mamba Block（SSMB），以实现更有效，更精确的特征提取。此外，我们提出了一种双阶段建模策略，以解决保留空间角和差异信息中状态空间的局限性，从而更全面地探索非本地空间 - 角度相关性。具体而言，在第I期中，我们引入了空间 - 角度残留子空间山区块（SA-RSMB），以用于浅空间角特征提取。在第二阶段中，我们使用了结合了异性平面Mamba块（EPMB）和Epilolar Plane Transformer Block（EPTB）的双支流平行结构，以进行深层的外观特征细化。在精心设计的模块和策略的基础上，我们引入了一个称为LFMT的混合Mamba-Transformer框架。 LFMT整合了LFSR的MAMBA和变压器模型的优势，从而在空间，角和面向平面域进行了全面的信息探索。实验结果表明，LFMT显着胜过LFSR中当前的最新方法，从而实现了性能的实质性改善，同时保持了现实和合成LF数据集的计算复杂性较低。</li>
</ul>

<h3>Title: PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination</h3>
<ul>
<li><strong>Authors: </strong>Ming Dai, Wenxuan Cheng, Jiedong Zhuang, Jiang-jiang Liu, Hongshen Zhao, Zhenhua Feng, Wankou Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04833">https://arxiv.org/abs/2509.04833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04833">https://arxiv.org/pdf/2509.04833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04833]] PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination(https://arxiv.org/abs/2509.04833)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in visual grounding have largely shifted away from traditional proposal-based two-stage frameworks due to their inefficiency and high computational complexity, favoring end-to-end direct reference paradigms. However, these methods rely exclusively on the referred target for supervision, overlooking the potential benefits of prominent prospective targets. Moreover, existing approaches often fail to incorporate multi-granularity discrimination, which is crucial for robust object identification in complex scenarios. To address these limitations, we propose PropVG, an end-to-end proposal-based framework that, to the best of our knowledge, is the first to seamlessly integrate foreground object proposal generation with referential object comprehension without requiring additional detectors. Furthermore, we introduce a Contrastive-based Refer Scoring (CRS) module, which employs contrastive learning at both sentence and word levels to enhance the capability in understanding and distinguishing referred objects. Additionally, we design a Multi-granularity Target Discrimination (MTD) module that fuses object- and semantic-level information to improve the recognition of absent targets. Extensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO (REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes and models are available at this https URL.</li>
<li><strong>摘要：</strong>视觉接地的最新进展很大程度上已经从传统的基于提案的两阶段框架转变，因为它们的效率低下和高度计算复杂性，有利于端到端直接参考范式。但是，这些方法仅依赖于引用的监督目标，从而忽略了突出的前瞻性目标的潜在好处。此外，现有方法通常无法纳入多粒度歧视，这对于在复杂场景中的强大对象识别至关重要。为了解决这些限制，我们提出了ProPVG，这是一个基于端到的建议的框架，据我们所知，它是第一个无需参考对象理解的前景对象提案生成而无需其他检测器的框架。此外，我们引入了一个基于对比的参考评分（CRS）模块，该模块在句子和单词级别上都采用对比度学习，以增强理解和区分引用对象的能力。此外，我们设计了一个多粒性目标歧视（MTD）模块，该模块融合了对象和语义级别的信息，以提高缺乏目标的识别。关于Grefcoco（Grec/GRES），Ref-ZOM，R-REFCOCO和REFCOCO（REC/RES）基准的广泛实验证明了Propvg的有效性。该代码和模型可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: TemporalFlowViz: Parameter-Aware Visual Analytics for Interpreting Scramjet Combustion Evolution</h3>
<ul>
<li><strong>Authors: </strong>Yifei Jia, Shiyu Cheng, Yu Dong, Guan Li, Dong Tian, Ruixiao Peng, Xuyi Lu, Yu Wang, Wei Yao, Guihua Shan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04834">https://arxiv.org/abs/2509.04834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04834">https://arxiv.org/pdf/2509.04834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04834]] TemporalFlowViz: Parameter-Aware Visual Analytics for Interpreting Scramjet Combustion Evolution(https://arxiv.org/abs/2509.04834)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Understanding the complex combustion dynamics within scramjet engines is critical for advancing high-speed propulsion technologies. However, the large scale and high dimensionality of simulation-generated temporal flow field data present significant challenges for visual interpretation, feature differentiation, and cross-case comparison. In this paper, we present TemporalFlowViz, a parameter-aware visual analytics workflow and system designed to support expert-driven clustering, visualization, and interpretation of temporal flow fields from scramjet combustion simulations. Our approach leverages hundreds of simulated combustion cases with varying initial conditions, each producing time-sequenced flow field images. We use pretrained Vision Transformers to extract high-dimensional embeddings from these frames, apply dimensionality reduction and density-based clustering to uncover latent combustion modes, and construct temporal trajectories in the embedding space to track the evolution of each simulation over time. To bridge the gap between latent representations and expert reasoning, domain specialists annotate representative cluster centroids with descriptive labels. These annotations are used as contextual prompts for a vision-language model, which generates natural-language summaries for individual frames and full simulation cases. The system also supports parameter-based filtering, similarity-based case retrieval, and coordinated multi-view exploration to facilitate in-depth analysis. We demonstrate the effectiveness of TemporalFlowViz through two expert-informed case studies and expert feedback, showing TemporalFlowViz enhances hypothesis generation, supports interpretable pattern discovery, and enhances knowledge discovery in large-scale scramjet combustion analysis.</li>
<li><strong>摘要：</strong>了解Scramjet发动机中的复杂燃烧动力学对于推进高速推进技术至关重要。但是，模拟生成的时间流场数据的大规模和高维度对视觉解释，特征分化和跨案例比较提出了重大挑战。在本文中，我们介绍了temalflowviz，这是一种参数感知的视觉分析工作流程和系统，旨在支持专家驱动的聚类，可视化和对ScramJET燃烧模拟的时间流场的解释。我们的方法利用数百个模拟燃烧案例，具有不同的初始条件，每种情况都会产生时间序列的流场图像。我们使用预审前的视觉变压器从这些框架中提取高维嵌入，应用尺寸降低和基于密度的聚类来揭示潜在的燃烧模式，并在嵌入空间中构建时间轨迹，以跟踪每个模拟的演变。为了弥合潜在表示和专家推理之间的差距，域专家注释了具有描述性标签的代表性群体。这些注释被用作视觉模型的上下文提示，该提示为单个帧和完整仿真案例生成了自然语言摘要。该系统还支持基于参数的过滤，基于相似性的病例检索和协调的多视图探索，以促进深入分析。我们通过两种专家知识的案例研究和专家反馈来证明暂时性的有效性，显示了暂时的flougalverviz增强了假设的产生，支持可解释的模式发现并增强了大规模的SCRAMJET燃烧分析中的知识发现。</li>
</ul>

<h3>Title: SynGen-Vision: Synthetic Data Generation for training industrial vision models</h3>
<ul>
<li><strong>Authors: </strong>Alpana Dubey, Suma Mani Kuriakose, Nitish Bhardwaj</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04894">https://arxiv.org/abs/2509.04894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04894">https://arxiv.org/pdf/2509.04894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04894]] SynGen-Vision: Synthetic Data Generation for training industrial vision models(https://arxiv.org/abs/2509.04894)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose an approach to generate synthetic data to train computer vision (CV) models for industrial wear and tear detection. Wear and tear detection is an important CV problem for predictive maintenance tasks in any industry. However, data curation for training such models is expensive and time-consuming due to the unavailability of datasets for different wear and tear scenarios. Our approach employs a vision language model along with a 3D simulation and rendering engine to generate synthetic data for varying rust conditions. We evaluate our approach by training a CV model for rust detection using the generated dataset and tested the trained model on real images of rusted industrial objects. The model trained with the synthetic data generated by our approach, outperforms the other approaches with a mAP50 score of 0.87. The approach is customizable and can be easily extended to other industrial wear and tear detection scenarios</li>
<li><strong>摘要：</strong>我们提出了一种生成合成数据以训练计算机视觉（CV）模型以进行工业磨损检测的方法。磨损检测是任何行业预测维护任务的重要CV问题。但是，由于数据集无法用于不同的磨损场景，因此用于培训此类模型的数据策划是昂贵且耗时的。我们的方法采用视觉语言模型以及3D模拟和渲染引擎来生成用于不同生锈条件的合成数据。我们通过使用生成的数据集训练CV模型来评估我们的方法，并在生锈的工业对象的真实图像上测试了经过训练的模型。使用我们方法生成的合成数据训练的模型，其MAP50分数为0.87。该方法是可定制的，可以轻松扩展到其他工业磨损检测方案</li>
</ul>

<h3>Title: Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper</h3>
<ul>
<li><strong>Authors: </strong>Gehui Chen, Guan'an Wang, Xiaowen Huang, Jitao Sang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04957">https://arxiv.org/abs/2509.04957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04957">https://arxiv.org/pdf/2509.04957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04957]] Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper(https://arxiv.org/abs/2509.04957)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent Video-to-Audio (V2A) generation relies on extracting semantic and temporal features from video to condition generative models. Training these models from scratch is resource intensive. Consequently, leveraging foundation models (FMs) has gained traction due to their cross-modal knowledge transfer and generalization capabilities. One prior work has explored fine-tuning a lightweight mapper network to connect a pre-trained visual encoder with a text-to-audio generation model for V2A. Inspired by this, we introduce the Multiple Foundation Model Mapper (MFM-Mapper). Compared to the previous mapper approach, MFM-Mapper benefits from richer semantic and temporal information by fusing features from dual visual encoders. Furthermore, by replacing a linear mapper with GPT-2, MFM-Mapper improves feature alignment, drawing parallels between cross-modal features mapping and autoregressive translation tasks. Our MFM-Mapper exhibits remarkable training efficiency. It achieves better performance in semantic and temporal consistency with fewer training consuming, requiring only 16\% of the training scale compared to previous mapper-based work, yet achieves competitive performance with models trained on a much larger scale.</li>
<li><strong>摘要：</strong>最近的视频对审计（V2A）的一代依赖于从视频中提取语义和时间特征到病生成模型。从头开始培训这些模型是资源密集的。因此，利用基础模型（FMS）由于其跨模式知识传递和概括能力而获得了吸引力。一项先前的工作探索了轻巧的映射网络，以将预先训练的视觉编码器与V2A的文本对审计生成模型联系起来。受此启发，我们介绍了多个基础模型映射器（MFM-Mapper）。与以前的映射器方法相比，MFM-mapper通过融合了双重视觉编码器的功能，从更丰富的语义和时间信息中受益。此外，通过用GPT-2替换线性映射器，MFM-Mapper改善了功能对齐方式，在跨模式特征映射和自动回调的翻译任务之间划分相似之处。我们的MFM-Mapper具有出色的培训效率。与以前的基于映射的工作相比，它在语义和时间一致性方面的性能更好，而培训量的消耗较少，而训练量表的范围仅为16％，但是通过在更大范围内训练的模型，可以实现竞争性能。</li>
</ul>

<h3>Title: Systematic Review and Meta-analysis of AI-driven MRI Motion Artifact Detection and Correction</h3>
<ul>
<li><strong>Authors: </strong>Mojtaba Safari, Zach Eidex, Richard L.J. Qiu, Matthew Goette, Tonghe Wang, Xiaofeng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.med-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05071">https://arxiv.org/abs/2509.05071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05071">https://arxiv.org/pdf/2509.05071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05071]] Systematic Review and Meta-analysis of AI-driven MRI Motion Artifact Detection and Correction(https://arxiv.org/abs/2509.05071)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Background: To systematically review and perform a meta-analysis of artificial intelligence (AI)-driven methods for detecting and correcting magnetic resonance imaging (MRI) motion artifacts, assessing current developments, effectiveness, challenges, and future research directions. Methods: A comprehensive systematic review and meta-analysis were conducted, focusing on deep learning (DL) approaches, particularly generative models, for the detection and correction of MRI motion artifacts. Quantitative data were extracted regarding utilized datasets, DL architectures, and performance metrics. Results: DL, particularly generative models, show promise for reducing motion artifacts and improving image quality; however, limited generalizability, reliance on paired training data, and risk of visual distortions remain key challenges that motivate standardized datasets and reporting. Conclusions: AI-driven methods, particularly DL generative models, show significant potential for improving MRI image quality by effectively addressing motion artifacts. However, critical challenges must be addressed, including the need for comprehensive public datasets, standardized reporting protocols for artifact levels, and more advanced, adaptable DL techniques to reduce reliance on extensive paired datasets. Addressing these aspects could substantially enhance MRI diagnostic accuracy, reduce healthcare costs, and improve patient care outcomes.</li>
<li><strong>摘要：</strong>背景：系统地审查和进行人工智能（AI）驱动的方法的荟萃分析，用于检测和纠正磁共振成像（MRI）运动伪影，评估当前的发展，有效性，挑战和未来的研究方向。方法：进行了全面的系统综述和荟萃分析，重点是深度学习方法（DL）方法，尤其是生成模型，用于检测和校正MRI运动伪像。提取了有关使用的数据集，DL架构和性能指标的定量数据。结果：DL，尤其是生成模型，显示出减少运动伪像和提高图像质量的希望；但是，有限的普遍性，对配对训练数据的依赖以及视觉扭曲的风险仍然是激励标准化数据集和报告的关键挑战。结论：AI驱动的方法，尤其是DL生成模型，通过有效解决运动伪像，可以提高MRI图像质量的重要潜力。但是，必须应对关键挑战，包括需要全面的公共数据集，人工制品级别的标准化报告协议以及更高级的，更高级，适应性的DL技术，以减少对广泛的配对数据集的依赖。解决这些方面可以大大提高MRI诊断准确性，降低医疗保健成本并改善患者护理结果。</li>
</ul>

<h3>Title: A Scalable Attention-Based Approach for Image-to-3D Texture Mapping</h3>
<ul>
<li><strong>Authors: </strong>Arianna Rampini, Kanika Madan, Bruno Roy, AmirHossein Zamani, Derek Cheung</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05131">https://arxiv.org/abs/2509.05131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05131">https://arxiv.org/pdf/2509.05131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05131]] A Scalable Attention-Based Approach for Image-to-3D Texture Mapping(https://arxiv.org/abs/2509.05131)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>High-quality textures are critical for realistic 3D content creation, yet existing generative methods are slow, rely on UV maps, and often fail to remain faithful to a reference image. To address these challenges, we propose a transformer-based framework that predicts a 3D texture field directly from a single image and a mesh, eliminating the need for UV mapping and differentiable rendering, and enabling faster texture generation. Our method integrates a triplane representation with depth-based backprojection losses, enabling efficient training and faster inference. Once trained, it generates high-fidelity textures in a single forward pass, requiring only 0.2s per shape. Extensive qualitative, quantitative, and user preference evaluations demonstrate that our method outperforms state-of-the-art baselines on single-image texture reconstruction in terms of both fidelity to the input image and perceptual quality, highlighting its practicality for scalable, high-quality, and controllable 3D content creation.</li>
<li><strong>摘要：</strong>高质量的纹理对于现实的3D内容创建至关重要，但是现有的生成方法很慢，依赖于紫外线图，并且通常无法忠于参考图像。为了应对这些挑战，我们提出了一个基于变压器的框架，该框架直接从单个图像和网格中直接预测3D纹理字段，从而消除了对紫外线映射和可区分渲染的需求，并启用更快的纹理生成。我们的方法将三平方表示与基于深度的反向投影损失相结合，从而实现有效的训练和更快的推断。一旦受过训练，它就会在单个正向通行证中产生高保真纹理，每个形状只需要0.2秒。广泛的定性，定量和用户偏好评估表明，我们的方法优于单像纹理重建的最先进的基线，从忠诚度到输入图像和感知质量方面，强调了其对可扩展，高品质和可控制的3D内容创造的实用性。</li>
</ul>

<h3>Title: Symbolic Graphics Programming with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yamei Chen, Haoquan Zhang, Yangyi Huang, Zeju Qiu, Kaipeng Zhang, Yandong Wen, Weiyang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05208">https://arxiv.org/abs/2509.05208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05208">https://arxiv.org/pdf/2509.05208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05208]] Symbolic Graphics Programming with Large Language Models(https://arxiv.org/abs/2509.05208)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel at program synthesis, yet their ability to produce symbolic graphics programs (SGPs) that render into precise visual content remains underexplored. We study symbolic graphics programming, where the goal is to generate an SGP from a natural-language description. This task also serves as a lens into how LLMs understand the visual world by prompting them to generate images rendered from SGPs. Among various SGPs, our paper sticks to scalable vector graphics (SVGs). We begin by examining the extent to which LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a comprehensive benchmark covering object fidelity, scene fidelity, and compositionality (attribute binding, spatial relations, numeracy). On SGP-GenBench, we discover that frontier proprietary models substantially outperform open-source models, and performance correlates well with general coding capabilities. Motivated by this gap, we aim to improve LLMs' ability to generate SGPs. We propose a reinforcement learning (RL) with verifiable rewards approach, where a format-validity gate ensures renderable SVG, and a cross-modal reward aligns text and the rendered image via strong vision encoders (e.g., SigLIP for text-image and DINO for image-image). Applied to Qwen-2.5-7B, our method substantially improves SVG generation quality and semantics, achieving performance on par with frontier systems. We further analyze training dynamics, showing that RL induces (i) finer decomposition of objects into controllable primitives and (ii) contextual details that improve scene coherence. Our results demonstrate that symbolic graphics programming offers a precise and interpretable lens on cross-modal grounding.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）在程序综合方面表现出色，但它们生产符号图形程序（SGP）的能力仍未得到充满反感。我们研究符号图形编程，其目标是从自然语言描述中生成SGP。此任务还可以作为LLM通过提示从SGP呈现的图像来理解视觉世界的视角。在各种SGP中，我们的纸张坚持可扩展的向量图形（SVG）。我们首先检查LLM可以生成SGP的程度。为此，我们介绍了SGP-GENBENCH，这是一个全面的基准，涵盖了对象保真度，场景保真度和组成性（属性绑定，空间关系，算术）。在SGP-Genbench上，我们发现边境专有模型基本上要优于开源模型，并且性能与一般编码功能良好相关。在这一差距的推动下，我们旨在提高LLMS生成SGP的能力。我们建议使用可验证的奖励方法进行加固学习（RL），其中格式 - 录音门可确保可渲染的SVG，并且通过强视觉编码器（例如，用于图像形象的文本图像和dino）的跨模式奖励使文本和渲染图像相结合。应用于QWEN-2.5-7B，我们的方法基本上提高了SVG的发电质量和语义，从而在边境系统中实现了性能。我们进一步分析了训练动力学，表明RL诱导（i）将对象分解为可控的原始素和（ii）上下文细节，以提高场景相干性。我们的结果表明，符号图形编程在跨模式接地方面提供了精确且可解释的镜头。</li>
</ul>

<h3>Title: COGITAO: A Visual Reasoning Framework To Study Compositionality & Generalization</h3>
<ul>
<li><strong>Authors: </strong>Yassine Taoudi-Benchekroun, Klim Troyan, Pascal Sager, Stefan Gerber, Lukas Tuggener, Benjamin Grewe</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05249">https://arxiv.org/abs/2509.05249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05249">https://arxiv.org/pdf/2509.05249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05249]] COGITAO: A Visual Reasoning Framework To Study Compositionality & Generalization(https://arxiv.org/abs/2509.05249)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The ability to compose learned concepts and apply them in novel settings is key to human intelligence, but remains a persistent limitation in state-of-the-art machine learning models. To address this issue, we introduce COGITAO, a modular and extensible data generation framework and benchmark designed to systematically study compositionality and generalization in visual domains. Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs rule-based tasks which apply a set of transformations to objects in grid-like environments. It supports composition, at adjustable depth, over a set of 28 interoperable transformations, along with extensive control over grid parametrization and object properties. This flexibility enables the creation of millions of unique task rules -- surpassing concurrent datasets by several orders of magnitude -- across a wide range of difficulties, while allowing virtually unlimited sample generation per rule. We provide baseline experiments using state-of-the-art vision models, highlighting their consistent failures to generalize to novel combinations of familiar elements, despite strong in-domain performance. COGITAO is fully open-sourced, including all code and datasets, to support continued research in this field.</li>
<li><strong>摘要：</strong>构成学习概念并将其应用于新颖设置的能力是人类智能的关键，但在最先进的机器学习模型中仍然是持续的限制。为了解决这个问题，我们介绍了Cogitao，这是一个模块化且可扩展的数据生成框架和基准测试，旨在系统地研究视觉域中的组成性和泛化。从Arc-Agi解决问题的问题中汲取灵感，Cogitao构造了基于规则的任务，这些任务将一组转换应用于网格般环境中的对象。它支持一组28个可互操作转换的组成，以及对网格参数化和对象属性的广泛控制。这种灵活性使创建数百万个独特的任务规则 - 超过了几个数量级的并发数据集 - 在广泛的困难中，同时允许每个规则几乎无限地生成样本。我们使用最先进的视觉模型提供基线实验，突出了它们的一致性失败，以推广到熟悉元素的新型组合，尽管表现出色。 Cogitao是完全开源的，包括所有代码和数据集，以支持该领域的持续研究。</li>
</ul>

<h3>Title: A Kolmogorov-Arnold Network for Interpretable Cyberattack Detection in AGC Systems</h3>
<ul>
<li><strong>Authors: </strong>Jehad Jilan, Niranjana Naveen Nambiar, Ahmad Mohammad Saber, Alok Paranjape, Amr Youssef, Deepa Kundur</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05259">https://arxiv.org/abs/2509.05259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05259">https://arxiv.org/pdf/2509.05259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05259]] A Kolmogorov-Arnold Network for Interpretable Cyberattack Detection in AGC Systems(https://arxiv.org/abs/2509.05259)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Automatic Generation Control (AGC) is essential for power grid stability but remains vulnerable to stealthy cyberattacks, such as False Data Injection Attacks (FDIAs), which can disturb the system's stability while evading traditional detection methods. Unlike previous works that relied on blackbox approaches, this work proposes Kolmogorov-Arnold Networks (KAN) as an interpretable and accurate method for FDIA detection in AGC systems, considering the system nonlinearities. KAN models include a method for extracting symbolic equations, and are thus able to provide more interpretability than the majority of machine learning models. The proposed KAN is trained offline to learn the complex nonlinear relationships between the AGC measurements under different operating scenarios. After training, symbolic formulas that describe the trained model's behavior can be extracted and leveraged, greatly enhancing interpretability. Our findings confirm that the proposed KAN model achieves FDIA detection rates of up to 95.97% and 95.9% for the initial model and the symbolic formula, respectively, with a low false alarm rate, offering a reliable approach to enhancing AGC cybersecurity.</li>
<li><strong>摘要：</strong>自动发电控制（AGC）对于电网稳定性至关重要，但仍然容易受到隐秘网络攻击的影响，例如虚假数据注射攻击（FDIA），这可能会干扰系统的稳定性，同时避免传统的检测方法。与以前依赖黑框方法的作品不同，这项工作提出了Kolmogorov-Arnold网络（KAN）作为AGC系统中FDIA检测的一种可解释和准确的方法，考虑到系统非线性。 KAN模型包括一种提取符号方程的方法，因此能够比大多数机器学习模型提供更多的解释性。拟议的KAN经过了训练的离线训练，以了解不同操作场景下AGC测量之间的复杂非线性关系。训练后，可以提取和利用描述训练有素的模型行为的符号公式，从而大大提高可解释性。我们的发现证实，提出的KAN模型的初始模型和符号公式的FDIA检测率分别为95.97％和95.9％，较低的错误警报率，为提高AGC网络安全性提供了可靠的方法。</li>
</ul>

<h3>Title: SpikingBrain Technical Report: Spiking Brain-inspired Large Models</h3>
<ul>
<li><strong>Authors: </strong>Yuqi Pan, Yupeng Feng, Jinghao Zhuang, Siyu Ding, Zehao Liu, Bohan Sun, Yuhong Chou, Han Xu, Xuerui Qiu, Anlin Deng, Anjie Hu, Peng Zhou, Man Yao, Jibin Wu, Jian Yang, Guoliang Sun, Bo Xu, Guoqi Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05276">https://arxiv.org/abs/2509.05276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05276">https://arxiv.org/pdf/2509.05276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05276]] SpikingBrain Technical Report: Spiking Brain-inspired Large Models(https://arxiv.org/abs/2509.05276)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Mainstream Transformer-based large language models face major efficiency bottlenecks: training computation scales quadratically with sequence length, and inference memory grows linearly, limiting long-context processing. Building large models on non-NVIDIA platforms also poses challenges for stable and efficient training. To address this, we introduce SpikingBrain, a family of brain-inspired models designed for efficient long-context training and inference. SpikingBrain leverages the MetaX GPU cluster and focuses on three aspects: (1) Model Architecture: linear and hybrid-linear attention architectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an efficient, conversion-based training pipeline and a dedicated spike coding framework; (3) System Engineering: customized training frameworks, operator libraries, and parallelism strategies tailored to MetaX hardware. Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM, and SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the feasibility of large-scale LLM development on non-NVIDIA platforms. SpikingBrain achieves performance comparable to open-source Transformer baselines while using only about 150B tokens for continual pre-training. Our models significantly improve long-sequence training efficiency and deliver inference with (partially) constant memory and event-driven spiking behavior. For example, SpikingBrain-7B attains over 100x speedup in Time to First Token for 4M-token sequences. Training remains stable for weeks on hundreds of MetaX C550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4 percent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling low-power operation. Overall, this work demonstrates the potential of brain-inspired mechanisms to drive the next generation of efficient and scalable large model design.</li>
<li><strong>摘要：</strong>基于主流变压器的大型语言模型面临着主要效率瓶颈：训练计算的量表尺度是序列长度的二次尺度，并且推理记忆线性增长，从而限制了长篇文化处理。在非NVIDIA平台上建立大型模型也为稳定有效的培训带来了挑战。为了解决这个问题，我们介绍了Spikingbrain，这是一个由大脑启发的模型家族，旨在有效的长期培训和推理。 SPIKINGBRAIN利用Metax GPU群集并着重于三个方面：（1）模型结构：具有自适应尖峰神经元的线性和混合线性注意体系结构； （2）算法优化：一种有效的，基于转换的训练管道和专用的尖峰编码框架； （3）系统工程：定制的培训框架，操作员库和并行性策略量身定制了Metax硬件。使用这些技术，我们开发了两个模型：Spikingbrain-7b，线性LLM和Spikingbrain-76b，即混合线性MOE LLM。这些模型证明了非NVIDIA平台上大型LLM开发的可行性。 Spikingbrain的性能与开源变压器基线相当，而仅使用约150B代币进行持续预训练。我们的模型可显着提高长期训练效率，并通过（部分）恒定的内存和事件驱动的尖峰行为提供推理。例如，Spikingbrain-7b及时获得超过100倍的速度，以首先以4m token序列的速度令牌。训练在数百个METAX C550 GPU上保持稳定数周，其中7B型号达到了23.4％的模型使用。拟议的尖峰方案达到了69.15％的稀疏性，可以实现低功率操作。总体而言，这项工作证明了受脑启发机制推动下一代高效且可扩展的大型模型设计的潜力。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
