<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-01</h1>
<h3>Title: Q-Align: Alleviating Attention Leakage in Zero-Shot Appearance Transfer via Query-Query Alignment</h3>
<ul>
<li><strong>Authors: </strong>Namu Kim, Wonbin Kweon, Minsoo Kim, Hwanjo Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21090">https://arxiv.org/abs/2508.21090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21090">https://arxiv.org/pdf/2508.21090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21090]] Q-Align: Alleviating Attention Leakage in Zero-Shot Appearance Transfer via Query-Query Alignment(https://arxiv.org/abs/2508.21090)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We observe that zero-shot appearance transfer with large-scale image generation models faces a significant challenge: Attention Leakage. This challenge arises when the semantic mapping between two images is captured by the Query-Key alignment. To tackle this issue, we introduce Q-Align, utilizing Query-Query alignment to mitigate attention leakage and improve the semantic alignment in zero-shot appearance transfer. Q-Align incorporates three core contributions: (1) Query-Query alignment, facilitating the sophisticated spatial semantic mapping between two images; (2) Key-Value rearrangement, enhancing feature correspondence through realignment; and (3) Attention refinement using rearranged keys and values to maintain semantic consistency. We validate the effectiveness of Q-Align through extensive experiments and analysis, and Q-Align outperforms state-of-the-art methods in appearance fidelity while maintaining competitive structure preservation.</li>
<li><strong>摘要：</strong>我们观察到，具有大规模图像生成模型的零射击外观转移面临着一个重大挑战：注意力泄漏。当查询键对齐捕获两个图像之间的语义映射时，就会出现此挑战。为了解决此问题，我们介绍了Q-Align，并利用查询Query Alignment来减轻注意力泄漏并改善零拍摄外观传递中的语义对齐。 Q-Align包含三个核心贡献：（1）查询 - 要素对齐，促进了两个图像之间复杂的空间语义映射； （2）键值重排，通过重新调整增强特征对应； （3）使用重新排列的键和值的注意力完善以保持语义一致性。我们通过广泛的实验和分析来验证Q对准的有效性，并且Q-Align在外观保真度中优于最先进的方法，同时保持竞争性结构保存。</li>
</ul>

<h3>Title: ERTACache: Error Rectification and Timesteps Adjustment for Efficient Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Xurui Peng, Hong Liu, Chenqian Yan, Rui Ma, Fangmin Chen, Xing Wang, Zhihua Wu, Songwei Liu, Mingbao Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21091">https://arxiv.org/abs/2508.21091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21091">https://arxiv.org/pdf/2508.21091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21091]] ERTACache: Error Rectification and Timesteps Adjustment for Efficient Diffusion(https://arxiv.org/abs/2508.21091)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models suffer from substantial computational overhead due to their inherently iterative inference process. While feature caching offers a promising acceleration strategy by reusing intermediate outputs across timesteps, naive reuse often incurs noticeable quality degradation. In this work, we formally analyze the cumulative error introduced by caching and decompose it into two principal components: feature shift error, caused by inaccuracies in cached outputs, and step amplification error, which arises from error propagation under fixed timestep schedules. To address these issues, we propose ERTACache, a principled caching framework that jointly rectifies both error types. Our method employs an offline residual profiling stage to identify reusable steps, dynamically adjusts integration intervals via a trajectory-aware correction coefficient, and analytically approximates cache-induced errors through a closed-form residual linearization model. Together, these components enable accurate and efficient sampling under aggressive cache reuse. Extensive experiments across standard image and video generation benchmarks show that ERTACache achieves up to 2x inference speedup while consistently preserving or even improving visual quality. Notably, on the state-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x acceleration with minimal VBench degradation, effectively maintaining baseline fidelity while significantly improving efficiency. The code is available at this https URL.</li>
<li><strong>摘要：</strong>扩散模型由于其固有的迭代推理过程而遭受了大量的计算开销。尽管功能缓存通过在时间段之间重复使用中间输出来提供有希望的加速策略，但天真的再利用通常会引起明显的质量降级。在这项工作中，我们正式分析了通过缓存引入的累积错误，并将其分解为两个主要组件：特征移位误差，这是由于缓存输出不准确引起的，而台阶放大误差引起的，这是由固定时间段计划下的错误传播引起的。为了解决这些问题，我们提出了Ertacache，这是一个原则上的缓存框架，共同纠正了这两种错误类型。我们的方法采用离线残差分析阶段来识别可重复使用的步骤，通过轨迹感知的校正系数动态调整集成间隔，并通过分析通过闭合形式的残差线性化模型来分析近似缓存诱导的误差。这些组件共同使在积极的缓存重复使用下实现准确有效的采样。跨标准图像和视频生成基准的广泛实验表明，厄尔塔赫（Ertacache）达到了2倍的推理速度，同时始终保持视觉质量甚至可以提高视觉质量。值得注意的是，在最先进的WAN2.1视频扩散模型上，Ertacache可提供2倍加速度，并以最小的VBench降级，有效地维持基线保真度，同时显着提高了效率。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Xiangtao Meng, Yingkai Dong, Ning Yu, Li Wang, Zheng Li, Shanqing Guo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21099">https://arxiv.org/abs/2508.21099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21099">https://arxiv.org/pdf/2508.21099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21099]] Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models(https://arxiv.org/abs/2508.21099)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Despite the advancements in Text-to-Image (T2I) generation models, their potential for misuse or even abuse raises serious safety concerns. Model developers have made tremendous efforts to introduce safety mechanisms that can address these concerns in T2I models. However, the existing safety mechanisms, whether external or internal, either remain susceptible to evasion under distribution shifts or require extensive model-specific adjustments. To address these limitations, we introduce Safe-Control, an innovative plug-and-play safety patch designed to mitigate unsafe content generation in T2I models. Using data-driven strategies and safety-aware conditions, Safe-Control injects safety control signals into the locked T2I model, acting as an update in a patch-like manner. Model developers can also construct various safety patches to meet the evolving safety requirements, which can be flexibly merged into a single, unified patch. Its plug-and-play design further ensures adaptability, making it compatible with other T2I models of similar denoising architecture. We conduct extensive evaluations on six diverse and public T2I models. Empirical results highlight that Safe-Control is effective in reducing unsafe content generation across six diverse T2I models with similar generative architectures, yet it successfully maintains the quality and text alignment of benign images. Compared to seven state-of-the-art safety mechanisms, including both external and internal defenses, Safe-Control significantly outperforms all baselines in reducing unsafe content generation. For example, it reduces the probability of unsafe content generation to 7%, compared to approximately 20% for most baseline methods, under both unsafe prompts and the latest adversarial attacks.</li>
<li><strong>摘要：</strong>尽管文本到图像（T2i）的生成模型取得了进步，但它们的滥用甚至滥用的潜力引起了严重的安全问题。模型开发人员已经做出了巨大的努力，以引入可以解决T2I模型中这些问题的安全机制。但是，现有的安全机制，无论是外部还是内部，要么容易在分配变化下逃避逃避，要么需要广泛的模型特定调整。为了解决这些限制，我们引入了Safe-Control，这是一种创新的插件安全补丁，旨在减轻T2I模型中不安全的内容的生成。使用数据驱动的策略和安全感知条件，安全控制将安全控制信号注入锁定的T2I模型，以贴片状的方式充当更新。模型开发人员还可以构建各种安全补丁，以满足不断发展的安全要求，这些要求可以灵活地合并为一个统一的补丁。它的插件设计进一步确保了适应性，使其与其他类似Denoising架构的T2I模型兼容。我们对六种不同和公共T2I模型进行了广泛的评估。经验结果表明，安全控制可以有效地减少具有类似生成架构的六种不同T2I模型的不安全内容产生，但它成功地维持了良性图像的质量和文本对齐。相比之下，包括外部防御和内部防御包括七种最先进的安全机制，安全控制在减少不安全内容的产生方面明显优于所有基准。例如，在不安全的提示和最新的对抗性攻击下，它将不安全内容产生的可能性降低到7％，而大多数基线方法的可能性约为20％。</li>
</ul>

<h3>Title: GENNAV: Polygon Mask Generation for Generalized Referring Navigable Regions</h3>
<ul>
<li><strong>Authors: </strong>Kei Katsumata, Yui Iioka, Naoki Hosomi, Teruhisa Misu, Kentaro Yamada, Komei Sugiura</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21102">https://arxiv.org/abs/2508.21102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21102">https://arxiv.org/pdf/2508.21102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21102]] GENNAV: Polygon Mask Generation for Generalized Referring Navigable Regions(https://arxiv.org/abs/2508.21102)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We focus on the task of identifying the location of target regions from a natural language instruction and a front camera image captured by a mobility. This task is challenging because it requires both existence prediction and segmentation, particularly for stuff-type target regions with ambiguous boundaries. Existing methods often underperform in handling stuff-type target regions, in addition to absent or multiple targets. To overcome these limitations, we propose GENNAV, which predicts target existence and generates segmentation masks for multiple stuff-type target regions. To evaluate GENNAV, we constructed a novel benchmark called GRiN-Drive, which includes three distinct types of samples: no-target, single-target, and multi-target. GENNAV achieved superior performance over baseline methods on standard evaluation metrics. Furthermore, we conducted real-world experiments with four automobiles operated in five geographically distinct urban areas to validate its zero-shot transfer performance. In these experiments, GENNAV outperformed baseline methods and demonstrated its robustness across diverse real-world environments. The project page is available at this https URL.</li>
<li><strong>摘要：</strong>我们专注于从自然语言指令和移动性捕获的前摄像头图像中识别目标区域的位置的任务。此任务是具有挑战性的，因为它既需要存在预测和细分，尤其是对于具有模棱两可的界限的东西类型的目标区域。除了缺乏或多个目标外，现有的方法在处理物质型目标区域的表现通常不佳。为了克服这些局限性，我们提出了GenNAV，该Gennav预测了目标存在并为多个物质类型目标区域生成分割掩模。为了评估Gennav，我们构建了一个名为Grin-Drive的新基准，其中包括三种不同类型的样本：无目标，单目标和多目标。 GenNAV在标准评估指标上实现了优于基线方法。此外，我们对在五个地理上不同的城市地区运行的四辆汽车进行了现实世界实验，以验证其零射击转移性能。在这些实验中，GenNAV胜过基线方法，并在各种现实世界环境中证明了其稳健性。该项目页面可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Privacy Auditing Synthetic Data Release through Local Likelihood Attacks</h3>
<ul>
<li><strong>Authors: </strong>Joshua Ward, Chi-Hua Wang, Guang Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21146">https://arxiv.org/abs/2508.21146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21146">https://arxiv.org/pdf/2508.21146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21146]] Privacy Auditing Synthetic Data Release through Local Likelihood Attacks(https://arxiv.org/abs/2508.21146)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Auditing the privacy leakage of synthetic data is an important but unresolved problem. Most existing privacy auditing frameworks for synthetic data rely on heuristics and unreasonable assumptions to attack the failure modes of generative models, exhibiting limited capability to describe and detect the privacy exposure of training data through synthetic data release. In this paper, we study designing Membership Inference Attacks (MIAs) that specifically exploit the observation that tabular generative models tend to significantly overfit to certain regions of the training distribution. Here, we propose Generative Likelihood Ratio Attack (Gen-LRA), a novel, computationally efficient No-Box MIA that, with no assumption of model knowledge or access, formulates its attack by evaluating the influence a test observation has in a surrogate model's estimation of a local likelihood ratio over the synthetic data. Assessed over a comprehensive benchmark spanning diverse datasets, model architectures, and attack parameters, we find that Gen-LRA consistently dominates other MIAs for generative models across multiple performance metrics. These results underscore Gen-LRA's effectiveness as a privacy auditing tool for the release of synthetic data, highlighting the significant privacy risks posed by generative model overfitting in real-world applications.</li>
<li><strong>摘要：</strong>审核合成数据的隐私泄漏是一个重要但尚未解决的问题。用于合成数据的大多数现有隐私审核框架都取决于启发式和不合理的假设来攻击生成模型的故障模式，从而表现出有限的能力来描述和检测通过合成数据释放来描述和检测培训数据的隐私暴露。在本文中，我们研究了设计成员推理攻击（MIA）的设计，这些攻击（MIA）专门利用了表格生成模型倾向于显着过于拟合训练分布的某些区域的观察。在这里，我们提出了一种生成的似然比攻击（Gen-LRA），这是一种新颖的计算有效的无盒MIA，而没有假设模型知识或访问，通过评估测试观察在替代模型中对局部可能性比率的估计，通过评估测试观察的影响来制定其攻击。在跨越各种数据集，模型体系结构和攻击参数的全面基准测试中，我们发现LRA始终在多个性能指标的生成模型中始终主导其他MIA。这些结果强调了Gen-LRA作为释放合成数据的隐私审核工具的有效性，强调了生成模型过于适应现实世界应用程序所带来的明显隐私风险。</li>
</ul>

<h3>Title: SYNBUILD-3D: A large, multi-modal, and semantically rich synthetic dataset of 3D building models at Level of Detail 4</h3>
<ul>
<li><strong>Authors: </strong>Kevin Mayer, Alex Vesel, Xinyi Zhao, Martin Fischer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21169">https://arxiv.org/abs/2508.21169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21169">https://arxiv.org/pdf/2508.21169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21169]] SYNBUILD-3D: A large, multi-modal, and semantically rich synthetic dataset of 3D building models at Level of Detail 4(https://arxiv.org/abs/2508.21169)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>3D building models are critical for applications in architecture, energy simulation, and navigation. Yet, generating accurate and semantically rich 3D buildings automatically remains a major challenge due to the lack of large-scale annotated datasets in the public domain. Inspired by the success of synthetic data in computer vision, we introduce SYNBUILD-3D, a large, diverse, and multi-modal dataset of over 6.2 million synthetic 3D residential buildings at Level of Detail (LoD) 4. In the dataset, each building is represented through three distinct modalities: a semantically enriched 3D wireframe graph at LoD 4 (Modality I), the corresponding floor plan images (Modality II), and a LiDAR-like roof point cloud (Modality III). The semantic annotations for each building wireframe are derived from the corresponding floor plan images and include information on rooms, doors, and windows. Through its tri-modal nature, future work can use SYNBUILD-3D to develop novel generative AI algorithms that automate the creation of 3D building models at LoD 4, subject to predefined floor plan layouts and roof geometries, while enforcing semantic-geometric consistency. Dataset and code samples are publicly available at this https URL.</li>
<li><strong>摘要：</strong>3D建筑模型对于在建筑，能源模拟和导航中的应用至关重要。但是，由于公共领域中缺乏大规模注释的数据集，因此自动生成准确和语义上富含的3D建筑物仍然是一个重大挑战。 Inspired by the success of synthetic data in computer vision, we introduce SYNBUILD-3D, a large, diverse, and multi-modal dataset of over 6.2 million synthetic 3D residential buildings at Level of Detail (LoD) 4. In the dataset, each building is represented through three distinct modalities: a semantically enriched 3D wireframe graph at LoD 4 (Modality I), the corresponding floor plan images (Modality II),以及像激光雷达一样的屋顶点云（模态III）。每个建筑物线框的语义注释均来自相应的平面图图像，并包括有关房间，门和窗户的信息。通过其三模式的性质，未来的工作可以使用Synbuild-3D来开发新颖的生成AI算法，该算法可以自动化LOD 4的3D建筑模型，但要遵守预定义的平面图布局和屋顶几何形状，同时实施了语义几何的一致性。数据集和代码示例可在此HTTPS URL上公开可用。</li>
</ul>

<h3>Title: Class Incremental Continual Learning with Self-Organizing Maps and Variational Autoencoders Using Synthetic Replay</h3>
<ul>
<li><strong>Authors: </strong>Pujan Thapa, Alexander Ororbia, Travis Desell</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21240">https://arxiv.org/abs/2508.21240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21240">https://arxiv.org/pdf/2508.21240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21240]] Class Incremental Continual Learning with Self-Organizing Maps and Variational Autoencoders Using Synthetic Replay(https://arxiv.org/abs/2508.21240)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This work introduces a novel generative continual learning framework based on self-organizing maps (SOMs) and variational autoencoders (VAEs) to enable memory-efficient replay, eliminating the need to store raw data samples or task labels. For high-dimensional input spaces, such as of CIFAR-10 and CIFAR-100, we design a scheme where the SOM operates over the latent space learned by a VAE, whereas, for lower-dimensional inputs, such as those found in MNIST and FashionMNIST, the SOM operates in a standalone fashion. Our method stores a running mean, variance, and covariance for each SOM unit, from which synthetic samples are then generated during future learning iterations. For the VAE-based method, generated samples are then fed through the decoder to then be used in subsequent replay. Experimental results on standard class-incremental benchmarks show that our approach performs competitively with state-of-the-art memory-based methods and outperforms memory-free methods, notably improving over best state-of-the-art single class incremental performance on CIFAR-10 and CIFAR-100 by nearly $10$\% and $7$\%, respectively. Our methodology further facilitates easy visualization of the learning process and can also be utilized as a generative model post-training. Results show our method's capability as a scalable, task-label-free, and memory-efficient solution for continual learning.</li>
<li><strong>摘要：</strong>这项工作引入了基于自组织图（SOM）和变化自动编码器（VAE）的新颖生成性持续学习框架，以实现记忆效率重播，从而消除了存储原始数据样本或任务标签的需求。对于诸如CIFAR-10和CIFAR-100之类的高维输入空间，我们设计了一个方案，其中SOM在VAE学到的潜在空间上运行，而对于低维输入，例如在Mnist和FashionMnist中发现的较低尺寸的输入，Some以独立的方式运行。我们的方法为每个SOM单元存储了一个均值，方差和协方差，然后在将来的学习迭代过程中生成合成样本。对于基于VAE的方法，然后通过解码器喂食生成的样品，然后在随后的重播中使用。对标准类收入基准测试的实验结果表明，我们的方法在最先进的基于内存的方法中竞争性能，并且优于不含内存的方法，尤其是在CIFAR-10上的最佳最佳单一类增量性能和CIFAR-100上的最佳单一类增量性能，分别将接近$ \％和$ 7 $ \％提高。我们的方法进一步促进了学习过程的简单可视化，也可以用作培训后的生成模型。结果表明，我们的方法的能力是可扩展的，无任务标签和记忆效率的解决方案，可连续学习。</li>
</ul>

<h3>Title: Reverse Imaging for Wide-spectrum Generalization of Cardiac MRI Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yidong Zhao, Peter Kellman, Hui Xue, Tongyun Yang, Yi Zhang, Yuchi Han, Orlando Simonetti, Qian Tao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21254">https://arxiv.org/abs/2508.21254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21254">https://arxiv.org/pdf/2508.21254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21254]] Reverse Imaging for Wide-spectrum Generalization of Cardiac MRI Segmentation(https://arxiv.org/abs/2508.21254)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Pretrained segmentation models for cardiac magnetic resonance imaging (MRI) struggle to generalize across different imaging sequences due to significant variations in image contrast. These variations arise from changes in imaging protocols, yet the same fundamental spin properties, including proton density, T1, and T2 values, govern all acquired images. With this core principle, we introduce Reverse Imaging, a novel physics-driven method for cardiac MRI data augmentation and domain adaptation to fundamentally solve the generalization problem. Our method reversely infers the underlying spin properties from observed cardiac MRI images, by solving ill-posed nonlinear inverse problems regularized by the prior distribution of spin properties. We acquire this "spin prior" by learning a generative diffusion model from the multiparametric SAturation-recovery single-SHot acquisition sequence (mSASHA) dataset, which offers joint cardiac T1 and T2 maps. Our method enables approximate but meaningful spin-property estimates from MR images, which provide an interpretable "latent variable" that lead to highly flexible image synthesis of arbitrary novel sequences. We show that Reverse Imaging enables highly accurate segmentation across vastly different image contrasts and imaging protocols, realizing wide-spectrum generalization of cardiac MRI segmentation.</li>
<li><strong>摘要：</strong>由于图像对比度的显着差异，预审前的心脏磁共振成像（MRI）的分割模型（MRI）努力跨不同的成像序列概括。这些变化来自成像协议的变化，但是包括质子密度，T1和T2值在内的相同基本旋转特性，控制了所有获得的图像。借助此核心原理，我们引入了反向成像，这是一种新型的物理驱动的方法，用于心脏MRI数据增强和域的适应性，以从根本上解决概括问题。我们的方法通过求解通过自旋特性的先前分布正规化的不适合的非线性逆问题，从观察到的心脏MRI图像中反复侵入潜在的自旋特性。我们通过从多组合饱和恢复单摄入序列（MSASHA）数据集中学习一个生成扩散模型来获取此“旋转先验”，该模型提供了联合心脏T1和T2映射。我们的方法可以从MR图像中估算近似但有意义的自旋特里估计，该估计提供了可解释的“潜在变量”，从而导致任意新型序列的高度灵活图像综合。我们表明，逆成像可以在截然不同的图像对比度和成像方案上进行高度准确的分割，从而实现了心脏MRI分割的广谱概括。</li>
</ul>

<h3>Title: MyGO: Memory Yielding Generative Offline-consolidation for Lifelong Learning Systems</h3>
<ul>
<li><strong>Authors: </strong>Shihao Ji, Zihui Song</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21296">https://arxiv.org/abs/2508.21296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21296">https://arxiv.org/pdf/2508.21296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21296]] MyGO: Memory Yielding Generative Offline-consolidation for Lifelong Learning Systems(https://arxiv.org/abs/2508.21296)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Continual or Lifelong Learning aims to develop models capable of acquiring new knowledge from a sequence of tasks without catastrophically forgetting what has been learned before. Existing approaches often rely on storing samples from previous tasks (experience replay) or employing complex regularization terms to protect learned weights. However, these methods face challenges related to data privacy, storage limitations, and performance degradation when tasks are dissimilar. To address these challenges, we introduce MyGO (Memory Yielding Generative Offline-consolidation), a novel lifelong learning framework inspired by the biological wake-sleep cycle. During the "wake" phase, the system rapidly learns a new task and trains a compact generative model (Generative Memory, G-mem) to capture its data distribution. During the "sleep" phase, the system enters an offline state, using all learned G-mem models to generate pseudo-data ("dreams") and consolidate new and old knowledge into a core feature extractor via knowledge distillation. This approach obviates the need to store any raw data, retaining only compact generative models, which offers significant advantages in privacy and storage efficiency. We evaluate MyGO on computer vision (Split-MNIST) and natural language processing (Split-AG News) benchmarks, comparing it against a sequential fine-tuning baseline. The results demonstrate that MyGO significantly mitigates catastrophic forgetting and maintains high average accuracy across tasks, proving the framework's effectiveness and domain-generality.</li>
<li><strong>摘要：</strong>持续或终身学习旨在开发能够从一系列任务中获取新知识的模型，而不会忘记以前学到的知识。现有方法通常依赖于以前的任务（经验重播）或采用复杂的正则化术语来保护学习权重的样本。但是，在任务不同时，这些方法面临与数据隐私，存储限制和性能下降有关的挑战。为了应对这些挑战，我们引入了MyGo（记忆产生生成的离线固化），这是一个受生物唤醒循环启发的新型终身学习框架。在“唤醒”阶段，系统迅速学习了一项新任务，并训练紧凑的生成模型（生成内存，G-MEM）以捕获其数据分布。在“睡眠”阶段，系统使用所有学到的G-MEM模型进入脱机状态，以生成伪数据（“ Dreams”），并通过知识蒸馏将新的和旧知识合并为核心特征提取器。这种方法避免了存储任何原始数据的需求，仅保留紧凑的生成模型，这在隐私和存储效率方面具有显着优势。我们在计算机视觉（分式）和自然语言处理（Split-AG新闻）基准上评估MyGo，并将其与顺序的微调基线进行比较。结果表明，MyGo显着减轻了灾难性的遗忘，并在整个任务之间保持了高平均精度，证明了该框架的有效性和域基因。</li>
</ul>

<h3>Title: Stage-Diff: Stage-wise Long-Term Time Series Generation Based on Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xuan Hou, Shuhan Liu, Zhaohui Peng, Yaohui Chu, Yue Zhang, Yining Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21330">https://arxiv.org/abs/2508.21330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21330">https://arxiv.org/pdf/2508.21330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21330]] Stage-Diff: Stage-wise Long-Term Time Series Generation Based on Diffusion Models(https://arxiv.org/abs/2508.21330)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative models have been successfully used in the field of time series generation. However, when dealing with long-term time series, which span over extended periods and exhibit more complex long-term temporal patterns, the task of generation becomes significantly more challenging. Long-term time series exhibit long-range temporal dependencies, but their data distribution also undergoes gradual changes over time. Finding a balance between these long-term dependencies and the drift in data distribution is a key challenge. On the other hand, long-term time series contain more complex interrelationships between different feature sequences, making the task of effectively capturing both intra-sequence and inter-sequence dependencies another important challenge. To address these issues, we propose Stage-Diff, a staged generative model for long-term time series based on diffusion models. First, through stage-wise sequence generation and inter-stage information transfer, the model preserves long-term sequence dependencies while enabling the modeling of data distribution shifts. Second, within each stage, progressive sequence decomposition is applied to perform channel-independent modeling at different time scales, while inter-stage information transfer utilizes multi-channel fusion modeling. This approach combines the robustness of channel-independent modeling with the information fusion advantages of multi-channel modeling, effectively balancing the intra-sequence and inter-sequence dependencies of long-term time series. Extensive experiments on multiple real-world datasets validate the effectiveness of Stage-Diff in long-term time series generation tasks.</li>
<li><strong>摘要：</strong>生成模型已成功地用于时间序列生成领域。但是，在处理长期延伸并表现出更复杂的长期时间模式的长期时间序列时，发电的任务变得更加挑战。长期时间序列表现出长期的时间依赖性，但它们的数据分布也随着时间的推移而逐渐变化。在这些长期依赖关系和数据分布的漂移之间找到平衡是一个关键挑战。另一方面，长期时间序列在不同特征序列之间包含更复杂的相互关系，这使得有效捕获序列内部和序列依赖关系的任务是另一个重要的挑战。为了解决这些问题，我们提出了基于扩散模型的长期时间序列的分阶段生成模型。首先，通过阶段序列的产生和阶段间信息传输，该模型可以保留长期序列依赖性，同时启用数据分布变化的建模。其次，在每个阶段，渐进序列分解用于在不同时间尺度上执行与信道无关的建模，而阶段间信息传输则利用多通道融合建模。这种方法结合了独立于通道的建模与多通道建模的信息融合优势，有效地平衡了长期时间序列的内部序列和序列依赖性。在多个现实世界数据集上进行的广泛实验验证了阶段 - 陷阱在长期序列生成任务中的有效性。</li>
</ul>

<h3>Title: DLGAN : Time Series Synthesis Based on Dual-Layer Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Xuan Hou, Shuhan Liu, Zhaohui Peng, Yaohui Chu, Yue Zhang, Yining Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21340">https://arxiv.org/abs/2508.21340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21340">https://arxiv.org/pdf/2508.21340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21340]] DLGAN : Time Series Synthesis Based on Dual-Layer Generative Adversarial Networks(https://arxiv.org/abs/2508.21340)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Time series synthesis is an effective approach to ensuring the secure circulation of time series data. Existing time series synthesis methods typically perform temporal modeling based on random sequences to generate target sequences, which often struggle to ensure the temporal dependencies in the generated time series. Additionally, directly modeling temporal features on random sequences makes it challenging to accurately capture the feature information of the original time series. To address the above issues, we propose a simple but effective generative model \textbf{D}ual-\textbf{L}ayer \textbf{G}enerative \textbf{A}dversarial \textbf{N}etworks, named \textbf{DLGAN}. The model decomposes the time series generation process into two stages: sequence feature extraction and sequence reconstruction. First, these two stages form a complete time series autoencoder, enabling supervised learning on the original time series to ensure that the reconstruction process can restore the temporal dependencies of the sequence. Second, a Generative Adversarial Network (GAN) is used to generate synthetic feature vectors that align with the real-time sequence feature vectors, ensuring that the generator can capture the temporal features from real time series. Extensive experiments on four public datasets demonstrate the superiority of this model across various evaluation metrics.</li>
<li><strong>摘要：</strong>时间序列合成是确保时间序列数据安全循环的有效方法。现有时间序列合成方法通常基于随机序列执行时间建模以生成目标序列，而目标序列通常难以确保生成的时间序列中的时间依赖性。此外，在随机序列上直接建模时间特征使得准确捕获原始时间序列的特征信息变得具有挑战性。为了解决上述问题，我们提出了一个简单但有效的生成模型\ textbf {d} ual- \ textbf {l} ayer \ textbf {g} eneralative \ textbf {a} dversarial \ textbf \ textbf {n}该模型将时间序列生成过程分为两个阶段：序列特征提取和序列重建。首先，这两个阶段构成了一个完整的时间序列自动编码器，从而在原始时间序列上实现了监督学习，以确保重建过程可以恢复序列的时间依赖性。其次，使用生成对抗网络（GAN）用于生成与实时序列特征向量对齐的合成特征向量，从而确保生成器可以从实时序列中捕获时间特征。四个公共数据集的广泛实验证明了该模型在各种评估指标上的优越性。</li>
</ul>

<h3>Title: MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Francisco Caetano, Christiaan Viviers, Peter H.H. de With, Fons van der Sommen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21435">https://arxiv.org/abs/2508.21435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21435">https://arxiv.org/pdf/2508.21435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21435]] MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation(https://arxiv.org/abs/2508.21435)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Synthetic medical data offers a scalable solution for training robust models, but significant domain gaps limit its generalizability to real-world clinical settings. This paper addresses the challenge of cross-domain translation between synthetic and real X-ray images of the head, focusing on bridging discrepancies in attenuation behavior, noise characteristics, and soft tissue representation. We propose MedShift, a unified class-conditional generative model based on Flow Matching and Schrodinger Bridges, which enables high-fidelity, unpaired image translation across multiple domains. Unlike prior approaches that require domain-specific training or rely on paired data, MedShift learns a shared domain-agnostic latent space and supports seamless translation between any pair of domains seen during training. We introduce X-DigiSkull, a new dataset comprising aligned synthetic and real skull X-rays under varying radiation doses, to benchmark domain translation models. Experimental results demonstrate that, despite its smaller model size compared to diffusion-based approaches, MedShift offers strong performance and remains flexible at inference time, as it can be tuned to prioritize either perceptual fidelity or structural consistency, making it a scalable and generalizable solution for domain adaptation in medical imaging. The code and dataset are available at this https URL</li>
<li><strong>摘要：</strong>合成医学数据为训练强大的模型提供了可扩展的解决方案，但是重要的域间隙限制了其对现实世界临床环境的普遍性。本文解决了头部合成和真实X射线图像之间的跨域翻译的挑战，重点是弥合衰减行为，噪声特征和软组织表示方面的差异。我们提出了MedShift，这是一种基于流量匹配和Schrodinger桥梁的统一类条件生成模型，该模型可以跨多个域启用高保真，未配对的图像翻译。与需要特定领域的培训或依靠配对数据的先前方法不同，MedShift学习了共享的域 - 不合命相的潜在空间，并支持训练过程中任何一对域之间的无缝翻译。我们将X-Digiskull（一种新的数据集介绍，包括在不同的辐射剂量下包含对齐的合成和真实的头骨X射线，以介绍基准域翻译模型。实验结果表明，尽管与基于扩散的方法相比，MEDSHIFT具有较小的模型大小，但在推理时间仍保持灵活的性能，因为它可以调整以优先考虑感知忠诚度或结构性一致性，从而使其成为医学成像中域适应的可扩展和概括的解决方案。该代码和数据集可在此HTTPS URL上找到</li>
</ul>

<h3>Title: Trees as Gaussians: Large-Scale Individual Tree Mapping</h3>
<ul>
<li><strong>Authors: </strong>Dimitri Gominski, Martin Brandt, Xiaoye Tong, Siyu Liu, Maurice Mugabowindekwe, Sizhuo Li, Florian Reiner, Andrew Davies, Rasmus Fensholt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21437">https://arxiv.org/abs/2508.21437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21437">https://arxiv.org/pdf/2508.21437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21437]] Trees as Gaussians: Large-Scale Individual Tree Mapping(https://arxiv.org/abs/2508.21437)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Trees are key components of the terrestrial biosphere, playing vital roles in ecosystem function, climate regulation, and the bioeconomy. However, large-scale monitoring of individual trees remains limited by inadequate modelling. Available global products have focused on binary tree cover or canopy height, which do not explicitely identify trees at individual level. In this study, we present a deep learning approach for detecting large individual trees in 3-m resolution PlanetScope imagery at a global scale. We simulate tree crowns with Gaussian kernels of scalable size, allowing the extraction of crown centers and the generation of binary tree cover maps. Training is based on billions of points automatically extracted from airborne lidar data, enabling the model to successfully identify trees both inside and outside forests. We compare against existing tree cover maps and airborne lidar with state-of-the-art performance (fractional cover R$^2 = 0.81$ against aerial lidar), report balanced detection metrics across biomes, and demonstrate how detection can be further improved through fine-tuning with manual labels. Our method offers a scalable framework for global, high-resolution tree monitoring, and is adaptable to future satellite missions offering improved imagery.</li>
<li><strong>摘要：</strong>树木是陆地生物圈的关键组成部分，在生态系统功能，气候调节和生物经济中起着至关重要的作用。但是，对单个树木的大规模监测仍然受到建模不足的限制。可用的全球产品集中在二进制树盖或冠层高度上，这些高度不会在个人层面上明确识别树木。在这项研究中，我们提出了一种深度学习方法，用于在全球范围内检测3-M分辨率Planetscope图像中的大型树木。我们用可扩展大小的高斯核对树冠进行模拟，从而提取冠中心和产生二进制树盖地图。培训是基于从空中激光雷达数据中自动提取的数十个点，使该模型能够成功地识别森林内部和外部的树木。我们将现有的树覆盖地图和空气传球雷达与最先进的性能进行比较（分数覆盖r $^2 = 0.81 $ firenail LiDAR），报告了跨生物群的平衡检测指标，并证明如何通过使用手动标签进行微调来进一步改善检测。我们的方法为全球高分辨率树监测提供了可扩展的框架，并适用于未来提供改进图像的卫星任务。</li>
</ul>

<h3>Title: Quantum enhanced ensemble GANs for anomaly detection in continuous biomanufacturing</h3>
<ul>
<li><strong>Authors: </strong>Rajiv Kailasanathan, William R. Clements, Mohammad Reza Boskabadi, Shawn M. Gibford, Emmanouil Papadakis, Christopher J. Savoie, Seyed Soheil Mansouri</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.OT, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21438">https://arxiv.org/abs/2508.21438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21438">https://arxiv.org/pdf/2508.21438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21438]] Quantum enhanced ensemble GANs for anomaly detection in continuous biomanufacturing(https://arxiv.org/abs/2508.21438)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The development of continuous biomanufacturing processes requires robust and early anomaly detection, since even minor deviations can compromise yield and stability, leading to disruptions in scheduling, reduced weekly production, and diminished economic performance. These processes are inherently complex and exhibit non-linear dynamics with intricate relationships between process variables, thus making advanced methods for anomaly detection essential for efficient operation. In this work, we present a novel framework for unsupervised anomaly detection in continuous biomanufacturing based on an ensemble of generative adversarial networks (GANs). We first establish a benchmark dataset simulating both normal and anomalous operation regimes in a continuous process for the production of a small molecule. We then demonstrate the effectiveness of our GAN-based framework in detecting anomalies caused by sudden feedstock variability. Finally, we evaluate the impact of using a hybrid quantum/classical GAN approach with both a simulated quantum circuit and a real photonic quantum processor on anomaly detection performance. We find that the hybrid approach yields improved anomaly detection rates. Our work shows the potential of hybrid quantum/classical approaches for solving real-world problems in complex continuous biomanufacturing processes.</li>
<li><strong>摘要：</strong>连续生物制造过程的开发需要强大的和早期的异常检测，因为即使是微小的偏差也会损害收益率和稳定性，从而导致调度中断，每周生产减少以及经济绩效降低。这些过程本质上是复杂的，并且在过程变量之间具有复杂的关系，表现出非线性动力学，因此为有效操作提供了高级方法。在这项工作中，我们提出了一个基于生成对抗网络（GAN）合奏的连续生物制造中无监督的异常检测的新框架。我们首先建立一个基准数据集，该数据集在生产小分子的连续过程中模拟正常和异常操作方案。然后，我们证明了基于GAN的框架在检测突然原料可变性引起的异常方面的有效性。最后，我们评估了使用模拟量子电路和实际光子量子处理器对异常检测性能使用杂交量子/经典GAN方法的影响。我们发现，混合方法产生改善的异常检测率。我们的工作表明了混合量子/经典方法在复杂的连续生物制造过程中解决现实世界问题的潜力。</li>
</ul>

<h3>Title: Controllable 3D Molecular Generation for Structure-Based Drug Design Through Bayesian Flow Networks and Gradient Integration</h3>
<ul>
<li><strong>Authors: </strong>Seungyeon Choi, Hwanhee Kim, Chihyun Park, Dahyeon Lee, Seungyong Lee, Yoonju Kim, Hyoungjoon Park, Sein Kwon, Youngwan Jo, Sanghyun Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21468">https://arxiv.org/abs/2508.21468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21468">https://arxiv.org/pdf/2508.21468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21468]] Controllable 3D Molecular Generation for Structure-Based Drug Design Through Bayesian Flow Networks and Gradient Integration(https://arxiv.org/abs/2508.21468)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in Structure-based Drug Design (SBDD) have leveraged generative models for 3D molecular generation, predominantly evaluating model performance by binding affinity to target proteins. However, practical drug discovery necessitates high binding affinity along with synthetic feasibility and selectivity, critical properties that were largely neglected in previous evaluations. To address this gap, we identify fundamental limitations of conventional diffusion-based generative models in effectively guiding molecule generation toward these diverse pharmacological properties. We propose CByG, a novel framework extending Bayesian Flow Network into a gradient-based conditional generative model that robustly integrates property-specific guidance. Additionally, we introduce a comprehensive evaluation scheme incorporating practical benchmarks for binding affinity, synthetic feasibility, and selectivity, overcoming the limitations of conventional evaluation methods. Extensive experiments demonstrate that our proposed CByG framework significantly outperforms baseline models across multiple essential evaluation criteria, highlighting its effectiveness and practicality for real-world drug discovery applications.</li>
<li><strong>摘要：</strong>基于结构的药物设计（SBDD）的最新进展已利用生成模型来进行3D分子产生，主要通过结合与靶蛋白的亲和力来评估模型性能。但是，实际药物发现需要高结合亲和力以及合成性和选择性，在先前评估中很大程度上被忽略的关键特性。为了解决这一差距，我们确定了有效指导分子生成这些多种药理特性的常规扩散生成模型的基本局限性。我们提出了CBYG，这是一个新颖的框架，将贝叶斯流网络扩展到基于梯度的条件生成模型，该模型可牢固地整合特定于属性的指导。此外，我们引入了一项全面的评估方案，该方案结合了实用基准，用于结合亲和力，合成可行性和选择性，克服了常规评估方法的局限性。广泛的实验表明，我们提出的CBYG框架在多个基本评估标准上显着优于基线模型，这突出了其对现实世界药物发现应用的有效性和实用性。</li>
</ul>

<h3>Title: Complete Gaussian Splats from a Single Image with Denoising Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ziwei Liao, Mohamed Sayed, Steven L. Waslander, Sara Vicente, Daniyar Turmukhambetov, Michael Firman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21542">https://arxiv.org/abs/2508.21542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21542">https://arxiv.org/pdf/2508.21542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21542]] Complete Gaussian Splats from a Single Image with Denoising Diffusion Models(https://arxiv.org/abs/2508.21542)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Gaussian splatting typically requires dense observations of the scene and can fail to reconstruct occluded and unobserved areas. We propose a latent diffusion model to reconstruct a complete 3D scene with Gaussian splats, including the occluded parts, from only a single image during inference. Completing the unobserved surfaces of a scene is challenging due to the ambiguity of the plausible surfaces. Conventional methods use a regression-based formulation to predict a single "mode" for occluded and out-of-frustum surfaces, leading to blurriness, implausibility, and failure to capture multiple possible explanations. Thus, they often address this problem partially, focusing either on objects isolated from the background, reconstructing only visible surfaces, or failing to extrapolate far from the input views. In contrast, we propose a generative formulation to learn a distribution of 3D representations of Gaussian splats conditioned on a single input image. To address the lack of ground-truth training data, we propose a Variational AutoReconstructor to learn a latent space only from 2D images in a self-supervised manner, over which a diffusion model is trained. Our method generates faithful reconstructions and diverse samples with the ability to complete the occluded surfaces for high-quality 360-degree renderings.</li>
<li><strong>摘要：</strong>高斯碎片通常需要对场景进行密集的观察，并且可能无法重建遮挡和未观察到的区域。我们提出了一个潜在扩散模型，以从推理期间仅从一个图像中重建一个用高斯夹层（包括遮挡的部分）重建完整的3D场景。由于合理表面的歧义，完成场景的未观察到的表面是具有挑战性的。常规方法使用基于回归的公式来预测遮挡和毛外表面的单个“模式”，从而导致模糊，不可思议和无法捕获多种可能的解释。因此，他们通常会部分解决此问题，将重点放在与背景隔离的对象上，仅重建可见的表面，或者未能远离输入视图。相比之下，我们提出了一种生成公式，以学习以单个输入图像为条件的高斯夹层的3D表示的分布。为了解决缺乏地面真相训练数据，我们提出了一个变异自动构造者，以自我监督的方式从2D图像中学习潜在空间，并在此训练了扩散模型。我们的方法产生了忠实的重建和不同的样本，能够完成高质量360度渲染的遮挡表面。</li>
</ul>

<h3>Title: EZ-Sort: Efficient Pairwise Comparison via Zero-Shot CLIP-Based Pre-Ordering and Human-in-the-Loop Sorting</h3>
<ul>
<li><strong>Authors: </strong>Yujin Park, Haejun Chung, Ikbeom Jang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21550">https://arxiv.org/abs/2508.21550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21550">https://arxiv.org/pdf/2508.21550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21550]] EZ-Sort: Efficient Pairwise Comparison via Zero-Shot CLIP-Based Pre-Ordering and Human-in-the-Loop Sorting(https://arxiv.org/abs/2508.21550)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Pairwise comparison is often favored over absolute rating or ordinal classification in subjective or difficult annotation tasks due to its improved reliability. However, exhaustive comparisons require a massive number of annotations (O(n^2)). Recent work has greatly reduced the annotation burden (O(n log n)) by actively sampling pairwise comparisons using a sorting algorithm. We further improve annotation efficiency by (1) roughly pre-ordering items using the Contrastive Language-Image Pre-training (CLIP) model hierarchically without training, and (2) replacing easy, obvious human comparisons with automated comparisons. The proposed EZ-Sort first produces a CLIP-based zero-shot pre-ordering, then initializes bucket-aware Elo scores, and finally runs an uncertainty-guided human-in-the-loop MergeSort. Validation was conducted using various datasets: face-age estimation (FGNET), historical image chronology (DHCI), and retinal image quality assessment (EyePACS). It showed that EZ-Sort reduced human annotation cost by 90.5% compared to exhaustive pairwise comparisons and by 19.8% compared to prior work (when n = 100), while improving or maintaining inter-rater reliability. These results demonstrate that combining CLIP-based priors with uncertainty-aware sampling yields an efficient and scalable solution for pairwise ranking.</li>
<li><strong>摘要：</strong>由于其可靠性提高，成对比较通常比主观或困难的注释任务中的绝对评级或序数分类受到青睐。但是，详尽的比较需要大量注释（O（n^2））。最近的工作大大减轻了注释负担（O（n log n）），通过使用分类算法积极采样成对比较。我们通过（1）使用对比度的语言图像预训练（剪辑）模型在没有培训的情况下进行大致预购项目进一步提高注释效率，（2）用自动比较替换简单，明显的人类比较。提出的EZ-SORT首先产生基于夹子的零击预订，然后初始化桶形感知的ELO分数，并最终运行不确定性引导的人类在环境中。使用各种数据集进行了验证：面部年龄估计（FGNET），历史图像年表（DHCI）和视网膜图像质量评估（EYEPACS）。它表明，与详尽的成对比较相比，EZ-SORT的成本降低了90.5％，与先前的工作相比（n = 100时），同时提高或维持律师间的可靠性，与详尽的成对比较相比，人类注释的成本降低了19.8％。这些结果表明，将基于夹子的先验与不确定性感知的采样相结合可以为成对排名提供有效且可扩展的解决方案。</li>
</ul>

<h3>Title: Temporal Flow Matching for Learning Spatio-Temporal Trajectories in 4D Longitudinal Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Nico Albert Disch, Yannick Kirchhoff, Robin Peretzke, Maximilian Rokuss, Saikat Roy, Constantin Ulrich, David Zimmerer, Klaus Maier-Hein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21580">https://arxiv.org/abs/2508.21580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21580">https://arxiv.org/pdf/2508.21580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21580]] Temporal Flow Matching for Learning Spatio-Temporal Trajectories in 4D Longitudinal Medical Imaging(https://arxiv.org/abs/2508.21580)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Understanding temporal dynamics in medical imaging is crucial for applications such as disease progression modeling, treatment planning and anatomical development tracking. However, most deep learning methods either consider only single temporal contexts, or focus on tasks like classification or regression, limiting their ability for fine-grained spatial predictions. While some approaches have been explored, they are often limited to single timepoints, specific diseases or have other technical restrictions. To address this fundamental gap, we introduce Temporal Flow Matching (TFM), a unified generative trajectory method that (i) aims to learn the underlying temporal distribution, (ii) by design can fall back to a nearest image predictor, i.e. predicting the last context image (LCI), as a special case, and (iii) supports $3D$ volumes, multiple prior scans, and irregular sampling. Extensive benchmarks on three public longitudinal datasets show that TFM consistently surpasses spatio-temporal methods from natural imaging, establishing a new state-of-the-art and robust baseline for $4D$ medical image prediction.</li>
<li><strong>摘要：</strong>了解医学成像中的时间动态对于诸如疾病进展建模，治疗计划和解剖学发展跟踪等应用至关重要。但是，大多数深度学习方法要么仅考虑单个时间上下文，要么专注于分类或回归等任务，从而限制了它们进行细粒度空间预测的能力。尽管已经探索了某些方法，但它们通常仅限于单个时间点，特定疾病或具有其他技术限制。 To address this fundamental gap, we introduce Temporal Flow Matching (TFM), a unified generative trajectory method that (i) aims to learn the underlying temporal distribution, (ii) by design can fall back to a nearest image predictor, i.e. predicting the last context image (LCI), as a special case, and (iii) supports $3D$ volumes, multiple prior scans, and irregular sampling.三个公共纵向数据集的广泛基准测试表明，TFM始终超过天然成像中的时空方法，建立了新的最先进和鲁棒的基线，以$ 4D $ $ MEDICAL IMAGE PRIVITATY。</li>
</ul>

<h3>Title: Introduction to the Analysis of Probabilistic Decision-Making Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Agustinus Kristiadi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21620">https://arxiv.org/abs/2508.21620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21620">https://arxiv.org/pdf/2508.21620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21620]] Introduction to the Analysis of Probabilistic Decision-Making Algorithms(https://arxiv.org/abs/2508.21620)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Decision theories offer principled methods for making choices under various types of uncertainty. Algorithms that implement these theories have been successfully applied to a wide range of real-world problems, including materials and drug discovery. Indeed, they are desirable since they can adaptively gather information to make better decisions in the future, resulting in data-efficient workflows. In scientific discovery, where experiments are costly, these algorithms can thus significantly reduce the cost of experimentation. Theoretical analyses of these algorithms are crucial for understanding their behavior and providing valuable insights for developing next-generation algorithms. However, theoretical analyses in the literature are often inaccessible to non-experts. This monograph aims to provide an accessible, self-contained introduction to the theoretical analysis of commonly used probabilistic decision-making algorithms, including bandit algorithms, Bayesian optimization, and tree search algorithms. Only basic knowledge of probability theory and statistics, along with some elementary knowledge about Gaussian processes, is assumed.</li>
<li><strong>摘要：</strong>决策理论提供了在各种不确定性下做出选择的原则方法。实施这些理论的算法已成功应用于各种现实世界中的问题，包括材料和药物发现。确实，它们是理想的选择，因为它们可以自适应地收集信息以在将来做出更好的决策，从而导致数据有效的工作流程。在科学发现中，在实验昂贵的情况下，这些算法可以显着降低实验成本。这些算法的理论分析对于理解其行为并为开发下一代算法提供宝贵的见解至关重要。但是，文献中的理论分析通常是非专家无法访问的。该专着旨在提供对常用概率决策算法的理论分析的易于访问，独立的介绍，包括强盗算法，贝叶斯优化和树搜索算法。只有对概率理论和统计的基本知识，以及有关高斯过程的一些基本知识。</li>
</ul>

<h3>Title: Unfolding Framework with Complex-Valued Deformable Attention for High-Quality Computer-Generated Hologram Generation</h3>
<ul>
<li><strong>Authors: </strong>Haomiao Zhang, Zhangyuan Li, Yanling Piao, Zhi Li, Xiaodong Wang, Miao Cao, Xiongfei Su, Qiang Song, Xin Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21657">https://arxiv.org/abs/2508.21657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21657">https://arxiv.org/pdf/2508.21657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21657]] Unfolding Framework with Complex-Valued Deformable Attention for High-Quality Computer-Generated Hologram Generation(https://arxiv.org/abs/2508.21657)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Computer-generated holography (CGH) has gained wide attention with deep learning-based algorithms. However, due to its nonlinear and ill-posed nature, challenges remain in achieving accurate and stable reconstruction. Specifically, ($i$) the widely used end-to-end networks treat the reconstruction model as a black box, ignoring underlying physical relationships, which reduces interpretability and flexibility. ($ii$) CNN-based CGH algorithms have limited receptive fields, hindering their ability to capture long-range dependencies and global context. ($iii$) Angular spectrum method (ASM)-based models are constrained to finite this http URL this paper, we propose a Deep Unfolding Network (DUN) that decomposes gradient descent into two modules: an adaptive bandwidth-preserving model (ABPM) and a phase-domain complex-valued denoiser (PCD), providing more flexibility. ABPM allows for wider working distances compared to ASM-based methods. At the same time, PCD leverages its complex-valued deformable self-attention module to capture global features and enhance performance, achieving a PSNR over 35 dB. Experiments on simulated and real data show state-of-the-art results.</li>
<li><strong>摘要：</strong>计算机生成的全息图（CGH）通过基于深度学习的算法引起了广泛的关注。但是，由于其非线性和不足的性质，在实现准确稳定的重建方面仍然存在挑战。具体而言，（$ i $）广泛使用的端到端网络将重建模型视为黑匣子，忽略了基本的物理关系，从而降低了解释性和灵活性。 （$ ii $）基于CNN的CGH算法的接受领域有限，阻碍了其捕获长期依赖性和全球环境的能力。 （$ iii $）基于角度的模型（基于ASM）的模型被限制在本文中有限，我们提出了一个深层展开的网络（DUN），将梯度下降分解为两个模块：一个自适应的bandwidth bandwidth playthth-twidth Presserving（ABPM）和相位符号复杂的复杂型号deNoiseriseriancitivition（PCD），PCD），PCD），PCD）。与基于ASM的方法相比，ABPM允许更广泛的工作距离。同时，PCD利用其复杂值可变形的自我发场模块来捕获全局特征并提高性能，从而达到35 dB的PSNR。模拟和真实数据的实验显示了最新结果。</li>
</ul>

<h3>Title: Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping</h3>
<ul>
<li><strong>Authors: </strong>Fatih Erdoğan, Merve Rabia Barın, Fatma Güney</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21689">https://arxiv.org/abs/2508.21689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21689">https://arxiv.org/pdf/2508.21689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21689]] Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping(https://arxiv.org/abs/2508.21689)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Constructing high-definition (HD) maps from sensory input requires accurately mapping the road elements in image space to the Bird's Eye View (BEV) space. The precision of this mapping directly impacts the quality of the final vectorized HD map. Existing HD mapping approaches outsource the projection to standard mapping techniques, such as attention-based ones. However, these methods struggle with accuracy due to generalization problems, often hallucinating non-existent road elements. Our key idea is to start with a geometric mapping based on camera parameters and adapt it to the scene to extract relevant map information from camera images. To implement this, we propose a novel probabilistic projection mechanism with confidence scores to (i) refine the mapping to better align with the scene and (ii) filter out irrelevant elements that should not influence HD map generation. In addition, we improve temporal processing by using confidence scores to selectively accumulate reliable information over time. Experiments on new splits of the nuScenes and Argoverse2 datasets demonstrate improved performance over state-of-the-art approaches, indicating better generalization. The improvements are particularly pronounced on nuScenes and in the challenging long perception range. Our code and model checkpoints are available at this https URL .</li>
<li><strong>摘要：</strong>从感觉输入中构建高清（HD）图需要准确地将图像空间中的道路元素映射到鸟类视图（BEV）空间。该映射的精度直接影响最终矢量化高清图的质量。现有的高清映射方法将投影外包给标准映射技术，例如基于注意力的映射技术。但是，由于概括问题，这些方法在准确性上遇到了困难，通常会幻觉不存在不存在的道路元素。我们的关键想法是从基于相机参数的几何映射开始，然后将其调整到场景中，以从相机图像中提取相关的地图信息。为了实现这一点，我们提出了一种具有置信度得分的新型概率投影机制，以（i）完善映射以更好地与场景保持一致，并过滤出不应影响HD MAP生成的无关元素。此外，我们通过使用置信度得分来选择性地积累可靠信息，随着时间的推移，我们可以改善时间处理。对Nuscenes和Argoverse2数据集的新拆分实验表明，对最新方法的性能提高，表明更好的概括。这些改进在Nuscenes和充满挑战的长期感知范围内尤为明显。我们的代码和模型检查点可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA</h3>
<ul>
<li><strong>Authors: </strong>Alvaro Patricio, Atabak Dehban, Rodrigo Ventura</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21712">https://arxiv.org/abs/2508.21712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21712">https://arxiv.org/pdf/2508.21712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21712]] FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA(https://arxiv.org/abs/2508.21712)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion-based generative models have demonstrated significant potential in augmenting scarce datasets for object detection tasks. Nevertheless, most recent models rely on resource-intensive full fine-tuning of large-scale diffusion models, requiring enterprise-grade GPUs (e.g., NVIDIA V100) and thousands of synthetic images. To address these limitations, we propose Flux LoRA Augmentation (FLORA), a lightweight synthetic data generation pipeline. Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned exclusively through Low-Rank Adaptation (LoRA). This dramatically reduces computational requirements, enabling synthetic dataset generation with a consumer-grade GPU (e.g., NVIDIA RTX 4090). We empirically evaluate our approach on seven diverse object detection datasets. Our results demonstrate that training object detectors with just 500 synthetic images generated by our approach yields superior detection performance compared to models trained on 5000 synthetic images from the ODGEN baseline, achieving improvements of up to 21.3% in mAP@.50:.95. This work demonstrates that it is possible to surpass state-of-the-art performance with far greater efficiency, as FLORA achieves superior results using only 10% of the data and a fraction of the computational cost. This work demonstrates that a quality and efficiency-focused approach is more effective than brute-force generation, making advanced synthetic data creation more practical and accessible for real-world scenarios.</li>
<li><strong>摘要：</strong>基于扩散的生成模型的最新进展表明，在增强对象检测任务的稀缺数据集方面具有巨大的潜力。然而，最近的模型依赖于资源密集型大规模扩散模型的全面微调，需要企业级GPU（例如NVIDIA V100）和数千个合成图像。为了解决这些局限性，我们提出了轻量级合成数据生成管道的Flux Lora增强（Flora）。我们的方法使用Flux 1.1 Dev扩散模型，该模型仅通过低级适应（LORA）进行微调。这大大降低了计算要求，从而可以使用消费级GPU（例如NVIDIA RTX 4090）生成合成数据集。我们在七个不同的对象检测数据集上经验评估了我们的方法。我们的结果表明，与在ODGEN基线上对5000个合成图像训练的模型相比，我们的方法生成的训练对象探测器只能产生卓越的检测性能，从而在MAP@.50：.95中获得了高达21.3％的提高。这项工作表明，有可能以更高的效率超越最先进的性能，因为植物群仅使用10％的数据和一小部分计算成本取得了卓越的结果。这项工作表明，以质量和效率为中心的方法比蛮力的生成更有效，这使得先进的合成数据创建更实用，并且在现实情况下更可访问。</li>
</ul>

<h3>Title: CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>João Valente, Atabak Dehban, Rodrigo Ventura</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21732">https://arxiv.org/abs/2508.21732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21732">https://arxiv.org/pdf/2508.21732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21732]] CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models(https://arxiv.org/abs/2508.21732)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities across various multimodal tasks. They continue, however, to struggle with trivial scenarios such as reading values from Digital Measurement Devices (DMDs), particularly in real-world conditions involving clutter, occlusions, extreme viewpoints, and motion blur; common in head-mounted cameras and Augmented Reality (AR) applications. Motivated by these limitations, this work introduces CAD2DMD-SET, a synthetic data generation tool designed to support visual question answering (VQA) tasks involving DMDs. By leveraging 3D CAD models, advanced rendering, and high-fidelity image composition, our tool produces diverse, VQA-labelled synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present DMDBench, a curated validation set of 1,000 annotated real-world images designed to evaluate model performance under practical constraints. Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein Similarity (ANLS) and further fine-tuning LoRA's of these models with CAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL showcasing a score increase of 200% without degrading on other tasks. This demonstrates that the CAD2DMD-SET training dataset substantially improves the robustness and performance of LVLMs when operating under the previously stated challenging conditions. The CAD2DMD-SET tool is expected to be released as open-source once the final version of this manuscript is prepared, allowing the community to add different measurement devices and generate their own datasets.</li>
<li><strong>摘要：</strong>大型视觉模型（LVLM）的最新进展表现出了各种多模式任务的令人印象深刻的能力。但是，他们继续在琐碎的情况下挣扎，例如数字测量设备（DMD）的阅读值，特别是在涉及混乱，遮挡，极端观点和运动模糊的现实情况下；在头部安装相机和增强现实（AR）应用中常见。在这些局限性的推动下，这项工作引入了CAD2DMD-SET，这是一种合成数据生成工具，旨在支持涉及DMD的视觉问题答案（VQA）任务。通过利用3D CAD模型，高级渲染和高保真图像组成，我们的工具可生产出适用于微调LVLMS的VQA标记的合成DMD数据集。此外，我们提出了DMDBench，这是一个精心策划的验证集，该集由1,000个注释的现实世界图像，旨在在实际约束下评估模型性能。使用平均归一化Levenshtein的相似性（ANLS）对三个最先进的LVLM进行基准测试，并通过CAD2DMD-SET生成的数据集对这些模型进行了进一步的微调Lora，从而实现了实质性改进，InternVL显示出200％的分数增加了200％，而不会降低其他任务。这表明CAD2DMD-SET训练数据集在以前规定的挑战性条件下运行时，可以显着提高LVLM的鲁棒性和性能。一旦准备了本手稿的最终版本，CAD2DMD-SET工具将作为开源版本发布，从而使社区可以添加不同的测量设备并生成自己的数据集。</li>
</ul>

<h3>Title: Achieving Hilbert-Schmidt Independence Under Rényi Differential Privacy for Fair and Private Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Tobias Hyrup, Emmanouil Panagiotou, Arjun Roy, Arthur Zimek, Eirini Ntoutsi, Peter Schneider-Kamp</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21815">https://arxiv.org/abs/2508.21815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21815">https://arxiv.org/pdf/2508.21815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21815]] Achieving Hilbert-Schmidt Independence Under Rényi Differential Privacy for Fair and Private Data Generation(https://arxiv.org/abs/2508.21815)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As privacy regulations such as the GDPR and HIPAA and responsibility frameworks for artificial intelligence such as the AI Act gain traction, the ethical and responsible use of real-world data faces increasing constraints. Synthetic data generation has emerged as a promising solution to risk-aware data sharing and model development, particularly for tabular datasets that are foundational to sensitive domains such as healthcare. To address both privacy and fairness concerns in this setting, we propose FLIP (Fair Latent Intervention under Privacy guarantees), a transformer-based variational autoencoder augmented with latent diffusion to generate heterogeneous tabular data. Unlike the typical setup in fairness-aware data generation, we assume a task-agnostic setup, not reliant on a fixed, defined downstream task, thus offering broader applicability. To ensure privacy, FLIP employs Rényi differential privacy (RDP) constraints during training and addresses fairness in the input space with RDP-compatible balanced sampling that accounts for group-specific noise levels across multiple sampling rates. In the latent space, we promote fairness by aligning neuron activation patterns across protected groups using Centered Kernel Alignment (CKA), a similarity measure extending the Hilbert-Schmidt Independence Criterion (HSIC). This alignment encourages statistical independence between latent representations and the protected feature. Empirical results demonstrate that FLIP effectively provides significant fairness improvements for task-agnostic fairness and across diverse downstream tasks under differential privacy constraints.</li>
<li><strong>摘要：</strong>作为GDPR和HIPAA之类的隐私法规以及人工智能的责任框架，例如AI ACT获得吸引力，对现实世界数据的道德和负责使用构成了越来越多的限制。合成数据的生成已成为一种有希望的方法来了解风险感知数据共享和模型开发，尤其是对于诸如医疗保健等敏感领域的基础的表格数据集。为了解决这种环境中的隐私和公平关注，我们提出了翻转（在隐私保证下进行公平的潜在干预），这是一种基于变压器的变异自动编码器增强的，并具有潜在的扩散，以生成异质性表格数据。与公平感知数据生成中的典型设置不同，我们假设任务不合时宜的设置不依赖于固定的，定义的下游任务，从而提供了更广泛的适用性。为了确保隐私，Flip在培训过程中采用Rényi差异隐私（RDP）约束，并通过与RDP兼容的平衡采样来解决输入空间中的公平性，这些抽样构成了多个采样率的特定组噪声水平。在潜在空间中，我们通过使用中心内核比对（CKA）在受保护组之间对齐神经元激活模式来促进公平，这是一种相似性测量，延伸了Hilbert-Schmidt独立标准（HSIC）。这种一致性鼓励潜在表示和受保护特征之间的统计独立性。经验结果表明，Flip有效地为任务不合时宜的公平和跨不同隐私限制下的各种下游任务提供了显着的公平改善。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
