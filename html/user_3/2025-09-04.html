<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-04</h1>
<h3>Title: Beyond Synthetic Augmentation: Group-Aware Threshold Calibration for Robust Balanced Accuracy in Imbalanced Learning</h3>
<ul>
<li><strong>Authors: </strong>Hunter Gittlin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02592">https://arxiv.org/abs/2509.02592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02592">https://arxiv.org/pdf/2509.02592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02592]] Beyond Synthetic Augmentation: Group-Aware Threshold Calibration for Robust Balanced Accuracy in Imbalanced Learning(https://arxiv.org/abs/2509.02592)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Class imbalance remains a fundamental challenge in machine learning, with traditional solutions often creating as many problems as they solve. We demonstrate that group-aware threshold calibration--setting different decision thresholds for different demographic groups--provides superior robustness compared to synthetic data generation methods. Through extensive experiments, we show that group-specific thresholds achieve 1.5-4% higher balanced accuracy than SMOTE and CT-GAN augmented models while improving worst-group balanced accuracy. Unlike single-threshold approaches that apply one cutoff across all groups, our group-aware method optimizes the Pareto frontier between balanced accuracy and worst-group balanced accuracy, enabling fine-grained control over group-level performance. Critically, we find that applying group thresholds to synthetically augmented data yields minimal additional benefit, suggesting these approaches are fundamentally redundant. Our results span seven model families including linear, tree-based, instance-based, and boosting methods, confirming that group-aware threshold calibration offers a simpler, more interpretable, and more effective solution to class imbalance.</li>
<li><strong>摘要：</strong>阶级失衡仍然是机器学习的基本挑战，传统解决方案通常会造成尽可能多的问题。我们证明了群体感知的阈值校准 - 对不同人群组设定不同的决策阈值 - 与合成数据生成方法相比，具有出色的鲁棒性。通过广泛的实验，我们表明特定于小组的阈值比Smote和CT-GAN增强模型提高了1.5-4％的平衡精度，同时提高了最差的组平衡精度。与在所有组中应用一个临界值的单阈值方法不同，我们的群体感知方法优化了平衡精度和最差的组平衡精度之间的帕累托前沿，从而可以对小组级别的性能进行细粒度的控制。至关重要的是，我们发现将组阈值应用于合成增强的数据可产生最小的额外好处，这表明这些方法从根本上是多余的。我们的结果涵盖了七个模型系列，包括线性，基于树的，基于实例的和提升方法，证实了群体感知的阈值校准提供了更简单，更容易解释，更有效的阶级失衡解决方案。</li>
</ul>

<h3>Title: Improving Generative Methods for Causal Evaluation via Simulation-Based Inference</h3>
<ul>
<li><strong>Authors: </strong>Pracheta Amaranath, Vinitra Muralikrishnan, Amit Sharma, David D. Jensen</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02892">https://arxiv.org/abs/2509.02892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02892">https://arxiv.org/pdf/2509.02892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02892]] Improving Generative Methods for Causal Evaluation via Simulation-Based Inference(https://arxiv.org/abs/2509.02892)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generating synthetic datasets that accurately reflect real-world observational data is critical for evaluating causal estimators, but remains a challenging task. Existing generative methods offer a solution by producing synthetic datasets anchored in the observed data (source data) while allowing variation in key parameters such as the treatment effect and amount of confounding bias. However, existing methods typically require users to provide point estimates of such parameters (rather than distributions) and fixed estimates (rather than estimates that can be improved with reference to the source data). This denies users the ability to express uncertainty over parameter values and removes the potential for posterior inference, potentially leading to unreliable estimator comparisons. We introduce simulation-based inference for causal evaluation (SBICE), a framework that models generative parameters as uncertain and infers their posterior distribution given a source dataset. Leveraging techniques in simulation-based inference, SBICE identifies parameter configurations that produce synthetic datasets closely aligned with the source data distribution. Empirical results demonstrate that SBICE improves the reliability of estimator evaluations by generating more realistic datasets, which supports a robust and data-consistent approach to causal benchmarking under uncertainty.</li>
<li><strong>摘要：</strong>生成准确反映现实世界观察数据的合成数据集对于评估因果估计器至关重要，但仍然是一项具有挑战性的任务。现有的生成方法通过产生锚定在观察到的数据（源数据）的合成数据集来提供解决方案，同时允许关键参数的变化，例如治疗效果和混杂偏差的数量。但是，现有方法通常要求用户提供此类参数（而不是分布）和固定估计的点估计值（而不是可以参考源数据进行改进的估计值）。这否认用户能够表达对参数值的不确定性并消除后推理的潜力，这可能会导致不可靠的估计器比较。我们介绍了基于仿真的因果评估（SBICE）的推断，该框架将生成参数建模为不确定并给定源数据集的后验分布。 SBICE在基于仿真的推理中利用技术确定了与源数据分布紧密对齐的合成数据集的参数配置。经验结果表明，SBICE通过生成更真实的数据集来提高估计器评估的可靠性，该数据集支持在不确定性下的因果基准测试的强大和数据一致的方法。</li>
</ul>

<h3>Title: PercepTwin: Modeling High-Fidelity Digital Twins for Sim2Real LiDAR-based Perception for Intelligent Transportation Systems</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Shahbaz, Shaurya Agarwal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02903">https://arxiv.org/abs/2509.02903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02903">https://arxiv.org/pdf/2509.02903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02903]] PercepTwin: Modeling High-Fidelity Digital Twins for Sim2Real LiDAR-based Perception for Intelligent Transportation Systems(https://arxiv.org/abs/2509.02903)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>LiDAR-based perception in intelligent transportation systems (ITS), for tasks such as object detection, tracking, and semantic and instance segmentation, is predominantly solved by deep neural network models which often require large-scale labeled datasets during training to achieve generalization. However, creating these datasets is costly. time consuming and require human labor before the datasets are ready for training models. This hinders scalability of the LiDAR-based perception systems in ITS. Sim2Real learning offers scalable alternative, however, its effectiveness is dependent on the fidelity of the source simulation(s) to real-world, in terms of environment structure, actor dynamics, and sensor emulations. In response, this paper introduces a rigorous and reproducible methodology for creating large-scale, high-quality synthetic datasets using High-Fidelity Digital Twins (HiFi DTs). The proposed workflow outlines the steps, tools, and best practices for digitally replicating real-world environments, encompassing static geometry modeling, road infrastructure replication, and dynamic traffic scenario generation. Leveraging open-source and readily available resources such as satellite imagery and OpenStreetMap data, alongside specific sensor configurations, this paper provides practical, detailed guidance for constructing robust synthetic environments. These environments subsequently facilitate scalable, cost-effective, and diverse dataset generation, forming a reliable foundation for robust Sim2Real learning.</li>
<li><strong>摘要：</strong>基于激光雷达的智能运输系统（ITS）的感知，用于诸如对象检测，跟踪以及语义和实例细分之类的任务，主要由深层神经网络模型解决，这些模型通常需要在训练过程中进行大规模标记的数据集以实现概括。但是，创建这些数据集是昂贵的。耗时，需要人工劳动，然后才能准备好进行培训模型。这阻碍了基于激光雷达的感知系统的可扩展性。 SIM2REAL学习提供了可扩展的替代方案，但是，其有效性取决于源模拟对现实世界的保真度，在环境结构，Actor动力学和传感器仿真方面。作为回应，本文介绍了一种严格且可重复的方法，用于使用高保真数字双胞胎（HIFI DTS）创建大规模，高质量的合成数据集。提议的工作流程概述了数字复制现实环境的步骤，工具和最佳实践，包括静态几何建模，道路基础架构复制以及动态的交通情况。本文利用卫星图像和OpenStreetMap数据等开源和可用的资源，以及特定的传感器配置，提供了实用的，详细的指导，用于构建可靠的合成环境。这些环境随后促进了可扩展，具有成本效益和多样化的数据集生成，为强大的SIM2REAL学习构成了可靠的基础。</li>
</ul>

<h3>Title: A Narrative Review of Clinical Decision Support Systems in Offloading Footwear for Diabetes-Related Foot Ulcers</h3>
<ul>
<li><strong>Authors: </strong>Kunal Kumar, Muhammad Ashad Kabir, Luke Donnan, Sayed Ahmed</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02923">https://arxiv.org/abs/2509.02923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02923">https://arxiv.org/pdf/2509.02923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02923]] A Narrative Review of Clinical Decision Support Systems in Offloading Footwear for Diabetes-Related Foot Ulcers(https://arxiv.org/abs/2509.02923)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Offloading footwear helps prevent and treat diabetic foot ulcers (DFUs) by lowering plantar pressure (PP), yet prescription decisions remain fragmented: feature selection varies, personalization is limited, and evaluation practices differ. We performed a narrative review of 45 studies (12 guidelines/protocols, 25 knowledge-based systems, 8 machine-learning applications) published to Aug 2025. We thematically analyzed knowledge type, decision logic, evaluation methods, and enabling technologies. Guidelines emphasize PP thresholds (<=200 kPa or >=25--30\% reduction) but rarely yield actionable, feature-level outputs. Knowledge-based systems use rule- and sensor-driven logic, integrating PP monitoring, adherence tracking, and usability testing. ML work introduces predictive, optimization, and generative models with high computational accuracy but limited explainability and clinical validation. Evaluation remains fragmented: protocols prioritize biomechanical tests; knowledge-based systems assess usability/adherence; ML studies focus on technical accuracy with weak linkage to long-term outcomes. From this synthesis we propose a five-part CDSS framework: (1) a minimum viable dataset; (2) a hybrid architecture combining rules, optimization, and explainable ML; (3) structured feature-level outputs; (4) continuous validation and evaluation; and (5) integration with clinical and telehealth workflows. This framework aims to enable scalable, patient-centered CDSSs for DFU care; prioritizing interoperable datasets, explainable models, and outcome-focused evaluation will be key to clinical adoption.</li>
<li><strong>摘要：</strong>卸载鞋类有助于通过降低足底压力（PP）来预防和治疗糖尿病足溃疡（DFU），但处方决策仍然分散：特征选择各不相同，个性化受到限制和评估实践不同。我们对2025年8月的45项研究（12项指南/协议，25个基于知识的系统，8个机器学习应用程序）进行了叙述性评论。我们主题分析了知识类型，决策逻辑，评估方法和启用技术。指南强调PP阈值（<= 200 kPa或> = 25--30 \％降低），但很少产生可操作的功能级输出。基于知识的系统使用规则和传感器驱动的逻辑，集成PP监视，依从性跟踪和可用性测试。 ML工作介绍具有高计算准确性但有限的解释性和临床验证的预测，优化和生成模型。评估仍然分散：协议优先考虑生物力学测试；基于知识的系统评估可用性/依从性； ML研究集中于技术准确性，与长期结局的联系较弱。从该合成中，我们提出了一个五部分的CDSS框架：（1）最小可行的数据集； （2）结合规则，优化和可解释的ML的混合体系结构； （3）结构化特征级输出； （4）连续验证和评估； （5）与临床和远程医疗工作流程集成。该框架旨在使以患者为中心的CDSS启用DFU护理；优先考虑可互操作的数据集，可解释的模型和以结果为中心的评估将是临床采用的关键。</li>
</ul>

<h3>Title: STAR: A Fast and Robust Rigid Registration Framework for Serial Histopathological Images</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Liu, Shengwei Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02952">https://arxiv.org/abs/2509.02952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02952">https://arxiv.org/pdf/2509.02952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02952]] STAR: A Fast and Robust Rigid Registration Framework for Serial Histopathological Images(https://arxiv.org/abs/2509.02952)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Registration of serial whole-slide histopathological images (WSIs) is critical for enabling direct comparison across diverse stains and for preparing paired datasets in artificial intelligence (AI) workflows such as virtual staining and biomarker prediction. While existing methods often rely on complex deformable or deep learning approaches that are computationally intensive and difficult to reproduce, lightweight rigid frameworks-sufficient for many consecutive-section scenarios-remain underdeveloped. We introduce STAR (Serial Tissue Alignment for Rigid registration), a fast and robust open-source framework for multi-WSI alignment. STAR integrates stain-conditioned preprocessing with a hierarchical coarse-to-fine correlation strategy, adaptive kernel scaling, and built-in quality control, achieving reliable rigid registration across heterogeneous tissue types and staining protocols, including hematoxylin-eosin (H&E), special histochemical stains (e.g., PAS, PASM, Masson's), and immunohistochemical (IHC) markers (e.g., CD31, KI67). Evaluated on the ANHIR 2019 and ACROBAT 2022 datasets spanning multiple organs and scanning conditions, STAR consistently produced stable alignments within minutes per slide, demonstrating robustness to cross-stain variability and partial tissue overlap. Beyond benchmarks, we present case studies on H&E-IHC alignment, construction of multi-IHC panels, and typical failure modes, underscoring both utility and limitations. Released as an open and lightweight tool, STAR provides a reproducible baseline that lowers the barrier for clinical adoption and enables large-scale paired data preparation for next-generation computational pathology.</li>
<li><strong>摘要：</strong>串行全裂缝组织病理学图像（WSI）的注册对于实现各种污渍的直接比较以及在人工智能（AI）工作流中的配对数据集（例如虚拟染色和生物标志物预测）中至关重要。尽管现有的方法通常依赖于计算密集并且难以再现的复杂可变形或深度学习方法，但对于许多连续的段情景不发达，轻巧的刚性框架就足够了。我们介绍星星（用于刚性登记的串行组织对齐），这是一个快速，可靠的开源框架，用于多WSI对齐。 Star通过等级的粗到细节相关策略，适应性内核缩放和内置质量控制，将染色条件的预处理整合在一起，从（IHC）标记（例如CD31，KI67）。在跨越多个器官和扫描条件的Anhir 2019和Acrobat 2022数据集上进行了评估，Star在每台幻灯片内始终产生稳定的对齐，表明对交叉污渍变异性和部分组织重叠的稳健性。除了基准测试之外，我们还介绍了有关H＆E-E-IHC对齐，多IHC面板的构建以​​及典型的故障模式的案例研究，强调了效用和局限性。 STAR作为开放且轻巧的工具发行，提供了可再现的基线，可降低临床采用的障碍，并实现大规模的配对数据准备下一代计算病理学。</li>
</ul>

<h3>Title: SPENet: Self-guided Prototype Enhancement Network for Few-shot Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Chao Fan, Xibin Jia, Anqi Xiao, Hongyuan Yu, Zhenghan Yang, Dawei Yang, Hui Xu, Yan Huang, Liang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.02993">https://arxiv.org/abs/2509.02993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.02993">https://arxiv.org/pdf/2509.02993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.02993]] SPENet: Self-guided Prototype Enhancement Network for Few-shot Medical Image Segmentation(https://arxiv.org/abs/2509.02993)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Few-Shot Medical Image Segmentation (FSMIS) aims to segment novel classes of medical objects using only a few labeled images. Prototype-based methods have made significant progress in addressing FSMIS. However, they typically generate a single global prototype for the support image to match with the query image, overlooking intra-class variations. To address this issue, we propose a Self-guided Prototype Enhancement Network (SPENet). Specifically, we introduce a Multi-level Prototype Generation (MPG) module, which enables multi-granularity measurement between the support and query images by simultaneously generating a global prototype and an adaptive number of local prototypes. Additionally, we observe that not all local prototypes in the support image are beneficial for matching, especially when there are substantial discrepancies between the support and query images. To alleviate this issue, we propose a Query-guided Local Prototype Enhancement (QLPE) module, which adaptively refines support prototypes by incorporating guidance from the query image, thus mitigating the negative effects of such discrepancies. Extensive experiments on three public medical datasets demonstrate that SPENet outperforms existing state-of-the-art methods, achieving superior performance.</li>
<li><strong>摘要：</strong>几乎没有射击的医学图像分割（FSMIS）旨在仅使用几个标记的图像来细分新的医疗对象类别。基于原型的方法在解决FSMI方面取得了重大进展。但是，它们通常会生成一个单个全局原型，以供支持图像与查询图像匹配，从而俯瞰着类内部变化。为了解决这个问题，我们提出了一个自引导的原型增强网络（SPENET）。具体而言，我们引入了多级原型生成（MPG）模块，该模块可以通过同时生成全局原型和自适应数量的本地原型来实现支持和查询图像之间的多粒度测量。此外，我们观察到，支持图像中并非所有局部原型都对匹配有益，尤其是在支持图像和查询图像之间存在很大差异时。为了减轻此问题，我们提出了一个查询引导的局部原型增强（QLPE）模块，该模块可以通过纳入查询图像中的指导来适应性地完善支持原型，从而减轻此类差异的负面影响。在三个公共医疗数据集上进行的广泛实验表明，Spenet的表现要优于现有的最新方法，从而达到了卓越的性能。</li>
</ul>

<h3>Title: Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers</h3>
<ul>
<li><strong>Authors: </strong>Tzuhsuan Huang, Cheng Yu Yeo, Tsai-Ling Huang, Hong-Han Shuai, Wen-Huang Cheng, Jun-Cheng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03006">https://arxiv.org/abs/2509.03006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03006">https://arxiv.org/pdf/2509.03006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03006]] Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers(https://arxiv.org/abs/2509.03006)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent studies on deep watermarking have predominantly focused on in-processing watermarking, which integrates the watermarking process into image generation. However, post-processing watermarking, which embeds watermarks after image generation, offers more flexibility. It can be applied to outputs from any generative model (e.g. GANs, diffusion models) without needing access to the model's internal structure. It also allows users to embed unique watermarks into individual images. Therefore, this study focuses on post-processing watermarking and enhances its robustness by incorporating an ensemble attack network during training. We construct various versions of attack networks using CNN and Transformer in both spatial and frequency domains to investigate how each combination influences the robustness of the watermarking model. Our results demonstrate that combining a CNN-based attack network in the spatial domain with a Transformer-based attack network in the frequency domain yields the highest robustness in watermarking models. Extensive evaluation on the WAVES benchmark, using average bit accuracy as the metric, demonstrates that our ensemble attack network significantly enhances the robustness of baseline watermarking methods under various stress tests. In particular, for the Regeneration Attack defined in WAVES, our method improves StegaStamp by 18.743%. The code is released at:this https URL.</li>
<li><strong>摘要：</strong>关于深水标记的最新研究主要集中在加工水印上，这将水印过程整合到图像生成中。但是，后加工的水印在图像产生后嵌入水印，提供了更大的灵活性。它可以应用于任何生成模型（例如gan，扩散模型）的输出，而无需访问模型的内部结构。它还允许用户将独特的水印嵌入单个图像中。因此，本研究的重点是后处理水印，并通过在训练过程中纳入合奏攻击网络来增强其鲁棒性。我们在空间和频域中都使用CNN和变压器构建各种攻击网络，以研究每种组合如何影响水印模型的鲁棒性。我们的结果表明，将空间域中的基于CNN的攻击网络与频域中的基于变压器的攻击网络相结合，在水印模型中产生了最高的鲁棒性。使用平均钻头准确度作为度量标准，对波的基准进行了广泛的评估，这表明我们的整体攻击网络在各种应力​​测试下都显着增强了基线水印方法的鲁棒性。特别是，对于在波浪中定义的再生攻击，我们的方法将Stegastamp提高了18.743％。该代码在以下位置发布：此HTTPS URL。</li>
</ul>

<h3>Title: Lesion-Aware Visual-Language Fusion for Automated Image Captioning of Ulcerative Colitis Endoscopic Examinations</h3>
<ul>
<li><strong>Authors: </strong>Alexis Ivan Lopez Escamilla, Gilberto Ochoa, Sharib Al</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03011">https://arxiv.org/abs/2509.03011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03011">https://arxiv.org/pdf/2509.03011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03011]] Lesion-Aware Visual-Language Fusion for Automated Image Captioning of Ulcerative Colitis Endoscopic Examinations(https://arxiv.org/abs/2509.03011)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present a lesion-aware image captioning framework for ulcerative colitis (UC). The model integrates ResNet embeddings, Grad-CAM heatmaps, and CBAM-enhanced attention with a T5 decoder. Clinical metadata (MES score 0-3, vascular pattern, bleeding, erythema, friability, ulceration) is injected as natural-language prompts to guide caption generation. The system produces structured, interpretable descriptions aligned with clinical practice and provides MES classification and lesion tags. Compared with baselines, our approach improves caption quality and MES classification accuracy, supporting reliable endoscopic reporting.</li>
<li><strong>摘要：</strong>我们提出了溃疡性结肠炎（UC）的病变感知图像标题框架。该模型通过T5解码器集成了Resnet嵌入，Grad-CAM热图和CBAM增强注意力。临床元数据（MES得分为0-3，血管模式，出血，红斑，易碎性，溃疡）被注入自然语言提示以指导字幕产生。该系统产生结构化的，可解释的描述，与临床实践一致，并提供MES分类和病变标签。与基线相比，我们的方法提高了标题质量和MES分类精度，从而支持可靠的内窥镜报告。</li>
</ul>

<h3>Title: Unveiling the Response of Large Vision-Language Models to Visually Absent Tokens</h3>
<ul>
<li><strong>Authors: </strong>Sohee Kim, Soohyun Ryu, Joonhyung Park, Eunho Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03025">https://arxiv.org/abs/2509.03025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03025">https://arxiv.org/pdf/2509.03025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03025]] Unveiling the Response of Large Vision-Language Models to Visually Absent Tokens(https://arxiv.org/abs/2509.03025)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) generate contextually relevant responses by jointly interpreting visual and textual inputs. However, our finding reveals they often mistakenly perceive text inputs lacking visual evidence as being part of the image, leading to erroneous responses. In light of this finding, we probe whether LVLMs possess an internal capability to determine if textual concepts are grounded in the image, and discover a specific subset of Feed-Forward Network (FFN) neurons, termed Visual Absence-aware (VA) neurons, that consistently signal the visual absence through a distinctive activation pattern. Leveraging these patterns, we develop a detection module that systematically classifies whether an input token is visually grounded. Guided by its prediction, we propose a method to refine the outputs by reinterpreting question prompts or replacing the detected absent tokens during generation. Extensive experiments show that our method effectively mitigates the models' tendency to falsely presume the visual presence of text input and its generality across various LVLMs.</li>
<li><strong>摘要：</strong>大型视觉模型（LVLM）通过共同解释视觉和文本输入来产生上下文相关的响应。但是，我们的发现表明，他们经常错误地认为缺乏视觉证据的文本输入是图像的一部分，从而导致错误的响应。鉴于这一发现，我们探测了LVLM是否具有内部能力来确定文本概念是否基于图像中的基础，并发现特定的馈送前馈网络（FFN）神经元的子集，称为视觉缺失感知（VA）神经元，通过独特的激活模式向视觉缺失始终如一地信号。利用这些模式，我们开发了一个检测模块，该模块系统地分类了是否在视觉上进行输入令牌。在其预测的指导下，我们提出了一种通过重新解释问题提示或替换生成过程中检测到的缺失令牌的方法来完善输出的方法。广泛的实验表明，我们的方法有效地减轻了模型错误地假定文本输入的视觉存在及其在各种LVLM中的通用性的趋势。</li>
</ul>

<h3>Title: DCDB: Dynamic Conditional Dual Diffusion Bridge for Ill-posed Multi-Tasks</h3>
<ul>
<li><strong>Authors: </strong>Chengjie Huang, Jiafeng Yan, Jing Li, Lu Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03044">https://arxiv.org/abs/2509.03044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03044">https://arxiv.org/pdf/2509.03044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03044]] DCDB: Dynamic Conditional Dual Diffusion Bridge for Ill-posed Multi-Tasks(https://arxiv.org/abs/2509.03044)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Conditional diffusion models have made impressive progress in the field of image processing, but the characteristics of constructing data distribution pathways make it difficult to exploit the intrinsic correlation between tasks in multi-task scenarios, which is even worse in ill-posed tasks with a lack of training data. In addition, traditional static condition control makes it difficult for networks to learn in multi-task scenarios with its dynamically evolving characteristics. To address these challenges, we propose a dynamic conditional double diffusion bridge training paradigm to build a general framework for ill-posed multi-tasks. Firstly, this paradigm decouples the diffusion and condition generation processes, avoiding the dependence of the diffusion model on supervised data in ill-posed tasks. Secondly, generated by the same noise schedule, dynamic conditions are used to gradually adjust their statistical characteristics, naturally embed time-related information, and reduce the difficulty of network learning. We analyze the learning objectives of the network under different conditional forms in the single-step denoising process and compare the changes in its attention weights in the network, demonstrating the superiority of our dynamic conditions. Taking dehazing and visible-infrared fusion as typical ill-posed multi-task scenarios, we achieve the best performance in multiple indicators on public datasets. The code has been publicly released at: this https URL.</li>
<li><strong>摘要：</strong>有条件的扩散模型在图像处理领域取得了令人印象深刻的进步，但是构建数据分布途径的特征使得在多任务场景中很难利用任务之间的固有相关性，这在缺乏训练数据的情况下，在不适合的任务中更糟。此外，传统的静态状况控制使网络很难在具有动态发展的特征的多任务场景中学习。为了应对这些挑战，我们提出了一个动态的有条件双扩散桥训练范式，以建立一个不适当的多任务的一般框架。首先，该范式将扩散和条件生成过程解开，避免扩散模型对不符合任务的监督数据的依赖性。其次，由相同的噪声时间表生成，动态条件用于逐步调整其统计特征，自然嵌入与时间相关的信息以及减少网络学习的难度。我们在单步授权过程中分析了不同条件形式的网络的学习目标，并比较了网络中其注意力权重的变化，以证明我们动态条件的优势。以典型的多任务情景为典型的幻影和可见的红外融合，我们在公共数据集中的多个指标中实现了最佳性能。该代码已公开发布，网址为：此HTTPS URL。</li>
</ul>

<h3>Title: Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers</h3>
<ul>
<li><strong>Authors: </strong>Xingyue Huang, Rishabh, Gregor Franke, Ziyi Yang, Jiamu Bai, Weijie Bai, Jinhe Bi, Zifeng Ding, Yiqun Duan, Chengyu Fan, Wendong Fan, Xin Gao, Ruohao Guo, Yuan He, Zhuangzhuang He, Xianglong Hu, Neil Johnson, Bowen Li, Fangru Lin, Siyu Lin, Tong Liu, Yunpu Ma, Hao Shen, Hao Sun, Beibei Wang, Fangyijie Wang, Hao Wang, Haoran Wang, Yang Wang, Yifeng Wang, Zhaowei Wang, Ziyang Wang, Yifan Wu, Zikai Xiao, Chengxing Xie, Fan Yang, Junxiao Yang, Qianshuo Ye, Ziyu Ye, Guangtao Zeng, Yuwen Ebony Zhang, Zeyu Zhang, Zihao Zhu, Bernard Ghanem, Philip Torr, Guohao Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03059">https://arxiv.org/abs/2509.03059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03059">https://arxiv.org/pdf/2509.03059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03059]] Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers(https://arxiv.org/abs/2509.03059)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) have shown that their reasoning capabilities can be significantly improved through Reinforcement Learning with Verifiable Reward (RLVR), particularly in domains like mathematics and programming, where ground-truth correctness can be automatically evaluated. However, extending this success to other reasoning-intensive domains remains challenging due to the scarcity of high-quality, verifiable datasets and the high cost of human supervision. In this work, we introduce the Loong Project: an open-source framework for scalable synthetic data generation and verification across a diverse range of reasoning-intensive domains. The framework consists of two key components: (1) LoongBench, a curated seed dataset containing 8,729 human-vetted examples across 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired with executable code and rich metadata; and (2) LoongEnv, a modular synthetic data generation environment that supports multiple prompting strategies to produce new question-answer-code triples. Together, these components form an agent-environment loop that enables reinforcement learning, where an LLM-based agent is rewarded for generating Chain-of-Thought (CoT) solutions that align with code-executed answers. Empirically, we benchmark LoongBench on a broad suite of both open-source and proprietary LLMs to evaluate domain coverage and reveal performance bottlenecks. In addition, we conduct a comprehensive analysis of synthetic data generated by LoongEnv, examining correctness, difficulty, and diversity. Code and documentation are available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的最新进展表明，通过可验证的奖励（RLVR）的加强学习可以显着提高其推理能力（尤其是在数学和编程等领域，可以自动评估基础真实性。但是，将这一成功扩展到其他推理密集型领域，由于高质量，可验证的数据集和人类监督的高成本，因此仍然具有挑战性。在这项工作中，我们介绍了Loong Project：一个开源框架，用于可扩展的合成数据生成和验证各种推理密集型域的验证。该框架由两个关键组成部分组成：（1）loongbench，一个策划的种子数据集，其中包含12个域中的8,729个人vet的示例（例如，高级数学，化学，化学，逻辑），每个都与可执行的代码和丰富的元数据配对； （2）loongenv，一个模块化的合成数据生成环境，支持多种提示策略生成新的问题 - 答案代码三元组。这些组件共同构成了一个代理环境循环，该环境可以实现增强学习，在该环境中，基于LLM的代理因生成与代码执行的答案相符的链条（COT）解决方案而获得了奖励。从经验上讲，我们在一套开源和专有LLM的广泛套件上进行基准测试，以评估域覆盖范围并揭示性能瓶颈。此外，我们对Loongenv生成的合成数据进行了全面分析，研究了正确性，难度和多样性。代码和文档可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: High Cursive Complex Character Recognition using GAN External Classifier</h3>
<ul>
<li><strong>Authors: </strong>S M Rafiuddin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03062">https://arxiv.org/abs/2509.03062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03062">https://arxiv.org/pdf/2509.03062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03062]] High Cursive Complex Character Recognition using GAN External Classifier(https://arxiv.org/abs/2509.03062)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Handwritten characters can be trickier to classify due to their complex and cursive nature compared to simple and non-cursive characters. We present an external classifier along with a Generative Adversarial Network that can classify highly cursive and complex characters. The generator network produces fake handwritten character images, which are then used to augment the training data after adding adversarially perturbed noise and achieving a confidence score above a threshold with the discriminator network. The results show that the accuracy of convolutional neural networks decreases as character complexity increases, but our proposed model, ADA-GAN, remains more robust and effective for both cursive and complex characters.</li>
<li><strong>摘要：</strong>与简单和非象征性字符相比，手写字符由于其复杂和草书的性质而进行分类可能更棘手。我们提出了一个外部分类器以及一个可以对高度草书和复杂字符进行分类的生成对抗网络。发电机网络会产生伪造的手写字符图像，然后在添加对抗性扰动的噪声并通过歧视网络的阈值高于阈值后，用于增强训练数据。结果表明，随着特征复杂性的增加，卷积神经网络的准确性会降低，但是我们提出的模型ADA-GAN对草书和复杂的特征都更加健壮和有效。</li>
</ul>

<h3>Title: TRELLIS-Enhanced Surface Features for Comprehensive Intracranial Aneurysm Analysis</h3>
<ul>
<li><strong>Authors: </strong>Clément Hervé, Paul Garnier, Jonathan Viquerat, Elie Hachem</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03095">https://arxiv.org/abs/2509.03095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03095">https://arxiv.org/pdf/2509.03095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03095]] TRELLIS-Enhanced Surface Features for Comprehensive Intracranial Aneurysm Analysis(https://arxiv.org/abs/2509.03095)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Intracranial aneurysms pose a significant clinical risk yet are difficult to detect, delineate and model due to limited annotated 3D data. We propose a cross-domain feature-transfer approach that leverages the latent geometric embeddings learned by TRELLIS, a generative model trained on large-scale non-medical 3D datasets, to augment neural networks for aneurysm analysis. By replacing conventional point normals or mesh descriptors with TRELLIS surface features, we systematically enhance three downstream tasks: (i) classifying aneurysms versus healthy vessels in the Intra3D dataset, (ii) segmenting aneurysm and vessel regions on 3D meshes, and (iii) predicting time-evolving blood-flow fields using a graph neural network on the AnXplore dataset. Our experiments show that the inclusion of these features yields strong gains in accuracy, F1-score and segmentation quality over state-of-the-art baselines, and reduces simulation error by 15\%. These results illustrate the broader potential of transferring 3D representations from general-purpose generative models to specialized medical tasks.</li>
<li><strong>摘要：</strong>颅内动脉瘤构成显着的临床风险，但由于有限的注释3D数据而难以检测，划定和模型。我们提出了一种跨域特征转移方法，该方法利用了Trellis学到的潜在几何嵌入，这是一种在大型非医学3D数据集中训练的生成模型，以增强神经网络进行动脉瘤分析。通过用格子表面特征替换常规点正常或网状描述符，我们可以系统地增强三个下游任务：（i）在Intra3D数据集中对动脉瘤与健康血管进行分类，（ii）通过3D网格和（iii）启动型型a型启动ANEAIRANTER-ANTREARTION ANEURYSM和容器区域进行a neation Naire naire Naire naire naire naire naire naire naire naire naire naire naire naire naire naire naire naire naire naire naire，我们的实验表明，这些特征的包含在最先进的基线方面的准确性，F1得分和分割质量的提高，并将模拟误差降低15 \％。这些结果说明了将3D表示从通用生成模型转移到专业医疗任务的更广泛潜力。</li>
</ul>

<h3>Title: Temporally-Aware Diffusion Model for Brain Progression Modelling with Bidirectional Temporal Regularisation</h3>
<ul>
<li><strong>Authors: </strong>Mattia Litrico, Francesco Guarnera, Mario Valerio Giuffrida, Daniele Ravì, Sebastiano Battiato</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03141">https://arxiv.org/abs/2509.03141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03141">https://arxiv.org/pdf/2509.03141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03141]] Temporally-Aware Diffusion Model for Brain Progression Modelling with Bidirectional Temporal Regularisation(https://arxiv.org/abs/2509.03141)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating realistic MRIs to accurately predict future changes in the structure of brain is an invaluable tool for clinicians in assessing clinical outcomes and analysing the disease progression at the patient level. However, current existing methods present some limitations: (i) some approaches fail to explicitly capture the relationship between structural changes and time intervals, especially when trained on age-imbalanced datasets; (ii) others rely only on scan interpolation, which lack clinical utility, as they generate intermediate images between timepoints rather than future pathological progression; and (iii) most approaches rely on 2D slice-based architectures, thereby disregarding full 3D anatomical context, which is essential for accurate longitudinal predictions. We propose a 3D Temporally-Aware Diffusion Model (TADM-3D), which accurately predicts brain progression on MRI volumes. To better model the relationship between time interval and brain changes, TADM-3D uses a pre-trained Brain-Age Estimator (BAE) that guides the diffusion model in the generation of MRIs that accurately reflect the expected age difference between baseline and generated follow-up scans. Additionally, to further improve the temporal awareness of TADM-3D, we propose the Back-In-Time Regularisation (BITR), by training TADM-3D to predict bidirectionally from the baseline to follow-up (forward), as well as from the follow-up to baseline (backward). Although predicting past scans has limited clinical applications, this regularisation helps the model generate temporally more accurate scans. We train and evaluate TADM-3D on the OASIS-3 dataset, and we validate the generalisation performance on an external test set from the NACC dataset. The code will be available upon acceptance.</li>
<li><strong>摘要：</strong>生成现实的MRI来准确预测大脑结构的未来变化是临床医生评估临床结果和分析患者水平疾病进展的宝贵工具。但是，当前现有方法提出了一些局限性：（i）某些方法无法明确捕获结构变化和时间间隔之间的关系，尤其是在年龄流亡的数据集中接受培训时； （ii）其他人仅依靠缺乏临床效用的扫描插值，因为它们在时间点之间产生中间图像而不是将来的病理进展； （iii）大多数方法都依赖于基于2D的架构，从而无视完整的3D解剖环境，这对于准确的纵向预测至关重要。我们提出了一个3D时间感知的扩散模型（TADM-3D），该模型准确地预测了MRI体积上的大脑进展。为了更好地建模时间间隔和大脑变化之间的关系，TADM-3D使用预先训练的脑年龄估计量（BAE），该估计器（BAE）指导MRI的生成中的扩散模型，以准确反映基线和生成的随访扫描之间的预期年龄差异。此外，为了进一步提高TADM-3D的时间意识，我们通过训练TADM-3D训练从基线到后续（向前）以及从后续到基线到基线到基线（向后），提出了后式正则化（BITR）。尽管预测过去的扫描的临床应用有限，但此正则化有助于该模型在时间上更准确地扫描。我们在OASIS-3数据集上训练和评估TADM-3D，并验证NACC数据集外部测试集的概括性能。该代码将在接受后可用。</li>
</ul>

<h3>Title: AIVA: An AI-based Virtual Companion for Emotion-aware Interaction</h3>
<ul>
<li><strong>Authors: </strong>Chenxi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03212">https://arxiv.org/abs/2509.03212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03212">https://arxiv.org/pdf/2509.03212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03212]] AIVA: An AI-based Virtual Companion for Emotion-aware Interaction(https://arxiv.org/abs/2509.03212)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) have significantly improved natural language understanding and generation, enhancing Human-Computer Interaction (HCI). However, LLMs are limited to unimodal text processing and lack the ability to interpret emotional cues from non-verbal signals, hindering more immersive and empathetic interactions. This work explores integrating multimodal sentiment perception into LLMs to create emotion-aware agents. We propose \ours, an AI-based virtual companion that captures multimodal sentiment cues, enabling emotionally aligned and animated HCI. \ours introduces a Multimodal Sentiment Perception Network (MSPN) using a cross-modal fusion transformer and supervised contrastive learning to provide emotional cues. Additionally, we develop an emotion-aware prompt engineering strategy for generating empathetic responses and integrate a Text-to-Speech (TTS) system and animated avatar module for expressive interactions. \ours provides a framework for emotion-aware agents with applications in companion robotics, social care, mental health, and human-centered AI.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的最新进展已显着改善了自然语言的理解和产生，从而增强了人类计算机的互动（HCI）。但是，LLM仅限于单峰文本处理，并且缺乏解释非语言信号的情绪提示的能力，从而阻碍了更加沉浸式和同理心相互作用。这项工作探讨了将多模式情感感知整合到LLM中，以创建情感感知的代理。我们提出了一个基于AI的虚拟伴侣\我们的建议，它捕获了多模式的情感提示，从而使情感和动画的HCI能够。 \我们的人使用跨模式融合变压器和监督的对比度学习引入了多模式情感感知网络（MSPN），以提供情感提示。此外，我们制定了一种情感感知的及时工程策略，以产生善解人意的响应，并整合文本到语音（TTS）系统和动画化的阿凡达模块，以进行表达互动。 \我们为情感感知的代理提供了一个框架，该框架在伴侣机器人技术，社会护理，心理健康和以人为中为中心的AI中提供了应用。</li>
</ul>

<h3>Title: RTGMFF: Enhanced fMRI-based Brain Disorder Diagnosis via ROI-driven Text Generation and Multimodal Feature Fusion</h3>
<ul>
<li><strong>Authors: </strong>Junhao Jia, Yifei Sun, Yunyou Liu, Cheng Yang, Changmiao Wang, Feiwei Qin, Yong Peng, Wenwen Min</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03214">https://arxiv.org/abs/2509.03214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03214">https://arxiv.org/pdf/2509.03214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03214]] RTGMFF: Enhanced fMRI-based Brain Disorder Diagnosis via ROI-driven Text Generation and Multimodal Feature Fusion(https://arxiv.org/abs/2509.03214)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Functional magnetic resonance imaging (fMRI) is a powerful tool for probing brain function, yet reliable clinical diagnosis is hampered by low signal-to-noise ratios, inter-subject variability, and the limited frequency awareness of prevailing CNN- and Transformer-based models. Moreover, most fMRI datasets lack textual annotations that could contextualize regional activation and connectivity patterns. We introduce RTGMFF, a framework that unifies automatic ROI-level text generation with multimodal feature fusion for brain-disorder diagnosis. RTGMFF consists of three components: (i) ROI-driven fMRI text generation deterministically condenses each subject's activation, connectivity, age, and sex into reproducible text tokens; (ii) Hybrid frequency-spatial encoder fuses a hierarchical wavelet-mamba branch with a cross-scale Transformer encoder to capture frequency-domain structure alongside long-range spatial dependencies; and (iii) Adaptive semantic alignment module embeds the ROI token sequence and visual features in a shared space, using a regularized cosine-similarity loss to narrow the modality gap. Extensive experiments on the ADHD-200 and ABIDE benchmarks show that RTGMFF surpasses current methods in diagnostic accuracy, achieving notable gains in sensitivity, specificity, and area under the ROC curve. Code is available at this https URL.</li>
<li><strong>摘要：</strong>功能磁共振成像（fMRI）是探测大脑功能的强大工具，但可靠的临床诊断受到信噪比低，受试者间的可变性以及对盛行的CNN-和变形金刚基于变压器模型的有限频率意识的阻碍。此外，大多数fMRI数据集都缺乏文本注释，这些注释可能会使区域激活和连接模式进行上下文化。我们介绍了RTGMFF，这是一个框架，该框架统一了自动ROI级文本生成，并具有多模式特征融合用于脑抑制诊断的框架。 RTGMFF由三个组成部分组成：（i）ROI驱动的fMRI文本生成确定性地将每个受试者的激活，连通性，年龄和性别凝结成可复制的文本令牌； （ii）混合频率空间编码器与跨尺度变压器编码器融合了分层小波 - 马姆巴分支，以捕获频域结构与远程空间依赖关系； （iii）自适应语义比对模块将ROI令牌序列和视觉特征嵌入共享空间中，并使用正则化余弦相似性损失来缩小模态差距。在ADHD-200和ABIDE基准上进行的广泛实验表明，RTGMFF超过了诊断准确性的当前方法，在ROC曲线下的灵敏度，特异性和面积上取得了显着提高。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Unsupervised Learning based Element Resource Allocation for Reconfigurable Intelligent Surfaces in mmWave Network</h3>
<ul>
<li><strong>Authors: </strong>Pujitha Mamillapalli, Yoghitha Ramamoorthi, Abhinav Kumar, Tomoki Murakami, Tomoaki Ogawa, Yasushi Takatori</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03241">https://arxiv.org/abs/2509.03241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03241">https://arxiv.org/pdf/2509.03241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03241]] Unsupervised Learning based Element Resource Allocation for Reconfigurable Intelligent Surfaces in mmWave Network(https://arxiv.org/abs/2509.03241)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The increasing demand for high data rates and seamless connectivity in wireless systems has sparked significant interest in reconfigurable intelligent surfaces (RIS) and artificial intelligence-based wireless applications. RIS typically comprises passive reflective antenna elements that control the wireless propagation environment by adequately tuning the phase of the reflective elements. The allocation of RIS elements to multipleuser equipment (UEs) is crucial for efficiently utilizing RIS. In this work, we formulate a joint optimization problem that optimizes the RIS phase configuration and resource allocation under an $\alpha$-fair scheduling framework and propose an efficient way of allocating RIS elements. Conventional iterative optimization methods, however, suffer from exponentially increasing computational complexity as the number of RIS elements increases and also complicate the generation of training labels for supervised learning. To overcome these challenges, we propose a five-layer fully connected neural network (FNN) combined with a preprocessing technique to significantly reduce input dimensionality, lower computational complexity, and enhance scalability. The simulation results show that our proposed NN-based solution reduces computational overhead while significantly improving system throughput by 6.8% compared to existing RIS element allocation schemes. Furthermore, the proposed system achieves better performance while reducing computational complexity, making it significantly more scalable than the iterative optimization algorithms.</li>
<li><strong>摘要：</strong>无线系统中对高数据速率和无缝连通性的需求不断增长，引发了人们对可重构智能表面（RIS）和基于人工智能的无线应用程序的重大兴趣。 RI通常包括被动反射天线元件，通过充分调整反射元件的相位来控制无线传播环境。 RIS元素对多使用者设备（UES）的分配对于有效利用RI至关重要。在这项工作中，我们制定了一个联合优化问题，该问题优化了RIS相位配置和资源分配，在$ \ alpha $ -fair调度框架下，并提出了一种有效的分配RIS元素的方法。然而，随着RIS元素的数量增加，传统的迭代优化方法的计算复杂性呈指数增长，并使培训标签的产生复杂化，以进行监督学习。为了克服这些挑战，我们提出了一个五层完全连接的神经网络（FNN），并结合了预处理技术，以显着降低输入维度，降低计算复杂性并提高可扩展性。仿真结果表明，与现有的RIS元素分配方案相比，我们提出的基于NN的解决方案将计算开销降低了6.8％。此外，所提出的系统在降低计算复杂性的同时，实现了更好的性能，使其比迭代优化算法更可扩展。</li>
</ul>

<h3>Title: SynBT: High-quality Tumor Synthesis for Breast Tumor Segmentation by 3D Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Hongxu Yang, Edina Timko, Levente Lippenszky, Vanda Czipczer, Lehel Ferenczi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03267">https://arxiv.org/abs/2509.03267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03267">https://arxiv.org/pdf/2509.03267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03267]] SynBT: High-quality Tumor Synthesis for Breast Tumor Segmentation by 3D Diffusion Model(https://arxiv.org/abs/2509.03267)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Synthetic tumors in medical images offer controllable characteristics that facilitate the training of machine learning models, leading to an improved segmentation performance. However, the existing methods of tumor synthesis yield suboptimal performances when tumor occupies a large spatial volume, such as breast tumor segmentation in MRI with a large field-of-view (FOV), while commonly used tumor generation methods are based on small patches. In this paper, we propose a 3D medical diffusion model, called SynBT, to generate high-quality breast tumor (BT) in contrast-enhanced MRI images. The proposed model consists of a patch-to-volume autoencoder, which is able to compress the high-resolution MRIs into compact latent space, while preserving the resolution of volumes with large FOV. Using the obtained latent space feature vector, a mask-conditioned diffusion model is used to synthesize breast tumors within selected regions of breast tissue, resulting in realistic tumor appearances. We evaluated the proposed method for a tumor segmentation task, which demonstrated the proposed high-quality tumor synthesis method can facilitate the common segmentation models with performance improvement of 2-3% Dice Score on a large public dataset, and therefore provides benefits for tumor segmentation in MRI images.</li>
<li><strong>摘要：</strong>医学图像中的合成肿瘤提供了可控的特征，可促进机器学习模型的训练，从而改善分割性能。然而，当肿瘤占据较大的空间体积时，现有的肿瘤合成方法会产生次优性能，例如MRI中具有较大视野（FOV）的MRI分割（FOV），而常用的肿瘤产生方法基于小斑块。在本文中，我们提出了一个称为Synbt的3D医学扩散模型，以在对比增强的MRI图像中产生高质量的乳腺肿瘤（BT）。所提出的模型由贴片到体积自动编码器组成，该模型能够将高分辨率MRIS压缩到紧凑的潜在空间中，同时保留具有大型FOV的体积的分辨率。使用获得的潜在空间特征矢量，使用掩模条件的扩散模型来合成乳腺组织选定区域内的乳腺肿瘤，从而导致逼真的肿瘤出现。我们评估了针对肿瘤分割任务的提出的方法，该方法证明了所提出的高质量肿瘤合成方法可以促进常见分割模型，在大型公共数据集中的性能提高了2-3％的骰子得分，因此在MRI图像中为肿瘤分割提供了益处。</li>
</ul>

<h3>Title: Meta-Imputation Balanced (MIB): An Ensemble Approach for Handling Missing Data in Biomedical Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Azad, Zoran Bosnić, Matjaž Kukar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03316">https://arxiv.org/abs/2509.03316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03316">https://arxiv.org/pdf/2509.03316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03316]] Meta-Imputation Balanced (MIB): An Ensemble Approach for Handling Missing Data in Biomedical Machine Learning(https://arxiv.org/abs/2509.03316)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Missing data represents a fundamental challenge in machine learning applications, often reducing model performance and reliability. This problem is particularly acute in fields like bioinformatics and clinical machine learning, where datasets are frequently incomplete due to the nature of both data generation and data collection. While numerous imputation methods exist, from simple statistical techniques to advanced deep learning models, no single method consistently performs well across diverse datasets and missingness mechanisms. This paper proposes a novel Meta-Imputation approach that learns to combine the outputs of multiple base imputers to predict missing values more accurately. By training the proposed method called Meta-Imputation Balanced (MIB) on synthetically masked data with known ground truth, the system learns to predict the most suitable imputed value based on the behavior of each method. Our work highlights the potential of ensemble learning in imputation and paves the way for more robust, modular, and interpretable preprocessing pipelines in real-world machine learning systems.</li>
<li><strong>摘要：</strong>缺少的数据代表了机器学习应用程序中的基本挑战，通常会降低模型性能和可靠性。在生物信息学和临床机器学习等领域，此问题尤其严重，由于数据生成和数据收集的性质，数据集经常不完整。尽管存在许多插补方法，从简单的统计技术到先进的深度学习模型，但在不同的数据集和缺失机制中，没有任何单一的方法始终如一地表现良好。本文提出了一种新颖的元输入方法，该方法学会了结合多个基本螺旋桨的输出，以更准确地预测缺失值。通过训练具有已知地面真实的合成掩盖数据上提出的称为元输注平衡（MIB）的方法，该系统学会根据每种方法的行为来预测最合适的估算值。我们的工作突出了集合学习中的潜力，并为在现实世界机器学习系统中更健壮，模块化和可解释的预处理管道铺平了道路。</li>
</ul>

<h3>Title: InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds</h3>
<ul>
<li><strong>Authors: </strong>Yixiong Jing, Cheng Zhang, Haibing Wu, Guangming Wang, Olaf Wysocki, Brian Sheil</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03324">https://arxiv.org/abs/2509.03324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03324">https://arxiv.org/pdf/2509.03324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03324]] InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds(https://arxiv.org/abs/2509.03324)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Point clouds are widely used for infrastructure monitoring by providing geometric information, where segmentation is required for downstream tasks such as defect detection. Existing research has automated semantic segmentation of structural components, while brick-level segmentation (identifying defects such as spalling and mortar loss) has been primarily conducted from RGB images. However, acquiring high-resolution images is impractical in low-light environments like masonry tunnels. Point clouds, though robust to dim lighting, are typically unstructured, sparse, and noisy, limiting fine-grained segmentation. We present InfraDiffusion, a zero-shot framework that projects masonry point clouds into depth maps using virtual cameras and restores them by adapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific training, InfraDiffusion enhances visual clarity and geometric consistency of depth maps. Experiments on masonry bridge and tunnel point cloud datasets show significant improvements in brick-level segmentation using the Segment Anything Model (SAM), underscoring its potential for automated inspection of masonry assets. Our code and data is available at this https URL.</li>
<li><strong>摘要：</strong>点云通过提供几何信息广泛用于基础架构监视，在下游任务（例如缺陷检测）中需要进行分割。现有的研究具有对结构成分的语义分割自动化的，而砖级分割（识别诸如剥落和迫击炮损失之类的缺陷）主要是由RGB图像进行的。然而，在砌体隧道（例如砌体隧道）中，获取高分辨率图像是不切实际的。点云虽然强大至昏暗的照明，但通常是非结构化的，稀疏和嘈杂的，限制了细粒度的细分。我们提出了一个零拍的框架，该框架是使用虚拟摄像机将砌体点云投射到深度图中的零拍摄框架，并通过调整DeNoising扩散零空间模型（DDNM）来恢复它们。如果没有特定于任务的训练，则不足会增强深度图的视觉清晰度和几何一致性。使用该段的任何模型（SAM），在砖石桥和隧道点云数据集上进行的实验显示了砖级分割的显着改善，从而强调了其自动检查砌体资产的潜力。我们的代码和数据可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Equivariant Flow Matching for Symmetry-Breaking Bifurcation Problems</h3>
<ul>
<li><strong>Authors: </strong>Fleur Hendriks, Ondřej Rokoš, Martin Doškář, Marc G.D. Geers, Vlado Menkovski</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03340">https://arxiv.org/abs/2509.03340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03340">https://arxiv.org/pdf/2509.03340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03340]] Equivariant Flow Matching for Symmetry-Breaking Bifurcation Problems(https://arxiv.org/abs/2509.03340)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Bifurcation phenomena in nonlinear dynamical systems often lead to multiple coexisting stable solutions, particularly in the presence of symmetry breaking. Deterministic machine learning models struggle to capture this multiplicity, averaging over solutions and failing to represent lower-symmetry outcomes. In this work, we propose a generative framework based on flow matching to model the full probability distribution over bifurcation outcomes. Our method enables direct sampling of multiple valid solutions while preserving system symmetries through equivariant modeling. We introduce a symmetric matching strategy that aligns predicted and target outputs under group actions, allowing accurate learning in equivariant settings. We validate our approach on a range of systems, from toy models to complex physical problems such as buckling beams and the Allen-Cahn equation. Our results demonstrate that flow matching significantly outperforms non-probabilistic and variational methods in capturing multimodal distributions and symmetry-breaking bifurcations, offering a principled and scalable solution for modeling multistability in high-dimensional systems.</li>
<li><strong>摘要：</strong>非线性动力学系统中的分叉现象通常会导致多个共存的稳定溶液，尤其是在对称性破坏的情况下。确定性的机器学习模型难以捕获这种多重性，平均对解决方案而无法代表低对称性结果。在这项工作中，我们提出了一个基于流量匹配的生成框架，以模拟分叉结果上的完整概率分布。我们的方法可以直接采样多个有效解决方案，同时通过模仿建模来保留系统对称性。我们介绍了一种对称匹配策略，该策略在小组操作下与预测和目标输出保持一致，从而可以在模棱两可的设置中进行准确的学习。我们在一系列系统上验证了我们的方法，从玩具模型到复杂的物理问题，例如屈曲梁和Allen-Cahn方程。我们的结果表明，流动匹配在捕获多模式分布和对称性的分叉方面显着优于非稳态和变异方法，为在高维系统中的多稳定性建模提供了原则可扩展的解决方案。</li>
</ul>

<h3>Title: On the MIA Vulnerability Gap Between Private GANs and Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ilana Sebag, Jean-Yves Franceschi, Alain Rakotomamonjy, Alexandre Allauzen, Jamal Atif</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03341">https://arxiv.org/abs/2509.03341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03341">https://arxiv.org/pdf/2509.03341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03341]] On the MIA Vulnerability Gap Between Private GANs and Diffusion Models(https://arxiv.org/abs/2509.03341)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Adversarial Networks (GANs) and diffusion models have emerged as leading approaches for high-quality image synthesis. While both can be trained under differential privacy (DP) to protect sensitive data, their sensitivity to membership inference attacks (MIAs), a key threat to data confidentiality, remains poorly understood. In this work, we present the first unified theoretical and empirical analysis of the privacy risks faced by differentially private generative models. We begin by showing, through a stability-based analysis, that GANs exhibit fundamentally lower sensitivity to data perturbations than diffusion models, suggesting a structural advantage in resisting MIAs. We then validate this insight with a comprehensive empirical study using a standardized MIA pipeline to evaluate privacy leakage across datasets and privacy budgets. Our results consistently reveal a marked privacy robustness gap in favor of GANs, even in strong DP regimes, highlighting that model type alone can critically shape privacy leakage.</li>
<li><strong>摘要：</strong>生成对抗网络（GAN）和扩散模型已成为高质量图像合成的领先方法。尽管两者都可以在差异隐私（DP）下进行培训以保护敏感数据，但它们对成员推理攻击（MIAS）的敏感性（对数据机密性的关键威胁）仍然很少了解。在这项工作中，我们介绍了第一个统一的理论和经验分析，对不同私人生成模型面临的隐私风险。我们首先通过基于稳定的分析来显示gan对数据扰动的敏感性比扩散模型较低，这表明抗MIA具有结构性优势。然后，我们通过使用标准化的MIA管道进行全面的实证研究来验证这种见解，以评估数据集和隐私预算之间的隐私泄漏。我们的结果始终揭示出明显的隐私鲁棒性差距，即使在强大的DP制度中，仅仅强调单独的模型类型就可以严重影响隐私泄漏。</li>
</ul>

<h3>Title: epiGPTope: A machine learning-based epitope generator and classifier</h3>
<ul>
<li><strong>Authors: </strong>Natalia Flechas Manrique, Alberto Martínez, Elena López-Martínez, Luc Andrea, Román Orus, Aitor Manteca, Aitziber L. Cortajarena, Llorenç Espinosa-Portalés</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03351">https://arxiv.org/abs/2509.03351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03351">https://arxiv.org/pdf/2509.03351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03351]] epiGPTope: A machine learning-based epitope generator and classifier(https://arxiv.org/abs/2509.03351)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Epitopes are short antigenic peptide sequences which are recognized by antibodies or immune cell receptors. These are central to the development of immunotherapies, vaccines, and diagnostics. However, the rational design of synthetic epitope libraries is challenging due to the large combinatorial sequence space, $20^n$ combinations for linear epitopes of n amino acids, making screening and testing unfeasible, even with high throughput experimental techniques. In this study, we present a large language model, epiGPTope, pre-trained on protein data and specifically fine-tuned on linear epitopes, which for the first time can directly generate novel epitope-like sequences, which are found to possess statistical properties analogous to the ones of known epitopes. This generative approach can be used to prepare libraries of epitope candidate sequences. We further train statistical classifiers to predict whether an epitope sequence is of bacterial or viral origin, thus narrowing the candidate library and increasing the likelihood of identifying specific epitopes. We propose that such combination of generative and predictive models can be of assistance in epitope discovery. The approach uses only primary amino acid sequences of linear epitopes, bypassing the need for a geometric framework or hand-crafted features of the sequences. By developing a method to create biologically feasible sequences, we anticipate faster and more cost-effective generation and screening of synthetic epitopes, with relevant applications in the development of new biotechnologies.</li>
<li><strong>摘要：</strong>表位是短抗原肽序列，由抗体或免疫细胞受体识别。这些是免疫疗法，疫苗和诊断的开发至关重要的。然而，由于较大的组合序列空间，$ 20^n $组合的合成表位库的合理设计是具有挑战性的，用于N氨基酸的线性表位，即使使用高吞吐量实验技术，也使筛查和测试不可行。在这项研究中，我们提出了一种大型语言模型，Epigptope，该模型在蛋白质数据上进行了预训练，并在线性表位上进行了细微调整，该模型首次可以直接产生新型的表位样序列，该序列被发现具有类似于已知表位的统计特性。这种生成方法可用于准备表位候选序列的库。我们进一步训练统计分类器，以预测表位序列是细菌或病毒的起源，从而缩小了候选文库并增加了识别特定表位的可能性。我们建议，这种生成和预测模型的组合可以在表位发现方面有所帮助。该方法仅使用线性表位的主要氨基酸序列，绕过对序列的几何框架或手工制作的特征的需求。通过开发一种创建生物可行序列的方法，我们预计合成表位的速度更快，更具成本效益的生成和筛选，并在新的生物技术的开发中使用了相关的应用。</li>
</ul>

<h3>Title: Human Preference-Aligned Concept Customization Benchmark via Decomposed Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Reina Ishikawa, Ryo Fujii, Hideo Saito, Ryo Hachiuma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03385">https://arxiv.org/abs/2509.03385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03385">https://arxiv.org/pdf/2509.03385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03385]] Human Preference-Aligned Concept Customization Benchmark via Decomposed Evaluation(https://arxiv.org/abs/2509.03385)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Evaluating concept customization is challenging, as it requires a comprehensive assessment of fidelity to generative prompts and concept images. Moreover, evaluating multiple concepts is considerably more difficult than evaluating a single concept, as it demands detailed assessment not only for each individual concept but also for the interactions among concepts. While humans can intuitively assess generated images, existing metrics often provide either overly narrow or overly generalized evaluations, resulting in misalignment with human preference. To address this, we propose Decomposed GPT Score (D-GPTScore), a novel human-aligned evaluation method that decomposes evaluation criteria into finer aspects and incorporates aspect-wise assessments using Multimodal Large Language Model (MLLM). Additionally, we release Human Preference-Aligned Concept Customization Benchmark (CC-AlignBench), a benchmark dataset containing both single- and multi-concept tasks, enabling stage-wise evaluation across a wide difficulty range -- from individual actions to multi-person interactions. Our method significantly outperforms existing approaches on this benchmark, exhibiting higher correlation with human preferences. This work establishes a new standard for evaluating concept customization and highlights key challenges for future research. The benchmark and associated materials are available at this https URL.</li>
<li><strong>摘要：</strong>评估概念自定义具有挑战性，因为它需要对生成提示和概念图像的保真度进行全面评估。此外，评估多个概念比评估单个概念要困难得多，因为它不仅需要对每个单独的概念，而且需要对概念之间的相互作用进行详细评估。尽管人类可以直观地评估生成的图像，但现有的指标通常会提供过度狭窄或过度概括的评估，从而导致与人类偏好的不一致。为了解决这个问题，我们提出了分解的GPT评分（D-GPTScore），这是一种新型的人类一致的评估方法，将评估标准分解为更精细的方面，并使用多模式大语言模型（MLLM）进行了方面评估。此外，我们发布了与人类偏好一致的概念定制基准（CC-Alignbench），这是一种包含单一和多概念任务的基准数据集，从个人动作到多人交互，从而在跨越的难度​​范围内启用阶段评估。我们的方法显着优于这种基准上的现有方法，与人类的偏好表现出更高的相关性。这项工作为评估概念定制的新标准建立了新的标准，并突出了未来研究的关键挑战。基准和相关材料可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Joint Training of Image Generator and Detector for Road Defect Detection</h3>
<ul>
<li><strong>Authors: </strong>Kuan-Chuan Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03465">https://arxiv.org/abs/2509.03465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03465">https://arxiv.org/pdf/2509.03465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03465]] Joint Training of Image Generator and Detector for Road Defect Detection(https://arxiv.org/abs/2509.03465)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Road defect detection is important for road authorities to reduce the vehicle damage caused by road defects. Considering the practical scenarios where the defect detectors are typically deployed on edge devices with limited memory and computational resource, we aim at performing road defect detection without using ensemble-based methods or test-time augmentation (TTA). To this end, we propose to Jointly Train the image Generator and Detector for road defect detection (dubbed as JTGD). We design the dual discriminators for the generative model to enforce both the synthesized defect patches and overall images to look plausible. The synthesized image quality is improved by our proposed CLIP-based Fréchet Inception Distance loss. The generative model in JTGD is trained jointly with the detector to encourage the generative model to synthesize harder examples for the detector. Since harder synthesized images of better quality caused by the aforesaid design are used in the data augmentation, JTGD outperforms the state-of-the-art method in the RDD2022 road defect detection benchmark across various countries under the condition of no ensemble and TTA. JTGD only uses less than 20% of the number of parameters compared with the competing baseline, which makes it more suitable for deployment on edge devices in practice.</li>
<li><strong>摘要：</strong>道路缺陷检测对于道路当局减少道路缺陷造成的车辆损害很重要。考虑到通常将缺陷探测器部署在具有有限内存和计算资源的边缘设备上的实际情况，我们旨在在不使用基于集合的方法或测试时间增强（TTA）的情况下执行道路缺陷检测。为此，我们建议共同训练图像发生器和检测器以进行道路缺陷检测（称为JTGD）。我们为生成模型设计了双重判别器，以实施合成的缺陷贴片和整体图像，以使其看起来合理。我们提出的基于夹子的Fréchet距离损失提高了合成的图像质量。 JTGD中的生成模型与检测器共同训练，以鼓励生成模型合成检测器的更难实例。由于在数据增强中使用了由上述设计引起的质量更高的质量较高的图像，因此在没有集合和TTA的条件下，JTGD在RDDDD2022 ROD缺陷检测基准中优于RDD2022 Road缺陷检测基准的最先进方法。与竞争基线相比，JTGD仅使用参数数量的20％，这使其更适合在实践中在边缘设备上部署。</li>
</ul>

<h3>Title: Parameter-Efficient Adaptation of mPLUG-Owl2 via Pixel-Level Visual Prompts for NR-IQA</h3>
<ul>
<li><strong>Authors: </strong>Yahya Benmahane, Mohammed El Hassouni</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03494">https://arxiv.org/abs/2509.03494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03494">https://arxiv.org/pdf/2509.03494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03494]] Parameter-Efficient Adaptation of mPLUG-Owl2 via Pixel-Level Visual Prompts for NR-IQA(https://arxiv.org/abs/2509.03494)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a novel parameter-efficient adaptation method for No- Reference Image Quality Assessment (NR-IQA) using visual prompts optimized in pixel-space. Unlike full fine-tuning of Multimodal Large Language Models (MLLMs), our approach trains only 600K parameters at most (< 0.01% of the base model), while keeping the underlying model fully frozen. During inference, these visual prompts are combined with images via addition and processed by mPLUG-Owl2 with the textual query "Rate the technical quality of the image." Evaluations across distortion types (synthetic, realistic, AI-generated) on KADID- 10k, KonIQ-10k, and AGIQA-3k demonstrate competitive performance against full finetuned methods and specialized NR-IQA models, achieving 0.93 SRCC on KADID-10k. To our knowledge, this is the first work to leverage pixel-space visual prompts for NR-IQA, enabling efficient MLLM adaptation for low-level vision tasks. The source code is publicly available at https: // github. com/ yahya-ben/ mplug2-vp-for-nriqa .</li>
<li><strong>摘要：</strong>在本文中，我们建议使用在像素空间中优化的视觉提示提示无参考图像质量评估（NR-IQA）的新型参数适应方法。与多模式大语言模型（MLLM）的全面微调不同，我们的方法最多只能训练600K参数（基本模型的0.01％），同时使基础模型完全冷冻。在推断期间，这些视觉提示与图像通过添加结合，并通过文本查询“对图像的技术质量”进行处理。在KADID-10K，KONIQ-10K和AGIQA-3K上进行跨失真类型（合成，现实，AI生成）的评估表明，在KADID-10K上实现了0.93 SRCC，对完整的Fineted方法和专业的NR-IQA模型进行了竞争性能。据我们所知，这是第一个利用NR-IQA的像素空间视觉提示的工作，从而为低级视觉任务提供了有效的MLLM适应性。源代码可在https：// github上公开获得。 com/ yahya-ben/ mplug2-vp-for-nriqa。</li>
</ul>

<h3>Title: OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation</h3>
<ul>
<li><strong>Authors: </strong>Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Wenrui Dai, Hongkai Xiong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03498">https://arxiv.org/abs/2509.03498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03498">https://arxiv.org/pdf/2509.03498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03498]] OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation(https://arxiv.org/abs/2509.03498)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce OneCAT, a unified multimodal model that seamlessly integrates understanding, generation, and editing within a novel, pure decoder-only transformer architecture. Our framework uniquely eliminates the need for external components such as Vision Transformers (ViT) or vision tokenizer during inference, leading to significant efficiency gains, especially for high-resolution inputs. This is achieved through a modality-specific Mixture-of-Experts (MoE) structure trained with a single autoregressive (AR) objective, which also natively supports dynamic resolutions. Furthermore, we pioneer a multi-scale visual autoregressive mechanism within the Large Language Model (LLM) that drastically reduces decoding steps compared to diffusion-based methods while maintaining state-of-the-art performance. Our findings demonstrate the powerful potential of pure autoregressive modeling as a sufficient and elegant foundation for unified multimodal intelligence. As a result, OneCAT sets a new performance standard, outperforming existing open-source unified multimodal models across benchmarks for multimodal generation, editing, and understanding.</li>
<li><strong>摘要：</strong>我们介绍了Onecat，这是一个统一的多模型模型，将理解，生成和编辑无缝整合到新颖的纯解码变压器体系结构中。我们的框架独特地消除了对推理期间视觉变压器（VIT）或视觉令牌等外部组件的需求，从而导致效率显着提高，尤其是对于高分辨率输入。这是通过具有单个自回归（AR）物镜训练的特定于模式的特定于特定于特定的专家（MOE）结构来实现的，该物镜还本地支持动态分辨率。此外，我们在大语言模型（LLM）中开创了一种多尺度视觉自回归机制，该机制与基于扩散的方法相比，在保持最新性能的同时，大大降低了解码步骤。我们的发现证明了纯自回归建模的强大潜力，这是统一多模式智能的足够优雅的基础。结果，Onecat设定了一个新的性能标准，在基准跨基准范围内优于现有的开源统一的多模型，用于多模式生成，编辑和理解。</li>
</ul>

<h3>Title: Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data</h3>
<ul>
<li><strong>Authors: </strong>Honglu Zhou, Xiangyu Peng, Shrikant Kendre, Michael S. Ryoo, Silvio Savarese, Caiming Xiong, Juan Carlos Niebles</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03501">https://arxiv.org/abs/2509.03501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03501">https://arxiv.org/pdf/2509.03501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03501]] Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data(https://arxiv.org/abs/2509.03501)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Next-generation AI companions must go beyond general video understanding to resolve spatial and temporal references in dynamic, real-world environments. Existing Video Large Language Models (Video LLMs), while capable of coarse-level comprehension, struggle with fine-grained, spatiotemporal reasoning, especially when user queries rely on time-based event references for temporal anchoring, or gestural cues for spatial anchoring to clarify object references and positions. To bridge this critical gap, we introduce Strefer, a synthetic instruction data generation framework designed to equip Video LLMs with spatiotemporal referring and reasoning capabilities. Strefer produces diverse instruction-tuning data using a data engine that pseudo-annotates temporally dense, fine-grained video metadata, capturing rich spatial and temporal information in a structured manner, including subjects, objects, their locations as masklets, and their action descriptions and timelines. Our approach enhances the ability of Video LLMs to interpret spatial and temporal references, fostering more versatile, space-time-aware reasoning essential for real-world AI companions. Without using proprietary models, costly human annotation, or the need to annotate large volumes of new videos, experimental evaluations show that models trained with data produced by Strefer outperform baselines on tasks requiring spatial and temporal disambiguation. Additionally, these models exhibit enhanced space-time-aware reasoning, establishing a new foundation for perceptually grounded, instruction-tuned Video LLMs.</li>
<li><strong>摘要：</strong>下一代AI伴侣必须超越一般视频理解，以解决动态的真实世界环境中的空间和时间参考。现有的视频大语言模型（视频LLM）虽然能够进行粗糙级别的理解，但与细粒度的时空推理作斗争，尤其是当用户查询依靠基于时间的事件参考来进行时间锚定或手势提示以阐明对象参考和位置时。为了弥合这个关键的差距，我们介绍了Strefer，这是一个合成指导数据生成框架，旨在为视频LLM配备时空参考和推理功能。 Strefer使用数据引擎产生多样化的指令调整数据，该数据引擎可以用结构化的方式捕获时间密集的，细粒度的视频元数据，以捕获丰富的空间和时间信息，包括主题，对象，其位置为口罩，以及他们的动作描述和时间表和时间表。我们的方法增强了视频LLM解释空间和时间参考的能力，从而促进了对现实世界AI同伴至关重要的更通用，时空感知的推理。在不使用专有模型，昂贵的人类注释或注释大量新视频的情况下，实验评估表明，使用Strefer -prothers Probropor -Baselines训练的模型在需要空间和暂时性歧义的任务上产生的数据。此外，这些模型表现出增强的时空感知推理，为感知接地的，指导的视频LLM建立了新的基础。</li>
</ul>

<h3>Title: LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Xingxuan Zhang, Gang Ren, Han Yu, Hao Yuan, Hui Wang, Jiansheng Li, Jiayun Wu, Lang Mo, Li Mao, Mingchao Hao, Ningbo Dai, Renzhe Xu, Shuyang Li, Tianyang Zhang, Yue He, Yuanrui Wang, Yunjia Zhang, Zijing Xu, Dongzhe Li, Fang Gao, Hao Zou, Jiandong Liu, Jiashuo Liu, Jiawei Xu, Kaijie Cheng, Kehan Li, Linjun Zhou, Qing Li, Shaohua Fan, Xiaoyu Lin, Xinyan Han, Xuanyue Li, Yan Lu, Yuan Xue, Yuanyuan Jiang, Zimu Wang, Zhenlei Wang, Peng Cui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03505">https://arxiv.org/abs/2509.03505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03505">https://arxiv.org/pdf/2509.03505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03505]] LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence(https://arxiv.org/abs/2509.03505)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We argue that progress toward general intelligence requires complementary foundation models grounded in language, the physical world, and structured data. This report presents LimiX, the first installment of our large structured-data models (LDMs). LimiX treats structured data as a joint distribution over variables and missingness, thus capable of addressing a wide range of tabular tasks through query-based conditional prediction via a single model. LimiX is pretrained using masked joint-distribution modeling with an episodic, context-conditional objective, where the model predicts for query subsets conditioned on dataset-specific contexts, supporting rapid, training-free adaptation at inference. We evaluate LimiX across 10 large structured-data benchmarks with broad regimes of sample size, feature dimensionality, class number, categorical-to-numerical feature ratio, missingness, and sample-to-feature ratios. With a single model and a unified interface, LimiX consistently surpasses strong baselines including gradient-boosting trees, deep tabular networks, recent tabular foundation models, and automated ensembles, as shown in Figure 1 and Figure 2. The superiority holds across a wide range of tasks, such as classification, regression, missing value imputation, and data generation, often by substantial margins, while avoiding task-specific architectures or bespoke training per task. All LimiX models are publicly accessible under Apache 2.0.</li>
<li><strong>摘要：</strong>我们认为，朝着通用情报的进展需要以语言，物理世界和结构化数据为基础的互补基础模型。该报告介绍了Limix，这是我们大型结构化数据模型（LDMS）的第一部分。 Limix将结构化数据视为变量和丢失的关节分布，因此能够通过单个模型通过基于查询的条件预测来解决广泛的表格任务。使用掩盖的联合分布建模对Limix进行了预审，该模型具有情节性，上下文条件目标，该模型可以预测在数据集特异性上下文中调节的查询子集，从而支持推断时快速，无训练的适应。我们在10个大型结构性数据基准中评估Limix，其样本量，特征维度，班级数，分类特征比率，缺失性和样本对功能比率具有广泛的状态。凭借单个模型和统一的界面，Limix始终超过强大的基线，包括梯度增强的树木，深层表格网络，最近的表格基础模型和自动合奏，如图1和图2所示。优越性在各种任务中保持着广泛的任务，例如分类，回归价值，缺失的价值，缺失的价值，缺失，数据生成，通常避免使用实体架构，或者构建既定任务，或者在构造任务中进行培训，而构造了任务。在Apache 2.0下，所有Limix型号均可公开访问。</li>
</ul>

<h3>Title: Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?</h3>
<ul>
<li><strong>Authors: </strong>Ouxiang Li, Yuan Wang, Xinting Hu, Huijuan Huang, Rui Chen, Jiarong Ou, Xin Tao, Pengfei Wan, Fuli Feng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.03516">https://arxiv.org/abs/2509.03516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.03516">https://arxiv.org/pdf/2509.03516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.03516]] Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?(https://arxiv.org/abs/2509.03516)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) generation aims to synthesize images from textual prompts, which jointly specify what must be shown and imply what can be inferred, thereby corresponding to two core capabilities: composition and reasoning. However, with the emerging advances of T2I models in reasoning beyond composition, existing benchmarks reveal clear limitations in providing comprehensive evaluations across and within these capabilities. Meanwhile, these advances also enable models to handle more complex prompts, whereas current benchmarks remain limited to low scene density and simplified one-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a comprehensive and complex benchmark that evaluates both composition and reasoning capabilities of T2I models. To ensure comprehensiveness, we structure composition around scene graph elements (instance, attribute, and relation) and reasoning around the philosophical framework of inference (deductive, inductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To increase complexity, driven by the inherent complexities of real-world scenarios, we curate each prompt with high compositional density for composition and multi-step inference for reasoning. We also pair each prompt with a checklist that specifies individual yes/no questions to assess each intended element independently to facilitate fine-grained and reliable evaluation. In statistics, our benchmark comprises 1,080 challenging prompts and around 13,500 checklist questions. Experiments across 27 current T2I models reveal that their composition capability still remains limited in complex high-density scenarios, while the reasoning capability lags even further behind as a critical bottleneck, with all models struggling to infer implicit elements from prompts. Our project page: this https URL.</li>
<li><strong>摘要：</strong>文本对图像（T2I）生成旨在从文本提示中综合图像，该提示共同指定了必须显示的内容并暗示可以推断的内容，从而对应于两个核心功能：组成和推理。但是，随着T2I模型在构图之外的推理中的新兴进步，现有的基准揭示了在这些功能之间和内部提供全面评估时明确的局限性。同时，这些进步还使模型能够处理更复杂的提示，而当前的基准仍限于低场景密度和简化的一对一推理。为了解决这些局限性，我们提出了T2i-Corebench，这是一种全面而复杂的基准，可以评估T2I模型的组成和推理能力。为了确保全面性，我们围绕场景元素（实例，属性和关系）结构组成，并围绕推理的哲学框架（演绎，归纳和绑架）进行推理，从而制定了12维评估分类法。为了增加实际情况的固有复杂性的驱动，我们将每个提示策划每个提示，并具有高组成密度，用于组成和推理多步推断。我们还将每个提示与一个清单配对，该清单指定个人是/否问题，以独立评估每个预期的元素，以促进细粒度和可靠的评估。在统计数据中，我们的基准包括1,080个具有挑战性的提示和大约13,500个清单问题。当前27个T2I模型的实验表明，在复杂的高密度方案中，它们的组成能力仍然保持限制，而推理能力的落后于关键瓶颈，所有模型都在努力从提示中推断出隐式元素。我们的项目页面：此HTTPS URL。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
