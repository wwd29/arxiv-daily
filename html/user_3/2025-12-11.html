<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-12-11</h1>
<h3>Title: LLM4XCE: Large Language Models for Extremely Large-Scale Massive MIMO Channel Estimation</h3>
<ul>
<li><strong>Authors: </strong>Renbin Li, Shuangshuang Li, Peihao Dong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08955">https://arxiv.org/abs/2512.08955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08955">https://arxiv.org/pdf/2512.08955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08955]] LLM4XCE: Large Language Models for Extremely Large-Scale Massive MIMO Channel Estimation(https://arxiv.org/abs/2512.08955)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Extremely large-scale massive multiple-input multiple-output (XL-MIMO) is a key enabler for sixth-generation (6G) networks, offering massive spatial degrees of freedom. Despite these advantages, the coexistence of near-field and far-field effects in hybrid-field channels presents significant challenges for accurate estimation, where traditional methods often struggle to generalize effectively. In recent years, large language models (LLMs) have achieved impressive performance on downstream tasks via fine-tuning, aligning with the semantic communication shift toward task-oriented understanding over bit-level accuracy. Motivated by this, we propose Large Language Models for XL-MIMO Channel Estimation (LLM4XCE), a novel channel estimation framework that leverages the semantic modeling capabilities of large language models to recover essential spatial-channel representations for downstream tasks. The model integrates a carefully designed embedding module with Parallel Feature-Spatial Attention, enabling deep fusion of pilot features and spatial structures to construct a semantically rich representation for LLM input. By fine-tuning only the top two Transformer layers, our method effectively captures latent dependencies in the pilot data while ensuring high training efficiency. Extensive simulations demonstrate that LLM4XCE significantly outperforms existing state-of-the-art methods under hybrid-field conditions, achieving superior estimation accuracy and generalization performance.</li>
<li><strong>摘要：</strong>超大规模多输入多输出 (XL-MIMO) 是第六代 (6G) 网络的关键推动因素，可提供巨大的空间自由度。尽管有这些优点，但混合场通道中近场和远场效应的共存对准确估计提出了重大挑战，传统方法往往难以有效推广。近年来，大型语言模型（LLM）通过微调，在下游任务上取得了令人印象深刻的性能，与语义通信向面向任务的理解而非位级准确性的转变相一致。受此启发，我们提出了用于 XL-MIMO 信道估计的大型语言模型 (LLM4XCE)，这是一种新颖的信道估计框架，它利用大型语言模型的语义建模功能来恢复下游任务的基本空间信道表示。该模型将精心设计的嵌入模块与并行特征空间注意力相结合，实现导频特征和空间结构的深度融合，为 LLM 输入构建语义丰富的表示。通过仅微调最上面的两个 Transformer 层，我们的方法有效地捕获了试点数据中的潜在依赖关系，同时确保了高训练效率。广泛的仿真表明，LLM4XCE 在混合场条件下显着优于现有的最先进方法，实现了卓越的估计精度和泛化性能。</li>
</ul>

<h3>Title: Consist-Retinex: One-Step Noise-Emphasized Consistency Training Accelerates High-Quality Retinex Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Jian Xu, Wei Chen, Shigui Li, Delu Zeng, John Paisley, Qibin Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08982">https://arxiv.org/abs/2512.08982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08982">https://arxiv.org/pdf/2512.08982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08982]] Consist-Retinex: One-Step Noise-Emphasized Consistency Training Accelerates High-Quality Retinex Enhancement(https://arxiv.org/abs/2512.08982)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable success in low-light image enhancement through Retinex-based decomposition, yet their requirement for hundreds of iterative sampling steps severely limits practical deployment. While recent consistency models offer promising one-step generation for \textit{unconditional synthesis}, their application to \textit{conditional enhancement} remains unexplored. We present \textbf{Consist-Retinex}, the first framework adapting consistency modeling to Retinex-based low-light enhancement. Our key insight is that conditional enhancement requires fundamentally different training dynamics than unconditional generation standard consistency training focuses on low-noise regions near the data manifold, while conditional mapping critically depends on large-noise regimes that bridge degraded inputs to enhanced outputs. We introduce two core innovations: (1) a \textbf{dual-objective consistency loss} combining temporal consistency with ground-truth alignment under randomized time sampling, providing full-spectrum supervision for stable convergence; and (2) an \textbf{adaptive noise-emphasized sampling strategy} that prioritizes training on large-noise regions essential for one-step conditional generation. On VE-LOL-L, Consist-Retinex achieves \textbf{state-of-the-art performance with single-step sampling} (\textbf{PSNR: 25.51 vs. 23.41, FID: 44.73 vs. 49.59} compared to Diff-Retinex++), while requiring only \textbf{1/8 of the training budget} relative to the 1000-step Diff-Retinex baseline.</li>
<li><strong>摘要：</strong>扩散模型通过基于 Retinex 的分解在低光图像增强方面取得了显着的成功，但其对数百个迭代采样步骤的要求严重限制了实际部署。虽然最近的一致性模型为 \textit{无条件合成} 提供了有前景的一步生成，但它们在 \textit{条件增强} 中的应用仍未得到探索。我们提出 \textbf{Consist-Retinex}，这是第一个将一致性建模应用于基于 Retinex 的低光增强的框架。我们的主要见解是，条件增强需要与无条件生成标准一致性训练完全不同的训练动态，该训练侧重于数据流形附近的低噪声区域，而条件映射关键取决于将降级输入桥接到增强输出的大噪声机制。我们引入了两个核心创新：（1）\textbf{双目标一致性损失}，在随机时间采样下将时间一致性与真实对齐相结合，为稳定收敛提供全谱监督； （2）一种\textbf{自适应噪声强调采样策略}，它优先考虑对一步条件生成所必需的大噪声区域进行训练。在 VE-LOL-L 上，Consist-Retinex 通过单步采样实现了最先进的性能（与 Diff-Retinex++ 相比，\textbf{PSNR：25.51 vs. 23.41，FID：44.73 vs. 49.59}），同时相对于 1000 步仅需要 \textbf{1/8 的训练预算} Diff-Retinex 基线。</li>
</ul>

<h3>Title: RAG-HAR: Retrieval Augmented Generation-based Human Activity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Nirhoshan Sivaroopan, Hansi Karunarathna, Chamara Madarasingha, Anura Jayasumana, Kanchana Thilakarathna</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08984">https://arxiv.org/abs/2512.08984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08984">https://arxiv.org/pdf/2512.08984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08984]] RAG-HAR: Retrieval Augmented Generation-based Human Activity Recognition(https://arxiv.org/abs/2512.08984)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Human Activity Recognition (HAR) underpins applications in healthcare, rehabilitation, fitness tracking, and smart environments, yet existing deep learning approaches demand dataset-specific training, large labeled corpora, and significant computational this http URL introduce RAG-HAR, a training-free retrieval-augmented framework that leverages large language models (LLMs) for HAR. RAG-HAR computes lightweight statistical descriptors, retrieves semantically similar samples from a vector database, and uses this contextual evidence to make LLM-based activity identification. We further enhance RAG-HAR by first applying prompt optimization and introducing an LLM-based activity descriptor that generates context-enriched vector databases for delivering accurate and highly relevant contextual information. Along with these mechanisms, RAG-HAR achieves state-of-the-art performance across six diverse HAR benchmarks. Most importantly, RAG-HAR attains these improvements without requiring model training or fine-tuning, emphasizing its robustness and practical applicability. RAG-HAR moves beyond known behaviors, enabling the recognition and meaningful labelling of multiple unseen human activities.</li>
<li><strong>摘要：</strong>人类活动识别 (HAR) 支撑着医疗保健、康复、健身跟踪和智能环境中的应用，但现有的深度学习方法需要特定于数据集的训练、大型标记语料库和大量计算。此 http URL 介绍了 RAG-HAR，这是一种无需训练的检索增强框架，利用 HAR 的大型语言模型 (LLM)。 RAG-HAR 计算轻量级统计描述符，从向量数据库中检索语义相似的样本，并使用此上下文证据进行基于 LLM 的活动识别。我们首先应用即时优化并引入基于 LLM 的活动描述符，该描述符生成上下文丰富的向量数据库，以提供准确且高度相关的上下文信息，从而进一步增强 RAG-HAR。除了这些机制之外，RAG-HAR 在六个不同的 HAR 基准测试中实现了最先进的性能。最重要的是，RAG-HAR 无需模型训练或微调即可实现这些改进，强调了其稳健性和实际适用性。 RAG-HAR 超越了已知的行为，能够识别多种看不见的人类活动并对其进行有意义的标记。</li>
</ul>

<h3>Title: An Efficient Test-Time Scaling Approach for Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Vignesh Sundaresha, Akash Haridas, Vikram Appia, Lav Varshney</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08985">https://arxiv.org/abs/2512.08985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08985">https://arxiv.org/pdf/2512.08985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08985]] An Efficient Test-Time Scaling Approach for Image Generation(https://arxiv.org/abs/2512.08985)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Image generation has emerged as a mainstream application of large generative AI models. Just as test-time compute and reasoning have helped language models improve their capabilities, similar benefits have also been observed with image generation models. In particular, searching over noise samples for diffusion and flow models has shown to scale well with test-time compute. While recent works have explored allocating non-uniform inference-compute budgets across different denoising steps, they rely on greedy algorithms and allocate the compute budget ineffectively. In this work, we study this problem and propose solutions to fix it. We propose the Verifier-Threshold method which automatically reallocates test-time compute and delivers substantial efficiency improvements. For the same performance on the GenEval benchmark, we achieve a 2-4x reduction in computational time over the state-of-the-art method.</li>
<li><strong>摘要：</strong>图像生成已成为大型生成人工智能模型的主流应用。正如测试时计算和推理帮助语言模型提高其能力一样，图像生成模型也观察到了类似的好处。特别是，在噪声样本中搜索扩散和流动模型已证明可以很好地适应测试时计算。虽然最近的工作探索了在不同的去噪步骤之间分配非均匀推理计算预算，但它们依赖于贪婪算法并且无效地分配计算预算。在这项工作中，我们研究这个问题并提出解决方案。我们提出了验证器阈值方法，该方法自动重新分配测试时间计算并显着提高效率。对于 GenEval 基准上相同的性能，我们的计算时间比最先进的方法减少了 2-4 倍。</li>
</ul>

<h3>Title: 3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yuze Hao, Linchao Zhu, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08987">https://arxiv.org/abs/2512.08987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08987">https://arxiv.org/pdf/2512.08987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08987]] 3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization(https://arxiv.org/abs/2512.08987)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Inverse design aims to design the input variables of a physical system to optimize a specified objective function, typically formulated as a search or optimization problem. However, in 3D domains, the design space grows exponentially, rendering exhaustive grid-based searches infeasible. Recent advances in deep learning have accelerated inverse design by providing powerful generative priors and differentiable surrogate models. Nevertheless, current methods tend to approximate the 3D design space using 2D projections or fine-tune existing 3D shapes. These approaches sacrifice volumetric detail and constrain design exploration, preventing true 3D design from scratch. In this paper, we propose a 3D Inverse Design (3DID) framework that directly navigates the 3D design space by coupling a continuous latent representation with a physics-aware optimization strategy. We first learn a unified physics-geometry embedding that compactly captures shape and physical field data in a continuous latent space. Then, we introduce a two-stage strategy to perform physics-aware optimization. In the first stage, a gradient-guided diffusion sampler explores the global latent manifold. In the second stage, an objective-driven, topology-preserving refinement further sculpts each candidate toward the target objective. This enables 3DID to generate high-fidelity 3D geometries, outperforming existing methods in both solution quality and design versatility.</li>
<li><strong>摘要：</strong>逆向设计旨在设计物理系统的输入变量以优化指定的目标函数，通常表述为搜索或优化问题。然而，在 3D 领域，设计空间呈指数级增长，使得基于网格的详尽搜索变得不可行。深度学习的最新进展通过提供强大的生成先验和可微的代理模型加速了逆向设计。然而，当前的方法倾向于使用 2D 投影来近似 3D 设计空间或微调现有的 3D 形状。这些方法牺牲了体积细节并限制了设计探索，从而阻碍了从头开始进行真正的 3D 设计。在本文中，我们提出了一种 3D 逆向设计 (3DID) 框架，该框架通过将连续潜在表示与物理感知优化策略相结合来直接导航 3D 设计空间。我们首先学习一个统一的物理几何嵌入，它可以在连续的潜在空间中紧凑地捕获形状和物理场数据。然后，我们引入两阶段策略来执行物理感知优化。在第一阶段，梯度引导扩散采样器探索全局潜在流形。在第二阶段，目标驱动的、保留拓扑的细化进一步塑造每个候选者以实现目标。这使得 3DID 能够生成高保真 3D 几何图形，在解决方案质量和设计多功能性方面均优于现有方法。</li>
</ul>

<h3>Title: Deterministic World Models for Verification of Closed-loop Vision-based Systems</h3>
<ul>
<li><strong>Authors: </strong>Yuang Geng, Zhuoyang Zhou, Zhongzheng Zhang, Siyuan Pan, Hoang-Dung Tran, Ivan Ruchkin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08991">https://arxiv.org/abs/2512.08991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08991">https://arxiv.org/pdf/2512.08991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08991]] Deterministic World Models for Verification of Closed-loop Vision-based Systems(https://arxiv.org/abs/2512.08991)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Verifying closed-loop vision-based control systems remains a fundamental challenge due to the high dimensionality of images and the difficulty of modeling visual environments. While generative models are increasingly used as camera surrogates in verification, their reliance on stochastic latent variables introduces unnecessary overapproximation error. To address this bottleneck, we propose a Deterministic World Model (DWM) that maps system states directly to generative images, effectively eliminating uninterpretable latent variables to ensure precise input bounds. The DWM is trained with a dual-objective loss function that combines pixel-level reconstruction accuracy with a control difference loss to maintain behavioral consistency with the real system. We integrate DWM into a verification pipeline utilizing Star-based reachability analysis (StarV) and employ conformal prediction to derive rigorous statistical bounds on the trajectory deviation between the world model and the actual vision-based system. Experiments on standard benchmarks show that our approach yields significantly tighter reachable sets and better verification performance than a latent-variable baseline.</li>
<li><strong>摘要：</strong>由于图像的高维性和视觉环境建模的难度，验证基于闭环视觉的控制系统仍然是一个基本挑战。虽然生成模型越来越多地用作验证中的相机替代物，但它们对随机潜在变量的依赖引入了不必要的过度逼近误差。为了解决这个瓶颈，我们提出了一种确定性世界模型（DWM），它将系统状态直接映射到生成图像，有效消除不可解释的潜在变量，以确保精确的输入范围。 DWM 使用双目标损失函数进行训练，该函数将像素级重建精度与控制差异损失相结合，以保持与真实系统的行为一致性。我们利用基于星的可达性分析 ​​(StarV) 将 DWM 集成到验证管道中，并采用共形预测来得出世界模型与实际基于视觉的系统之间的轨迹偏差的严格统计界限。标准基准测试的实验表明，与潜在变量基线相比，我们的方法产生了更紧密的可达集和更好的验证性能。</li>
</ul>

<h3>Title: Demo: Generative AI helps Radiotherapy Planning with User Preference</h3>
<ul>
<li><strong>Authors: </strong>Riqiang Gao, Simon Arberet, Martin Kraus, Han Liu, Wilko FAR Verbakel, Dorin Comaniciu, Florin-Cristian Ghesu, Ali Kamen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.08996">https://arxiv.org/abs/2512.08996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.08996">https://arxiv.org/pdf/2512.08996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.08996]] Demo: Generative AI helps Radiotherapy Planning with User Preference(https://arxiv.org/abs/2512.08996)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Radiotherapy planning is a highly complex process that often varies significantly across institutions and individual planners. Most existing deep learning approaches for 3D dose prediction rely on reference plans as ground truth during training, which can inadvertently bias models toward specific planning styles or institutional preferences. In this study, we introduce a novel generative model that predicts 3D dose distributions based solely on user-defined preference flavors. These customizable preferences enable planners to prioritize specific trade-offs between organs-at-risk (OARs) and planning target volumes (PTVs), offering greater flexibility and personalization. Designed for seamless integration with clinical treatment planning systems, our approach assists users in generating high-quality plans efficiently. Comparative evaluations demonstrate that our method can surpasses the Varian RapidPlan model in both adaptability and plan quality in some scenarios.</li>
<li><strong>摘要：</strong>放射治疗计划是一个高度复杂的过程，不同机构和个人计划者之间的差异往往很大。大多数现有的 3D 剂量预测深度学习方法都依赖参考计划作为训练期间的基本事实，这可能会无意中使模型偏向特定的计划风格或机构偏好。在这项研究中，我们引入了一种新颖的生成模型，该模型仅根据用户定义的偏好口味来预测 3D 剂量分布。这些可定制的首选项使规划人员能够优先考虑危及器官 (OAR) 和规划目标体积 (PTV) 之间的具体权衡，从而提供更大的灵活性和个性化。我们的方法专为与临床治疗计划系统无缝集成而设计，可帮助用户有效地生成高质量的计划。对比评估表明，在某些场景下，我们的方法在适应性和计划质量方面都可以超越 Varian RapidPlan 模型。</li>
</ul>

<h3>Title: A Physics-Constrained, Design-Driven Methodology for Defect Dataset Generation in Optical Lithography</h3>
<ul>
<li><strong>Authors: </strong>Yuehua Hu, Jiyeong Kong, Dong-yeol Shin, Jaekyun Kim, Kyung-Tae Kang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09001">https://arxiv.org/abs/2512.09001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09001">https://arxiv.org/pdf/2512.09001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09001]] A Physics-Constrained, Design-Driven Methodology for Defect Dataset Generation in Optical Lithography(https://arxiv.org/abs/2512.09001)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The efficacy of Artificial Intelligence (AI) in micro/nano manufacturing is fundamentally constrained by the scarcity of high-quality and physically grounded training data for defect inspection. Lithography defect data from semiconductor industry are rarely accessible for research use, resulting in a shortage of publicly available datasets. To address this bottleneck in lithography, this study proposes a novel methodology for generating large-scale, physically valid defect datasets with pixel-level annotations. The framework begins with the ab initio synthesis of defect layouts using controllable, physics-constrained mathematical morphology operations (erosion and dilation) applied to the original design-level layout. These synthesized layouts, together with their defect-free counterparts, are fabricated into physical samples via high-fidelity digital micromirror device (DMD)-based lithography. Optical micrographs of the synthesized defect samples and their defect-free references are then compared to create consistent defect delineation annotations. Using this methodology, we constructed a comprehensive dataset of 3,530 Optical micrographs containing 13,365 annotated defect instances including four classes: bridge, burr, pinch, and contamination. Each defect instance is annotated with a pixel-accurate segmentation mask, preserving full contour and geometry. The segmentation-based Mask R-CNN achieves AP@0.5 of 0.980, 0.965, and 0.971, compared with 0.740, 0.719, and 0.717 for Faster R-CNN on bridge, burr, and pinch classes, representing a mean AP@0.5 improvement of approximately 34%. For the contamination class, Mask R-CNN achieves an AP@0.5 roughly 42% higher than Faster R-CNN. These consistent gains demonstrate that our proposed methodology to generate defect datasets with pixel-level annotations is feasible for robust AI-based Measurement/Inspection (MI) in semiconductor fabrication.</li>
<li><strong>摘要：</strong>人工智能（AI）在微纳米制造中的功效从根本上受到缺乏用于缺陷检查的高质量且有物理基础的训练数据的限制。半导体行业的光刻缺陷数据很少可供研究使用，导致公开数据集的短缺。为了解决光刻中的这一瓶颈，本研究提出了一种新颖的方法，用于生成具有像素级注释的大规模、物理有效的缺陷数据集。该框架首先使用应用于原始设计级布局的可控的、物理约束的数学形态学运算（侵蚀和膨胀）来从头开始综合缺陷布局。这些合成布局与其无缺陷的对应布局一起，通过基于高保真数字微镜器件 (DMD) 的光刻技术制作成物理样品。然后将合成缺陷样品的光学显微照片与其无缺陷参考进行比较，以创建一致的缺陷描绘注释。使用这种方法，我们构建了一个包含 3,530 张光学显微照片的综合数据集，其中包含 13,365 个带注释的缺陷实例，包括四类：桥接、毛刺、收缩和污染。每个缺陷实例都用像素精确的分割掩模进行注释，保留完整的轮廓和几何形状。基于分割的 Mask R-CNN 在桥接、毛刺和挤压类别上的 AP@0.5 达到了 0.980、0.965 和 0.971，而 Faster R-CNN 的 AP@0.5 为 0.740、0.719 和 0.717，这意味着平均 AP@0.5 提高了约 34%。对于污染类别，Mask R-CNN 的 AP@0.5 比 Faster R-CNN 高大约 42%。这些一致的成果表​​明，我们提出的生成具有像素级注释的缺陷数据集的方法对于半导体制造中基于人工智能的稳健测量/检测 (MI) 是可行的。</li>
</ul>

<h3>Title: A Survey of Body and Face Motion: Datasets, Performance Evaluation Metrics and Generative Techniques</h3>
<ul>
<li><strong>Authors: </strong>Lownish Rai Sookha, Nikhil Pakhale, Mudasir Ganaie, Abhinav Dhall</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09005">https://arxiv.org/abs/2512.09005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09005">https://arxiv.org/pdf/2512.09005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09005]] A Survey of Body and Face Motion: Datasets, Performance Evaluation Metrics and Generative Techniques(https://arxiv.org/abs/2512.09005)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Body and face motion play an integral role in communication. They convey crucial information on the participants. Advances in generative modeling and multi-modal learning have enabled motion generation from signals such as speech, conversational context and visual cues. However, generating expressive and coherent face and body dynamics remains challenging due to the complex interplay of verbal / non-verbal cues and individual personality traits. This survey reviews body and face motion generation, covering core concepts, representations techniques, generative approaches, datasets and evaluation metrics. We highlight future directions to enhance the realism, coherence and expressiveness of avatars in dyadic settings. To the best of our knowledge, this work is the first comprehensive review to cover both body and face motion. Detailed resources are listed on this https URL.</li>
<li><strong>摘要：</strong>身体和面部动作在交流中发挥着不可或缺的作用。它们传达了参与者的重要信息。生成建模和多模态学习的进步使得能够根据语音、对话上下文和视觉提示等信号生成运动。然而，由于言语/非言语线索和个人性格特征的复杂相互作用，生成富有表现力和连贯的面部和身体动态仍然具有挑战性。这项调查回顾了身体和面部运动的生成，涵盖核心概念、表示技术、生成方法、数据集和评估指标。我们强调了未来的方向，以增强二元环境中化身的真实性、连贯性和表现力。据我们所知，这项工作是第一个涵盖身体和面部运动的全面综述。此 https URL 上列出了详细资源。</li>
</ul>

<h3>Title: Learning to Remove Lens Flare in Event Camera</h3>
<ul>
<li><strong>Authors: </strong>Haiqian Han, Lingdong Kong, Jianing Li, Ao Liang, Chengtao Zhu, Jiacheng Lyu, Lai Xing Ng, Xiangyang Ji, Wei Tsang Ooi, Benoit R. Cottereau</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09016">https://arxiv.org/abs/2512.09016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09016">https://arxiv.org/pdf/2512.09016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09016]] Learning to Remove Lens Flare in Event Camera(https://arxiv.org/abs/2512.09016)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Event cameras have the potential to revolutionize vision systems with their high temporal resolution and dynamic range, yet they remain susceptible to lens flare, a fundamental optical artifact that causes severe degradation. In event streams, this optical artifact forms a complex, spatio-temporal distortion that has been largely overlooked. We present E-Deflare, the first systematic framework for removing lens flare from event camera data. We first establish the theoretical foundation by deriving a physics-grounded forward model of the non-linear suppression mechanism. This insight enables the creation of the E-Deflare Benchmark, a comprehensive resource featuring a large-scale simulated training set, E-Flare-2.7K, and the first-ever paired real-world test set, E-Flare-R, captured by our novel optical system. Empowered by this benchmark, we design E-DeflareNet, which achieves state-of-the-art restoration performance. Extensive experiments validate our approach and demonstrate clear benefits for downstream tasks. Code and datasets are publicly available.</li>
<li><strong>摘要：</strong>事件相机具有高时间分辨率和动态范围，有可能彻底改变视觉系统，但它们仍然容易受到镜头眩光的影响，这是一种导致严重退化的基本光学伪影。在事件流中，这种光学伪影形成了一种复杂的时空扭曲，而这种扭曲在很大程度上被忽视了。我们推出了 E-Deflare，这是第一个用于从事件摄像机数据中消除镜头眩光的系统框架。我们首先通过推导非线性抑制机制的基于物理的正演模型来建立理论基础。这种洞察力使得 E-Deflare 基准的创建成为可能，这是一种综合资源，具有大规模模拟训练集 E-Flare-2.7K 以及由我们的新型光学系统捕获的首个配对的真实世界测试集 E-Flare-R。在此基准的支持下，我们设计了 E-DeflareNet，它实现了最先进的恢复性能。大量的实验验证了我们的方法，并展示了对下游任务的明显好处。代码和数据集是公开的。</li>
</ul>

<h3>Title: KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification</h3>
<ul>
<li><strong>Authors: </strong>Erfan Nourbakhsh, Nasrin Sanjari, Ali Nourbakhsh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09069">https://arxiv.org/abs/2512.09069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09069">https://arxiv.org/pdf/2512.09069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09069]] KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification(https://arxiv.org/abs/2512.09069)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency- accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at this https URL OCT.</li>
<li><strong>摘要：</strong>年龄相关性黄斑变性 (AMD) 和脉络膜新生血管 (CNV) 相关疾病是全球视力丧失的主要原因，光学相干断层扫描 (OCT) 是早期检测和治疗的基石。然而，在临床环境中部署像 ConvNeXtV2-Large 这样最先进的深度学习模型却受到其计算需求的阻碍。因此，需要开发有效的模型，在实现实时部署的同时保持高诊断性能。在本研究中，提出了一种称为 KD-OCT 的新型知识蒸馏框架，将高性能 ConvNeXtV2-Large 教师模型（通过高级增强、随机权重平均和焦点损失增强）压缩为轻量级 EfficientNet-B2 学生模型，用于对正常、玻璃疣和 CNV 病例进行分类。 KD-OCT 采用实时蒸馏，结合损失平衡软教师知识转移和硬事实监督。使用患者级交叉验证在努尔眼科医院 (NEH) 数据集上评估所提出方法的有效性。实验结果表明，KD-OCT 在效率-准确度平衡方面优于可比较的多尺度或特征融合 OCT 分类器，实现了接近教师的性能，同时大幅减少了模型大小和推理时间。尽管进行了压缩，学生模型还是超越了大多数现有框架，促进了 AMD 筛查的边缘部署。代码可在此 https URL OCT 获取。</li>
</ul>

<h3>Title: AgentComp: From Agentic Reasoning to Compositional Mastery in Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Arman Zarei, Jiacheng Pan, Matthew Gwilliam, Soheil Feizi, Zhenheng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09081">https://arxiv.org/abs/2512.09081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09081">https://arxiv.org/pdf/2512.09081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09081]] AgentComp: From Agentic Reasoning to Compositional Mastery in Text-to-Image Models(https://arxiv.org/abs/2512.09081)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image generative models have achieved remarkable visual quality but still struggle with compositionality$-$accurately capturing object relationships, attribute bindings, and fine-grained details in prompts. A key limitation is that models are not explicitly trained to differentiate between compositionally similar prompts and images, resulting in outputs that are close to the intended description yet deviate in fine-grained details. To address this, we propose AgentComp, a framework that explicitly trains models to better differentiate such compositional variations and enhance their reasoning ability. AgentComp leverages the reasoning and tool-use capabilities of large language models equipped with image generation, editing, and VQA tools to autonomously construct compositional datasets. Using these datasets, we apply an agentic preference optimization method to fine-tune text-to-image models, enabling them to better distinguish between compositionally similar samples and resulting in overall stronger compositional generation ability. AgentComp achieves state-of-the-art results on compositionality benchmarks such as T2I-CompBench, without compromising image quality$-$a common drawback in prior approaches$-$and even generalizes to other capabilities not explicitly trained for, such as text rendering.</li>
<li><strong>摘要：</strong>文本到图像生成模型已经实现了卓越的视觉质量，但仍然难以准确捕捉提示中的对象关系、属性绑定和细粒度细节。一个关键的限制是，模型没有经过明确的训练来区分成分相似的提示和图像，导致输出接近预期描述，但在细粒度细节上有所偏差。为了解决这个问题，我们提出了 AgentComp，这是一个显式训练模型的框架，以更好地区分此类成分变化并增强其推理能力。 AgentComp 利用配备图像生成、编辑和 VQA 工具的大型语言模型的推理和工具使用功能来自主构建组合数据集。使用这些数据集，我们应用代理偏好优化方法来微调文本到图像模型，使它们能够更好地区分成分相似的样本，并产生整体更强的成分生成能力。 AgentComp 在 T2I-CompBench 等组合性基准上取得了最先进的结果，并且不影响图像质量$-$这是先前方法中的一个常见缺点$-$，甚至可以推广到未明确训练的其他功能，例如文本渲染。</li>
</ul>

<h3>Title: Explaining the Unseen: Multimodal Vision-Language Reasoning for Situational Awareness in Underground Mining Disasters</h3>
<ul>
<li><strong>Authors: </strong>Mizanur Rahman Jewel, Mohamed Elmahallawy, Sanjay Madria, Samuel Frimpong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09092">https://arxiv.org/abs/2512.09092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09092">https://arxiv.org/pdf/2512.09092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09092]] Explaining the Unseen: Multimodal Vision-Language Reasoning for Situational Awareness in Underground Mining Disasters(https://arxiv.org/abs/2512.09092)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Underground mining disasters produce pervasive darkness, dust, and collapses that obscure vision and make situational awareness difficult for humans and conventional systems. To address this, we propose MDSE, Multimodal Disaster Situation Explainer, a novel vision-language framework that automatically generates detailed textual explanations of post-disaster underground scenes. MDSE has three-fold innovations: (i) Context-Aware Cross-Attention for robust alignment of visual and textual features even under severe degradation; (ii) Segmentation-aware dual pathway visual encoding that fuses global and region-specific embeddings; and (iii) Resource-Efficient Transformer-Based Language Model for expressive caption generation with minimal compute cost. To support this task, we present the Underground Mine Disaster (UMD) dataset--the first image-caption corpus of real underground disaster scenes--enabling rigorous training and evaluation. Extensive experiments on UMD and related benchmarks show that MDSE substantially outperforms state-of-the-art captioning models, producing more accurate and contextually relevant descriptions that capture crucial details in obscured environments, improving situational awareness for underground emergency response. The code is at this https URL.</li>
<li><strong>摘要：</strong>地下采矿灾害产生普遍的黑暗、灰尘和塌方，遮蔽视线，使人类和传统系统难以进行态势感知。为了解决这个问题，我们提出了 MDSE（多模式灾难情况解释器），这是一种新颖的视觉语言框架，可以自动生成灾后地下场景的详细文本解释。 MDSE 具有三重创新：（i）上下文感知交叉注意力，即使在严重退化的情况下也能实现视觉和文本特征的稳健对齐； (ii) 融合全局和特定区域嵌入的分段感知双路径视觉编码； (iii) 资源高效的基于 Transformer 的语言模型，用于以最小的计算成本生成表达性字幕。为了支持这项任务，我们提出了地下矿难（UMD）数据集——第一个真实地下灾害场景的图像字幕语料库——支持严格的训练和评估。针对 UMD 和相关基准的大量实验表明，MDSE 的性能远远优于最先进的字幕模型，可以生成更准确且与上下文相关的描述，捕获模糊环境中的关键细节，从而提高地下应急响应的态势感知能力。代码位于此 https URL。</li>
</ul>

<h3>Title: Food Image Generation on Multi-Noun Categories</h3>
<ul>
<li><strong>Authors: </strong>Xinyue Pan, Yuhao Chen, Jiangpeng He, Fengqing Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09095">https://arxiv.org/abs/2512.09095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09095">https://arxiv.org/pdf/2512.09095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09095]] Food Image Generation on Multi-Noun Categories(https://arxiv.org/abs/2512.09095)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generating realistic food images for categories with multiple nouns is surprisingly challenging. For instance, the prompt "egg noodle" may result in images that incorrectly contain both eggs and noodles as separate entities. Multi-noun food categories are common in real-world datasets and account for a large portion of entries in benchmarks such as UEC-256. These compound names often cause generative models to misinterpret the semantics, producing unintended ingredients or objects. This is due to insufficient multi-noun category related knowledge in the text encoder and misinterpretation of multi-noun relationships, leading to incorrect spatial layouts. To overcome these challenges, we propose FoCULR (Food Category Understanding and Layout Refinement) which incorporates food domain knowledge and introduces core concepts early in the generation process. Experimental results demonstrate that the integration of these techniques improves image generation performance in the food domain.</li>
<li><strong>摘要：</strong>为具有多个名词的类别生成逼真的食物图像非常具有挑战性。例如，提示“鸡蛋面条”可能会导致图像错误地包含鸡蛋和面条作为单独的实体。多名词食物类别在现实世界的数据集中很常见，并且在 UEC-256 等基准中占据很大一部分条目。这些复合名称通常会导致生成模型误解语义，产生意想不到的成分或对象。这是由于文本编码器中多名词类别相关知识不足以及对多名词关系的误解，导致空间布局不正确。为了克服这些挑战，我们提出了 FoCULR（食品类别理解和布局细化），它结合了食品领域知识并在生成过程的早期引入了核心概念。实验结果表明，这些技术的集成提高了食品领域的图像生成性能。</li>
</ul>

<h3>Title: Learning Unmasking Policies for Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Metod Jazbec, Theo X. Olausson, Louis Béthune, Pierre Ablin, Michael Kirchhof, Joao Monterio, Victor Turrisi, Jason Ramapuram, Marco Cuturi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09106">https://arxiv.org/abs/2512.09106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09106">https://arxiv.org/pdf/2512.09106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09106]] Learning Unmasking Policies for Diffusion Language Models(https://arxiv.org/abs/2512.09106)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their autoregressive counterparts on many tasks, while holding the promise of being more efficient during inference. One particularly successful variant is masked discrete diffusion, in which a buffer filled with special mask tokens is progressively replaced with tokens sampled from the model's vocabulary. Efficiency can be gained by unmasking several tokens in parallel, but doing too many at once risks degrading the generation quality. Thus, one critical design aspect of dLLMs is the sampling procedure that selects, at each step of the diffusion process, which tokens to replace. Indeed, recent work has found that heuristic strategies such as confidence thresholding lead to both higher quality and token throughput compared to random unmasking. However, such heuristics have downsides: they require manual tuning, and we observe that their performance degrades with larger buffer sizes. In this work, we instead propose to train sampling procedures using reinforcement learning. Specifically, we formalize masked diffusion sampling as a Markov decision process in which the dLLM serves as the environment, and propose a lightweight policy architecture based on a single-layer transformer that maps dLLM token confidences to unmasking decisions. Our experiments show that these trained policies match the performance of state-of-the-art heuristics when combined with semi-autoregressive generation, while outperforming them in the full diffusion setting. We also examine the transferability of these policies, finding that they can generalize to new underlying dLLMs and longer sequence lengths. However, we also observe that their performance degrades when applied to out-of-domain data, and that fine-grained tuning of the accuracy-efficiency trade-off can be challenging with our approach.</li>
<li><strong>摘要：</strong>扩散（大型）语言模型 (dLLM) 现在在许多任务上与自回归模型的下游性能相匹配，同时有望在推理过程中提高效率。一种特别成功的变体是掩码离散扩散，其中填充特殊掩码标记的缓冲区逐渐被从模型词汇表中采样的标记替换。通过并行揭露多个代币可以提高效率，但同时揭露太多代币可能会降低生成质量。因此，dLLM 的一个关键设计方面是采样程序，该程序在扩散过程的每一步选择要替换的令牌。事实上，最近的工作发现，与随机揭露相比，置信阈值等启发式策略可以带来更高的质量和令牌吞吐量。然而，这种启发式方法也有缺点：它们需要手动调整，并且我们观察到它们的性能会随着缓冲区大小的增大而降低。在这项工作中，我们建议使用强化学习来训练采样程序。具体来说，我们将屏蔽扩散采样形式化为马尔可夫决策过程，其中 dLLM 作为环境，并提出了一种基于单层转换器的轻量级策略架构，该架构将 dLLM 令牌置信度映射到揭露决策。我们的实验表明，这些经过训练的策略与半自回归生成相结合时，与最先进的启发式方法的性能相匹配，同时在完全扩散设置中优于它们。我们还检查了这些策略的可转移性，发现它们可以推广到新的底层 dLLM 和更长的序列长度。然而，我们还观察到，当应用于域外数据时，它们的性能会下降，并且对我们的方法进行精度-效率权衡的细粒度调整可能具有挑战性。</li>
</ul>

<h3>Title: GimbalDiffusion: Gravity-Aware Camera Control for Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Frédéric Fortier-Chouinard, Yannick Hold-Geoffroy, Valentin Deschaintre, Matheus Gadelha, Jean-François Lalonde</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09112">https://arxiv.org/abs/2512.09112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09112">https://arxiv.org/pdf/2512.09112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09112]] GimbalDiffusion: Gravity-Aware Camera Control for Video Generation(https://arxiv.org/abs/2512.09112)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent progress in text-to-video generation has achieved remarkable realism, yet fine-grained control over camera motion and orientation remains elusive. Existing approaches typically encode camera trajectories through relative or ambiguous representations, limiting explicit geometric control. We introduce GimbalDiffusion, a framework that enables camera control grounded in physical-world coordinates, using gravity as a global reference. Instead of describing motion relative to previous frames, our method defines camera trajectories in an absolute coordinate system, allowing precise and interpretable control over camera parameters without requiring an initial reference frame. We leverage panoramic 360-degree videos to construct a wide variety of camera trajectories, well beyond the predominantly straight, forward-facing trajectories seen in conventional video data. To further enhance camera guidance, we introduce null-pitch conditioning, an annotation strategy that reduces the model's reliance on text content when conflicting with camera specifications (e.g., generating grass while the camera points towards the sky). Finally, we establish a benchmark for camera-aware video generation by rebalancing SpatialVID-HQ for comprehensive evaluation under wide camera pitch variation. Together, these contributions advance the controllability and robustness of text-to-video models, enabling precise, gravity-aligned camera manipulation within generative frameworks.</li>
<li><strong>摘要：</strong>文本到视频生成的最新进展已经实现了显着的真实感，但对相机运动和方向的细粒度控制仍然难以实现。现有方法通常通过相对或模糊的表示对相机轨迹进行编码，限制了显式的几何控制。我们引入了 GimbalDiffusion，这是一个框架，可以使用重力作为全局参考，以物理世界坐标为基础进行相机控制。我们的方法不是描述相对于先前帧的运动，而是在绝对坐标系中定义相机轨迹，从而允许对相机参数进行精确且可解释的控制，而无需初始参考帧。我们利用全景 360 度视频构建各种摄像机轨迹，远远超出传统视频数据中主要是直的、面向前方的轨迹。为了进一步增强相机引导，我们引入了零距调节，这是一种注释策略，可以在与相机规格冲突时减少模型对文本内容的依赖（例如，在相机指向天空时生成草地）。最后，我们通过重新平衡 SpatialVID-HQ 来建立相机感知视频生成的基准，以在宽相机间距变化下进行综合评估。这些贡献共同提高了文本到视频模型的可控性和鲁棒性，从而在生成框架内实现精确的、重力对齐的相机操作。</li>
</ul>

<h3>Title: SuperF: Neural Implicit Fields for Multi-Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Sander Riisøen Jyhne, Christian Igel, Morten Goodwin, Per-Arne Andersen, Serge Belongie, Nico Lang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09115">https://arxiv.org/abs/2512.09115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09115">https://arxiv.org/pdf/2512.09115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09115]] SuperF: Neural Implicit Fields for Multi-Image Super-Resolution(https://arxiv.org/abs/2512.09115)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>High-resolution imagery is often hindered by limitations in sensor technology, atmospheric conditions, and costs. Such challenges occur in satellite remote sensing, but also with handheld cameras, such as our smartphones. Hence, super-resolution aims to enhance the image resolution algorithmically. Since single-image super-resolution requires solving an inverse problem, such methods must exploit strong priors, e.g. learned from high-resolution training data, or be constrained by auxiliary data, e.g. by a high-resolution guide from another modality. While qualitatively pleasing, such approaches often lead to "hallucinated" structures that do not match reality. In contrast, multi-image super-resolution (MISR) aims to improve the (optical) resolution by constraining the super-resolution process with multiple views taken with sub-pixel shifts. Here, we propose SuperF, a test-time optimization approach for MISR that leverages coordinate-based neural networks, also called neural fields. Their ability to represent continuous signals with an implicit neural representation (INR) makes them an ideal fit for the MISR task. The key characteristic of our approach is to share an INR for multiple shifted low-resolution frames and to jointly optimize the frame alignment with the INR. Our approach advances related INR baselines, adopted from burst fusion for layer separation, by directly parameterizing the sub-pixel alignment as optimizable affine transformation parameters and by optimizing via a super-sampled coordinate grid that corresponds to the output resolution. Our experiments yield compelling results on simulated bursts of satellite imagery and ground-level images from handheld cameras, with upsampling factors of up to 8. A key advantage of SuperF is that this approach does not rely on any high-resolution training data.</li>
<li><strong>摘要：</strong>高分辨率图像常常受到传感器技术、大气条件和成本的限制。这些挑战不仅存在于卫星遥感领域，也存在于手持相机（例如我们的智能手机）中。因此，超分辨率旨在通过算法增强图像分辨率。由于单图像超分辨率需要解决逆问题，因此此类方法必须利用强先验，例如从高分辨率训练数据中学习，或受到辅助数据的约束，例如通过来自另一种模式的高分辨率指南。虽然在质量上令人愉悦，但这种方法常常会导致与现实不符的“幻觉”结构。相比之下，多图像超分辨率（MISR）旨在通过使用子像素移位拍摄的多个视图来约束超分辨率过程来提高（光学）分辨率。在这里，我们提出了 SuperF，一种 MISR 的测试时优化方法，它利用基于坐标的神经网络（也称为神经场）。它们能够用隐式神经表示 (INR) 表示连续信号，这使得它们非常适合 MISR 任务。我们方法的关键特征是为多个移位的低分辨率帧共享 INR，并与 INR 联合优化帧对齐。我们的方法通过直接将子像素对齐参数化为可优化的仿射变换参数，并通过与输出分辨率相对应的超采样坐标网格进行优化，改进了相关的 INR 基线，采用突发融合进行层分离。我们的实验在卫星图像和手持式摄像机地面图像的模拟突发上产生了令人信服的结果，上采样因子高达 8。SuperF 的一个关键优势是这种方法不依赖于任何高分辨率训练数据。</li>
</ul>

<h3>Title: WonderZoom: Multi-Scale 3D World Generation</h3>
<ul>
<li><strong>Authors: </strong>Jin Cao, Hong-Xing Yu, Jiajun Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09164">https://arxiv.org/abs/2512.09164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09164">https://arxiv.org/pdf/2512.09164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09164]] WonderZoom: Multi-Scale 3D World Generation(https://arxiv.org/abs/2512.09164)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to "zoom into" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in this https URL</li>
<li><strong>摘要：</strong>我们推出了 WonderZoom，这是一种从单个图像生成跨多个空间尺度内容的 3D 场景的新颖方法。现有的 3D 世界生成模型仍然仅限于单尺度合成，无法生成不同粒度的连贯场景内容。根本挑战是缺乏能够生成和渲染空间大小差异很大的内容的缩放感知 3D 表示。 WonderZoom 通过两项关键创新解决了这个问题：(1) 用于生成和实时渲染多尺度 3D 场景的尺度自适应高斯面元，以及 (2) 渐进式细节合成器，可迭代生成更精细尺度的 3D 内容。我们的方法使用户能够“放大”3D 区域并自动回归合成以前不存在的精细细节（从景观到微观特征）。实验表明，WonderZoom 在质量和对齐方面显着优于最先进的视频和 3D 模型，从而能够从单个图像创建多尺度 3D 世界。我们在此 https URL 中展示了生成的多尺度 3D 世界的视频结果和交互式查看器</li>
</ul>

<h3>Title: Learning Patient-Specific Disease Dynamics with Latent Flow Matching for Longitudinal Imaging Generation</h3>
<ul>
<li><strong>Authors: </strong>Hao Chen, Rui Yin, Yifan Chen, Qi Chen, Chao Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09185">https://arxiv.org/abs/2512.09185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09185">https://arxiv.org/pdf/2512.09185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09185]] Learning Patient-Specific Disease Dynamics with Latent Flow Matching for Longitudinal Imaging Generation(https://arxiv.org/abs/2512.09185)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Understanding disease progression is a central clinical challenge with direct implications for early diagnosis and personalized treatment. While recent generative approaches have attempted to model progression, key mismatches remain: disease dynamics are inherently continuous and monotonic, yet latent representations are often scattered, lacking semantic structure, and diffusion-based models disrupt continuity with random denoising process. In this work, we propose to treat the disease dynamic as a velocity field and leverage Flow Matching (FM) to align the temporal evolution of patient data. Unlike prior methods, it captures the intrinsic dynamic of disease, making the progression more interpretable. However, a key challenge remains: in latent space, Auto-Encoders (AEs) do not guarantee alignment across patients or correlation with clinical-severity indicators (e.g., age and disease conditions). To address this, we propose to learn patient-specific latent alignment, which enforces patient trajectories to lie along a specific axis, with magnitude increasing monotonically with disease severity. This leads to a consistent and semantically meaningful latent space. Together, we present $\Delta$-LFM, a framework for modeling patient-specific latent progression with flow matching. Across three longitudinal MRI benchmarks, $\Delta$-LFM demonstrates strong empirical performance and, more importantly, offers a new framework for interpreting and visualizing disease dynamics.</li>
<li><strong>摘要：</strong>了解疾病进展是一项重要的临床挑战，对早期诊断和个性化治疗具有直接影响。虽然最近的生成方法试图对进展进行建模，但关键的不匹配仍然存在：疾病动态本质上是连续和单调的，但潜在表示往往是分散的，缺乏语义结构，并且基于扩散的模型破坏了随机去噪过程的连续性。在这项工作中，我们建议将疾病动态视为速度场，并利用流量匹配（FM）来调整患者数据的时间演变。与之前的方法不同，它捕捉了疾病的内在动态，使疾病的进展更容易解释。然而，一个关键的挑战仍然存在：在潜在空间中，自动编码器（AE）不能保证患者之间的一致性或与临床严重程度指标（例如年龄和疾病状况）的相关性。为了解决这个问题，我们建议学习患者特定的潜在对齐，这强制患者的轨迹沿着特定的轴，其幅度随着疾病的严重程度单调增加。这导致了一致且语义上有意义的潜在空间。我们共同提出了 $\Delta$-LFM，这是一个通过流量匹配对患者特定的潜在进展进行建模的框架。在三个纵向 MRI 基准中，$\Delta$-LFM 表现出了强大的实证性能，更重要的是，它提供了一个解释和可视化疾病动态的新框架。</li>
</ul>

<h3>Title: LLMs for Analog Circuit Design Continuum (ACDC)</h3>
<ul>
<li><strong>Authors: </strong>Yasaman Esfandiari, Jocelyn Rego, Austin Meyer, Jonathan Gallagher, Mia Levy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09199">https://arxiv.org/abs/2512.09199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09199">https://arxiv.org/pdf/2512.09199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09199]] LLMs for Analog Circuit Design Continuum (ACDC)(https://arxiv.org/abs/2512.09199)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) and transformer architectures have shown impressive reasoning and generation capabilities across diverse natural language tasks. However, their reliability and robustness in real-world engineering domains remain largely unexplored, limiting their practical utility in human-centric workflows. In this work, we investigate the applicability and consistency of LLMs for analog circuit design -- a task requiring domain-specific reasoning, adherence to physical constraints, and structured representations -- focusing on AI-assisted design where humans remain in the loop. We study how different data representations influence model behavior and compare smaller models (e.g., T5, GPT-2) with larger foundation models (e.g., Mistral-7B, GPT-oss-20B) under varying training conditions. Our results highlight key reliability challenges, including sensitivity to data format, instability in generated designs, and limited generalization to unseen circuit configurations. These findings provide early evidence on the limits and potential of LLMs as tools to enhance human capabilities in complex engineering tasks, offering insights into designing reliable, deployable foundation models for structured, real-world applications.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 和转换器架构在不同的自然语言任务中表现出了令人印象深刻的推理和生成能力。然而，它们在现实工程领域中的可靠性和稳健性在很大程度上仍未得到探索，限制了它们在以人为中心的工作流程中的实际用途。在这项工作中，我们研究了法学硕士在模拟电路设计中的适用性和一致性——一项需要特定领域推理、遵守物理约束和结构化表示的任务——重点关注人类参与循环的人工智能辅助设计。我们研究不同的数据表示如何影响模型行为，并在不同的训练条件下将较小的模型（例如 T5、GPT-2）与较大的基础模型（例如 Mistral-7B、GPT-oss-20B）进行比较。我们的结果突出了关键的可靠性挑战，包括对数据格式的敏感性、生成设计的不稳定性以及对看不见的电路配置的有限泛化。这些发现为法学硕士作为增强人类复杂工程任务能力的工具的局限性和潜力提供了早期证据，为为结构化的现实应用程序设计可靠、可部署的基础模型提供了见解。</li>
</ul>

<h3>Title: Enabling Next-Generation Consumer Experience with Feature Coding for Machines</h3>
<ul>
<li><strong>Authors: </strong>Md Eimran Hossain Eimon, Juan Merlos, Ashan Perera, Hari Kalva, Velibor Adzic, Borko Furht</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09232">https://arxiv.org/abs/2512.09232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09232">https://arxiv.org/pdf/2512.09232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09232]] Enabling Next-Generation Consumer Experience with Feature Coding for Machines(https://arxiv.org/abs/2512.09232)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As consumer devices become increasingly intelligent and interconnected, efficient data transfer solutions for machine tasks have become essential. This paper presents an overview of the latest Feature Coding for Machines (FCM) standard, part of MPEG-AI and developed by the Moving Picture Experts Group (MPEG). FCM supports AI-driven applications by enabling the efficient extraction, compression, and transmission of intermediate neural network features. By offloading computationally intensive operations to base servers with high computing resources, FCM allows low-powered devices to leverage large deep learning models. Experimental results indicate that the FCM standard maintains the same level of accuracy while reducing bitrate requirements by 75.90% compared to remote inference.</li>
<li><strong>摘要：</strong>随着消费设备变得越来越智能和互联，机器任务的高效数据传输解决方案变得至关重要。本文概述了最新的机器特征编码 (FCM) 标准，该标准是 MPEG-AI 的一部分，由运动图像专家组 (MPEG) 开发。 FCM 通过实现中间神经网络特征的高效提取、压缩和传输来支持人工智能驱动的应用程序。通过将计算密集型操作卸载到具有高计算资源的基础服务器，FCM 允许低功耗设备利用大型深度学习模型。实验结果表明，与远程推理相比，FCM 标准在保持相同水平的精度的同时，将码率要求降低了 75.90%。</li>
</ul>

<h3>Title: OmniPSD: Layered PSD Generation with Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Cheng Liu, Yiren Song, Haofan Wang, Mike Zheng Shou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09247">https://arxiv.org/abs/2512.09247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09247">https://arxiv.org/pdf/2512.09247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09247]] OmniPSD: Layered PSD Generation with Diffusion Transformer(https://arxiv.org/abs/2512.09247)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models have greatly improved image generation and editing, yet generating or reconstructing layered PSD files with transparent alpha channels remains highly challenging. We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning. For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention, producing semantically coherent and hierarchically structured layers. For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image. An RGBA-VAE is employed as an auxiliary representation module to preserve transparency without affecting structure learning. Extensive experiments on our new RGBA-layered dataset demonstrate that OmniPSD achieves high-fidelity generation, structural consistency, and transparency awareness, offering a new paradigm for layered design generation and decomposition with diffusion transformers.</li>
<li><strong>摘要：</strong>扩散模型的最新进展极大地改进了图像生成和编辑，但生成或重建具有透明 Alpha 通道的分层 PSD 文件仍然极具挑战性。我们提出了 OmniPSD，这是一个基于 Flux 生态系统构建的统一扩散框架，可通过上下文学习实现文本到 PSD 生成和图像到 PSD 分解。对于文本到 PSD 的生成，OmniPSD 将多个目标图层在空间上排列到单个画布中，并通过空间注意力学习它们的组成关系，从而生成语义连贯和层次结构的图层。对于图像到 PSD 的分解，它执行迭代上下文编辑，逐步提取和擦除文本和前景组件，以从单个展平图像重建可编辑的 PSD 图层。采用 RGBA-VAE 作为辅助表示模块，以在不影响结构学习的情况下保持透明度。对我们新的 RGBA 分层数据集的大量实验表明，OmniPSD 实现了高保真生成、结构一致性和透明度感知，为分层设计生成和扩散变压器分解提供了新的范例。</li>
</ul>

<h3>Title: LongT2IBench: A Benchmark for Evaluating Long Text-to-Image Generation with Graph-structured Annotations</h3>
<ul>
<li><strong>Authors: </strong>Zhichao Yang, Tianjiao Gu, Jianjie Wang, Feiyu Lin, Xiangfei Sheng, Pengfei Chen, Leida Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09271">https://arxiv.org/abs/2512.09271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09271">https://arxiv.org/pdf/2512.09271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09271]] LongT2IBench: A Benchmark for Evaluating Long Text-to-Image Generation with Graph-structured Annotations(https://arxiv.org/abs/2512.09271)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The increasing popularity of long Text-to-Image (T2I) generation has created an urgent need for automatic and interpretable models that can evaluate the image-text alignment in long prompt scenarios. However, the existing T2I alignment benchmarks predominantly focus on short prompt scenarios and only provide MOS or Likert scale annotations. This inherent limitation hinders the development of long T2I evaluators, particularly in terms of the interpretability of alignment. In this study, we contribute LongT2IBench, which comprises 14K long text-image pairs accompanied by graph-structured human annotations. Given the detail-intensive nature of long prompts, we first design a Generate-Refine-Qualify annotation protocol to convert them into textual graph structures that encompass entities, attributes, and relations. Through this transformation, fine-grained alignment annotations are achieved based on these granular elements. Finally, the graph-structed annotations are converted into alignment scores and interpretations to facilitate the design of T2I evaluation models. Based on LongT2IBench, we further propose LongT2IExpert, a LongT2I evaluator that enables multi-modal large language models (MLLMs) to provide both quantitative scores and structured interpretations through an instruction-tuning process with Hierarchical Alignment Chain-of-Thought (CoT). Extensive experiments and comparisons demonstrate the superiority of the proposed LongT2IExpert in alignment evaluation and interpretation. Data and code have been released in this https URL.</li>
<li><strong>摘要：</strong>长文本到图像（T2I）生成的日益普及，迫切需要能够在长提示场景中评估图像文本对齐的自动且可解释的模型。然而，现有的T2I对齐基准主要关注短提示场景，并且仅提供MOS或Likert量表注释。这种固有的限制阻碍了长 T2I 评估器的发展，特别是在对齐的可解释性方面。在这项研究中，我们贡献了 LongT2IBench，它包含 14K 长文本图像对，并附有图形结构的人工注释。考虑到长提示的细节密集性，我们首先设计了一个Generate-Refine-Qualify注释协议，将它们转换为包含实体、属性和关系的文本图形结构。通过这种转换，基于这些粒度元素实现了细粒度的对齐注释。最后，图结构注释被转换为对齐分数和解释，以方便 T2I 评估模型的设计。基于LongT2IBench，我们进一步提出了LongT2IExpert，一种LongT2I评估器，它使多模态大语言模型（MLLM）能够通过层次对齐思想链（CoT）的指令调整过程提供定量分数和结构化解释。大量的实验和比较证明了所提出的 LongT2IExpert 在对齐评估和解释方面的优越性。数据和代码已在此 https URL 中发布。</li>
</ul>

<h3>Title: Dynamic Facial Expressions Analysis Based Parkinson's Disease Auxiliary Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Xiaochen Huang, Xiaochen Bi, Cuihua Lv, Xin Wang, Haoyan Zhang, Wenjing Jiang, Xin Ma, Yibin Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09276">https://arxiv.org/abs/2512.09276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09276">https://arxiv.org/pdf/2512.09276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09276]] Dynamic Facial Expressions Analysis Based Parkinson's Disease Auxiliary Diagnosis(https://arxiv.org/abs/2512.09276)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Parkinson's disease (PD), a prevalent neurodegenerative disorder, significantly affects patients' daily functioning and social interactions. To facilitate a more efficient and accessible diagnostic approach for PD, we propose a dynamic facial expression analysis-based PD auxiliary diagnosis method. This method targets hypomimia, a characteristic clinical symptom of PD, by analyzing two manifestations: reduced facial expressivity and facial rigidity, thereby facilitating the diagnosis process. We develop a multimodal facial expression analysis network to extract expression intensity features during patients' performance of various facial expressions. This network leverages the CLIP architecture to integrate visual and textual features while preserving the temporal dynamics of facial expressions. Subsequently, the expression intensity features are processed and input into an LSTM-based classification network for PD diagnosis. Our method achieves an accuracy of 93.1%, outperforming other in-vitro PD diagnostic approaches. This technique offers a more convenient detection method for potential PD patients, improving their diagnostic experience.</li>
<li><strong>摘要：</strong>帕金森病 (PD) 是一种常见的神经退行性疾病，显着影响患者的日常功能和社交互动。为了促进更有效和更容易的PD诊断方法，我们提出了一种基于动态面部表情分析的PD辅助诊断方法。该方法针对帕金森病的特征性临床症状——低语症，通过分析面部表情减少和面部僵硬这两种表现，从而促进诊断过程。我们开发了一个多模态面部表情分析网络，以提取患者表现各种面部表情时的表情强度特征。该网络利用 CLIP 架构集成视觉和文本特征，同时保留面部表情的时间动态。随后，对表达强度特征进行处理并输入到基于 LSTM 的分类网络中进行 PD 诊断。我们的方法的准确率达到 93.1%，优于其他体外 PD 诊断方法。该技术为潜在的PD患者提供了更便捷的检测方法，改善了他们的诊断体验。</li>
</ul>

<h3>Title: FoundIR-v2: Optimizing Pre-Training Data Mixtures for Image Restoration Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Xiang Chen, Jinshan Pan, Jiangxin Dong, Jian Yang, Jinhui Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09282">https://arxiv.org/abs/2512.09282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09282">https://arxiv.org/pdf/2512.09282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09282]] FoundIR-v2: Optimizing Pre-Training Data Mixtures for Image Restoration Foundation Model(https://arxiv.org/abs/2512.09282)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Recent studies have witnessed significant advances in image restoration foundation models driven by improvements in the scale and quality of pre-training data. In this work, we find that the data mixture proportions from different restoration tasks are also a critical factor directly determining the overall performance of all-in-one image restoration models. To this end, we propose a high-capacity diffusion-based image restoration foundation model, FoundIR-v2, which adopts a data equilibrium scheduling paradigm to dynamically optimize the proportions of mixed training datasets from different tasks. By leveraging the data mixing law, our method ensures a balanced dataset composition, enabling the model to achieve consistent generalization and comprehensive performance across diverse tasks. Furthermore, we introduce an effective Mixture-of-Experts (MoE)-driven scheduler into generative pre-training to flexibly allocate task-adaptive diffusion priors for each restoration task, accounting for the distinct degradation forms and levels exhibited by different tasks. Extensive experiments demonstrate that our method can address over 50 sub-tasks across a broader scope of real-world scenarios and achieves favorable performance against state-of-the-art approaches.</li>
<li><strong>摘要：</strong>最近的研究见证了在预训练数据规模和质量改进的推动下，图像恢复基础模型取得了重大进展。在这项工作中，我们发现不同恢复任务的数据混合比例也是直接决定一体化图像恢复模型整体性能的关键因素。为此，我们提出了一种基于高容量扩散的图像恢复基础模型FoundIR-v2，该模型采用数据平衡调度范式来动态优化来自不同任务的混合训练数据集的比例。通过利用数据混合定律，我们的方法确保了平衡的数据集组成，使模型能够在不同的任务中实现一致的泛化和综合性能。此外，我们在生成预训练中引入了有效的专家混合（MoE）驱动的调度程序，为每个恢复任务灵活分配任务自适应扩散先验，考虑到不同任务表现出的不同退化形式和水平。大量实验表明，我们的方法可以在更广泛的现实世界场景中解决 50 多个子任务，并与最先进的方法相比取得了良好的性能。</li>
</ul>

<h3>Title: VABench: A Comprehensive Benchmark for Audio-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Daili Hua, Xizhi Wang, Bohan Zeng, Xinyi Huang, Hao Liang, Junbo Niu, Xinlong Chen, Quanqing Xu, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09299">https://arxiv.org/abs/2512.09299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09299">https://arxiv.org/pdf/2512.09299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09299]] VABench: A Comprehensive Benchmark for Audio-Video Generation(https://arxiv.org/abs/2512.09299)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in video generation have been remarkable, enabling models to produce visually compelling videos with synchronized audio. While existing video generation benchmarks provide comprehensive metrics for visual quality, they lack convincing evaluations for audio-video generation, especially for models aiming to generate synchronized audio-video outputs. To address this gap, we introduce VABench, a comprehensive and multi-dimensional benchmark framework designed to systematically evaluate the capabilities of synchronous audio-video generation. VABench encompasses three primary task types: text-to-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It further establishes two major evaluation modules covering 15 dimensions. These dimensions specifically assess pairwise similarities (text-video, text-audio, video-audio), audio-video synchronization, lip-speech consistency, and carefully curated audio and video question-answering (QA) pairs, among others. Furthermore, VABench covers seven major content categories: animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds. We provide a systematic analysis and visualization of the evaluation results, aiming to establish a new standard for assessing video generation models with synchronous audio capabilities and to promote the comprehensive advancement of the field.</li>
<li><strong>摘要：</strong>视频生成方面的最新进展非常显着，使模型能够生成具有同步音频的视觉上引人注目的视频。虽然现有的视频生成基准提供了视觉质量的全面指标，但它们缺乏对音频视频生成的令人信服的评估，特别是对于旨在生成同步音频视频输出的模型。为了解决这一差距，我们引入了 VABench，这是一个全面的、多维度的基准框架，旨在系统地评估同步音视频生成的能力。 VABench 包含三种主要任务类型：文本到音频视频 (T2AV)、图像到音频视频 (I2AV) 和立体声音频视频生成。进一步建立了涵盖15个维度的两大评价模块。这些维度专门评估成对相似性（文本-视频、文本-音频、视频-音频）、音频-视频同步、唇语一致性以及精心策划的音频和视频问答 (QA) 对等。此外，VABench涵盖七大内容类别：动物、人声、音乐、环境声音、同步物理声音、复杂场景和虚拟世界。我们对评估结果进行系统分析和可视化，旨在建立评估具有同步音频能力的视频生成模型的新标准，推动该领域的全面进步。</li>
</ul>

<h3>Title: UniLS: End-to-End Audio-Driven Avatars for Unified Listening and Speaking</h3>
<ul>
<li><strong>Authors: </strong>Xuangeng Chu, Ruicong Liu, Yifei Huang, Yun Liu, Yichen Peng, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09327">https://arxiv.org/abs/2512.09327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09327">https://arxiv.org/pdf/2512.09327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09327]] UniLS: End-to-End Audio-Driven Avatars for Unified Listening and Speaking(https://arxiv.org/abs/2512.09327)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating lifelike conversational avatars requires modeling not just isolated speakers, but the dynamic, reciprocal interaction of speaking and listening. However, modeling the listener is exceptionally challenging: direct audio-driven training fails, producing stiff, static listening motions. This failure stems from a fundamental imbalance: the speaker's motion is strongly driven by speech audio, while the listener's motion primarily follows an internal motion prior and is only loosely guided by external speech. This challenge has led most methods to focus on speak-only generation. The only prior attempt at joint generation relies on extra speaker's motion to produce the listener. This design is not end-to-end, thereby hindering the real-time applicability. To address this limitation, we present UniLS, the first end-to-end framework for generating unified speak-listen expressions, driven by only dual-track audio. Our method introduces a novel two-stage training paradigm. Stage 1 first learns the internal motion prior by training an audio-free autoregressive generator, capturing the spontaneous dynamics of natural facial motion. Stage 2 then introduces the dual-track audio, fine-tuning the generator to modulate the learned motion prior based on external speech cues. Extensive evaluations show UniLS achieves state-of-the-art speaking accuracy. More importantly, it delivers up to 44.1\% improvement in listening metrics, generating significantly more diverse and natural listening expressions. This effectively mitigates the stiffness problem and provides a practical, high-fidelity audio-driven solution for interactive digital humans.</li>
<li><strong>摘要：</strong>生成逼真的对话化身不仅需要对孤立的说话者进行建模，还需要对说话和倾听的动态、交互交互进行建模。然而，对听者进行建模非常具有挑战性：直接音频驱动的训练失败，会产生僵硬、静态的聆听动作。这种失败源于一种根本性的不平衡：说话者的运动由语音音频强烈驱动，而听者的运动主要遵循先验的内部运动，并且仅由外部语音松散地引导。这一挑战导致大多数方法都专注于仅说话的生成。联合生成的唯一先前尝试依赖于额外的说话者的运动来产生听者。这种设计不是端到端的，从而阻碍了实时适用性。为了解决这个限制，我们提出了 UniLS，这是第一个用于生成统一的说-听表达式的端到端框架，仅由双轨音频驱动。我们的方法引入了一种新颖的两阶段训练范例。第一阶段首先通过训练无音频自回归发生器来学习内部运动，捕捉自然面部运动的自发动态。然后，第二阶段引入双轨音频，微调发生器以根据外部语音提示先验调制学习到的运动。广泛的评估表明 UniLS 实现了最先进的口语准确性。更重要的是，它的听力指标提升高达 44.1%，产生更加多样化和自然的听力表达。这有效地缓解了僵硬问题，并为交互式数字人类提供了实用的、高保真音频驱动的解决方案。</li>
</ul>

<h3>Title: StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Ke Xing, Longfei Li, Yuyang Yin, Hanwen Liang, Guixun Luo, Chen Fang, Jue Wang, Konstantinos N. Plataniotis, Xiaojie Jin, Yao Zhao, Yunchao Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09363">https://arxiv.org/abs/2512.09363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09363">https://arxiv.org/pdf/2512.09363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09363]] StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation(https://arxiv.org/abs/2512.09363)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at this https URL.</li>
<li><strong>摘要：</strong>XR 设备的日益普及推动了对高质量立体视频的强劲需求，但其生产成本仍然很高且容易出现伪影。为了应对这一挑战，我们提出了 StereoWorld，这是一个端到端框架，它重新利用预训练的视频生成器来生成高保真单目到立体视频。我们的框架在单目视频输入上联合调节模型，同时通过几何感知正则化明确监督生成，以确保 3D 结构保真度。进一步集成时空切片方案，以实现高效、高分辨率的合成。为了实现大规模训练和评估，我们策划了一个高清立体视频数据集，其中包含超过 11M 帧，与自然人类瞳距 (IPD) 对齐。大量实验表明，StereoWorld 的性能大大优于现有方法，可生成具有卓越视觉保真度和几何一致性的立体视频。项目网页可通过此 https URL 获取。</li>
</ul>

<h3>Title: ASSIST-3D: Adapted Scene Synthesis for Class-Agnostic 3D Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Shengchao Zhou, Jiehong Lin, Jiahui Liu, Shizhen Zhao, Chirui Chang, Xiaojuan Qi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09364">https://arxiv.org/abs/2512.09364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09364">https://arxiv.org/pdf/2512.09364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09364]] ASSIST-3D: Adapted Scene Synthesis for Class-Agnostic 3D Instance Segmentation(https://arxiv.org/abs/2512.09364)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Class-agnostic 3D instance segmentation tackles the challenging task of segmenting all object instances, including previously unseen ones, without semantic class reliance. Current methods struggle with generalization due to the scarce annotated 3D scene data or noisy 2D segmentations. While synthetic data generation offers a promising solution, existing 3D scene synthesis methods fail to simultaneously satisfy geometry diversity, context complexity, and layout reasonability, each essential for this task. To address these needs, we propose an Adapted 3D Scene Synthesis pipeline for class-agnostic 3D Instance SegmenTation, termed as ASSIST-3D, to synthesize proper data for model generalization enhancement. Specifically, ASSIST-3D features three key innovations, including 1) Heterogeneous Object Selection from extensive 3D CAD asset collections, incorporating randomness in object sampling to maximize geometric and contextual diversity; 2) Scene Layout Generation through LLM-guided spatial reasoning combined with depth-first search for reasonable object placements; and 3) Realistic Point Cloud Construction via multi-view RGB-D image rendering and fusion from the synthetic scenes, closely mimicking real-world sensor data acquisition. Experiments on ScanNetV2, ScanNet++, and S3DIS benchmarks demonstrate that models trained with ASSIST-3D-generated data significantly outperform existing methods. Further comparisons underscore the superiority of our purpose-built pipeline over existing 3D scene synthesis approaches.</li>
<li><strong>摘要：</strong>与类无关的 3D 实例分割解决了在不依赖语义类的情况下分割所有对象实例（包括以前未见过的实例）的挑战性任务。由于缺乏带注释的 3D 场景数据或嘈杂的 2D 分割，当前的方法难以泛化。虽然合成数据生成提供了一个有前途的解决方案，但现有的 3D 场景合成方法无法同时满足几何多样性、上下文复杂性和布局合理性，而这三者对于该任务至关重要。为了满足这些需求，我们提出了一种用于与类无关的 3D 实例分段的自适应 3D 场景合成管道（称为 ASSIST-3D），以合成适当的数据以增强模型泛化能力。具体而言，ASSIST-3D 具有三项关键创新，包括 1) 从广泛的 3D CAD 资产集合中选择异构对象，将随机性纳入对象采样中，以最大限度地提高几何和上下文多样性； 2）通过LLM引导的空间推理结合深度优先搜索合理的对象放置来生成场景布局； 3) 通过多视图 RGB-D 图像渲染和合成场景融合来构建真实点云，紧密模仿真实世界的传感器数据采集。 ScanNetV2、ScanNet++ 和 S3DIS 基准测试的实验表明，使用 ASSIST-3D 生成的数据训练的模型显着优于现有方法。进一步的比较强调了我们专门构建的管道相对于现有 3D 场景合成方法的优越性。</li>
</ul>

<h3>Title: Black-Box Behavioral Distillation Breaks Safety Alignment in Medical LLMs</h3>
<ul>
<li><strong>Authors: </strong>Sohely Jahan, Ruimin Sun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09403">https://arxiv.org/abs/2512.09403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09403">https://arxiv.org/pdf/2512.09403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09403]] Black-Box Behavioral Distillation Breaks Safety Alignment in Medical LLMs(https://arxiv.org/abs/2512.09403)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>As medical large language models (LLMs) become increasingly integrated into clinical workflows, concerns around alignment robustness, and safety are escalating. Prior work on model extraction has focused on classification models or memorization leakage, leaving the vulnerability of safety-aligned generative medical LLMs underexplored. We present a black-box distillation attack that replicates the domain-specific reasoning of safety-aligned medical LLMs using only output-level access. By issuing 48,000 instruction queries to Meditron-7B and collecting 25,000 benign instruction response pairs, we fine-tune a LLaMA3 8B surrogate via parameter efficient LoRA under a zero-alignment supervision setting, requiring no access to model weights, safety filters, or training data. With a cost of $12, the surrogate achieves strong fidelity on benign inputs while producing unsafe completions for 86% of adversarial prompts, far exceeding both Meditron-7B (66%) and the untuned base model (46%). This reveals a pronounced functional-ethical gap, task utility transfers, while alignment collapses. To analyze this collapse, we develop a dynamic adversarial evaluation framework combining Generative Query (GQ)-based harmful prompt generation, verifier filtering, category-wise failure analysis, and adaptive Random Search (RS) jailbreak attacks. We also propose a layered defense system, as a prototype detector for real-time alignment drift in black-box deployments. Our findings show that benign-only black-box distillation exposes a practical and under-recognized threat: adversaries can cheaply replicate medical LLM capabilities while stripping safety mechanisms, underscoring the need for extraction-aware safety monitoring.</li>
<li><strong>摘要：</strong>随着医学大语言模型 (LLM) 越来越多地集成到临床工作流程中，对对齐鲁棒性和安全性的担忧不断升级。之前关于模型提取的工作主要集中在分类模型或记忆泄漏上，导致安全性生成医学法学硕士的脆弱性尚未得到充分探索。我们提出了一种黑盒蒸馏攻击，仅使用输出级访问来复制安全相关的医学法学硕士的特定领域推理。通过向 Meditron-7B 发出 48,000 个指令查询并收集 25,000 个良性指令响应对，我们在零对齐监督设置下通过参数高效的 LoRA 微调 LLaMA3 8B 代理，无需访问模型权重、安全过滤器或训练数据。该代理模型以 12 美元的成本实现了对良性输入的高度保真度，同时对 86% 的对抗性提示产生不安全的完成结果，远远超过 Meditron-7B (66%) 和未调整的基础模型 (46%)。这揭示了明显的功能-伦理差距、任务效用转移，而一致性崩溃。为了分析这种崩溃，我们开发了一个动态对抗性评估框架，结合了基于生成查询（GQ）的有害提示生成、验证者过滤、分类故障分析和自适应随机搜索（RS）越狱攻击。我们还提出了一种分层防御系统，作为黑盒部署中实时对准漂移的原型探测器。我们的研究结果表明，良性的黑盒蒸馏暴露了一个实际且未被充分认识的威胁：对手可以廉价地复制医学法学硕士能力，同时剥离安全机制，这凸显了提取感知安全监控的必要性。</li>
</ul>

<h3>Title: Generative Point Cloud Registration</h3>
<ul>
<li><strong>Authors: </strong>Haobo Jiang, Jin Xie, Jian Yang, Liang Yu, Jianmin Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09407">https://arxiv.org/abs/2512.09407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09407">https://arxiv.org/pdf/2512.09407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09407]] Generative Point Cloud Registration(https://arxiv.org/abs/2512.09407)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a novel 3D registration paradigm, Generative Point Cloud Registration, which bridges advanced 2D generative models with 3D matching tasks to enhance registration performance. Our key idea is to generate cross-view consistent image pairs that are well-aligned with the source and target point clouds, enabling geometry-color feature fusion to facilitate robust matching. To ensure high-quality matching, the generated image pair should feature both 2D-3D geometric consistency and cross-view texture consistency. To achieve this, we introduce Match-ControlNet, a matching-specific, controllable 2D generative model. Specifically, it leverages the depth-conditioned generation capability of ControlNet to produce images that are geometrically aligned with depth maps derived from point clouds, ensuring 2D-3D geometric consistency. Additionally, by incorporating a coupled conditional denoising scheme and coupled prompt guidance, Match-ControlNet further promotes cross-view feature interaction, guiding texture consistency generation. Our generative 3D registration paradigm is general and could be seamlessly integrated into various registration methods to enhance their performance. Extensive experiments on 3DMatch and ScanNet datasets verify the effectiveness of our approach.</li>
<li><strong>摘要：</strong>在本文中，我们提出了一种新颖的 3D 配准范例，即生成点云配准，它将先进的 2D 生成模型与 3D 匹配任务连接起来，以提高配准性能。我们的关键思想是生成与源点云和目标点云良好对齐的跨视图一致图像对，从而实现几何颜色特征融合以促进稳健匹配。为了确保高质量匹配，生成的图像对应同时具有 2D-3D 几何一致性和跨视图纹理一致性。为了实现这一目标，我们引入了 Match-ControlNet，这是一种特定于匹配的可控 2D 生成模型。具体来说，它利用 ControlNet 的深度条件生成功能来生成与从点云导出的深度图几何对齐的图像，确保 2D-3D 几何一致性。此外，通过结合耦合条件去噪方案和耦合提示指导，Match-ControlNet 进一步促进跨视图特征交互，指导纹理一致性生成。我们的生成 3D 配准范例是通用的，可以无缝集成到各种配准方法中以增强其性能。对 3DMatch 和 ScanNet 数据集的大量实验验证了我们方法的有效性。</li>
</ul>

<h3>Title: Label-free Motion-Conditioned Diffusion Model for Cardiac Ultrasound Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Zhe Li, Hadrien Reynaud, Johanna P Müller, Bernhard Kainz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09418">https://arxiv.org/abs/2512.09418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09418">https://arxiv.org/pdf/2512.09418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09418]] Label-free Motion-Conditioned Diffusion Model for Cardiac Ultrasound Synthesis(https://arxiv.org/abs/2512.09418)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Ultrasound echocardiography is essential for the non-invasive, real-time assessment of cardiac function, but the scarcity of labelled data, driven by privacy restrictions and the complexity of expert annotation, remains a major obstacle for deep learning methods. We propose the Motion Conditioned Diffusion Model (MCDM), a label-free latent diffusion framework that synthesises realistic echocardiography videos conditioned on self-supervised motion features. To extract these features, we design the Motion and Appearance Feature Extractor (MAFE), which disentangles motion and appearance representations from videos. Feature learning is further enhanced by two auxiliary objectives: a re-identification loss guided by pseudo appearance features and an optical flow loss guided by pseudo flow fields. Evaluated on the EchoNet-Dynamic dataset, MCDM achieves competitive video generation performance, producing temporally coherent and clinically realistic sequences without reliance on manual labels. These results demonstrate the potential of self-supervised conditioning for scalable echocardiography synthesis. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>超声心动图对于心功能的非侵入性实时评估至关重要，但由于隐私限制和专家注释的复杂性，标记数据的稀缺仍然是深度学习方法的主要障碍。我们提出了运动条件扩散模型（MCDM），这是一种无标签的潜在扩散框架，可以根据自监督运动特征合成真实的超声心动图视频。为了提取这些特征，我们设计了运动和外观特征提取器（MAFE），它可以从视频中分离出运动和外观表示。特征学习通过两个辅助目标进一步增强：由伪外观特征引导的重新识别损失和由伪流场引导的光流损失。在 EchoNet-Dynamic 数据集上进行评估，MCDM 实现了具有竞争力的视频生成性能，无需依赖手动标签即可生成时间连贯且临床真实的序列。这些结果证明了自我监督调节对于可扩展的超声心动图合成的潜力。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: FunPhase: A Periodic Functional Autoencoder for Motion Generation via Phase Manifolds</h3>
<ul>
<li><strong>Authors: </strong>Marco Pegoraro, Evan Atherton, Bruno Roy, Aliasghar Khani, Arianna Rampini</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09423">https://arxiv.org/abs/2512.09423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09423">https://arxiv.org/pdf/2512.09423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09423]] FunPhase: A Periodic Functional Autoencoder for Motion Generation via Phase Manifolds(https://arxiv.org/abs/2512.09423)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generation</a></li>
<li><strong>Abstract: </strong>Learning natural body motion remains challenging due to the strong coupling between spatial geometry and temporal dynamics. Embedding motion in phase manifolds, latent spaces that capture local periodicity, has proven effective for motion prediction; however, existing approaches lack scalability and remain confined to specific settings. We introduce FunPhase, a functional periodic autoencoder that learns a phase manifold for motion and replaces discrete temporal decoding with a function-space formulation, enabling smooth trajectories that can be sampled at arbitrary temporal resolutions. FunPhase supports downstream tasks such as super-resolution and partial-body motion completion, generalizes across skeletons and datasets, and unifies motion prediction and generation within a single interpretable manifold. Our model achieves substantially lower reconstruction error than prior periodic autoencoder baselines while enabling a broader range of applications and performing on par with state-of-the-art motion generation methods.</li>
<li><strong>摘要：</strong>由于空间几何和时间动力学之间的强耦合，学习自然的身体运动仍然具有挑战性。将运动嵌入相位流形（捕获局部周期性的潜在空间）中已被证明对于运动预测是有效的；然而，现有方法缺乏可扩展性，并且仍然局限于特定设置。我们引入了 FunPhase，一种功能周期性自动编码器，它学习运动的相位流形，并用函数空间公式代替离散时间解码，从而实现可以以任意时间分辨率采样的平滑轨迹。 FunPhase 支持下游任务，例如超分辨率和部分身体运动完成，跨骨架和数据集进行泛化，并将运动预测和生成统一在单个可解释流形中。我们的模型实现了比之前的周期性自动编码器基线低得多的重建误差，同时支持更广泛的应用，并且与最先进的运动生成方法性能相当。</li>
</ul>

<h3>Title: UniPart: Part-Level 3D Generation with Unified 3D Geom-Seg Latents</h3>
<ul>
<li><strong>Authors: </strong>Xufan He, Yushuang Wu, Xiaoyang Guo, Chongjie Ye, Jiaqing Zhou, Tianlei Hu, Xiaoguang Han, Dong Du</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09435">https://arxiv.org/abs/2512.09435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09435">https://arxiv.org/pdf/2512.09435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09435]] UniPart: Part-Level 3D Generation with Unified 3D Geom-Seg Latents(https://arxiv.org/abs/2512.09435)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Part-level 3D generation is essential for applications requiring decomposable and structured 3D synthesis. However, existing methods either rely on implicit part segmentation with limited granularity control or depend on strong external segmenters trained on large annotated datasets. In this work, we observe that part awareness emerges naturally during whole-object geometry learning and propose Geom-Seg VecSet, a unified geometry-segmentation latent representation that jointly encodes object geometry and part-level structure. Building on this representation, we introduce UniPart, a two-stage latent diffusion framework for image-guided part-level 3D generation. The first stage performs joint geometry generation and latent part segmentation, while the second stage conditions part-level diffusion on both whole-object and part-specific latents. A dual-space generation scheme further enhances geometric fidelity by predicting part latents in both global and canonical spaces. Extensive experiments demonstrate that UniPart achieves superior segmentation controllability and part-level geometric quality compared with existing approaches.</li>
<li><strong>摘要：</strong>零件级 3D 生成对于需要可分解和结构化 3D 合成的应用至关重要。然而，现有的方法要么依赖于具有有限粒度控制的隐式部分分割，要么依赖于在大型注释数据集上训练的强大的外部分割器。在这项工作中，我们观察到零件意识在整个对象几何学习过程中自然出现，并提出了 Geom-Seg VecSet，一种统一的几何分割潜在表示，可联合编码对象几何和零件级结构。在此表示的基础上，我们引入了 UniPart，这是一种用于图像引导的零件级 3D 生成的两阶段潜在扩散框架。第一阶段执行联合几何生成和潜在部分分割，而第二阶段则对整个对象和部分特定潜在部分进行部分级扩散。双空间生成方案通过预测全局和规范空间中的部分潜伏进一步增强了几何保真度。大量实验表明，与现有方法相比，UniPart 实现了卓越的分割可控性和零件级几何质量。</li>
</ul>

<h3>Title: Color encoding in Latent Space of Stable Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Guillem Arias, Ariadna Solà, Martí Armengod, Maria Vanrell</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09477">https://arxiv.org/abs/2512.09477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09477">https://arxiv.org/pdf/2512.09477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09477]] Color encoding in Latent Space of Stable Diffusion Models(https://arxiv.org/abs/2512.09477)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion-based generative models have achieved remarkable visual fidelity, yet a detailed understanding of how specific perceptual attributes - such as color and shape - are internally represented remains limited. This work explores how color is encoded in a generative model through a systematic analysis of the latent representations in Stable Diffusion. Through controlled synthetic datasets, principal component analysis (PCA) and similarity metrics, we reveal that color information is encoded along circular, opponent axes predominantly captured in latent channels c_3 and c_4, whereas intensity and shape are primarily represented in channels c_1 and c_2. Our findings indicate that the latent space of Stable Diffusion exhibits an interpretable structure aligned with a efficient coding representation. These insights provide a foundation for future work in model understanding, editing applications, and the design of more disentangled generative frameworks.</li>
<li><strong>摘要：</strong>基于扩散的生成模型的最新进展已经实现了显着的视觉保真度，但对特定感知属性（例如颜色和形状）如何在内部表示的详细理解仍然有限。这项工作通过对稳定扩散中的潜在表示进行系统分析，探讨了如何在生成模型中编码颜色。通过受控合成数据集、主成分分析 (PCA) 和相似性度量，我们揭示了颜色信息沿着圆形对立轴编码，主要在潜在通道 c_3 和 c_4 中捕获，而强度和形状主要在通道 c_1 和 c_2 中表示。我们的研究结果表明，稳定扩散的潜在空间表现出与有效编码表示相一致的可解释结构。这些见解为未来模型理解、编辑应用程序和设计更清晰的生成框架的工作奠定了基础。</li>
</ul>

<h3>Title: Masked Registration and Autoencoding of CT Images for Predictive Tibia Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Hongyou Zhou, Cederic Aßmann, Alaa Bejaoui, Heiko Tzschätzsch, Mark Heyland, Julian Zierke, Niklas Tuttle, Sebastian Hölzl, Timo Auer, David A. Back, Marc Toussaint</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09525">https://arxiv.org/abs/2512.09525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09525">https://arxiv.org/pdf/2512.09525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09525]] Masked Registration and Autoencoding of CT Images for Predictive Tibia Reconstruction(https://arxiv.org/abs/2512.09525)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Surgical planning for complex tibial fractures can be challenging for surgeons, as the 3D structure of the later desirable bone alignment may be diffi- cult to imagine. To assist in such planning, we address the challenge of predicting a patient-specific reconstruction target from a CT of the fractured tibia. Our ap- proach combines neural registration and autoencoder models. Specifically, we first train a modified spatial transformer network (STN) to register a raw CT to a standardized coordinate system of a jointly trained tibia prototype. Subsequently, various autoencoder (AE) architectures are trained to model healthy tibial varia- tions. Both the STN and AE models are further designed to be robust to masked input, allowing us to apply them to fractured CTs and decode to a prediction of the patient-specific healthy bone in standard coordinates. Our contributions include: i) a 3D-adapted STN for global spatial registration, ii) a comparative analysis of AEs for bone CT modeling, and iii) the extension of both to handle masked inputs for predictive generation of healthy bone structures. Project page: this https URL</li>
<li><strong>摘要：</strong>复杂胫骨骨折的手术计划对于外科医生来说可能具有挑战性，因为随后所需的骨排列的 3D 结构可能很难想象。为了协助制定此类计划，我们解决了根据骨折胫骨 CT 预测患者特异性重建目标的挑战。我们的方法结合了神经配准和自动编码器模型。具体来说，我们首先训练一个改进的空间变换网络（STN），将原始 CT 注册到联合训练的胫骨原型的标准化坐标系上。随后，各种自动编码器（AE）架构被训练来模拟健康的胫骨变化。 STN 和 AE 模型都经过进一步设计，对屏蔽输入具有鲁棒性，使我们能够将它们应用于骨折 CT 并解码以在标准坐标中预测患者特定的健康骨骼。我们的贡献包括：i) 用于全局空间配准的 3D 适应 STN，ii) 用于骨 CT 建模的 AE 的比较分析，以及 iii) 两者的扩展以处理屏蔽输入以预测生成健康骨骼结构。项目页面：此 https URL</li>
</ul>

<h3>Title: Latent-Autoregressive GP-VAE Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yves Ruffenach</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09535">https://arxiv.org/abs/2512.09535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09535">https://arxiv.org/pdf/2512.09535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09535]] Latent-Autoregressive GP-VAE Language Model(https://arxiv.org/abs/2512.09535)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We investigate a fully Latent AutoRegressive scheme based on a Gaussian Process (GP) integrated into a Variational Autoencoder (VAE). In this setting, sequential dynamics are transferred from the observation space to a continuous latent space, while linguistic generation remains parallel through a non-autoregressive decoder. We present a complete methodological formulation, including a causal GP prior, a structured amortized posterior, and a training protocol based on a regularized ELBO. Empirical evaluation, conducted within a deliberately constrained proof-of-concept (POC) framework, shows that the model can be trained stably and that the sequential and parallel sampling variants exhibit consistent behavior. Overall, the results suggest that part of the temporal structure in a language model can be supported by the probabilistic geometry of the latent space rather than by explicit neural operations.</li>
<li><strong>摘要：</strong>我们研究了一种基于集成到变分自编码器（VAE）中的高斯过程（GP）的完全潜在自回归方案。在这种情况下，顺序动力学从观察空间转移到连续的潜在空间，而语言生成通过非自回归解码器保持并行。我们提出了一个完整的方法论公式，包括因果 GP 先验、结构化摊销后验和基于正则化 ELBO 的训练协议。在刻意约束的概念验证 (POC) 框架内进行的实证评估表明，该模型可以稳定地进行训练，并且顺序和并行采样变体表现出一致的行为。总体而言，结果表明语言模型中的部分时间结构可以由潜在空间的概率几何支持，而不是由显式神经操作支持。</li>
</ul>

<h3>Title: A Dual-Domain Convolutional Network for Hyperspectral Single-Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Murat Karayaka, Usman Muhammad, Jorma Laaksonen, Md Ziaul Hoque, Tapio Seppänen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09546">https://arxiv.org/abs/2512.09546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09546">https://arxiv.org/pdf/2512.09546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09546]] A Dual-Domain Convolutional Network for Hyperspectral Single-Image Super-Resolution(https://arxiv.org/abs/2512.09546)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>This study presents a lightweight dual-domain super-resolution network (DDSRNet) that combines Spatial-Net with the discrete wavelet transform (DWT). Specifically, our proposed model comprises three main components: (1) a shallow feature extraction module, termed Spatial-Net, which performs residual learning and bilinear interpolation; (2) a low-frequency enhancement branch based on the DWT that refines coarse image structures; and (3) a shared high-frequency refinement branch that simultaneously enhances the LH (horizontal), HL (vertical), and HH (diagonal) wavelet subbands using a single CNN with shared weights. As a result, the DWT enables subband decomposition, while the inverse DWT reconstructs the final high-resolution output. By doing so, the integration of spatial- and frequency-domain learning enables DDSRNet to achieve highly competitive performance with low computational cost on three hyperspectral image datasets, demonstrating its effectiveness for hyperspectral image super-resolution.</li>
<li><strong>摘要：</strong>本研究提出了一种轻量级双域超分辨率网络（DDSRNet），它将空间网络与离散小波变换（DWT）相结合。具体来说，我们提出的模型包括三个主要组件：（1）浅层特征提取模块，称为空间网络，它执行残差学习和双线性插值； (2)基于DWT的低频增强分支，细化粗糙的图像结构； (3) 共享高频细化分支，使用具有共享权重的单个 CNN 同时增强 LH（水平）、HL（垂直）和 HH（对角）小波子带。因此，DWT 可以实现子带分解，而逆 DWT 则可以重构最终的高分辨率输出。通过这样做，空间域和频域学习的集成使DDSRNet能够在三个高光谱图像数据集上以较低的计算成本实现极具竞争力的性能，证明了其在高光谱图像超分辨率方面的有效性。</li>
</ul>

<h3>Title: Building Reasonable Inference for Vision-Language Models in Blind Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Yuan Li, Zitang Sun, Yen-ju Chen, Shin'ya Nishida</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09555">https://arxiv.org/abs/2512.09555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09555">https://arxiv.org/pdf/2512.09555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09555]] Building Reasonable Inference for Vision-Language Models in Blind Image Quality Assessment(https://arxiv.org/abs/2512.09555)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Recent progress in BIQA has been driven by VLMs, whose semantic reasoning abilities suggest that they might extract visual features, generate descriptive text, and infer quality in a human-like manner. However, these models often produce textual descriptions that contradict their final quality predictions, and the predicted scores can change unstably during inference - behaviors not aligned with human reasoning. To understand these issues, we analyze the factors that cause contradictory assessments and instability. We first estimate the relationship between the final quality predictions and the generated visual features, finding that the predictions are not fully grounded in the features and that the logical connection between them is weak. Moreover, decoding intermediate VLM layers shows that the model frequently relies on a limited set of candidate tokens, which contributes to prediction instability. To encourage more human-like reasoning, we introduce a two-stage tuning method that explicitly separates visual perception from quality inference. In the first stage, the model learns visual features; in the second, it infers quality solely from these features. Experiments on SPAQ and KONIQ demonstrate that our approach reduces prediction instability from 22.00% to 12.39% and achieves average gains of 0.3124/0.3507 in SRCC/PLCC across LIVE, CSIQ, SPAQ, and KONIQ compared to the baseline. Further analyses show that our method improves both stability and the reliability of the inference process.</li>
<li><strong>摘要：</strong>BIQA 的最新进展是由 VLM 推动的，其语义推理能力表明它们可以提取视觉特征、生成描述性文本并以类似人类的方式推断质量。然而，这些模型通常会产生与其最终质量预测相矛盾的文本描述，并且预测分数在推理过程中可能会不稳定地变化 - 与人类推理不相符的行为。为了理解这些问题，我们分析了导致矛盾评估和不稳定的因素。我们首先估计最终质量预测和生成的视觉特征之间的关系，发现预测并不完全基于特征，并且它们之间的逻辑联系很弱。此外，解码中间 VLM 层表明该模型经常依赖于一组有限的候选标记，这会导致预测不稳定。为了鼓励更加类似于人类的推理，我们引入了一种两阶段调整方法，该方法将视觉感知与质量推理明确分开。在第一阶段，模型学习视觉特征；第二种，它仅从这些特征来推断质量。 SPAQ 和 KONIQ 上的实验表明，与基线相比，我们的方法将预测不稳定性从 22.00% 降低到 12.39%，并且在 LIVE、CSIQ、SPAQ 和 KONIQ 的 SRCC/PLCC 中实现了 0.3124/0.3507 的平均增益。进一步的分析表明，我们的方法提高了推理过程的稳定性和可靠性。</li>
</ul>

<h3>Title: From Graphs to Gates: DNS-HyXNet, A Lightweight and Deployable Sequential Model for Real-Time DNS Tunnel Detection</h3>
<ul>
<li><strong>Authors: </strong>Faraz Ali, Muhammad Afaq, Mahmood Niazi, Muzammil Behzad</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09565">https://arxiv.org/abs/2512.09565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09565">https://arxiv.org/pdf/2512.09565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09565]] From Graphs to Gates: DNS-HyXNet, A Lightweight and Deployable Sequential Model for Real-Time DNS Tunnel Detection(https://arxiv.org/abs/2512.09565)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Domain Name System (DNS) tunneling remains a covert channel for data exfiltration and command-and-control communication. Although graph-based methods such as GraphTunnel achieve strong accuracy, they introduce significant latency and computational overhead due to recursive parsing and graph construction, limiting their suitability for real-time deployment. This work presents DNS-HyXNet, a lightweight extended Long Short-Term Memory (xLSTM) hybrid framework designed for efficient sequence-based DNS tunnel detection. DNS-HyXNet integrates tokenized domain embeddings with normalized numerical DNS features and processes them through a two-layer xLSTM network that directly learns temporal dependencies from packet sequences, eliminating the need for graph reconstruction and enabling single-stage multi-class classification. The model was trained and evaluated on two public benchmark datasets with carefully tuned hyperparameters to ensure low memory consumption and fast inference. Across all experimental splits of the DNS-Tunnel-Datasets, DNS-HyXNet achieved up to 99.99% accuracy, with macro-averaged precision, recall, and F1-scores exceeding 99.96%, and demonstrated a per-sample detection latency of just 0.041 ms, confirming its scalability and real-time readiness. These results show that sequential modeling with xLSTM can effectively replace computationally expensive recursive graph generation, offering a deployable and energy-efficient alternative for real-time DNS tunnel detection on commodity hardware.</li>
<li><strong>摘要：</strong>域名系统 (DNS) 隧道仍然是数据泄露和命令与控制通信的隐蔽通道。尽管 GraphTunnel 等基于图的方法具有很高的准确性，但由于递归解析和图构造，它们引入了显着的延迟和计算开销，限制了它们对实时部署的适用性。这项工作提出了 DNS-HyXNet，这是一种轻量级扩展长短期记忆 (xLSTM) 混合框架，专为高效的基于序列的 DNS 隧道检测而设计。 DNS-HyXNet 将标记化域嵌入与标准化数值 DNS 特征集成在一起，并通过两层 xLSTM 网络对其进行处理，该网络直接从数据包序列中学习时间依赖性，从而无需图形重建并实现单阶段多类分类。该模型在两个公共基准数据集上进行了训练和评估，并仔细调整了超参数，以确保低内存消耗和快速推理。在 DNS-Tunnel-Datasets 的所有实验分割中，DNS-HyXNet 的准确率高达 99.99%，宏观平均准确率、召回率和 F1 分数超过 99.96%，并且每个样本的检测延迟仅为 0.041 毫秒，证实了其可扩展性和实时就绪性。这些结果表明，使用 xLSTM 进行顺序建模可以有效地取代计算成本高昂的递归图生成，为商用硬件上的实时 DNS 隧道检测提供可部署且节能的替代方案。</li>
</ul>

<h3>Title: Investigate the Low-level Visual Perception in Vision-Language based Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Yuan Li, Zitang Sun, Yen-Ju Chen, Shin'ya Nishida</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09573">https://arxiv.org/abs/2512.09573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09573">https://arxiv.org/pdf/2512.09573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09573]] Investigate the Low-level Visual Perception in Vision-Language based Image Quality Assessment(https://arxiv.org/abs/2512.09573)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Recent advances in Image Quality Assessment (IQA) have leveraged Multi-modal Large Language Models (MLLMs) to generate descriptive explanations. However, despite their strong visual perception modules, these models often fail to reliably detect basic low-level distortions such as blur, noise, and compression, and may produce inconsistent evaluations across repeated inferences. This raises an essential question: do MLLM-based IQA systems truly perceive the visual features that matter? To examine this issue, we introduce a low-level distortion perception task that requires models to classify specific distortion types. Our component-wise analysis shows that although MLLMs are structurally capable of representing such distortions, they tend to overfit training templates, leading to biases in quality scoring. As a result, critical low-level features are weakened or lost during the vision-language alignment transfer stage. Furthermore, by computing the semantic distance between visual features and corresponding semantic tokens before and after component-wise fine-tuning, we show that improving the alignment of the vision encoder dramatically enhances distortion recognition accuracy, increasing it from 14.92% to 84.43%. Overall, these findings indicate that incorporating dedicated constraints on the vision encoder can strengthen text-explainable visual representations and enable MLLM-based pipelines to produce more coherent and interpretable reasoning in vision-centric tasks.</li>
<li><strong>摘要：</strong>图像质量评估 (IQA) 的最新进展利用多模态大型语言模型 (MLLM) 来生成描述性解释。然而，尽管它们具有强大的视觉感知模块，但这些模型通常无法可靠地检测基本的低级失真，例如模糊、噪声和压缩，并且可能在重复推理中产生不一致的评估。这就提出了一个基本问题：基于 MLLM 的 IQA 系统能否真正感知重要的视觉特征？为了研究这个问题，我们引入了一个低级失真感知任务，该任务需要模型对特定失真类型进行分类。我们的组件分析表明，尽管 MLLM 在结构上能够表示这种扭曲，但它们往往会过度拟合训练模板，从而导致质量评分出现偏差。因此，在视觉语言对齐转移阶段，关键的低级特征被削弱或丢失。此外，通过计算逐组件微调之前和之后的视觉特征和相应语义标记之间的语义距离，我们表明，改进视觉编码器的对齐可以显着提高失真识别精度，将其从 14.92% 提高到 84.43%。总的来说，这些发现表明，在视觉编码器上加入专用约束可以增强文本可解释的视觉表示，并使基于 MLLM 的管道能够在以视觉为中心的任务中产生更连贯和可解释的推理。</li>
</ul>

<h3>Title: FROMAT: Multiview Material Appearance Transfer via Few-Shot Self-Attention Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Hubert Kompanowski, Varun Jampani, Aaryaman Vasishta, Binh-Son Hua</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09617">https://arxiv.org/abs/2512.09617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09617">https://arxiv.org/pdf/2512.09617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09617]] FROMAT: Multiview Material Appearance Transfer via Few-Shot Self-Attention Adaptation(https://arxiv.org/abs/2512.09617)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Multiview diffusion models have rapidly emerged as a powerful tool for content creation with spatial consistency across viewpoints, offering rich visual realism without requiring explicit geometry and appearance representation. However, compared to meshes or radiance fields, existing multiview diffusion models offer limited appearance manipulation, particularly in terms of material, texture, or style. In this paper, we present a lightweight adaptation technique for appearance transfer in multiview diffusion models. Our method learns to combine object identity from an input image with appearance cues rendered in a separate reference image, producing multi-view-consistent output that reflects the desired materials, textures, or styles. This allows explicit specification of appearance parameters at generation time while preserving the underlying object geometry and view coherence. We leverage three diffusion denoising processes responsible for generating the original object, the reference, and the target images, and perform reverse sampling to aggregate a small subset of layer-wise self-attention features from the object and the reference to influence the target generation. Our method requires only a few training examples to introduce appearance awareness to pretrained multiview models. The experiments show that our method provides a simple yet effective way toward multiview generation with diverse appearance, advocating the adoption of implicit generative 3D representations in practice.</li>
<li><strong>摘要：</strong>多视图扩散模型已迅速成为一种强大的内容创建工具，具有跨视点的空间一致性，无需显式几何和外观表示即可提供丰富的视觉真实感。然而，与网格或辐射场相比，现有的多视图扩散模型提供的外观操作有限，特别是在材料、纹理或风格方面。在本文中，我们提出了一种用于多视图扩散模型中的外观迁移的轻量级自适应技术。我们的方法学习将输入图像中的对象标识与单独参考图像中渲染的外观线索相结合，生成反映所需材质、纹理或样式的多视图一致输出。这允许在生成时明确指定外观参数，同时保留底层对象几何形状和视图一致性。我们利用三个扩散去噪过程负责生成原始对象、参考图像和目标图像，并执行反向采样以聚合来自对象和参考的分层自注意力特征的小子集以影响目标生成。我们的方法只需要几个训练示例即可将外观意识引入预训练的多视图模型。实验表明，我们的方法为具有不同外观的多视图生成提供了一种简单而有效的方法，提倡在实践中采用隐式生成 3D 表示。</li>
</ul>

<h3>Title: VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification</h3>
<ul>
<li><strong>Authors: </strong>Wanyue Zhang, Lin Geng Foo, Thabo Beeler, Rishabh Dabral, Christian Theobalt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09646">https://arxiv.org/abs/2512.09646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09646">https://arxiv.org/pdf/2512.09646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09646]] VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification(https://arxiv.org/abs/2512.09646)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Synthesizing realistic human-object interactions (HOI) in video is challenging due to the complex, instance-specific interaction dynamics of both humans and objects. Incorporating controllability in video generation further adds to the complexity. Existing controllable video generation approaches face a trade-off: sparse controls like keypoint trajectories are easy to specify but lack instance-awareness, while dense signals such as optical flow, depths or 3D meshes are informative but costly to obtain. We propose VHOI, a two-stage framework that first densifies sparse trajectories into HOI mask sequences, and then fine-tunes a video diffusion model conditioned on these dense masks. We introduce a novel HOI-aware motion representation that uses color encodings to distinguish not only human and object motion, but also body-part-specific dynamics. This design incorporates a human prior into the conditioning signal and strengthens the model's ability to understand and generate realistic HOI dynamics. Experiments demonstrate state-of-the-art results in controllable HOI video generation. VHOI is not limited to interaction-only scenarios and can also generate full human navigation leading up to object interactions in an end-to-end manner. Project page: this https URL.</li>
<li><strong>摘要：</strong>由于人类和物体的复杂的、特定于实例的交互动态，在视频中合成真实的人与物体交互 (HOI) 具有挑战性。在视频生成中纳入可控性进一步增加了复杂性。现有的可控视频生成方法面临着一个权衡：关键点轨迹等稀疏控制很容易指定，但缺乏实例感知，而光流、深度或 3D 网格等密集信号信息丰富，但获取成本高昂。我们提出了 VHOI，这是一个两阶段框架，首先将稀疏轨迹致密为 HOI 掩码序列，然后根据这些致密掩码对视频扩散模型进行微调。我们引入了一种新颖的 HOI 感知运动表示，它使用颜色编码不仅可以区分人类和物体的运动，还可以区分特定于身体部位的动态。该设计将人类先验融入调节信号中，并增强了模型理解和生成真实 HOI 动态的能力。实验证明了可控 HOI 视频生成的最先进结果。 VHOI 不仅限于仅交互场景，还可以生成完整的人类导航，从而以端到端的方式进行对象交互。项目页面：此 https URL。</li>
</ul>

<h3>Title: Membership and Dataset Inference Attacks on Large Audio Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Jakub Proboszcz, Paweł Kochanski, Karol Korszun, Donato Crisostomi, Giorgio Strano, Emanuele Rodolà, Kamil Deja, Jan Dubinski</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09654">https://arxiv.org/abs/2512.09654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09654">https://arxiv.org/pdf/2512.09654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09654]] Membership and Dataset Inference Attacks on Large Audio Generative Models(https://arxiv.org/abs/2512.09654)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative audio models, based on diffusion and autoregressive architectures, have advanced rapidly in both quality and expressiveness. This progress, however, raises pressing copyright concerns, as such models are often trained on vast corpora of artistic and commercial works. A central question is whether one can reliably verify if an artist's material was included in training, thereby providing a means for copyright holders to protect their content. In this work, we investigate the feasibility of such verification through membership inference attacks (MIA) on open-source generative audio models, which attempt to determine whether a specific audio sample was part of the training set. Our empirical results show that membership inference alone is of limited effectiveness at scale, as the per-sample membership signal is weak for models trained on large and diverse datasets. However, artists and media owners typically hold collections of works rather than isolated samples. Building on prior work in text and vision domains, in this work we focus on dataset inference (DI), which aggregates diverse membership evidence across multiple samples. We find that DI is successful in the audio domain, offering a more practical mechanism for assessing whether an artist's works contributed to model training. Our results suggest DI as a promising direction for copyright protection and dataset accountability in the era of large audio generative models.</li>
<li><strong>摘要：</strong>基于扩散和自回归架构的生成音频模型在质量和表现力方面都取得了快速进步。然而，这一进展引起了紧迫的版权问题，因为此类模型通常是在大量艺术和商业作品上进行训练的。一个核心问题是，人们是否能够可靠地验证艺术家的材料是否包含在培训中，从而为版权所有者提供一种保护其内容的方法。在这项工作中，我们通过对开源生成音频模型的成员推理攻击（MIA）来调查这种验证的可行性，该攻击试图确定特定的音频样本是否是训练集的一部分。我们的实证结果表明，单独的隶属推断在规模上的有效性有限，因为对于在大型和多样化数据集上训练的模型来说，每个样本的隶属信号很弱。然而，艺术家和媒体所有者通常持有作品集而不是孤立的样本。在文本和视觉领域的先前工作的基础上，在这项工作中，我们重点关注数据集推断（DI），它聚合了多个样本中的不同成员资格证据。我们发现 DI 在音频领域取得了成功，为评估艺术家的作品是否对模型训练做出了贡献提供了更实用的机制。我们的结果表明 DI 是大型音频生成模型时代版权保护和数据集责任的一个有前途的方向。</li>
</ul>

<h3>Title: IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting</h3>
<ul>
<li><strong>Authors: </strong>Tao Zhang, Yuyang Hong, Yang Xia, Kun Ding, Zeyu Zhang, Ying Wang, Shiming Xiang, Chunhong Pan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09663">https://arxiv.org/abs/2512.09663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09663">https://arxiv.org/pdf/2512.09663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09663]] IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting(https://arxiv.org/abs/2512.09663)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs. The benchmark and code are available at this https URL.</li>
<li><strong>摘要：</strong>多模式大语言模型 (MLLM) 的最新进展在各种基准测试中取得了令人印象深刻的进展。然而，它们理解红外图像的能力仍有待探索。为了解决这一差距，我们推出了 IF-Bench，这是第一个专为评估红外图像的多模态理解而设计的高质量基准。 IF-Bench 包含来自 23 个红外数据集的 499 张图像和 680 个精心策划的视觉问答对，涵盖图像理解的 10 个基本维度。在此基准基础上，我们系统地评估了40多个开源和闭源MLLM，采用循环评估、双语评估和混合判断策略来提高结果的可靠性。我们的分析揭示了模型规模、架构和推理范式如何影响红外图像理解，为该领域提供了宝贵的见解。此外，我们提出了一种免训练的生成视觉提示（GenViP）方法，该方法利用先进的图像编辑模型将红外图像转换为语义和空间对齐的 RGB 对应图像，从而减轻域分布变化。大量实验表明，我们的方法在各种 MLLM 中始终能带来显着的性能改进。基准测试和代码可从此 https URL 获取。</li>
</ul>

<h3>Title: An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Gil Weissman, Amir Ivry, Israel Cohen</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09670">https://arxiv.org/abs/2512.09670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09670">https://arxiv.org/pdf/2512.09670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09670]] An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence(https://arxiv.org/abs/2512.09670)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The proliferation of satellite constellations, coupled with reduced tasking latency and diverse sensor capabilities, has expanded the opportunities for automated Earth observation. This paper introduces a fully automated Tip-and-Cue framework designed for satellite imaging tasking and scheduling. In this context, tips are generated from external data sources or analyses of prior satellite imagery, identifying spatiotemporal targets and prioritizing them for downstream planning. Corresponding cues are the imaging tasks formulated in response, which incorporate sensor constraints, timing requirements, and utility functions. The system autonomously generates candidate tasks, optimizes their scheduling across multiple satellites using continuous utility functions that reflect the expected value of each observation, and processes the resulting imagery using artificial-intelligence-based models, including object detectors and vision-language models. Structured visual reports are generated to support both interpretability and the identification of new insights for downstream tasking. The efficacy of the framework is demonstrated through a maritime vessel tracking scenario, utilizing Automatic Identification System (AIS) data for trajectory prediction, targeted observations, and the generation of actionable outputs. Maritime vessel tracking is a widely researched application, often used to benchmark novel approaches to satellite tasking, forecasting, and analysis. The system is extensible to broader applications such as smart-city monitoring and disaster response, where timely tasking and automated analysis are critical.</li>
<li><strong>摘要：</strong>卫星星座的激增，加上任务延迟的减少和传感器功能的多样化，扩大了自动化地球观测的机会。本文介绍了一种专为卫星成像任务分配和调度而设计的全自动 Tip-and-Cue 框架。在这种情况下，提示是根据外部数据源或对先前卫星图像的分析生成的，识别时空目标并优先考虑它们以进行下游规划。相应的线索是响应中制定的成像任务，其中包含传感器约束、时序要求和实用函数。该系统自动生成候选任务，使用反映每次观测预期值的连续效用函数优化多颗卫星的调度，并使用基于人工智能的模型（包括物体探测器和视觉语言模型）处理生成的图像。生成结构化可视化报告以支持可解释性和识别下游任务的新见解。该框架的有效性通过海上船舶跟踪场景得到了证明，利用自动识别系统（AIS）数据进行轨迹预测、有针对性的观察和生成可操作的输出。海上船舶跟踪是一项广泛研究的应用，通常用于对卫星任务分配、预测和分析的新方法进行基准测试。该系统可扩展到更广泛的应用，例如智能城市监控和灾难响应，其中及时的任务分配和自动分析至关重要。</li>
</ul>

<h3>Title: Unconsciously Forget: Mitigating Memorization; Without Knowing What is being Memorized</h3>
<ul>
<li><strong>Authors: </strong>Er Jin, Yang Zhang, Yongli Mou, Yanfei Dong, Stefan Decker, Kenji Kawaguchi, Johannes Stegmaier</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09687">https://arxiv.org/abs/2512.09687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09687">https://arxiv.org/pdf/2512.09687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09687]] Unconsciously Forget: Mitigating Memorization; Without Knowing What is being Memorized(https://arxiv.org/abs/2512.09687)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative models have demonstrated an exceptional ability to produce highly realistic images. However, previous studies show that generated images often resemble the training data, and this problem becomes more severe as the model size increases. Memorizing training data can lead to legal challenges, including copyright infringement, violations of portrait rights, and trademark violations. Existing approaches to mitigating memorization mainly focus on manipulating the denoising sampling process to steer image embeddings away from the memorized embedding space or employ unlearning methods that require training on datasets containing specific sets of memorized concepts. However, existing methods often incur substantial computational overhead during sampling, or focus narrowly on removing one or more groups of target concepts, imposing a significant limitation on their scalability. To understand and mitigate these problems, our work, UniForget, offers a new perspective on understanding the root cause of memorization. Our work demonstrates that specific parts of the model are responsible for copyrighted content generation. By applying model pruning, we can effectively suppress the probability of generating copyrighted content without targeting specific concepts while preserving the general generative capabilities of the model. Additionally, we show that our approach is both orthogonal and complementary to existing unlearning methods, thereby highlighting its potential to improve current unlearning and de-memorization techniques.</li>
<li><strong>摘要：</strong>生成模型的最新进展已经证明了生成高度逼真图像的卓越能力。然而，之前的研究表明，生成的图像通常与训练数据相似，并且随着模型尺寸的增加，这个问题变得更加严重。记住训练数据可能会导致法律挑战，包括侵犯版权、侵犯肖像权和侵犯商标权。现有的减轻记忆的方法主要集中在操纵去噪采样过程，以引导图像嵌入远离记忆的嵌入空间，或者采用需要对包含特定记忆概念集的数据集进行训练的忘却方法。然而，现有方法通常在采样过程中产生大量计算开销，或者狭隘地专注于删除一组或多组目标概念，从而对其可扩展性造成重大限制。为了理解和缓解这些问题，我们的工作 UniForget 为理解记忆的根本原因提供了新的视角。我们的工作表明模型的特定部分负责版权内容的生成。通过应用模型剪枝，我们可以在不针对特定概念的情况下有效抑制生成版权内容的概率，同时保留模型的一般生成能力。此外，我们表明我们的方法与现有的遗忘方法既正交又互补，从而突出了其改进当前遗忘和去记忆技术的潜力。</li>
</ul>

<h3>Title: DynaIP: Dynamic Image Prompt Adapter for Scalable Zero-shot Personalized Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhizhong Wang, Tianyi Chu, Zeyi Huang, Nanyang Wang, Kehan Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09814">https://arxiv.org/abs/2512.09814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09814">https://arxiv.org/pdf/2512.09814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09814]] DynaIP: Dynamic Image Prompt Adapter for Scalable Zero-shot Personalized Text-to-Image Generation(https://arxiv.org/abs/2512.09814)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Personalized Text-to-Image (PT2I) generation aims to produce customized images based on reference images. A prominent interest pertains to the integration of an image prompt adapter to facilitate zero-shot PT2I without test-time fine-tuning. However, current methods grapple with three fundamental challenges: 1. the elusive equilibrium between Concept Preservation (CP) and Prompt Following (PF), 2. the difficulty in retaining fine-grained concept details in reference images, and 3. the restricted scalability to extend to multi-subject personalization. To tackle these challenges, we present Dynamic Image Prompt Adapter (DynaIP), a cutting-edge plugin to enhance the fine-grained concept fidelity, CP-PF balance, and subject scalability of SOTA T2I multimodal diffusion transformers (MM-DiT) for PT2I generation. Our key finding is that MM-DiT inherently exhibit decoupling learning behavior when injecting reference image features into its dual branches via cross attentions. Based on this, we design an innovative Dynamic Decoupling Strategy that removes the interference of concept-agnostic information during inference, significantly enhancing the CP-PF balance and further bolstering the scalability of multi-subject compositions. Moreover, we identify the visual encoder as a key factor affecting fine-grained CP and reveal that the hierarchical features of commonly used CLIP can capture visual information at diverse granularity levels. Therefore, we introduce a novel Hierarchical Mixture-of-Experts Feature Fusion Module to fully leverage the hierarchical features of CLIP, remarkably elevating the fine-grained concept fidelity while also providing flexible control of visual granularity. Extensive experiments across single- and multi-subject PT2I tasks verify that our DynaIP outperforms existing approaches, marking a notable advancement in the field of PT2l generation.</li>
<li><strong>摘要：</strong>个性化文本到图像 (PT2I) 生成旨在根据参考图像生成定制图像。一个突出的兴趣涉及图像提示适配器的集成，以促进零样本 PT2I，而无需测试时间微调。然而，当前的方法面临三个基本挑战：1.概念保留（CP）和提示跟随（PF）之间难以捉摸的平衡，2.在参考图像中保留细粒度概念细节的困难，以及3.扩展到多主体个性化的有限可扩展性。为了应对这些挑战，我们推出了动态图像提示适配器 (DynaIP)，这是一个尖端插件，可增强用于 PT2I 生成的 SOTA T2I 多模态扩散变压器 (MM-DiT) 的细粒度概念保真度、CP-PF 平衡和主题可扩展性。我们的主要发现是，当通过交叉注意力将参考图像特征注入其双分支时，MM-DiT 本质上表现出解耦学习行为。基于此，我们设计了一种创新的动态解耦策略，消除了推理过程中概念不可知信息的干扰，显着增强了 CP-PF 平衡，并进一步增强了多主题组合的可扩展性。此外，我们将视觉编码器确定为影响细粒度 CP 的关键因素，并揭示常用 CLIP 的分层特征可以捕获不同粒度级别的视觉信息。因此，我们引入了一种新颖的分层专家混合特征融合模块，以充分利用 CLIP 的分层特征，显着提高细粒度概念保真度，同时提供视觉粒度的灵活控制。跨单主体和多主体 PT2I 任务的广泛实验验证了我们的 DynaIP 优于现有方法，标志着 PT2l 生成领域的显着进步。</li>
</ul>

<h3>Title: UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Hao Lu, Ziyang Liu, Guangfeng Jiang, Yuanfei Luo, Sheng Chen, Yangang Zhang, Ying-Cong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09864">https://arxiv.org/abs/2512.09864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09864">https://arxiv.org/pdf/2512.09864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09864]] UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving(https://arxiv.org/abs/2512.09864)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.</li>
<li><strong>摘要：</strong>由于有限的世界知识和薄弱的视觉动态建模，自动驾驶（AD）系统在长尾场景中举步维艰。现有的基于视觉-语言-动作（VLA）的方法无法利用未标记的视频进行视觉因果学习，而基于世界模型的方法缺乏大型语言模型的推理能力。在本文中，我们构建了多个专门的数据集，为复杂场景提供推理和规划注释。然后，提出了一个名为 UniUGP 的统一理解-生成-规划框架，通过混合专家架构来协同场景推理、未来视频生成和轨迹规划。通过集成预先训练的 VLM 和视频生成模型，UniUGP 利用视觉动态和语义推理来提高规划性能。以多帧观察和语言指令作为输入，它产生可解释的思维链推理、物理上一致的轨迹和连贯的未来视频。我们引入了一个四阶段训练策略，可以在多个现有 AD 数据集以及建议的专用数据集上逐步构建这些功能。实验证明了其在感知、推理和决策方面的最先进的性能，并对具有挑战性的长尾情况具有卓越的泛化能力。</li>
</ul>

<h3>Title: MedForget: Hierarchy-Aware Multimodal Unlearning Testbed for Medical AI</h3>
<ul>
<li><strong>Authors: </strong>Fengli Wu, Vaidehi Patil, Jaehong Yoon, Yue Zhang, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09867">https://arxiv.org/abs/2512.09867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09867">https://arxiv.org/pdf/2512.09867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09867]] MedForget: Hierarchy-Aware Multimodal Unlearning Testbed for Medical AI(https://arxiv.org/abs/2512.09867)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Pretrained Multimodal Large Language Models (MLLMs) are increasingly deployed in medical AI systems for clinical reasoning, diagnosis support, and report generation. However, their training on sensitive patient data raises critical privacy and compliance challenges under regulations such as HIPAA and GDPR, which enforce the "right to be forgotten". Unlearning, the process of tuning models to selectively remove the influence of specific training data points, offers a potential solution, yet its effectiveness in complex medical settings remains underexplored. To systematically study this, we introduce MedForget, a Hierarchy-Aware Multimodal Unlearning Testbed with explicit retain and forget splits and evaluation sets containing rephrased variants. MedForget models hospital data as a nested hierarchy (Institution -> Patient -> Study -> Section), enabling fine-grained assessment across eight organizational levels. The benchmark contains 3840 multimodal (image, question, answer) instances, each hierarchy level having a dedicated unlearning target, reflecting distinct unlearning challenges. Experiments with four SOTA unlearning methods on three tasks (generation, classification, cloze) show that existing methods struggle to achieve complete, hierarchy-aware forgetting without reducing diagnostic performance. To test whether unlearning truly deletes hierarchical pathways, we introduce a reconstruction attack that progressively adds hierarchical level context to prompts. Models unlearned at a coarse granularity show strong resistance, while fine-grained unlearning leaves models vulnerable to such reconstruction. MedForget provides a practical, HIPAA-aligned testbed for building compliant medical AI systems.</li>
<li><strong>摘要：</strong>预训练的多模态大语言模型 (MLLM) 越来越多地部署在医疗 AI 系统中，用于临床推理、诊断支持和报告生成。然而，他们对敏感患者数据的培训在 HIPAA 和 GDPR 等强制执行“被遗忘权”的法规下提出了严峻的隐私和合规性挑战。取消学习是调整模型以有选择地消除特定训练数据点的影响的过程，提供了一种潜在的解决方案，但其在复杂医疗环境中的有效性仍未得到充分探索。为了系统地研究这一点，我们引入了 MedForget，这是一个层次结构感知的多模态忘却测试床，具有明确的保留和忘记分割以及包含改写变体的评估集。 MedForget 将医院数据建模为嵌套层次结构（机构 -> 患者 -> 研究 -> 部门），从而能够跨八个组织级别进行细粒度评估。该基准包含 3840 个多模式（图像、问题、答案）实例，每个层次结构级别都有一个专门的遗忘目标，反映了不同的遗忘挑战。在三个任务（生成、分类、完形填空）上使用四种 SOTA 遗忘方法进行的实验表明，现有方法很难在不降低诊断性能的情况下实现完整的、层次结构感知的遗忘。为了测试遗忘是否真正删除了分层路径，我们引入了一种重建攻击，该攻击逐渐将分层级别上下文添加到提示中。在粗粒度上不学习的模型表现出很强的抵抗力，而在细粒度上不学习的模型则容易受到这种重建的影响。 MedForget 提供了一个实用的、符合 HIPAA 的测试平台，用于构建合规的医疗人工智能系统。</li>
</ul>

<h3>Title: Provably Learning from Modern Language Models via Low Logit Rank</h3>
<ul>
<li><strong>Authors: </strong>Noah Golowich, Allen Liu, Abhishek Shetty</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DS, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09892">https://arxiv.org/abs/2512.09892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09892">https://arxiv.org/pdf/2512.09892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09892]] Provably Learning from Modern Language Models via Low Logit Rank(https://arxiv.org/abs/2512.09892)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While modern language models and their inner workings are incredibly complex, recent work (Golowich, Liu & Shetty; 2025) has proposed a simple and potentially tractable abstraction for them through the observation that empirically, these language models all seem to have approximately low logit rank. Roughly, this means that a matrix formed by the model's log probabilities of various tokens conditioned on certain sequences of tokens is well approximated by a low rank matrix. In this paper, our focus is on understanding how this structure can be exploited algorithmically for obtaining provable learning guarantees. Since low logit rank models can encode hard-to-learn distributions such as noisy parities, we study a query learning model with logit queries that reflects the access model for common APIs. Our main result is an efficient algorithm for learning any approximately low logit rank model from queries. We emphasize that our structural assumption closely reflects the behavior that is empirically observed in modern language models. Thus, our result gives what we believe is the first end-to-end learning guarantee for a generative model that plausibly captures modern language models.</li>
<li><strong>摘要：</strong>虽然现代语言模型及其内部工作原理非常复杂，但最近的工作（Golowich、Liu 和 Shetty；2025）通过观察发现，从经验上看，这些语言模型似乎都具有大约较低的 Logit 等级，为它们提出了一个简单且可能易于处理的抽象。粗略地说，这意味着由模型以某些标记序列为条件的各种标记的对数概率形成的矩阵可以很好地用低秩矩阵来近似。在本文中，我们的重点是了解如何在算法上利用这种结构来获得可证明的学习保证。由于低 logit 等级模型可以对难以学习的分布进行编码，例如噪声奇偶校验，因此我们研究了一种具有 logit 查询的查询学习模型，该模型反映了常见 API 的访问模型。我们的主要结果是一种有效的算法，用于从查询中学习任何近似低 Logit 排名模型。我们强调，我们的结构假设密切反映了现代语言模型中凭经验观察到的行为。因此，我们的结果为我们认为能够捕获现代语言模型的生成模型提供了第一个端到端学习保证。</li>
</ul>

<h3>Title: FALCON: Few-step Accurate Likelihoods for Continuous Flows</h3>
<ul>
<li><strong>Authors: </strong>Danyal Rehman, Tara Akhound-Sadegh, Artem Gazizov, Yoshua Bengio, Alexander Tong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09914">https://arxiv.org/abs/2512.09914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09914">https://arxiv.org/pdf/2512.09914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09914]] FALCON: Few-step Accurate Likelihoods for Continuous Flows(https://arxiv.org/abs/2512.09914)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann Generators tackle this problem by pairing a generative model, capable of exact likelihood computation, with importance sampling to obtain consistent samples under the target distribution. Current Boltzmann Generators primarily use continuous normalizing flows (CNFs) trained with flow matching for efficient training of powerful models. However, likelihood calculation for these models is extremely costly, requiring thousands of function evaluations per sample, severely limiting their adoption. In this work, we propose Few-step Accurate Likelihoods for Continuous Flows (FALCON), a method which allows for few-step sampling with a likelihood accurate enough for importance sampling applications by introducing a hybrid training objective that encourages invertibility. We show FALCON outperforms state-of-the-art normalizing flow models for molecular Boltzmann sampling and is two orders of magnitude faster than the equivalently performing CNF model.</li>
<li><strong>摘要：</strong>热力学平衡中分子状态的可扩展采样是统计物理学中长期存在的挑战。玻尔兹曼生成器通过将能够进行精确似然计算的生成模型与重要性采样配对来解决这个问题，以在目标分布下获得一致的样本。当前的玻尔兹曼生成器主要使用经过流匹配训练的连续归一化流 (CNF)，以有效训练强大的模型。然而，这些模型的似然计算成本极高，每个样本需要数千次函数评估，严重限制了它们的采用。在这项工作中，我们提出了连续流的少步精确似然法（FALCON），该方法通过引入鼓励可逆性的混合训练目标，允许对重要性采样应用程序提供足够准确的似然率采样。我们证明 FALCON 的性能优于最先进的分子玻尔兹曼采样归一化流模型，并且比同等性能的 CNF 模型快两个数量级。</li>
</ul>

<h3>Title: ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Liu, Hangjie Yuan, Yujie Wei, Jiazheng Xing, Yujin Han, Jiahao Pan, Yanbiao Ma, Chi-Min Chan, Kang Zhao, Shiwei Zhang, Wenhan Luo, Yike Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.09924">https://arxiv.org/abs/2512.09924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.09924">https://arxiv.org/pdf/2512.09924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.09924]] ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning(https://arxiv.org/abs/2512.09924)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.</li>
<li><strong>摘要：</strong>视频统一模型在理解和生成方面表现出强大的能力，但即使配备了强大的内部视觉语言模型（VLM），它们也难以进行基于理性的可视化编辑。我们将这种差距归因于两个因素：1）现有数据集不足以训练和评估推理感知视频编辑，2）模型的推理和编辑功能之间固有的脱节，这阻碍了丰富的理解有效地指导编辑过程。弥合这一差距需要一个将推理与视觉转换联系起来的集成框架。为了解决这一差距，我们引入了基于原因的视频编辑（RVE）任务，该任务需要在编辑过程中对物理合理性和因果动态进行推理。为了支持系统评估，我们构建了 RVE-Bench，这是一个具有两个互补子集的综合基准：推理知情视频编辑和上下文视频生成。这些子集涵盖了不同的推理维度和现实世界的编辑场景。在此基础上，我们提出了 ReViSE，一种自反思推理 (SRF) 框架，它将生成和评估统一在一个架构中。该模型的内部 VLM 通过评估编辑的视频在逻辑上是否满足给定的指令来提供内在反馈。在训练过程中细化生成器推理行为的差分反馈。 RVE-Bench 上的大量实验表明，ReViSE 显着提高了编辑准确性和视觉保真度，与最先进的方法相比，推理视频编辑子集的总体得分提高了 32%。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
