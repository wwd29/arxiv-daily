<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-12-24</h1>
<h3>Title: Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference</h3>
<ul>
<li><strong>Authors: </strong>Zhan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19717">https://arxiv.org/abs/2512.19717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19717">https://arxiv.org/pdf/2512.19717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19717]] Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference(https://arxiv.org/abs/2512.19717)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Finding rare but useful solutions in very large candidate spaces is a recurring practical challenge across language generation, planning, and reinforcement learning. We present a practical framework, \emph{Inverted Causality Focusing Algorithm} (ICFA), that treats search as a target-conditioned reweighting process. ICFA reuses an available proposal sampler and a task-specific similarity function to form a focused sampling distribution, while adaptively controlling focusing strength to avoid degeneracy. We provide a clear recipe, a stability diagnostic based on effective sample size, a compact theoretical sketch explaining when ICFA can reduce sample needs, and two reproducible experiments: constrained language generation and sparse-reward navigation. We further show how structured prompts instantiate an approximate, language-level form of ICFA and describe a hybrid architecture combining prompted inference with algorithmic reweighting.</li>
<li><strong>摘要：</strong>在非常大的候选空间中寻找罕见但有用的解决方案是语言生成、规划和强化学习中反复出现的实际挑战。我们提出了一个实用的框架，\emph{反向因果聚焦算法}（ICFA），它将搜索视为目标条件的重新加权过程。 ICFA 重用可用的提案采样器和特定于任务的相似性函数来形成聚焦采样分布，同时自适应控制聚焦强度以避免简并。我们提供了一个清晰的方案、基于有效样本量的稳定性诊断、解释 ICFA 何时可以减少样本需求的紧凑理论草图，以及两个可重复的实验：约束语言生成和稀疏奖励导航。我们进一步展示了结构化提示如何实例化 ICFA 的近似语言级形式，并描述了将提示推理与算法重新加权相结合的混合架构。</li>
</ul>

<h3>Title: End-to-End Data Quality-Driven Framework for Machine Learning in Production Environment</h3>
<ul>
<li><strong>Authors: </strong>Firas Bayram, Bestoun S. Ahmed, Erik Hallin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19723">https://arxiv.org/abs/2512.19723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19723">https://arxiv.org/pdf/2512.19723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19723]] End-to-End Data Quality-Driven Framework for Machine Learning in Production Environment(https://arxiv.org/abs/2512.19723)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel end-to-end framework that efficiently integrates data quality assessment with machine learning (ML) model operations in real-time production environments. While existing approaches treat data quality assessment and ML systems as isolated processes, our framework addresses the critical gap between theoretical methods and practical implementation by combining dynamic drift detection, adaptive data quality metrics, and MLOps into a cohesive, lightweight system. The key innovation lies in its operational efficiency, enabling real-time, quality-driven ML decision-making with minimal computational overhead. We validate the framework in a steel manufacturing company's Electroslag Remelting (ESR) vacuum pumping process, demonstrating a 12% improvement in model performance (R2 = 94%) and a fourfold reduction in prediction latency. By exploring the impact of data quality acceptability thresholds, we provide actionable insights into balancing data quality standards and predictive performance in industrial applications. This framework represents a significant advancement in MLOps, offering a robust solution for time-sensitive, data-driven decision-making in dynamic industrial environments.</li>
<li><strong>摘要：</strong>本文介绍了一种新颖的端到端框架，该框架可有效地将数据质量评估与实时生产环境中的机器学习（ML）模型操作集成起来。虽然现有方法将数据质量评估和机器学习系统视为孤立的过程，但我们的框架通过将动态漂移检测、自适应数据质量指标和 MLOps 结合到一个有凝聚力的轻量级系统中，解决了理论方法和实际实现之间的关键差距。关键的创新在于其运营效率，能够以最小的计算开销实现实时、质量驱动的机器学习决策。我们在一家钢铁制造公司的电渣重熔 (ESR) 真空泵送工艺中验证了该框架，结果表明模型性能提高了 12%（R2 = 94%），预测延迟降低了四倍。通过探索数据质量可接受阈值的影响，我们提供了平衡工业应用中的数据质量标准和预测性能的可行见解。该框架代表了 MLOps 的重大进步，为动态工业环境中时间敏感、数据驱动的决策提供了强大的解决方案。</li>
</ul>

<h3>Title: High-Performance Self-Supervised Learning by Joint Training of Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Kosuke Ukita, Tsuyoshi Okita</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19729">https://arxiv.org/abs/2512.19729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19729">https://arxiv.org/pdf/2512.19729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19729]] High-Performance Self-Supervised Learning by Joint Training of Flow Matching(https://arxiv.org/abs/2512.19729)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models can learn rich representations during data generation, showing potential for Self-Supervised Learning (SSL), but they face a trade-off between generative quality and discriminative performance. Their iterative sampling also incurs substantial computational and energy costs, hindering industrial and edge AI applications. To address these issues, we propose the Flow Matching-based Foundation Model (FlowFM), which jointly trains a representation encoder and a conditional flow matching generator. This decoupled design achieves both high-fidelity generation and effective recognition. By using flow matching to learn a simpler velocity field, FlowFM accelerates and stabilizes training, improving its efficiency for representation learning. Experiments on wearable sensor data show FlowFM reduces training time by 50.4\% compared to a diffusion-based approach. On downstream tasks, FlowFM surpassed the state-of-the-art SSL method (SSL-Wearables) on all five datasets while achieving up to a 51.0x inference speedup and maintaining high generative quality. The implementation code is available at this https URL.</li>
<li><strong>摘要：</strong>扩散模型可以在数据生成过程中学习丰富的表示，显示出自我监督学习（SSL）的潜力，但它们面临着生成质量和判别性能之间的权衡。它们的迭代采样还会产生大量的计算和能源成本，阻碍工业和边缘人工智能应用。为了解决这些问题，我们提出了基于流匹配的基础模型（FlowFM），它联合训练表示编码器和条件流匹配生成器。这种解耦设计既实现了高保真生成，又实现了有效识别。通过使用流匹配来学习更简单的速度场，FlowFM 加速并稳定了训练，提高了表示学习的效率。可穿戴传感器数据实验表明，与基于扩散的方法相比，FlowFM 将训练时间减少了 50.4%。在下游任务中，FlowFM 在所有五个数据集上都超越了最先进的 SSL 方法（SSL-Wearables），同时实现了高达 51.0 倍的推理加速并保持了高生成质量。实现代码可在此 https URL 获取。</li>
</ul>

<h3>Title: CoPHo: Classifier-guided Conditional Topology Generation with Persistent Homology</h3>
<ul>
<li><strong>Authors: </strong>Gongli Xi, Ye Tian, Mengyu Yang, Zhenyu Zhao, Yuchao Zhang, Xiangyang Gong, Xirong Que, Wendong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19736">https://arxiv.org/abs/2512.19736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19736">https://arxiv.org/pdf/2512.19736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19736]] CoPHo: Classifier-guided Conditional Topology Generation with Persistent Homology(https://arxiv.org/abs/2512.19736)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The structure of topology underpins much of the research on performance and robustness, yet available topology data are typically scarce, necessitating the generation of synthetic graphs with desired properties for testing or release. Prior diffusion-based approaches either embed conditions into the diffusion model, requiring retraining for each attribute and hindering real-time applicability, or use classifier-based guidance post-training, which does not account for topology scale and practical constraints. In this paper, we show from a discrete perspective that gradients from a pre-trained graph-level classifier can be incorporated into the discrete reverse diffusion posterior to steer generation toward specified structural properties. Based on this insight, we propose Classifier-guided Conditional Topology Generation with Persistent Homology (CoPHo), which builds a persistent homology filtration over intermediate graphs and interprets features as guidance signals that steer generation toward the desired properties at each denoising step. Experiments on four generic/network datasets demonstrate that CoPHo outperforms existing methods at matching target metrics, and we further validate its transferability on the QM9 molecular dataset.</li>
<li><strong>摘要：</strong>拓扑结构支撑着许多关于性能和鲁棒性的研究，但可用的拓扑数据通常很少，因此需要生成具有所需属性的合成图以进行测试或发布。先前基于扩散的方法要么将条件嵌入到扩散模型中，需要对每个属性进行重新训练并阻碍实时适用性，要么使用基于分类器的指导后训练，这不考虑拓扑规模和实际约束。在本文中，我们从离散的角度展示了来自预训练的图级分类器的梯度可以合并到离散反向扩散中，以引导生成朝向指定的结构特性。基于这一见解，我们提出了具有持久同源性的分类器引导条件拓扑生成（CoPHo），它在中间图上构建持久同源过滤，并将特征解释为指导信号，引导生成在每个去噪步骤中实现所需的属性。对四个通用/网络数据集的实验表明，CoPHo 在匹配目标指标方面优于现有方法，并且我们进一步验证了其在 QM9 分子数据集上的可转移性。</li>
</ul>

<h3>Title: OpComm: A Reinforcement Learning Framework for Adaptive Buffer Control in Warehouse Volume Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Wilson Fung, Lu Guo, Drake Hilliard, Alessandro Casadei, Raj Ratan, Sreyoshi Bhaduri, Adi Surve, Nikhil Agarwal, Rohit Malshe, Pavan Mullapudi, Hungjen Wang, Saurabh Doodhwala, Ankush Pole, Arkajit Rakshit</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19738">https://arxiv.org/abs/2512.19738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19738">https://arxiv.org/pdf/2512.19738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19738]] OpComm: A Reinforcement Learning Framework for Adaptive Buffer Control in Warehouse Volume Forecasting(https://arxiv.org/abs/2512.19738)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate forecasting of package volumes at delivery stations is critical for last-mile logistics, where errors lead to inefficient resource allocation, higher costs, and delivery delays. We propose OpComm, a forecasting and decision-support framework that combines supervised learning with reinforcement learning-based buffer control and a generative AI-driven communication module. A LightGBM regression model generates station-level demand forecasts, which serve as context for a Proximal Policy Optimization (PPO) agent that selects buffer levels from a discrete action set. The reward function penalizes under-buffering more heavily than over-buffering, reflecting real-world trade-offs between unmet demand risks and resource inefficiency. Station outcomes are fed back through a Monte Carlo update mechanism, enabling continual policy adaptation. To enhance interpretability, a generative AI layer produces executive-level summaries and scenario analyses grounded in SHAP-based feature attributions. Across 400+ stations, OpComm reduced Weighted Absolute Percentage Error (WAPE) by 21.65% compared to manual forecasts, while lowering under-buffering incidents and improving transparency for decision-makers. This work shows how contextual reinforcement learning, coupled with predictive modeling, can address operational forecasting challenges and bridge statistical rigor with practical decision-making in high-stakes logistics environments.</li>
<li><strong>摘要：</strong>准确预测配送站的包裹数量对于最后一英里物流至关重要，其中错误会导致资源分配效率低下、成本上升和配送延误。我们提出了 OpComm，这是一种预测和决策支持框架，它将监督学习与基于强化学习的缓冲区控制和生成式人工智能驱动的通信模块相结合。 LightGBM 回归模型生成站级需求预测，作为近端策略优化 (PPO) 代理的上下文，该代理从离散操作集中选择缓冲级别。奖励函数对缓冲不足的惩罚比对缓冲过度的惩罚更严重，反映了现实世界中未满足的需求风险和资源效率低下之间的权衡。站点结果通过蒙特卡罗更新机制反馈，从而实现持续的政策调整。为了增强可解释性，生成人工智能层根据基于 SHAP 的特征归因生成执行级摘要和场景分析。在 400 多个站点中，与手动预测相比，OpComm 将加权绝对百分比误差 (WAPE) 降低了 21.65%，同时减少了缓冲不足事件并提高了决策者的透明度。这项工作展示了情境强化学习如何与预测建模相结合，解决运营预测挑战，并将统计严谨性与高风险物流环境中的实际决策联系起来。</li>
</ul>

<h3>Title: OASI: Objective-Aware Surrogate Initialization for Multi-Objective Bayesian Optimization in TinyML Keyword Spotting</h3>
<ul>
<li><strong>Authors: </strong>Soumen Garai, Suman Samui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19739">https://arxiv.org/abs/2512.19739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19739">https://arxiv.org/pdf/2512.19739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19739]] OASI: Objective-Aware Surrogate Initialization for Multi-Objective Bayesian Optimization in TinyML Keyword Spotting(https://arxiv.org/abs/2512.19739)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Voice assistants utilize Keyword Spotting (KWS) to enable efficient, privacy-friendly activation. However, realizing accurate KWS models on ultra-low-power TinyML devices (often with less than $<2$ MB of flash memory) necessitates a delicate balance between accuracy with strict resource constraints. Multi-objective Bayesian Optimization (MOBO) is an ideal candidate for managing such a trade-off but is highly initialization-dependent, especially under the budgeted black-box setting. Existing methods typically fall back to naive, ad-hoc sampling routines (e.g., Latin Hypercube Sampling (LHS), Sobol sequences, or Random search) that are adapted to neither the Pareto front nor undergo rigorous statistical comparison. To address this, we propose Objective-Aware Surrogate Initialization (OASI), a novel initialization strategy that leverages Multi-Objective Simulated Annealing (MOSA) to generate a seed Pareto set of high-performing and diverse configurations that explicitly balance accuracy and model size. Evaluated in a TinyML KWS setting, OASI outperforms LHS, Sobol, and Random initialization, achieving the highest hypervolume (0.0627) and the lowest generational distance (0.0) across multiple runs, with only a modest increase in computation time (1934 s vs. $\sim$1500 s). A non-parametric statistical analysis using the Kruskal-Wallis test ($H = 5.40$, $p = 0.144$, $\eta^2 = 0.0007$) and Dunn's post-hoc test confirms OASI's superior consistency despite the non-significant overall difference with respect to the $\alpha=0.05$ threshold.</li>
<li><strong>摘要：</strong>语音助手利用关键字识别 (KWS) 来实现高效、保护隐私的激活。然而，在超低功耗 TinyML 设备（通常具有不到 2$MB 的闪存）上实现准确的 KWS 模型需要在准确性与严格的资源限制之间取得微妙的平衡。多目标贝叶斯优化 (MOBO) 是管理此类权衡的理想选择，但高度依赖于初始化，尤其是在预算黑盒设置下。现有的方法通常会退回到朴素的、临时的采样例程（例如，拉丁超立方采样（LHS）、索波尔序列或随机搜索），这些例程既不适合帕累托前沿，也不经过严格的统计比较。为了解决这个问题，我们提出了目标感知代理初始化（OASI），这是一种新颖的初始化策略，利用多目标模拟退火（MOSA）来生成高性能和多样化配置的种子帕累托集，明确平衡精度和模型大小。在 TinyML KWS 设置中进行评估，OASI 优于 LHS、Sobol 和随机初始化，在多次运行中实现了最高的超体积 (0.0627) 和最低的代距离 (0.0)，计算时间仅略有增加（1934 秒 vs. $\sim$1500 秒）。使用 Kruskal-Wallis 检验（$H = 5.40$、$p = 0.144$、$\eta^2 = 0.0007$）和 Dunn 事后检验的非参数统计分析证实了 OASI 的卓越一致性，尽管相对于 $\alpha=0.05$ 阈值而言总体差异不显着。</li>
</ul>

<h3>Title: DeepBridge: A Unified and Production-Ready Framework for Multi-Dimensional Machine Learning Validation</h3>
<ul>
<li><strong>Authors: </strong>Gustavo Coelho Haase, Paulo Henrique Dourado da Silva</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19744">https://arxiv.org/abs/2512.19744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19744">https://arxiv.org/pdf/2512.19744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19744]] DeepBridge: A Unified and Production-Ready Framework for Multi-Dimensional Machine Learning Validation(https://arxiv.org/abs/2512.19744)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present DeepBridge, an 80K-line Python library that unifies multi-dimensional validation, automatic compliance verification, knowledge distillation, and synthetic data generation. DeepBridge offers: (i) 5 validation suites (fairness with 15 metrics, robustness with weakness detection, uncertainty via conformal prediction, resilience with 5 drift types, hyperparameter sensitivity), (ii) automatic EEOC/ECOA/GDPR verification, (iii) multi-format reporting system (interactive/static HTML, PDF, JSON), (iv) HPM-KD framework for knowledge distillation with meta-learning, and (v) scalable synthetic data generation via Dask. Through 6 case studies (credit scoring, hiring, healthcare, mortgage, insurance, fraud) we demonstrate that DeepBridge: reduces validation time by 89% (17 min vs. 150 min with fragmented tools), automatically detects fairness violations with complete coverage (10/10 features vs. 2/10 from existing tools), generates audit-ready reports in minutes. HPM-KD demonstrates consistent superiority across compression ratios 2.3--7x (CIFAR100): +1.00--2.04pp vs. Direct Training (p<0.05), confirming that Knowledge Distillation is effective at larger teacher-student gaps. Usability study with 20 participants shows SUS score 87.5 (top 10%, ``excellent''), 95% success rate, and low cognitive load (NASA-TLX 28/100). DeepBridge is open-source under MIT license at this https URL, with complete documentation at this https URL</li>
<li><strong>摘要：</strong>我们推出 DeepBridge，一个 8 万行的 Python 库，它统一了多维验证、自动合规性验证、知识蒸馏和合成数据生成。 DeepBridge 提供：(i) 5 个验证套件（15 个指标的公平性、弱点检测的稳健性、共形预测的不确定性、5 种漂移类型的弹性、超参数敏感性），(ii) 自动 EEOC/ECOA/GDPR 验证，(iii) 多格式报告系统（交互式/静态 HTML、PDF、JSON），(iv) 通过元学习进行知识蒸馏的 HPM-KD 框架，以及 (v) 通过 Dask 生成可扩展的合成数据。通过 6 个案例研究（信用评分、招聘、医疗保健、抵押贷款、保险、欺诈），我们证明 DeepBridge：将验证时间缩短 89%（使用碎片化工具为 17 分钟，而使用分散工具为 150 分钟），自动检测完全覆盖的公平违规行为（10/10 功能与现有工具的 2/10），在几分钟内生成审计就绪报告。 HPM-KD 在压缩比 2.3--7x (CIFAR100) 上表现出一致的优越性：与直接训练 (p<0.05) 相比，+1.00--2.04pp，证实知识蒸馏在较大的师生差距时是有效的。 20 名参与者参与的可用性研究显示 SUS 得分 87.5（前 10%，“优秀”）、95% 的成功率和较低的认知负荷 (NASA-TLX 28/100)。 DeepBridge 是在 MIT 许可下开源的，位于此 https URL，完整的文档位于此 https URL</li>
</ul>

<h3>Title: Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra</h3>
<ul>
<li><strong>Authors: </strong>Maxime Lacour, Pu Ren, Rie Nakata, Nori Nakata, Michael Mahoney</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19909">https://arxiv.org/abs/2512.19909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19909">https://arxiv.org/pdf/2512.19909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19909]] Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra(https://arxiv.org/abs/2512.19909)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent developments in non-ergodic ground-motion models (GMMs) explicitly model systematic spatial variations in source, site, and path effects, reducing standard deviation to 30-40% of ergodic models and enabling more accurate site-specific seismic hazard analysis. Current non-ergodic GMMs rely on Gaussian Process (GP) methods with prescribed correlation functions and thus have computational limitations for large-scale predictions. This study proposes a deep-learning approach called Conditional Generative Modeling for Fourier Amplitude Spectra (CGM-FAS) as an alternative to GP-based methods for modeling non-ergodic path effects in Fourier Amplitude Spectra (FAS). CGM-FAS uses a Conditional Variational Autoencoder architecture to learn spatial patterns and interfrequency correlation directly from data by using geographical coordinates of earthquakes and stations as conditional variables. Using San Francisco Bay Area earthquake data, we compare CGM-FAS against a recent GP-based GMM for the region and demonstrate consistent predictions of non-ergodic path effects. Additionally, CGM-FAS offers advantages compared to GP-based approaches in learning spatial patterns without prescribed correlation functions, capturing interfrequency correlations, and enabling rapid predictions, generating maps for 10,000 sites across 1,000 frequencies within 10 seconds using a few GB of memory. CGM-FAS hyperparameters can be tuned to ensure generated path effects exhibit variability consistent with the GP-based empirical GMM. This work demonstrates a promising direction for efficient non-ergodic ground-motion prediction across multiple frequencies and large spatial domains.</li>
<li><strong>摘要：</strong>非遍历地震动模型 (GMM) 的最新发展明确地模拟了震源、场地和路径效应的系统空间变化，将标准差减少至遍历模型的 30-40%，并实现更准确的特定场地地震危险性分析。当前的非遍历 GMM 依赖于具有规定相关函数的高斯过程 (GP) 方法，因此对于大规模预测具有计算限制。本研究提出了一种称为傅里叶振幅谱条件生成建模 (CGM-FAS) 的深度学习方法，作为基于 GP 的方法的替代方案，用于对傅里叶振幅谱 (FAS) 中的非遍历路径效应进行建模。 CGM-FAS 使用条件变分自动编码器架构，通过使用地震和台站的地理坐标作为条件变量，直接从数据中学习空间模式和频间相关性。使用旧金山湾区地震数据，我们将 CGM-FAS 与该地区最近基于 GP 的 GMM 进行比较，并证明了对非遍历路径效应的一致预测。此外，与基于 GP 的方法相比，CGM-FAS 在无需指定相关函数的情况下学习空间模式、捕获频率间相关性以及实现快速预测、使用几 GB 内存在 10 秒内生成跨 1,000 个频率的 10,000 个站点的地图方面具有优势。可以调整 CGM-FAS 超参数，以确保生成的路径效应表现出与基于 GP 的经验 GMM 一致的可变性。这项工作展示了跨多个频率和大空间域的有效非遍历地面运动预测的有前途的方向。</li>
</ul>

<h3>Title: Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Houston H. Zhang, Tao Zhang, Baoze Lin, Yuanqi Xue, Yincheng Zhu, Huan Liu, Li Gu, Linfeng Ye, Ziqiang Wang, Xinxin Zuo, Yang Wang, Yuanhao Yu, Zhixiang Chi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19918">https://arxiv.org/abs/2512.19918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19918">https://arxiv.org/pdf/2512.19918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19918]] Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs(https://arxiv.org/abs/2512.19918)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>User interface to code (UI2Code) aims to generate executable code that can faithfully reconstruct a given input UI. Prior work focuses largely on web pages and mobile screens, leaving app widgets underexplored. Unlike web or mobile UIs with rich hierarchical context, widgets are compact, context-free micro-interfaces that summarize key information through dense layouts and iconography under strict spatial constraints. Moreover, while (image, code) pairs are widely available for web or mobile UIs, widget designs are proprietary and lack accessible markup. We formalize this setting as the Widget-to-Code (Widget2Code) and introduce an image-only widget benchmark with fine-grained, multi-dimensional evaluation metrics. Benchmarking shows that although generalized multimodal large language models (MLLMs) outperform specialized UI2Code methods, they still produce unreliable and visually inconsistent code. To address these limitations, we develop a baseline that jointly advances perceptual understanding and structured code generation. At the perceptual level, we follow widget design principles to assemble atomic components into complete layouts, equipped with icon retrieval and reusable visualization modules. At the system level, we design an end-to-end infrastructure, WidgetFactory, which includes a framework-agnostic widget-tailored domain-specific language (WidgetDSL) and a compiler that translates it into multiple front-end implementations (e.g., React, HTML/CSS). An adaptive rendering module further refines spatial dimensions to satisfy compactness constraints. Together, these contributions substantially enhance visual fidelity, establishing a strong baseline and unified infrastructure for future Widget2Code research.</li>
<li><strong>摘要：</strong>用户界面代码（UI2Code）旨在生成可以忠实地重建给定输入 UI 的可执行代码。之前的工作主要集中在网页和移动屏幕上，而对应用程序小部件的探索还不够。与具有丰富层次上下文的 Web 或移动 UI 不同，小部件是紧凑的、上下文无关的微界面，可在严格的空间限制下通过密集的布局和图像来汇总关键信息。此外，虽然（图像、代码）对广泛用于 Web 或移动 UI，但小部件设计是专有的并且缺乏可访问的标记。我们将此设置形式化为 Widget-to-Code (Widget2Code)，并引入具有细粒度、多维评估指标的纯图像小部件基准。基准测试表明，尽管广义多模态大语言模型 (MLLM) 的性能优于专门的 UI2Code 方法，但它们仍然会生成不可靠且视觉上不一致的代码。为了解决这些限制，我们开发了一个共同推进感知理解和结构化代码生成的基线。在感知层面，我们遵循小部件设计原则，将原子组件组装成完整的布局，配备图标检索和可重用的可视化模块。在系统层面，我们设计了一个端到端的基础设施 WidgetFactory，它包括一个与框架无关的小部件定制的特定领域语言（WidgetDSL）和一个将其转换为多个前端实现（例如 React、HTML/CSS）的编译器。自适应渲染模块进一步细化空间维度以满足紧凑性约束。这些贡献共同大大增强了视觉保真度，为未来的 Widget2Code 研究建立了强大的基线和统一的基础设施。</li>
</ul>

<h3>Title: SE360: Semantic Edit in 360$^\circ$ Panoramas via Hierarchical Data Construction</h3>
<ul>
<li><strong>Authors: </strong>Haoyi Zhong, Fang-Lue Zhang, Andrew Chalmers, Taehyun Rhee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19943">https://arxiv.org/abs/2512.19943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19943">https://arxiv.org/pdf/2512.19943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19943]] SE360: Semantic Edit in 360$^\circ$ Panoramas via Hierarchical Data Construction(https://arxiv.org/abs/2512.19943)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While instruction-based image editing is emerging, extending it to 360$^\circ$ panoramas introduces additional challenges. Existing methods often produce implausible results in both equirectangular projections (ERP) and perspective views. To address these limitations, we propose SE360, a novel framework for multi-condition guided object editing in 360$^\circ$ panoramas. At its core is a novel coarse-to-fine autonomous data generation pipeline without manual intervention. This pipeline leverages a Vision-Language Model (VLM) and adaptive projection adjustment for hierarchical analysis, ensuring the holistic segmentation of objects and their physical context. The resulting data pairs are both semantically meaningful and geometrically consistent, even when sourced from unlabeled panoramas. Furthermore, we introduce a cost-effective, two-stage data refinement strategy to improve data realism and mitigate model overfitting to erase artifacts. Based on the constructed dataset, we train a Transformer-based diffusion model to allow flexible object editing guided by text, mask, or reference image in 360$^\circ$ panoramas. Our experiments demonstrate that our method outperforms existing methods in both visual quality and semantic accuracy.</li>
<li><strong>摘要：</strong>虽然基于指令的图像编辑正在兴起，但将其扩展到 360$^\circ$ 全景会带来额外的挑战。现有方法通常在等距柱状投影 (ERP) 和透视图中产生令人难以置信的结果。为了解决这些限制，我们提出了 SE360，这是一种在 360$^\circ$ 全景图中进行多条件引导对象编辑的新颖框架。其核心是一种新颖的从粗到细的自主数据生成管道，无需人工干预。该管道利用视觉语言模型 (VLM) 和自适应投影调整进行分层分析，确保对象及其物理环境的整体分割。即使来自未标记的全景图，生成的数据对在语义上也是有意义的并且在几何上是一致的。此外，我们引入了一种经济高效的两阶段数据细化策略，以提高数据真实性并减轻模型过度拟合以消除伪影。基于构建的数据集，我们训练基于 Transformer 的扩散模型，以允许在 360$^\circ$ 全景图中通过文本、掩模或参考图像进行灵活的对象编辑。我们的实验表明，我们的方法在视觉质量和语义准确性方面都优于现有方法。</li>
</ul>

<h3>Title: How Much 3D Do Video Foundation Models Encode?</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Huang, Xiang Li, Zhaoyang Lv, James M. Rehg</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19949">https://arxiv.org/abs/2512.19949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19949">https://arxiv.org/pdf/2512.19949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19949]] How Much 3D Do Video Foundation Models Encode?(https://arxiv.org/abs/2512.19949)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.</li>
<li><strong>摘要：</strong>视频是 3D 世界的连续 2D 投影。经过大视频数据的训练后，全局 3D 理解会自然出现吗？我们通过量化对大量视频数据预训练的现有视频基础模型 (VidFM) 的 3D 理解来研究这一问题。我们提出了第一个与模型无关的框架，通过浅读出从其特征估计多个 3D 属性，来测量各种 VidFM 的 3D 感知。我们的研究提出了有关 VidFM 多轴 3D 感知的有意义的发现。特别是，我们表明，最先进的视频生成模型表现出对 3D 对象和场景的深刻理解，尽管没有接受任何 3D 数据的训练。这种理解甚至可以超越专门针对 3D 任务训练的大型专家模型。我们的研究结果以及主要 VidFM 的 3D 基准测试为构建可扩展的 3D 模型提供了宝贵的观察结果。</li>
</ul>

<h3>Title: Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zhenhao Li, Shaohan Yi, Zheng Liu, Leonartinus Gao, Minh Ngoc Le, Ambrose Ling, Zhuoran Wang, Md Amirul Islam, Zhixiang Chi, Yuanhao Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20000">https://arxiv.org/abs/2512.20000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20000">https://arxiv.org/pdf/2512.20000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20000]] Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models(https://arxiv.org/abs/2512.20000)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) have recently achieved impressive photorealism in image and video generation. However, their application to image animation remains limited, even when trained on large-scale datasets. Two primary challenges contribute to this: the high dimensionality of video signals leads to a scarcity of training data, causing DMs to favor memorization over prompt compliance when generating motion; moreover, DMs struggle to generalize to novel motion patterns not present in the training set, and fine-tuning them to learn such patterns, especially using limited training data, is still under-explored. To address these limitations, we propose Modular Image-to-Video Adapter (MIVA), a lightweight sub-network attachable to a pre-trained DM, each designed to capture a single motion pattern and scalable via parallelization. MIVAs can be efficiently trained on approximately ten samples using a single consumer-grade GPU. At inference time, users can specify motion by selecting one or multiple MIVAs, eliminating the need for prompt engineering. Extensive experiments demonstrate that MIVA enables more precise motion control while maintaining, or even surpassing, the generation quality of models trained on significantly larger datasets.</li>
<li><strong>摘要：</strong>扩散模型 (DM) 最近在图像和视频生成方面取得了令人印象深刻的照片级真实感。然而，即使在大规模数据集上进行训练，它们在图像动画中的应用仍然有限。造成这种情况的两个主要挑战是：视频信号的高维性导致训练数据的稀缺，导致 DM 在生成运动时更倾向于记忆而不是迅速遵守；此外，DM 很难推广到训练集中不存在的新颖运动模式，并且对它们进行微调以学习这些模式，特别是使用有限的训练数据，仍然有待探索。为了解决这些限制，我们提出了模块化图像到视频适配器（MIVA），这是一种可附加到预先训练的 DM 的轻量级子网络，每个子网络都旨在捕获单个运动模式并通过并行化进行扩展。使用单个消费级 GPU 可以对大约 10 个样本进行有效的 MIVA 训练。在推理时，用户可以通过选择一个或多个 MIVA 来指定运动，从而无需进行即时工程。大量实验表明，MIVA 可以实现更精确的运动控制，同时保持甚至超越在更大的数据集上训练的模型的生成质量。</li>
</ul>

<h3>Title: SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images</h3>
<ul>
<li><strong>Authors: </strong>Zepeng Xin, Kaiyu Li, Luodi Chen, Wanchen Li, Yuchen Xiao, Hui Qiao, Weizhan Zhang, Deyu Meng, Xiangyong Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20013">https://arxiv.org/abs/2512.20013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20013">https://arxiv.org/pdf/2512.20013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20013]] SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images(https://arxiv.org/abs/2512.20013)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model's effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at this https URL.</li>
<li><strong>摘要：</strong>对于灾难响应和环境监测等应用来说，有效地将复杂语言与遥感 (RS) 图像中的像素结合起来是一个严峻的挑战。当前的模型可以解析简单的单目标命令，但在遇到复杂的地理空间场景时会失败，例如以各种粒度分割对象、执行多目标指令以及解释隐式用户意图。为了推动克服这些失败的进展，我们推出了 LaSeRS，这是第一个为跨语言引导分割的四个关键维度进行全面训练和评估而构建的大型数据集：层次粒度、目标多重性、推理要求和语言可变性。通过捕获这些维度，LaSeRS 超越了简单的命令，为复杂的地理空间推理提供了基准。这解决了一个关键差距：现有数据集过于简单化，导致现实世界模型容易敏感。我们还提出了 SegEarth-R2，这是一种专为 RS 中全面的语言引导分割而设计的 MLLM 架构，它直接面对这些挑战。该模型的有效性源于两个关键改进：（1）空间注意监督机制专门处理小对象及其组件的定位，（2）灵活高效的分割查询机制，可以处理单目标和多目标场景。实验结果表明，我们的 SegEarth-R2 在 LaSeRS 和其他基准测试上取得了出色的性能，为下一代地理空间分割奠定了强大的基线。所有数据和代码都将在此 https URL 发布。</li>
</ul>

<h3>Title: PairFlow: Closed-Form Source-Target Coupling for Few-Step Generation in Discrete Flow Models</h3>
<ul>
<li><strong>Authors: </strong>Mingue Park, Jisung Hwang, Seungwoo Yoo, Kyeongmin Yeo, Minhyuk Sung</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20063">https://arxiv.org/abs/2512.20063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20063">https://arxiv.org/pdf/2512.20063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20063]] PairFlow: Closed-Form Source-Target Coupling for Few-Step Generation in Discrete Flow Models(https://arxiv.org/abs/2512.20063)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce $\texttt{PairFlow}$, a lightweight preprocessing step for training Discrete Flow Models (DFMs) to achieve few-step sampling without requiring a pretrained teacher. DFMs have recently emerged as a new class of generative models for discrete data, offering strong performance. However, they suffer from slow sampling due to their iterative nature. Existing acceleration methods largely depend on finetuning, which introduces substantial additional training overhead. $\texttt{PairFlow}$ addresses this issue with a lightweight preprocessing step. Inspired by ReFlow and its extension to DFMs, we train DFMs from coupled samples of source and target distributions, without requiring any pretrained teacher. At the core of our approach is a closed-form inversion for DFMs, which allows efficient construction of paired source-target samples. Despite its extremely low cost, taking only up to 1.7% of the compute needed for full model training, $\texttt{PairFlow}$ matches or even surpasses the performance of two-stage training involving finetuning. Furthermore, models trained with our framework provide stronger base models for subsequent distillation, yielding further acceleration after finetuning. Experiments on molecular data as well as binary and RGB images demonstrate the broad applicability and effectiveness of our approach.</li>
<li><strong>摘要：</strong>我们引入了$\texttt{PairFlow}$，这是一个轻量级的预处理步骤，用于训练离散流模型（DFM）以实现几步采样，而无需预先训练的教师。 DFM 最近作为一类新型离散数据生成模型出现，提供了强大的性能。然而，由于其迭代性质，它们的采样速度很慢。现有的加速方法很大程度上依赖于微调，这会带来大量额外的训练开销。 $\texttt{PairFlow}$ 通过轻量级预处理步骤解决了这个问题。受 ReFlow 及其对 DFM 扩展的启发，我们从源分布和目标分布的耦合样本中训练 DFM，而不需要任何预先训练的教师。我们方法的核心是 DFM 的封闭式反演，它允许有效构建配对的源-目标样本。尽管其成本极低，仅占完整模型训练所需计算量的 1.7%，但 $\texttt{PairFlow}$ 匹配甚至超过了涉及微调的两阶段训练的性能。此外，使用我们的框架训练的模型为后续蒸馏提供了更强大的基础模型，在微调后产生进一步的加速。对分子数据以及二进制和 RGB 图像的实验证明了我们的方法的广泛适用性和有效性。</li>
</ul>

<h3>Title: LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs</h3>
<ul>
<li><strong>Authors: </strong>Haiyun Wei, Fan Lu, Yunwei Zhu, Zehan Zheng, Weiyi Xue, Lin Shao, Xudong Zhang, Ya Wu, Rong Fu, Guang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20105">https://arxiv.org/abs/2512.20105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20105">https://arxiv.org/pdf/2512.20105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20105]] LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs(https://arxiv.org/abs/2512.20105)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating realistic and diverse LiDAR point clouds is crucial for autonomous driving simulation. Although previous methods achieve LiDAR point cloud generation from user inputs, they struggle to attain high-quality results while enabling versatile controllability, due to the imbalance between the complex distribution of LiDAR point clouds and the simple control signals. To address the limitation, we propose LiDARDraft, which utilizes the 3D layout to build a bridge between versatile conditional signals and LiDAR point clouds. The 3D layout can be trivially generated from various user inputs such as textual descriptions and images. Specifically, we represent text, images, and point clouds as unified 3D layouts, which are further transformed into semantic and depth control signals. Then, we employ a rangemap-based ControlNet to guide LiDAR point cloud generation. This pixel-level alignment approach demonstrates excellent performance in controllable LiDAR point clouds generation, enabling "simulation from scratch", allowing self-driving environments to be created from arbitrary textual descriptions, images and sketches.</li>
<li><strong>摘要：</strong>生成真实且多样化的激光雷达点云对于自动驾驶仿真至关重要。尽管以前的方法可以根据用户输入实现激光雷达点云生成，但由于激光雷达点云的复杂分布与简单的控制信号之间的不平衡，它们很难在实现多功能可控性的同时获得高质量的结果。为了解决这一限制，我们提出了 LiDARDraft，它利用 3D 布局在多功能条件信号和 LiDAR 点云之间建立桥梁。 3D 布局可以根据各种用户输入（例如文本描述和图像）轻松生成。具体来说，我们将文本、图像和点云表示为统一的 3D 布局，并进一步转换为语义和深度控制信号。然后，我们采用基于范围图的 ControlNet 来指导 LiDAR 点云生成。这种像素级对齐方法在可控 LiDAR 点云生成方面展示了出色的性能，实现了“从头开始模拟”，允许从任意文本描述、图像和草图创建自动驾驶环境。</li>
</ul>

<h3>Title: UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Thanh-Tung Le, Tuan Pham, Tung Nguyen, Deying Kong, Xiaohui Xie, Stephan Mandt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20107">https://arxiv.org/abs/2512.20107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20107">https://arxiv.org/pdf/2512.20107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20107]] UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis(https://arxiv.org/abs/2512.20107)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Novel view synthesis (NVS) seeks to render photorealistic, 3D-consistent images of a scene from unseen camera poses given only a sparse set of posed views. Existing deterministic networks render observed regions quickly but blur unobserved areas, whereas stochastic diffusion-based methods hallucinate plausible content yet incur heavy training- and inference-time costs. In this paper, we propose a hybrid framework that unifies the strengths of both paradigms. A bidirectional transformer encodes multi-view image tokens and Plucker-ray embeddings, producing a shared latent representation. Two lightweight heads then act on this representation: (i) a feed-forward regression head that renders pixels where geometry is well constrained, and (ii) a masked autoregressive diffusion head that completes occluded or unseen regions. The entire model is trained end-to-end with joint photometric and diffusion losses, without handcrafted 3D inductive biases, enabling scalability across diverse scenes. Experiments demonstrate that our method attains state-of-the-art image quality while reducing rendering time by an order of magnitude compared with fully generative baselines.</li>
<li><strong>摘要：</strong>新颖的视图合成 (NVS) 旨在仅在给定一组稀疏的姿势视图的情况下，根据看不见的相机姿势渲染场景的真实感、3D 一致性图像。现有的确定性网络可以快速渲染观察到​​的区域，但模糊了未观察到的区域，而基于随机扩散的方法会产生看似合理的内容，但会产生大量的训练和推理时间成本。在本文中，我们提出了一个混合框架，结合了两种范式的优点。双向转换器对多视图图像标记和 Plucker 射线嵌入进行编码，产生共享的潜在表示。然后，两个轻量级头对该表示起作用：（i）一个前馈回归头，用于渲染几何形状受到良好约束的像素，以及（ii）一个遮罩自回归扩散头，用于完成遮挡或看不见的区域。整个模型通过联合光度和扩散损失进行端到端训练，没有手工制作的 3D 归纳偏差，从而实现了跨不同场景的可扩展性。实验表明，与完全生成基线相比，我们的方法获得了最先进的图像质量，同时将渲染时间缩短了一个数量级。</li>
</ul>

<h3>Title: Generative Latent Coding for Ultra-Low Bitrate Image Compression</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyang Jia, Jiahao Li, Bin Li, Houqiang Li, Yan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20194">https://arxiv.org/abs/2512.20194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20194">https://arxiv.org/pdf/2512.20194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20194]] Generative Latent Coding for Ultra-Low Bitrate Image Compression(https://arxiv.org/abs/2512.20194)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Most existing image compression approaches perform transform coding in the pixel space to reduce its spatial redundancy. However, they encounter difficulties in achieving both high-realism and high-fidelity at low bitrate, as the pixel-space distortion may not align with human perception. To address this issue, we introduce a Generative Latent Coding (GLC) architecture, which performs transform coding in the latent space of a generative vector-quantized variational auto-encoder (VQ-VAE), instead of in the pixel space. The generative latent space is characterized by greater sparsity, richer semantic and better alignment with human perception, rendering it advantageous for achieving high-realism and high-fidelity compression. Additionally, we introduce a categorical hyper module to reduce the bit cost of hyper-information, and a code-prediction-based supervision to enhance the semantic consistency. Experiments demonstrate that our GLC maintains high visual quality with less than 0.04 bpp on natural images and less than 0.01 bpp on facial images. On the CLIC2020 test set, we achieve the same FID as MS-ILLM with 45% fewer bits. Furthermore, the powerful generative latent space enables various applications built on our GLC pipeline, such as image restoration and style transfer. The code is available at this https URL.</li>
<li><strong>摘要：</strong>大多数现有的图像压缩方法在像素空间中执行变换编码以减少其空间冗余。然而，他们在低比特率下实现高真实感和高保真度时遇到困难，因为像素空间失真可能与人类感知不相符。为了解决这个问题，我们引入了生成潜在编码（GLC）架构，它在生成矢量量化变分自动编码器（VQ-VAE）的潜在空间中而不是在像素空间中执行变换编码。生成潜在空间具有更大的稀疏性、更丰富的语义以及更好地符合人类感知的特点，使其有利于实现高真实感和高保真压缩。此外，我们引入了一个分类超级模块来降低超级信息的比特成本，并引入了基于代码预测的监督来增强语义一致性。实验表明，我们的 GLC 保持了较高的视觉质量，在自然图像上小于 0.04 bpp，在面部图像上小于 0.01 bpp。在 CLIC2020 测试集上，我们实现了与 MS-ILLM 相同的 FID，但位数减少了 45%。此外，强大的生成潜在空间使我们能够在 GLC 管道上构建各种应用程序，例如图像恢复和风格迁移。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: How I Met Your Bias: Investigating Bias Amplification in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Nathan Roos, Ekaterina Iakovleva, Ani Gjergji, Vito Paolo Pastore, Enzo Tartaglione</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20233">https://arxiv.org/abs/2512.20233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20233">https://arxiv.org/pdf/2512.20233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20233]] How I Met Your Bias: Investigating Bias Amplification in Diffusion Models(https://arxiv.org/abs/2512.20233)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based generative models demonstrate state-of-the-art performance across various image synthesis tasks, yet their tendency to replicate and amplify dataset biases remains poorly understood. Although previous research has viewed bias amplification as an inherent characteristic of diffusion models, this work provides the first analysis of how sampling algorithms and their hyperparameters influence bias amplification. We empirically demonstrate that samplers for diffusion models -- commonly optimized for sample quality and speed -- have a significant and measurable effect on bias amplification. Through controlled studies with models trained on Biased MNIST, Multi-Color MNIST and BFFHQ, and with Stable Diffusion, we show that sampling hyperparameters can induce both bias reduction and amplification, even when the trained model is fixed. Source code is available at this https URL.</li>
<li><strong>摘要：</strong>基于扩散的生成模型在各种图像合成任务中展示了最先进的性能，但它们复制和放大数据集偏差的倾向仍然知之甚少。尽管之前的研究将偏差放大视为扩散模型的固有特征，但这项工作首次分析了采样算法及其超参数如何影响偏差放大。我们凭经验证明，扩散模型的采样器（通常针对样本质量和速度进行优化）对偏差放大具有显着且可测量的影响。通过对在 Biased MNIST、Multi-Color MNIST 和 BFFHQ 上训练的模型以及稳定扩散进行的对照研究，我们表明，即使训练的模型是固定的，采样超参数也可以导致偏差减少和放大。源代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Degradation-Aware Metric Prompting for Hyperspectral Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Binfeng Wang, Di Wang, Haonan Guo, Ying Fu, Jing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20251">https://arxiv.org/abs/2512.20251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20251">https://arxiv.org/pdf/2512.20251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20251]] Degradation-Aware Metric Prompting for Hyperspectral Image Restoration(https://arxiv.org/abs/2512.20251)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Unified hyperspectral image (HSI) restoration aims to recover various degraded HSIs using a single model, offering great practical value. However, existing methods often depend on explicit degradation priors (e.g., degradation labels) as prompts to guide restoration, which are difficult to obtain due to complex and mixed degradations in real-world scenarios. To address this challenge, we propose a Degradation-Aware Metric Prompting (DAMP) framework. Instead of relying on predefined degradation priors, we design spatial-spectral degradation metrics to continuously quantify multi-dimensional degradations, serving as Degradation Prompts (DP). These DP enable the model to capture cross-task similarities in degradation distributions and enhance shared feature learning. Furthermore, we introduce a Spatial-Spectral Adaptive Module (SSAM) that dynamically modulates spatial and spectral feature extraction through learnable parameters. By integrating SSAM as experts within a Mixture-of-Experts architecture, and using DP as the gating router, the framework enables adaptive, efficient, and robust restoration under diverse, mixed, or unseen degradations. Extensive experiments on natural and remote sensing HSI datasets show that DAMP achieves state-of-the-art performance and demonstrates exceptional generalization capability. Code is publicly available at this https URL.</li>
<li><strong>摘要：</strong>统一高光谱图像（HSI）恢复旨在使用单一模型恢复各种退化的HSI，具有很大的实用价值。然而，现有的方法通常依赖于显式的退化先验（例如退化标签）作为指导恢复的提示，但由于现实场景中复杂且混合的退化而很难获得。为了应对这一挑战，我们提出了一个退化感知指标提示（DAMP）框架。我们不依赖预定义的退化先验，而是设计空间光谱退化指标来连续量化多维退化，作为退化提示（DP）。这些 DP 使模型能够捕获退化分布中的跨任务相似性并增强共享特征学习。此外，我们引入了空间光谱自适应模块（SSAM），它通过可学习的参数动态调制空间和光谱特征提取。通过将 SSAM 作为专家集成到专家混合架构中，并使用 DP 作为门控路由器，该框架可以在多种、混合或看不见的退化下实现自适应、高效和鲁棒的恢复。对自然和遥感 HSI 数据集的大量实验表明，DAMP 实现了最先进的性能，并展示了卓越的泛化能力。代码可通过此 https URL 公开获取。</li>
</ul>

<h3>Title: HGAN-SDEs: Learning Neural Stochastic Differential Equations with Hermite-Guided Adversarial Training</h3>
<ul>
<li><strong>Authors: </strong>Yuanjian Xu, Yuan Shuai, Jianing Hao, Guang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20272">https://arxiv.org/abs/2512.20272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20272">https://arxiv.org/pdf/2512.20272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20272]] HGAN-SDEs: Learning Neural Stochastic Differential Equations with Hermite-Guided Adversarial Training(https://arxiv.org/abs/2512.20272)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Neural Stochastic Differential Equations (Neural SDEs) provide a principled framework for modeling continuous-time stochastic processes and have been widely adopted in fields ranging from physics to finance. Recent advances suggest that Generative Adversarial Networks (GANs) offer a promising solution to learning the complex path distributions induced by SDEs. However, a critical bottleneck lies in designing a discriminator that faithfully captures temporal dependencies while remaining computationally efficient. Prior works have explored Neural Controlled Differential Equations (CDEs) as discriminators due to their ability to model continuous-time dynamics, but such architectures suffer from high computational costs and exacerbate the instability of adversarial training. To address these limitations, we introduce HGAN-SDEs, a novel GAN-based framework that leverages Neural Hermite functions to construct a structured and efficient discriminator. Hermite functions provide an expressive yet lightweight basis for approximating path-level dynamics, enabling both reduced runtime complexity and improved training stability. We establish the universal approximation property of our framework for a broad class of SDE-driven distributions and theoretically characterize its convergence behavior. Extensive empirical evaluations on synthetic and real-world systems demonstrate that HGAN-SDEs achieve superior sample quality and learning efficiency compared to existing generative models for SDEs</li>
<li><strong>摘要：</strong>神经随机微分方程（神经 SDE）为连续时间随机过程建模提供了原则框架，并已广泛应用于从物理到金融等领域。最近的进展表明，生成对抗网络 (GAN) 为学习 SDE 引起的复杂路径分布提供了一种有前景的解决方案。然而，一个关键的瓶颈在于设计一个鉴别器，该鉴别器能够忠实地捕获时间依赖性，同时保持计算效率。之前的工作已经探索了神经控制微分方程（CDE）作为判别器，因为它们能够模拟连续时间动态，但这种架构的计算成本很高，并加剧了对抗训练的不稳定性。为了解决这些限制，我们引入了 HGAN-SDE，这是一种基于 GAN 的新型框架，利用神经 Hermite 函数构建结构化且高效的判别器。 Hermite 函数为近似路径级动态提供了富有表现力且轻量级的基础，从而降低了运行时复杂性并提高了训练稳定性。我们为一类广泛的 SDE 驱动分布建立了框架的通用逼近性质，并从理论上描述了其收敛行为。对合成系统和现实世界系统的广泛实证评估表明，与现有的 SDE 生成模型相比，HGAN-SDE 实现了卓越的样本质量和学习效率</li>
</ul>

<h3>Title: TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation</h3>
<ul>
<li><strong>Authors: </strong>Ji-Hoon Kim, Junseok Ahn, Doyeop Kwak, Joon Son Chung, Shinji Watanabe</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.AS, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20296">https://arxiv.org/abs/2512.20296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20296">https://arxiv.org/pdf/2512.20296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20296]] TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation(https://arxiv.org/abs/2512.20296)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The objective of this paper is to jointly synthesize interactive videos and conversational speech from text and reference images. With the ultimate goal of building human-like conversational systems, recent studies have explored talking or listening head generation as well as conversational speech generation. However, these works are typically studied in isolation, overlooking the multimodal nature of human conversation, which involves tightly coupled audio-visual interactions. In this paper, we introduce TAVID, a unified framework that generates both interactive faces and conversational speech in a synchronized manner. TAVID integrates face and speech generation pipelines through two cross-modal mappers (i.e., a motion mapper and a speaker mapper), which enable bidirectional exchange of complementary information between the audio and visual modalities. We evaluate our system across four dimensions: talking face realism, listening head responsiveness, dyadic interaction fluency, and speech quality. Extensive experiments demonstrate the effectiveness of our approach across all these aspects.</li>
<li><strong>摘要：</strong>本文的目标是从文本和参考图像联合合成交互式视频和会话语音。为了构建类人对话系统的最终目标，最近的研究探索了说话或倾听的头部生成以及会话语音生成。然而，这些作品通常是孤立研究的，忽视了人类对话的多模态性质，其中涉及紧密耦合的视听交互。在本文中，我们介绍了 TAVID，这是一个统一的框架，可以同步生成交互式面孔和会话语音。 TAVID 通过两个跨模式映射器（即运动映射器和说话者映射器）集成面部和语音生成管道，从而实现音频和视觉模态之间互补信息的双向交换。我们从四个维度评估我们的系统：说话的面部真实度、聆听头的响应能力、二元交互的流畅性和语音质量。大量的实验证明了我们的方法在所有这些方面的有效性。</li>
</ul>

<h3>Title: Inverse Autoregressive Flows for Zero Degree Calorimeter fast simulation</h3>
<ul>
<li><strong>Authors: </strong>Emilia Majerz, Witold Dzwinel, Jacek Kitowski</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20346">https://arxiv.org/abs/2512.20346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20346">https://arxiv.org/pdf/2512.20346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20346]] Inverse Autoregressive Flows for Zero Degree Calorimeter fast simulation(https://arxiv.org/abs/2512.20346)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Physics-based machine learning blends traditional science with modern data-driven techniques. Rather than relying exclusively on empirical data or predefined equations, this methodology embeds domain knowledge directly into the learning process, resulting in models that are both more accurate and robust. We leverage this paradigm to accelerate simulations of the Zero Degree Calorimeter (ZDC) of the ALICE experiment at CERN. Our method introduces a novel loss function and an output variability-based scaling mechanism, which enhance the model's capability to accurately represent the spatial distribution and morphology of particle showers in detector outputs while mitigating the influence of rare artefacts on the training. Leveraging Normalizing Flows (NFs) in a teacher-student generative framework, we demonstrate that our approach not only outperforms classic data-driven model assimilation but also yields models that are 421 times faster than existing NF implementations in ZDC simulation literature.</li>
<li><strong>摘要：</strong>基于物理的机器学习将传统科学与现代数据驱动技术融为一体。这种方法不是完全依赖经验数据或预定义的方程，而是将领域知识直接嵌入到学习过程中，从而产生更准确、更稳健的模型。我们利用这种范例来加速 CERN ALICE 实验的零度量热计 (ZDC) 的模拟。我们的方法引入了一种新颖的损失函数和基于输出变异性的缩放机制，增强了模型准确表示探测器输出中粒子簇射的空间分布和形态的能力，同时减轻了罕见伪影对训练的影响。利用师生生成框架中的归一化流（NF），我们证明了我们的方法不仅优于经典的数据驱动模型同化，而且生成的模型比 ZDC 模拟文献中现有的 NF 实现快 421 倍。</li>
</ul>

<h3>Title: Field-Space Attention for Structure-Preserving Earth System Transformers</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Witte, Johannes Meuer, Étienne Plésiat, Christopher Kadow</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, math-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20350">https://arxiv.org/abs/2512.20350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20350">https://arxiv.org/pdf/2512.20350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20350]] Field-Space Attention for Structure-Preserving Earth System Transformers(https://arxiv.org/abs/2512.20350)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generation, generative</a></li>
<li><strong>Abstract: </strong>Accurate and physically consistent modeling of Earth system dynamics requires machine-learning architectures that operate directly on continuous geophysical fields and preserve their underlying geometric structure. Here we introduce Field-Space attention, a mechanism for Earth system Transformers that computes attention in the physical domain rather than in a learned latent space. By maintaining all intermediate representations as continuous fields on the sphere, the architecture enables interpretable internal states and facilitates the enforcement of scientific constraints. The model employs a fixed, non-learned multiscale decomposition and learns structure-preserving deformations of the input field, allowing coherent integration of coarse and fine-scale information while avoiding the optimization instabilities characteristic of standard single-scale Vision Transformers. Applied to global temperature super-resolution on a HEALPix grid, Field-Space Transformers converge more rapidly and stably than conventional Vision Transformers and U-Net baselines, while requiring substantially fewer parameters. The explicit preservation of field structure throughout the network allows physical and statistical priors to be embedded directly into the architecture, yielding improved fidelity and reliability in data-driven Earth system modeling. These results position Field-Space Attention as a compact, interpretable, and physically grounded building block for next-generation Earth system prediction and generative modeling frameworks.</li>
<li><strong>摘要：</strong>对地球系统动力学进行准确且物理一致的建模需要直接在连续地球物理场上运行并保留其底层几何结构的机器学习架构。在这里，我们介绍场空间注意力，这是地球系统 Transformer 的一种机制，它在物理域而不是在学习的潜在空间中计算注意力。通过将所有中间表示维持为球体上的连续场，该架构可以实现可解释的内部状态并促进科学约束的执行。该模型采用固定的、非学习的多尺度分解，并学习输入场的结构保持变形，允许粗尺度和细尺度信息的相干集成，同时避免标准单尺度视觉变换器的优化不稳定性特征。应用于 HEALPix 网格上的全局温度超分辨率时，Field-Space Transformers 比传统 Vision Transformers 和 U-Net 基线收敛得更快、更稳定，同时需要的参数要少得多。整个网络中场结构的显式保留允许将物理和统计先验直接嵌入到架构中，从而提高数据驱动的地球系统建模的保真度和可靠性。这些结果将场空间注意力定位为下一代地球系统预测和生成建模框架的紧凑、可解释且物理基础的构建块。</li>
</ul>

<h3>Title: CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>V. Kovalev, A. Kuvshinov, A. Buzovkin, D. Pokidov, D. Timonin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20362">https://arxiv.org/abs/2512.20362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20362">https://arxiv.org/pdf/2512.20362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20362]] CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation(https://arxiv.org/abs/2512.20362)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping. We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop. Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.</li>
<li><strong>摘要：</strong>最近的工作表明，推理时间推理和反射可以改进文本到图像的生成，而无需重新训练。然而，现有的方法通常依赖于隐含的、整体的批评或不受约束的即时重写，使得它们的行为难以可靠地解释、控制或停止。相比之下，大型语言模型受益于基于验证、有针对性的纠正和提前停止的明确、结构化的**思维**形式。我们引入了 CRAFT（连续推理和代理反馈调整），这是一种免训练、与模型无关的框架，它将这种结构化推理范式引入多模态图像生成中。 CRAFT 将提示分解为依赖结构的视觉问题，使用视觉语言模型验证生成的图像，并仅在约束失败时通过 LLM 代理应用有针对性的提示编辑。一旦满足所有约束，该过程就会使用显式停止标准进行迭代，从而产生可解释且可控的推理时间细化循环。在多个模型系列和具有挑战性的基准中，CRAFT 不断提高构图准确性、文本渲染和基于偏好的评估，尤其是轻量级生成器的收益尤其强劲。重要的是，这些改进只产生可以忽略不计的推理时间开销，允许更小或更便宜的模型接近更昂贵的系统的质量。我们的结果表明，明确结构化、约束驱动的推理时间推理是提高多模态生成模型可靠性的关键因素。</li>
</ul>

<h3>Title: SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images</h3>
<ul>
<li><strong>Authors: </strong>Linfei Li, Lin Zhang, Zhong Wang, Ying Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20377">https://arxiv.org/abs/2512.20377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20377">https://arxiv.org/pdf/2512.20377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20377]] SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images(https://arxiv.org/abs/2512.20377)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative AI have accelerated the production of ultra-high-resolution visual content, posing significant challenges for efficient compression and real-time decoding on end-user devices. Inspired by 3D Gaussian Splatting, recent 2D Gaussian image models improve representation efficiency, yet existing methods struggle to balance compression ratio and reconstruction fidelity in ultra-high-resolution scenarios. To address this issue, we propose SmartSplat, a highly adaptive and feature-aware GS-based image compression framework that supports arbitrary image resolutions and compression ratios. SmartSplat leverages image-aware features such as gradients and color variances, introducing a Gradient-Color Guided Variational Sampling strategy together with an Exclusion-based Uniform Sampling scheme to improve the non-overlapping coverage of Gaussian primitives in pixel space. In addition, we propose a Scale-Adaptive Gaussian Color Sampling method to enhance color initialization across scales. Through joint optimization of spatial layout, scale, and color initialization, SmartSplat efficiently captures both local structures and global textures using a limited number of Gaussians, achieving high reconstruction quality under strong compression. Extensive experiments on DIV8K and a newly constructed 16K dataset demonstrate that SmartSplat consistently outperforms state-of-the-art methods at comparable compression ratios and exceeds their compression limits, showing strong scalability and practical applicability. The code is publicly available at this https URL.</li>
<li><strong>摘要：</strong>生成式人工智能的最新进展加速了超高分辨率视觉内容的制作，对最终用户设备上的高效压缩和实时解码提出了重大挑战。受 3D 高斯分布的启发，最近的 2D 高斯图像模型提高了表示效率，但现有方法难以在超高分辨率场景中平衡压缩比和重建保真度。为了解决这个问题，我们提出了 SmartSplat，一种高度自适应和特征感知的基于 GS 的图像压缩框架，支持任意图像分辨率和压缩比。 SmartSplat 利用梯度和颜色方差等图像感知功能，引入渐变颜色引导变分采样策略以及基于排除的均匀采样方案，以改善像素空间中高斯基元的非重叠覆盖。此外，我们提出了一种尺度自适应高斯颜色采样方法来增强跨尺度的颜色初始化。通过空间布局、尺度和颜色初始化的联合优化，SmartSplat 使用有限数量的高斯有效捕获局部结构和全局纹理，在强压缩下实现高重建质量。在 DIV8K 和新构建的 16K 数据集上进行的大量实验表明，SmartSplat 在相当的压缩比下始终优于最先进的方法，并超出了其压缩极限，显示出强大的可扩展性和实际适用性。该代码可通过此 https URL 公开获取。</li>
</ul>

<h3>Title: UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images</h3>
<ul>
<li><strong>Authors: </strong>Yiming Zhao, Yuanpeng Gao, Yuxuan Luo, Jiwei Duan, Shisong Lin, Longfei Xiong, Zhouhui Lian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20479">https://arxiv.org/abs/2512.20479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20479">https://arxiv.org/pdf/2512.20479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20479]] UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images(https://arxiv.org/abs/2512.20479)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>AI-assisted graphic design has emerged as a powerful tool for automating the creation and editing of design elements such as posters, banners, and advertisements. While diffusion-based text-to-image models have demonstrated strong capabilities in visual content generation, their text rendering performance, particularly for small-scale typography and non-Latin scripts, remains limited. In this paper, we propose UTDesign, a unified framework for high-precision stylized text editing and conditional text generation in design images, supporting both English and Chinese scripts. Our framework introduces a novel DiT-based text style transfer model trained from scratch on a synthetic dataset, capable of generating transparent RGBA text foregrounds that preserve the style of reference glyphs. We further extend this model into a conditional text generation framework by training a multi-modal condition encoder on a curated dataset with detailed text annotations, enabling accurate, style-consistent text synthesis conditioned on background images, prompts, and layout specifications. Finally, we integrate our approach into a fully automated text-to-design (T2D) pipeline by incorporating pre-trained text-to-image (T2I) models and an MLLM-based layout planner. Extensive experiments demonstrate that UTDesign achieves state-of-the-art performance among open-source methods in terms of stylistic consistency and text accuracy, and also exhibits unique advantages compared to proprietary commercial approaches. Code and data for this paper are available at this https URL.</li>
<li><strong>摘要：</strong>人工智能辅助图形设计已成为自动创建和编辑海报、横幅和广告等设计元素的强大工具。虽然基于扩散的文本到图像模型在视觉内容生成方面表现出了强大的能力，但它们的文本渲染性能，特别是对于小规模排版和非拉丁文字，仍然有限。在本文中，我们提出了UTDesign，一个用于设计图像中高精度风格化文本编辑和条件文本生成的统一框架，支持英文和中文脚本。我们的框架引入了一种新颖的基于 DiT 的文本样式传输模型，该模型在合成数据集上从头开始训练，能够生成透明的 RGBA 文本前景，从而保留参考字形的样式。我们通过在具有详细文本注释的精选数据集上训练多模式条件编码器，进一步将该模型扩展到条件文本生成框架，从而实现以背景图像、提示和布局规范为条件的准确、风格一致的文本合成。最后，我们通过结合预先训练的文本到图像 (T2I) 模型和基于 MLLM 的布局规划器，将我们的方法集成到完全自动化的文本到设计 (T2D) 管道中。大量实验表明，UTDesign 在文体一致性和文本准确性方面实现了开源方法中最先进的性能，并且与专有商业方法相比还表现出独特的优势。本文的代码和数据可从此 https URL 获取。</li>
</ul>

<h3>Title: Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition</h3>
<ul>
<li><strong>Authors: </strong>Gorjan Radevski</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20501">https://arxiv.org/abs/2512.20501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20501">https://arxiv.org/pdf/2512.20501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20501]] Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition(https://arxiv.org/abs/2512.20501)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This manuscript explores multimodal alignment, translation, fusion, and transference to enhance machine understanding of complex inputs. We organize the work into five chapters, each addressing unique challenges in multimodal machine learning. Chapter 3 introduces Spatial-Reasoning Bert for translating text-based spatial relations into 2D arrangements between clip-arts. This enables effective decoding of spatial language into visual representations, paving the way for automated scene generation aligned with human spatial understanding. Chapter 4 presents a method for translating medical texts into specific 3D locations within an anatomical atlas. We introduce a loss function leveraging spatial co-occurrences of medical terms to create interpretable mappings, significantly enhancing medical text navigability. Chapter 5 tackles translating structured text into canonical facts within knowledge graphs. We develop a benchmark for linking natural language to entities and predicates, addressing ambiguities in text extraction to provide clearer, actionable insights. Chapter 6 explores multimodal fusion methods for compositional action recognition. We propose a method fusing video frames and object detection representations, improving recognition robustness and accuracy. Chapter 7 investigates multimodal knowledge transference for egocentric action recognition. We demonstrate how multimodal knowledge distillation enables RGB-only models to mimic multimodal fusion-based capabilities, reducing computational requirements while maintaining performance. These contributions advance methodologies for spatial language understanding, medical text interpretation, knowledge graph enrichment, and action recognition, enhancing computational systems' ability to process complex, multimodal inputs across diverse applications.</li>
<li><strong>摘要：</strong>本手稿探讨了多模态对齐、翻译、融合和转移，以增强机器对复杂输入的理解。我们将工作分为五章，每一章都解决多模式机器学习中的独特挑战。第 3 章介绍了 Spatial-Reasoning Bert，用于将基于文本的空间关系转换为剪贴画之间的 2D 排列。这使得能够将空间语言有效解码为视觉表示，为与人类空间理解相一致的自动场景生成铺平道路。第 4 章介绍了一种将医学文本翻译成解剖图集中特定 3D 位置的方法。我们引入了一种损失函数，利用医学术语的空间共现来创建可解释的映射，从而显着增强医学文本的可导航性。第 5 章讨论将结构化文本转换为知识图中的规范事实。我们开发了一个基准，用于将自然语言与实体和谓词联系起来，解决文本提取中的歧义，以提供更清晰、可操作的见解。第 6 章探讨了组合动作识别的多模态融合方法。我们提出了一种融合视频帧和对象检测表示的方法，提高了识别的鲁棒性和准确性。第 7 章研究了以自我为中心的行为识别的多模态知识迁移。我们演示了多模态知识蒸馏如何使仅 RGB 模型能够模仿基于多模态融合的功能，从而在保持性能的同时降低计算要求。这些贡献推进了空间语言理解、医学文本解释、知识图丰富和动作识别的方法，增强了计算系统跨不同应用处理复杂、多模态输入的能力。</li>
</ul>

<h3>Title: Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs</h3>
<ul>
<li><strong>Authors: </strong>Rui Pan, Zhuofu Chen, Ravi Netravali</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20573">https://arxiv.org/abs/2512.20573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20573">https://arxiv.org/pdf/2512.20573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20573]] Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs(https://arxiv.org/abs/2512.20573)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It "fails fast" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and "wins big" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\times$ speedup over vanilla decoding, 1.7$\times$ over the best naive dLLM drafter, and 1.4$\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at this https URL.</li>
<li><strong>摘要：</strong>扩散大型语言模型（dLLM）提供快速、并行的代币生成，但它们的独立使用受到固有的效率与质量权衡的困扰。我们表明，如果仔细应用，dLLM 的属性实际上可以成为起草者使用自回归 (AR) 验证器进行推测解码的优势。我们的核心见解是，dLLM 的并行解码速度大大降低了代价高昂的拒绝风险，提供了一种实用的机制来有效实现（难以捉摸的）冗长的草稿，从而通过推测性解码大幅加速。我们提出了 FailFast，一种基于 dLLM 的推测解码框架，它通过动态调整其推测长度来实现这种方法。它通过在难以推测的区域中花费最少的计算来缩短推测延迟来“快速失败”，并通过在较容易的区域中积极延长草稿长度以减少验证延迟来“赢得巨大”（在许多情况下，一次推测并接受 70 个代币！）。无需任何微调，FailFast 即可实现 AR LLM 的无损加速，在不同模型和工作负载下，与普通解码相比，速度提升高达 4.9$\times$，比最佳 naive dLLM 起草者提升 1.7$\times$，比 EAGLE-3 提升 1.4$\times$。我们在此 https URL 开源 FailFast。</li>
</ul>

<h3>Title: Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning</h3>
<ul>
<li><strong>Authors: </strong>Seijin Kobayashi, Yanick Schimpf, Maximilian Schlegel, Angelika Steger, Maciej Wolczyk, Johannes von Oswald, Nino Scherre, Kaitlin Maile, Guillaume Lajoie, Blake A. Richards, Rif A. Saurous, James Manyika, Blaise Agüera y Arcas, Alexander Meulemans, João Sacramento</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20605">https://arxiv.org/abs/2512.20605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20605">https://arxiv.org/pdf/2512.20605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20605]] Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning(https://arxiv.org/abs/2512.20605)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term "internal RL", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.</li>
<li><strong>摘要：</strong>针对下一个标记预测进行预训练并通过强化学习 (RL) 进行微调的大规模自回归模型在许多问题领域取得了前所未有的成功。在强化学习期间，这些模型通过生成新输出（一次一个令牌）来进行探索。然而，逐个对动作进行采样可能会导致学习效率极低，尤其是在奖励稀疏的情况下。在这里，我们证明可以通过在自回归模型的内部表示中进行行动和探索来克服这个问题。具体来说，为了发现时间抽象动作，我们引入了一个高阶、非因果序列模型，其输出控制基本自回归模型的残差流激活。在具有层次结构的网格世界和基于 MuJoCo 的任务中，我们发现高阶模型学习将长激活序列块压缩到内部控制器上。至关重要的是，每个控制器执行一系列具有行为意义的动作，这些动作在很长的时间尺度上展开，并伴随着学习的终止条件，这样随着时间的推移组合多个控制器可以有效地探索新任务。我们证明，直接内部控制器强化（我们称之为“内部强化学习”的过程）可以在标准强化学习微调失败的情况下从稀疏奖励中进行学习。我们的结果证明了自回归模型中潜在动作生成和强化的好处，表明内部强化学习是在基础模型中实现分层强化学习的有前途的途径。</li>
</ul>

<h3>Title: Active Intelligence in Video Avatars via Closed-loop World Modeling</h3>
<ul>
<li><strong>Authors: </strong>Xuanhua He, Tianyu Yang, Ke Cao, Ruiqi Wu, Cheng Meng, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Qifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20615">https://arxiv.org/abs/2512.20615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20615">https://arxiv.org/pdf/2512.20615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20615]] Active Intelligence in Video Avatars via Closed-loop World Modeling(https://arxiv.org/abs/2512.20615)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.</li>
<li><strong>摘要：</strong>当前的视频头像生成方法擅长身份保存和运动对齐，但缺乏真正的代理，它们无法通过自适应环境交互自主地追求长期目标。我们通过引入 L-IVA（长视野交互式视觉化身）和 ORCA（在线推理和认知架构）来解决这个问题，L-IVA 是一种在随机生成环境中评估目标导向规划的任务和基准，ORCA 是第一个在视频化身中实现主动智能的框架。 ORCA 通过两项关键创新体现了内部世界模型 (IWM) 功能：(1) 闭环 OTAR 循环（观察-思考-行动-反思），通过不断验证实际生成的预测结​​果，在生成不确定性下保持强大的状态跟踪；(2) 分层双系统架构，其中系统 2 通过状态预测进行战略推理，而系统 1 将抽象计划转换为精确的、特定于模型的动作说明。通过将化身控制制定为 POMDP 并通过结果验证实现持续信念更新，ORCA 能够在开放域场景中自主完成多步骤任务。大量实验表明，ORCA 在任务成功率和行为一致性方面显着优于开环和非反射基线，验证了我们受 IWM 启发的设计，可将视频头像智能从被动动画提升为主动、目标导向的行为。</li>
</ul>

<h3>Title: SemanticGen: Video Generation in Semantic Space</h3>
<ul>
<li><strong>Authors: </strong>Jianhong Bai, Xiaoshi Wu, Xintao Wang, Fu Xiao, Yuanxing Zhang, Qinghe Wang, Xiaoyu Shi, Menghan Xia, Zuozhu Liu, Haoji Hu, Pengfei Wan, Kun Gai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.20619">https://arxiv.org/abs/2512.20619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.20619">https://arxiv.org/pdf/2512.20619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.20619]] SemanticGen: Video Generation in Semantic Space(https://arxiv.org/abs/2512.20619)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.</li>
<li><strong>摘要：</strong>最先进的视频生成模型通常会学习 VAE 空间中视频潜伏的分布，并使用 VAE 解码器将它们映射到像素。虽然这种方法可以生成高质量的视频，但它的收敛速度较慢，并且在生成长视频时计算成本较高。在本文中，我们介绍了 SemanticGen，这是一种通过在语义空间中生成视频来解决这些限制的新颖解决方案。我们的主要见解是，由于视频固有的冗余性，生成过程应该从紧凑的高级语义空间开始进行全局规划，然后添加高频细节，而不是使用双向注意力直接对大量低级视频标记进行建模。 SemanticGen 采用两阶段生成过程。在第一阶段，扩散模型生成紧凑的语义视频特征，定义视频的全局布局。在第二阶段，另一个扩散模型根据这些语义特征生成 VAE 潜伏，以产生最终输出。我们观察到，与 VAE 潜在空间相比，语义空间中的生成会导致更快的收敛。当扩展到长视频生成时，我们的方法也是有效且计算效率高的。大量的实验表明，SemanticGen 可以生成高质量的视频，并且性能优于最先进的方法和强大的基线。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
