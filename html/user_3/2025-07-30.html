<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-30</h1>
<h3>Title: Deep Unfolding for MIMO Signal Detection</h3>
<ul>
<li><strong>Authors: </strong>Hangli Ge, Noboru Koshizuka</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21152">https://arxiv.org/abs/2507.21152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21152">https://arxiv.org/pdf/2507.21152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21152]] Deep Unfolding for MIMO Signal Detection(https://arxiv.org/abs/2507.21152)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a deep unfolding neural network-based MIMO detector that incorporates complex-valued computations using Wirtinger calculus. The method, referred as Dynamic Partially Shrinkage Thresholding (DPST), enables efficient, interpretable, and low-complexity MIMO signal detection. Unlike prior approaches that rely on real-valued approximations, our method operates natively in the complex domain, aligning with the fundamental nature of signal processing tasks. The proposed algorithm requires only a small number of trainable parameters, allowing for simplified training. Numerical results demonstrate that the proposed method achieves superior detection performance with fewer iterations and lower computational complexity, making it a practical solution for next-generation massive MIMO systems.</li>
<li><strong>摘要：</strong>在本文中，我们提出了一个深层展开的基于神经网络的MIMO检测器，该探测器结合了使用Wirewinger conculus进行复杂值计算的计算。该方法被称为动态部分收缩阈值（DPST），可实现有效，可解释和低复杂性mimo信号检测。与依靠实现近似值的先前方法不同，我们的方法本地在复杂的域中运行，与信号处理任务的基本性质保持一致。所提出的算法仅需要少数可训练的参数，以进行简化的培训。数值结果表明，所提出的方法以更少的迭代和较低的计算复杂性实现了出色的检测性能，从而使其成为下一代大型MIMO系统的实用解决方案。</li>
</ul>

<h3>Title: EdgeAgentX-DT: Integrating Digital Twins and Generative AI for Resilient Edge Intelligence in Tactical Networks</h3>
<ul>
<li><strong>Authors: </strong>Abir Ray</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21196">https://arxiv.org/abs/2507.21196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21196">https://arxiv.org/pdf/2507.21196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21196]] EdgeAgentX-DT: Integrating Digital Twins and Generative AI for Resilient Edge Intelligence in Tactical Networks(https://arxiv.org/abs/2507.21196)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce EdgeAgentX-DT, an advanced extension of the EdgeAgentX framework that integrates digital twin simulations and generative AI-driven scenario training to significantly enhance edge intelligence in military networks. EdgeAgentX-DT utilizes network digital twins, virtual replicas synchronized with real-world edge devices, to provide a secure, realistic environment for training and validation. Leveraging generative AI methods, such as diffusion models and transformers, the system creates diverse and adversarial scenarios for robust simulation-based agent training. Our multi-layer architecture includes: (1) on-device edge intelligence; (2) digital twin synchronization; and (3) generative scenario training. Experimental simulations demonstrate notable improvements over EdgeAgentX, including faster learning convergence, higher network throughput, reduced latency, and improved resilience against jamming and node failures. A case study involving a complex tactical scenario with simultaneous jamming attacks, agent failures, and increased network loads illustrates how EdgeAgentX-DT sustains operational performance, whereas baseline methods fail. These results highlight the potential of digital-twin-enabled generative training to strengthen edge AI deployments in contested environments.</li>
<li><strong>摘要：</strong>我们介绍了Edgeagentx-DT，这是Edgeagentx框架的高级扩展，该框架集成了数字双模拟和生成AI驱动的场景训练，以显着增强军事网络中的边缘智能。 Edgeagentx-DT利用网络数字双胞胎，虚拟复制品与现实世界边缘设备同步，为培训和验证提供了安全，现实的环境。利用生成的AI方法，例如扩散模型和变压器，该系统为基于稳健的基于模拟的代理训练创造了多样化和对抗性的场景。我们的多层体系结构包括：（1）设备边缘智能； （2）数字双同步； （3）生成场景培训。实验模拟显示了对Edgeagentx的显着改善，包括更快的学习收敛性，更高的网络吞吐量，减少延迟以及针对干扰和节点失败的弹性提高。案例研究涉及复杂的战术场景，并同时干扰攻击，代理失败和增加网络负载，这说明了Edgeagentx-DT如何维持运营性能，而基线方法则失败。这些结果突出了基于数字双线的生成培训的潜力，以增强有争议的环境中的边缘AI部署。</li>
</ul>

<h3>Title: PanoGAN A Deep Generative Model for Panoramic Dental Radiographs</h3>
<ul>
<li><strong>Authors: </strong>Soren Pedersen, Sanyam Jain, Mikkel Chavez, Viktor Ladehoff, Bruna Neves de Freitas, Ruben Pauwels</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.ET, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21200">https://arxiv.org/abs/2507.21200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21200">https://arxiv.org/pdf/2507.21200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21200]] PanoGAN A Deep Generative Model for Panoramic Dental Radiographs(https://arxiv.org/abs/2507.21200)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents the development of a generative adversarial network (GAN) for synthesizing dental panoramic radiographs. Although exploratory in nature, the study aims to address the scarcity of data in dental research and education. We trained a deep convolutional GAN (DCGAN) using a Wasserstein loss with gradient penalty (WGANGP) on a dataset of 2322 radiographs of varying quality. The focus was on the dentoalveolar regions, other anatomical structures were cropped out. Extensive preprocessing and data cleaning were performed to standardize the inputs while preserving anatomical variability. We explored four candidate models by varying critic iterations, feature depth, and the use of denoising prior to training. A clinical expert evaluated the generated radiographs based on anatomical visibility and realism, using a 5-point scale (1 very poor 5 excellent). Most images showed moderate anatomical depiction, although some were degraded by artifacts. A trade-off was observed the model trained on non-denoised data yielded finer details especially in structures like the mandibular canal and trabecular bone, while a model trained on denoised data offered superior overall image clarity and sharpness. These findings provide a foundation for future work on GAN-based methods in dental imaging.</li>
<li><strong>摘要：</strong>本文介绍了用于合成牙齿全景X光片的生成对抗网络（GAN）的开发。尽管本质上是探索性的，但该研究旨在解决牙科研究和教育中数据的稀缺性。我们在2322张质量不同的X光片数据集上使用了Wasserstein损失（WGANGP）训练了深度卷积GAN（DCGAN）。重点是齿状肺泡区域，其他解剖结构被裁剪出来。进行了广泛的预处理和数据清洁，以标准化输入，同时保留解剖变异性。我们通过改变批评家的迭代，特征深度以及在培训前使用DeNoing的使用来探索四个候选模型。一位临床专家使用5分制（1非常差5个出色）评估了基于解剖学可见性和现实主义的产生的X光片。大多数图像显示中等解剖学描述，尽管有些是被伪影降解的。观察到了对未命名数据训练的模型产生的细节，尤其是在下颌运河和小梁骨等结构中，而对deno的数据训练的模型则提供了卓越的整体图像清晰度和清晰度。这些发现为牙科成像中基于GAN的方法的未来工作奠定了基础。</li>
</ul>

<h3>Title: Learning from Limited and Imperfect Data</h3>
<ul>
<li><strong>Authors: </strong>Harsh Rangwani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21205">https://arxiv.org/abs/2507.21205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21205">https://arxiv.org/pdf/2507.21205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21205]] Learning from Limited and Imperfect Data(https://arxiv.org/abs/2507.21205)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The distribution of data in the world (eg, internet, etc.) significantly differs from the well-curated datasets and is often over-populated with samples from common categories. The algorithms designed for well-curated datasets perform suboptimally when used for learning from imperfect datasets with long-tailed imbalances and distribution shifts. To expand the use of deep models, it is essential to overcome the labor-intensive curation process by developing robust algorithms that can learn from diverse, real-world data distributions. Toward this goal, we develop practical algorithms for Deep Neural Networks which can learn from limited and imperfect data present in the real world. This thesis is divided into four segments, each covering a scenario of learning from limited or imperfect data. The first part of the thesis focuses on Learning Generative Models from Long-Tail Data, where we mitigate the mode-collapse and enable diverse aesthetic image generations for tail (minority) classes. In the second part, we enable effective generalization on tail classes through Inductive Regularization schemes, which allow tail classes to generalize as effectively as the head classes without requiring explicit generation of images. In the third part, we develop algorithms for Optimizing Relevant Metrics for learning from long-tailed data with limited annotation (semi-supervised), followed by the fourth part, which focuses on the Efficient Domain Adaptation of the model to various domains with very few to zero labeled samples.</li>
<li><strong>摘要：</strong>世界上数据的分布（例如，互联网等）与策划的数据集有很大不同，并且通常与共同类别的样本过度居民。当用于从不完美的数据集中学习时，专为经过精心策划的数据集设计的算法次优地表现出色。为了扩大深层模型的使用，必须通过开发可从多样化的现实世界数据分布中学习的强大算法来克服劳动密集型策展过程。为了实现这一目标，我们开发了深层神经网络的实用算法，这些算法可以从现实世界中存在的有限和不完美的数据中学习。该论文分为四个部分，每个部分涵盖了从有限或不完美数据中学习的情况。论文的第一部分重点是从长尾数据中学习生成模型，在这里我们减轻模式崩溃，并使尾巴（少数族裔）类别的各种美学图像世代。在第二部分中，我们通过归纳正则化方案在尾部类中有效地概括，该方案使尾部类别能够像头类一样有效地概括，而无需明确生成图像。在第三部分中，我们开发了用于优化相关指标的算法，以从有限的注释（半监督）中从长尾数据中学习，然后是第四部分，该部分的重点是该模型对各个域的有效域适应，具有很少的标记样品。</li>
</ul>

<h3>Title: On Explaining Visual Captioning with Hybrid Markov Logic Networks</h3>
<ul>
<li><strong>Authors: </strong>Monika Shah, Somdeb Sarkhel, Deepak Venugopal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21246">https://arxiv.org/abs/2507.21246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21246">https://arxiv.org/pdf/2507.21246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21246]] On Explaining Visual Captioning with Hybrid Markov Logic Networks(https://arxiv.org/abs/2507.21246)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deep Neural Networks (DNNs) have made tremendous progress in multimodal tasks such as image captioning. However, explaining/interpreting how these models integrate visual information, language information and knowledge representation to generate meaningful captions remains a challenging problem. Standard metrics to measure performance typically rely on comparing generated captions with human-written ones that may not provide a user with a deep insights into this integration. In this work, we develop a novel explanation framework that is easily interpretable based on Hybrid Markov Logic Networks (HMLNs) - a language that can combine symbolic rules with real-valued functions - where we hypothesize how relevant examples from the training data could have influenced the generation of the observed caption. To do this, we learn a HMLN distribution over the training instances and infer the shift in distributions over these instances when we condition on the generated sample which allows us to quantify which examples may have been a source of richer information to generate the observed caption. Our experiments on captions generated for several state-of-the-art captioning models using Amazon Mechanical Turk illustrate the interpretability of our explanations, and allow us to compare these models along the dimension of explainability.</li>
<li><strong>摘要：</strong>深度神经网络（DNN）在多模式任务（例如图像字幕）中取得了巨大进步。但是，解释/解释这些模型如何整合视觉信息，语言信息和知识表示形式以生成有意义的标题仍然是一个具有挑战性的问题。衡量性能的标准指标通常依赖于将生成的字幕与可能无法为用户提供对该集成的深刻见解的标题。在这项工作中，我们开发了一个新颖的解释框架，该框架可以基于混合马尔可夫逻辑网络（HMLN）易于解释，该语言可以将符号规则与实用值的功能相结合 - 我们假设来自训练数据中的相关示例如何影响观察到的字幕的产生。为此，我们在培训实例上学习了HMLN分布，并在生成样本的条件下推断分布的变化，这使我们能够量化哪些示例可能是更丰富的信息来源以生成观察到的字幕。我们对使用亚马逊机械土耳其人为几个最新字幕模型生成的标题实验的实验说明了我们的解释性，并允许我们沿解释性的维度比较这些模型。</li>
</ul>

<h3>Title: Adaptive Multimodal Protein Plug-and-Play with Diffusion-Based Priors</h3>
<ul>
<li><strong>Authors: </strong>Amartya Banerjee, Xingyu Xu, Caroline Moosmüller, Harlin Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21260">https://arxiv.org/abs/2507.21260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21260">https://arxiv.org/pdf/2507.21260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21260]] Adaptive Multimodal Protein Plug-and-Play with Diffusion-Based Priors(https://arxiv.org/abs/2507.21260)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In an inverse problem, the goal is to recover an unknown parameter (e.g., an image) that has typically undergone some lossy or noisy transformation during measurement. Recently, deep generative models, particularly diffusion models, have emerged as powerful priors for protein structure generation. However, integrating noisy experimental data from multiple sources to guide these models remains a significant challenge. Existing methods often require precise knowledge of experimental noise levels and manually tuned weights for each data modality. In this work, we introduce Adam-PnP, a Plug-and-Play framework that guides a pre-trained protein diffusion model using gradients from multiple, heterogeneous experimental sources. Our framework features an adaptive noise estimation scheme and a dynamic modality weighting mechanism integrated into the diffusion process, which reduce the need for manual hyperparameter tuning. Experiments on complex reconstruction tasks demonstrate significantly improved accuracy using Adam-PnP.</li>
<li><strong>摘要：</strong>在反问题中，目标是恢复通常在测量过程中经历过一些有损或嘈杂的转换的未知参数（例如图像）。最近，深层生成模型，尤其是扩散模型，已成为蛋白质结构产生的强大先验。但是，从多个来源集成嘈杂的实验数据来指导这些模型仍然是一个重大挑战。现有方法通常需要针对每种数据模式的实验噪声水平和手动调整权重的精确知识。在这项工作中，我们引入了Adam-PNP，这是一种插件框架，该框架使用来自多个异质实验来源的梯度引导预训练的蛋白扩散模型。我们的框架具有自适应噪声估计方案和集成到扩散过程中的动态形态加权机制，从而减少了对手动超参数调整的需求。关于复杂重建任务的实验表明，使用ADAM-PNP可以显着提高准确性。</li>
</ul>

<h3>Title: VoluMe -- Authentic 3D Video Calls from Live Gaussian Splat Prediction</h3>
<ul>
<li><strong>Authors: </strong>Martin de La Gorce, Charlie Hewitt, Tibor Takacs, Robert Gerdisch, Zafiirah Hosenie, Givi Meishvili, Marek Kowalski, Thomas J. Cashman, Antonio Criminisi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21311">https://arxiv.org/abs/2507.21311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21311">https://arxiv.org/pdf/2507.21311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21311]] VoluMe -- Authentic 3D Video Calls from Live Gaussian Splat Prediction(https://arxiv.org/abs/2507.21311)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Virtual 3D meetings offer the potential to enhance copresence, increase engagement and thus improve effectiveness of remote meetings compared to standard 2D video calls. However, representing people in 3D meetings remains a challenge; existing solutions achieve high quality by using complex hardware, making use of fixed appearance via enrolment, or by inverting a pre-trained generative model. These approaches lead to constraints that are unwelcome and ill-fitting for videoconferencing applications. We present the first method to predict 3D Gaussian reconstructions in real time from a single 2D webcam feed, where the 3D representation is not only live and realistic, but also authentic to the input video. By conditioning the 3D representation on each video frame independently, our reconstruction faithfully recreates the input video from the captured viewpoint (a property we call authenticity), while generalizing realistically to novel viewpoints. Additionally, we introduce a stability loss to obtain reconstructions that are temporally stable on video sequences. We show that our method delivers state-of-the-art accuracy in visual quality and stability metrics compared to existing methods, and demonstrate our approach in live one-to-one 3D meetings using only a standard 2D camera and display. This demonstrates that our approach can allow anyone to communicate volumetrically, via a method for 3D videoconferencing that is not only highly accessible, but also realistic and authentic.</li>
<li><strong>摘要：</strong>与标准2D视频呼叫相比，虚拟3D会议提供了增强综合，增加参与度并从而提高远程会议的有效性的潜力。但是，在3D会议中代表人们仍然是一个挑战。现有解决方案通过使用复杂的硬件，通过注册使用固定外观或反转预先训练的生成模型来实现高质量。这些方法导致了对视频会议应用程序不受欢迎且不合适的限制。我们提出了第一种实时预测3D高斯重建的方法，从单个2D网络摄像头提要中，3D表示不仅是实时和现实的，而且对输入视频也是真实的。通过独立地在每个视频框架上调节3D表示，我们的重建从捕获的观点（我们称为真实性）忠实地重新创建了输入视频，同时将实际概括为新的观点。此外，我们引入了稳定损失，以获得视频序列上时间稳定的重建。我们表明，与现有方法相比，我们的方法可提供最新的视觉质量和稳定性指标的准确性，并仅使用标准2D摄像头和显示器展示我们在实时一对一的3D会议中的方法。这表明我们的方法可以使任何人通过3D视频会议的方法进行沟通，该方法不仅易于访问，而且现实且真实。</li>
</ul>

<h3>Title: Multimodal LLMs as Customized Reward Models for Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Shijie Zhou, Ruiyi Zhang, Huaisheng Zhu, Branislav Kveton, Yufan Zhou, Jiuxiang Gu, Jian Chen, Changyou Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21391">https://arxiv.org/abs/2507.21391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21391">https://arxiv.org/pdf/2507.21391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21391]] Multimodal LLMs as Customized Reward Models for Text-to-Image Generation(https://arxiv.org/abs/2507.21391)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce LLaVA-Reward, an efficient reward model designed to automatically evaluate text-to-image (T2I) generations across multiple perspectives, leveraging pretrained multimodal large language models (MLLMs). Existing MLLM-based approaches require instruction-following data for supervised fine-tuning and evaluate generation quality on analyzing text response, which is time-consuming and difficult to train. To address this problem, we propose LLaVA-Reward, which directly utilizes the hidden states of MLLMs given text-image pairs. To enhance the bidirectional interaction between visual and textual representations in decoder-only MLLMs, we further propose adding a Skip-connection Cross Attention (SkipCA) module. This design enhances text-image correlation reasoning by connecting early-layer visual features with later-layer hidden this http URL addition, LLaVA-Reward supports different types of preference data for efficient fine-tuning, including paired preference data and unpaired data. We train LLaVA-Reward on four evaluation perspectives: text-image alignment, fidelity/artifact, safety, and overall ranking. Empirical results demonstrate that LLaVA-Reward outperforms conventional and MLLM-based methods in generating human-aligned scores for automatic evaluations and inference-time scaling in text-to-image generations.</li>
<li><strong>摘要：</strong>我们介绍了Llava-Reward，这是一种有效的奖励模型，旨在自动评估文本对图像（T2I）的几代，利用了预处理的多峰大语言模型（MLLMS）。现有的基于MLLM的方法需要遵循指导数据的数据，以监督微调和评估分析文本响应时的发电质量，这是耗时且难以训练的。为了解决这个问题，我们提出了Llava-Reward，它直接利用了给定文本图像对的MLLM的隐藏状态。为了增强仅解码器MLLM中视觉和文本表示之间的双向相互作用，我们进一步提出了添加跳过连接的交叉注意（SKIPCA）模块。该设计通过将早期视觉特征与后期层连接到隐藏的HTTP URL添加来增强文本图像相关性推理，Llava-Reward支持不同类型的偏好数据，以进行有效的微调，包括配对的首选项数据和不纪念数据。我们以四种评估观点来训练Llava-Reward：文本图像对齐，忠诚/人工制，安全性和整体排名。经验结果表明，LLAVA奖励在生成人类对准的分数以自动评估和推理时间缩放的文本到图像中的传统和基于MLLM的方法。</li>
</ul>

<h3>Title: MapDiffusion: Generative Diffusion for Vectorized Online HD Map Construction and Uncertainty Estimation in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Thomas Monninger, Zihan Zhang, Zhipeng Mo, Md Zafar Anwar, Steffen Staab, Sihao Ding</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21423">https://arxiv.org/abs/2507.21423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21423">https://arxiv.org/pdf/2507.21423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21423]] MapDiffusion: Generative Diffusion for Vectorized Online HD Map Construction and Uncertainty Estimation in Autonomous Driving(https://arxiv.org/abs/2507.21423)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Autonomous driving requires an understanding of the static environment from sensor data. Learned Bird's-Eye View (BEV) encoders are commonly used to fuse multiple inputs, and a vector decoder predicts a vectorized map representation from the latent BEV grid. However, traditional map construction models provide deterministic point estimates, failing to capture uncertainty and the inherent ambiguities of real-world environments, such as occlusions and missing lane markings. We propose MapDiffusion, a novel generative approach that leverages the diffusion paradigm to learn the full distribution of possible vectorized maps. Instead of predicting a single deterministic output from learned queries, MapDiffusion iteratively refines randomly initialized queries, conditioned on a BEV latent grid, to generate multiple plausible map samples. This allows aggregating samples to improve prediction accuracy and deriving uncertainty estimates that directly correlate with scene ambiguity. Extensive experiments on the nuScenes dataset demonstrate that MapDiffusion achieves state-of-the-art performance in online map construction, surpassing the baseline by 5% in single-sample performance. We further show that aggregating multiple samples consistently improves performance along the ROC curve, validating the benefit of distribution modeling. Additionally, our uncertainty estimates are significantly higher in occluded areas, reinforcing their value in identifying regions with ambiguous sensor input. By modeling the full map distribution, MapDiffusion enhances the robustness and reliability of online vectorized HD map construction, enabling uncertainty-aware decision-making for autonomous vehicles in complex environments.</li>
<li><strong>摘要：</strong>自动驾驶需要从传感器数据中了解静态环境。学到的鸟眼视图（BEV）编码器通常用于融合多个输入，并且矢量解码器预测来自潜在BEV网格的矢量化MAP表示。但是，传统的地图构造模型提供了确定的点估计值，无法捕获不确定性和现实世界环境的固有歧义，例如遮挡和缺少车道标记。我们提出了MapDiffusion，这是一种新型生成方法，利用扩散范式学习可能的矢量化图的完整分布。 Map -Diffusion不是从学习的查询中预测单个确定性输出，而是在BEV潜在网格上进行了随机初始化的查询，以生成多个合理的MAP样本。这允许汇总样品提高预测准确性，并得出与场景歧义直接相关的不确定性估计。 Nuscenes数据集的广泛实验表明，MapDiffusion在在线地图构造中实现了最先进的性能，在单样本性能中超过了5％的基线。我们进一步表明，汇总多个样本始终提高ROC曲线的性能，从而验证了分布建模的好处。此外，我们的不确定性估计值在封闭区域中明显更高，从而增强了它们在用模棱两可的传感器输入识别区域方面的价值。通过对完整的地图分布进行建模，MapDiffusion可以增强在线矢量化高清图构建的鲁棒性和可靠性，从而为复杂环境中的自动驾驶汽车提供不确定性的决策。</li>
</ul>

<h3>Title: Retrieve-Augmented Generation for Speeding up Diffusion Policy without Additional Training</h3>
<ul>
<li><strong>Authors: </strong>Sodtavilan Odonchimed, Tatsuya Matsushima, Simon Holk, Yusuke Iwasawa, Yutaka Matsuo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21452">https://arxiv.org/abs/2507.21452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21452">https://arxiv.org/pdf/2507.21452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21452]] Retrieve-Augmented Generation for Speeding up Diffusion Policy without Additional Training(https://arxiv.org/abs/2507.21452)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion Policies (DPs) have attracted attention for their ability to achieve significant accuracy improvements in various imitation learning tasks. However, DPs depend on Diffusion Models, which require multiple noise removal steps to generate a single action, resulting in long generation times. To solve this problem, knowledge distillation-based methods such as Consistency Policy (CP) have been proposed. However, these methods require a significant amount of training time, especially for difficult tasks. In this study, we propose RAGDP (Retrieve-Augmented Generation for Diffusion Policies) as a novel framework that eliminates the need for additional training using a knowledge base to expedite the inference of pre-trained DPs. In concrete, RAGDP encodes observation-action pairs through the DP encoder to construct a vector database of expert demonstrations. During inference, the current observation is embedded, and the most similar expert action is extracted. This extracted action is combined with an intermediate noise removal step to reduce the number of steps required compared to the original diffusion step. We show that by using RAGDP with the base model and existing acceleration methods, we improve the accuracy and speed trade-off with no additional training. Even when accelerating the models 20 times, RAGDP maintains an advantage in accuracy, with a 7% increase over distillation models such as CP.</li>
<li><strong>摘要：</strong>扩散政策（DPS）引起了人们的关注，因为它们能够在各种模仿学习任务中取得显着准确的改进。但是，DPS取决于扩散模型，该模型需要多个降噪步骤以生成单个动作，从而导致长生成时间。为了解决这个问题，已经提出了基于知识蒸馏的方法，例如一致性政策（CP）。但是，这些方法需要大量的培训时间，尤其是对于艰巨的任务。在这项研究中，我们提出了RAGDP（用于扩散策略的检索增强的生成）作为一种新型框架，它消除了使用知识库加快预先训练的DPS推断的额外训练的需求。在混凝土中，RAGDP通过DP编码器编码观察行动对，以构建专家演示的矢量数据库。在推断过程中，当前的观察被嵌入，并提取了最相似的专家行动。将这种提取的动作与中间噪声删除步骤相结合，以减少与原始扩散步骤相比所需的步骤数。我们表明，通过将RAGDP与基本模型和现有加速方法一起使用，我们可以提高准确性和速度权衡，而无需额外的培训。即使加速模型20次，RAGDP仍保持准确性的优势，比蒸馏模型（例如CP）增加了7％。</li>
</ul>

<h3>Title: Chain-of-Cooking:Cooking Process Visualization via Bidirectional Chain-of-Thought Guidance</h3>
<ul>
<li><strong>Authors: </strong>Mengling Xu, Ming Tao, Bing-Kun Bao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21529">https://arxiv.org/abs/2507.21529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21529">https://arxiv.org/pdf/2507.21529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21529]] Chain-of-Cooking:Cooking Process Visualization via Bidirectional Chain-of-Thought Guidance(https://arxiv.org/abs/2507.21529)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Cooking process visualization is a promising task in the intersection of image generation and food analysis, which aims to generate an image for each cooking step of a recipe. However, most existing works focus on generating images of finished foods based on the given recipes, and face two challenges to visualize the cooking process. First, the appearance of ingredients changes variously across cooking steps, it is difficult to generate the correct appearances of foods that match the textual description, leading to semantic inconsistency. Second, the current step might depend on the operations of previous step, it is crucial to maintain the contextual coherence of images in sequential order. In this work, we present a cooking process visualization model, called Chain-of-Cooking. Specifically, to generate correct appearances of ingredients, we present a Dynamic Patch Selection Module to retrieve previously generated image patches as references, which are most related to current textual contents. Furthermore, to enhance the coherence and keep the rational order of generated images, we propose a Semantic Evolution Module and a Bidirectional Chain-of-Thought (CoT) Guidance. To better utilize the semantics of previous texts, the Semantic Evolution Module establishes the semantical association between latent prompts and current cooking step, and merges it with the latent features. Then the CoT Guidance updates the merged features to guide the current cooking step remain coherent with the previous step. Moreover, we construct a dataset named CookViz, consisting of intermediate image-text pairs for the cooking process. Quantitative and qualitative experiments show that our method outperforms existing methods in generating coherent and semantic consistent cooking process.</li>
<li><strong>摘要：</strong>烹饪过程可视化是图像产生和食物分析的交集中的一项有希望的任务，该任务旨在为食谱的每个烹饪步骤生成图像。但是，大多数现有作品都专注于根据给定食谱生成成品图像，并面临两个可视化烹饪过程的挑战。首先，成分的外观在烹饪步骤中各种变化，很难产生与文本描述相匹配的食物的正确外观，从而导致语义不一致。其次，当前的步骤可能取决于上一步的操作，至关重要的是按顺序保持图像的上下文连贯性。在这项工作中，我们提出了一个烹饪过程可视化模型，称为烹饪链。具体来说，为了生成正确的成分外观，我们提出了一个动态补丁选择模块，以检索先前生成的图像补丁作为引用，这与当前文本内容最相关。此外，为了提高连贯性并保持生成的图像的合理顺序，我们提出了一个语义演化模块和双向链链（COT）指导。为了更好地利用先前文本的语义，语义演化模块建立了潜在提示和当前烹饪步骤之间的语义关联，并将其与潜在特征合并。然后，COT指南更新了合并的功能，以指导当前的烹饪步骤与上一步保持一致。此外，我们构建了一个名为cookviz的数据集，该数据集由用于烹饪过程的中间图像文本对组成。定量和定性实验表明，我们的方法在产生连贯和语义一致的烹饪过程方面优于现有方法。</li>
</ul>

<h3>Title: Locally Controlled Face Aging with Latent Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Lais Isabelle Alves dos Santos, Julien Despois, Thibaut Chauffier, Sileye O. Ba, Giovanni Palma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21600">https://arxiv.org/abs/2507.21600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21600">https://arxiv.org/pdf/2507.21600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21600]] Locally Controlled Face Aging with Latent Diffusion Models(https://arxiv.org/abs/2507.21600)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present a novel approach to face aging that addresses the limitations of current methods which treat aging as a global, homogeneous process. Existing techniques using GANs and diffusion models often condition generation on a reference image and target age, neglecting that facial regions age heterogeneously due to both intrinsic chronological factors and extrinsic elements like sun exposure. Our method leverages latent diffusion models to selectively age specific facial regions using local aging signs. This approach provides significantly finer-grained control over the generation process, enabling more realistic and personalized aging. We employ a latent diffusion refiner to seamlessly blend these locally aged regions, ensuring a globally consistent and natural-looking synthesis. Experimental results demonstrate that our method effectively achieves three key criteria for successful face aging: robust identity preservation, high-fidelity and realistic imagery, and a natural, controllable aging progression.</li>
<li><strong>摘要：</strong>我们提出了一种新型的面对衰老方法，该方法解决了当前方法的局限性，这些方法将衰老视为全球，均匀的过程。现有的技术使用gan和扩散模型通常会在参考图像和目标年龄上生成，因此由于内在的年代学因素和外在元素（如太阳暴露）而忽略了面部区域的年龄异质。我们的方法利用潜在扩散模型使用局部衰老标志有选择地年龄的面部区域。这种方法为生成过程提供了明显的细粒度控制，从而实现了更现实和个性化的衰老。我们采用潜在扩散炼油厂来无缝融合这些本地老化的区域，以确保全球一致且自然的合成。实验结果表明，我们的方法有效地达到了成功面部衰老的三个关键标准：稳健的身份保存，高保真和现实的图像以及自然，可控制的衰老进展。</li>
</ul>

<h3>Title: Semantic Segmentation of iPS Cells: Case Study on Model Complexity in Biomedical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Maoquan Zhang, Bisser Raytchev, Xiujuan Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21608">https://arxiv.org/abs/2507.21608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21608">https://arxiv.org/pdf/2507.21608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21608]] Semantic Segmentation of iPS Cells: Case Study on Model Complexity in Biomedical Imaging(https://arxiv.org/abs/2507.21608)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Medical image segmentation requires not only accuracy but also robustness under challenging imaging conditions. In this study, we show that a carefully configured DeepLabv3 model can achieve high performance in segmenting induced pluripotent stem (iPS) cell colonies, and, under our experimental conditions, outperforms large-scale foundation models such as SAM2 and its medical variant MedSAM2 without structural modifications. These results suggest that, for specialized tasks characterized by subtle, low-contrast boundaries, increased model complexity does not necessarily translate to better performance. Our work revisits the assumption that ever-larger and more generalized architectures are always preferable, and provides evidence that appropriately adapted, simpler models may offer strong accuracy and practical reliability in domain-specific biomedical applications. We also offer an open-source implementation that includes strategies for small datasets and domain-specific encoding, with the aim of supporting further advances in semantic segmentation for regenerative medicine and related fields.</li>
<li><strong>摘要：</strong>医疗图像分割不仅需要精度，而且需要在具有挑战性的成像条件下的鲁棒性。在这项研究中，我们表明，经过精心构造的DEEPLABV3模型可以在分割诱导的多能茎（IPS）细胞菌落中实现高性能，并且在我们的实验条件下，优于SAM2及其Medical MedSAM2的大规模基础模型，而无需结构修饰。这些结果表明，对于以微妙的低对比度边界为特征的专门任务，增加的模型复杂性并不一定转化为更好的性能。我们的工作重新审视了这样的假设，即始终优选越来越多的架构，并提供了适当适应性，更简单模型的证据，可以在域特异性的生物医学应用中提供强大的准确性和实践可靠性。我们还提供了一个开源实施，其中包括针对小型数据集和特定领域的编码策略，目的是支持再生医学和相关领域的语义细分进一步进步。</li>
</ul>

<h3>Title: GuidPaint: Class-Guided Image Inpainting with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Qimin Wang, Xinda Liu, Guohua Geng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21627">https://arxiv.org/abs/2507.21627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21627">https://arxiv.org/pdf/2507.21627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21627]] GuidPaint: Class-Guided Image Inpainting with Diffusion Models(https://arxiv.org/abs/2507.21627)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In recent years, diffusion models have been widely adopted for image inpainting tasks due to their powerful generative capabilities, achieving impressive results. Existing multimodal inpainting methods based on diffusion models often require architectural modifications and retraining, resulting in high computational cost. In contrast, context-aware diffusion inpainting methods leverage the model's inherent priors to adjust intermediate denoising steps, enabling high-quality inpainting without additional training and significantly reducing computation. However, these methods lack fine-grained control over the masked regions, often leading to semantically inconsistent or visually implausible content. To address this issue, we propose GuidPaint, a training-free, class-guided image inpainting framework. By incorporating classifier guidance into the denoising process, GuidPaint enables precise control over intermediate generations within the masked areas, ensuring both semantic consistency and visual realism. Furthermore, it integrates stochastic and deterministic sampling, allowing users to select preferred intermediate results and deterministically refine them. Experimental results demonstrate that GuidPaint achieves clear improvements over existing context-aware inpainting methods in both qualitative and quantitative evaluations.</li>
<li><strong>摘要：</strong>近年来，由于其强大的生成能力，扩散模型已被广泛用于图像覆盖任务，从而取得了令人印象深刻的结果。现有的基于扩散模型的多模式浇头方法通常需要进行体系结构修改和重新培训，从而导致高计算成本。相比之下，上下文感知的扩散介绍方法利用该模型的固有先验来调整中间的剥离步骤，从而无需进行其他培训并显着降低计算，从而实现了高质量的介绍。但是，这些方法缺乏对蒙面区域的细粒度控制，通常会导致语义上不一致或视觉上令人难以置信的内容。为了解决这个问题，我们提出了GuidPaint，这是一个无培训的，班级指导的图像介绍框架。通过将分类器的指导纳入剥离过程中，GUIDPAIRT可以精确控制蒙面区域内中间世代，从而确保语义一致性和视觉现实主义。此外，它集成了随机和确定性的采样，使用户可以选择首选的中间结果并确定性地完善它们。实验结果表明，GUIDPAINT对定性和定量评估中现有的上下文感知涂料方法取得了明显的改进。</li>
</ul>

<h3>Title: APT: Improving Diffusion Models for High Resolution Image Generation with Adaptive Path Tracing</h3>
<ul>
<li><strong>Authors: </strong>Sangmin Han, Jinho Jeong, Jinwoo Kim, Seon Joo Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21690">https://arxiv.org/abs/2507.21690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21690">https://arxiv.org/pdf/2507.21690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21690]] APT: Improving Diffusion Models for High Resolution Image Generation with Adaptive Path Tracing(https://arxiv.org/abs/2507.21690)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Latent Diffusion Models (LDMs) are generally trained at fixed resolutions, limiting their capability when scaling up to high-resolution images. While training-based approaches address this limitation by training on high-resolution datasets, they require large amounts of data and considerable computational resources, making them less practical. Consequently, training-free methods, particularly patch-based approaches, have become a popular alternative. These methods divide an image into patches and fuse the denoising paths of each patch, showing strong performance on high-resolution generation. However, we observe two critical issues for patch-based approaches, which we call ``patch-level distribution shift" and ``increased patch monotonicity." To address these issues, we propose Adaptive Path Tracing (APT), a framework that combines Statistical Matching to ensure patch distributions remain consistent in upsampled latents and Scale-aware Scheduling to deal with the patch monotonicity. As a result, APT produces clearer and more refined details in high-resolution images. In addition, APT enables a shortcut denoising process, resulting in faster sampling with minimal quality degradation. Our experimental results confirm that APT produces more detailed outputs with improved inference speed, providing a practical approach to high-resolution image generation.</li>
<li><strong>摘要：</strong>潜在扩散模型（LDMS）通常以固定的分辨率进行训练，从而限制其在高分辨率图像时的能力。尽管基于培训的方法通过在高分辨率数据集中培训来解决此限制，但它们需要大量数据和大量的计算资源，从而使它们变得不太实际。因此，无培训方法，尤其是基于补丁的方法，已成为一种流行的选择。这些方法将图像划分为斑块并融合每个贴片的去核路径，显示出高分辨率生成的强烈性能。但是，我们观察到基于补丁的方法的两个关键问题，我们称``贴片级分配变化''和``增加的补丁单调性''。为了解决这些问题，我们提出了自适应路径跟踪（APT），该框架结合了统计匹配，以确保补丁分布在UPSPEMPLAPTER LESTENTS和SCALE ANAVAN APAIN ANCAIN ACTINED中保持一致，以处理补丁单调性。结果，APT在高分辨率图像中产生更清晰，更精致的细节。此外，APT可以实现快捷方式去索尼过程，从而更快地采样质量降解。我们的实验结果证实，APT可以通过提高推理速度产生更详细的输出，从而为高分辨率图像生成提供了实用的方法。</li>
</ul>

<h3>Title: Data-Driven Extended Corresponding State Approach for Residual Property Prediction of Hydrofluoroolefins</h3>
<ul>
<li><strong>Authors: </strong>Gang Wang, Peng Hu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21720">https://arxiv.org/abs/2507.21720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21720">https://arxiv.org/pdf/2507.21720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21720]] Data-Driven Extended Corresponding State Approach for Residual Property Prediction of Hydrofluoroolefins(https://arxiv.org/abs/2507.21720)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Hydrofluoroolefins are considered the most promising next-generation refrigerants due to their extremely low global warming potential values, which can effectively mitigate the global warming effect. However, the lack of reliable thermodynamic data hinders the discovery and application of newer and superior hydrofluoroolefin refrigerants. In this work, integrating the strengths of theoretical method and data-driven method, we proposed a neural network extended corresponding state model to predict the residual thermodynamic properties of hydrofluoroolefin refrigerants. The innovation is that the fluids are characterized through their microscopic molecular structures by the inclusion of graph neural network module and the specialized design of model architecture to enhance its generalization ability. The proposed model is trained using the highly accurate data of available known fluids, and evaluated via the leave-one-out cross-validation method. Compared to conventional extended corresponding state models or cubic equation of state, the proposed model shows significantly improved accuracy for density and energy properties in liquid and supercritical regions, with average absolute deviation of 1.49% (liquid) and 2.42% (supercritical) for density, 3.37% and 2.50% for residual entropy, 1.85% and 1.34% for residual enthalpy. These results demonstrate the effectiveness of embedding physics knowledge into the machine learning model. The proposed neural network extended corresponding state model is expected to significantly accelerate the discovery of novel hydrofluoroolefin refrigerants.</li>
<li><strong>摘要：</strong>由于全球变暖的潜在值极低，水氟芬素被认为是最有希望的下一代制冷剂，这可以有效地减轻全球变暖效果。然而，缺乏可靠的热力学数据阻碍了新的和上级的氢化素制冷剂的发现和应用。在这项工作中，整合了理论方法的优势和数据驱动方法，我们提出了一个神经网络扩展的相应状态模型，以预测Hydrofluoroolefin制冷剂的残留热力学特性。创新是通过包含图神经网络模块和模型结构的专业设计来通过其微观分子结构来表征流体，从而增强其通用能力。使用可用已知流体的高度准确数据对所提出的模型进行了训练，并通过剩余的交叉验证方法进行评估。与传统的扩展相应状态模型或状态的立方方程相比，拟议的模型显示出液体和超临界区域密度和能量性能的准确性显着提高，平均绝对偏差为1.49％（液体）和2.42％（超临界）密度，密度为3.37％，残留零件的3.37％和2.50％的残留物为1.85％和1.34％和1.34％。这些结果证明了将物理知识嵌入到机器学习模型中的有效性。提出的神经网络扩展相应的状态模型有望显着加速新型Hydrofluoroolefin制冷剂的发现。</li>
</ul>

<h3>Title: Zero-Shot Machine Unlearning with Proxy Adversarial Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Huiqiang Chen, Tianqing Zhu, Xin Yu, Wanlei Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21738">https://arxiv.org/abs/2507.21738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21738">https://arxiv.org/pdf/2507.21738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21738]] Zero-Shot Machine Unlearning with Proxy Adversarial Data Generation(https://arxiv.org/abs/2507.21738)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Machine unlearning aims to remove the influence of specific samples from a trained model. A key challenge in this process is over-unlearning, where the model's performance on the remaining data significantly drops due to the change in the model's parameters. Existing unlearning algorithms depend on the remaining data to prevent this issue. As such, these methods are inapplicable in a more practical scenario, where only the unlearning samples are available (i.e., zero-shot unlearning). This paper presents a novel framework, ZS-PAG, to fill this gap. Our approach offers three key innovations: (1) we approximate the inaccessible remaining data by generating adversarial samples; (2) leveraging the generated samples, we pinpoint a specific subspace to perform the unlearning process, therefore preventing over-unlearning in the challenging zero-shot scenario; and (3) we consider the influence of the unlearning process on the remaining samples and design an influence-based pseudo-labeling strategy. As a result, our method further improves the model's performance after unlearning. The proposed method holds a theoretical guarantee, and experiments on various benchmarks validate the effectiveness and superiority of our proposed method over several baselines.</li>
<li><strong>摘要：</strong>Machine Unerning旨在从训练有素的模型中删除特定样本的影响。在此过程中，一个关键的挑战是超学习，由于模型参数的变化，该模型在其余数据上的性能显着下降。现有的未学习算法取决于剩余的数据以防止此问题。因此，这些方法在更实用的情况下是不适用的，在更实用的情况下，只有未学习的样本可用（即，零射门未学习）。本文提出了一个新颖的框架ZS-PAG，以填补这一空白。我们的方法提供了三个关键的创新：（1）我们通过生成对抗样本来近似无法访问的剩余数据； （2）利用生成的样品，我们指出了一个特定的子空间来执行未来的过程，因此在挑战性的零照片方案中阻止了过度实现； （3）我们考虑学习过程对其余样本的影响，并设计基于影响力的伪标记策略。结果，我们的方法进一步改善了模型的表现。所提出的方法具有理论保证，并且对各种基准测试的实验验证了我们提出的方法比几个基线的有效性和优越性。</li>
</ul>

<h3>Title: MAGE: Multimodal Alignment and Generation Enhancement via Bridging Visual and Semantic Spaces</h3>
<ul>
<li><strong>Authors: </strong>Shaojun E, Yuchen Yang, Jiaheng Wu, Yan Zhang, Tiejun Zhao, Ziyan Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21741">https://arxiv.org/abs/2507.21741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21741">https://arxiv.org/pdf/2507.21741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21741]] MAGE: Multimodal Alignment and Generation Enhancement via Bridging Visual and Semantic Spaces(https://arxiv.org/abs/2507.21741)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In the latest advancements in multimodal learning, effectively addressing the spatial and semantic losses of visual data after encoding remains a critical challenge. This is because the performance of large multimodal models is positively correlated with the coupling between visual encoders and large language models. Existing approaches often face issues such as vector gaps or semantic disparities, resulting in information loss during the propagation process. To address these issues, we propose MAGE (Multimodal Alignment and Generation Enhancement), a novel framework that bridges the semantic spaces of vision and text through an innovative alignment mechanism. By introducing the Intelligent Alignment Network (IAN), MAGE achieves dimensional and semantic alignment. To reduce the gap between synonymous heterogeneous data, we employ a training strategy that combines cross-entropy and mean squared error, significantly enhancing the alignment effect. Moreover, to enhance MAGE's "Any-to-Any" capability, we developed a fine-tuning dataset for multimodal tool-calling instructions to expand the model's output capability boundaries. Finally, our proposed multimodal large model architecture, MAGE, achieved significantly better performance compared to similar works across various evaluation benchmarks, including MME, MMBench, and SEED. Complete code and appendix are available at: this https URL.</li>
<li><strong>摘要：</strong>在多模式学习的最新进展中，编码后有效解决视觉数据的空间和语义损失仍然是一个关键的挑战。这是因为大型多模型的性能与视觉编码器和大型语言模型之间的耦合呈正相关。现有方法通常面临诸如向量差距或语义差异之类的问题，从而导致传播过程中的信息丢失。为了解决这些问题，我们提出了法师（多模式对齐和生成增强），这是一个新颖的框架，通过创新的对准机制弥合了视觉和文本的语义空间。通过引入智能一致性网络（IAN），法师实现了维度和语义对齐。为了减少同义异构数据之间的差距，我们采用了训练策略，该培训策略结合了跨凝结和平方误差，从而显着增强了对齐效果。此外，为了增强Mage的“任何一对一”功能，我们为多模式工具称呼指令开发了一个微调数据集，以扩展模型的输出能力边界。最后，与在包括MME，MMBENCH和SEED在内的各种评估基准中的类似作品相比，我们提出的多模式大型架构Mage的性能明显更好。完整的代码和附录可在以下网址提供：此HTTPS URL。</li>
</ul>

<h3>Title: TempRe: Template generation for single and direct multi-step retrosynthesis</h3>
<ul>
<li><strong>Authors: </strong>Nguyen Xuan-Vu, Daniel Armstrong, Zlatko Joncev, Philippe Schwaller</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21762">https://arxiv.org/abs/2507.21762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21762">https://arxiv.org/pdf/2507.21762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21762]] TempRe: Template generation for single and direct multi-step retrosynthesis(https://arxiv.org/abs/2507.21762)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Retrosynthesis planning remains a central challenge in molecular discovery due to the vast and complex chemical reaction space. While traditional template-based methods offer tractability, they suffer from poor scalability and limited generalization, and template-free generative approaches risk generating invalid reactions. In this work, we propose TempRe, a generative framework that reformulates template-based approaches as sequence generation, enabling scalable, flexible, and chemically plausible retrosynthesis. We evaluated TempRe across single-step and multi-step retrosynthesis tasks, demonstrating its superiority over both template classification and SMILES-based generation methods. On the PaRoutes multi-step benchmark, TempRe achieves strong top-k route accuracy. Furthermore, we extend TempRe to direct multi-step synthesis route generation, providing a lightweight and efficient alternative to conventional single-step and search-based approaches. These results highlight the potential of template generative modeling as a powerful paradigm in computer-aided synthesis planning.</li>
<li><strong>摘要：</strong>由于庞大而复杂的化学反应空间，逆合合成计划仍然是分子发现中的核心挑战。虽然传统的基于模板的方法提供了障碍性，但它们的可扩展性和有限的概括性却很差，并且无模板生成方法可能会产生无效的反应。在这项工作中，我们提出了Tempre，这是一种生成框架，将基于模板的方法重新定义为序列产生，从而启用可扩展，灵活和化学上合理的逆合合成。我们评估了跨单步和多步返回合成任务的速度，证明了它在模板分类和基于微笑的生成方法上的优越性。在模拟的多步基准测试中，Tempre实现了强大的TOP-K路线精度。此外，我们将TEMPRE扩展到指导多步综合路线的生成，提供了一种轻巧有效的替代方案，可用于常规的单步和基于搜索的方法。这些结果突出了模板生成建模作为计算机辅助合成计划中强大的范式的潜力。</li>
</ul>

<h3>Title: HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D Worlds from Words or Pixels</h3>
<ul>
<li><strong>Authors: </strong>HunyuanWorld Team, Zhenwei Wang, Yuhao Liu, Junta Wu, Zixiao Gu, Haoyuan Wang, Xuhui Zuo, Tianyu Huang, Wenhuan Li, Sheng Zhang, Yihang Lian, Yulin Tsai, Lifu Wang, Sicong Liu, Puhua Jiang, Xianghui Yang, Dongyuan Guo, Yixuan Tang, Xinyue Mao, Jiaao Yu, Junlin Yu, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Chao Zhang, Yonghao Tan, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Minghui Chen, Zhan Li, Wangchen Qin, Lei Wang, Yifu Sun, Lin Niu, Xiang Yuan, Xiaofeng Yang, Yingping He, Jie Xiao, Yangyu Tao, Jianchen Zhu, Jinbao Xue, Kai Liu, Chongqing Zhao, Xinming Wu, Tian Liu, Peng Chen, Di Wang, Yuhong Liu, Linus, Jie Jiang, Tengfei Wang, Chunchao Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21809">https://arxiv.org/abs/2507.21809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21809">https://arxiv.org/pdf/2507.21809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21809]] HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D Worlds from Words or Pixels(https://arxiv.org/abs/2507.21809)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Creating immersive and playable 3D worlds from texts or images remains a fundamental challenge in computer vision and graphics. Existing world generation approaches typically fall into two categories: video-based methods that offer rich diversity but lack 3D consistency and rendering efficiency, and 3D-based methods that provide geometric consistency but struggle with limited training data and memory-inefficient representations. To address these limitations, we present HunyuanWorld 1.0, a novel framework that combines the best of both worlds for generating immersive, explorable, and interactive 3D scenes from text and image conditions. Our approach features three key advantages: 1) 360° immersive experiences via panoramic world proxies; 2) mesh export capabilities for seamless compatibility with existing computer graphics pipelines; 3) disentangled object representations for augmented interactivity. The core of our framework is a semantically layered 3D mesh representation that leverages panoramic images as 360° world proxies for semantic-aware world decomposition and reconstruction, enabling the generation of diverse 3D worlds. Extensive experiments demonstrate that our method achieves state-of-the-art performance in generating coherent, explorable, and interactive 3D worlds while enabling versatile applications in virtual reality, physical simulation, game development, and interactive content creation.</li>
<li><strong>摘要：</strong>从文本或图像中创建沉浸式和可玩的3D世界仍然是计算机视觉和图形中的一个基本挑战。现有的世界一代方法通常分为两类：基于视频的方法，可提供丰富的多样性，但缺乏3D一致性和渲染效率，以及基于3D的方法，这些方法提供了几何学一致性，但在有限的培训数据和内存信息方面挣扎。为了解决这些局限性，我们提出了Hunyuanworld 1.0，这是一个新颖的框架，结合了两全其美的世界，以从文本和图像条件中产生沉浸式，可探索和交互式3D场景。我们的方法具有三个关键优势：1）通过全景世界代理人360°沉浸式体验； 2）与现有计算机图形管道无缝兼容的网格导出功能； 3）删除的对象表示，以增强交互性。我们框架的核心是一种语义分层的3D网格表示形式，它将全景图像作为360°世界代理进行语义感知世界的世界分解和重建，从而实现了不同的3D世界。广泛的实验表明，我们的方法在生成连贯，可探索和交互式3D世界方面取得了最新的性能，同时在虚拟现实，物理模拟，游戏开发和交互式内容创建中启用多功能应用程序。</li>
</ul>

<h3>Title: Aether Weaver: Multimodal Affective Narrative Co-Generation with Dynamic Scene Graphs</h3>
<ul>
<li><strong>Authors: </strong>Saeed Ghorbani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21893">https://arxiv.org/abs/2507.21893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21893">https://arxiv.org/pdf/2507.21893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21893]] Aether Weaver: Multimodal Affective Narrative Co-Generation with Dynamic Scene Graphs(https://arxiv.org/abs/2507.21893)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce Aether Weaver, a novel, integrated framework for multimodal narrative co-generation that overcomes limitations of sequential text-to-visual pipelines. Our system concurrently synthesizes textual narratives, dynamic scene graph representations, visual scenes, and affective soundscapes, driven by a tightly integrated, co-generation mechanism. At its core, the Narrator, a large language model, generates narrative text and multimodal prompts, while the Director acts as a dynamic scene graph manager, and analyzes the text to build and maintain a structured representation of the story's world, ensuring spatio-temporal and relational consistency for visual rendering and subsequent narrative generation. Additionally, a Narrative Arc Controller guides the high-level story structure, influencing multimodal affective consistency, further complemented by an Affective Tone Mapper that ensures congruent emotional expression across all modalities. Through qualitative evaluations on a diverse set of narrative prompts encompassing various genres, we demonstrate that Aether Weaver significantly enhances narrative depth, visual fidelity, and emotional resonance compared to cascaded baseline approaches. This integrated framework provides a robust platform for rapid creative prototyping and immersive storytelling experiences.</li>
<li><strong>摘要：</strong>我们介绍了Aether Weaver，这是一个新颖的集成框架，用于多模式叙事共同生成，它克服了顺序的文本到视觉管道的局限性。我们的系统同时综合了文本叙述，动态场景图表，视觉场景和情感音景，由紧密整合的，共同的共同生成机制驱动。叙述者的核心是一种大型语言模型，生成叙事文本和多模式提示，而导演则充当动态场景图管理器，并分析文本以构建和维护故事世界的结构化表示，确保时空和关系的一致性，以实现视觉渲染和后续叙述性。此外，叙事弧控制器指导高级故事结构，影响了多模式的情感一致性，进一步得到了一个情感映射器的补充，该映射器可确保在所有方式上保持一致的情感表达。通过对包括各种流派的各种叙事提示的定性评估，我们证明，与级联的基线方法相比，以太织布工可以显着增强叙事深度，视觉效果和情感共鸣。这个集成的框架为快速创造性的原型制作和沉浸式讲故事的体验提供了一个强大的平台。</li>
</ul>

<h3>Title: Evaluating Deepfake Detectors in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Viacheslav Pirogov, Maksim Artemev</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21905">https://arxiv.org/abs/2507.21905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21905">https://arxiv.org/pdf/2507.21905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21905]] Evaluating Deepfake Detectors in the Wild(https://arxiv.org/abs/2507.21905)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deepfakes powered by advanced machine learning models present a significant and evolving threat to identity verification and the authenticity of digital media. Although numerous detectors have been developed to address this problem, their effectiveness has yet to be tested when applied to real-world data. In this work we evaluate modern deepfake detectors, introducing a novel testing procedure designed to mimic real-world scenarios for deepfake detection. Using state-of-the-art deepfake generation methods, we create a comprehensive dataset containing more than 500,000 high-quality deepfake images. Our analysis shows that detecting deepfakes still remains a challenging task. The evaluation shows that in fewer than half of the deepfake detectors tested achieved an AUC score greater than 60%, with the lowest being 50%. We demonstrate that basic image manipulations, such as JPEG compression or image enhancement, can significantly reduce model performance. All code and data are publicly available at this https URL.</li>
<li><strong>摘要：</strong>由先进的机器学习模型提供支持的深击对身份验证和数字媒体的真实性构成了重大而不断发展的威胁。尽管已经开发了许多探测器来解决这个问题，但是当应用于现实世界数据时，其有效性尚未得到测试。在这项工作中，我们评估了现代的深层检测器，引入了一种新型的测试程序，旨在模仿现实世界中的深层检测。使用最先进的深层生成方法，我们创建了一个综合数据集，其中包含超过500,000个高质量的深层图像。我们的分析表明，检测深击仍然是一项具有挑战性的任务。评估表明，在不到一半的深泡探测器中，测试的AUC得分大于60％，最低为50％。我们证明，基本图像操作（例如JPEG压缩或图像增强）可以显着降低模型性能。所有代码和数据均在此HTTPS URL上公开可用。</li>
</ul>

<h3>Title: ArtSeek: Deep artwork understanding via multimodal in-context reasoning and late interaction retrieval</h3>
<ul>
<li><strong>Authors: </strong>Nicola Fanelli, Gennaro Vessio, Giovanna Castellano</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21917">https://arxiv.org/abs/2507.21917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21917">https://arxiv.org/pdf/2507.21917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21917]] ArtSeek: Deep artwork understanding via multimodal in-context reasoning and late interaction retrieval(https://arxiv.org/abs/2507.21917)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Analyzing digitized artworks presents unique challenges, requiring not only visual interpretation but also a deep understanding of rich artistic, contextual, and historical knowledge. We introduce ArtSeek, a multimodal framework for art analysis that combines multimodal large language models with retrieval-augmented generation. Unlike prior work, our pipeline relies only on image input, enabling applicability to artworks without links to Wikidata or Wikipedia-common in most digitized collections. ArtSeek integrates three key components: an intelligent multimodal retrieval module based on late interaction retrieval, a contrastive multitask classification network for predicting artist, genre, style, media, and tags, and an agentic reasoning strategy enabled through in-context examples for complex visual question answering and artwork explanation via Qwen2.5-VL. Central to this approach is WikiFragments, a Wikipedia-scale dataset of image-text fragments curated to support knowledge-grounded multimodal reasoning. Our framework achieves state-of-the-art results on multiple benchmarks, including a +8.4% F1 improvement in style classification over GraphCLIP and a +7.1 BLEU@1 gain in captioning on ArtPedia. Qualitative analyses show that ArtSeek can interpret visual motifs, infer historical context, and retrieve relevant knowledge, even for obscure works. Though focused on visual arts, our approach generalizes to other domains requiring external knowledge, supporting scalable multimodal AI research. Both the dataset and the source code will be made publicly available at this https URL.</li>
<li><strong>摘要：</strong>分析数字化的艺术品提出了独特的挑战，不仅需要视觉解释，而且需要对丰富的艺术，上下文和历史知识的深刻理解。我们介绍了Artseek，这是一个多式联运的艺术分析框架，该框架将多模式模型与检索成绩结合在一起。与先前的工作不同，我们的管道仅依赖于图像输入，在大多数数字化集合中无需链接到Wikidata或Wikipedia-Common的艺术品。 Artseek集成了三个关键组成部分：基于晚期互动检索的智能多模式检索模块，一种用于预测艺术家，类型，样式，样式，媒体和标签的对比性多任务分类网络，以及通过context示例启用的代理推理策略，用于复杂的视觉检测和艺术品答案和艺术作品解释，并通过qwen2.5-vl。这种方法的核心是WikifRagments，这是一个策划的图像文本片段的Wikipedia尺度数据集，该数据集策划了，以支持知识接地的多模式推理。我们的框架在多个基准测试中实现了最新的结果，包括 +8.4％的F1样式分类改善了GraphClip和A +7.1 BLEU@1在Artpedia字幕上获得1增益。定性分析表明，Artseek可以解释视觉图案，推断历史上下文并检索相关知识，即使对于晦涩的作品也是如此。尽管专注于视觉艺术，但我们的方法将概括为需要外部知识的其他领域，并支持可扩展的多模式AI研究。数据集和源代码都将在此HTTPS URL上公开可用。</li>
</ul>

<h3>Title: MMAT-1M: A Large Reasoning Dataset for Multimodal Agent Tuning</h3>
<ul>
<li><strong>Authors: </strong>Tianhong Gao, Yannian Fu, Weiqun Wu, Haixiao Yue, Shanshan Liu, Gang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21924">https://arxiv.org/abs/2507.21924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21924">https://arxiv.org/pdf/2507.21924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21924]] MMAT-1M: A Large Reasoning Dataset for Multimodal Agent Tuning(https://arxiv.org/abs/2507.21924)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), enhanced through agent tuning, have demonstrated remarkable capabilities in Chain-of-Thought (CoT) and tool utilization, significantly surpassing the performance of standalone models. However, the multimodal domain still lacks a large-scale, high-quality agent tuning dataset to unlock the full potential of multimodal large language models. To bridge this gap, we introduce MMAT-1M, the first million-scale multimodal agent tuning dataset designed to support CoT, reflection, and dynamic tool usage. Our dataset is constructed through a novel four-stage data engine: 1) We first curate publicly available multimodal datasets containing question-answer pairs; 2) Then, leveraging GPT-4o, we generate rationales for the original question-answer pairs and dynamically integrate API calls and Retrieval Augmented Generation (RAG) information through a multi-turn paradigm; 3) Furthermore, we refine the rationales through reflection to ensure logical consistency and accuracy, creating a multi-turn dialogue dataset with both Rationale and Reflection (RR); 4) Finally, to enhance efficiency, we optionally compress multi-turn dialogues into a One-turn Rationale and Reflection (ORR) format. By fine-tuning open-source multimodal models on the MMAT-1M, we observe significant performance gains. For instance, the InternVL2.5-8B-RR model achieves an average improvement of 2.7% across eight public benchmarks and 8.8% on the RAG benchmark Dyn-VQA, demonstrating the dataset's effectiveness in enhancing multimodal reasoning and tool-based capabilities. The dataset is publicly available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）通过代理调整增强，在思考链（COT）和工具利用率中表现出了显着的功能，可显着超过独立模型的性能。但是，多模式域仍然缺乏大规模的高质量代理调整数据集，无法解锁多模式大语言模型的全部潜力。为了弥合这一差距，我们介绍了MMAT-1M，这是第一个旨在支持COT，反射和动态工具使用的百万尺度多模式调整数据集。我们的数据集是通过新颖的四阶段数据引擎构建的：1）我们首先策划包含问答对的公开可用的多模式数据集； 2）然后，利用GPT-4O，我们为原始的问题解答对生成了理由，并通过多转移范式动态整合API调用和检索增强发电（RAG）信息； 3）此外，我们通过反思来完善理由，以确保逻辑一致性和准确性，从而创建一个具有理由和反射（RR）的多转向对话数据集； 4）最后，为了提高效率，我们可以选择地将多转化的对话压缩为一转的原理和反射（ORR）格式。通过微调MMAT-1M上的开源多模型，我们观察到了显着的性能增长。例如，Intervl2.5-8B-RR模型在八个公共基准中的平均提高2.7％，而在RAG基准DYN-VQA上，平均提高了8.8％，这证明了数据集在增强多模式推理和基于工具的功能方面的有效性。该数据集可在此HTTPS URL上公开可用。</li>
</ul>

<h3>Title: Attention-Driven Multimodal Alignment for Long-term Action Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Xin Wang, Peng-Jie Li, Yuan-Yuan Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21945">https://arxiv.org/abs/2507.21945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21945">https://arxiv.org/pdf/2507.21945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21945]] Attention-Driven Multimodal Alignment for Long-term Action Quality Assessment(https://arxiv.org/abs/2507.21945)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Long-term action quality assessment (AQA) focuses on evaluating the quality of human activities in videos lasting up to several minutes. This task plays an important role in the automated evaluation of artistic sports such as rhythmic gymnastics and figure skating, where both accurate motion execution and temporal synchronization with background music are essential for performance assessment. However, existing methods predominantly fall into two categories: unimodal approaches that rely solely on visual features, which are inadequate for modeling multimodal cues like music; and multimodal approaches that typically employ simple feature-level contrastive fusion, overlooking deep cross-modal collaboration and temporal dynamics. As a result, they struggle to capture complex interactions between modalities and fail to accurately track critical performance changes throughout extended sequences. To address these challenges, we propose the Long-term Multimodal Attention Consistency Network (LMAC-Net). LMAC-Net introduces a multimodal attention consistency mechanism to explicitly align multimodal features, enabling stable integration of visual and audio information and enhancing feature representations. Specifically, we introduce a multimodal local query encoder module to capture temporal semantics and cross-modal relations, and use a two-level score evaluation for interpretable results. In addition, attention-based and regression-based losses are applied to jointly optimize multimodal alignment and score fusion. Experiments conducted on the RG and Fis-V datasets demonstrate that LMAC-Net significantly outperforms existing methods, validating the effectiveness of our proposed approach.</li>
<li><strong>摘要：</strong>长期行动质量评估（AQA）的重点是评估持续数分钟的视频中人类活动质量。这项任务在自动评估艺术运动（例如节奏体操和花样滑冰）中起着重要作用，在这种运动中，精确的运动执行和与背景音乐的时间同步对于性能评估至关重要。但是，现有方法主要属于两类：仅依赖视觉特征的单峰方法，这些方法不足以建模音乐之类的多模式线索；以及通常采用简单特征级对比度融合的多模式方法，俯瞰着深层模式协作和时间动态。结果，他们难以捕获模式之间的复杂相互作用，并且无法准确跟踪整个扩展序列的关键性能变化。为了应对这些挑战，我们提出了长期的多模式关注一致性网络（LMAC-NET）。 LMAC-NET引入了多模式的注意一致性机制，以显式地对齐多模式特征，从而稳定地集成视觉和音频信息并增强特征表示。具体而言，我们引入了多模式的本地查询编码器模块，以捕获时间语义和跨模式关系，并使用两级得分评估来获得可解释的结果。此外，基于注意力和基于回归的损失将用于共同优化多模式比对和得分融合。在RG和FIS-V数据集上进行的实验表明，LMAC-NET显着胜过现有方法，从而验证了我们提出的方法的有效性。</li>
</ul>

<h3>Title: Enhancing Generalization in Data-free Quantization via Mixup-class Prompting</h3>
<ul>
<li><strong>Authors: </strong>Jiwoong Park, Chaeun Lee, Yongseok Choi, Sein Park, Deokki Hong, Jungwook Choi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21947">https://arxiv.org/abs/2507.21947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21947">https://arxiv.org/pdf/2507.21947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21947]] Enhancing Generalization in Data-free Quantization via Mixup-class Prompting(https://arxiv.org/abs/2507.21947)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Post-training quantization (PTQ) improves efficiency but struggles with limited calibration data, especially under privacy constraints. Data-free quantization (DFQ) mitigates this by generating synthetic images using generative models such as generative adversarial networks (GANs) and text-conditioned latent diffusion models (LDMs), while applying existing PTQ algorithms. However, the relationship between generated synthetic images and the generalizability of the quantized model during PTQ remains underexplored. Without investigating this relationship, synthetic images generated by previous prompt engineering methods based on single-class prompts suffer from issues such as polysemy, leading to performance degradation. We propose \textbf{mixup-class prompt}, a mixup-based text prompting strategy that fuses multiple class labels at the text prompt level to generate diverse, robust synthetic data. This approach enhances generalization, and improves optimization stability in PTQ. We provide quantitative insights through gradient norm and generalization error analysis. Experiments on convolutional neural networks (CNNs) and vision transformers (ViTs) show that our method consistently outperforms state-of-the-art DFQ methods like GenQ. Furthermore, it pushes the performance boundary in extremely low-bit scenarios, achieving new state-of-the-art accuracy in challenging 2-bit weight, 4-bit activation (W2A4) quantization.</li>
<li><strong>摘要：</strong>培训后量化（PTQ）提高了效率，但在校准数据有限的情况下，尤其是在隐私限制下进行斗争。无数据量化（DFQ）通过使用生成对抗性网络（GAN）和文本条件潜在扩散模型（LDMS）来生成合成图像，从而缓解这种情况，同时应用现有的PTQ算法。然而，生成的合成图像与PTQ期间量化模型的普遍性之间的关系仍然没有被忽略。在没有调查这种关系的情况下，基于单级提示，由以前的及时工程方法产生的合成图像遭受了诸如多义之类的问题，导致性能降解。我们提出了\ textbf {混合级提示}，这是一种基于混音的文本提示策略，该策略在文本提示级别融合了多个类标签，以生成多样化的，可靠的合成数据。这种方法增强了概括，并提高了PTQ中的优化稳定性。我们通过梯度规范和概括误差分析提供定量见解。关于卷积神经网络（CNN）和视觉变压器（VIT）的实验表明，我们的方法始终优于GENQ等最先进的DFQ方法。此外，它在极低的场景中推动了性能边界，在挑战2位重量，4位激活（W2A4）量化方面达到了新的最新精度。</li>
</ul>

<h3>Title: Contrast-Prior Enhanced Duality for Mask-Free Shadow Removal</h3>
<ul>
<li><strong>Authors: </strong>Jiyu Wu, Yifan Liu, Jiancheng Huang, Mingfu Yan, Shifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21949">https://arxiv.org/abs/2507.21949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21949">https://arxiv.org/pdf/2507.21949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21949]] Contrast-Prior Enhanced Duality for Mask-Free Shadow Removal(https://arxiv.org/abs/2507.21949)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Existing shadow removal methods often rely on shadow masks, which are challenging to acquire in real-world scenarios. Exploring intrinsic image cues, such as local contrast information, presents a potential alternative for guiding shadow removal in the absence of explicit masks. However, the cue's inherent ambiguity becomes a critical limitation in complex scenes, where it can fail to distinguish true shadows from low-reflectance objects and intricate background textures. To address this motivation, we propose the Adaptive Gated Dual-Branch Attention (AGBA) mechanism. AGBA dynamically filters and re-weighs the contrast prior to effectively disentangle shadow features from confounding visual elements. Furthermore, to tackle the persistent challenge of restoring soft shadow boundaries and fine-grained details, we introduce a diffusion-based Frequency-Contrast Fusion Network (FCFN) that leverages high-frequency and contrast cues to guide the generative process. Extensive experiments demonstrate that our method achieves state-of-the-art results among mask-free approaches while maintaining competitive performance relative to mask-based methods.</li>
<li><strong>摘要：</strong>现有的阴影去除方法通常依赖于阴影面具，在现实世界中，这些面罩具有挑战性。探索固有的图像提示，例如局部对比信息，为在没有明确掩模的情况下提供了指导阴影去除的潜在替代方法。但是，提示的固有歧义成为复杂场景中的关键限制，在复杂的场景中，它无法将真实阴影与低反射对象和复杂的背景纹理区分开。为了解决这一动机，我们提出了自适应的封闭式双分支注意力（AGBA）机制。 AGBA在有效地将阴影特征与混淆的视觉元素中有效地解开阴影特征之前，动态过滤并重新换对对比度。此外，为了应对恢复柔和阴影边界和细粒细节的持续挑战，我们引入了基于扩散的频率对比度融合网络（FCFN），该网络（FCFN）利用高频和对比度提示来指导生成过程。广泛的实验表明，我们的方法在无面膜方法之间实现了最新的结果，同时相对于基于面具的方法，保持竞争性能。</li>
</ul>

<h3>Title: PanoSplatt3R: Leveraging Perspective Pretraining for Generalized Unposed Wide-Baseline Panorama Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Ren, Mochu Xiang, Jiajun Zhu, Yuchao Dai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21960">https://arxiv.org/abs/2507.21960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21960">https://arxiv.org/pdf/2507.21960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21960]] PanoSplatt3R: Leveraging Perspective Pretraining for Generalized Unposed Wide-Baseline Panorama Reconstruction(https://arxiv.org/abs/2507.21960)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Wide-baseline panorama reconstruction has emerged as a highly effective and pivotal approach for not only achieving geometric reconstruction of the surrounding 3D environment, but also generating highly realistic and immersive novel views. Although existing methods have shown remarkable performance across various benchmarks, they are predominantly reliant on accurate pose information. In real-world scenarios, the acquisition of precise pose often requires additional computational resources and is highly susceptible to noise. These limitations hinder the broad applicability and practicality of such methods. In this paper, we present PanoSplatt3R, an unposed wide-baseline panorama reconstruction method. We extend and adapt the foundational reconstruction pretrainings from the perspective domain to the panoramic domain, thus enabling powerful generalization capabilities. To ensure a seamless and efficient domain-transfer process, we introduce RoPE rolling that spans rolled coordinates in rotary positional embeddings across different attention heads, maintaining a minimal modification to RoPE's mechanism, while modeling the horizontal periodicity of panorama images. Comprehensive experiments demonstrate that PanoSplatt3R, even in the absence of pose information, significantly outperforms current state-of-the-art methods. This superiority is evident in both the generation of high-quality novel views and the accuracy of depth estimation, thereby showcasing its great potential for practical applications. Project page: this https URL</li>
<li><strong>摘要：</strong>宽基线全景重建已成为一种高效且关键的方法，不仅可以实现周围3D环境的几何重建，而且还产生了高度逼真和沉浸式的新颖观点。尽管现有方法在各种基准中表现出了显着的性能，但它们主要依赖于准确的姿势信息。在实际情况下，精确姿势的获取通常需要其他计算资源，并且非常容易受到噪声的影响。这些限制阻碍了这种方法的广泛适用性和实用性。在本文中，我们提出了一种未亮相的宽基线全景重建方法Panosplatt3R。我们将基础重建预处理从透视域扩展和调整到全景域，从而实现强大的概括能力。为了确保无缝有效的域转移工艺，我们引入绳索滚动，使跨不同注意力头的旋转位置嵌入沿旋转位置的坐标，保持最小的修饰以使绳索的机制建模，同时对Panorama图像的水平周期性进行建模。全面的实验表明，即使没有姿势信息，Panosplatt3R也明显超过当前最新方法。这种优势在高质量的新颖观点和深度估计的准确性中都显而易见，从而展示了其实用应用的巨大潜力。项目页面：此HTTPS URL</li>
</ul>

<h3>Title: Improving Generative Ad Text on Facebook using Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Daniel R. Jiang, Alex Nikulkov, Yu-Chia Chen, Yang Bai, Zheqing Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21983">https://arxiv.org/abs/2507.21983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21983">https://arxiv.org/pdf/2507.21983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21983]] Improving Generative Ad Text on Facebook using Reinforcement Learning(https://arxiv.org/abs/2507.21983)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Generative artificial intelligence (AI), in particular large language models (LLMs), is poised to drive transformative economic change. LLMs are pre-trained on vast text data to learn general language patterns, but a subsequent post-training phase is critical to align them for specific real-world tasks. Reinforcement learning (RL) is the leading post-training technique, yet its economic impact remains largely underexplored and unquantified. We examine this question through the lens of the first deployment of an RL-trained LLM for generative advertising on Facebook. Integrated into Meta's Text Generation feature, our model, "AdLlama," powers an AI tool that helps advertisers create new variations of human-written ad text. To train this model, we introduce reinforcement learning with performance feedback (RLPF), a post-training method that uses historical ad performance data as a reward signal. In a large-scale 10-week A/B test on Facebook spanning nearly 35,000 advertisers and 640,000 ad variations, we find that AdLlama improves click-through rates by 6.7% (p=0.0296) compared to a supervised imitation model trained on curated ads. This represents a substantial improvement in advertiser return on investment on Facebook. We also find that advertisers who used AdLlama generated more ad variations, indicating higher satisfaction with the model's outputs. To our knowledge, this is the largest study to date on the use of generative AI in an ecologically valid setting, offering an important data point quantifying the tangible impact of RL post-training. Furthermore, the results show that RLPF is a promising and generalizable approach for metric-driven post-training that bridges the gap between highly capable language models and tangible outcomes.</li>
<li><strong>摘要：</strong>生成人工智能（AI），特别是大型语言模型（LLM），有望推动变革性的经济变化。 LLM在大量的文本数据上进行了预先培训，以学习通用语言模式，但是随后的训练后阶段对于将其与特定的现实世界任务保持一致至关重要。强化学习（RL）是领先的培训后技术，但其经济影响仍然很大程度上没有被倍增和未经量化。我们通过在Facebook上首次部署RL培训的LLM的镜头来研究这个问题。集成到Meta的文本生成功能，我们的模型“ Adllama”为AI工具提供动力，该工具可帮助广告商创建人写的广告文本的新变体。为了训练该模型，我们使用绩效反馈（RLPF）引入强化学习，这是一种使用历史广告绩效数据作为奖励信号的训练后方法。在跨越近35,000个广告客户和640,000个广告变体的Facebook上的10周大规模A/B测试中，与经过精心策划的广告相比，Adllama的点击率提高了6.7％（P = 0.0296）。这代表了广告商在Facebook上的投资回报率的重大改善。我们还发现，使用ADLLAMA的广告商产生了更多的广告变化，表明对模型的输出的满意度更高。据我们所知，这是迄今为止关于在生态有效的环境中使用生成AI的最大研究，它提供了一个重要的数据点，量化了RL后训练的切实影响。此外，结果表明，RLPF是公制驱动后训练的一种有前途且可普遍的方法，它弥合了高度强大的语言模型和有形结果之间的差距。</li>
</ul>

<h3>Title: ZIUM: Zero-Shot Intent-Aware Adversarial Attack on Unlearned Models</h3>
<ul>
<li><strong>Authors: </strong>Hyun Jun Yook, Ga San Jhun, Jae Hyun Cho, Min Jeon, Donghyun Kim, Tae Hyung Kim, Youn Kyu Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21985">https://arxiv.org/abs/2507.21985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21985">https://arxiv.org/pdf/2507.21985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21985]] ZIUM: Zero-Shot Intent-Aware Adversarial Attack on Unlearned Models(https://arxiv.org/abs/2507.21985)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Machine unlearning (MU) removes specific data points or concepts from deep learning models to enhance privacy and prevent sensitive content generation. Adversarial prompts can exploit unlearned models to generate content containing removed concepts, posing a significant security risk. However, existing adversarial attack methods still face challenges in generating content that aligns with an attacker's intent while incurring high computational costs to identify successful prompts. To address these challenges, we propose ZIUM, a Zero-shot Intent-aware adversarial attack on Unlearned Models, which enables the flexible customization of target attack images to reflect an attacker's intent. Additionally, ZIUM supports zero-shot adversarial attacks without requiring further optimization for previously attacked unlearned concepts. The evaluation across various MU scenarios demonstrated ZIUM's effectiveness in successfully customizing content based on user-intent prompts while achieving a superior attack success rate compared to existing methods. Moreover, its zero-shot adversarial attack significantly reduces the attack time for previously attacked unlearned concepts.</li>
<li><strong>摘要：</strong>Machine Unrearning（MU）从深度学习模型中删除了特定的数据点或概念，以增强隐私并防止敏感内容产生。对抗性提示可以利用未学习的模型生成包含删除概念的内容，从而带来重大的安全风险。但是，现有的对抗性攻击方法仍然面临挑战，即产生与攻击者意图保持一致的内容，同时产生高计算成本以识别成功的提示。为了应对这些挑战，我们提出了Zium，Zium是对未学习模型的零射击意见攻击的攻击，这使目标攻击图像可以灵活地自定义以反映攻击者的意图。此外，Zium支持零拍对对抗攻击，而无需对先前攻击的未经学习的概念进行进一步优化。在各种MU方案中的评估表明，与现有方法相比，Zium在基于用户大的提示中成功定制内容的有效性，同时实现了卓越的攻击成功率。此外，其零拍对对抗性攻击大大减少了先前攻击的未经学习概念的攻击时间。</li>
</ul>

<h3>Title: Teach Me to Trick: Exploring Adversarial Transferability via Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Siddhartha Pradhan, Shikshya Shiwakoti, Neha Bathuri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.21992">https://arxiv.org/abs/2507.21992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.21992">https://arxiv.org/pdf/2507.21992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.21992]] Teach Me to Trick: Exploring Adversarial Transferability via Knowledge Distillation(https://arxiv.org/abs/2507.21992)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We investigate whether knowledge distillation (KD) from multiple heterogeneous teacher models can enhance the generation of transferable adversarial examples. A lightweight student model is trained using two KD strategies: curriculum-based switching and joint optimization, with ResNet50 and DenseNet-161 as teachers. The trained student is then used to generate adversarial examples using FG, FGS, and PGD attacks, which are evaluated against a black-box target model (GoogLeNet). Our results show that student models distilled from multiple teachers achieve attack success rates comparable to ensemble-based baselines, while reducing adversarial example generation time by up to a factor of six. An ablation study further reveals that lower temperature settings and the inclusion of hard-label supervision significantly enhance transferability. These findings suggest that KD can serve not only as a model compression technique but also as a powerful tool for improving the efficiency and effectiveness of black-box adversarial attacks.</li>
<li><strong>摘要：</strong>我们研究了来自多个异构教师模型的知识蒸馏（KD）是否可以增强可转移的对抗性实例的产生。轻巧的学生模型使用两种KD策略进行培训：基于课程的开关和关节优化，并以Resnet50和Densenet-161为教师。然后，受过训练的学生用于使用FG，FGS和PGD攻击来生成对抗性示例，这些攻击是针对黑盒目标模型（Googlenet）评估的。我们的结果表明，从多个教师中提取的学生模型达到的攻击成功率与基于整体的基准相当，同时将对抗性示例的生成时间减少到六倍。一项消融研究进一步表明，较低的温度设置和硬标签监督的包含可显着提高可转移性。这些发现表明，KD不仅可以用作模型压缩技术，而且可以作为提高黑盒对抗攻击的效率和有效性的有力工具。</li>
</ul>

<h3>Title: See Different, Think Better: Visual Variations Mitigating Hallucinations in LVLMs</h3>
<ul>
<li><strong>Authors: </strong>Ziyun Dai, Xiaoqiang Li, Shaohua Zhang, Yuanchen Wu, Jide Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22003">https://arxiv.org/abs/2507.22003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22003">https://arxiv.org/pdf/2507.22003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22003]] See Different, Think Better: Visual Variations Mitigating Hallucinations in LVLMs(https://arxiv.org/abs/2507.22003)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in visual understanding and multimodal reasoning. However, LVLMs frequently exhibit hallucination phenomena, manifesting as the generated textual responses that demonstrate inconsistencies with the provided visual content. Existing hallucination mitigation methods are predominantly text-centric, the challenges of visual-semantic alignment significantly limit their effectiveness, especially when confronted with fine-grained visual understanding scenarios. To this end, this paper presents ViHallu, a Vision-Centric Hallucination mitigation framework that enhances visual-semantic alignment through Visual Variation Image Generation and Visual Instruction Construction. ViHallu introduces \textbf{\textit{visual variation images}} with controllable visual alterations while maintaining the overall image structure. These images, combined with carefully constructed visual instructions, enable LVLMs to better understand fine-grained visual content through fine-tuning, allowing models to more precisely capture the correspondence between visual content and text, thereby enhancing visual-semantic alignment. Extensive experiments on multiple benchmarks show that ViHallu effectively enhances models' fine-grained visual understanding while significantly reducing hallucination tendencies. Furthermore, we release ViHallu-Instruction, a visual instruction dataset specifically designed for hallucination mitigation and visual-semantic alignment. Code is available at this https URL.</li>
<li><strong>摘要：</strong>大型视觉模型（LVLM）在视觉理解和多模式推理方面表现出显着的功能。然而，LVLM经常表现出幻觉现象，表现为产生的文本响应，证明与所提供的视觉内容不一致。现有的缓解幻觉方法主要以文本为中心，视觉语义对齐的挑战显着限制了它们的有效性，尤其是在面对细粒度的视觉理解场景时。为此，本文介绍了Vihallu，这是一个以视觉为中心的减轻幻觉框架，可通过视觉变化图像产生和视觉教学结构来增强视觉语义对齐。 Vihallu引入\ textbf {\ textIt {Visual {Visual变化图像}}，并在维护整体图像结构的同时具有可控的视觉更改。这些图像与精心构造的视觉说明结合在一起，使LVLMS能够通过微调更好地理解细粒度的视觉内容，从而使模型可以更精确地捕获视觉内容和文本之间的对应关系，从而增强视觉 - 语义对齐。对多个基准测试的广泛实验表明，Vihallu有效地增强了模型的细粒视觉理解，同时显着降低了幻觉趋势。此外，我们发布了Vihallu-Instruction，这是一种专门设计用于减轻幻觉和视觉语义对齐的视觉指令数据集。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again</h3>
<ul>
<li><strong>Authors: </strong>Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, Linus, Di Wang, Jie Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.22058">https://arxiv.org/abs/2507.22058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.22058">https://arxiv.org/pdf/2507.22058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.22058]] X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again(https://arxiv.org/abs/2507.22058)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Numerous efforts have been made to extend the ``next token prediction'' paradigm to visual contents, aiming to create a unified approach for both image generation and understanding. Nevertheless, attempts to generate images through autoregressive modeling with discrete tokens have been plagued by issues such as low visual fidelity, distorted outputs, and failure to adhere to complex instructions when rendering intricate details. These shortcomings are likely attributed to cumulative errors during autoregressive inference or information loss incurred during the discretization process. Probably due to this challenge, recent research has increasingly shifted toward jointly training image generation with diffusion objectives and language generation with autoregressive objectives, moving away from unified modeling approaches. In this work, we demonstrate that reinforcement learning can effectively mitigate artifacts and largely enhance the generation quality of a discrete autoregressive modeling method, thereby enabling seamless integration of image and language generation. Our framework comprises a semantic image tokenizer, a unified autoregressive model for both language and images, and an offline diffusion decoder for image generation, termed X-Omni. X-Omni achieves state-of-the-art performance in image generation tasks using a 7B language model, producing images with high aesthetic quality while exhibiting strong capabilities in following instructions and rendering long texts.</li>
<li><strong>摘要：</strong>已经做出了许多努力，以将``临近的标记预测''范式扩展到视觉内容，旨在为图像产生和理解创建统一的方法。然而，试图通过使用离散代币的自回旋建模生成图像的尝试受到低视觉保真度，扭曲的输出以及在呈现复杂的细节时无法遵守复杂指令的问题所困扰。这些缺点很可能归因于在离散过程中自回旋推理或信息损失期间的累积错误。可能是由于这一挑战，最近的研究越来越多地转向以扩散目标和语言产生自回归目标的共同培训图像产生，远离统一的建模方法。在这项工作中，我们证明了强化学习可以有效地减轻人工制品，并在很大程度上提高了离散自回旋建模方法的产生质量，从而实现了图像和语言生成的无缝集成。我们的框架包括语义图像令牌，语言和图像的统一自回归模型，以及用于图像生成的离线扩散解码器，称为X-omni。 X-OMNI使用7B语言模型在图像生成任务中实现最先进的性能，生成具有较高审美质量的图像，同时在以下说明和呈现长文本方面表现出强大的功能。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
