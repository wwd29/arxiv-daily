<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-19</h1>
<h3>Title: A Comprehensive Survey on Deep Learning-Based LiDAR Super-Resolution for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>June Moh Goo, Zichao Zeng, Jan Boehm</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15904">https://arxiv.org/abs/2602.15904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15904">https://arxiv.org/pdf/2602.15904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15904]] A Comprehensive Survey on Deep Learning-Based LiDAR Super-Resolution for Autonomous Driving(https://arxiv.org/abs/2602.15904)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>LiDAR sensors are often considered essential for autonomous driving, but high-resolution sensors remain expensive while affordable low-resolution sensors produce sparse point clouds that miss critical details. LiDAR super-resolution addresses this challenge by using deep learning to enhance sparse point clouds, bridging the gap between different sensor types and enabling cross-sensor compatibility in real-world deployments. This paper presents the first comprehensive survey of LiDAR super-resolution methods for autonomous driving. Despite the importance of practical deployment, no systematic review has been conducted until now. We organize existing approaches into four categories: CNN-based architectures, model-based deep unrolling, implicit representation methods, and Transformer and Mamba-based approaches. We establish fundamental concepts including data representations, problem formulation, benchmark datasets and evaluation metrics. Current trends include the adoption of range image representation for efficient processing, extreme model compression and the development of resolution-flexible architectures. Recent research prioritizes real-time inference and cross-sensor generalization for practical deployment. We conclude by identifying open challenges and future research directions for advancing LiDAR super-resolution technology.</li>
<li><strong>摘要：</strong>LiDAR 传感器通常被认为对于自动驾驶至关重要，但高分辨率传感器仍然昂贵，而经济实惠的低分辨率传感器会产生稀疏的点云，从而错过关键细节。 LiDAR 超分辨率通过使用深度学习增强稀疏点云、弥合不同传感器类型之间的差距并在实际部署中实现跨传感器兼容性来解决这一挑战。本文首次对自动驾驶激光雷达超分辨率方法进行了全面综述。尽管实际部署很重要，但迄今为止尚未进行系统审查。我们将现有方法分为四类：基于 CNN 的架构、基于模型的深度展开、隐式表示方法以及基于 Transformer 和 Mamba 的方法。我们建立了基本概念，包括数据表示、问题表述、基准数据集和评估指标。当前的趋势包括采用范围图像表示来实现高效处理、极限模型压缩以及分辨率灵活的架构的开发。最近的研究优先考虑实时推理和跨传感器泛化以实现实际部署。最后，我们确定了推进激光雷达超分辨率技术的开放挑战和未来研究方向。</li>
</ul>

<h3>Title: Visual Memory Injection Attacks for Multi-Turn Conversations</h3>
<ul>
<li><strong>Authors: </strong>Christian Schlarmann, Matthias Hein</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15927">https://arxiv.org/abs/2602.15927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15927">https://arxiv.org/pdf/2602.15927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15927]] Visual Memory Injection Attacks for Multi-Turn Conversations(https://arxiv.org/abs/2602.15927)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at this https URL</li>
<li><strong>摘要：</strong>生成式大型视觉语言模型（LVLM）最近取得了令人印象深刻的性能提升，并且其用户群正在快速增长。然而，LVLM 的安全性，特别是在长上下文多轮设置中，在很大程度上尚未得到充分研究。在本文中，我们考虑攻击者将经过操纵的图像上传到网络/社交媒体的现实场景。良性用户下载此图像并将其用作 LVLM 的输入。我们新颖的隐形视觉内存注入（VMI）攻击的设计使得在正常提示下 LVLM 表现出名义行为，但一旦用户给出触发提示，LVLM 就会输出特定的规定目标消息来操纵用户，例如用于对抗性营销或政治说服。与之前专注于单轮攻击的工作相比，VMI 即使在与用户进行长时间的多轮对话后也是有效的。我们展示了对最近几个开放权重 LVLM 的攻击。因此，本文表明，在多轮对话设置中使用扰动图像对用户进行大规模操纵是可行的，因此需要 LVLM 具有更好的鲁棒性来抵御这些攻击。我们在此 https URL 发布源代码</li>
</ul>

<h3>Title: Position-Aware Scene-Appearance Disentanglement for Bidirectional Photoacoustic Microscopy Registration</h3>
<ul>
<li><strong>Authors: </strong>Yiwen Wang, Jiahao Qin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15959">https://arxiv.org/abs/2602.15959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15959">https://arxiv.org/pdf/2602.15959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15959]] Position-Aware Scene-Appearance Disentanglement for Bidirectional Photoacoustic Microscopy Registration(https://arxiv.org/abs/2602.15959)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>High-speed optical-resolution photoacoustic microscopy (OR-PAM) with bidirectional raster scanning doubles imaging speed but introduces coupled domain shift and geometric misalignment between forward and backward scan lines. Existing registration methods, constrained by brightness constancy assumptions, achieve limited alignment quality, while recent generative approaches address domain shift through complex architectures that lack temporal awareness across frames. We propose GPEReg-Net, a scene-appearance disentanglement framework that separates domain-invariant scene features from domain-specific appearance codes via Adaptive Instance Normalization (AdaIN), enabling direct image-to-image registration without explicit deformation field estimation. To exploit temporal structure in sequential acquisitions, we introduce a Global Position Encoding (GPE) module that combines learnable position embeddings with sinusoidal encoding and cross-frame attention, allowing the network to leverage context from neighboring frames for improved temporal coherence. On the OR-PAM-Reg-4K benchmark (432 test samples), GPEReg-Net achieves NCC of 0.953, SSIM of 0.932, and PSNR of 34.49dB, surpassing the state-of-the-art by 3.8% in SSIM and 1.99dB in PSNR while maintaining competitive NCC. Code is available at this https URL.</li>
<li><strong>摘要：</strong>具有双向光栅扫描功能的高速光学分辨率光声显微镜 (OR-PAM) 使成像速度加倍，但在前向和后向扫描线之间引入了耦合域移位和几何失准。现有的配准方法受到亮度恒定性假设的限制，只能实现有限的对齐质量，而最近的生成方法通过缺乏跨帧时间感知的复杂架构来解决域转移问题。我们提出了 GPEReg-Net，一种场景外观解开框架，通过自适应实例归一化（AdaIN）将域不变的场景特征与域特定的外观代码分开，从而无需显式变形场估计即可直接进行图像到图像的配准。为了利用顺序采集中的时间结构，我们引入了全局位置编码（GPE）模块，该模块将可学习的位置嵌入与正弦编码和跨帧注意相结合，允许网络利用相邻帧的上下文来提高时间一致性。在 OR-PAM-Reg-4K 基准测试（432 个测试样本）上，GPEReg-Net 的 NCC 为 0.953，SSIM 为 0.932，PSNR 为 34.49dB，在 SSIM 方面超越现有技术 3.8%，在 PSNR 方面超越现有技术 1.99dB，同时保持具有竞争力的 NCC。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: R$^2$Energy: A Large-Scale Benchmark for Robust Renewable Energy Forecasting under Diverse and Extreme Conditions</h3>
<ul>
<li><strong>Authors: </strong>Zhi Sheng, Yuan Yuan, Guozhen Zhang, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15961">https://arxiv.org/abs/2602.15961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15961">https://arxiv.org/pdf/2602.15961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15961]] R$^2$Energy: A Large-Scale Benchmark for Robust Renewable Energy Forecasting under Diverse and Extreme Conditions(https://arxiv.org/abs/2602.15961)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid expansion of renewable energy, particularly wind and solar power, has made reliable forecasting critical for power system operations. While recent deep learning models have achieved strong average accuracy, the increasing frequency and intensity of climate-driven extreme weather events pose severe threats to grid stability and operational security. Consequently, developing robust forecasting models that can withstand volatile conditions has become a paramount challenge. In this paper, we present R$^2$Energy, a large-scale benchmark for NWP-assisted renewable energy forecasting. It comprises over 10.7 million high-fidelity hourly records from 902 wind and solar stations across four provinces in China, providing the diverse meteorological conditions necessary to capture the wide-ranging variability of renewable generation. We further establish a standardized, leakage-free forecasting paradigm that grants all models identical access to future Numerical Weather Prediction (NWP) signals, enabling fair and reproducible comparison across state-of-the-art representative forecasting architectures. Beyond aggregate accuracy, we incorporate regime-wise evaluation with expert-aligned extreme weather annotations, uncovering a critical ``robustness gap'' typically obscured by average metrics. This gap reveals a stark robustness-complexity trade-off: under extreme conditions, a model's reliability is driven by its meteorological integration strategy rather than its architectural complexity. R$^2$Energy provides a principled foundation for evaluating and developing forecasting models for safety-critical power system applications.</li>
<li><strong>摘要：</strong>可再生能源（特别是风能和太阳能）的快速扩张使得可靠的预测对于电力系统的运行至关重要。虽然最近的深度学习模型取得了很高的平均精度，但气候驱动的极端天气事件的频率和强度不断增加，对电网稳定性和运行安全构成了严重威胁。因此，开发能够承受波动条件的稳健预测模型已成为首要挑战。在本文中，我们提出了 R$^2$Energy，这是 NWP 辅助可再生能源预测的大型基准。它包含来自中国四个省的 902 个风能和太阳能站的超过 1070 万条高保真每小时记录，为捕获可再生能源发电的广泛变化提供了必要的多样化气象条件。我们进一步建立了一个标准化、无泄漏的预测范式，允许所有模型对未来数值天气预报（NWP）信号进行相同的访问，从而能够在最先进的代表性预测架构之间进行公平和可重复的比较。除了总体准确性之外，我们还将按制度进行的评估与专家一致的极端天气注释相结合，发现了通常被平均指标掩盖的关键“稳健性差距”。这种差距揭示了一种明显的鲁棒性与复杂性的权衡：在极端条件下，模型的可靠性是由其气象集成策略而不是其架构复杂性驱动的。 R$^2$Energy 为评估和开发安全关键电力系统应用的预测模型提供了原则基础。</li>
</ul>

<h3>Title: B-DENSE: Branching For Dense Ensemble Network Learning</h3>
<ul>
<li><strong>Authors: </strong>Cherish Puniani, Tushar Kumar, Arnav Bendre, Gaurav Kumar, Shree Singhi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15971">https://arxiv.org/abs/2602.15971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15971">https://arxiv.org/pdf/2602.15971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15971]] B-DENSE: Branching For Dense Ensemble Network Learning(https://arxiv.org/abs/2602.15971)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Inspired by non-equilibrium thermodynamics, diffusion models have achieved state-of-the-art performance in generative modeling. However, their iterative sampling nature results in high inference latency. While recent distillation techniques accelerate sampling, they discard intermediate trajectory steps. This sparse supervision leads to a loss of structural information and introduces significant discretization errors. To mitigate this, we propose B-DENSE, a novel framework that leverages multi-branch trajectory alignment. We modify the student architecture to output $K$-fold expanded channels, where each subset corresponds to a specific branch representing a discrete intermediate step in the teacher's trajectory. By training these branches to simultaneously map to the entire sequence of the teacher's target timesteps, we enforce dense intermediate trajectory alignment. Consequently, the student model learns to navigate the solution space from the earliest stages of training, demonstrating superior image generation quality compared to baseline distillation frameworks.</li>
<li><strong>摘要：</strong>受非平衡热力学的启发，扩散模型在生成建模中取得了最先进的性能。然而，它们的迭代采样性质导致推理延迟较高。虽然最近的蒸馏技术加速了采样，但它们放弃了中间轨迹步骤。这种稀疏监督会导致结构信息丢失并引入显着的离散化错误。为了缓解这个问题，我们提出了 B-DENSE，这是一种利用多分支轨迹对齐的新颖框架。我们修改学生架构以输出 $K$ 倍扩展通道，其中每个子集对应于代表教师轨迹中离散中间步骤的特定分支。通过训练这些分支同时映射到教师目标时间步的整个序列，我们强制执行密集的中间轨迹对齐。因此，学生模型从训练的最早阶段就学会了导航解决方案空间，与基线蒸馏框架相比，展示了卓越的图像生成质量。</li>
</ul>

<h3>Title: BTReport: A Framework for Brain Tumor Radiology Report Generation with Clinically Relevant Features</h3>
<ul>
<li><strong>Authors: </strong>Juampablo E. Heras Rivera, Dickson T. Chen, Tianyi Ren, Daniel K. Low, Asma Ben Abacha, Alberto Santamaria-Pang, Mehmet Kurt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16006">https://arxiv.org/abs/2602.16006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16006">https://arxiv.org/pdf/2602.16006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16006]] BTReport: A Framework for Brain Tumor Radiology Report Generation with Clinically Relevant Features(https://arxiv.org/abs/2602.16006)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in radiology report generation (RRG) have been driven by large paired image-text datasets; however, progress in neuro-oncology has been limited due to a lack of open paired image-report datasets. Here, we introduce BTReport, an open-source framework for brain tumor RRG that constructs natural language radiology reports using deterministically extracted imaging features. Unlike existing approaches that rely on large general-purpose or fine-tuned vision-language models for both image interpretation and report composition, BTReport performs deterministic feature extraction for image analysis and uses large language models only for syntactic structuring and narrative formatting. By separating RRG into a deterministic feature extraction step and a report generation step, the generated reports are completely interpretable and less prone to hallucinations. We show that the features used for report generation are predictive of key clinical outcomes, including survival and IDH mutation status, and reports generated by BTReport are more closely aligned with reference clinical reports than existing baselines for RRG. Finally, we introduce BTReport-BraTS, a companion dataset that augments BraTS imaging with synthetically generated radiology reports produced with BTReport. Code for this project can be found at  this https URL.</li>
<li><strong>摘要：</strong>放射学报告生成 (RRG) 的最新进展是由大型图像-文本配对数据集推动的；然而，由于缺乏开放的配对图像报告数据集，神经肿瘤学的进展受到限制。在这里，我们介绍 BTReport，这是一个脑肿瘤 RRG 的开源框架，它使用确定性提取的成像特征构建自然语言放射学报告。与依赖大型通用或微调视觉语言模型进行图像解释和报告撰写的现有方法不同，BTReport 执行图像分析的确定性特征提取，并仅使用大型语言模型进行句法结构和叙述格式设置。通过将 RRG 分为确定性特征提取步骤和报告生成步骤，生成的报告完全可解释并且不易产生幻觉。我们表明，用于报告生成的特征可以预测关键的临床结果，包括生存和 IDH 突变状态，并且与 RRG 的现有基线相比，BTReport 生成的报告与参考临床报告更加一致。最后，我们介绍 BTReport-BraTS，这是一个配套数据集，它通过 BTReport 生成的综合生成的放射学报告来增强 BraTS 成像。该项目的代码可以在此 https URL 中找到。</li>
</ul>

<h3>Title: MolCrystalFlow: Molecular Crystal Structure Prediction via Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Cheng Zeng, Harry W. Sullivan, Thomas Egg, Maya M. Martirossyan, Philipp Höllmer, Jirui Jin, Richard G. Hennig, Adrian Roitberg, Stefano Martiniani, Ellad B. Tadmor, Mingjie Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16020">https://arxiv.org/abs/2602.16020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16020">https://arxiv.org/pdf/2602.16020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16020]] MolCrystalFlow: Molecular Crystal Structure Prediction via Flow Matching(https://arxiv.org/abs/2602.16020)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Molecular crystal structure prediction represents a grand challenge in computational chemistry due to large sizes of constituent molecules and complex intra- and intermolecular interactions. While generative modeling has revolutionized structure discovery for molecules, inorganic solids, and metal-organic frameworks, extending such approaches to fully periodic molecular crystals is still elusive. Here, we present MolCrystalFlow, a flow-based generative model for molecular crystal structure prediction. The framework disentangles intramolecular complexity from intermolecular packing by embedding molecules as rigid bodies and jointly learning the lattice matrix, molecular orientations, and centroid positions. Centroids and orientations are represented on their native Riemannian manifolds, allowing geodesic flow construction and graph neural network operations that respects geometric symmetries. We benchmark our model against state-of-the-art generative models for large-size periodic crystals and rule-based structure generation methods on two open-source molecular crystal datasets. We demonstrate an integration of MolCrystalFlow model with universal machine learning potential to accelerate molecular crystal structure prediction, paving the way for data-driven generative discovery of molecular crystals.</li>
<li><strong>摘要：</strong>由于组成分子的尺寸较大以及分子内和分子间相互作用的复杂性，分子晶体结构预测是计算化学中的一个巨大挑战。虽然生成建模彻底改变了分子、无机固体和金属有机框架的结构发现，但将这种方法扩展到完全周期性的分子晶体仍然难以实现。在这里，我们提出了 MolCrystalFlow，一种用于分子晶体结构预测的基于流的生成模型。该框架通过将分子作为刚体嵌入并共同学习晶格矩阵、分子取向和质心位置，将分子内的复杂性与分子间的堆积分开。质心和方向用其原生黎曼流形表示，允许测地线流构造和尊重几何对称性的图神经网络操作。我们将我们的模型与最先进的大尺寸周期性晶体生成模型和两个开源分子晶体数据集上基于规则的结构生成方法进行基准测试。我们展示了 MolCrystalFlow 模型与通用机器学习潜力的集成，可加速分子晶体结构预测，为数据驱动的分子晶体生成发现铺平道路。</li>
</ul>

<h3>Title: Can Generative Artificial Intelligence Survive Data Contamination? Theoretical Guarantees under Contaminated Recursive Training</h3>
<ul>
<li><strong>Authors: </strong>Kevin Wang, Hongqian Niu, Didong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16065">https://arxiv.org/abs/2602.16065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16065">https://arxiv.org/pdf/2602.16065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16065]] Can Generative Artificial Intelligence Survive Data Contamination? Theoretical Guarantees under Contaminated Recursive Training(https://arxiv.org/abs/2602.16065)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Artificial Intelligence (AI), such as large language models (LLMs), has become a transformative force across science, industry, and society. As these systems grow in popularity, web data becomes increasingly interwoven with this AI-generated material and it is increasingly difficult to separate them from naturally generated content. As generative models are updated regularly, later models will inevitably be trained on mixtures of human-generated data and AI-generated data from earlier versions, creating a recursive training process with data contamination. Existing theoretical work has examined only highly simplified settings, where both the real data and the generative model are discrete or Gaussian, where it has been shown that such recursive training leads to model collapse. However, real data distributions are far more complex, and modern generative models are far more flexible than Gaussian and linear mechanisms. To fill this gap, we study recursive training in a general framework with minimal assumptions on the real data distribution and allow the underlying generative model to be a general universal approximator. In this framework, we show that contaminated recursive training still converges, with a convergence rate equal to the minimum of the baseline model's convergence rate and the fraction of real data used in each iteration. To the best of our knowledge, this is the first (positive) theoretical result on recursive training without distributional assumptions on the data. We further extend the analysis to settings where sampling bias is present in data collection and support all theoretical results with empirical studies.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 等生成人工智能 (AI) 已成为跨科学、工业和社会的变革力量。随着这些系统越来越受欢迎，网络数据与人工智能生成的材料越来越交织在一起，并且越来越难以将它们与自然生成的内容分开。由于生成模型定期更新，后期模型将不可避免地接受早期版本中人类生成的数据和人工智能生成的数据的混合训练，从而创建一个带有数据污染的递归训练过程。现有的理论工作仅研究了高度简化的设置，其中真实数据和生成模型都是离散的或高斯的，已经证明这种递归训练会导致模型崩溃。然而，真实的数据分布要复杂得多，现代生成模型比高斯和线性机制要灵活得多。为了填补这一空白，我们在一个通用框架中研究递归训练，对真实数据分布进行最小的假设，并允许底层生成模型成为通用的通用逼近器。在此框架中，我们表明受污染的递归训练仍然收敛，收敛速率等于基线模型收敛速率和每次迭代中使用的真实数据比例的最小值。据我们所知，这是在没有数据分布假设的情况下递归训练的第一个（积极的）理论结果。我们进一步将分析扩展到数据收集中存在抽样偏差的环境，并通过实证研究支持所有理论结果。</li>
</ul>

<h3>Title: LGQ: Learning Discretization Geometry for Scalable and Stable Image Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Idil Bilge Altun, Mert Onur Cakiroglu, Elham Buxton, Mehmet Dalkilic, Hasan Kurban</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16086">https://arxiv.org/abs/2602.16086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16086">https://arxiv.org/pdf/2602.16086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16086]] LGQ: Learning Discretization Geometry for Scalable and Stable Image Tokenization(https://arxiv.org/abs/2602.16086)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Discrete image tokenization is a key bottleneck for scalable visual generation: a tokenizer must remain compact for efficient latent-space priors while preserving semantic structure and using discrete capacity effectively. Existing quantizers face a trade-off: vector-quantized tokenizers learn flexible geometries but often suffer from biased straight-through optimization, codebook under-utilization, and representation collapse at large vocabularies. Structured scalar or implicit tokenizers ensure stable, near-complete utilization by design, yet rely on fixed discretization geometries that may allocate capacity inefficiently under heterogeneous latent statistics. We introduce Learnable Geometric Quantization (LGQ), a discrete image tokenizer that learns discretization geometry end-to-end. LGQ replaces hard nearest-neighbor lookup with temperature-controlled soft assignments, enabling fully differentiable training while recovering hard assignments at inference. The assignments correspond to posterior responsibilities of an isotropic Gaussian mixture and minimize a variational free-energy objective, provably converging to nearest-neighbor quantization in the low-temperature limit. LGQ combines a token-level peakedness regularizer with a global usage regularizer to encourage confident yet balanced code utilization without imposing rigid grids. Under a controlled VQGAN-style backbone on ImageNet across multiple vocabulary sizes, LGQ achieves stable optimization and balanced utilization. At 16K codebook size, LGQ improves rFID by 11.88% over FSQ while using 49.96% fewer active codes, and improves rFID by 6.06% over SimVQ with 49.45% lower effective representation rate, achieving comparable fidelity with substantially fewer active entries. Our GitHub repository is available at: this https URL</li>
<li><strong>摘要：</strong>离散图像标记化是可扩展视觉生成的关键瓶颈：标记器必须保持紧凑，以实现高效的潜在空间先验，同时保留语义结构并有效使用离散容量。现有的量化器面临着一个权衡：矢量量化分词器学习灵活的几何结构，但经常遭受有偏差的直通优化、码本利用率不足以及大词汇表下表示崩溃的问题。结构化标量或隐式分词器通过设计确保稳定、近乎完全的利用，但依赖于固定的离散化几何结构，这可能会在异构潜在统计数据下低效地分配容量。我们引入了可学习几何量化（LGQ），这是一种离散图像分词器，可以端到端学习离散化几何。 LGQ 用温度控制的软分配取代了硬最近邻查找，从而实现完全可微的训练，同时在推理时恢复硬分配。这些分配对应于各向同性高斯混合的后验责任，并最小化变分自由能目标，可证明在低温极限下收敛于最近邻量化。 LGQ 将令牌级峰值正则化器与全局使用正则化器相结合，以鼓励自信而平衡的代码使用，而无需强加严格的网格。在 ImageNet 上跨多个词汇量的受控 VQGAN 式主干下，LGQ 实现了稳定的优化和平衡的利用。在 16K 码本大小下，LGQ 比 FSQ 使 rFID 提高了 11.88%，同时使用的活动代码减少了 49.96%；与 SimVQ 相比，rFID 提高了 6.06%，有效表示率降低了 49.45%，从而以大幅减少的活动条目实现了相当的保真度。我们的 GitHub 存储库位于：此 https URL</li>
</ul>

<h3>Title: Why Any-Order Autoregressive Models Need Two-Stream Attention: A Structural-Semantic Tradeoff</h3>
<ul>
<li><strong>Authors: </strong>Patrick Pynadath, Ruqi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16092">https://arxiv.org/abs/2602.16092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16092">https://arxiv.org/pdf/2602.16092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16092]] Why Any-Order Autoregressive Models Need Two-Stream Attention: A Structural-Semantic Tradeoff(https://arxiv.org/abs/2602.16092)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Any-order autoregressive models (AO-ARMs) offer a promising path toward efficient masked diffusion by enabling native key-value caching, but competitive performance has so far required two-stream attention, typically motivated as a means of decoupling token content from position. In this work, we argue that two-stream attention may be serving a more subtle role. We identify a structural-semantic tradeoff in any-order generation: the hidden representation at each step must simultaneously attend to semantically informative tokens for prediction and structurally recent tokens for summarization, objectives that compete for attention capacity in a single stream but can specialize across two streams. To isolate this tradeoff from position-content separation, we propose Decoupled RoPE, a modification to rotary position embeddings that provides target position information without revealing target content. Decoupled RoPE performs competitively at short sequence lengths--where semantic and structural proximity coincide--but degrades as sequence length increases and the two orderings diverge. These results suggest that the success of two-stream attention stems not merely from separating position from content, but from circumventing the deeper structural-semantic tradeoff inherent to any-order generation.</li>
<li><strong>摘要：</strong>任意阶自回归模型（AO-ARM）通过启用本机键值缓存，为高效屏蔽扩散提供了一条有希望的途径，但迄今为止，竞争性能需要双流注意力，通常作为将令牌内容与位置解耦的一种手段。在这项工作中，我们认为双流注意力可能发挥着更微妙的作用。我们确定了任意顺序生成中的结构语义权衡：每个步骤的隐藏表示必须同时关注用于预测的语义信息标记和用于总结的结构上最新的标记，这些目标在单个流中争夺注意力能力，但可以专门化两个流。为了将这种权衡与位置内容分离隔离开来，我们提出了解耦 RoPE，这是对旋转位置嵌入的一种修改，可提供目标位置信息而不泄露目标内容。解耦 RoPE 在短序列长度（语义和结构接近性一致）下表现出竞争力，但随着序列长度增加和两个排序不同而性能下降。这些结果表明，双流注意力的成功不仅源于将位置与内容分开，还源于规避了任意顺序生成所固有的更深层次的结构语义权衡。</li>
</ul>

<h3>Title: Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing</h3>
<ul>
<li><strong>Authors: </strong>Huichan Seo, Minki Hong, Sieun Choi, Jihie Kim, Jean Oh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16149">https://arxiv.org/abs/2602.16149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16149">https://arxiv.org/pdf/2602.16149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16149]] Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing(https://arxiv.org/abs/2602.16149)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Demographic bias in text-to-image (T2I) generation is well studied, yet demographic-conditioned failures in instruction-guided image-to-image (I2I) editing remain underexplored. We examine whether identical edit instructions yield systematically different outcomes across subject demographics in open-weight I2I editors. We formalize two failure modes: Soft Erasure, where edits are silently weakened or ignored in the output image, and Stereotype Replacement, where edits introduce unrequested, stereotype-consistent attributes. We introduce a controlled benchmark that probes demographic-conditioned behavior by generating and editing portraits conditioned on race, gender, and age using a diagnostic prompt set, and evaluate multiple editors with vision-language model (VLM) scoring and human evaluation. Our analysis shows that identity preservation failures are pervasive, demographically uneven, and shaped by implicit social priors, including occupation-driven gender inference. Finally, we demonstrate that a prompt-level identity constraint, without model updates, can substantially reduce demographic change for minority groups while leaving majority-group portraits largely unchanged, revealing asymmetric identity priors in current editors. Together, our findings establish identity preservation as a central and demographically uneven failure mode in I2I editing and motivate demographic-robust editing systems. Project page: this https URL</li>
<li><strong>摘要：</strong>文本到图像（T2I）生成中的人口统计学偏差已得到充分研究，但指令引导的图像到图像（I2I）编辑中人口统计学条件下的失败仍未得到充分研究。我们研究在开放权重 I2I 编辑器中，相同的编辑指令是否会在不同的受试者人口统计数据中产生系统不同的结果。我们形式化了两种失败模式：软擦除，其中编辑在输出图像中被悄悄削弱或忽略，以及刻板印象替换，其中编辑引入了未经请求的、刻板印象一致的属性。我们引入了一个受控基准，通过使用诊断提示集生成和编辑以种族、性别和年龄为条件的肖像来探测人口统计行为，并使用视觉语言模型（VLM）评分和人类评估来评估多个编辑器。我们的分析表明，身份保护失败是普遍存在的、人口结构不平衡，并受到隐性社会先验的影响，包括职业驱动的性别推断。最后，我们证明，在没有模型更新的情况下，即时级别的身份约束可以大大减少少数群体的人口变化，同时使多数群体的肖像基本保持不变，从而揭示当前编辑中的不对称身份先验。总之，我们的研究结果将身份保存确定为 I2I 编辑中的一个核心且人口统计不均衡的失败模式，并激发了人口统计稳健的编辑系统。项目页面：此 https URL</li>
</ul>

<h3>Title: Discrete Stochastic Localization for Non-autoregressive Generation</h3>
<ul>
<li><strong>Authors: </strong>Yunshu Wu, Jiayi Cheng, Partha Thakuria, Rob Brekelmans, Evangelos E. Papalexakis, Greg Ver Steeg</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16169">https://arxiv.org/abs/2602.16169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16169">https://arxiv.org/pdf/2602.16169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16169]] Discrete Stochastic Localization for Non-autoregressive Generation(https://arxiv.org/abs/2602.16169)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Non-autoregressive (NAR) generation reduces decoding latency by predicting many tokens in parallel, but iterative refinement often suffers from error accumulation and distribution shift under self-generated drafts. Masked diffusion language models (MDLMs) and their remasking samplers (e.g., ReMDM) can be viewed as modern NAR iterative refinement, where generation repeatedly revises a partially observed draft. In this work we show that \emph{training alone} can substantially improve the step-efficiency of MDLM/ReMDM sampling. We propose \textsc{DSL} (Discrete Stochastic Localization), which trains a single SNR-invariant denoiser across a continuum of corruption levels, bridging intermediate draft noise and mask-style endpoint corruption within one Diffusion Transformer. On OpenWebText, \textsc{DSL} fine-tuning yields large MAUVE gains at low step budgets, surpassing the MDLM+ReMDM baseline with \(\sim\)4$\times$ fewer denoiser evaluations, and matches autoregressive quality at high budgets. Analyses show improved self-correction and uncertainty calibration, making remasking markedly more compute-efficient.</li>
<li><strong>摘要：</strong>非自回归（NAR）生成通过并行预测许多令牌来减少解码延迟，但迭代细化通常会在自生成草稿下遭受错误累积和分布偏移的影响。屏蔽扩散语言模型 (MDLM) 及其重新屏蔽采样器（例如 ReMDM）可以被视为现代 NAR 迭代细化，其中生成过程会反复修改部分观察到的草稿。在这项工作中，我们证明 \emph{单独训练} 可以显着提高 MDLM/ReMDM 采样的步骤效率。我们提出 \textsc{DSL} （离散随机定位），它在一个连续的损坏级别上训练单个 SNR 不变的降噪器，在一个扩散变压器内桥接中间草案噪声和掩模式端点损坏。在 OpenWebText 上，\textsc{DSL} 微调在低步长预算下产生巨大的 MAUVE 增益，以少 \(\sim\)4$\times 的降噪器评估超越 MDLM+ReMDM 基线，并在高预算下匹配自回归质量。分析显示，自我校正和不确定性校准得到了改进，使重新屏蔽的计算效率显着提高。</li>
</ul>

<h3>Title: Towards Secure and Scalable Energy Theft Detection: A Federated Learning Approach for Resource-Constrained Smart Meters</h3>
<ul>
<li><strong>Authors: </strong>Diego Labate, Dipanwita Thakur, Giancarlo Fortino</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16181">https://arxiv.org/abs/2602.16181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16181">https://arxiv.org/pdf/2602.16181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16181]] Towards Secure and Scalable Energy Theft Detection: A Federated Learning Approach for Resource-Constrained Smart Meters(https://arxiv.org/abs/2602.16181)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Energy theft poses a significant threat to the stability and efficiency of smart grids, leading to substantial economic losses and operational challenges. Traditional centralized machine learning approaches for theft detection require aggregating user data, raising serious concerns about privacy and data security. These issues are further exacerbated in smart meter environments, where devices are often resource-constrained and lack the capacity to run heavy models. In this work, we propose a privacy-preserving federated learning framework for energy theft detection that addresses both privacy and computational constraints. Our approach leverages a lightweight multilayer perceptron (MLP) model, suitable for deployment on low-power smart meters, and integrates basic differential privacy (DP) by injecting Gaussian noise into local model updates before aggregation. This ensures formal privacy guarantees without compromising learning performance. We evaluate our framework on a real-world smart meter dataset under both IID and non-IID data distributions. Experimental results demonstrate that our method achieves competitive accuracy, precision, recall, and AUC scores while maintaining privacy and efficiency. This makes the proposed solution practical and scalable for secure energy theft detection in next-generation smart grid infrastructures.</li>
<li><strong>摘要：</strong>能源盗窃对智能电网的稳定性和效率构成重大威胁，导致巨大的经济损失和运营挑战。用于盗窃检测的传统集中式机器学习方法需要聚合用户数据，这引发了对隐私和数据安全的严重担忧。在智能电表环境中，这些问题会进一步加剧，因为在智能电表环境中，设备通常资源有限，并且缺乏运行重型模型的能力。在这项工作中，我们提出了一种用于能源盗窃检测的隐私保护联合学习框架，该框架解决了隐私和计算限制。我们的方法利用适合部署在低功耗智能电表上的轻量级多层感知器（MLP）模型，并通过在聚合之前将高斯噪声注入本地模型更新来集成基本差分隐私（DP）。这确保了正式的隐私保证，同时又不影响学习成绩。我们在 IID 和非 IID 数据分布下的真实智能电表数据集上评估我们的框架。实验结果表明，我们的方法在保持隐私和效率的同时，实现了具有竞争力的准确度、精确度、召回率和 AUC 分数。这使得所提出的解决方案对于下一代智能电网基础设施中的安全能源盗窃检测具有实用性和可扩展性。</li>
</ul>

<h3>Title: Training-Free Adaptation of Diffusion Models via Doob's $h$-Transform</h3>
<ul>
<li><strong>Authors: </strong>Qijie Zhu, Zeqi Ye, Han Liu, Zhaoran Wang, Minshuo Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16198">https://arxiv.org/abs/2602.16198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16198">https://arxiv.org/pdf/2602.16198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16198]] Training-Free Adaptation of Diffusion Models via Doob's $h$-Transform(https://arxiv.org/abs/2602.16198)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Adaptation methods have been a workhorse for unlocking the transformative power of pre-trained diffusion models in diverse applications. Existing approaches often abstract adaptation objectives as a reward function and steer diffusion models to generate high-reward samples. However, these approaches can incur high computational overhead due to additional training, or rely on stringent assumptions on the reward such as differentiability. Moreover, despite their empirical success, theoretical justification and guarantees are seldom established. In this paper, we propose DOIT (Doob-Oriented Inference-time Transformation), a training-free and computationally efficient adaptation method that applies to generic, non-differentiable rewards. The key framework underlying our method is a measure transport formulation that seeks to transport the pre-trained generative distribution to a high-reward target distribution. We leverage Doob's $h$-transform to realize this transport, which induces a dynamic correction to the diffusion sampling process and enables efficient simulation-based computation without modifying the pre-trained model. Theoretically, we establish a high probability convergence guarantee to the target high-reward distribution via characterizing the approximation error in the dynamic Doob's correction. Empirically, on D4RL offline RL benchmarks, our method consistently outperforms state-of-the-art baselines while preserving sampling efficiency.</li>
<li><strong>摘要：</strong>适应方法一直是在不同应用中释放预训练扩散模型变革能力的主力。现有方法通常将适应目标抽象为奖励函数，并引导扩散模型生成高奖励样本。然而，这些方法可能会由于额外的训练而产生较高的计算开销，或者依赖于对奖励（例如可微分性）的严格假设。此外，尽管它们在实证上取得了成功，但理论依据和保证却很少建立。在本文中，我们提出了 DOIT（面向 Doob 的推理时间变换），这是一种免训练且计算高效的适应方法，适用于通用的、不可微分的奖励。我们方法的关键框架是一种度量传输公式，旨在将预训练的生成分布传输到高奖励目标分布。我们利用 Doob 的 $h$ 变换来实现这种传输，从而对扩散采样过程进行动态校正，并在不修改预先训练的模型的情况下实现基于模拟的高效计算。理论上，我们通过表征动态杜布校正中的近似误差，建立了对目标高奖励分布的高概率收敛保证。根据经验，在 D4RL 离线 RL 基准上，我们的方法始终优于最先进的基线，同时保持采样效率。</li>
</ul>

<h3>Title: Factored Latent Action World Models</h3>
<ul>
<li><strong>Authors: </strong>Zizhao Wang, Chang Shi, Jiaheng Hu, Kevin Rohling, Roberto Martín-Martín, Amy Zhang, Peter Stone</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16229">https://arxiv.org/abs/2602.16229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16229">https://arxiv.org/pdf/2602.16229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16229]] Factored Latent Action World Models(https://arxiv.org/abs/2602.16229)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Learning latent actions from action-free video has emerged as a powerful paradigm for scaling up controllable world model learning. Latent actions provide a natural interface for users to iteratively generate and manipulate videos. However, most existing approaches rely on monolithic inverse and forward dynamics models that learn a single latent action to control the entire scene, and therefore struggle in complex environments where multiple entities act simultaneously. This paper introduces Factored Latent Action Model (FLAM), a factored dynamics framework that decomposes the scene into independent factors, each inferring its own latent action and predicting its own next-step factor value. This factorized structure enables more accurate modeling of complex multi-entity dynamics and improves video generation quality in action-free video settings compared to monolithic models. Based on experiments on both simulation and real-world multi-entity datasets, we find that FLAM outperforms prior work in prediction accuracy and representation quality, and facilitates downstream policy learning, demonstrating the benefits of factorized latent action models.</li>
<li><strong>摘要：</strong>从无动作视频中学习潜在动作已成为扩大可控世界模型学习的强大范例。潜在动作为用户迭代生成和操作视频提供了一个自然的界面。然而，大多数现有方法依赖于整体逆向和正向动力学模型，这些模型学习单个潜在动作来控制整个场景，因此在多个实体同时动作的复杂环境中陷入困境。本文介绍了因子式潜在动作模型（FLAM），这是一种因子式动力学框架，它将场景分解为独立的因素，每个因子都推断自己的潜在动作并预测自己的下一步因子值。与整体模型相比，这种分解结构可以更准确地对复杂的多实体动态进行建模，并提高无动作视频设置中的视频生成质量。基于模拟和现实世界多实体数据集的实验，我们发现 FLAM 在预测精度和表示质量方面优于先前的工作，并促进下游策略学习，展示了因子化潜在动作模型的好处。</li>
</ul>

<h3>Title: DataCube: A Video Retrieval Platform via Natural Language Semantic Profiling</h3>
<ul>
<li><strong>Authors: </strong>Yiming Ju, Hanyu Zhao, Quanyue Ma, Donglin Hao, Chengwei Wu, Ming Li, Songjing Wang, Tengfei Pan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16231">https://arxiv.org/abs/2602.16231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16231">https://arxiv.org/pdf/2602.16231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16231]] DataCube: A Video Retrieval Platform via Natural Language Semantic Profiling(https://arxiv.org/abs/2602.16231)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large-scale video repositories are increasingly available for modern video understanding and generation tasks. However, transforming raw videos into high-quality, task-specific datasets remains costly and inefficient. We present DataCube, an intelligent platform for automatic video processing, multi-dimensional profiling, and query-driven retrieval. DataCube constructs structured semantic representations of video clips and supports hybrid retrieval with neural re-ranking and deep semantic matching. Through an interactive web interface, users can efficiently construct customized video subsets from massive repositories for training, analysis, and evaluation, and build searchable systems over their own private video collections. The system is publicly accessible at this https URL. Demo Video: this https URL</li>
<li><strong>摘要：</strong>大型视频存储库越来越多地可用于现代视频理解和生成任务。然而，将原始视频转换为高质量、特定于任务的数据集仍然成本高昂且效率低下。我们推出 DataCube，一个用于自动视频处理、多维分析和查询驱动检索的智能平台。 DataCube 构建视频剪辑的结构化语义表示，并支持神经重新排序和深度语义匹配的混合检索。通过交互式网络界面，用户可以从海量存储库中高效地构建定制视频子集，用于训练、分析和评估，并在自己的私人视频集合上构建可搜索系统。该系统可通过此 https URL 公开访问。演示视频：此 https URL</li>
</ul>

<h3>Title: EasyControlEdge: A Foundation-Model Fine-Tuning for Edge Detection</h3>
<ul>
<li><strong>Authors: </strong>Hiroki Nakamura, Hiroto Iino, Masashi Okada, Tadahiro Taniguchi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16238">https://arxiv.org/abs/2602.16238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16238">https://arxiv.org/pdf/2602.16238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16238]] EasyControlEdge: A Foundation-Model Fine-Tuning for Edge Detection(https://arxiv.org/abs/2602.16238)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose EasyControlEdge, adapting an image-generation foundation model to edge detection. In real-world edge detection (e.g., floor-plan walls, satellite roads/buildings, and medical organ boundaries), crispness and data efficiency are crucial, yet producing crisp raw edge maps with limited training samples remains challenging. Although image-generation foundation models perform well on many downstream tasks, their pretrained priors for data-efficient transfer and iterative refinement for high-frequency detail preservation remain underexploited for edge detection. To enable crisp and data-efficient edge detection using these capabilities, we introduce an edge-specialized adaptation of image-generation foundation models. To better specialize the foundation model for edge detection, we incorporate an edge-oriented objective with an efficient pixel-space loss. At inference, we introduce guidance based on unconditional dynamics, enabling a single model to control the edge density through a guidance scale. Experiments on BSDS500, NYUDv2, BIPED, and CubiCasa compare against state-of-the-art methods and show consistent gains, particularly under no-post-processing crispness evaluation and with limited training data.</li>
<li><strong>摘要：</strong>我们提出了 EasyControlEdge，将图像生成基础模型应用于边缘检测。在现实世界的边缘检测（例如，平面图墙、卫星道路/建筑物和医疗器官边界）中，清晰度和数据效率至关重要，但用有限的训练样本生成清晰的原始边缘图仍然具有挑战性。尽管图像生成基础模型在许多下游任务上表现良好，但其用于数据高效传输的预训练先验和用于高频细节保留的迭代细化在边缘检测方面仍未得到充分利用。为了使用这些功能实现清晰且数据高效的边缘检测，我们引入了图像生成基础模型的边缘专用适应。为了更好地专门化边缘检测的基础模型，我们将面向边缘的目标与有效的像素空间损失结合起来。在推理时，我们引入了基于无条件动力学的引导，使单个模型能够通过引导尺度来控制边缘密度。 BSDS500、NYUDv2、BIPED 和 CubiCasa 上的实验与最先进的方法进行了比较，并显示出一致的增益，特别是在无后处理脆度评估和有限训练数据的情况下。</li>
</ul>

<h3>Title: GICDM: Mitigating Hubness for Reliable Distance-Based Generative Model Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Salvy, Hugues Talbot, Bertrand Thirion</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16449">https://arxiv.org/abs/2602.16449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16449">https://arxiv.org/pdf/2602.16449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16449]] GICDM: Mitigating Hubness for Reliable Distance-Based Generative Model Evaluation(https://arxiv.org/abs/2602.16449)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative model evaluation commonly relies on high-dimensional embedding spaces to compute distances between samples. We show that dataset representations in these spaces are affected by the hubness phenomenon, which distorts nearest neighbor relationships and biases distance-based metrics. Building on the classical Iterative Contextual Dissimilarity Measure (ICDM), we introduce Generative ICDM (GICDM), a method to correct neighborhood estimation for both real and generated data. We introduce a multi-scale extension to improve empirical behavior. Extensive experiments on synthetic and real benchmarks demonstrate that GICDM resolves hubness-induced failures, restores reliable metric behavior, and improves alignment with human judgment.</li>
<li><strong>摘要：</strong>生成模型评估通常依赖于高维嵌入空间来计算样本之间的距离。我们表明，这些空间中的数据集表示受到中心现象的影响，这种现象扭曲了最近邻居关系并使基于距离的度量产生偏差。在经典的迭代上下文相异性测量（ICDM）的基础上，我们引入了生成 ICDM（GICDM），这是一种纠正真实数据和生成数据的邻域估计的方法。我们引入多尺度扩展来改善经验行为。对合成和真实基准的大量实验表明，GICDM 解决了中心引起的故障，恢复了可靠的度量行为，并提高了与人类判断的一致性。</li>
</ul>

<h3>Title: Fast and Scalable Analytical Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Shang, Peng Sun, Jingyu Lin, Zhiqiang Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16498">https://arxiv.org/abs/2602.16498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16498">https://arxiv.org/pdf/2602.16498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16498]] Fast and Scalable Analytical Diffusion(https://arxiv.org/abs/2602.16498)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Analytical diffusion models offer a mathematically transparent path to generative modeling by formulating the denoising score as an empirical-Bayes posterior mean. However, this interpretability comes at a prohibitive cost: the standard formulation necessitates a full-dataset scan at every timestep, scaling linearly with dataset size. In this work, we present the first systematic study addressing this scalability bottleneck. We challenge the prevailing assumption that the entire training data is necessary, uncovering the phenomenon of Posterior Progressive Concentration: the effective golden support of the denoising score is not static but shrinks asymptotically from the global manifold to a local neighborhood as the signal-to-noise ratio increases. Capitalizing on this, we propose Dynamic Time-Aware Golden Subset Diffusion (GoldDiff), a training-free framework that decouples inference complexity from dataset size. Instead of static retrieval, GoldDiff uses a coarse-to-fine mechanism to dynamically pinpoint the ''Golden Subset'' for inference. Theoretically, we derive rigorous bounds guaranteeing that our sparse approximation converges to the exact score. Empirically, GoldDiff achieves a $\bf 71 \times$ speedup on AFHQ while matching or achieving even better performance than full-scan baselines. Most notably, we demonstrate the first successful scaling of analytical diffusion to ImageNet-1K, unlocking a scalable, training-free paradigm for large-scale generative modeling.</li>
<li><strong>摘要：</strong>分析扩散模型通过将去噪分数表示为经验贝叶斯后验平均值，为生成建模提供了一条数学上透明的路径。然而，这种可解释性的代价高昂：标准公式需要在每个时间步进行完整数据集扫描，并随数据集大小线性缩放。在这项工作中，我们提出了第一个解决这一可扩展性瓶颈的系统研究。我们挑战了整个训练数据是必要的普遍假设，揭示了后验渐进集中现象：去噪分数的有效黄金支持不是静态的，而是随着信噪比的增加从全局流形渐近收缩到局部邻域。利用这一点，我们提出了动态时间感知黄金子集扩散（GoldDiff），这是一种免训练框架，可将推理复杂性与数据集大小解耦。 GoldDiff 没有使用静态检索，而是使用从粗到细的机制来动态定位“黄金子集”以进行推理。理论上，我们得出严格的界限，保证我们的稀疏近似收敛到精确的分数。根据经验，GoldDiff 在 AFHQ 上实现了 $\bf 71 \times$ 加速，同时匹配或实现了比全扫描基线更好的性能。最值得注意的是，我们首次成功地将分析扩散扩展到 ImageNet-1K，为大规模生成建模解锁了可扩展、免训练的范式。</li>
</ul>

<h3>Title: DressWild: Feed-Forward Pose-Agnostic Garment Sewing Pattern Generation from In-the-Wild Images</h3>
<ul>
<li><strong>Authors: </strong>Zeng Tao, Ying Jiang, Yunuo Chen, Tianyi Xie, Huamin Wang, Yingnian Wu, Yin Yang, Abishek Sampath Kumar, Kenji Tashiro, Chenfanfu Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16502">https://arxiv.org/abs/2602.16502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16502">https://arxiv.org/pdf/2602.16502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16502]] DressWild: Feed-Forward Pose-Agnostic Garment Sewing Pattern Generation from In-the-Wild Images(https://arxiv.org/abs/2602.16502)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in garment pattern generation have shown promising progress. However, existing feed-forward methods struggle with diverse poses and viewpoints, while optimization-based approaches are computationally expensive and difficult to scale. This paper focuses on sewing pattern generation for garment modeling and fabrication applications that demand editable, separable, and simulation-ready garments. We propose DressWild, a novel feed-forward pipeline that reconstructs physics-consistent 2D sewing patterns and the corresponding 3D garments from a single in-the-wild image. Given an input image, our method leverages vision-language models (VLMs) to normalize pose variations at the image level, then extract pose-aware, 3D-informed garment features. These features are fused through a transformer-based encoder and subsequently used to predict sewing pattern parameters, which can be directly applied to physical simulation, texture synthesis, and multi-layer virtual try-on. Extensive experiments demonstrate that our approach robustly recovers diverse sewing patterns and the corresponding 3D garments from in-the-wild images without requiring multi-view inputs or iterative optimization, offering an efficient and scalable solution for realistic garment simulation and animation.</li>
<li><strong>摘要：</strong>服装图案生成的最新进展显示出有希望的进展。然而，现有的前馈方法难以应对不同的姿势和视角，而基于优化的方法计算成本昂贵且难以扩展。本文重点关注服装建模和制造应用程序的缝纫图案生成，这些应用程序需要可编辑、可分离和可模拟的服装。我们提出了 DressWild，一种新颖的前馈管道，可以从单个野外图像重建物理一致的 2D 缝纫图案和相应的 3D 服装。给定输入图像，我们的方法利用视觉语言模型 (VLM) 在图像级别标准化姿势变化，然后提取姿势感知、3D 信息服装特征。这些特征通过基于变压器的编码器融合，随后用于预测缝纫图案参数，可直接应用于物理模拟、纹理合成和多层虚拟试穿。大量实验表明，我们的方法可以从野外图像中稳健地恢复各种缝纫图案和相应的 3D 服装，而无需多视图输入或迭代优化，为真实的服装模拟和动画提供高效且可扩展的解决方案。</li>
</ul>

<h3>Title: RIDER: 3D RNA Inverse Design with Reinforcement Learning-Guided Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Tianmeng Hu, Yongzheng Cui, Biao Luo, Ke Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16548">https://arxiv.org/abs/2602.16548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16548">https://arxiv.org/pdf/2602.16548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16548]] RIDER: 3D RNA Inverse Design with Reinforcement Learning-Guided Diffusion(https://arxiv.org/abs/2602.16548)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The inverse design of RNA three-dimensional (3D) structures is crucial for engineering functional RNAs in synthetic biology and therapeutics. While recent deep learning approaches have advanced this field, they are typically optimized and evaluated using native sequence recovery, which is a limited surrogate for structural fidelity, since different sequences can fold into similar 3D structures and high recovery does not necessarily indicate correct folding. To address this limitation, we propose RIDER, an RNA Inverse DEsign framework with Reinforcement learning that directly optimizes for 3D structural similarity. First, we develop and pre-train a GNN-based generative diffusion model conditioned on the target 3D structure, achieving a 9% improvement in native sequence recovery over state-of-the-art methods. Then, we fine-tune the model with an improved policy gradient algorithm using four task-specific reward functions based on 3D self-consistency metrics. Experimental results show that RIDER improves structural similarity by over 100% across all metrics and discovers designs that are distinct from native sequences.</li>
<li><strong>摘要：</strong>RNA 三维 (3D) 结构的逆向设计对于合成生物学和治疗学中的功能性 RNA 工程至关重要。虽然最近的深度学习方法在这一领域取得了进展，但它们通常使用本机序列恢复进行优化和评估，这是结构保真度的有限替代品，因为不同的序列可以折叠成类似的 3D 结构，而高恢复并不一定表明折叠正确。为了解决这个限制，我们提出了 RIDER，一种具有强化学习功能的 RNA 逆向设计框架，可直接优化 3D 结构相似性。首先，我们开发并预训练了一个以目标 3D 结构为条件的基于 GNN 的生成扩散模型，与最先进的方法相比，原生序列恢复提高了 9%。然后，我们使用基于 3D 自一致性指标的四个特定于任务的奖励函数，通过改进的策略梯度算法对模型进行微调。实验结果表明，RIDER 将所有指标的结构相似性提高了 100% 以上，并发现了与天然序列不同的设计。</li>
</ul>

<h3>Title: Arc2Morph: Identity-Preserving Facial Morphing with Arc2Face</h3>
<ul>
<li><strong>Authors: </strong>Nicolò Di Domenico, Annalisa Franco, Matteo Ferrara, Davide Maltoni</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16569">https://arxiv.org/abs/2602.16569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16569">https://arxiv.org/pdf/2602.16569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16569]] Arc2Morph: Identity-Preserving Facial Morphing with Arc2Face(https://arxiv.org/abs/2602.16569)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Face morphing attacks are widely recognized as one of the most challenging threats to face recognition systems used in electronic identity documents. These attacks exploit a critical vulnerability in passport enrollment procedures adopted by many countries, where the facial image is often acquired without a supervised live capture process. In this paper, we propose a novel face morphing technique based on Arc2Face, an identity-conditioned face foundation model capable of synthesizing photorealistic facial images from compact identity representations. We demonstrate the effectiveness of the proposed approach by comparing the morphing attack potential metric on two large-scale sequestered face morphing attack detection datasets against several state-of-the-art morphing methods, as well as on two novel morphed face datasets derived from FEI and ONOT. Experimental results show that the proposed deep learning-based approach achieves a morphing attack potential comparable to that of landmark-based techniques, which have traditionally been regarded as the most challenging. These findings confirm the ability of the proposed method to effectively preserve and manage identity information during the morph generation process.</li>
<li><strong>摘要：</strong>人脸变形攻击被广泛认为是对电子身份证件中使用的人脸识别系统最具挑战性的威胁之一。这些攻击利用了许多国家采用的护照登记程序中的一个关键漏洞，其中面部图像通常是在没有监督的实时捕获过程的情况下获取的。在本文中，我们提出了一种基于 Arc2Face 的新型面部变形技术，Arc2Face 是一种身份条件面部基础模型，能够从紧凑的身份表示中合成照片级真实感面部图像。我们通过将两个大规模隔离人脸变形攻击检测数据集与几种最先进的变形方法以及从 FEI 和 ONOT 派生的两个新颖的变形人脸数据集上的变形攻击潜在指标进行比较，证明了所提出方法的有效性。实验结果表明，所提出的基于深度学习的方法实现了与传统上被认为最具挑战性的基于地标的技术相当的变形攻击潜力。这些发现证实了所提出的方法在变形生成过程中有效保存和管理身份信息的能力。</li>
</ul>

<h3>Title: Steering diffusion models with quadratic rewards: a fine-grained analysis</h3>
<ul>
<li><strong>Authors: </strong>Ankur Moitra, Andrej Risteski, Dhruv Rohatgi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16570">https://arxiv.org/abs/2602.16570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16570">https://arxiv.org/pdf/2602.16570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16570]] Steering diffusion models with quadratic rewards: a fine-grained analysis(https://arxiv.org/abs/2602.16570)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Inference-time algorithms are an emerging paradigm in which pre-trained models are used as subroutines to solve downstream tasks. Such algorithms have been proposed for tasks ranging from inverse problems and guided image generation to reasoning. However, the methods currently deployed in practice are heuristics with a variety of failure modes -- and we have very little understanding of when these heuristics can be efficiently improved. In this paper, we consider the task of sampling from a reward-tilted diffusion model -- that is, sampling from $p^{\star}(x) \propto p(x) \exp(r(x))$ -- given a reward function $r$ and pre-trained diffusion oracle for $p$. We provide a fine-grained analysis of the computational tractability of this task for quadratic rewards $r(x) = x^\top A x + b^\top x$. We show that linear-reward tilts are always efficiently sampleable -- a simple result that seems to have gone unnoticed in the literature. We use this as a building block, along with a conceptually new ingredient -- the Hubbard-Stratonovich transform -- to provide an efficient algorithm for sampling from low-rank positive-definite quadratic tilts, i.e. $r(x) = x^\top A x$ where $A$ is positive-definite and of rank $O(1)$. For negative-definite tilts, i.e. $r(x) = - x^\top A x$ where $A$ is positive-definite, we prove that the problem is intractable even if $A$ is of rank 1 (albeit with exponentially-large entries).</li>
<li><strong>摘要：</strong>推理时间算法是一种新兴范例，其中预训练模型用作子例程来解决下游任务。此类算法已被提出用于从反问题和引导图像生成到推理的各种任务。然而，目前在实践中部署的方法都是具有多种故障模式的启发式方法，而我们对何时可以有效改进这些启发式方法知之甚少。在本文中，我们考虑从奖励倾斜扩散模型中采样的任务——即从 $p^{\star}(x) \propto p(x) \exp(r(x))$ 中采样——给定奖励函数 $r$ 和针对 $p$ 的预训练扩散预言。我们对该任务的二次奖励 $r(x) = x^\top A x + b^\top x$ 的计算易处理性进行了细粒度分析。我们证明了线性奖励倾斜总是可以有效地采样——这是一个在文献中似乎被忽视的简单结果。我们使用它作为构建块，以及概念上的新成分 - 哈伯德-斯特拉托诺维奇变换 - 提供一种有效的算法，用于从低秩正定二次倾斜中采样，即 $r(x) = x^\top A x$，其中 $A$ 是正定的并且等级为 $O(1)$。对于负定倾斜，即 $r(x) = - x^\top A x$，其中 $A$ 是正定的，我们证明即使 $A$ 的秩为 1（尽管条目呈指数级大），问题也是棘手的。</li>
</ul>

<h3>Title: MoDE-Boost: Boosting Shared Mobility Demand with Edge-Ready Prediction Models</h3>
<ul>
<li><strong>Authors: </strong>Antonios Tziorvas, George S. Theodoropoulos, Yannis Theodoridis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16573">https://arxiv.org/abs/2602.16573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16573">https://arxiv.org/pdf/2602.16573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16573]] MoDE-Boost: Boosting Shared Mobility Demand with Edge-Ready Prediction Models(https://arxiv.org/abs/2602.16573)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Urban demand forecasting plays a critical role in optimizing routing, dispatching, and congestion management within Intelligent Transportation Systems. By leveraging data fusion and analytics techniques, traffic demand forecasting serves as a key intermediate measure for identifying emerging spatial and temporal demand patterns. In this paper, we tackle this challenge by proposing two gradient boosting model variations, one for classiffication and one for regression, both capable of generating demand forecasts at various temporal horizons, from 5 minutes up to one hour. Our overall approach effectively integrates temporal and contextual features, enabling accurate predictions that are essential for improving the efficiency of shared (micro-) mobility services. To evaluate its effectiveness, we utilize open shared mobility data derived from e-scooter and e-bike networks in five metropolitan areas. These real-world datasets allow us to compare our approach with state-of-the-art methods as well as a Generative AI-based model, demonstrating its effectiveness in capturing the complexities of modern urban mobility. Ultimately, our methodology offers novel insights on urban micro-mobility management, helping to tackle the challenges arising from rapid urbanization and thus, contributing to more sustainable, efficient, and livable cities.</li>
<li><strong>摘要：</strong>城市需求预测在优化智能交通系统中的路线、调度和拥堵管理方面发挥着关键作用。通过利用数据融合和分析技术，交通需求预测可作为识别新兴空间和时间需求模式的关键中间措施。在本文中，我们通过提出两种梯度提升模型变体来应对这一挑战，一种用于分类，一种用于回归，两者都能够在不同时间范围（从 5 分钟到一小时）生成需求预测。我们的整体方法有效地集成了时间和上下文特征，从而实现了准确的预测，这对于提高共享（微）移动服务的效率至关重要。为了评估其有效性，我们利用来自五个大都市区的电动滑板车和电动自行车网络的开放共享移动数据。这些真实世界的数据集使我们能够将我们的方法与最先进的方法以及基于生成人工智能的模型进行比较，证明其在捕捉现代城市交通的复杂性方面的有效性。最终，我们的方法为城市微交通管理提供了新颖的见解，有助于应对快速城市化带来的挑战，从而为建设更加可持续、高效和宜居的城市做出贡献。</li>
</ul>

<h3>Title: Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition</h3>
<ul>
<li><strong>Authors: </strong>Bo Pan, Peter Zhiping Zhang, Hao-Wei Pang, Alex Zhu, Xiang Yu, Liying Zhang, Liang Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16684">https://arxiv.org/abs/2602.16684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16684">https://arxiv.org/pdf/2602.16684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16684]] Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition(https://arxiv.org/abs/2602.16684)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Matched molecular pairs (MMPs) capture the local chemical edits that medicinal chemists routinely use to design analogs, but existing ML approaches either operate at the whole-molecule level with limited edit controllability or learn MMP-style edits from restricted settings and small models. We propose a variable-to-variable formulation of analog generation and train a foundation model on large-scale MMP transformations (MMPTs) to generate diverse variables conditioned on an input variable. To enable practical control, we develop prompting mechanisms that let the users specify preferred transformation patterns during generation. We further introduce MMPT-RAG, a retrieval-augmented framework that uses external reference analogs as contextual guidance to steer generation and generalize from project-specific series. Experiments on general chemical corpora and patent-specific datasets demonstrate improved diversity, novelty, and controllability, and show that our method recovers realistic analog structures in practical discovery scenarios.</li>
<li><strong>摘要：</strong>匹配分子对 (MMP) 捕获药物化学家通常用来设计类似物的局部化学编辑，但现有的 ML 方法要么在编辑可控性有限的全分子水平上运行，要么从受限设置和小模型中学习 MMP 式编辑。我们提出了模拟生成的变量到变量公式，并在大规模 MMP 变换（MMPT）上训练基础模型，以生成以输入变量为条件的各种变量。为了实现实际控制，我们开发了提示机制，让用户在生成过程中指定首选的转换模式。我们进一步介绍了 MMPT-RAG，这是一种检索增强框架，它使用外部参考类似物作为上下文指导来引导特定项目系列的生成和概括。对通用化学语料库和专利特定数据集的实验证明了多样性、新颖性和可控性的提高，并表明我们的方法在实际发现场景中恢复了真实的模拟结构。</li>
</ul>

<h3>Title: Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Mingjia Shi, Yinhan He, Yaochen Zhu, Jundong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.16702">https://arxiv.org/abs/2602.16702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.16702">https://arxiv.org/pdf/2602.16702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.16702]] Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning(https://arxiv.org/abs/2602.16702)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning.</li>
<li><strong>摘要：</strong>视觉语言模型（VLM）旨在通过联合利用视觉和文本模式进行推理。虽然分配额外的推理时间计算已被证明对大型语言模型 (LLM) 有效，但在 VLM 中实现类似的扩展仍然具有挑战性。一个关键障碍是，视觉输入通常仅在生成开始时提供一次，而文本推理（例如，早期视觉摘要）是自回归生成的，导致推理变得越来越以文本为主，并允许早期视觉基础错误累积。此外，推理过程中视觉基础的普通指导通常是粗糙且嘈杂的，使得很难在长文本上进行推理。为了应对这些挑战，我们提出\emph{显着性感知原则}（SAP）选择。 SAP 基于高级推理原则而不是令牌级轨迹运行，这使得能够在噪声反馈下稳定控制离散生成，同时允许后续推理步骤在需要重新基础时重新查阅视觉证据。此外，SAP支持多路由推理，可以并行探索多种推理行为。 SAP 与模型无关且无需数据，无需额外培训。实证结果表明，在可比较的令牌生成预算下，SAP 实现了具有竞争力的性能，特别是在减少对象幻觉方面，同时比 CoT 式长顺序推理产生更稳定的推理和更低的响应延迟。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
