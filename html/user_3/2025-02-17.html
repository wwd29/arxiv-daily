<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-02-17</h1>
<h3>Title: GraphCompNet: A Position-Aware Model for Predicting and Compensating Shape Deviations in 3D Printing</h3>
<ul>
<li><strong>Authors: </strong>Lei (Rachel)Chen, Juheon Lee, Juan Carlos Catana, Tsegai Yhdego, Nathan Moroney, Mohammad Amin Nabian, Hui Wang, Jun Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.09652">https://arxiv.org/abs/2502.09652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.09652">https://arxiv.org/pdf/2502.09652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.09652]] GraphCompNet: A Position-Aware Model for Predicting and Compensating Shape Deviations in 3D Printing(https://arxiv.org/abs/2502.09652)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a data-driven algorithm for modeling and compensating shape deviations in additive manufacturing (AM), addressing challenges in geometric accuracy and batch production. While traditional methods, such as analytical models and metrology, laid the groundwork for geometric precision, they are often impractical for large-scale production. Recent advancements in machine learning (ML) have improved compensation precision, but issues remain in generalizing across complex geometries and adapting to position-dependent variations. We present a novel approach for powder bed fusion (PBF) processes, using GraphCompNet, which is a computational framework combining graph-based neural networks with a generative adversarial network (GAN)-inspired training process. By leveraging point cloud data and dynamic graph convolutional neural networks (DGCNNs), GraphCompNet models complex shapes and incorporates position-specific thermal and mechanical factors. A two-stage adversarial training procedure iteratively refines compensated designs via a compensator-predictor architecture, offering real-time feedback and optimization. Experimental validation across diverse shapes and positions shows the framework significantly improves compensation accuracy (35 to 65 percent) across the entire print space, adapting to position-dependent variations. This work advances the development of Digital Twin technology for AM, enabling scalable, real-time monitoring and compensation, and addressing critical gaps in AM process control. The proposed method supports high-precision, automated industrial-scale design and manufacturing systems.</li>
<li><strong>摘要：</strong>本文介绍了一种数据驱动的算法，用于建模和补偿添加剂制造（AM）的形状偏差，从而解决了几何精度和批处理生产的挑战。虽然传统方法（例如分析模型和计量学）为几何精确度奠定了基础，但它们通常对于大规模生产而言是不切实际的。机器学习（ML）的最新进展提高了补偿精度，但是问题仍然是跨越复杂的几何形状并适应依赖位置的变化的问题。我们使用GraphCompnet提出了一种用于粉末床融合（PBF）过程的新方法，该方法是将基于图形的神经网络与生成性对抗网络（GAN）启发的训练过程相结合的计算框架。通过利用点云数据和动态图卷积神经网络（DGCNN），GraphCompnet模型复杂形状并结合了特定位置的热和机械因素。两阶段的对抗训练程序迭代地通过补偿器预言架构进行了补偿设计，从而提供了实时反馈和优化。跨不同形状和位置的实验验证显示，框架在整个打印空间中显着提高了补偿准确性（35％至65％），可适应依赖位置的变化。这项工作为AM的数字双技术开发，实现了可扩展，实时监控和补偿，并解决了AM过程控制中的关键差距。所提出的方法支持高精度，自动化的工业规模设计和制造系统。</li>
</ul>

<h3>Title: Image Super-Resolution with Guarantees via Conformal Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Eduardo Adame, Daniel Csillag, Guilherme Tegoni Goedert</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.09664">https://arxiv.org/abs/2502.09664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.09664">https://arxiv.org/pdf/2502.09664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.09664]] Image Super-Resolution with Guarantees via Conformal Generative Models(https://arxiv.org/abs/2502.09664)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>The increasing use of generative ML foundation models for image super-resolution calls for robust and interpretable uncertainty quantification methods. We address this need by presenting a novel approach based on conformal prediction techniques to create a "confidence mask" capable of reliably and intuitively communicating where the generated image can be trusted. Our method is adaptable to any black-box generative model, including those locked behind an opaque API, requires only easily attainable data for calibration, and is highly customizable via the choice of a local image similarity metric. We prove strong theoretical guarantees for our method that span fidelity error control (according to our local image similarity metric), reconstruction quality, and robustness in the face of data leakage. Finally, we empirically evaluate these results and establish our method's solid performance.</li>
<li><strong>摘要：</strong>对于图像超分辨率的生成ML基础模型的越来越多地要求使用鲁棒和可解释的不确定性量化方法。我们通过提出一种基于共形预测技术的新方法来解决这种需求，以创建一个“信心面具”，能够可靠，直觉地传达可以信任生成的图像的地方。我们的方法适用于任何黑盒生成模型，包括锁定不透明API的模型，仅需要易于达到的校准数据，并且可以通过选择本地图像相似性度量来高度自定义。我们证明了跨越保真度误差控制的方法（根据我们的本地图像相似度度量），重建质量以及面对数据泄漏时的鲁棒性。最后，我们通过经验评估这些结果并确定方法的扎实性能。</li>
</ul>

<h3>Title: Revealing Subtle Phenotypes in Small Microscopy Datasets Using Latent Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Anis Bourou, Biel Castaño Segade, Thomas Boye, Valérie Mezger, Auguste Genovesio</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.09665">https://arxiv.org/abs/2502.09665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.09665">https://arxiv.org/pdf/2502.09665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.09665]] Revealing Subtle Phenotypes in Small Microscopy Datasets Using Latent Diffusion Models(https://arxiv.org/abs/2502.09665)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Identifying subtle phenotypic variations in cellular images is critical for advancing biological research and accelerating drug discovery. These variations are often masked by the inherent cellular heterogeneity, making it challenging to distinguish differences between experimental conditions. Recent advancements in deep generative models have demonstrated significant potential for revealing these nuanced phenotypes through image translation, opening new frontiers in cellular and molecular biology as well as the identification of novel biomarkers. Among these generative models, diffusion models stand out for their ability to produce high-quality, realistic images. However, training diffusion models typically requires large datasets and substantial computational resources, both of which can be limited in biological research. In this work, we propose a novel approach that leverages pre-trained latent diffusion models to uncover subtle phenotypic changes. We validate our approach qualitatively and quantitatively on several small datasets of microscopy images. Our findings reveal that our approach enables effective detection of phenotypic variations, capturing both visually apparent and imperceptible differences. Ultimately, our results highlight the promising potential of this approach for phenotype detection, especially in contexts constrained by limited data and computational capacity.</li>
<li><strong>摘要：</strong>识别细胞图像中细微的表型变化对于推进生物学研究和加速药物发现至关重要。这些变化通常被固有的细胞异质性掩盖，从而使区分实验条件之间的差异具有挑战性。深层生成模型的最新进展表明，通过图像翻译，开放细胞和分子生物学的新边界以及新型生物标志物的鉴定，可以揭示这些细微的表型的重要潜力。在这些生成模型中，扩散模型在产生高质量，逼真的图像的能力方面脱颖而出。但是，培训扩散模型通常需要大量的数据集和大量的计算资源，这两者在生物学研究中都可能受到限制。在这项工作中，我们提出了一种新型方法，该方法利用预先训练的潜扩散模型来揭示微妙的表型变化。我们在微观图像的几个小数据集上对我们的方法进行定性和定量验证。我们的发现表明，我们的方法可以有效地检测表型变化，从而捕获视觉上明显和不可察觉的差异。最终，我们的结果突出了这种方法对表型检测的有希望的潜力，尤其是在受到有限的数据和计算能力约束的上下文中。</li>
</ul>

<h3>Title: Towards Virtual Clinical Trials of Radiology AI with Conditional Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Benjamin D. Killeen, Bohua Wan, Aditya V. Kulkarni, Nathan Drenkow, Michael Oberst, Paul H. Yi, Mathias Unberath</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.09688">https://arxiv.org/abs/2502.09688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.09688">https://arxiv.org/pdf/2502.09688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.09688]] Towards Virtual Clinical Trials of Radiology AI with Conditional Generative Modeling(https://arxiv.org/abs/2502.09688)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) is poised to transform healthcare by enabling personalized and efficient care through data-driven insights. Although radiology is at the forefront of AI adoption, in practice, the potential of AI models is often overshadowed by severe failures to generalize: AI models can have performance degradation of up to 20% when transitioning from controlled test environments to clinical use by radiologists. This mismatch raises concerns that radiologists will be misled by incorrect AI predictions in practice and/or grow to distrust AI, rendering these promising technologies practically ineffectual. Exhaustive clinical trials of AI models on abundant and diverse data is thus critical to anticipate AI model degradation when encountering varied data samples. Achieving these goals, however, is challenging due to the high costs of collecting diverse data samples and corresponding annotations. To overcome these limitations, we introduce a novel conditional generative AI model designed for virtual clinical trials (VCTs) of radiology AI, capable of realistically synthesizing full-body CT images of patients with specified attributes. By learning the joint distribution of images and anatomical structures, our model enables precise replication of real-world patient populations with unprecedented detail at this scale. We demonstrate meaningful evaluation of radiology AI models through VCTs powered by our synthetic CT study populations, revealing model degradation and facilitating algorithmic auditing for bias-inducing data attributes. Our generative AI approach to VCTs is a promising avenue towards a scalable solution to assess model robustness, mitigate biases, and safeguard patient care by enabling simpler testing and evaluation of AI models in any desired range of diverse patient populations.</li>
<li><strong>摘要：</strong>人工智能（AI）有望通过数据驱动的见解来实现个性化和有效的护理来改变医疗保健。尽管放射学是AI采用的最前沿，但实际上，AI模型的潜力通常被严重失败概括而掩盖：当从受控的测试环境转变为放射科医生的临床使用时，AI模型可以使高达20％的性能降解。这种不匹配引起了人们的担忧，即放射科医生会因实践中的AI预测和/或成长为不信任的AI而被误导，从而使这些有前途的技术实际上是无效的。因此，在遇到各种数据样本时，对AI模型的详尽临床试验对于预测AI模型降解至关重要。但是，由于收集各种数据样本和相应的注释的高成本，实现这些目标是具有挑战性的。为了克服这些局限性，我们引入了一种针对放射学AI虚拟临床试验（VCT）设计的新型有条件生成的AI模型，该模型能够实际合成具有指定属性的患者的全身CT图像。通过学习图像和解剖结构的联合分布，我们的模型可以在此规模上以前所未有的细节来精确复制现实世界中的患者人群。我们通过我们的合成CT研究人群提供动力的VCT来证明对放射学AI模型的有意义评估，揭示了模型降级并促进算法审核，以诱导偏见的数据属性。我们对VCTS的生成AI方法是一种有希望的途径，可扩展解决方案，以评估模型的稳健性，减轻偏见和保护患者护理，从而在任何所需的多种患者人群中对AI模型进行更简单的测试和评估。</li>
</ul>

<h3>Title: Non-Markovian Discrete Diffusion with Causal Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yangtian Zhang, Sizhuang He, Daniel Levine, Lawrence Zhao, David Zhang, Syed A Rizvi, Emanuele Zappala, Rex Ying, David van Dijk</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.09767">https://arxiv.org/abs/2502.09767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.09767">https://arxiv.org/pdf/2502.09767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.09767]] Non-Markovian Discrete Diffusion with Causal Language Models(https://arxiv.org/abs/2502.09767)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models have emerged as a flexible and controllable paradigm for structured sequence modeling, yet they still lag behind causal language models in expressiveness. To bridge the gap between two paradigms, we introduce CaDDi, a causal discrete diffusion model that unifies sequential and temporal modeling within a non-Markovian diffusion framework. Unlike conventional diffusion models that operate step by step with no access to prior states, CaDDi integrates the temporal trajectory, enabling more expressive and controllable generation. Our approach also treats causal language models as a special case, allowing seamless adoption of pretrained large language models (LLMs) for discrete diffusion without the need for architectural modifications. Empirically, we demonstrate that CaDDi outperforms state-of-the-art discrete diffusion models on both natural language and biological sequence tasks, narrowing the gap between diffusion-based methods and large-scale autoregressive transformers.</li>
<li><strong>摘要：</strong>离散扩散模型已成为用于结构化序列建模的灵活且可控的范式，但它们仍然落后于因果语言模型。为了弥合两个范式之间的间隙，我们引入了Caddi，这是一种因果离散扩散模型，该模型在非马克维亚扩散框架内统一了顺序和时间建模。与传统的扩散模型逐步运行，无法访问先前状态，CADDI集成了时间轨迹，从而实现了更具表现力和可控制的生成。我们的方法还将因果语言模型视为一种特殊情况，从而无缝地采用了验证的大型语言模型（LLMS）进行离散扩散而无需进行建筑修改。从经验上讲，我们证明，在自然语言和生物序列任务上，卡迪的表现优于最先进的离散扩散模型，从而缩小了基于扩散的方法和大规模自动回应变压器之间的差异。</li>
</ul>

<h3>Title: Noise Controlled CT Super-Resolution with Conditional Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Yuang Wang, Siyeop Yoon, Rui Hu, Baihui Yu, Duhgoon Lee, Rajiv Gupta, Li Zhang, Zhiqiang Chen, Dufan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.09793">https://arxiv.org/abs/2502.09793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.09793">https://arxiv.org/pdf/2502.09793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.09793]] Noise Controlled CT Super-Resolution with Conditional Diffusion Model(https://arxiv.org/abs/2502.09793)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Improving the spatial resolution of CT images is a meaningful yet challenging task, often accompanied by the issue of noise amplification. This article introduces an innovative framework for noise-controlled CT super-resolution utilizing the conditional diffusion model. The model is trained on hybrid datasets, combining noise-matched simulation data with segmented details from real data. Experimental results with real CT images validate the effectiveness of our proposed framework, showing its potential for practical applications in CT imaging.</li>
<li><strong>摘要：</strong>改善CT图像的空间分辨率是一项有意义但具有挑战性的任务，通常伴随着噪声放大问题。本文介绍了利用条件扩散模型的噪声控制CT超分辨率的创新框架。该模型是在混合数据集上训练的，将噪声匹配的仿真数据与实际数据分割的细节相结合。实际CT图像的实验结果验证了我们提出的框架的有效性，显示了其在CT成像中实际应用的潜力。</li>
</ul>

<h3>Title: Face Deepfakes - A Comprehensive Review</h3>
<ul>
<li><strong>Authors: </strong>Tharindu Fernando, Darshana Priyasad, Sridha Sridharan, Arun Ross, Clinton Fookes</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.09812">https://arxiv.org/abs/2502.09812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.09812">https://arxiv.org/pdf/2502.09812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.09812]] Face Deepfakes - A Comprehensive Review(https://arxiv.org/abs/2502.09812)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In recent years, remarkable advancements in deep- fake generation technology have led to unprecedented leaps in its realism and capabilities. Despite these advances, we observe a notable lack of structured and deep analysis deepfake technology. The principal aim of this survey is to contribute a thorough theoretical analysis of state-of-the-art face deepfake generation and detection methods. Furthermore, we provide a coherent and systematic evaluation of the implications of deepfakes on face biometric recognition approaches. In addition, we outline key applications of face deepfake technology, elucidating both positive and negative applications of the technology, provide a detailed discussion regarding the gaps in existing research, and propose key research directions for further investigation.</li>
<li><strong>摘要：</strong>近年来，深假一代技术的显着进步导致其现实主义和能力前所未有。尽管取得了这些进步，但我们观察到显着缺乏结构化和深层分析的深层技术。这项调查的主要目的是对最先进的面部涂料产生和检测方法进行彻底的理论分析。此外，我们对深层摄影对面部生物识别识别方法的含义提供了连贯而系统的评估。此外，我们概述了面部深冰技术的关键应用，阐明了该技术的正面和负面应用，提供了有关现有研究差距的详细讨论，并提出了关键的研究方向以进行进一步研究。</li>
</ul>

<h3>Title: A Solver-Aided Hierarchical Language for LLM-Driven CAD Design</h3>
<ul>
<li><strong>Authors: </strong>Benjamin T. Jones, Felix Hähnlein, Zihan Zhang, Maaz Ahmad, Vladimir Kim, Adriana Schulz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.09819">https://arxiv.org/abs/2502.09819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.09819">https://arxiv.org/pdf/2502.09819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.09819]] A Solver-Aided Hierarchical Language for LLM-Driven CAD Design(https://arxiv.org/abs/2502.09819)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been enormously successful in solving a wide variety of structured and unstructured generative tasks, but they struggle to generate procedural geometry in Computer Aided Design (CAD). These difficulties arise from an inability to do spatial reasoning and the necessity to guide a model through complex, long range planning to generate complex geometry. We enable generative CAD Design with LLMs through the introduction of a solver-aided, hierarchical domain specific language (DSL) called AIDL, which offloads the spatial reasoning requirements to a geometric constraint solver. Additionally, we show that in the few-shot regime, AIDL outperforms even a language with in-training data (OpenSCAD), both in terms of generating visual results closer to the prompt and creating objects that are easier to post-process and reason about.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在解决各种结构化和非结构化生成任务方面取得了巨大成功，但它们很难在计算机辅助设计（CAD）中生成程序几何形状。这些困难源于无法进行空间推理以及通过复杂的远距离计划引导模型以产生复杂的几何形状的必要性。我们通过引入称为AIDL的求解器，分层特定语言（DSL）的生成CAD设计，该设计将空间推理要求卸载到几何约束求解器上。此外，我们表明，在几次镜头方面，艾滋病甚至优于具有训练数据的语言（OPENSCAD），这是在更接近提示的视觉结果和创建对象方面更易于后处理和理由的对象。 。</li>
</ul>

<h3>Title: HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, Siliang Tang, Jun Xiao, Hui Lin, Yueting Zhuang, Beng Chin Ooi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.09838">https://arxiv.org/abs/2502.09838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.09838">https://arxiv.org/pdf/2502.09838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.09838]] HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation(https://arxiv.org/abs/2502.09838)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs). This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a tailored hierarchical visual perception approach and a three-stage learning strategy. To effectively learn the HealthGPT, we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks. Our project can be accessed at this https URL.</li>
<li><strong>摘要：</strong>我们提出了HealthGpt，这是一种强大的医学大型视觉模型（MED-LVLM），该模型将医学视觉理解和发电能力集成到统一自动回归范式中。我们的引导理念是逐步将异质的理解和发电知识适应预先训练的大语言模型（LLMS）。这是通过一种新型的异质低级适应（H-Lora）技术来实现的，该技术通过量身定制的层次视觉感知方法和三阶段的学习策略来补充。为了有效地学习HealthGPT，我们设计了一个全面的医学领域的理解和发电数据集，称为VL-Health。实验结果表明，医学视觉统一任务中HealthGPT的出色性能和可伸缩性。可以在此HTTPS URL访问我们的项目。</li>
</ul>

<h3>Title: Automated Hypothesis Validation with Agentic Sequential Falsifications</h3>
<ul>
<li><strong>Authors: </strong>Kexin Huang, Ying Jin, Ryan Li, Michael Y. Li, Emmanuel Candès, Jure Leskovec</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.09858">https://arxiv.org/abs/2502.09858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.09858">https://arxiv.org/pdf/2502.09858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.09858]] Automated Hypothesis Validation with Agentic Sequential Falsifications(https://arxiv.org/abs/2502.09858)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Hypotheses are central to information acquisition, decision-making, and discovery. However, many real-world hypotheses are abstract, high-level statements that are difficult to validate directly. This challenge is further intensified by the rise of hypothesis generation from Large Language Models (LLMs), which are prone to hallucination and produce hypotheses in volumes that make manual validation impractical. Here we propose Popper, an agentic framework for rigorous automated validation of free-form hypotheses. Guided by Karl Popper's principle of falsification, Popper validates a hypothesis using LLM agents that design and execute falsification experiments targeting its measurable implications. A novel sequential testing framework ensures strict Type-I error control while actively gathering evidence from diverse observations, whether drawn from existing data or newly conducted procedures. We demonstrate Popper on six domains including biology, economics, and sociology. Popper delivers robust error control, high power, and scalability. Furthermore, compared to human scientists, Popper achieved comparable performance in validating complex biological hypotheses while reducing time by 10 folds, providing a scalable, rigorous solution for hypothesis validation.</li>
<li><strong>摘要：</strong>假设对于信息获取，决策和发现至关重要。但是，许多现实世界中的假设是抽象的高级陈述，难以直接验证。大型语言模型（LLMS）的假设产生的兴起进一步加剧了这一挑战，这些模型容易产生幻觉，并在体积中产生假设，从而使手动验证不切实际。在这里，我们提出了Popper，这是一种对自动形式假设进行严格自动验证的代理框架。在Karl Popper的伪造原则的指导下，Popper使用LLM代理验证了一个假设，该假设设计和执行针对其可测量含义的伪造实验。一个新颖的顺序测试框架可确保严格的I型错误控制，同时积极地从不同的观察结果中收集证据，无论是从现有数据还是新执行的程序中得出的。我们在包括生物学，经济学和社会学在内的六个领域展示了Popper。 Popper提供可靠的错误控制，高功率和可扩展性。此外，与人类科学家相比，Popper在验证复杂的生物学假设的同时，在将时间减少10倍方面取得了可比的性能，为假设验证提供了可扩展，严格的解决方案。</li>
</ul>

<h3>Title: Compression-Aware One-Step Diffusion Model for JPEG Artifact Removal</h3>
<ul>
<li><strong>Authors: </strong>Jinpei Guo, Zheng Chen, Wenbo Li, Yong Guo, Yulun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.09873">https://arxiv.org/abs/2502.09873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.09873">https://arxiv.org/pdf/2502.09873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.09873]] Compression-Aware One-Step Diffusion Model for JPEG Artifact Removal(https://arxiv.org/abs/2502.09873)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated remarkable success in image restoration tasks. However, their multi-step denoising process introduces significant computational overhead, limiting their practical deployment. Furthermore, existing methods struggle to effectively remove severe JPEG artifact, especially in highly compressed images. To address these challenges, we propose CODiff, a compression-aware one-step diffusion model for JPEG artifact removal. The core of CODiff is the compression-aware visual embedder (CaVE), which extracts and leverages JPEG compression priors to guide the diffusion model. We propose a dual learning strategy that combines explicit and implicit learning. Specifically, explicit learning enforces a quality prediction objective to differentiate low-quality images with different compression levels. Implicit learning employs a reconstruction objective that enhances the model's generalization. This dual learning allows for a deeper and more comprehensive understanding of JPEG compression. Experimental results demonstrate that CODiff surpasses recent leading methods in both quantitative and visual quality metrics. The code and models will be released at this https URL.</li>
<li><strong>摘要：</strong>扩散模型在图像恢复任务中表现出了显着的成功。但是，他们的多步降解过程引入了大量的计算开销，从而限制了它们的实际部署。此外，现有方法难以有效地去除严重的JPEG伪像，尤其是在高度压缩的图像中。为了应对这些挑战，我们提出了Codiff，这是一种用于JPEG伪像的压缩意识的一步扩散模型。 Codiff的核心是压缩感知的视觉嵌入器（Cave），它提取并利用JPEG压缩先验来指导扩散模型。我们提出了一种双重学习策略，将明确和隐性学习结合在一起。具体而言，明确的学习实施了质量预测目标，以区分具有不同压缩水平的低质量图像。隐式学习采用一个重建目标，可以增强模型的概括。这种双重学习可以使对JPEG压缩有更深入，更全面的了解。实验结果表明，Codiff超过了定量和视觉质量指标的最新领先方法。代码和模型将在此HTTPS URL上发布。</li>
</ul>

<h3>Title: Symmetry-Preserving Diffusion Models via Target Symmetrization</h3>
<ul>
<li><strong>Authors: </strong>Vinh Tong, Yun Ye, Trung-Dung Hoang, Anji Liu, Guy Van den Broeck, Mathias Niepert</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.09890">https://arxiv.org/abs/2502.09890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.09890">https://arxiv.org/pdf/2502.09890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.09890]] Symmetry-Preserving Diffusion Models via Target Symmetrization(https://arxiv.org/abs/2502.09890)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are powerful tools for capturing complex distributions, but modeling data with inherent symmetries, such as molecular structures, remains challenging. Equivariant denoisers are commonly used to address this, but they introduce architectural complexity and optimization challenges, including noisy gradients and convergence issues. We propose a novel approach that enforces equivariance through a symmetrized loss function, which applies a time-dependent weighted averaging operation over group actions to the model's prediction target. This ensures equivariance without explicit architectural constraints and reduces gradient variance, leading to more stable and efficient optimization. Our method uses Monte Carlo sampling to estimate the average, incurring minimal computational overhead. We provide theoretical guarantees of equivariance for the minimizer of our loss function and demonstrate its effectiveness on synthetic datasets and the molecular conformation generation task using the GEOM-QM9 dataset. Experiments show improved sample quality compared to existing methods, highlighting the potential of our approach to enhance the scalability and practicality of equivariant diffusion models in generative tasks.</li>
<li><strong>摘要：</strong>扩散模型是捕获复杂分布的强大工具，但是用固有的对称性（例如分子结构）建模数据仍然具有挑战性。 doivariant denoiser通常用于解决此问题，但他们引入了建筑复杂性和优化挑战，包括嘈杂的梯度和融合问题。我们提出了一种新颖的方法，该方法通过对称的损失函数来实现肩variance，该函数将对小组动作的时间依赖性加权平均操作应用于模型的预测目标。这样可以确保无明确的体系结构约束并减少梯度差异，从而实现更稳定和有效的优化。我们的方法使用蒙特卡洛抽样来估计平均值，从而产生最小的计算开销。我们为我们的损失函数的最小化提供了模棱两可的理论保证，并使用GEOM-QM9数据集证明了其对合成数据集和分子构象生成任务的有效性。与现有方法相比，实验表明样品质量的提高，强调了我们方法在生成任务中增强模棱两可扩散模型的可扩展性和实用性的潜力。</li>
</ul>

<h3>Title: Insect-Foundation: A Foundation Model and Large Multimodal Dataset for Vision-Language Insect Understanding</h3>
<ul>
<li><strong>Authors: </strong>Thanh-Dat Truong, Hoang-Quan Nguyen, Xuan-Bac Nguyen, Ashley Dowling, Xin Li, Khoa Luu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.09906">https://arxiv.org/abs/2502.09906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.09906">https://arxiv.org/pdf/2502.09906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.09906]] Insect-Foundation: A Foundation Model and Large Multimodal Dataset for Vision-Language Insect Understanding(https://arxiv.org/abs/2502.09906)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multimodal conversational generative AI has shown impressive capabilities in various vision and language understanding through learning massive text-image data. However, current conversational models still lack knowledge about visual insects since they are often trained on the general knowledge of vision-language data. Meanwhile, understanding insects is a fundamental problem in precision agriculture, helping to promote sustainable development in agriculture. Therefore, this paper proposes a novel multimodal conversational model, Insect-LLaVA, to promote visual understanding in insect-domain knowledge. In particular, we first introduce a new large-scale Multimodal Insect Dataset with Visual Insect Instruction Data that enables the capability of learning the multimodal foundation models. Our proposed dataset enables conversational models to comprehend the visual and semantic features of the insects. Second, we propose a new Insect-LLaVA model, a new general Large Language and Vision Assistant in Visual Insect Understanding. Then, to enhance the capability of learning insect features, we develop an Insect Foundation Model by introducing a new micro-feature self-supervised learning with a Patch-wise Relevant Attention mechanism to capture the subtle differences among insect images. We also present Description Consistency loss to improve micro-feature learning via text descriptions. The experimental results evaluated on our new Visual Insect Question Answering benchmarks illustrate the effective performance of our proposed approach in visual insect understanding and achieve State-of-the-Art performance on standard benchmarks of insect-related tasks.</li>
<li><strong>摘要：</strong>多模式对话生成的AI通过学习大量文本图像数据在各种视野和语言理解中表现出了令人印象深刻的能力。但是，当前的对话模型仍然缺乏有关视觉昆虫的知识，因为它们经常受到视觉数据的一般知识的训练。同时，了解昆虫是精确农业中的一个基本问题，有助于促进农业的可持续发展。因此，本文提出了一种新型的多模式对话模型，昆虫式的模型，以促进昆虫域知识中的视觉理解。特别是，我们首先引入了一个新的大型多模式昆虫数据集，并具有视觉昆虫指导数据，该数据能够学习多模式基础模型。我们提出的数据集使对话模型能够理解昆虫的视觉和语义特征。其次，我们提出了一种新的昆虫式模型，这是一种新的通用大型语言和视觉昆虫理解的助手。然后，为了增强学习昆虫特征的能力，我们通过引入一种新的微功能自我监督学习的学习来开发昆虫基础模型，并通过贴片相关的注意机制来捕获昆虫图像之间的微妙差异。我们还提出了描述一致性损失，以通过文本描述改善微功能学习。对我们新的视觉昆虫问题回答基准测试的实验结果进行了评估，这说明了我们提出的视觉昆虫理解方法的有效表现，并在与昆虫相关的任务的标准基准上实现最先进的表现。</li>
</ul>

<h3>Title: AffectSRNet : Facial Emotion-Aware Super-Resolution Network</h3>
<ul>
<li><strong>Authors: </strong>Syed Sameen Ahmad Rizvi, Soham Kumar, Aryan Seth, Pratik Narang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.09932">https://arxiv.org/abs/2502.09932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.09932">https://arxiv.org/pdf/2502.09932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.09932]] AffectSRNet : Facial Emotion-Aware Super-Resolution Network(https://arxiv.org/abs/2502.09932)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Facial expression recognition (FER) systems in low-resolution settings face significant challenges in accurately identifying expressions due to the loss of fine-grained facial details. This limitation is especially problematic for applications like surveillance and mobile communications, where low image resolution is common and can compromise recognition accuracy. Traditional single-image face super-resolution (FSR) techniques, however, often fail to preserve the emotional intent of expressions, introducing distortions that obscure the original affective content. Given the inherently ill-posed nature of single-image super-resolution, a targeted approach is required to balance image quality enhancement with emotion retention. In this paper, we propose AffectSRNet, a novel emotion-aware super-resolution framework that reconstructs high-quality facial images from low-resolution inputs while maintaining the intensity and fidelity of facial expressions. Our method effectively bridges the gap between image resolution and expression accuracy by employing an expression-preserving loss function, specifically tailored for FER applications. Additionally, we introduce a new metric to assess emotion preservation in super-resolved images, providing a more nuanced evaluation of FER system performance in low-resolution scenarios. Experimental results on standard datasets, including CelebA, FFHQ, and Helen, demonstrate that AffectSRNet outperforms existing FSR approaches in both visual quality and emotion fidelity, highlighting its potential for integration into practical FER applications. This work not only improves image clarity but also ensures that emotion-driven applications retain their core functionality in suboptimal resolution environments, paving the way for broader adoption in FER systems.</li>
<li><strong>摘要：</strong>低分辨率设置中的面部表达识别系统（FER）系统在由于缺失细粒度的面部细节而准确地识别表达方面面临重大挑战。对于监视和移动通信等应用程序，这种限制尤其有问题，在监视和移动通信中，低图像分辨率很常见并且可能损害识别精度。但是，传统的单像面临超分辨率（FSR）技术，但是通常无法保留表达的情感意图，引入了掩盖原始情感内容的扭曲。鉴于单位超级分辨率的固有性质本质上的性质，需要采取针对性的方法来平衡图像质量的增强与情绪保留。在本文中，我们提出了一种新型的情感感知的超分辨率框架，该框架从低分辨率输入中重建高质量的面部图像，同时保持面部表情的强度和忠诚度。我们的方法通过采用表达式保护损耗函数，特别是针对FER应用定制的，有效地弥合了图像分辨率和表达精度之间的差距。此外，我们引入了一个新的指标，以评估超级分辨图像中的情绪保存，从而在低分辨率方案中对FER系统性能进行了更细微的评估。包括Celeba，FFHQ和Helen在内的标准数据集的实验结果表明，影响SRNET在视觉质量和情感忠诚中都优于现有的FSR方法，从而突出了其将其整合到实用的FER应用中的潜力。这项工作不仅提高了图像的清晰度，而且还确保了情绪驱动的应用程序在次优的分辨率环境中保留其核心功能，从而为在FER系统中更广泛的采用铺平了道路。</li>
</ul>

<h3>Title: Precise Parameter Localization for Textual Generation in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Łukasz Staniszewski, Bartosz Cywiński, Franziska Boenisch, Kamil Deja, Adam Dziedzic</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.09935">https://arxiv.org/abs/2502.09935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.09935">https://arxiv.org/pdf/2502.09935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.09935]] Precise Parameter Localization for Textual Generation in Diffusion Models(https://arxiv.org/abs/2502.09935)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Novel diffusion models can synthesize photo-realistic images with integrated high-quality text. Surprisingly, we demonstrate through attention activation patching that only less than 1% of diffusion models' parameters, all contained in attention layers, influence the generation of textual content within the images. Building on this observation, we improve textual generation efficiency and performance by targeting cross and joint attention layers of diffusion models. We introduce several applications that benefit from localizing the layers responsible for textual content generation. We first show that a LoRA-based fine-tuning solely of the localized layers enhances, even more, the general text-generation capabilities of large diffusion models while preserving the quality and diversity of the diffusion models' generations. Then, we demonstrate how we can use the localized layers to edit textual content in generated images. Finally, we extend this idea to the practical use case of preventing the generation of toxic text in a cost-free manner. In contrast to prior work, our localization approach is broadly applicable across various diffusion model architectures, including U-Net (e.g., LDM and SDXL) and transformer-based (e.g., DeepFloyd IF and Stable Diffusion 3), utilizing diverse text encoders (e.g., from CLIP to the large language models like T5). Project page available at this https URL.</li>
<li><strong>摘要：</strong>新颖的扩散模型可以通过集成的高质量文本合成光真实的图像。出乎意料的是，我们通过注意激活修补的证明，只有不到1％的扩散模型参数包含在注意层中，影响了图像中文本内容的产生。在这一观察结果的基础上，我们通过针对扩散模型的交叉和关注层来提高文本产生效率和性能。我们介绍了几个应用程序，这些应用程序可以从本地化负责文本内容生成的层次进行局部。我们首先表明，基于洛拉的局部层的微调可以增强大型扩散模型的一般文本生成能力，同时保留了扩散模型的世代的质量和多样性。然后，我们演示了如何使用本地化层在生成的图像中编辑文本内容。最后，我们将这个想法扩展到了以不可成本的方式防止有毒文本产生的实际用例。与先前的工作相反，我们的本地化方法广泛适用于各种扩散模型体系结构，包括U-NET（例如LDM和SDXL）和基于变压器（例如，使用多样的文本编码器（例如，从剪辑到大型语言模型，例如T5）。项目页面可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Generating on Generated: An Approach Towards Self-Evolving Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xulu Zhang, Xiaoyong Wei, Jinlin Wu, Jiaxin Wu, Zhaoxiang Zhang, Zhen Lei, Qing Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.09963">https://arxiv.org/abs/2502.09963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.09963">https://arxiv.org/pdf/2502.09963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.09963]] Generating on Generated: An Approach Towards Self-Evolving Diffusion Models(https://arxiv.org/abs/2502.09963)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recursive Self-Improvement (RSI) enables intelligence systems to autonomously refine their capabilities. This paper explores the application of RSI in text-to-image diffusion models, addressing the challenge of training collapse caused by synthetic data. We identify two key factors contributing to this collapse: the lack of perceptual alignment and the accumulation of generative hallucinations. To mitigate these issues, we propose three strategies: (1) a prompt construction and filtering pipeline designed to facilitate the generation of perceptual aligned data, (2) a preference sampling method to identify human-preferred samples and filter out generative hallucinations, and (3) a distribution-based weighting scheme to penalize selected samples with hallucinatory errors. Our extensive experiments validate the effectiveness of these approaches.</li>
<li><strong>摘要：</strong>递归自我完善（RSI）使智能系统能够自主完善其能力。本文探讨了RSI在文本到图像扩散模型中的应用，以解决由合成数据引起的训练崩溃的挑战。我们确定了导致这种崩溃的两个关键因素：缺乏知觉比对和产生幻觉的积累。为了减轻这些问题，我们提出了三种策略：（1）旨在促进感知对齐数据的生成的及时建筑和过滤管道，（2）一种偏好采样方法，以识别人类偏爱的样本并过滤掉生成性幻觉，以及（（ 3）一种基于分配的加权方案，以通过幻觉错误来惩罚选定的样本。我们的广泛实验验证了这些方法的有效性。</li>
</ul>

<h3>Title: ManiTrend: Bridging Future Generation and Action Prediction with 3D Flow for Robotic Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Yuxin He, Qiang Nie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.10028">https://arxiv.org/abs/2502.10028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.10028">https://arxiv.org/pdf/2502.10028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.10028]] ManiTrend: Bridging Future Generation and Action Prediction with 3D Flow for Robotic Manipulation(https://arxiv.org/abs/2502.10028)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Language-conditioned manipulation is a vital but challenging robotic task due to the high-level abstraction of language. To address this, researchers have sought improved goal representations derived from natural language. In this paper, we highlight 3D flow - representing the motion trend of 3D particles within a scene - as an effective bridge between language-based future image generation and fine-grained action prediction. To this end, we develop ManiTrend, a unified framework that models the dynamics of 3D particles, vision observations and manipulation actions with a causal transformer. Within this framework, features for 3D flow prediction serve as additional conditions for future image generation and action prediction, alleviating the complexity of pixel-wise spatiotemporal modeling and providing seamless action guidance. Furthermore, 3D flow can substitute missing or heterogeneous action labels during large-scale pretraining on cross-embodiment demonstrations. Experiments on two comprehensive benchmarks demonstrate that our method achieves state-of-the-art performance with high efficiency. Our code and model checkpoints will be available upon acceptance.</li>
<li><strong>摘要：</strong>由于语言的高级抽象，语言条件的操作是一项至关重要但挑战性的机器人任务。为了解决这个问题，研究人员寻求改进的自然语言的目标表征。在本文中，我们重点介绍了3D流 - 代表场景中3D粒子的运动趋势 - 作为基于语言的未来图像生成和细粒度的动作预测之间的有效桥梁。为此，我们开发了Manitrend，这是一个统一的框架，该框架模拟了3D粒子，视觉观察和通过因果变压器的操纵作用的动力学。在此框架内，3D流预测的功能是未来图像生成和动作预测的其他条件，从而减轻了像素时空建模的复杂性并提供无缝的动作指导。此外，在跨体型示范中进行大规模预处理期间，3D流可以代替缺失或异质作用标签。对两个综合基准的实验表明，我们的方法以高效率实现了最先进的性能。我们的代码和模型检查点将在接受后提供。</li>
</ul>

<h3>Title: RealCam-I2V: Real-World Image-to-Video Generation with Interactive Complex Camera Control</h3>
<ul>
<li><strong>Authors: </strong>Teng Li, Guangcong Zheng, Rui Jiang, Shuigenzhan, Tao Wu, Yehao Lu, Yining Lin, Xi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.10059">https://arxiv.org/abs/2502.10059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.10059">https://arxiv.org/pdf/2502.10059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.10059]] RealCam-I2V: Real-World Image-to-Video Generation with Interactive Complex Camera Control(https://arxiv.org/abs/2502.10059)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in camera-trajectory-guided image-to-video generation offer higher precision and better support for complex camera control compared to text-based approaches. However, they also introduce significant usability challenges, as users often struggle to provide precise camera parameters when working with arbitrary real-world images without knowledge of their depth nor scene scale. To address these real-world application issues, we propose RealCam-I2V, a novel diffusion-based video generation framework that integrates monocular metric depth estimation to establish 3D scene reconstruction in a preprocessing step. During training, the reconstructed 3D scene enables scaling camera parameters from relative to absolute values, ensuring compatibility and scale consistency across diverse real-world images. In inference, RealCam-I2V offers an intuitive interface where users can precisely draw camera trajectories by dragging within the 3D scene. To further enhance precise camera control and scene consistency, we propose scene-constrained noise shaping, which shapes high-level noise and also allows the framework to maintain dynamic, coherent video generation in lower noise stages. RealCam-I2V achieves significant improvements in controllability and video quality on the RealEstate10K and out-of-domain images. We further enables applications like camera-controlled looping video generation and generative frame interpolation. We will release our absolute-scale annotation, codes, and all checkpoints. Please see dynamic results in this https URL.</li>
<li><strong>摘要：</strong>与基于文本的方法相比，摄像头 - 标题引导的图像到视频生成的最新进展为复杂的相机控制提供了更高的精度和更好的支持。但是，他们还引入了重大的可用性挑战，因为用户在使用任意现实世界图像的情况下，在不了解其深度或场景量表的情况下使用任意现实世界的图像时通常很难提供精确的相机参数。为了解决这些实际应用程序问题，我们提出了Realcam-I2V，这是一个基于扩散的新型视频生成框架，该框架集成了单眼度量深度估计，以在预处理步骤中建立3D场景重建。在训练过程中，重建的3D场景可以使摄像机参数从相对值与绝对值相对，从而确保跨不同现实世界图像的兼容性和尺度一致性。在推理中，Realcam-I2V提供了一个直观的接口，用户可以通过在3D场景中拖动来精确地绘制相机轨迹。为了进一步增强精确的摄像头控制和场景的一致性，我们提出了场景约束的噪声塑造，从而塑造了高级噪声，还允许框架在较低的噪声阶段保持动态，连贯的视频生成。 Realcam-I2V在RealEstate10k和室外图像上的可控性和视频质量方面取得了重大改善。我们进一步启用了诸如相机控制的循环视频生成和生成框架插值之类的应用程序。我们将发布我们的绝对规模注释，代码和所有检查点。请参阅此HTTPS URL中的动态结果。</li>
</ul>

<h3>Title: A novel approach to data generation in generative model</h3>
<ul>
<li><strong>Authors: </strong>JaeHong Kim (1), Jaewon Shim (2) ((1) Healthcare, Legal and Policy Center, Graduate school of Law, Korea University, Seoul 02841, Korea, Human-Inspired AI Research, Korea University, Seoul 02841, Korea , (2) Center for 0D Nanofluidics, Institute of Applied Physics, Department of Physics and Astronomy, Seoul National University, Seoul 08826, Korea)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.10092">https://arxiv.org/abs/2502.10092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.10092">https://arxiv.org/pdf/2502.10092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.10092]] A novel approach to data generation in generative model(https://arxiv.org/abs/2502.10092)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Variational Autoencoders (VAEs) and other generative models are widely employed in artificial intelligence to synthesize new data. However, current approaches rely on Euclidean geometric assumptions and statistical approximations that fail to capture the structured and emergent nature of data generation. This paper introduces the Convergent Fusion Paradigm (CFP) theory, a novel geometric framework that redefines data generation by integrating dimensional expansion accompanied by qualitative transformation. By modifying the latent space geometry to interact with emergent high-dimensional structures, CFP theory addresses key challenges such as identifiability issues and unintended artifacts like hallucinations in Large Language Models (LLMs). CFP theory is based on two key conceptual hypotheses that redefine how generative models structure relationships between data and algorithms. Through the lens of CFP theory, we critically examine existing metric-learning approaches. CFP theory advances this perspective by introducing time-reversed metric embeddings and structural convergence mechanisms, leading to a novel geometric approach that better accounts for data generation as a structured epistemic process. Beyond its computational implications, CFP theory provides philosophical insights into the ontological underpinnings of data generation. By offering a systematic framework for high-dimensional learning dynamics, CFP theory contributes to establishing a theoretical foundation for understanding the data-relationship structures in AI. Finally, future research in CFP theory will be led to its implications for fully realizing qualitative transformations, introducing the potential of Hilbert space in generative modeling.</li>
<li><strong>摘要：</strong>各种自动编码器（VAE）和其他生成模型被广泛用于人工智能中，以合成新数据。但是，当前的方法依赖于无法捕获数据生成的结构化和新兴本质的欧几里得几何假设和统计近似。本文介绍了收敛融合范式（CFP）理论，这是一个新型的几何框架，通过整合尺寸扩展并伴随定性变换来重新定义数据生成。通过修改潜在的空间几何形状以与新兴的高维结构相互作用，CFP理论解决了主要挑战，例如可识别性问题和意想不到的文物，例如大语言模型（LLMS）的幻觉。 CFP理论基于两个关键的概念假设，这些假设重新定义了生成模型如何在数据和算法之间结构关系。通过CFP理论的角度，我们批判性地检查了现有的度量学习方法。 CFP理论通过引入时间转移的度量嵌入和结构收敛机制来提高这一观点，从而导致了一种新型的几何方法，该方法可以更好地说明数据生成作为结构化认识论过程。除了其计算含义外，CFP理论还提供了对数据生成本体论基础的哲学见解。通过为高维学习动态提供系统的框架，CFP理论有助于建立理论基础，以理解AI中的数据关联结构。最后，CFP理论中未来的研究将导致其对完全实现定性转变的影响，从而引入了希尔伯特领域在生成建模中的潜力。</li>
</ul>

<h3>Title: NeuroXVocal: Detection and Explanation of Alzheimer's Disease through Non-invasive Analysis of Picture-prompted Speech</h3>
<ul>
<li><strong>Authors: </strong>Nikolaos Ntampakis, Konstantinos Diamantaras, Ioanna Chouvarda, Magda Tsolaki, Vasileios Argyriou, Panagiotis Sarigianndis</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.10108">https://arxiv.org/abs/2502.10108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.10108">https://arxiv.org/pdf/2502.10108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.10108]] NeuroXVocal: Detection and Explanation of Alzheimer's Disease through Non-invasive Analysis of Picture-prompted Speech(https://arxiv.org/abs/2502.10108)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The early diagnosis of Alzheimer's Disease (AD) through non invasive methods remains a significant healthcare challenge. We present NeuroXVocal, a novel dual-component system that not only classifies but also explains potential AD cases through speech analysis. The classification component (Neuro) processes three distinct data streams: acoustic features capturing speech patterns and voice characteristics, textual features extracted from speech transcriptions, and precomputed embeddings representing linguistic patterns. These streams are fused through a custom transformer-based architecture that enables robust cross-modal interactions. The explainability component (XVocal) implements a Retrieval-Augmented Generation (RAG) approach, leveraging Large Language Models combined with a domain-specific knowledge base of AD research literature. This architecture enables XVocal to retrieve relevant clinical studies and research findings to generate evidence-based context-sensitive explanations of the acoustic and linguistic markers identified in patient speech. Using the IS2021 ADReSSo Challenge benchmark dataset, our system achieved state-of-the-art performance with 95.77% accuracy in AD classification, significantly outperforming previous approaches. The explainability component was qualitatively evaluated using a structured questionnaire completed by medical professionals, validating its clinical relevance. NeuroXVocal's unique combination of high-accuracy classification and interpretable, literature-grounded explanations demonstrates its potential as a practical tool for supporting clinical AD diagnosis.</li>
<li><strong>摘要：</strong>通过非侵入性方法对阿尔茨海默氏病（AD）的早期诊断仍然是一个重大的医疗挑战。我们提出了一种新型的双重组件系统NeuroxVocal，不仅可以分类，而且可以通过语音分析来解释潜在的AD病例。分类组件（NEURO）处理三个不同的数据流：声学特征捕获语音模式和语音特征，从语音转录中提取的文本特征以及代表语言模式的预先计算的嵌入式。这些流是通过基于自定义变压器的体系结构融合的，该体系结构可实现强大的跨模式交互。解释性成分（XVocal）实现了检索功能的生成（RAG）方法，利用大型语言模型与广告研究文献的特定领域知识基础相结合。该体系结构使Xvocal能够检索相关的临床研究和研究结果，从而产生基于证据的上下文敏感解释，对患者言语中确定的声学和语言标记。使用IS2021 Adresso Challenge基准数据集，我们的系统在AD分类中获得了95.77％的精度，实现了最先进的性能，并明显优于先前的方法。使用医学专业人员完成的结构化问卷对解释性组成部分进行定性评估，以验证其临床相关性。 Neuroxvocal高准确性分类和可解释的文献基础解释的独特组合证明了其作为支持临床AD诊断的实用工具的潜力。</li>
</ul>

<h3>Title: Leveraging V2X for Collaborative HD Maps Construction Using Scene Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Gamal Elghazaly, Raphael Frank</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.10127">https://arxiv.org/abs/2502.10127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.10127">https://arxiv.org/pdf/2502.10127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.10127]] Leveraging V2X for Collaborative HD Maps Construction Using Scene Graph Generation(https://arxiv.org/abs/2502.10127)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>High-Definition (HD) maps play a crucial role in autonomous vehicle navigation, complementing onboard perception sensors for improved accuracy and safety. Traditional HD map generation relies on dedicated mapping vehicles, which are costly and fail to capture real-time infrastructure changes. This paper presents HDMapLaneNet, a novel framework leveraging V2X communication and Scene Graph Generation to collaboratively construct a localized geometric layer of HD maps. The approach extracts lane centerlines from front-facing camera images, represents them as graphs, and transmits the data for global aggregation to the cloud via V2X. Preliminary results on the nuScenes dataset demonstrate superior association prediction performance compared to a state-of-the-art method.</li>
<li><strong>摘要：</strong>高清（HD）地图在自动驾驶汽车导航中起着至关重要的作用，可以补充机上感知传感器，以提高准确性和安全性。传统的高清图生成依赖于专用的映射车辆，这些映射车是昂贵且无法捕获实时基础架构的变化。本文介绍了HDMAPLANENET，这是一个新颖的框架，利用V2X通信和场景图生成来协作构建HD地图的局部几何层。该方法从前面摄像头图像中提取车道中心线，表示它们为图形，并通过V2X将全局聚合的数据传输到云。 Nuscenes数据集的初步结果表明，与最新方法相比，相比之下。</li>
</ul>

<h3>Title: Shaping Inductive Bias in Diffusion Models through Frequency-Based Noise Control</h3>
<ul>
<li><strong>Authors: </strong>Thomas Jiralerspong, Berton Earnshaw, Jason Hartford, Yoshua Bengio, Luca Scimeca</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.10236">https://arxiv.org/abs/2502.10236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.10236">https://arxiv.org/pdf/2502.10236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.10236]] Shaping Inductive Bias in Diffusion Models through Frequency-Based Noise Control(https://arxiv.org/abs/2502.10236)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion Probabilistic Models (DPMs) are powerful generative models that have achieved unparalleled success in a number of generative tasks. In this work, we aim to build inductive biases into the training and sampling of diffusion models to better accommodate the target distribution of the data to model. For topologically structured data, we devise a frequency-based noising operator to purposefully manipulate, and set, these inductive biases. We first show that appropriate manipulations of the noising forward process can lead DPMs to focus on particular aspects of the distribution to learn. We show that different datasets necessitate different inductive biases, and that appropriate frequency-based noise control induces increased generative performance compared to standard diffusion. Finally, we demonstrate the possibility of ignoring information at particular frequencies while learning. We show this in an image corruption and recovery task, where we train a DPM to recover the original target distribution after severe noise corruption.</li>
<li><strong>摘要：</strong>扩散概率模型（DPM）是强大的生成模型，在许多生成任务中取得了无与伦比的成功。在这项工作中，我们旨在在扩散模型的训练和采样中建立归纳偏见，以更好地适应数据的目标分布。对于拓扑结构化的数据，我们设计了一个基于频率的no级操作员，以故意操纵和设置这些感应性偏见。我们首先表明，适当的操纵尖锐的远期过程可以导致DPM专注于要学习的分布的特定方面。我们表明，不同的数据集需要不同的电感偏见，与标准扩散相比，适当的基于频率的噪声控制会导致生成性能提高。最后，我们证明了在学习时忽略特定频率信息的可能性。我们在图像腐败和恢复任务中显示了这一点，在此，我们在严重的噪声腐败后训练DPM以恢复原始目标分布。</li>
</ul>

<h3>Title: Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, Yuchu Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.10248">https://arxiv.org/abs/2502.10248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.10248">https://arxiv.org/pdf/2502.10248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.10248]] Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model(https://arxiv.org/abs/2502.10248)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality. User prompts are encoded using two bilingual text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames. A video-based DPO approach, Video-DPO, is applied to reduce artifacts and improve the visual quality of the generated videos. We also detail our training strategies and share key observations and insights. Step-Video-T2V's performance is evaluated on a novel video generation benchmark, Step-Video-T2V-Eval, demonstrating its state-of-the-art text-to-video quality when compared with both open-source and commercial engines. Additionally, we discuss the limitations of current diffusion-based model paradigm and outline future directions for video foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval available at this https URL. The online version can be accessed from this https URL as well. Our goal is to accelerate the innovation of video foundation models and empower video content creators.</li>
<li><strong>摘要：</strong>我们提出了Step-Video-T2V，这是一种具有30b参数的最先进的文本对电视预训练的模型，并且能够生成长度204帧的视频。深层压缩变量自动编码器Video-VAE是为视频生成任务而设计的，可实现16x16的空间和8倍的时间压缩比，同时保持出色的视频重建质量。用户提示使用两个双语文本编码器编码以处理英语和中文。使用流量匹配训练了具有3D全心注意的DIT，并被用来将输入噪声转化为潜在的框架。采用基于视频的DPO方法Video-DPO，用于减少工件并提高生成视频的视觉质量。我们还详细介绍了我们的培训策略，并分享关键的观察和见解。 Step-Video-T2V的性能是在新型视频生成基准测试中进行评估的，即Step-Video-T2V-eval，与开源和商用发动机相比，它证明了其最先进的文本对视频质量。此外，我们讨论了当前基于扩散的模型范式的局限性，并概述了视频基础模型的未来指示。我们可以在此HTTPS URL上提供Step-Video-T2V和Step-Video-T2V-Eval。也可以从此HTTPS URL访问在线版本。我们的目标是加速视频基础模型的创新并授权视频内容创建者。</li>
</ul>

<h3>Title: Probabilistic Super-Resolution for High-Fidelity Physical System Simulations with Uncertainty Quantification</h3>
<ul>
<li><strong>Authors: </strong>Pengyu Zhang, Connor Duffin, Alex Glyn-Davies, Arnaud Vadeboncoeur, Mark Girolami</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.10280">https://arxiv.org/abs/2502.10280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.10280">https://arxiv.org/pdf/2502.10280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.10280]] Probabilistic Super-Resolution for High-Fidelity Physical System Simulations with Uncertainty Quantification(https://arxiv.org/abs/2502.10280)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>Super-resolution (SR) is a promising tool for generating high-fidelity simulations of physical systems from low-resolution data, enabling fast and accurate predictions in engineering applications. However, existing deep-learning based SR methods, require large labeled datasets and lack reliable uncertainty quantification (UQ), limiting their applicability in real-world scenarios. To overcome these challenges, we propose a probabilistic SR framework that leverages the Statistical Finite Element Method and energy-based generative modeling. Our method enables efficient high-resolution predictions with inherent UQ, while eliminating the need for extensive labeled datasets. The method is validated on a 2D Poisson example and compared with bicubic interpolation upscaling. Results demonstrate a computational speed-up over high-resolution numerical solvers while providing reliable uncertainty estimates.</li>
<li><strong>摘要：</strong>超分辨率（SR）是一种有前途的工具，可从低分辨率数据中生成物理系统的高保真模拟，从而在工程应用中实现快速准确的预测。但是，现有的基于深度学习的SR方法需要大型标记的数据集，并且缺乏可靠的不确定性量化（UQ），从而限制了其在现实情况下的适用性。为了克服这些挑战，我们提出了一个利用统计有限元方法和基于能量的生成模型的概率SR框架。我们的方法可以通过固有的UQ实现有效的高分辨率预测，同时消除了对广泛标记的数据集的需求。该方法在2D Poisson示例上进行了验证，并将其与双色插值上尺度进行比较。结果证明了高分辨率数值求解器的计算加速，同时提供了可靠的不确定性估计。</li>
</ul>

<h3>Title: ExplainReduce: Summarising local explanations via proxies</h3>
<ul>
<li><strong>Authors: </strong>Lauri Seppäläinen, Mudong Guo, Kai Puolamäki</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.10311">https://arxiv.org/abs/2502.10311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.10311">https://arxiv.org/pdf/2502.10311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.10311]] ExplainReduce: Summarising local explanations via proxies(https://arxiv.org/abs/2502.10311)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Most commonly used non-linear machine learning methods are closed-box models, uninterpretable to humans. The field of explainable artificial intelligence (XAI) aims to develop tools to examine the inner workings of these closed boxes. An often-used model-agnostic approach to XAI involves using simple models as local approximations to produce so-called local explanations; examples of this approach include LIME, SHAP, and SLISEMAP. This paper shows how a large set of local explanations can be reduced to a small "proxy set" of simple models, which can act as a generative global explanation. This reduction procedure, ExplainReduce, can be formulated as an optimisation problem and approximated efficiently using greedy heuristics.</li>
<li><strong>摘要：</strong>最常用的非线性机器学习方法是封闭框模型，对人类无法解释。可解释的人工智能（XAI）的领域旨在开发工具来检查这些封闭框的内部工作。 XAI的一种经常使用的模型无关方法涉及使用简单模型作为局部近似值来产生所谓的局部解释。这种方法的示例包括石灰，外形和slisemap。本文展示了如何将大量局部解释简化为一个简单模型的小“代理”，这些模型可以充当生成性的全球解释。解释说，这种还原程序可以作为优化问题提出，并使用贪婪的启发式方法有效地近似。</li>
</ul>

<h3>Title: Ocular Disease Classification Using CNN with Deep Convolutional Generative Adversarial Network</h3>
<ul>
<li><strong>Authors: </strong>Arun Kunwar, Dibakar Raj Pant, Jukka Heikkonen, Rajeev Kanth</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.10334">https://arxiv.org/abs/2502.10334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.10334">https://arxiv.org/pdf/2502.10334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.10334]] Ocular Disease Classification Using CNN with Deep Convolutional Generative Adversarial Network(https://arxiv.org/abs/2502.10334)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The Convolutional Neural Network (CNN) has shown impressive performance in image classification because of its strong learning capabilities. However, it demands a substantial and balanced dataset for effective training. Otherwise, networks frequently exhibit over fitting and struggle to generalize to new examples. Publicly available dataset of fundus images of ocular disease is insufficient to train any classification model to achieve satisfactory accuracy. So, we propose Generative Adversarial Network(GAN) based data generation technique to synthesize dataset for training CNN based classification model and later use original disease containing ocular images to test the model. During testing the model classification accuracy with the original ocular image, the model achieves an accuracy rate of 78.6% for myopia, 88.6% for glaucoma, and 84.6% for cataract, with an overall classification accuracy of 84.6%.</li>
<li><strong>摘要：</strong>由于其强大的学习能力，卷积神经网络（CNN）在图像分类中表现出了令人印象深刻的表现。但是，它需要一个实质性平衡的数据集来进行有效的培训。否则，网络经常表现出拟合和努力以推广新示例的努力。公开可用的眼部疾病底面图像的数据集不足以训练任何分类模型以达到令人满意的准确性。因此，我们提出了基于生成的对抗网络（GAN）的数据生成技术来合成基于CNN的分类模型的数据集，并随后使用包含眼图像的原始疾病来测试该模型。在使用原始眼图测试模型分类精度时，该模型的近视精度为78.6％，青光眼的精度为88.6％，白内障的精度为84.6％，总体分类准确度为84.6％。</li>
</ul>

<h3>Title: Dimension-free Score Matching and Time Bootstrapping for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Syamantak Kumar, Dheeraj Nagaraj, Purnamrita Sarkar</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.10354">https://arxiv.org/abs/2502.10354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.10354">https://arxiv.org/pdf/2502.10354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.10354]] Dimension-free Score Matching and Time Bootstrapping for Diffusion Models(https://arxiv.org/abs/2502.10354)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models generate samples by estimating the score function of the target distribution at various noise levels. The model is trained using samples drawn from the target distribution, progressively adding noise. In this work, we establish the first (nearly) dimension-free sample complexity bounds for learning these score functions, achieving a double exponential improvement in dimension over prior results. A key aspect of our analysis is the use of a single function approximator to jointly estimate scores across noise levels, a critical feature of diffusion models in practice which enables generalization across timesteps. Our analysis introduces a novel martingale-based error decomposition and sharp variance bounds, enabling efficient learning from dependent data generated by Markov processes, which may be of independent interest. Building on these insights, we propose Bootstrapped Score Matching (BSM), a variance reduction technique that utilizes previously learned scores to improve accuracy at higher noise levels. These results provide crucial insights into the efficiency and effectiveness of diffusion models for generative modeling.</li>
<li><strong>摘要：</strong>扩散模型通过在各种噪声水平下估算目标分布的得分函数来生成样品。使用从目标分布中绘制的样品训练该模型，从而逐渐增加噪声。在这项工作中，我们建立了学习这些分数功能的第一个（几乎）无维的样本复杂性界限，从而实现了比先前结果的双重指数改进。我们分析的一个关键方面是使用单个函数近似值来跨噪声水平共同估计得分，这是扩散模型的关键特征，实践中可以跨时步中进行概括。我们的分析介绍了一种新颖的基于Martingale的错误分解和急剧方差界限，从而从Markov过程产生的依赖数据中有效学习，这可能具有独立的兴趣。在这些见解的基础上，我们提出了自举得分匹配（BSM），这是一种降低差异技术，利用先前学到的分数来提高较高噪声水平的准确性。这些结果为生成建模的扩散模型的效率和有效性提供了至关重要的见解。</li>
</ul>

<h3>Title: AffinityFlow: Guided Flows for Antibody Affinity Maturation</h3>
<ul>
<li><strong>Authors: </strong>Can Chen, Karla-Luise Herpoldt, Chenchao Zhao, Zichen Wang, Marcus Collins, Shang Shang, Ron Benson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.10365">https://arxiv.org/abs/2502.10365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.10365">https://arxiv.org/pdf/2502.10365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.10365]] AffinityFlow: Guided Flows for Antibody Affinity Maturation(https://arxiv.org/abs/2502.10365)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Antibodies are widely used as therapeutics, but their development requires costly affinity maturation, involving iterative mutations to enhance binding this http URL paper explores a sequence-only scenario for affinity maturation, using solely antibody and antigen sequences. Recently AlphaFlow wraps AlphaFold within flow matching to generate diverse protein structures, enabling a sequence-conditioned generative model of structure. Building on this, we propose an alternating optimization framework that (1) fixes the sequence to guide structure generation toward high binding affinity using a structure-based affinity predictor, then (2) applies inverse folding to create sequence mutations, refined by a sequence-based affinity predictor for post selection. To address this, we develop a co-teaching module that incorporates valuable information from noisy biophysical energies into predictor refinement. The sequence-based predictor selects consensus samples to teach the structure-based predictor, and vice versa. Our method, AffinityFlow, achieves state-of-the-art performance in affinity maturation experiments. We plan to open-source our code after acceptance.</li>
<li><strong>摘要：</strong>抗体被广泛用作治疗剂，但是它们的发育需要昂贵的亲和力成熟，涉及迭代突变以增强本HTTP URL论文的结合，探讨了使用独特的抗体和抗原序列探索仅序列的仅序列情景，以实现亲和力的成熟。最近，Alphaflow将Alphafold包裹在流匹配中以生成多种蛋白质结构，从而实现了序列条件的结构模型。在此基础上，我们提出了一个交替的优化框架，该框架（1）使用基于结构的亲和力预测指标来固定序列，以指导结构生成高结合亲和力，然后（2）应用反向折叠以创建序列突变，并通过序列 - 完善基于后选择的基于亲和力预测指标。为了解决这个问题，我们开发了一个共同的教学模块，该模块将嘈杂的生物物理能量中的有价值的信息纳入预测指标。基于序列的预测指标选择共识样本来教授基于结构的预测指标，反之亦然。我们的方法，亲和力，在亲和力成熟实验中实现了最先进的性能。我们计划在接受后打开代码。</li>
</ul>

<h3>Title: Region-Adaptive Sampling for Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Ziming Liu, Yifan Yang, Chengruidong Zhang, Yiqi Zhang, Lili Qiu, Yang You, Yuqing Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.10389">https://arxiv.org/abs/2502.10389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.10389">https://arxiv.org/pdf/2502.10389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.10389]] Region-Adaptive Sampling for Diffusion Transformers(https://arxiv.org/abs/2502.10389)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps or reusing intermediate results, failing to leverage variations across spatial regions within the image due to the constraints of convolutional U-Net structures. By harnessing the flexibility of Diffusion Transformers (DiTs) in handling variable number of tokens, we introduce RAS, a novel, training-free sampling strategy that dynamically assigns different sampling ratios to regions within an image based on the focus of the DiT model. Our key observation is that during each sampling step, the model concentrates on semantically meaningful regions, and these areas of focus exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the regions currently in focus, while other regions are updated using cached noise from the previous step. The model's focus is determined based on the output from the preceding step, capitalizing on the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up to 2.36x and 2.51x, respectively, with minimal degradation in generation quality. Additionally, a user study reveals that RAS delivers comparable qualities under human evaluation while achieving a 1.6x speedup. Our approach makes a significant step towards more efficient diffusion transformers, enhancing their potential for real-time applications.</li>
<li><strong>摘要：</strong>扩散模型（DMS）已成为跨不同领域生成任务的主要选择。但是，它们对多个顺序前向的依赖会显着限制实时性能。先前的加速方法主要集中于减少采样步骤的数量或重复使用中间结果，由于卷积U-NET结构的限制，未能利用图像内空间区域的变化。通过利用在处理可变的令牌数量中的扩散变压器（DIT）的灵活性，我们引入了RAS，RAS是一种新颖的，无训练的采样策略，该策略将基于DIT模型的焦点的图像中的区域动态分配不同的采样比。我们的主要观察结果是，在每个采样步骤中，模型都集中在语义上有意义的区域上，而这些重点领域在连续步骤之间表现出很强的连续性。利用这种见解，RAS仅更新当前焦点的区域，而其他区域则使用上一步中的噪声更新。模型的重点是根据前面步骤的输出确定的，并利用了我们观察到的时间一致性。我们在稳定扩散3和Lumina-Next-T2I上评估RAS，分别达到2.36倍和2.51倍的加速，生成质量的降解最小。此外，一项用户研究表明，在达到1.6倍加速的同时，RAS在人类评估下提供了可比的品质。我们的方法迈出了更有效的扩散变压器迈出的重要一步，增强了其实时应用的潜力。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
