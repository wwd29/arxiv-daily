<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-08-13</h1>
<h3>Title: Benchmarking Large Language Models for Geolocating Colonial Virginia Land Grants</h3>
<ul>
<li><strong>Authors: </strong>Ryan Mioduski</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08266">https://arxiv.org/abs/2508.08266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08266">https://arxiv.org/pdf/2508.08266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08266]] Benchmarking Large Language Models for Geolocating Colonial Virginia Land Grants(https://arxiv.org/abs/2508.08266)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Virginia's seventeenth- and eighteenth-century land patents survive primarily as narrative metes-and-bounds descriptions, limiting spatial analysis. This study systematically evaluates current-generation large language models (LLMs) in converting these prose abstracts into geographically accurate latitude/longitude coordinates within a focused evaluation context. A digitized corpus of 5,471 Virginia patent abstracts (1695-1732) is released, with 43 rigorously verified test cases serving as an initial, geographically focused benchmark. Six OpenAI models across three architectures (o-series, GPT-4-class, and GPT-3.5) were tested under two paradigms: direct-to-coordinate and tool-augmented chain-of-thought invoking external geocoding APIs. Results were compared with a GIS-analyst baseline, the Stanford NER geoparser, Mordecai-3, and a county-centroid heuristic. The top single-call model, o3-2025-04-16, achieved a mean error of 23 km (median 14 km), outperforming the median LLM (37.4 km) by 37.5%, the weakest LLM (50.3 km) by 53.5%, and external baselines by 67% (GIS analyst) and 70% (Stanford NER). A five-call ensemble further reduced errors to 19 km (median 12 km) at minimal additional cost (approx. USD 0.20 per grant), outperforming the median LLM by 48.6%. A patentee-name-redaction ablation increased error by about 9%, indicating reliance on textual landmark and adjacency descriptions rather than memorization. The cost-efficient gpt-4o-2024-08-06 model maintained a 28 km mean error at USD 1.09 per 1,000 grants, establishing a strong cost-accuracy benchmark; external geocoding tools offered no measurable benefit in this evaluation. These findings demonstrate the potential of LLMs for scalable, accurate, and cost-effective historical georeferencing.</li>
<li><strong>摘要：</strong>弗吉尼亚州的十七世纪和18世纪的土地专利主要作为叙事仪表描述，限制了空间分析。这项研究系统地评估了当前生成的大语言模型（LLM），以将这些散文摘要转换为在集中评估环境中的地理准确的纬度/经度坐标。释放了5,471个弗吉尼亚专利摘要（1695-1732）的数字化语料库，其中43个严格验证的测试用例是最初的地理位置重点基准的基准。在两个范式下测试了三个架构（O系列，GPT-4类和GPT-3.5）的六个OpenAI模型：直接到坐标和工具增强型链的链链调用外部地理编码API。将结果与GIS-Analyst基线，Stanford Ner Geoparser，Mordecai-3和县中心式启发式进行了比较。 The top single-call model, o3-2025-04-16, achieved a mean error of 23 km (median 14 km), outperforming the median LLM (37.4 km) by 37.5%, the weakest LLM (50.3 km) by 53.5%, and external baselines by 67% (GIS analyst) and 70% (Stanford NER).一个五号合奏将错误进一步降低至19公里（中位数12公里），额外费用最低（每笔赠款的0.20美元约为0.20美元），表现的中位数LLM的表现可增加48.6％。 Aptentee-Name-Name-Name-Name Recration消融增加了约9％，表明依赖文本地标和邻接描述而不是记忆。成本效益的GPT-4O-2024-08-06型号的平均误差为28公里，每1,000赠款1.09美元，建立了强大的成本准确性基准；外部地理编码工具在此评估中没有可衡量的好处。这些发现证明了LLM对可扩展，准确且具有成本效益的历史地理发行的潜力。</li>
</ul>

<h3>Title: Doctor Sun: A Bilingual Multimodal Large Language Model for Biomedical AI</h3>
<ul>
<li><strong>Authors: </strong>Dong Xue, Ziyao Shao, Zhaoyang Duan, Fangzhou Liu, Bing Li, Zhongheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08270">https://arxiv.org/abs/2508.08270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08270">https://arxiv.org/pdf/2508.08270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08270]] Doctor Sun: A Bilingual Multimodal Large Language Model for Biomedical AI(https://arxiv.org/abs/2508.08270)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Large multimodal models (LMMs) have demonstrated significant potential in providing innovative solutions for various biomedical tasks, including pathology analysis, radiology report generation, and biomedical assistance. However, the existing multimodal biomedical AI is typically based on foundation LLMs, thus hindering the understanding of intricate medical concepts with limited medical training data. Moreover, recent LLaVA-induced medical LMMs struggle to effectively capture the intricate relationship between the texts and the images. Therefore, we introduce Doctor Sun, a large multimodal generative model specialized in medicine, developed to encode, integrate, and interpret diverse biomedical data modalities such as text and images. In particular, Doctor Sun integrates a pre-trained vision encoder with a medical LLM and conducts two-stage training on various medical datasets, focusing on feature alignment and instruction tuning. Moreover, we release SunMed-VL, a wide-range bilingual medical multimodal dataset, along with all associated models, code, and resources, to freely support the advancement of biomedical multimodal research.</li>
<li><strong>摘要：</strong>大型多模型模型（LMM）在为各种生物医学任务（包括病理学分析，放射学报告生成和生物医学援助）提供创新解决方案方面具有巨大潜力。但是，现有的多模式生物医学AI通常基于基础LLM，从而阻碍了使用有限的医学培训数据对复杂的医学概念的理解。此外，最近的LLAVA诱导的医学LMM难以有效捕获文本与图像之间的复杂关系。因此，我们介绍了Sun Doctor，这是一种专门从事医学的大型多模式生成模型，旨在编码，整合和解释各种生物医学数据模式，例如文本和图像。特别是，Sun Doctor将预先训练的视觉编码器与医学LLM相结合，并在各种医疗数据集上进行了两阶段的培训，重点是功能一致性和教学调整。此外，我们发布了Sunmed-VL，这是一种大型双语医学多模式数据集，以及所有相关的模型，代码和资源，以自由支持生物医学多模式研究的进步。</li>
</ul>

<h3>Title: Probabilistic Emissivity Retrieval from Hyperspectral Data via Physics-Guided Variational Inference</h3>
<ul>
<li><strong>Authors: </strong>Joshua R. Tempelman, Kevin Mitchell, Adam J. Wachtor, Eric B. Flynn</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08291">https://arxiv.org/abs/2508.08291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08291">https://arxiv.org/pdf/2508.08291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08291]] Probabilistic Emissivity Retrieval from Hyperspectral Data via Physics-Guided Variational Inference(https://arxiv.org/abs/2508.08291)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent research has proven neural networks to be a powerful tool for performing hyperspectral imaging (HSI) target identification. However, many deep learning frameworks deliver a single material class prediction and operate on a per-pixel basis; such approaches are limited in their interpretability and restricted to predicting materials that are accessible in available training libraries. In this work, we present an inverse modeling approach in the form of a physics-conditioned generative model.A probabilistic latent-variable model learns the underlying distribution of HSI radiance measurements and produces the conditional distribution of the emissivity spectrum. Moreover, estimates of the HSI scene's atmosphere and background are used as a physically relevant conditioning mechanism to contextualize a given radiance measurement during the encoding and decoding processes. Furthermore, we employ an in-the-loop augmentation scheme and physics-based loss criteria to avoid bias towards a predefined training material set and to encourage the model to learn physically consistent inverse mappings. Monte-Carlo sampling of the model's conditioned posterior delivers a sought emissivity distribution and allows for interpretable uncertainty quantification. Moreover, a distribution-based material matching scheme is presented to return a set of likely material matches for an inferred emissivity distribution. Hence, we present a strategy to incorporate contextual information about a given HSI scene, capture the possible variation of underlying material spectra, and provide interpretable probability measures of a candidate material accounting for given remotely-sensed radiance measurement.</li>
<li><strong>摘要：</strong>最近的研究已证明神经网络是执行高光谱成像（HSI）目标识别的强大工具。但是，许多深度学习框架提供了单个物质类别的预测，并以每个像素为基础运行。这种方法的解释性受到限制，并且仅限于预测可用培训库中可访问的材料。在这项工作中，我们以物理条件的生成模型的形式提出了一种反向建模方法。A概率潜在可变性模型了解了HSI辐射测量的潜在分布，并产生了发射光谱的条件分布。此外，对HSI场景的大气和背景的估计用作物理相关的调节机制，可以在编码和解码过程中对给定的辐射测量进行环境化。此外，我们采用了环境内的增强计划和基于物理的损失标准，以避免对预定义的训练材料集有偏见，并鼓励模型学习物理一致的逆映射。该模型条件后验的蒙特卡洛采样可提供寻求的发射率分布，并允许可解释的不确定性定量。此外，提出了基于分布的材料匹配方案，以返回一组可能的材料匹配，以进行推断的发射率分布。因此，我们提出了一种策略，旨在结合有关给定HSI场景的上下文信息，捕获潜在材料光谱的可能变化，并提供可解释的概率测量，以占用给定的远程敏感的辐射度测量的候选材料。</li>
</ul>

<h3>Title: MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling</h3>
<ul>
<li><strong>Authors: </strong>Qian Wang, Ziqi Huang, Ruoxi Jia, Paul Debevec, Ning Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08487">https://arxiv.org/abs/2508.08487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08487">https://arxiv.org/pdf/2508.08487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08487]] MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling(https://arxiv.org/abs/2508.08487)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Despite recent advances, long-sequence video generation frameworks still suffer from significant limitations: poor assistive capability, suboptimal visual quality, and limited expressiveness. To mitigate these limitations, we propose MAViS, an end-to-end multi-agent collaborative framework for long-sequence video storytelling. MAViS orchestrates specialized agents across multiple stages, including script writing, shot designing, character modeling, keyframe generation, video animation, and audio generation. In each stage, agents operate under the 3E Principle -- Explore, Examine, and Enhance -- to ensure the completeness of intermediate outputs. Considering the capability limitations of current generative models, we propose the Script Writing Guidelines to optimize compatibility between scripts and generative tools. Experimental results demonstrate that MAViS achieves state-of-the-art performance in assistive capability, visual quality, and video expressiveness. Its modular framework further enables scalability with diverse generative models and tools. With just a brief user prompt, MAViS is capable of producing high-quality, expressive long-sequence video storytelling, enriching inspirations and creativity for users. To the best of our knowledge, MAViS is the only framework that provides multimodal design output -- videos with narratives and background music.</li>
<li><strong>摘要：</strong>尽管最近进步，但长期的视频生成框架仍然受到重大局限性：辅助能力差，次优质量和表现力有限。为了减轻这些局限性，我们建议Mavis，Mavis是一个端到端的多代理协作框架，用于长期视频讲故事。 Mavis在多个阶段策划了专门的代理，包括脚本编写，镜头设计，角色建模，钥匙帧生成，视频动画和音频生成。在每个阶段，代理在3E原理下运行 - 探索，检查和增强 - 以确保中间产出的完整性。考虑到当前生成模型的能力局限性，我们提出了脚本编写指南，以优化脚本和生成工具之间的兼容性。实验结果表明，Mavis在辅助能力，视觉质量和视频表现力方面取得了最先进的表现。它的模块化框架进一步通过不同的生成模型和工具实现了可扩展性。仅仅是一个简短的用户提示，Mavis能够产生高质量的，表现力的长期视频讲故事，为用户提供丰富的灵感和创造力。据我们所知，Mavis是唯一提供多模式设计输出的框架 - 带有叙事和背景音乐的视频。</li>
</ul>

<h3>Title: Re:Verse -- Can Your VLM Read a Manga?</h3>
<ul>
<li><strong>Authors: </strong>Aaditya Baranwal, Madhav Kataria, Naitik Agrawal, Yogesh S Rawat, Shruti Vyas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08508">https://arxiv.org/abs/2508.08508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08508">https://arxiv.org/pdf/2508.08508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08508]] Re:Verse -- Can Your VLM Read a Manga?(https://arxiv.org/abs/2508.08508)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Current Vision Language Models (VLMs) demonstrate a critical gap between surface-level recognition and deep narrative reasoning when processing sequential visual storytelling. Through a comprehensive investigation of manga narrative understanding, we reveal that while recent large multimodal models excel at individual panel interpretation, they systematically fail at temporal causality and cross-panel cohesion, core requirements for coherent story comprehension. We introduce a novel evaluation framework that combines fine-grained multimodal annotation, cross-modal embedding analysis, and retrieval-augmented assessment to systematically characterize these limitations. Our methodology includes (i) a rigorous annotation protocol linking visual elements to narrative structure through aligned light novel text, (ii) comprehensive evaluation across multiple reasoning paradigms, including direct inference and retrieval-augmented generation, and (iii) cross-modal similarity analysis revealing fundamental misalignments in current VLMs' joint representations. Applying this framework to Re:Zero manga across 11 chapters with 308 annotated panels, we conduct the first systematic study of long-form narrative understanding in VLMs through three core evaluation axes: generative storytelling, contextual dialogue grounding, and temporal reasoning. Our findings demonstrate that current models lack genuine story-level intelligence, struggling particularly with non-linear narratives, character consistency, and causal inference across extended sequences. This work establishes both the foundation and practical methodology for evaluating narrative intelligence, while providing actionable insights into the capability of deep sequential understanding of Discrete Visual Narratives beyond basic recognition in Multimodal Models.</li>
<li><strong>摘要：</strong>当前的视觉语言模型（VLMS）在处理顺序的视觉讲故事时表明了表面层面识别和深层叙事推理之间的关键差距。通过对漫画叙事理解的全面调查，我们透露，尽管最近的大型多模型模型在各个面板解释方面都表现出色，但它们在时间因果关系和跨面板凝聚力方面有系统地失败，这是一致故事理解的核心要求。我们介绍了一个新颖的评估框架，该框架结合了细粒度的多模式注释，跨模式嵌入分析和检索调查评估，以系统地表征这些局限性。我们的方法包括（i）通过对齐的轻型新文本将视觉元素与叙事结构联系起来的严格注释协议，（ii）跨多个推理范式的全面评估，包括直接推理和检索型发电，以及（iii）跨模式相似性分析揭示了当前VLMS的关节表现中的基本损失。将此框架应用于：在11章中使用308个带注释的面板的零漫画，我们通过三个核心评估轴对VLMS中的长形叙事理解进行了首次系统研究：生成的讲故事，上下文对话接地和时间推理。我们的发现表明，当前的模型缺乏真正的故事级智慧，尤其是在非线性叙事，角色一致性和跨扩展序列的因果推理方面挣扎。这项工作既建立了评估叙事智力的基础和实用方法，同时为对离散视觉叙事的深刻理解的能力提供了可行的见解，而不是多模型模型中的基本识别。</li>
</ul>

<h3>Title: Unlocking the Potential of Diffusion Priors in Blind Face Restoration</h3>
<ul>
<li><strong>Authors: </strong>Yunqi Miao, Zhiyu Qu, Mingqi Gao, Changrui Chen, Jifei Song, Jungong Han, Jiankang Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08556">https://arxiv.org/abs/2508.08556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08556">https://arxiv.org/pdf/2508.08556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08556]] Unlocking the Potential of Diffusion Priors in Blind Face Restoration(https://arxiv.org/abs/2508.08556)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Although diffusion prior is rising as a powerful solution for blind face restoration (BFR), the inherent gap between the vanilla diffusion model and BFR settings hinders its seamless adaptation. The gap mainly stems from the discrepancy between 1) high-quality (HQ) and low-quality (LQ) images and 2) synthesized and real-world images. The vanilla diffusion model is trained on images with no or less degradations, whereas BFR handles moderately to severely degraded images. Additionally, LQ images used for training are synthesized by a naive degradation model with limited degradation patterns, which fails to simulate complex and unknown degradations in real-world scenarios. In this work, we use a unified network FLIPNET that switches between two modes to resolve specific gaps. In Restoration mode, the model gradually integrates BFR-oriented features and face embeddings from LQ images to achieve authentic and faithful face restoration. In Degradation mode, the model synthesizes real-world like degraded images based on the knowledge learned from real-world degradation datasets. Extensive evaluations on benchmark datasets show that our model 1) outperforms previous diffusion prior based BFR methods in terms of authenticity and fidelity, and 2) outperforms the naive degradation model in modeling the real-world degradations.</li>
<li><strong>摘要：</strong>尽管扩散的先验是作为盲人恢复（BFR）的有力解决方案的上升，但香草扩散模型和BFR设置之间的固有差距阻碍了其无缝的适应性。差距主要源于1）高质量（HQ）和低质量（LQ）图像和2）合成和现实世界图像之间的差异。在没有或多或少降解的图像上对香草扩散模型进行了训练，而BFR则处理中度的图像严重降解。此外，用于训练的LQ图像是由具有有限的降解模式的幼稚降解模型合成的，该模型未能在现实世界中模拟复杂且未知的降解。在这项工作中，我们使用一个统一的网络夹板，该网络flipnet在两种模式之间切换来解决特定的差距。在恢复模式下，该模型逐渐整合了面向BFR的特征和LQ图像中的面部嵌入，以实现真实而忠实的面部修复。在退化模式下，该模型根据从现实世界中降解数据集中学到的知识综合了现实世界，例如退化的图像。关于基准数据集的广泛评估表明，我们的模型1）在真实性和忠诚度方面优于先前基于基于的BFR方法，并且2）在建模现实世界中的降级时优于天真的退化模型。</li>
</ul>

<h3>Title: RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space</h3>
<ul>
<li><strong>Authors: </strong>Jingyun Liang, Jingkai Zhou, Shikai Li, Chenjie Cao, Lei Sun, Yichen Qian, Weihua Chen, Fan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08588">https://arxiv.org/abs/2508.08588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08588">https://arxiv.org/pdf/2508.08588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08588]] RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space(https://arxiv.org/abs/2508.08588)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating human videos with realistic and controllable motions is a challenging task. While existing methods can generate visually compelling videos, they lack separate control over four key video elements: foreground subject, background video, human trajectory and action patterns. In this paper, we propose a decomposed human motion control and video generation framework that explicitly decouples motion from appearance, subject from background, and action from trajectory, enabling flexible mix-and-match composition of these elements. Concretely, we first build a ground-aware 3D world coordinate system and perform motion editing directly in the 3D space. Trajectory control is implemented by unprojecting edited 2D trajectories into 3D with focal-length calibration and coordinate transformation, followed by speed alignment and orientation adjustment; actions are supplied by a motion bank or generated via text-to-motion methods. Then, based on modern text-to-video diffusion transformer models, we inject the subject as tokens for full attention, concatenate the background along the channel dimension, and add motion (trajectory and action) control signals by addition. Such a design opens up the possibility for us to generate realistic videos of anyone doing anything anywhere. Extensive experiments on benchmark datasets and real-world cases demonstrate that our method achieves state-of-the-art performance on both element-wise controllability and overall video quality.</li>
<li><strong>摘要：</strong>以现实和可控动作生成人类视频是一项艰巨的任务。尽管现有方法可以生成视觉引人注目的视频，但它们对四个关键视频元素缺乏单独的控制：前景主题，背景视频，人类轨迹和动作模式。在本文中，我们提出了一个分解的人类运动控制和视频生成框架，该框架明确地将运动与外观，受到背景和轨迹的动作分解，从而使这些元素的柔性混合和匹配组成能够。具体而言，我们首先建立了一个接地的3D世界坐标系统，并直接在3D空间中进行运动编辑。轨迹控制是通过将编辑的2D轨迹取得焦点校准和坐标转换来实现的，然后进行速度对齐和方向调整；动作由运动库提供或通过文本到动作方法生成。然后，基于现代文本到视频扩散变压器模型，我们将主题注入令牌，以备值，以全部关注，沿通道维度串联背景，并通过添加添加运动（轨迹和动作）控制信号。这样的设计为我们开辟了可能在任何地方做任何事情的任何人的现实视频的可能性。基准数据集和现实情况的广泛实验表明，我们的方法在元素可控性和整体视频质量方面都能达到最新的性能。</li>
</ul>

<h3>Title: Yan: Foundational Interactive Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yan Team</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08601">https://arxiv.org/abs/2508.08601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08601">https://arxiv.org/pdf/2508.08601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08601]] Yan: Foundational Interactive Video Generation(https://arxiv.org/abs/2508.08601)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present Yan, a foundational framework for interactive video generation, covering the entire pipeline from simulation and generation to editing. Specifically, Yan comprises three core modules. AAA-level Simulation: We design a highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based shift-window denoising inference process, achieving real-time 1080P/60FPS interactive simulation. Multi-Modal Generation: We introduce a hierarchical autoregressive caption method that injects game-specific knowledge into open-domain multi-modal video diffusion models (VDMs), then transforming the VDM into a frame-wise, action-controllable, real-time infinite interactive video generator. Notably, when the textual and visual prompts are sourced from different domains, the model demonstrates strong generalization, allowing it to blend and compose the style and mechanics across domains flexibly according to user prompts. Multi-Granularity Editing: We propose a hybrid model that explicitly disentangles interactive mechanics simulation from visual rendering, enabling multi-granularity video content editing during interaction through text. Collectively, Yan offers an integration of these modules, pushing interactive video generation beyond isolated capabilities toward a comprehensive AI-driven interactive creation paradigm, paving the way for the next generation of creative tools, media, and entertainment. The project page is: this https URL.</li>
<li><strong>摘要：</strong>我们展示了Yan，这是一个用于交互式视频生成的基础框架，涵盖了从模拟和生成到编辑的整个管道。具体而言，YAN包括三个核心模块。 AAA级仿真：我们设计了一个高度压缩的低延迟3D-VAE，并与基于KV的基于KV-CACH-CACH-WINDOW DENOISING推理过程相结合，实现了实时1080p/60fps交互式模拟。多模式生成：我们引入了一种层次自动回归标题方法，该方法将特定于游戏的知识注入开放域的多模式视频扩散模型（VDM），然后将VDM转换为符合框架，可控制的，可控制的，实时的无限互动视频生成器。值得注意的是，当文本和视觉提示来自不同的域时，该模型表现出强烈的概括，使其可以根据用户提示灵活地融合和构成跨域的样式和机制。多粒性编辑：我们提出了一个混合模型，该模型将交互式力学仿真从视觉渲染中删除，从而可以通过文本进行交互期间的多粒性视频内容编辑。总体而言，Yan提供了这些模块的集成，将交互式视频生成推向了隔离的功能，使其成为全面的AI驱动的互动创作范式，为下一代创意工具，媒体和娱乐铺平了道路。项目页面是：此HTTPS URL。</li>
</ul>

<h3>Title: $\text{M}^{2}$LLM: Multi-view Molecular Representation Learning with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Ju, Yizhen Zheng, Huan Yee Koh, Can Wang, Shirui Pan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08657">https://arxiv.org/abs/2508.08657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08657">https://arxiv.org/pdf/2508.08657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08657]] $\text{M}^{2}$LLM: Multi-view Molecular Representation Learning with Large Language Models(https://arxiv.org/abs/2508.08657)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accurate molecular property prediction is a critical challenge with wide-ranging applications in chemistry, materials science, and drug discovery. Molecular representation methods, including fingerprints and graph neural networks (GNNs), achieve state-of-the-art results by effectively deriving features from molecular structures. However, these methods often overlook decades of accumulated semantic and contextual knowledge. Recent advancements in large language models (LLMs) demonstrate remarkable reasoning abilities and prior knowledge across scientific domains, leading us to hypothesize that LLMs can generate rich molecular representations when guided to reason in multiple perspectives. To address these gaps, we propose $\text{M}^{2}$LLM, a multi-view framework that integrates three perspectives: the molecular structure view, the molecular task view, and the molecular rules view. These views are fused dynamically to adapt to task requirements, and experiments demonstrate that $\text{M}^{2}$LLM achieves state-of-the-art performance on multiple benchmarks across classification and regression tasks. Moreover, we demonstrate that representation derived from LLM achieves exceptional performance by leveraging two core functionalities: the generation of molecular embeddings through their encoding capabilities and the curation of molecular features through advanced reasoning processes.</li>
<li><strong>摘要：</strong>准确的分子性质预测是化学，材料科学和药物发现中广泛应用的关键挑战。分子表示方法，包括指纹和图神经网络（GNNS），通过有效从分子结构得出特征来实现最先进的结果。但是，这些方法通常忽略了数十年来积累的语义和上下文知识。大型语言模型（LLM）的最新进展表明了跨科学领域的显着推理能力和先验知识，这使我们假设LLM可以在多种观点中引导推理时会产生丰富的分子表示。为了解决这些差距，我们提出了$ \ text {m}^{2} $ llm，这是一个集成了三个视角的多视图框架：分子结构视图，分子任务视图和分子规则视图。这些视图是动态融合的，以适应任务要求，实验表明$ \ text {m}^{2} $ llm在分类和回归任务的多个基准测试中实现了最先进的性能。此外，我们证明了从LLM得出的表示，通过利用两个核心功能来实现出色的性能：通过其编码能力生成分子嵌入的能力以及通过先进的推理过程的分子特征的策划。</li>
</ul>

<h3>Title: Expert-Guided Diffusion Planner for Auto-bidding</h3>
<ul>
<li><strong>Authors: </strong>Yunshan Peng, Wenzheng Shu, Jiahao Sun, Yanxiang Zeng, Jinan Pang, Wentao Bai, Yunke Bai, Xialong Liu, Peng Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08687">https://arxiv.org/abs/2508.08687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08687">https://arxiv.org/pdf/2508.08687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08687]] Expert-Guided Diffusion Planner for Auto-bidding(https://arxiv.org/abs/2508.08687)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Auto-bidding is extensively applied in advertising systems, serving a multitude of advertisers. Generative bidding is gradually gaining traction due to its robust planning capabilities and generalizability. In contrast to traditional reinforcement learning-based bidding, generative bidding does not rely on the Markov Decision Process (MDP) exhibiting superior planning capabilities in long-horizon scenarios. Conditional diffusion modeling approaches have demonstrated significant potential in the realm of auto-bidding. However, relying solely on return as the optimality condition is weak to guarantee the generation of genuinely optimal decision sequences, lacking personalized structural information. Moreover, diffusion models' t-step autoregressive generation mechanism inherently carries timeliness risks. To address these issues, we propose a novel conditional diffusion modeling method based on expert trajectory guidance combined with a skip-step sampling strategy to enhance generation efficiency. We have validated the effectiveness of this approach through extensive offline experiments and achieved statistically significant results in online A/B testing, achieving an increase of 11.29% in conversion and a 12.35% in revenue compared with the baseline.</li>
<li><strong>摘要：</strong>自动投标广泛应用于广告系统，为许多广告商服务。由于其强大的计划能力和概括性，生成招标正在逐渐获得吸引力。与传统的基于学习的招标相反，生成招标不依赖于马尔可夫决策过程（MDP）在长期胜利方案中表现出卓越的计划能力。有条件的扩散建模方法在自动铸造领域表现出了巨大的潜力。但是，仅依靠回报，因为最佳条件很弱，无法确保产生真正的最佳决策序列，缺乏个性化的结构信息。此外，扩散模型的T-步骤自回归产生机制固有地带有及时风险。为了解决这些问题，我们提出了一种基于专家轨迹指导的新型条件扩散模型方法，并结合了跳过步骤采样策略，以提高发电效率。我们通过广泛的离线实验验证了这种方法的有效性，并在在线A/B测试中取得了统计学意义的结果，与基线相比，转化率的增长增长了11.29％，收入增长了12.35％。</li>
</ul>

<h3>Title: Subjective and Objective Quality Assessment of Banding Artifacts on Compressed Videos</h3>
<ul>
<li><strong>Authors: </strong>Qi Zheng, Li-Heng Chen, Chenlong He, Neil Berkbeck, Yilin Wang, Balu Adsumilli, Alan C. Bovik, Yibo Fan, Zhengzhong Tu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08700">https://arxiv.org/abs/2508.08700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08700">https://arxiv.org/pdf/2508.08700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08700]] Subjective and Objective Quality Assessment of Banding Artifacts on Compressed Videos(https://arxiv.org/abs/2508.08700)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Although there have been notable advancements in video compression technologies in recent years, banding artifacts remain a serious issue affecting the quality of compressed videos, particularly on smooth regions of high-definition videos. Noticeable banding artifacts can severely impact the perceptual quality of videos viewed on a high-end HDTV or high-resolution screen. Hence, there is a pressing need for a systematic investigation of the banding video quality assessment problem for advanced video codecs. Given that the existing publicly available datasets for studying banding artifacts are limited to still picture data only, which cannot account for temporal banding dynamics, we have created a first-of-a-kind open video dataset, dubbed LIVE-YT-Banding, which consists of 160 videos generated by four different compression parameters using the AV1 video codec. A total of 7,200 subjective opinions are collected from a cohort of 45 human subjects. To demonstrate the value of this new resources, we tested and compared a variety of models that detect banding occurrences, and measure their impact on perceived quality. Among these, we introduce an effective and efficient new no-reference (NR) video quality evaluator which we call CBAND. CBAND leverages the properties of the learned statistics of natural images expressed in the embeddings of deep neural networks. Our experimental results show that the perceptual banding prediction performance of CBAND significantly exceeds that of previous state-of-the-art models, and is also orders of magnitude faster. Moreover, CBAND can be employed as a differentiable loss function to optimize video debanding models. The LIVE-YT-Banding database, code, and pre-trained model are all publically available at this https URL.</li>
<li><strong>摘要：</strong>尽管近年来视频压缩技术取得了显着的进步，但频道工件仍然是一个严重的问题，影响了压缩视频的质量，尤其是在高清视频的平滑区域上。引人注目的频带工件会严重影响在高端HDTV或高分辨率屏幕上观看的视频的感知质量。因此，迫切需要对高级视频编解码器的带视频质量评估问题进行系统的研究。鉴于现有用于研究绑带工件的公开可用数据集仅限于静止图片数据，而这些数据无法说明时间频段动态，因此我们创建了一个首次使用的开放式视频数据集，称为Live-yt-banding，由160个视频组成，该视频由四个使用AV1视频CODEC生成的160个视频。总共有45名人类受试者的队列收集了7,200个主观意见。为了证明这种新资源的价值，我们测试并比较了各种检测到频段事件的模型，并衡量了它们对感知质量的影响。其中，我们介绍了一个有效而有效的新不参考（NR）视频质量评估器，我们称为Cband。 CBAND利用在深神经网络的嵌入中表达的自然图像的学习统计数据。我们的实验结果表明，Cband的感知谱带预测性能显着超过了先前的最新模型的预测，并且也更快地数量级。此外，Cband可以用作可区分的损耗功能来优化视频脱键模型。 Live-YT带数据库，代码和预培训模型均在此HTTPS URL上公开可用。</li>
</ul>

<h3>Title: SafeFix: Targeted Model Repair via Controlled Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Ouyang Xu, Baoming Zhang, Ruiyu Mao, Yunhui Guo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08701">https://arxiv.org/abs/2508.08701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08701">https://arxiv.org/pdf/2508.08701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08701]] SafeFix: Targeted Model Repair via Controlled Image Generation(https://arxiv.org/abs/2508.08701)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Deep learning models for visual recognition often exhibit systematic errors due to underrepresented semantic subpopulations. Although existing debugging frameworks can pinpoint these failures by identifying key failure attributes, repairing the model effectively remains difficult. Current solutions often rely on manually designed prompts to generate synthetic training images -- an approach prone to distribution shift and semantic errors. To overcome these challenges, we introduce a model repair module that builds on an interpretable failure attribution pipeline. Our approach uses a conditional text-to-image model to generate semantically faithful and targeted images for failure cases. To preserve the quality and relevance of the generated samples, we further employ a large vision-language model (LVLM) to filter the outputs, enforcing alignment with the original data distribution and maintaining semantic consistency. By retraining vision models with this rare-case-augmented synthetic dataset, we significantly reduce errors associated with rare cases. Our experiments demonstrate that this targeted repair strategy improves model robustness without introducing new bugs. Code is available at this https URL</li>
<li><strong>摘要：</strong>视觉识别的深度学习模型通常由于代表性不足的语义亚群而出现系统错误。尽管现有的调试框架可以通过识别关键故障属性来查明这些失败，但有效地修复模型仍然很困难。当前的解决方案通常依靠手动设计的提示来生成合成训练图像 - 一种容易转移和语义错误的方法。为了克服这些挑战，我们引入了一个基于可解释的故障归因管道的模型维修模块。我们的方法使用有条件的文本对图像模型来为失败案例生成语义上忠实和有针对性的图像。为了保留生成的样品的质量和相关性，我们进一步采用了大型视觉模型（LVLM）来过滤输出，从而与原始数据分布相加并保持语义一致性。通过使用此稀有案例的合成数据集对视力模型进行重新探测模型，我们可以显着减少与罕见情况相关的错误。我们的实验表明，这种目标维修策略可改善模型鲁棒性，而无需引入新错误。代码可在此HTTPS URL上找到</li>
</ul>

<h3>Title: Generative Modeling for Robust Deep Reinforcement Learning on the Traveling Salesman Problem</h3>
<ul>
<li><strong>Authors: </strong>Michael Li, Eric Bae, Christopher Haberland, Natasha Jaques</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08718">https://arxiv.org/abs/2508.08718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08718">https://arxiv.org/pdf/2508.08718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08718]] Generative Modeling for Robust Deep Reinforcement Learning on the Traveling Salesman Problem(https://arxiv.org/abs/2508.08718)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The Traveling Salesman Problem (TSP) is a classic NP-hard combinatorial optimization task with numerous practical applications. Classic heuristic solvers can attain near-optimal performance for small problem instances, but become computationally intractable for larger problems. Real-world logistics problems such as dynamically re-routing last-mile deliveries demand a solver with fast inference time, which has led researchers to investigate specialized neural network solvers. However, neural networks struggle to generalize beyond the synthetic data they were trained on. In particular, we show that there exist TSP distributions that are realistic in practice, which also consistently lead to poor worst-case performance for existing neural approaches. To address this issue of distribution robustness, we present Combinatorial Optimization with Generative Sampling (COGS), where training data is sampled from a generative TSP model. We show that COGS provides better data coverage and interpolation in the space of TSP training distributions. We also present TSPLib50, a dataset of realistically distributed TSP samples, which tests real-world generalization ability without conflating this issue with instance size. We evaluate our method on various synthetic datasets as well as TSPLib50, and compare to state-of-the-art neural baselines. We demonstrate that COGS improves distribution robustness, with most performance gains coming from worst-case scenarios.</li>
<li><strong>摘要：</strong>旅行推销员问题（TSP）是一项经典的NP-HARD组合优化任务，具有许多实际应用。经典的启发式求解器可以在小问题实例中获得近乎最佳的性能，但在大型问题上变得棘手。现实世界中的物流问题，例如动态重新布置的最后一英里交付需要快速推理时间的求解器，这导致研究人员研究了专门的神经网络求解器。但是，神经网络努力概括超出他们接受过培训的合成数据。特别是，我们表明存在在实践中现实的TSP分布，这也始终导致现有神经方法的最差表现不佳。为了解决这个分布鲁棒性的问题，我们将组合优化与生成采样（COGS）提出，其中训练数据是从生成性TSP模型中采样的。我们表明，COGS在TSP培训分布的空间中提供了更好的数据覆盖范围和插值。我们还提供了TSPLIB50，这是一个现实分布的TSP样本的数据集，该数据集测试了现实世界中的概括能力，而无需将此问题与实例大小混为一谈。我们在各种合成数据集和TSPLIB50上评估了我们的方法，并与最先进的神经基准进行了比较。我们证明，COGS提高了分配鲁棒性，大多数性能提高来自最差的情况。</li>
</ul>

<h3>Title: Elucidating Rectified Flow with Deterministic Sampler: Polynomial Discretization Complexity for Multi and One-step Models</h3>
<ul>
<li><strong>Authors: </strong>Ruofeng Yang, Zhaoyu Zhu, Bo Jiang, Cheng Chen, Shuai Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08735">https://arxiv.org/abs/2508.08735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08735">https://arxiv.org/pdf/2508.08735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08735]] Elucidating Rectified Flow with Deterministic Sampler: Polynomial Discretization Complexity for Multi and One-step Models(https://arxiv.org/abs/2508.08735)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently, rectified flow (RF)-based models have achieved state-of-the-art performance in many areas for both the multi-step and one-step generation. However, only a few theoretical works analyze the discretization complexity of RF-based models. Existing works either focus on flow-based models with stochastic samplers or establish complexity results that exhibit exponential dependence on problem parameters. In this work, under the realistic bounded support assumption, we prove the first polynomial discretization complexity for multi-step and one-step RF-based models with a deterministic sampler simultaneously. For the multi-step setting, inspired by the predictor-corrector framework of diffusion models, we introduce a Langevin process as a corrector and show that RF-based models can achieve better polynomial discretization complexity than diffusion models. To achieve this result, we conduct a detailed analysis of the RF-based model and explain why it is better than previous popular models, such as variance preserving (VP) and variance exploding (VE)-based models. Based on the observation of multi-step RF-based models, we further provide the first polynomial discretization complexity result for one-step RF-based models, improving upon prior results for one-step diffusion-based models. These findings mark the first step toward theoretically understanding the impressive empirical performance of RF-based models in both multi-step and one-step generation.</li>
<li><strong>摘要：</strong>最近，基于整流的流量（RF）模型已在许多领域的多步骤和一步一代都达到了最先进的性能。但是，只有少数理论作品分析了基于RF的模型的离散化复杂性。现有的作品要么关注具有随机采样器的基于流的模型，要么建立对问题参数呈指数依赖性的复杂性结果。在这项工作中，在现实的有限支持假设下，我们证明了具有同时确定性采样器的多步和一步基于RF的模型的第一个多项式离散性复杂性。对于多步骤设置，受扩散模型的预测 - 矫正器框架的启发，我们引入了langevin过程作为校正器，并表明基于RF的模型比扩散模型可以实现更好的多项式离散化复杂性。为了实现这一结果，我们对基于RF的模型进行了详细的分析，并解释了为什么它比以前的流行模型更好，例如保留方差（VP）和方差爆炸（VE）基于基于（VE）的模型。基于对基于多步射频的模型的观察，我们进一步为一步基于RF的模型提供了第一个多项式离散性复杂性结果，从而改善了基于一步扩散的模型的先前结果。这些发现标志着从理论上理解基于RF的模型在多步骤和一步一代中的令人印象深刻的经验表现的第一步。</li>
</ul>

<h3>Title: DiffPose-Animal: A Language-Conditioned Diffusion Framework for Animal Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Xiong, Dayi Tan, Wei Tian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08783">https://arxiv.org/abs/2508.08783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08783">https://arxiv.org/pdf/2508.08783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08783]] DiffPose-Animal: A Language-Conditioned Diffusion Framework for Animal Pose Estimation(https://arxiv.org/abs/2508.08783)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Animal pose estimation is a fundamental task in computer vision, with growing importance in ecological monitoring, behavioral analysis, and intelligent livestock management. Compared to human pose estimation, animal pose estimation is more challenging due to high interspecies morphological diversity, complex body structures, and limited annotated data. In this work, we introduce DiffPose-Animal, a novel diffusion-based framework for top-down animal pose estimation. Unlike traditional heatmap regression methods, DiffPose-Animal reformulates pose estimation as a denoising process under the generative framework of diffusion models. To enhance semantic guidance during keypoint generation, we leverage large language models (LLMs) to extract both global anatomical priors and local keypoint-wise semantics based on species-specific prompts. These textual priors are encoded and fused with image features via cross-attention modules to provide biologically meaningful constraints throughout the denoising process. Additionally, a diffusion-based keypoint decoder is designed to progressively refine pose predictions, improving robustness to occlusion and annotation sparsity. Extensive experiments on public animal pose datasets demonstrate the effectiveness and generalization capability of our method, especially under challenging scenarios with diverse species, cluttered backgrounds, and incomplete keypoints.</li>
<li><strong>摘要：</strong>动物姿势估计是计算机视觉中的一项基本任务，在生态监测，行为分析和智能牲畜管理中的重要性越来越重要。与人姿势估计相比，由于高种间形态学多样性，复杂的身体结构和有限的注释数据，动物姿势估计更具挑战性。在这项工作中，我们引入了Diffpose-Animal，这是一种新型基于扩散的框架，用于自上而下的动物姿势估计。与传统的热图回归方法不同，在扩散模型的生成框架下，分散型动物可以将姿势估计作为一个脱糖过程。为了增强关键点生成期间的语义指导，我们利用大型语言模型（LLMS）根据物种特定的提示提取全球解剖学先验和局部关键语义语义。这些文本先验是通过交叉意见模块编码并与图像特征融合在一起的，以在整个剥离过程中提供具有生物学意义的约束。此外，基于扩散的关键点解码器旨在逐步完善姿势预测，从而提高了对遮挡和注释稀疏性的鲁棒性。对公共动物姿势数据集进行的广泛实验证明了我们方法的有效性和概括能力，尤其是在具有不同物种，混乱背景和不完整关键点的充满挑战的情况下。</li>
</ul>

<h3>Title: Identity-Preserving Aging and De-Aging of Faces in the StyleGAN Latent Space</h3>
<ul>
<li><strong>Authors: </strong>Luis S. Luevano, Pavel Korshunov, Sebastien Marcel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08808">https://arxiv.org/abs/2508.08808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08808">https://arxiv.org/pdf/2508.08808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08808]] Identity-Preserving Aging and De-Aging of Faces in the StyleGAN Latent Space(https://arxiv.org/abs/2508.08808)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Face aging or de-aging with generative AI has gained significant attention for its applications in such fields like forensics, security, and media. However, most state of the art methods rely on conditional Generative Adversarial Networks (GANs), Diffusion-based models, or Visual Language Models (VLMs) to age or de-age faces based on predefined age categories and conditioning via loss functions, fine-tuning, or text prompts. The reliance on such conditioning leads to complex training requirements, increased data needs, and challenges in generating consistent results. Additionally, identity preservation is rarely taken into accountor evaluated on a single face recognition system without any control or guarantees on whether identity would be preserved in a generated aged/de-aged face. In this paper, we propose to synthesize aged and de-aged faces via editing latent space of StyleGAN2 using a simple support vector modeling of aging/de-aging direction and several feature selection approaches. By using two state-of-the-art face recognition systems, we empirically find the identity preserving subspace within the StyleGAN2 latent space, so that an apparent age of a given face can changed while preserving the identity. We then propose a simple yet practical formula for estimating the limits on aging/de-aging parameters that ensures identity preservation for a given input face. Using our method and estimated parameters we have generated a public dataset of synthetic faces at different ages that can be used for benchmarking cross-age face recognition, age assurance systems, or systems for detection of synthetic images. Our code and dataset are available at the project page this https URL</li>
<li><strong>摘要：</strong>具有生成AI的面部衰老或衰老因其在取证，安全和媒体等领域的应用而引起了极大的关注。但是，大多数最先进的方法依赖于条件生成的对抗网络（GAN），基于扩散的模型或视觉语言模型（VLMS），以基于预定义的年龄类别和通过损失功能，微调或文本提示来基于预定义的年龄类别和调节。对这种条件的依赖会导致复杂的培训要求，增加数据需求以及产生一致结果的挑战。此外，很少将身份保存评估为在单个面部识别系统上评估的会计师，而没有任何控制或保证是否将身份保留在生成的老年/衰老面中。在本文中，我们建议通过使用简单的衰老/衰老方向和几种功能选择方法来编辑StyleGAN2的潜在空间来合成老化和衰落的面孔。通过使用两个最先进的面部识别系统，我们从经验上找到了在stylegan2潜在空间内保留子空间的身份，以便在保留身份的同时可以改变给定面孔的明显年龄。然后，我们提出了一个简单而实用的公式，用于估计确保给定输入面的身份保存的老化/衰老参数的限制。使用我们的方法和估计参数，我们生成了一个不同年龄段的合成面的公共数据集，可用于基准跨年龄面部识别，年龄保证系统或用于检测合成图像的系统。我们的代码和数据集可在此https url的项目页面上找到</li>
</ul>

<h3>Title: TARA: Token-Aware LoRA for Composable Personalization in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yuqi Peng, Lingtao Zheng, Yufeng Yang, Yi Huang, Mingfu Yan, Jianzhuang Liu, Shifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08812">https://arxiv.org/abs/2508.08812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08812">https://arxiv.org/pdf/2508.08812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08812]] TARA: Token-Aware LoRA for Composable Personalization in Diffusion Models(https://arxiv.org/abs/2508.08812)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Personalized text-to-image generation aims to synthesize novel images of a specific subject or style using only a few reference images. Recent methods based on Low-Rank Adaptation (LoRA) enable efficient single-concept customization by injecting lightweight, concept-specific adapters into pre-trained diffusion models. However, combining multiple LoRA modules for multi-concept generation often leads to identity missing and visual feature leakage. In this work, we identify two key issues behind these failures: (1) token-wise interference among different LoRA modules, and (2) spatial misalignment between the attention map of a rare token and its corresponding concept-specific region. To address these issues, we propose Token-Aware LoRA (TARA), which introduces a token mask to explicitly constrain each module to focus on its associated rare token to avoid interference, and a training objective that encourages the spatial attention of a rare token to align with its concept region. Our method enables training-free multi-concept composition by directly injecting multiple independently trained TARA modules at inference time. Experimental results demonstrate that TARA enables efficient multi-concept inference and effectively preserving the visual identity of each concept by avoiding mutual interference between LoRA modules. The code and models are available at this https URL.</li>
<li><strong>摘要：</strong>个性化的文本到图像生成旨在仅使用几个参考图像综合特定主题或样式的新颖图像。基于低级别适应性（LORA）的最新方法通过将轻巧的，特定于概念的适配器注入预训练的扩散模型，从而实现了有效的单概念自定义。但是，将多个洛拉模块组合用于多概念生成通常会导致身份缺失和视觉特征泄漏。在这项工作中，我们确定了这些失败背后的两个关键问题：（1）不同洛拉模块之间的令牌干扰，以及（2）稀有令牌的注意力图及其相应概念特定区域之间的空间未对准。为了解决这些问题，我们提出了令牌感知的洛拉（TARA），它引入了令牌面具，以明确地限制每个模块，以专注于其相关的稀有代币，以避免干扰，并鼓励稀有标志的空间注意力以与其概念区域保持一致。我们的方法通过在推理时直接注入多个独立训练的塔拉模块来实现无训练的多概念组合。实验结果表明，塔拉（Tara）可以通过避免洛拉模块之间的相互干扰来实现有效的多概念推理，并有效地保留每个概念的视觉认同。代码和模型可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: 3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Noor Ahmed, Cameron Braunstein, Steffen Eger, Eddy Ilg</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08821">https://arxiv.org/abs/2508.08821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08821">https://arxiv.org/pdf/2508.08821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08821]] 3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs(https://arxiv.org/abs/2508.08821)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent Multi-Modal Large Language Models (MLLMs) have demonstrated strong capabilities in learning joint representations from text and images. However, their spatial reasoning remains limited. We introduce 3DFroMLLM, a novel framework that enables the generation of 3D object prototypes directly from MLLMs, including geometry and part labels. Our pipeline is agentic, comprising a designer, coder, and visual inspector operating in a refinement loop. Notably, our approach requires no additional training data or detailed user instructions. Building on prior work in 2D generation, we demonstrate that rendered images produced by our framework can be effectively used for image classification pretraining tasks and outperforms previous methods by 15%. As a compelling real-world use case, we show that the generated prototypes can be leveraged to improve fine-grained vision-language models by using the rendered, part-labeled prototypes to fine-tune CLIP for part segmentation and achieving a 55% accuracy improvement without relying on any additional human-labeled data.</li>
<li><strong>摘要：</strong>最近的多模式大型语言模型（MLLM）证明了从文本和图像学习联合表示方面具有很强的能力。但是，他们的空间推理仍然有限。我们介绍了3DFROMLLM，这是一个新颖的框架，可以直接从MLLM（包括几何图形和部分标签）中生成3D对象原型。我们的管道是代理的，包括设计人员，编码器和视觉检查员在改进循环中运行。值得注意的是，我们的方法不需要其他培训数据或详细的用户说明。在第二代的先前工作的基础上，我们证明了由我们的框架产生的渲染图像可有效地用于图像分类预处理任务，并以先前的方法优于15％。作为一种引人注目的现实用例，我们表明可以利用生成的原型来通过使用渲染的，部分标记的原型来微调片段来进行零件分割并实现55％的准确性改进，而无需依靠任何其他人标记的数据，可以利用生成的原型来改善细粒度的视觉模型。</li>
</ul>

<h3>Title: A Parametric Bi-Directional Curvature-Based Framework for Image Artifact Classification and Quantification</h3>
<ul>
<li><strong>Authors: </strong>Diego Frias</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08824">https://arxiv.org/abs/2508.08824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08824">https://arxiv.org/pdf/2508.08824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08824]] A Parametric Bi-Directional Curvature-Based Framework for Image Artifact Classification and Quantification(https://arxiv.org/abs/2508.08824)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>This work presents a novel framework for No-Reference Image Quality Assessment (NR-IQA) founded on the analysis of directional image curvature. Within this framework, we define a measure of Anisotropic Texture Richness (ATR), which is computed at the pixel level using two tunable thresholds -- one permissive and one restrictive -- that quantify orthogonal texture suppression. When its parameters are optimized for a specific artifact, the resulting ATR score serves as a high-performance quality metric, achieving Spearman correlations with human perception of approximately -0.93 for Gaussian blur and -0.95 for white noise on the LIVE dataset. The primary contribution is a two-stage system that leverages the differential response of ATR to various distortions. First, the system utilizes the signature from two specialist ATR configurations to classify the primary artifact type (blur vs. noise) with over 97% accuracy. Second, following classification, it employs a dedicated regression model mapping the relevant ATR score to a quality rating to quantify the degradation. On a combined dataset, the complete system predicts human scores with a coefficient of determination (R2) of 0.892 and a Root Mean Square Error (RMSE) of 5.17 DMOS points. This error corresponds to just 7.4% of the dataset's total quality range, demonstrating high predictive accuracy. This establishes our framework as a robust, dual-purpose tool for the classification and subsequent quantification of image degradation.</li>
<li><strong>摘要：</strong>这项工作提出了一个新的引用图像质量评估（NR-IQA）的新框架，该框架建立在方向图像曲率分析上。在此框架内，我们定义了各向异性纹理丰富度（ATR）的度量，该度量是在像素级别计算的，使用两个可调阈值（一种允许性和一个限制性）来量化正交纹理抑制。当对特定工件进行优化的参数时，由此产生的ATR得分是高性能质量度量标准，与高斯模糊的人类感知达到了大约-0.93的相关性和-0.95，而实时数据集上的白噪声则达到了-0.95。主要贡献是一个两阶段系统，该系统利用ATR对各种扭曲的差异响应。首先，该系统利用来自两种专家ATR配置的签名来以超过97％的精度对主要人工制品类型（Blur vs.噪声）进行分类。其次，在分类之后，它采用专用的回归模型将相关的ATR分数映射到质量等级以量化降解。在合并的数据集上，完整的系统以0.892的确定系数（R2）和5.17 DMOS点的根平方误差（RMSE）预测人类得分。此错误仅对应于数据集总质量范围的7.4％，这表明了高预测精度。这将我们的框架确定为一种可靠的双重用途工具，用于分类和随后的图像降解量化。</li>
</ul>

<h3>Title: Adaptive High-Frequency Preprocessing for Video Coding</h3>
<ul>
<li><strong>Authors: </strong>Yingxue Pang, Shijie Zhao, Junlin Li, Li Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08849">https://arxiv.org/abs/2508.08849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08849">https://arxiv.org/pdf/2508.08849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08849]] Adaptive High-Frequency Preprocessing for Video Coding(https://arxiv.org/abs/2508.08849)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>High-frequency components are crucial for maintaining video clarity and realism, but they also significantly impact coding bitrate, resulting in increased bandwidth and storage costs. This paper presents an end-to-end learning-based framework for adaptive high-frequency preprocessing to enhance subjective quality and save bitrate in video coding. The framework employs the Frequency-attentive Feature pyramid Prediction Network (FFPN) to predict the optimal high-frequency preprocessing strategy, guiding subsequent filtering operators to achieve the optimal tradeoff between bitrate and quality after compression. For training FFPN, we pseudo-label each training video with the optimal strategy, determined by comparing the rate-distortion (RD) performance across different preprocessing types and strengths. Distortion is measured using the latest quality assessment metric. Comprehensive evaluations on multiple datasets demonstrate the visually appealing enhancement capabilities and bitrate savings achieved by our framework.</li>
<li><strong>摘要：</strong>高频组件对于维持视频清晰度和现实主义至关重要，但它们也会显着影响编码比特率，从而增加带宽和存储成本。本文提出了一个基于端到端学习的框架，用于自适应高频预处理，以提高主观质量并在视频编码中节省比特率。该框架采用频率激烈的特征金字塔预测网络（FFPN）来预测最佳的高频预处理策略，从而指导后续的过滤操作员在压缩后实现比特率和质量之间的最佳权衡。对于培训FFPN，我们使用最佳策略为每个培训视频进行了为每个培训视频进行伪造，并通过比较不同预处理类型和优势的速率降低（RD）性能来确定。使用最新的质量评估度量测量失真。在多个数据集上进行的全面评估证明了视觉上吸引人的增强功能和通过我们的框架获得的比特率节省。</li>
</ul>

<h3>Title: Flow Battery Manifold Design with Heterogeneous Inputs Through Generative Adversarial Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Eric Seng, Hugh O'Connor, Adam Boyce, Josh J. Bailey, Anton van Beek (School of Mechanical and Materials Engineering, University College Dublin, Dublin, Ireland)</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08863">https://arxiv.org/abs/2508.08863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08863">https://arxiv.org/pdf/2508.08863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08863]] Flow Battery Manifold Design with Heterogeneous Inputs Through Generative Adversarial Neural Networks(https://arxiv.org/abs/2508.08863)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative machine learning has emerged as a powerful tool for design representation and exploration. However, its application is often constrained by the need for large datasets of existing designs and the lack of interpretability about what features drive optimality. To address these challenges, we introduce a systematic framework for constructing training datasets tailored to generative models and demonstrate how these models can be leveraged for interpretable design. The novelty of this work is twofold: (i) we present a systematic framework for generating archetypes with internally homogeneous but mutually heterogeneous inputs that can be used to generate a training dataset, and (ii) we show how integrating generative models with Bayesian optimization can enhance the interpretability of the latent space of admissible designs. These findings are validated by using the framework to design a flow battery manifold, demonstrating that it effectively captures the space of feasible designs, including novel configurations while enabling efficient exploration. This work broadens the applicability of generative machine-learning models in system designs by enhancing quality and reliability.</li>
<li><strong>摘要：</strong>生成机器学习已成为设计表示和探索的强大工具。但是，它的应用程序通常受到现有设计的大量数据集的需求以及对哪些功能驱动最佳性的解释性的限制。为了应对这些挑战，我们介绍了一个系统的框架，用于构建针对生成模型的培训数据集，并演示如何利用这些模型用于可解释的设计。这项工作的新颖性是双重的：（i）我们提出了一个系统的框架，用于生成具有内部均匀但相互异质的投入的原型，可用于生成培训数据集，（ii）我们如何将生成模型与贝叶斯优化整合可以增强可允许设计的潜在空间的解释能力。这些发现通过使用框架设计流动电池歧管来验证，这表明它有效地捕获了可行设计的空间，包括新型配置，同时实现了有效的探索。这项工作扩大了生成机器学习模型在系统设计中的适用性，通过提高质量和可靠性。</li>
</ul>

<h3>Title: GaussianUpdate: Continual 3D Gaussian Splatting Update for Changing Environments</h3>
<ul>
<li><strong>Authors: </strong>Lin Zeng, Boming Zhao, Jiarui Hu, Xujie Shen, Ziqiang Dang, Hujun Bao, Zhaopeng Cui</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08867">https://arxiv.org/abs/2508.08867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08867">https://arxiv.org/pdf/2508.08867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08867]] GaussianUpdate: Continual 3D Gaussian Splatting Update for Changing Environments(https://arxiv.org/abs/2508.08867)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Novel view synthesis with neural models has advanced rapidly in recent years, yet adapting these models to scene changes remains an open problem. Existing methods are either labor-intensive, requiring extensive model retraining, or fail to capture detailed types of changes over time. In this paper, we present GaussianUpdate, a novel approach that combines 3D Gaussian representation with continual learning to address these challenges. Our method effectively updates the Gaussian radiance fields with current data while preserving information from past scenes. Unlike existing methods, GaussianUpdate explicitly models different types of changes through a novel multi-stage update strategy. Additionally, we introduce a visibility-aware continual learning approach with generative replay, enabling self-aware updating without the need to store images. The experiments on the benchmark dataset demonstrate our method achieves superior and real-time rendering with the capability of visualizing changes over different times</li>
<li><strong>摘要：</strong>近年来，与神经模型的新型视图合成迅速发展，但是将这些模型适应场景变化仍然是一个开放的问题。现有方法要么是劳动力密集的，需要广泛的模型再培训，要么无法捕获随着时间的推移详细的变化类型。在本文中，我们提出了高斯update，这是一种新颖的方法，将3D高斯代表与持续学习结合在一起以应对这些挑战。我们的方法有效地更新了高斯辐射字段，并通过保留过去场景的信息。与现有方法不同，高斯update通过新颖的多阶段更新策略明确地对不同类型的更改进行建模。此外，我们通过生成重播引入了一种可见性的持续学习方法，从而无需存储图像就可以自我了解。基准数据集上的实验证明了我们的方法具有在不同时间内可视化变化的能力的优越和实时渲染</li>
</ul>

<h3>Title: Preview WB-DH: Towards Whole Body Digital Human Bench for the Generation of Whole-body Talking Avatar Videos</h3>
<ul>
<li><strong>Authors: </strong>Chaoyi Wang, Yifan Yang, Jun Pei, Lijie Xia, Jianpo Liu, Xiaobing Yuan, Xinhan Di</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08891">https://arxiv.org/abs/2508.08891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08891">https://arxiv.org/pdf/2508.08891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08891]] Preview WB-DH: Towards Whole Body Digital Human Bench for the Generation of Whole-body Talking Avatar Videos(https://arxiv.org/abs/2508.08891)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Creating realistic, fully animatable whole-body avatars from a single portrait is challenging due to limitations in capturing subtle expressions, body movements, and dynamic backgrounds. Current evaluation datasets and metrics fall short in addressing these complexities. To bridge this gap, we introduce the Whole-Body Benchmark Dataset (WB-DH), an open-source, multi-modal benchmark designed for evaluating whole-body animatable avatar generation. Key features include: (1) detailed multi-modal annotations for fine-grained guidance, (2) a versatile evaluation framework, and (3) public access to the dataset and tools at this https URL.</li>
<li><strong>摘要：</strong>由于捕获微妙的表情，身体运动和动态背景的局限性，从单个肖像中创建现实，完全动画的全身化身是具有挑战性的。当前的评估数据集和指标在解决这些复杂性方面缺乏。为了弥合这一差距，我们介绍了全身基准数据集（WB-DH），这是一种开源的多模式基准测试，旨在评估全身动画化的阿凡达生成。关键功能包括：（1）用于细粒度指导的详细多模式注释，（2）多功能评估框架以及（3）在此HTTPS URL上公开访问数据集和工具。</li>
</ul>

<h3>Title: Automatic and standardized surgical reporting for central nervous system tumors</h3>
<ul>
<li><strong>Authors: </strong>David Bouget, Mathilde Gajda Faanes, Asgeir Store Jakola, Frederik Barkhof, Hilko Ardon, Lorenzo Bello, Mitchel S. Berger, Shawn L. Hervey-Jumper, Julia Furtner, Albert J. S. Idema, Barbara Kiesel, Georg Widhalm, Rishi Nandoe Tewarie, Emmanuel Mandonnet, Pierre A. Robe, Michiel Wagemakers, Timothy R. Smith, Philip C. De Witt Hamer, Ole solheim, Ingerid Reinertsen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08916">https://arxiv.org/abs/2508.08916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08916">https://arxiv.org/pdf/2508.08916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08916]] Automatic and standardized surgical reporting for central nervous system tumors(https://arxiv.org/abs/2508.08916)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Magnetic resonance (MR) imaging is essential for evaluating central nervous system (CNS) tumors, guiding surgical planning, treatment decisions, and assessing postoperative outcomes and complication risks. While recent work has advanced automated tumor segmentation and report generation, most efforts have focused on preoperative data, with limited attention to postoperative imaging analysis. This study introduces a comprehensive pipeline for standardized postsurtical reporting in CNS tumors. Using the Attention U-Net architecture, segmentation models were trained for the preoperative (non-enhancing) tumor core, postoperative contrast-enhancing residual tumor, and resection cavity. Additionally, MR sequence classification and tumor type identification for contrast-enhancing lesions were explored using the DenseNet architecture. The models were integrated into a reporting pipeline, following the RANO 2.0 guidelines. Training was conducted on multicentric datasets comprising 2000 to 7000 patients, using a 5-fold cross-validation. Evaluation included patient-, voxel-, and object-wise metrics, with benchmarking against the latest BraTS challenge results. The segmentation models achieved average voxel-wise Dice scores of 87%, 66%, 70%, and 77% for the tumor core, non-enhancing tumor core, contrast-enhancing residual tumor, and resection cavity, respectively. Classification models reached 99.5% balanced accuracy in MR sequence classification and 80% in tumor type classification. The pipeline presented in this study enables robust, automated segmentation, MR sequence classification, and standardized report generation aligned with RANO 2.0 guidelines, enhancing postoperative evaluation and clinical decision-making. The proposed models and methods were integrated into Raidionics, open-source software platform for CNS tumor analysis, now including a dedicated module for postsurgical analysis.</li>
<li><strong>摘要：</strong>磁共振（MR）成像对于评估中枢神经系统（CNS）肿瘤，指导手术计划，治疗决策以及评估术后结局和并发症风险至关重要。尽管最近的工作已经提高了自动化的肿瘤细分和报告的生成，但大多数工作都集中在术前数据上，并且对术后成像分析的关注有限。这项研究介绍了CNS肿瘤中标准化术后报告的全面管道。使用注意力U-NET结构，对分割模型进行了训练，以培训术前（非增强）肿瘤核心，术后增强造影剂残留肿瘤和切除腔。此外，使用Densenet结构探索了对比增强病变的MR序列分类和肿瘤类型鉴定。按照RANO 2.0指南，将模型集成到报告管道中。使用5倍的交叉验证，对包括2000至7000名患者的多中心数据集进行了培训。评估包括患者，体素和对象指标，并根据最新的Brats挑战结果进行基准测试。分割模型的平均体素骰子得分分别为87％，66％，70％和77％的肿瘤核心，非增强肿瘤核心，增强对比的残留肿瘤和切除腔。分类模型在MR序列分类中达到99.5％的平衡精度，肿瘤类型分类达到80％。这项研究中提出的管道可实现与RANO 2.0指南一致的强大，自动分割，MR序列分类和标准化的报告生成，从而增强了术后评估和临床决策。提出的模型和方法集成到Raidionics，开源软件平台，用于CNS肿瘤分析，现在包括用于术后分析的专用模块。</li>
</ul>

<h3>Title: Lay2Story: Extending Diffusion Transformers for Layout-Togglable Story Generation</h3>
<ul>
<li><strong>Authors: </strong>Ao Ma, Jiasong Feng, Ke Cao, Jing Wang, Yun Wang, Quanwei Zhang, Zhanjie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08949">https://arxiv.org/abs/2508.08949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08949">https://arxiv.org/pdf/2508.08949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08949]] Lay2Story: Extending Diffusion Transformers for Layout-Togglable Story Generation(https://arxiv.org/abs/2508.08949)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Storytelling tasks involving generating consistent subjects have gained significant attention recently. However, existing methods, whether training-free or training-based, continue to face challenges in maintaining subject consistency due to the lack of fine-grained guidance and inter-frame interaction. Additionally, the scarcity of high-quality data in this field makes it difficult to precisely control storytelling tasks, including the subject's position, appearance, clothing, expression, and posture, thereby hindering further advancements. In this paper, we demonstrate that layout conditions, such as the subject's position and detailed attributes, effectively facilitate fine-grained interactions between frames. This not only strengthens the consistency of the generated frame sequence but also allows for precise control over the subject's position, appearance, and other key details. Building on this, we introduce an advanced storytelling task: Layout-Togglable Storytelling, which enables precise subject control by incorporating layout conditions. To address the lack of high-quality datasets with layout annotations for this task, we develop Lay2Story-1M, which contains over 1 million 720p and higher-resolution images, processed from approximately 11,300 hours of cartoon videos. Building on Lay2Story-1M, we create Lay2Story-Bench, a benchmark with 3,000 prompts designed to evaluate the performance of different methods on this task. Furthermore, we propose Lay2Story, a robust framework based on the Diffusion Transformers (DiTs) architecture for Layout-Togglable Storytelling tasks. Through both qualitative and quantitative experiments, we find that our method outperforms the previous state-of-the-art (SOTA) techniques, achieving the best results in terms of consistency, semantic correlation, and aesthetic quality.</li>
<li><strong>摘要：</strong>最近，涉及产生一致主题的讲述任务最近引起了重大关注。但是，由于缺乏细粒度的指导和框架间的相互作用，现有的方法，无论是无培训还是基于培训，无论是无培训还是基于培训，都在保持对象一致性方面的挑战。此外，该领域的高质量数据的稀缺性使得很难精确控制讲故事的任务，包括主题的位置，外观，衣服，表情和姿势，从而阻碍进一步的进步。在本文中，我们证明了布局条件，例如受试者的位置和详细属性，有效地促进了框架之间的细粒度相互作用。这不仅增强了生成的帧序列的一致性，而且还可以精确控制对象的位置，外观和其他关键细节。在此基础上，我们介绍了一项高级讲故事的任务：布局可敲门的讲故事，该任务可以通过结合布局条件来精确的主题控制。为了解决缺乏此任务的布局注释的高质量数据集，我们开发了Lay2Story-1m，其中包含超过100万720p和高分辨率图像，并从大约11300个小时的卡通视频处理。在Lay2Story-1m上，我们创建了Lay2Story-Bench，这是一个基准标准，其中有3,000个提示，旨在评估此任务上不同方法的性能。此外，我们提出了Lay2Story，这是一个基于扩散变压器（DITS）架构的稳健框架，用于布局可访问的讲故事任务。通过定性和定量实验，我们发现我们的方法的表现优于先前的最新技术（SOTA）技术，从而在一致性，语义相关性和审美质量方面取得了最佳结果。</li>
</ul>

<h3>Title: TaoCache: Structure-Maintained Video Generation Acceleration</h3>
<ul>
<li><strong>Authors: </strong>Zhentao Fan, Zongzuo Wang, Weiwei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08978">https://arxiv.org/abs/2508.08978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08978">https://arxiv.org/pdf/2508.08978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08978]] TaoCache: Structure-Maintained Video Generation Acceleration(https://arxiv.org/abs/2508.08978)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing cache-based acceleration methods for video diffusion models primarily skip early or mid denoising steps, which often leads to structural discrepancies relative to full-timestep generation and can hinder instruction following and character consistency. We present TaoCache, a training-free, plug-and-play caching strategy that, instead of residual-based caching, adopts a fixed-point perspective to predict the model's noise output and is specifically effective in late denoising stages. By calibrating cosine similarities and norm ratios of consecutive noise deltas, TaoCache preserves high-resolution structure while enabling aggressive skipping. The approach is orthogonal to complementary accelerations such as Pyramid Attention Broadcast (PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks. Across Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially higher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the same speedups.</li>
<li><strong>摘要：</strong>现有的基于缓存的视频扩散模型的加速方法主要跳过早期或中端的步骤，这通常会导致相对于全磁性生成的结构差异，并且可以阻碍以下和角色一致性。我们提出了Taocache，这是一种无训练的，插件的缓存策略，该策略并非基于剩余的缓存，而是采用了定点透视图来预测模型的噪声输出，并且在晚期DeNoing阶段特别有效。通过校准连续噪声三角洲的余弦相似性和标准比率，陶氏（Taocache）保留了高分辨率结构，同时可以进行积极的跳过。这种方法与互补的加速度（例如金字塔关注广播（PAB）和TEACACHE）是正交的，它无缝地集成到基于DIT的框架中。在Latte-1，OpenSora-Plan-Plan V110和WAN2.1中，Taocache的视觉质量（LPIPS，SSIM，PSNR）大大要高于相同速度下的先前缓存方法。</li>
</ul>

<h3>Title: ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Ding Xia, Naoto Inoue, Qianru Qiu, Kotaro Kikuchi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08987">https://arxiv.org/abs/2508.08987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08987">https://arxiv.org/pdf/2508.08987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08987]] ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation(https://arxiv.org/abs/2508.08987)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Colors play a crucial role in the design of vector graphic documents by enhancing visual appeal, facilitating communication, improving usability, and ensuring accessibility. In this context, color recommendation involves suggesting appropriate colors to complete or refine a design when one or more colors are missing or require alteration. Traditional methods often struggled with these challenges due to the complex nature of color design and the limited data availability. In this study, we explored the use of pretrained Large Language Models (LLMs) and their commonsense reasoning capabilities for color recommendation, raising the question: Can pretrained LLMs serve as superior designers for color recommendation tasks? To investigate this, we developed a robust, rigorously validated pipeline, ColorGPT, that was built by systematically testing multiple color representations and applying effective prompt engineering techniques. Our approach primarily targeted color palette completion by recommending colors based on a set of given colors and accompanying context. Moreover, our method can be extended to full palette generation, producing an entire color palette corresponding to a provided textual description. Experimental results demonstrated that our LLM-based pipeline outperformed existing methods in terms of color suggestion accuracy and the distribution of colors in the color palette completion task. For the full palette generation task, our approach also yielded improvements in color diversity and similarity compared to current techniques.</li>
<li><strong>摘要：</strong>通过增强视觉吸引力，促进沟通，提高可用性和确保可访问性，颜色在矢量图形文档的设计中起着至关重要的作用。在这种情况下，颜色建议涉及建议在缺少一种或多种颜色或需要更改时完成或完善设计的适当颜色。由于颜色设计的复杂性质和有限的数据可用性，传统方法通常在这些挑战中苦苦挣扎。在这项研究中，我们探讨了验证的大语言模型（LLM）及其常识性推理功能的颜色推荐功能，提出了一个问题：审核的LLM可以用作彩色推荐任务的优越设计师吗？为了调查这一点，我们开发了一种可靠，严格验证的管道ColorGPT，该管道是通过系统地测试多种颜色表示并应用有效的及时工程技术来构建的。我们的方法主要针对完整的调色板完成，通过基于一组给定的颜色和随附的上下文推荐颜色。此外，我们的方法可以扩展到完整的调色板生成，从而产生与提供的文本描述相对应的整个调色板。实验结果表明，我们基于LLM的管道在颜色建议准确性和调色板完成任务中的颜色分布方面优于现有方法。对于完整的调色板生成任务，与当前技术相比，我们的方法还可以改善色彩多样性和相似性。</li>
</ul>

<h3>Title: Spatial-Temporal Multi-Scale Quantization for Flexible Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Zan Wang, Jingze Zhang, Yixin Chen, Baoxiong Jia, Wei Liang, Siyuan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.08991">https://arxiv.org/abs/2508.08991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.08991">https://arxiv.org/pdf/2508.08991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.08991]] Spatial-Temporal Multi-Scale Quantization for Flexible Motion Generation(https://arxiv.org/abs/2508.08991)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Despite significant advancements in human motion generation, current motion representations, typically formulated as discrete frame sequences, still face two critical limitations: (i) they fail to capture motion from a multi-scale perspective, limiting the capability in complex patterns modeling; (ii) they lack compositional flexibility, which is crucial for model's generalization in diverse generation tasks. To address these challenges, we introduce MSQ, a novel quantization method that compresses the motion sequence into multi-scale discrete tokens across spatial and temporal dimensions. MSQ employs distinct encoders to capture body parts at varying spatial granularities and temporally interpolates the encoded features into multiple scales before quantizing them into discrete tokens. Building on this representation, we establish a generative mask modeling model to effectively support motion editing, motion control, and conditional motion generation. Through quantitative and qualitative analysis, we show that our quantization method enables the seamless composition of motion tokens without requiring specialized design or re-training. Furthermore, extensive evaluations demonstrate that our approach outperforms existing baseline methods on various benchmarks.</li>
<li><strong>摘要：</strong>尽管人类运动产生的显着进步，但通常以离散框架序列为代表的当前运动表示仍面临两个临界局限性：（i）他们无法从多尺度的角度捕获运动，从而限制了复杂模式建模的能力； （ii）它们缺乏组成灵活性，这对于模型在不同生成任务中的概括至关重要。为了应对这些挑战，我们引入了MSQ，这是一种新颖的量化方法，将运动序列压缩为跨空间和时间维度的多尺度离散令牌。 MSQ采用不同的编码器在不同的空间粒度下捕获身体部位，并在将编码的特征插入多个尺度上，然后将其插入多个尺度，然后将其量化为离散令牌。在此表示的基础上，我们建立了一个生成性掩盖模型，以有效地支持运动编辑，运动控制和有条件运动。通过定量和定性分析，我们表明我们的量化方法可以使运动令牌无缝组成，而无需专门的设计或重新训练。此外，广泛的评估表明，我们的方法在各种基准上都优于现有的基线方法。</li>
</ul>

<h3>Title: MechaFormer: Sequence Learning for Kinematic Mechanism Design Automation</h3>
<ul>
<li><strong>Authors: </strong>Diana Bolanos, Mohammadmehdi Ataei, Pradeep Kumar Jayaraman</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09005">https://arxiv.org/abs/2508.09005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09005">https://arxiv.org/pdf/2508.09005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09005]] MechaFormer: Sequence Learning for Kinematic Mechanism Design Automation(https://arxiv.org/abs/2508.09005)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Designing mechanical mechanisms to trace specific paths is a classic yet notoriously difficult engineering problem, characterized by a vast and complex search space of discrete topologies and continuous parameters. We introduce MechaFormer, a Transformer-based model that tackles this challenge by treating mechanism design as a conditional sequence generation task. Our model learns to translate a target curve into a domain-specific language (DSL) string, simultaneously determining the mechanism's topology and geometric parameters in a single, unified process. MechaFormer significantly outperforms existing baselines, achieving state-of-the-art path-matching accuracy and generating a wide diversity of novel and valid designs. We demonstrate a suite of sampling strategies that can dramatically improve solution quality and offer designers valuable flexibility. Furthermore, we show that the high-quality outputs from MechaFormer serve as excellent starting points for traditional optimizers, creating a hybrid approach that finds superior solutions with remarkable efficiency.</li>
<li><strong>摘要：</strong>设计机械机制以追踪特定的路径是一个经典但臭名昭著的工程问题，其特征是离散拓扑和连续参数的庞大而复杂的搜索空间。我们介绍了Mechaformer，这是一个基于变压器的模型，该模型通过将机制设计视为有条件的序列生成任务来应对这一挑战。我们的模型学会将目标曲线转换为特定于域的语言（DSL）字符串，同时确定了单个统一过程中机制的拓扑和几何参数。机甲形式大大胜过现有的基线，实现了最新的路径匹配准确性，并产生了各种新颖和有效的设计。我们展示了一套抽样策略，可以显着提高解决方案质量并为设计师提供宝贵的灵活性。此外，我们表明，Mechaformer的高质量输出是传统优化器的绝佳起点，创建了一种混合方法，从而找到了具有出色效率的出色解决方案。</li>
</ul>

<h3>Title: Uncertainty-aware Cross-training for Semi-supervised Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Kaiwen Huang, Tao Zhou, Huazhu Fu, Yizhe Zhang, Yi Zhou, Xiao-Jun Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09014">https://arxiv.org/abs/2508.09014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09014">https://arxiv.org/pdf/2508.09014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09014]] Uncertainty-aware Cross-training for Semi-supervised Medical Image Segmentation(https://arxiv.org/abs/2508.09014)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Semi-supervised learning has gained considerable popularity in medical image segmentation tasks due to its capability to reduce reliance on expert-examined annotations. Several mean-teacher (MT) based semi-supervised methods utilize consistency regularization to effectively leverage valuable information from unlabeled data. However, these methods often heavily rely on the student model and overlook the potential impact of cognitive biases within the model. Furthermore, some methods employ co-training using pseudo-labels derived from different inputs, yet generating high-confidence pseudo-labels from perturbed inputs during training remains a significant challenge. In this paper, we propose an Uncertainty-aware Cross-training framework for semi-supervised medical image Segmentation (UC-Seg). Our UC-Seg framework incorporates two distinct subnets to effectively explore and leverage the correlation between them, thereby mitigating cognitive biases within the model. Specifically, we present a Cross-subnet Consistency Preservation (CCP) strategy to enhance feature representation capability and ensure feature consistency across the two subnets. This strategy enables each subnet to correct its own biases and learn shared semantics from both labeled and unlabeled data. Additionally, we propose an Uncertainty-aware Pseudo-label Generation (UPG) component that leverages segmentation results and corresponding uncertainty maps from both subnets to generate high-confidence pseudo-labels. We extensively evaluate the proposed UC-Seg on various medical image segmentation tasks involving different modality images, such as MRI, CT, ultrasound, colonoscopy, and so on. The results demonstrate that our method achieves superior segmentation accuracy and generalization performance compared to other state-of-the-art semi-supervised methods. Our code will be released at this https URL.</li>
<li><strong>摘要：</strong>半监督的学习在医学图像细分任务中获得了很大的普及，因为它有能力减少对专家审查的注释的依赖。几种基于均值的老师（MT）的半监督方法利用一致性正则化来有效利用未标记数据的有价值的信息。但是，这些方法通常在很大程度上依赖学生模型，并忽略了认知偏见在模型中的潜在影响。此外，某些方法采用来自不同输入的伪标签采用共同训练，但在训练过程中从扰动输入中产生了高信心的伪标记仍然是一个重大挑战。在本文中，我们提出了半监督医学图像分割（UC-SEG）的不确定性感知跨训练框架。我们的UC-SEG框架结合了两个不同的子网，以有效地探索和利用它们之间的相关性，从而减轻模型中的认知偏见。具体而言，我们提出了一种跨收益一致性保存（CCP）策略，以增强特征表示能力并确保两个子网的特征一致性。该策略使每个子网能够纠正自己的偏见，并从标记和未标记数据中学习共享语义。此外，我们提出了一个不确定性感知的伪标签生成（UPG）组件，该组件利用分割结果并从两个子网中的相应不确定性图生成高信心伪标签。我们对涉及不同模态图像的各种医学图像分割任务（例如MRI，CT，Ultrasound，Colonsoscopicy等）的各种医学图像分割任务进行了广泛的评估。结果表明，与其他最先进的半监督方法相比，我们的方法可实现卓越的分割精度和概括性能。我们的代码将在此HTTPS URL上发布。</li>
</ul>

<h3>Title: When Deepfakes Look Real: Detecting AI-Generated Faces with Unlabeled Data due to Annotation Challenges</h3>
<ul>
<li><strong>Authors: </strong>Zhiqiang Yang, Renshuai Tao, Xiaolong Zheng, Guodong Yang, Chunjie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09022">https://arxiv.org/abs/2508.09022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09022">https://arxiv.org/pdf/2508.09022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09022]] When Deepfakes Look Real: Detecting AI-Generated Faces with Unlabeled Data due to Annotation Challenges(https://arxiv.org/abs/2508.09022)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing deepfake detection methods heavily depend on labeled training data. However, as AI-generated content becomes increasingly realistic, even \textbf{human annotators struggle to distinguish} between deepfakes and authentic images. This makes the labeling process both time-consuming and less reliable. Specifically, there is a growing demand for approaches that can effectively utilize large-scale unlabeled data from online social networks. Unlike typical unsupervised learning tasks, where categories are distinct, AI-generated faces closely mimic real image distributions and share strong similarities, causing performance drop in conventional strategies. In this paper, we introduce the Dual-Path Guidance Network (DPGNet), to tackle two key challenges: (1) bridging the domain gap between faces from different generation models, and (2) utilizing unlabeled image samples. The method features two core modules: text-guided cross-domain alignment, which uses learnable prompts to unify visual and textual embeddings into a domain-invariant feature space, and curriculum-driven pseudo label generation, which dynamically exploit more informative unlabeled samples. To prevent catastrophic forgetting, we also facilitate bridging between domains via cross-domain knowledge distillation. Extensive experiments on \textbf{11 popular datasets}, show that DPGNet outperforms SoTA approaches by \textbf{6.3\%}, highlighting its effectiveness in leveraging unlabeled data to address the annotation challenges posed by the increasing realism of deepfakes.</li>
<li><strong>摘要：</strong>现有的DeepFake检测方法在很大程度上取决于标记的训练数据。但是，随着AI生成的内容变得越来越现实，即使是\ textbf {人类注释者都难以区分深层图像和真实图像。这使得标签过程既耗时又不可靠。具体而言，对方法的需求不断增长，可以有效地利用来自在线社交网络的大规模未标记数据。与典型的无监督学习任务不同，类别是不同的，AI生成的面孔紧密模仿真实的图像分布并具有强烈的相似性，从而导致传统策略的性能下降。在本文中，我们介绍了双路径指南网络（DPGNET），以应对两个关键挑战：（1）桥接不同生成模型的面部之间的域间隙，以及（2）使用未标记的图像样本。该方法具有两个核心模块：文本引导的跨域对齐，它使用可学习的提示将视觉和文本嵌入统一为域不变的特征空间，以及课程驱动的伪标签生成，该伪驱动的伪标记生成，该生成动态利用了更有信息的无标记的未遗嘱未贴印的样品。为了防止灾难性的遗忘，我们还通过跨域知识蒸馏促进了域之间的桥接。在\ textBf {11流行数据集}上进行的广泛实验表明，DPGNET通过\ textbf {6.3 \％}优于SOTA方法，突出了其在利用未标记的数据方面的有效性，以解决越来越多的深层现实主义现实主义所带来的注释挑战。</li>
</ul>

<h3>Title: Bridging Formal Language with Chain-of-Thought Reasoning to Geometry Problem Solving</h3>
<ul>
<li><strong>Authors: </strong>Tianyun Yang, Yunwen Li, Ziniu Li, Zhihang Lin, Ruoyu Sun, Tian Ding</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09099">https://arxiv.org/abs/2508.09099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09099">https://arxiv.org/pdf/2508.09099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09099]] Bridging Formal Language with Chain-of-Thought Reasoning to Geometry Problem Solving(https://arxiv.org/abs/2508.09099)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large vision language models exhibit notable limitations on Geometry Problem Solving (GPS) because of their unreliable diagram interpretation and pure natural-language reasoning. A recent line of work mitigates this by using symbolic solvers: the model directly generates a formal program that a geometry solver can execute. However, this direct program generation lacks intermediate reasoning, making the decision process opaque and prone to errors. In this work, we explore a new approach that integrates Chain-of-Thought (CoT) with formal language. The model interleaves natural language reasoning with incremental emission of solver-executable code, producing a hybrid reasoning trace in which critical derivations are expressed in formal language. To teach this behavior at scale, we combine (1) supervised fine-tuning on an 11K newly developed synthetic dataset with interleaved natural language reasoning and automatic formalization, and (2) solver-in-the-loop reinforcement learning that jointly optimizes both the CoT narrative and the resulting program through outcome-based rewards. Built on Qwen2.5-VL-7B, our new model, named GF-Reasoner, achieves up to 15% accuracy improvements on standard GPS benchmarks, surpassing both 7B-scale peers and the much larger model Qwen2.5-VL-72B. By exploiting high-order geometric knowledge and offloading symbolic computation to the solver, the generated reasoning traces are noticeably shorter and cleaner. Furthermore, we present a comprehensive analysis of method design choices (e.g., reasoning paradigms, data synthesis, training epochs, etc.), providing actionable insights for future research.</li>
<li><strong>摘要：</strong>大型视觉语言模型由于图表的解释不可靠和纯自然语言推理而显示出对几何问题解决（GP）的明显局限性。最近的工作线通过使用符号求解器来减轻此功能：该模型直接生成一个形式程序，几何求解器可以执行该程序。但是，这种直接的计划产生缺乏中间的推理，使决策过程不透明并容易出现错误。在这项工作中，我们探索了一种将思想链（COT）与形式语言相结合的新方法。该模型将自然语言推理与可求解器的可观代码的逐步发射相结合，从而产生了一种混合推理痕迹，其中以形式的语言表达了关键推导。为了按大规模教授这种行为，我们将（1）在新开发的合成数据集中进行了调节，并通过相互交织的自然语言推理和自动形式化进行了微调，以及（2）在循环的增强式学习中，共同优化了COT叙事和通过基于成果的回报的结果。基于我们的新型号（名为GF-Reasoninger）建立在QWEN2.5-VL-7B上，在标准GPS基准测试方面的准确性提高了15％，超过了7B规模的同行和更大的模型QWEN2.5-VL-72B。通过利用高阶几何知识并将符号计算卸载到求解器中，生成的推理痕迹明显较短和清洁。此外，我们对方法设计选择（例如，推理范式，数据合成，培训时代等）进行了全面分析，为未来的研究提供了可行的见解。</li>
</ul>

<h3>Title: Turbo-VAED: Fast and Stable Transfer of Video-VAEs to Mobile Devices</h3>
<ul>
<li><strong>Authors: </strong>Ya Zou, Jingfeng Yao, Siyuan Yu, Shuai Zhang, Wenyu Liu, Xinggang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.09136">https://arxiv.org/abs/2508.09136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.09136">https://arxiv.org/pdf/2508.09136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.09136]] Turbo-VAED: Fast and Stable Transfer of Video-VAEs to Mobile Devices(https://arxiv.org/abs/2508.09136)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>There is a growing demand for deploying large generative AI models on mobile devices. For recent popular video generative models, however, the Variational AutoEncoder (VAE) represents one of the major computational bottlenecks. Both large parameter sizes and mismatched kernels cause out-of-memory errors or extremely slow inference on mobile devices. To address this, we propose a low-cost solution that efficiently transfers widely used video VAEs to mobile devices. (1) We analyze redundancy in existing VAE architectures and get empirical design insights. By integrating 3D depthwise separable convolutions into our model, we significantly reduce the number of parameters. (2) We observe that the upsampling techniques in mainstream video VAEs are poorly suited to mobile hardware and form the main bottleneck. In response, we propose a decoupled 3D pixel shuffle scheme that slashes end-to-end delay. Building upon these, we develop a universal mobile-oriented VAE decoder, Turbo-VAED. (3) We propose an efficient VAE decoder training method. Since only the decoder is used during deployment, we distill it to Turbo-VAED instead of retraining the full VAE, enabling fast mobile adaptation with minimal performance loss. To our knowledge, our method enables real-time 720p video VAE decoding on mobile devices for the first time. This approach is widely applicable to most video VAEs. When integrated into four representative models, with training cost as low as $95, it accelerates original VAEs by up to 84.5x at 720p resolution on GPUs, uses as low as 17.5% of original parameter count, and retains 96.9% of the original reconstruction quality. Compared to mobile-optimized VAEs, Turbo-VAED achieves a 2.9x speedup in FPS and better reconstruction quality on the iPhone 16 Pro. The code and models will soon be available at this https URL.</li>
<li><strong>摘要：</strong>在移动设备上部署大型生成AI模型的需求不断增长。但是，对于最近流行的视频生成模型，变分自动编码器（VAE）代表了主要的计算瓶颈之一。大型参数大小和不匹配的内核都会导致移动设备上的过度记忆错误或推断极慢。为了解决这个问题，我们提出了一种低成本解决方案，该解决方案有效地将广泛使用的视频VAE传输到移动设备。 （1）我们分析现有VAE架构中的冗余并获得经验设计见解。通过将3D深度可分离卷积整合到我们的模型中，我们可以显着减少参数的数量。 （2）我们观察到主流视频VAE中的升采样技术不适合移动硬件并形成主要瓶颈。作为响应，我们提出了一个脱钩的3D像素散装方案，该方案削减了端到端的延迟。在这些基础上，我们开发了一种以移动移动为导向的Vae解码器，涡轮增压器。 （3）我们提出了一种有效的VAE解码器训练方法。由于在部署过程中仅使用解码器，因此我们将其提炼为涡轮增压，而不是重新训练完整的VAE，从而可以快速移动适应，并且性能损失最少。据我们所知，我们的方法可以首次在移动设备上解码实时720p视频VAE。这种方法广泛适用于大多数视频VAE。当将培训成本低至95美元，将原始VAE纳入四个代表性型号时，以720p分辨率在GPU上以84.5倍的速度加速了原始参数计数的17.5％，并保留了原始重建质量的96.9％。与移动优化的VAE相比，Turbo Vaed在FPS方面达到了2.9倍的速度，并且在iPhone 16 Pro上的重建质量更好。代码和模型将很快在此HTTPS URL上提供。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
