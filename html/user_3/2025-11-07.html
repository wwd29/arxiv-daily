<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-11-07</h1>
<h3>Title: Laugh, Relate, Engage: Stylized Comment Generation for Short Videos</h3>
<ul>
<li><strong>Authors: </strong>Xuan Ouyang, Senan Wang, Bouzhou Wang, Siyuan Xiahou, Jinrong Zhou, Yuekang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.03757">https://arxiv.org/abs/2511.03757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.03757">https://arxiv.org/pdf/2511.03757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.03757]] Laugh, Relate, Engage: Stylized Comment Generation for Short Videos(https://arxiv.org/abs/2511.03757)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Short-video platforms have become a central medium in the modern Internet landscape, where efficient information delivery and strong interactivity are reshaping user engagement and cultural dissemination. Among the various forms of user interaction, comments play a vital role in fostering community participation and enabling content re-creation. However, generating comments that are both compliant with platform guidelines and capable of exhibiting stylistic diversity and contextual awareness remains a significant challenge. We introduce LOLGORITHM, a modular multi-agent system (MAS) designed for controllable short-video comment generation. The system integrates video segmentation, contextual and affective analysis, and style-aware prompt construction. It supports six distinct comment styles: puns (homophones), rhyming, meme application, sarcasm (irony), plain humor, and content extraction. Powered by a multimodal large language model (MLLM), LOLGORITHM directly processes video inputs and achieves fine-grained style control through explicit prompt markers and few-shot examples. To support development and evaluation, we construct a bilingual dataset using official APIs from Douyin (Chinese) and YouTube (English), covering five popular video genres: comedy skits, daily life jokes, funny animal clips, humorous commentary, and talk shows. Evaluation combines automated metrics originality, relevance, and style conformity with a large-scale human preference study involving 40 videos and 105 participants. Results show that LOLGORITHM significantly outperforms baseline models, achieving preference rates of over 90% on Douyin and 87.55% on YouTube. This work presents a scalable and culturally adaptive framework for stylized comment generation on short-video platforms, offering a promising path to enhance user engagement and creative interaction.</li>
<li><strong>摘要：</strong>短视频平台已成为现代互联网格局的核心媒介，高效的信息传递和强大的交互性正在重塑用户参与和文化传播。在各种形式的用户交互中，评论在促进社区参与和内容再创作方面发挥着至关重要的作用。然而，生成既符合平台指南又能够展现风格多样性和情境意识的评论仍然是一个重大挑战。我们推出了 LOLGORITHM，这是一种模块化多智能体系统（MAS），专为可控短视频评论生成而设计。该系统集成了视频分割、上下文和情感分析以及风格感知提示构建。它支持六种不同的评论风格：双关语（同音词）、押韵、模因应用、讽刺（讽刺）、简单幽默和内容提取。 LOLGORITHM 由多模态大语言模型 (MLLM) 提供支持，直接处理视频输入，并通过明确的提示标记和少量示例实现细粒度的风格控制。为了支持开发和评估，我们使用抖音（中文）和 YouTube（英文）的官方 API 构建了双语数据集，涵盖五种流行的视频类型：喜剧短剧、日常生活笑话、搞笑动物片段、幽默评论和脱口秀。评估将自动化指标的原创性、相关性和风格一致性与涉及 40 个视频和 105 名参与者的大规模人类偏好研究结合起来。结果显示，LOLGORITHM 的性能显着优于基线模型，在抖音上的偏好率超过 90%，在 YouTube 上的偏好率达到 87.55%。这项工作为短视频平台上的风格化评论生成提供了一个可扩展且具有文化适应性的框架，为增强用户参与度和创造性互动提供了一条有希望的途径。</li>
</ul>

<h3>Title: Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model</h3>
<ul>
<li><strong>Authors: </strong>Abdulmumin Sa'ad, Sulaimon Oyeniyi Adebayo, Abdul Jabbar Siddiqui</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.03888">https://arxiv.org/abs/2511.03888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.03888">https://arxiv.org/pdf/2511.03888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.03888]] Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model(https://arxiv.org/abs/2511.03888)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The global waste crisis is escalating, with solid waste generation expected to increase by 70% by 2050. Traditional waste collection methods, particularly in remote or harsh environments like deserts, are labor-intensive, inefficient, and often hazardous. Recent advances in computer vision and deep learning have opened the door to automated waste detection systems, yet most research focuses on urban environments and recyclable materials, overlooking organic and hazardous waste and underexplored terrains such as deserts. In this work, we propose an enhanced real-time object detection framework based on a pruned, lightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT) and specialized data augmentation strategies. Using the DroneTrashNet dataset, we demonstrate significant improvements in precision, recall, and mean average precision (mAP), while achieving low latency and compact model size suitable for deployment on resource-constrained aerial drones. Benchmarking our model against state-of-the-art lightweight YOLO variants further highlights its optimal balance of accuracy and efficiency. Our results validate the effectiveness of combining data-centric and model-centric enhancements for robust, real-time waste detection in desert environments.</li>
<li><strong>摘要：</strong>全球废物危机正在升级，预计到 2050 年，固体废物产生量将增加 70%。传统的废物收集方法，特别是在沙漠等偏远或恶劣环境中，是劳动密集型、低效的，而且往往很危险。计算机视觉和深度学习的最新进展为自动废物检测系统打开了大门，但大多数研究都集中在城市环境和可回收材料上，忽视了有机和危险废物以及沙漠等未开发的地形。在这项工作中，我们提出了一种增强的实时目标检测框架，该框架基于经过修剪的轻量级 YOLOv12 版本，并与自我对抗训练 (SAT) 和专门的数据增强策略相集成。使用 DroneTrashNet 数据集，我们展示了精度、召回率和平均精度 (mAP) 方面的显着改进，同时实现了低延迟和紧凑的模型大小，适合部署在资源有限的空中无人机上。将我们的模型与最先进的轻量级 YOLO 变体进行基准测试，进一步凸显了其准确性和效率的最佳平衡。我们的结果验证了结合以数据为中心和以模型为中心的增强功能在沙漠环境中进行稳健、实时废物检测的有效性。</li>
</ul>

<h3>Title: I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Nand Kumar Yadav, Rodrigue Rizk, William CW Chen, KC Santosh (AI Research Lab, Department of Computer Science and Biomedical and Translational Sciences, Sanford School of Medicine, University Of South Dakota, Vermillion, SD, USA)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.03912">https://arxiv.org/abs/2511.03912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.03912">https://arxiv.org/pdf/2511.03912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.03912]] I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging(https://arxiv.org/abs/2511.03912)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Unknown anomaly detection in medical imaging remains a fundamental challenge due to the scarcity of labeled anomalies and the high cost of expert supervision. We introduce an unsupervised, oracle-free framework that incrementally expands a trusted set of normal samples without any anomaly labels. Starting from a small, verified seed of normal images, our method alternates between lightweight adapter updates and uncertainty-gated sample admission. A frozen pretrained vision backbone is augmented with tiny convolutional adapters, ensuring rapid domain adaptation with negligible computational overhead. Extracted embeddings are stored in a compact coreset enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during incremental expansion is enforced by dual probabilistic gates, a sample is admitted into the normal memory only if its distance to the existing coreset lies within a calibrated z-score threshold, and its SWAG-based epistemic uncertainty remains below a seed-calibrated bound. This mechanism prevents drift and false inclusions without relying on generative reconstruction or replay buffers. Empirically, our system steadily refines the notion of normality as unlabeled data arrive, producing substantial gains over baselines. On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5, ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These results highlight the effectiveness and efficiency of the proposed framework for real-world, label-scarce medical imaging applications.</li>
<li><strong>摘要：</strong>由于标记异常的稀缺性和专家监督的高昂成本，医学成像中的未知异常检测仍然是一个基本挑战。我们引入了一个无监督、无预言机的框架，该框架逐步扩展一组可信的正常样本，没有任何异常标签。从一个小的、经过验证的正常图像种子开始，我们的方法在轻量级适配器更新和不确定性门控样本准入之间交替。冻结的预训练视觉主干通过微小的卷积适配器进行增强，确保快速域适应，而计算开销可以忽略不计。提取的嵌入存储在紧凑的核心集中，可实现高效的 k 最近邻异常 (k-NN) 评分。增量扩展期间的安全性由双概率门强制执行，仅当样本与现有核心集的距离位于校准的 z 分数阈值内时，样本才会被允许进入正常内存，并且其基于 SWAG 的认知不确定性仍低于种子校准的界限。这种机制可以防止漂移和错误包含，而不依赖于生成重建或重播缓冲区。根据经验，随着未标记数据的到来，我们的系统稳步完善了正态性的概念，从而产生了比基线更大的收益。在 COVID-CXR 上，ROC-AUC 从 0.9489 提高到 0.9982（F1：0.8048 到 0.9746）；对于肺炎 CXR，ROC-AUC 从 0.6834 上升至 0.8968；在脑 MRI ND-5 上，ROC-AUC 从 0.6041 增加到 0.7269，PR-AUC 从 0.7539 增加到 0.8211。这些结果凸显了所提出的框架对于现实世界、标签稀缺的医学成像应用的有效性和效率。</li>
</ul>

<h3>Title: Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images</h3>
<ul>
<li><strong>Authors: </strong>Sam Bahrami, Dylan Campbell</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.03970">https://arxiv.org/abs/2511.03970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.03970">https://arxiv.org/pdf/2511.03970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.03970]] Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images(https://arxiv.org/abs/2511.03970)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modern scene reconstruction methods are able to accurately recover 3D surfaces that are visible in one or more images. However, this leads to incomplete reconstructions, missing all occluded surfaces. While much progress has been made on reconstructing entire objects given partial observations using generative models, the structural elements of a scene, like the walls, floors and ceilings, have received less attention. We argue that these scene elements should be relatively easy to predict, since they are typically planar, repetitive and simple, and so less costly approaches may be suitable. In this work, we present a synthetic dataset -- Room Envelopes -- that facilitates progress on this task by providing a set of RGB images and two associated pointmaps for each image: one capturing the visible surface and one capturing the first surface once fittings and fixtures are removed, that is, the structural layout. As we show, this enables direct supervision for feed-forward monocular geometry estimators that predict both the first visible surface and the first layout surface. This confers an understanding of the scene's extent, as well as the shape and location of its objects.</li>
<li><strong>摘要：</strong>现代场景重建方法能够准确地恢复一幅或多幅图像中可见的 3D 表面。然而，这会导致重建不完整，丢失所有被遮挡的表面。虽然在使用生成模型进行部分观察来重建整个对象方面已经取得了很大进展，但场景的结构元素（如墙壁、地板和天花板）受到的关注较少。我们认为这些场景元素应该相对容易预测，因为它们通常是平面的、重复的和简单的，因此成本较低的方法可能是合适的。在这项工作中，我们提出了一个合成数据集 - Room Envelopes - 通过提供一组 RGB 图像和每个图像的两个关联点图来促进此任务的进展：一个捕获可见表面，一个捕获移除配件和固定装置后的第一个表面，即结构布局。正如我们所展示的，这使得能够直接监督前馈单目几何估计器，从而预测第一个可见表面和第一个布局表面。这有助于了解场景的范围以及其对象的形状和位置。</li>
</ul>

<h3>Title: PETRA: Pretrained Evolutionary Transformer for SARS-CoV-2 Mutation Prediction</h3>
<ul>
<li><strong>Authors: </strong>Xu Zou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.03976">https://arxiv.org/abs/2511.03976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.03976">https://arxiv.org/pdf/2511.03976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.03976]] PETRA: Pretrained Evolutionary Transformer for SARS-CoV-2 Mutation Prediction(https://arxiv.org/abs/2511.03976)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Since its emergence, SARS-CoV-2 has demonstrated a rapid and unpredictable evolutionary trajectory, characterized by the continual emergence of immune-evasive variants. This poses persistent challenges to public health and vaccine development. While large-scale generative pre-trained transformers (GPTs) have revolutionized the modeling of sequential data, their direct applications to noisy viral genomic sequences are limited. In this paper, we introduce PETRA(Pretrained Evolutionary TRAnsformer), a novel transformer approach based on evolutionary trajectories derived from phylogenetic trees rather than raw RNA sequences. This method effectively mitigates sequencing noise and captures the hierarchical structure of viral evolution. With a weighted training framework to address substantial geographical and temporal imbalances in global sequence data, PETRA excels in predicting future SARS-CoV-2 mutations, achieving a weighted recall@1 of 9.45% for nucleotide mutations and 17.10\% for spike amino-acid mutations, compared to 0.49% and 6.64% respectively for the best baseline. PETRA also demonstrates its ability to aid in the real-time mutation prediction of major clades like 24F(XEC) and 25A(LP.8.1). The code is open sourced on this https URL</li>
<li><strong>摘要：</strong>自出现以来，SARS-CoV-2 表现出快速且不可预测的进化轨迹，其特点是不断出现免疫逃避变异体。这对公共卫生和疫苗开发提出了持续的挑战。虽然大规模生成预训练变压器（GPT）彻底改变了序列数据的建模，但它们在嘈杂的病毒基因组序列中的直接应用是有限的。在本文中，我们介绍了 PETRA（预训练进化 TRansformer），这是一种基于系统发育树而非原始 RNA 序列的进化轨迹的新型 Transformer 方法。该方法有效地减轻了测序噪音并捕获了病毒进化的层次结构。 PETRA 借助加权训练框架来解决全球序列数据中严重的地理和时间不平衡问题，在预测未来 SARS-CoV-2 突变方面表现出色，实现了核苷酸突变的加权召回率@1 为 9.45%，尖峰氨基酸突变的加权召回率为 17.10\%，而最佳基线的加权召回率分别为 0.49% 和 6.64%。 PETRA 还展示了其帮助实时预测主要进化枝（如 24F(XEC) 和 25A(LP.8.1)）突变的能力。该代码在此 https URL 上开源</li>
</ul>

<h3>Title: PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection</h3>
<ul>
<li><strong>Authors: </strong>Peiyao Wang, Weining Wang, Qi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.03997">https://arxiv.org/abs/2511.03997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.03997">https://arxiv.org/pdf/2511.03997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.03997]] PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection(https://arxiv.org/abs/2511.03997)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-video generation have achieved impressive perceptual quality, yet generated content often violates fundamental principles of physical plausibility - manifesting as implausible object dynamics, incoherent interactions, and unrealistic motion patterns. Such failures hinder the deployment of video generation models in embodied AI, robotics, and simulation-intensive domains. To bridge this gap, we propose PhysCorr, a unified framework for modeling, evaluating, and optimizing physical consistency in video generation. Specifically, we introduce PhysicsRM, the first dual-dimensional reward model that quantifies both intra-object stability and inter-object interactions. On this foundation, we develop PhyDPO, a novel direct preference optimization pipeline that leverages contrastive feedback and physics-aware reweighting to guide generation toward physically coherent outputs. Our approach is model-agnostic and scalable, enabling seamless integration into a wide range of video diffusion and transformer-based backbones. Extensive experiments across multiple benchmarks demonstrate that PhysCorr achieves significant improvements in physical realism while preserving visual fidelity and semantic alignment. This work takes a critical step toward physically grounded and trustworthy video generation.</li>
<li><strong>摘要：</strong>文本到视频生成的最新进展已经实现了令人印象深刻的感知质量，但生成的内容常常违反物理合理性的基本原则 - 表现为不可信的对象动力学、不连贯的交互和不切实际的运动模式。此类失败阻碍了视频生成模型在具体人工智能、机器人和模拟密集型领域的部署。为了弥补这一差距，我们提出了 PhysCorr，这是一个用于建模、评估和优化视频生成中物理一致性的统一框架。具体来说，我们引入了PhysicsRM，这是第一个量化对象内稳定性和对象间交互的双维奖励模型。在此基础上，我们开发了 PhyDPO，这是一种新颖的直接偏好优化管道，它利用对比反馈和物理感知重新加权来指导生成物理相干的输出。我们的方法与模型无关且可扩展，能够无缝集成到各种视频传播和基于变压器的骨干网中。跨多个基准的大量实验表明，PhysCorr 在保持视觉保真度和语义对齐的同时，在物理真实感方面取得了显着改进。这项工作朝着物理基础和值得信赖的视频生成迈出了关键的一步。</li>
</ul>

<h3>Title: Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations</h3>
<ul>
<li><strong>Authors: </strong>Kyaw Hpone Myint, Zhe Wu, Alexandre G.R. Day, Giri Iyengar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04000">https://arxiv.org/abs/2511.04000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04000">https://arxiv.org/pdf/2511.04000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04000]] Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations(https://arxiv.org/abs/2511.04000)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Decision trees are widely used in high-stakes fields like finance and healthcare due to their interpretability. This work introduces an efficient, scalable method for generating synthetic pre-training data to enable meta-learning of decision trees. Our approach samples near-optimal decision trees synthetically, creating large-scale, realistic datasets. Using the MetaTree transformer architecture, we demonstrate that this method achieves performance comparable to pre-training on real-world data or with computationally expensive optimal decision trees. This strategy significantly reduces computational costs, enhances data generation flexibility, and paves the way for scalable and efficient meta-learning of interpretable decision tree models.</li>
<li><strong>摘要：</strong>决策树由于其可解释性而被广泛应用于金融和医疗保健等高风险领域。这项工作引入了一种高效、可扩展的方法，用于生成合成预训练数据，以实现决策树的元学习。我们的方法对接近最优决策树进行综合采样，创建大规模、真实的数据集。使用 MetaTree 转换器架构，我们证明了该方法的性能可与现实世界数据的预训练或计算成本昂贵的最优决策树相媲美。该策略显着降低了计算成本，增强了数据生成灵活性，并为可解释决策树模型的可扩展且高效的元学习铺平了道路。</li>
</ul>

<h3>Title: Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing</h3>
<ul>
<li><strong>Authors: </strong>Mingyu Sung, Vikas Palakonda, Suhwan Im, Sunghwan Moon, Il-Min Kim, Sangseok Yun, Jae-Mo Kang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04002">https://arxiv.org/abs/2511.04002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04002">https://arxiv.org/pdf/2511.04002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04002]] Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing(https://arxiv.org/abs/2511.04002)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved near-human performance across diverse reasoning tasks, yet their deployment on resource-constrained Internet-of-Things (IoT) devices remains impractical due to massive parameter footprints and memory-intensive autoregressive decoding. While split computing offers a promising solution by partitioning model execution between edge devices and cloud servers, existing approaches fail to address the unique challenges of autoregressive inference, particularly the iterative token generation process and expanding key-value (KV) cache requirements. This work introduces the first autoregressive-aware split computing framework designed explicitly for LLM deployment on edge devices. Our approach makes three key contributions. First, we develop one-point split compression (OPSC), a mixed-precision quantization scheme that prevents out-of-memory failures by strategically partitioning models into front-end and back-end segments with different precision levels. Second, we propose a two-stage intermediate compression pipeline that combines threshold splitting (TS) and token-wise adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations while dramatically reducing communication overhead. Third, we formulate a unified optimization framework that jointly selects optimal split points, quantization settings, and sequence lengths to satisfy strict memory and latency constraints. Extensive evaluations across diverse LLMs and hardware platforms demonstrate superior performance compared to state-of-the-art quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework achieves a 1.49 inference speedup and significant communication overhead reduction while maintaining or improving model accuracy.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种推理任务中已经实现了接近人类的性能，但由于大量参数占用和内存密集型自回归解码，它们在资源有限的物联网 (IoT) 设备上的部署仍然不切实际。虽然分割计算通过在边缘设备和云服务器之间划分模型执行来提供有前景的解决方案，但现有方法无法解决自回归推理的独特挑战，特别是迭代令牌生成过程和扩展键值 (KV) 缓存要求。这项工作引入了第一个专为边缘设备上的 LLM 部署而设计的自回归感知分割计算框架。我们的方法做出了三个关键贡献。首先，我们开发了单点分割压缩（OPSC），这是一种混合精度量化方案，通过策略性地将模型划分为不同精度级别的前端和后端段来防止内存不足故障。其次，我们提出了一种两级中间压缩管道，它结合了阈值分割（TS）和令牌式自适应位量化（TAB-Q），以保留精度关键的激活，同时显着减少通信开销。第三，我们制定了一个统一的优化框架，联合选择最佳分割点、量化设置和序列长度，以满足严格的内存和延迟约束。对不同法学硕士和硬件平台的广泛评估表明，与最先进的量化方法（包括 SmoothQuant、OmniQuant 和 Atom）相比，其具有卓越的性能。该框架实现了 1.49 的推理加速并显着降低了通信开销，同时保持或提高了模型准确性。</li>
</ul>

<h3>Title: Near-Lossless 3D Voxel Representation Free from Iso-surface</h3>
<ul>
<li><strong>Authors: </strong>Yihao Luo, Xianglong He, Chuanyu Pan, Yiwen Chen, Jiaqi Wu, Yangguang Li, Wanli Ouyang, Yuanming Hu, Guang Yang, ChoonHwai Yap</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04029">https://arxiv.org/abs/2511.04029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04029">https://arxiv.org/pdf/2511.04029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04029]] Near-Lossless 3D Voxel Representation Free from Iso-surface(https://arxiv.org/abs/2511.04029)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accurate and efficient voxelized representations of 3D meshes are the foundation of 3D reconstruction and generation. However, existing representations based on iso-surface heavily rely on water-tightening or rendering optimization, which inevitably compromise geometric fidelity. We propose Faithful Contouring, a sparse voxelized representation that supports 2048+ resolutions for arbitrary meshes, requiring neither converting meshes to field functions nor extracting the isosurface during remeshing. It achieves near-lossless fidelity by preserving sharpness and internal structures, even for challenging cases with complex geometry and topology. The proposed method also shows flexibility for texturing, manipulation, and editing. Beyond representation, we design a dual-mode autoencoder for Faithful Contouring, enabling scalable and detail-preserving shape reconstruction. Extensive experiments show that Faithful Contouring surpasses existing methods in accuracy and efficiency for both representation and reconstruction. For direct representation, it achieves distance errors at the $10^{-5}$ level; for mesh reconstruction, it yields a 93\% reduction in Chamfer Distance and a 35\% improvement in F-score over strong baselines, confirming superior fidelity as a representation for 3D learning tasks.</li>
<li><strong>摘要：</strong>准确高效的 3D 网格体素化表示是 3D 重建和生成的基础。然而，现有的基于等值面的表示严重依赖水密或渲染优化，这不可避免地会损害几何保真度。我们提出了 Faithful Contouring，这是一种稀疏体素化表示，支持任意网格的 2048+ 分辨率，不需要将网格转换为场函数，也不需要在重新网格划分期间提取等值面。它通过保留清晰度和内部结构来实现近乎无损的保真度，即使对于具有复杂几何和拓扑的挑战性情况也是如此。所提出的方法还显示了纹理、操作和编辑的灵活性。除了表示之外，我们还设计了用于忠实轮廓的双模式自动编码器，从而实现可扩展且保留细节的形状重建。大量实验表明，忠实轮廓在表示和重建方面的准确性和效率方面均优于现有方法。对于直接表示，它实现了 $10^{-5}$ 级别的距离误差；对于网格重建，与强基线相比，倒角距离减少了 93%，F 分数提高了 35%，证实了作为 3D 学习任务表示的卓越保真度。</li>
</ul>

<h3>Title: SpatialLock: Precise Spatial Control in Text-to-Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Biao Liu, Yuanzhi Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04112">https://arxiv.org/abs/2511.04112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04112">https://arxiv.org/pdf/2511.04112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04112]] SpatialLock: Precise Spatial Control in Text-to-Image Synthesis(https://arxiv.org/abs/2511.04112)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-Image (T2I) synthesis has made significant advancements in recent years, driving applications such as generating datasets automatically. However, precise control over object localization in generated images remains a challenge. Existing methods fail to fully utilize positional information, leading to an inadequate understanding of object spatial layouts. To address this issue, we propose SpatialLock, a novel framework that leverages perception signals and grounding information to jointly control the generation of spatial locations. SpatialLock incorporates two components: Position-Engaged Injection (PoI) and Position-Guided Learning (PoG). PoI directly integrates spatial information through an attention layer, encouraging the model to learn the grounding information effectively. PoG employs perception-based supervision to further refine object localization. Together, these components enable the model to generate objects with precise spatial arrangements and improve the visual quality of the generated images. Experiments show that SpatialLock sets a new state-of-the-art for precise object positioning, achieving IOU scores above 0.9 across multiple datasets.</li>
<li><strong>摘要：</strong>近年来，文本到图像 (T2I) 合成取得了重大进展，推动了自动生成数据集等应用。然而，对生成图像中的对象定位进行精确控制仍然是一个挑战。现有方法未能充分利用位置信息，导致对对象空间布局的理解不足。为了解决这个问题，我们提出了 SpatialLock，这是一种利用感知信号和接地信息来共同控制空间位置生成的新颖框架。 SpatialLock 包含两个组件：位置接合注射 (PoI) 和位置引导学习 (PoG)。 PoI 通过注意力层直接整合空间信息，鼓励模型有效地学习基础信息。 PoG 采用基于感知的监督来进一步完善对象定位。这些组件共同使模型能够生成具有精确空间排列的对象，并提高生成图像的视觉质量。实验表明，SpatialLock 为精确对象定位设定了新的最先进技术，在多个数据集上实现了 0.9 以上的 IOU 分数。</li>
</ul>

<h3>Title: Tortoise and Hare Guidance: Accelerating Diffusion Model Inference with Multirate Integration</h3>
<ul>
<li><strong>Authors: </strong>Yunghee Lee, Byeonghyun Pak, Junwha Hong, Hoseong Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04117">https://arxiv.org/abs/2511.04117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04117">https://arxiv.org/pdf/2511.04117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04117]] Tortoise and Hare Guidance: Accelerating Diffusion Model Inference with Multirate Integration(https://arxiv.org/abs/2511.04117)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In this paper, we propose Tortoise and Hare Guidance (THG), a training-free strategy that accelerates diffusion sampling while maintaining high-fidelity generation. We demonstrate that the noise estimate and the additional guidance term exhibit markedly different sensitivity to numerical error by reformulating the classifier-free guidance (CFG) ODE as a multirate system of ODEs. Our error-bound analysis shows that the additional guidance branch is more robust to approximation, revealing substantial redundancy that conventional solvers fail to exploit. Building on this insight, THG significantly reduces the computation of the additional guidance: the noise estimate is integrated with the tortoise equation on the original, fine-grained timestep grid, while the additional guidance is integrated with the hare equation only on a coarse grid. We also introduce (i) an error-bound-aware timestep sampler that adaptively selects step sizes and (ii) a guidance-scale scheduler that stabilizes large extrapolation spans. THG reduces the number of function evaluations (NFE) by up to 30% with virtually no loss in generation fidelity ($\Delta$ImageReward $\leq$ 0.032) and outperforms state-of-the-art CFG-based training-free accelerators under identical computation budgets. Our findings highlight the potential of multirate formulations for diffusion solvers, paving the way for real-time high-quality image synthesis without any model retraining. The source code is available at this https URL.</li>
<li><strong>摘要：</strong>在本文中，我们提出了龟兔指导（THG），这是一种无需训练的策略，可以加速扩散采样，同时保持高保真生成。我们通过将无分类器制导 (CFG) ODE 重新表述为 ODE 的多速率系统，证明噪声估计和附加制导项对数值误差表现出明显不同的敏感性。我们的误差界限分析表明，附加指导分支对近似更加稳健，揭示了传统求解器无法利用的大量冗余。基于这一见解，THG 显着减少了附加指导的计算：噪声估计与原始细粒度时间步网格上的乌龟方程集成，而附加指导仅在粗网格上与野兔方程集成。我们还引入了（i）一个可自适应选择步长的误差界限感知时间步采样器，以及（ii）一个可稳定大外推跨度的指导规模调度器。 THG 将功能评估 (NFE) 的数量减少了 30%，几乎没有损失生成保真度 ($\Delta$ImageReward $\leq$ 0.032)，并且在相同的计算预算下优于最先进的基于 CFG 的免训练加速器。我们的研究结果凸显了扩散求解器多速率公式的潜力，为无需任何模型重新训练的实时高质量图像合成铺平了道路。源代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Text to Sketch Generation with Multi-Styles</h3>
<ul>
<li><strong>Authors: </strong>Tengjie Li, Shikui Tu, Lei Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04123">https://arxiv.org/abs/2511.04123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04123">https://arxiv.org/pdf/2511.04123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04123]] Text to Sketch Generation with Multi-Styles(https://arxiv.org/abs/2511.04123)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in vision-language models have facilitated progress in sketch generation. However, existing specialized methods primarily focus on generic synthesis and lack mechanisms for precise control over sketch styles. In this work, we propose a training-free framework based on diffusion models that enables explicit style guidance via textual prompts and referenced style sketches. Unlike previous style transfer methods that overwrite key and value matrices in self-attention, we incorporate the reference features as auxiliary information with linear smoothing and leverage a style-content guidance mechanism. This design effectively reduces content leakage from reference sketches and enhances synthesis quality, especially in cases with low structural similarity between reference and target sketches. Furthermore, we extend our framework to support controllable multi-style generation by integrating features from multiple reference sketches, coordinated via a joint AdaIN module. Extensive experiments demonstrate that our approach achieves high-quality sketch generation with accurate style alignment and improved flexibility in style control. The official implementation of M3S is available at this https URL.</li>
<li><strong>摘要：</strong>视觉语言模型的最新进展促进了草图生成的进展。然而，现有的专业方法主要侧重于通用合成，缺乏对草图样式的精确控制机制。在这项工作中，我们提出了一个基于扩散模型的免训练框架，该框架可以通过文本提示和参考风格草图实现明确的风格指导。与之前在自注意力中覆盖键和值矩阵的风格转移方法不同，我们将参考特征作为辅助信息与线性平滑结合起来，并利用风格内容引导机制。这种设计有效地减少了参考草图的内容泄漏并提高了合成质量，特别是在参考草图和目标草图之间结构相似度较低的情况下。此外，我们通过集成来自多个参考草图的特征（通过联合 AdaIN 模块进行协调）来扩展我们的框架，以支持可控的多样式生成。大量的实验表明，我们的方法可以通过准确的样式对齐和改进的样式控制灵活性来实现高质量的草图生成。 M3S 的官方实现可在此 https URL 获取。</li>
</ul>

<h3>Title: Exploring the Feasibility of End-to-End Large Language Model as a Compiler</h3>
<ul>
<li><strong>Authors: </strong>Hongbin Zhang, Shihao Gao, Yang Liu, Mingjie Xing, Yanjun Wu, Chen Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04132">https://arxiv.org/abs/2511.04132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04132">https://arxiv.org/pdf/2511.04132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04132]] Exploring the Feasibility of End-to-End Large Language Model as a Compiler(https://arxiv.org/abs/2511.04132)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In recent years, end-to-end Large Language Model (LLM) technology has shown substantial advantages across various domains. As critical system software and infrastructure, compilers are responsible for transforming source code into target code. While LLMs have been leveraged to assist in compiler development and maintenance, their potential as an end-to-end compiler remains largely unexplored. This paper explores the feasibility of LLM as a Compiler (LaaC) and its future directions. We designed the CompilerEval dataset and framework specifically to evaluate the capabilities of mainstream LLMs in source code comprehension and assembly code generation. In the evaluation, we analyzed various errors, explored multiple methods to improve LLM-generated code, and evaluated cross-platform compilation capabilities. Experimental results demonstrate that LLMs exhibit basic capabilities as compilers but currently achieve low compilation success rates. By optimizing prompts, scaling up the model, and incorporating reasoning methods, the quality of assembly code generated by LLMs can be significantly enhanced. Based on these findings, we maintain an optimistic outlook for LaaC and propose practical architectural designs and future research directions. We believe that with targeted training, knowledge-rich prompts, and specialized infrastructure, LaaC has the potential to generate high-quality assembly code and drive a paradigm shift in the field of compilation.</li>
<li><strong>摘要：</strong>近年来，端到端大语言模型（LLM）技术在各个领域都显示出了巨大的优势。作为关键的系统软件和基础设施，编译器负责将源代码转换为目标代码。虽然法学硕士已被用来协助编译器开发和维护，但它们作为端到端编译器的潜力在很大程度上仍未得到开发。本文探讨了LLM作为编译器（LaaC）的可行性及其未来发展方向。我们专门设计了CompilerEval数据集和框架来评估主流LLM在源代码理解和汇编代码生成方面的能力。在评估中，我们分析了各种错误，探索了多种改进LLM生成代码的方法，并评估了跨平台编译能力。实验结果表明，LLM 具有作为编译器的基本功能，但目前编译成功率较低。通过优化提示、扩展模型并结合推理方法，可以显着提高法学硕士生成的汇编代码的质量。基于这些发现，我们对 LaaC 保持乐观的前景，并提出实用的架构设计和未来的研究方向。我们相信，通过有针对性的培训、丰富的知识提示和专门的基础设施，LaaC 有潜力生成高质量的汇编代码并推动编译领域的范式转变。</li>
</ul>

<h3>Title: Learning to Land Anywhere: Transferable Generative Models for Aircraft Trajectories</h3>
<ul>
<li><strong>Authors: </strong>Olav Finne Praesteng Larsen, Massimiliano Ruocco, Michail Spitieris, Abdulmajid Murad, Martina Ragosta</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04155">https://arxiv.org/abs/2511.04155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04155">https://arxiv.org/pdf/2511.04155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04155]] Learning to Land Anywhere: Transferable Generative Models for Aircraft Trajectories(https://arxiv.org/abs/2511.04155)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Access to trajectory data is a key requirement for developing and validating Air Traffic Management (ATM) solutions, yet many secondary and regional airports face severe data scarcity. This limits the applicability of machine learning methods and the ability to perform large-scale simulations or "what-if" analyses. In this paper, we investigate whether generative models trained on data-rich airports can be efficiently adapted to data-scarce airports using transfer learning. We adapt state-of-the-art diffusion- and flow-matching-based architectures to the aviation domain and evaluate their transferability between Zurich (source) and Dublin (target) landing trajectory datasets. Models are pretrained on Zurich and fine-tuned on Dublin with varying amounts of local data, ranging from 0% to 100%. Results show that diffusion-based models achieve competitive performance with as little as 5% of the Dublin data and reach baseline-level performance around 20%, consistently outperforming models trained from scratch across metrics and visual inspections. Latent flow matching and latent diffusion models also benefit from pretraining, though with more variable gains, while flow matching models show weaker generalization. Despite challenges in capturing rare trajectory patterns, these findings demonstrate the potential of transfer learning to substantially reduce data requirements for trajectory generation in ATM, enabling realistic synthetic data generation even in environments with limited historical records.</li>
<li><strong>摘要：</strong>获取轨迹数据是开发和验证空中交通管理 (ATM) 解决方案的关键要求，但许多二级机场和支线机场面临着严重的数据稀缺问题。这限制了机器学习方法的适用性以及执行大规模模拟或“假设”分析的能力。在本文中，我们研究了在数据丰富的机场上训练的生成模型是否可以使用迁移学习有效地适应数据稀缺的机场。我们将最先进的基于扩散和流匹配的架构应用于航空领域，并评估它们在苏黎世（源）和都柏林（目标）着陆轨迹数据集之间的可转移性。模型在苏黎世进行预训练，并在都柏林使用不同数量的本地数据（范围从 0% 到 100%）进行微调。结果表明，基于扩散的模型只需 5% 的都柏林数据即可实现具有竞争力的性能，并达到 20% 左右的基线水平性能，在指标和视觉检查方面始终优于从头开始训练的模型。潜在流匹配和潜在扩散模型也受益于预训练，尽管具有更多的可变增益，而流匹配模型的泛化能力较弱。尽管在捕获罕见轨迹模式方面存在挑战，但这些发现证明了迁移学习在大幅减少 ATM 中轨迹生成的数据需求方面的潜力，即使在历史记录有限的环境中也能生成真实的合成数据。</li>
</ul>

<h3>Title: Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery</h3>
<ul>
<li><strong>Authors: </strong>Claudio Giusti, Luca Guarnera, Sebastiano Battiato</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04260">https://arxiv.org/abs/2511.04260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04260">https://arxiv.org/pdf/2511.04260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04260]] Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery(https://arxiv.org/abs/2511.04260)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The growing sophistication of synthetic image and deepfake generation models has turned source attribution and authenticity verification into a critical challenge for modern computer vision systems. Recent studies suggest that diffusion pipelines unintentionally imprint persistent statistical traces, known as signal leaks, within their outputs, particularly in latent representations. Building on this observation, we propose Proto-LeakNet, a signal-leak-aware and interpretable attribution framework that integrates closed-set classification with a density-based open-set evaluation on the learned embeddings, enabling analysis of unseen generators without retraining. Operating in the latent domain of diffusion models, our method re-simulates partial forward diffusion to expose residual generator-specific cues. A temporal attention encoder aggregates multi-step latent features, while a feature-weighted prototype head structures the embedding space and enables transparent attribution. Trained solely on closed data and achieving a Macro AUC of 98.13%, Proto-LeakNet learns a latent geometry that remains robust under post-processing, surpassing state-of-the-art methods, and achieves strong separability between known and unseen generators. These results demonstrate that modeling signal-leak bias in latent space enables reliable and interpretable AI-image and deepfake forensics. The code for the whole work will be available upon submission.</li>
<li><strong>摘要：</strong>合成图像和深度伪造生成模型的日益复杂，使得来源归属和真实性验证成为现代计算机视觉系统的关键挑战。最近的研究表明，扩散管道无意中在其输出中留下了持久的统计痕迹，称为信号泄漏，特别是在潜在表示中。基于这一观察，我们提出了 Proto-LeakNet，这是一种信号泄漏感知和可解释的归因框架，它将闭集分类与对学习嵌入的基于密度的开放集评估相结合，无需重新训练即可分析看不见的生成器。在扩散模型的潜在域中操作，我们的方法重新模拟部分前向扩散以暴露残留的特定于生成器的线索。时间注意力编码器聚合多步潜在特征，而特征加权原型头构建嵌入空间并实现透明归因。 Proto-LeakNet 仅基于封闭数据进行训练，并实现了 98.13% 的宏 AUC，它学习了一种在后处理下仍然保持鲁棒性的潜在几何结构，超越了最先进的方法，并在已知和不可见的生成器之间实现了强大的可分离性。这些结果表明，对潜在空间中的信号泄漏偏差进行建模可以实现可靠且可解释的人工智能图像和深度伪造取证。整个作品的代码将在提交后提供。</li>
</ul>

<h3>Title: Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data</h3>
<ul>
<li><strong>Authors: </strong>Robin Spanier, Thorsten Hoeser, Claudia Kuenzer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04304">https://arxiv.org/abs/2511.04304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04304">https://arxiv.org/pdf/2511.04304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04304]] Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data(https://arxiv.org/abs/2511.04304)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The recent and ongoing expansion of marine infrastructure, including offshore wind farms, oil and gas platforms, artificial islands, and aquaculture facilities, highlights the need for effective monitoring systems. The development of robust models for offshore infrastructure detection relies on comprehensive, balanced datasets, but falls short when samples are scarce, particularly for underrepresented object classes, shapes, and sizes. By training deep learning-based YOLOv10 object detection models with a combination of synthetic and real Sentinel-1 satellite imagery acquired in the fourth quarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of Guinea, and Coast of Brazil), this study investigates the use of synthetic training data to enhance model performance. We evaluated this approach by applying the model to detect offshore platforms in three unseen regions (Gulf of Mexico, North Sea, Persian Gulf) and thereby assess geographic transferability. This region-holdout evaluation demonstrated that the model generalises beyond the training areas. In total, 3,529 offshore platforms were detected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and 1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which improved to 0.90 upon incorporating synthetic data. We analysed how synthetic data enhances the representation of unbalanced classes and overall model performance, taking a first step toward globally transferable detection of offshore infrastructure. This study underscores the importance of balanced datasets and highlights synthetic data generation as an effective strategy to address common challenges in remote sensing, demonstrating the potential of deep learning for scalable, global offshore infrastructure monitoring.</li>
<li><strong>摘要：</strong>最近和持续的海洋基础设施扩张，包括海上风电场、石油和天然气平台、人工岛屿和水产养殖设施，凸显了对有效监测系统的需求。用于海上基础设施检测的鲁棒模型的开发依赖于全面、平衡的数据集，但当样本稀缺时，特别是对于代表性不足的物体类别、形状和大小，模型的开发就显得不够了。本研究通过结合 2023 年第四季度从四个地区（里海、南海、几内亚湾和巴西海岸）获取的合成和真实 Sentinel-1 卫星图像来训练基于深度学习的 YOLOv10 目标检测模型，研究如何使用合成训练数据来增强模型性能。我们通过应用该模型来检测三个看不见的区域（墨西哥湾、北海、波斯湾）的海上平台来评估这种方法，从而评估地理可转移性。这种区域保留评估表明该模型可以推广到训练区域之外。总共检测到3,529个海上平台，其中北海有411个，墨西哥湾有1,519个，波斯湾有1,593个。该模型的 F1 得分为 0.85，在合并合成数据后提高到 0.90。我们分析了合成数据如何增强不平衡类的表示和整体模型性能，向全球可转移的海上基础设施检测迈出了第一步。这项研究强调了平衡数据集的重要性，并强调合成数据生成是解决遥感常见挑战的有效策略，展示了深度学习在可扩展的全球海上基础设施监测方面的潜力。</li>
</ul>

<h3>Title: RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiangjun Zhang, Litong Gong, Yinglin Zheng, Yansong Liu, Wentao Jiang, Mingyi Xu, Biao Wang, Tiezheng Ge, Ming Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04317">https://arxiv.org/abs/2511.04317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04317">https://arxiv.org/pdf/2511.04317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04317]] RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation(https://arxiv.org/abs/2511.04317)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Most text-to-video(T2V) diffusion models depend on pre-trained text encoders for semantic alignment, yet they often fail to maintain video quality when provided with concise prompts rather than well-designed ones. The primary issue lies in their limited textual semantics understanding. Moreover, these text encoders cannot rephrase prompts online to better align with user intentions, which limits both the scalability and usability of the models, To address these challenges, we introduce RISE-T2V, which uniquely integrates the processes of prompt rephrasing and semantic feature extraction into a single and seamless step instead of two separate steps. RISE-T2V is universal and can be applied to various pre-trained LLMs and video diffusion models(VDMs), significantly enhancing their capabilities for T2V tasks. We propose an innovative module called the Rephrasing Adapter, enabling diffusion models to utilize text hidden states during the next token prediction of the LLM as a condition for video generation. By employing a Rephrasing Adapter, the video generation model can implicitly rephrase basic prompts into more comprehensive representations that better match the user's intent. Furthermore, we leverage the powerful capabilities of LLMs to enable video generation models to accomplish a broader range of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a versatile framework applicable to different video diffusion model architectures, significantly enhancing the ability of T2V models to generate high-quality videos that align with user intent. Visual results are available on the webpage at this https URL.</li>
<li><strong>摘要：</strong>大多数文本到视频（T2V）扩散模型依赖于预先训练的文本编码器进行语义对齐，但当提供简洁的提示而不是精心设计的提示时，它们通常无法保持视频质量。主要问题在于他们对文本语义的理解有限。此外，这些文本编码器无法在线重新措辞提示以更好地符合用户意图，这限制了模型的可扩展性和可用性。为了解决这些挑战，我们引入了RISE-T2V，它将提示重新措辞和语义特征提取的过程独特地集成到单个无缝步骤中，而不是两个单独的步骤。 RISE-T2V具有通用性，可应用于各种预训练的LLM和视频扩散模型（VDM），显着增强其执行T2V任务的能力。我们提出了一个名为 Rephrasing Adapter 的创新模块，使扩散模型能够在 LLM 的下一个标记预测期间利用文本隐藏状态作为视频生成的条件。通过采用改写适配器，视频生成模型可以将基本提示隐式改写为更全面的表示，以更好地匹配用户的意图。此外，我们利用法学硕士的强大功能，使视频生成模型能够完成更广泛的 T2V 任务。大量实验表明，RISE-T2V 是一个适用于不同视频扩散模型架构的通用框架，显着增强了 T2V 模型生成符合用户意图的高质量视频的能力。视觉结果可在网页上的 https URL 上找到。</li>
</ul>

<h3>Title: Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA</h3>
<ul>
<li><strong>Authors: </strong>Itbaan Safwan, Muhammad Annas Shaikh, Muhammad Haaris, Ramail Khan, Muhammad Atif Tahir</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04384">https://arxiv.org/abs/2511.04384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04384">https://arxiv.org/pdf/2511.04384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04384]] Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA(https://arxiv.org/abs/2511.04384)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present a multi-task framework for the MediaEval Medico 2025 challenge, leveraging a LoRA-tuned Florence-2 model for simultaneous visual question answering (VQA), explanation generation, and visual grounding. The proposed system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer learning, (2) a synthetically enriched explanation dataset offering structured medical reasoning, and (3) text-to-region pairs linking visual features with segmentation masks. This multi-task setup enables the model to jointly learn visual grounding, reasoning, and interpretation, producing responses that are both accurate and interpretable. Extensive evaluation demonstrates that our approach substantially improves over single-task baselines in both answer accuracy and visual localization, highlighting the effectiveness of grounded multi-task learning for medical VQA applications.</li>
<li><strong>摘要：</strong>我们为 MediaEval Medico 2025 挑战提出了一个多任务框架，利用 LoRA 调整的 Florence-2 模型进行同步视觉问答 (VQA)、解释生成和视觉基础。所提出的系统集成了三个精选数据集：（1）用于问答学习的 Kvasir-VQA-x1，（2）提供结构化医学推理的综合丰富的解释数据集，以及（3）将视觉特征与分割掩模联系起来的文本到区域对。这种多任务设置使模型能够共同学习视觉基础、推理和解释，从而产生准确且可解释的响应。广泛的评估表明，我们的方法在答案准确性和视觉定位方面都比单任务基线有了显着提高，凸显了扎根多任务学习在医学 VQA 应用中的有效性。</li>
</ul>

<h3>Title: ForecastGAN: A Decomposition-Based Adversarial Framework for Multi-Horizon Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Syeda Sitara Wishal Fatima, Afshin Rahimi</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04445">https://arxiv.org/abs/2511.04445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04445">https://arxiv.org/pdf/2511.04445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04445]] ForecastGAN: A Decomposition-Based Adversarial Framework for Multi-Horizon Time Series Forecasting(https://arxiv.org/abs/2511.04445)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Time series forecasting is essential across domains from finance to supply chain management. This paper introduces ForecastGAN, a novel decomposition based adversarial framework addressing limitations in existing approaches for multi-horizon predictions. Although transformer models excel in long-term forecasting, they often underperform in short-term scenarios and typically ignore categorical features. ForecastGAN operates through three integrated modules: a Decomposition Module that extracts seasonality and trend components; a Model Selection Module that identifies optimal neural network configurations based on forecasting horizon; and an Adversarial Training Module that enhances prediction robustness through Conditional Generative Adversarial Network training. Unlike conventional approaches, ForecastGAN effectively integrates both numerical and categorical features. We validate our framework on eleven benchmark multivariate time series datasets that span various forecasting horizons. The results show that ForecastGAN consistently outperforms state-of-the-art transformer models for short-term forecasting while remaining competitive for long-term horizons. This research establishes a more generalizable approach to time series forecasting that adapts to specific contexts while maintaining strong performance across diverse data characteristics without extensive hyperparameter tuning.</li>
<li><strong>摘要：</strong>时间序列预测在从金融到供应链管理的各个领域都至关重要。本文介绍了 ForecastGAN，这是一种基于分解的新型对抗框架，解决了现有多水平预测方法的局限性。尽管 Transformer 模型在长期预测方面表现出色，但它们在短期场景中通常表现不佳，并且通常会忽略分类特征。 ForecastGAN 通过三个集成模块进行操作： 分解模块，提取季节性和趋势成分；模型选择模块，根据预测范围识别最佳神经网络配置；以及通过条件生成对抗网络训练增强预测鲁棒性的对抗训练模块。与传统方法不同，ForecastGAN 有效地集成了数值特征和分类特征。我们在跨越不同预测范围的 11 个基准多元时间序列数据集上验证了我们的框架。结果表明，ForecastGAN 在短期预测方面始终优于最先进的 Transformer 模型，同时在长期预测方面保持竞争力。这项研究建立了一种更通用的时间序列预测方法，可以适应特定的环境，同时在不同的数据特​​征上保持强大的性能，而无需进行大量的超参数调整。</li>
</ul>

<h3>Title: Towards Causal Market Simulators</h3>
<ul>
<li><strong>Authors: </strong>Dennis Thumm, Luis Ontaneda Mijares</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.CP, stat.OT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04469">https://arxiv.org/abs/2511.04469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04469">https://arxiv.org/pdf/2511.04469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04469]] Towards Causal Market Simulators(https://arxiv.org/abs/2511.04469)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Market generators using deep generative models have shown promise for synthetic financial data generation, but existing approaches lack causal reasoning capabilities essential for counterfactual analysis and risk assessment. We propose a Time-series Neural Causal Model VAE (TNCM-VAE) that combines variational autoencoders with structural causal models to generate counterfactual financial time series while preserving both temporal dependencies and causal relationships. Our approach enforces causal constraints through directed acyclic graphs in the decoder architecture and employs the causal Wasserstein distance for training. We validate our method on synthetic autoregressive models inspired by the Ornstein-Uhlenbeck process, demonstrating superior performance in counterfactual probability estimation with L1 distances as low as 0.03-0.10 compared to ground truth. The model enables financial stress testing, scenario analysis, and enhanced backtesting by generating plausible counterfactual market trajectories that respect underlying causal mechanisms.</li>
<li><strong>摘要：</strong>使用深度生成模型的市场生成器已显示出合成金融数据生成的前景，但现有方法缺乏反事实分析和风险评估所必需的因果推理能力。我们提出了一种时间序列神经因果模型 VAE (TNCM-VAE)，它将变分自动编码器与结构因果模型相结合，生成反事实的金融时间序列，同时保留时间依赖性和因果关系。我们的方法通过解码器架构中的有向非循环图强制执行因果约束，并采用因果 Wasserstein 距离进行训练。我们在受 Ornstein-Uhlenbeck 过程启发的合成自回归模型上验证了我们的方法，证明了在反事实概率估计方面的优越性能，与真实情况相比，L1 距离低至 0.03-0.10。该模型通过生成尊重潜在因果机制的合理的反事实市场轨迹，实现财务压力测试、情景分析和增强回测。</li>
</ul>

<h3>Title: THEval. Evaluation Framework for Talking Head Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Nabyl Quignon, Baptiste Chopin, Yaohui Wang, Antitza Dantcheva</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04520">https://arxiv.org/abs/2511.04520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04520">https://arxiv.org/pdf/2511.04520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04520]] THEval. Evaluation Framework for Talking Head Video Generation(https://arxiv.org/abs/2511.04520)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Video generation has achieved remarkable progress, with generated videos increasingly resembling real ones. However, the rapid advance in generation has outpaced the development of adequate evaluation metrics. Currently, the assessment of talking head generation primarily relies on limited metrics, evaluating general video quality, lip synchronization, and on conducting user studies. Motivated by this, we propose a new evaluation framework comprising 8 metrics related to three dimensions (i) quality, (ii) naturalness, and (iii) synchronization. In selecting the metrics, we place emphasis on efficiency, as well as alignment with human preferences. Based on this considerations, we streamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as well as face quality. Our extensive experiments on 85,000 videos generated by 17 state-of-the-art models suggest that while many algorithms excel in lip synchronization, they face challenges with generating expressiveness and artifact-free details. These videos were generated based on a novel real dataset, that we have curated, in order to mitigate bias of training data. Our proposed benchmark framework is aimed at evaluating the improvement of generative methods. Original code, dataset and leaderboards will be publicly released and regularly updated with new methods, in order to reflect progress in the field.</li>
<li><strong>摘要：</strong>视频生成取得了显着的进步，生成的视频越来越像真实的视频。然而，一代人的快速进步已经超过了适当评估指标的发展速度。目前，对头像生成的评估主要依赖于有限的指标、评估一般视频质量、唇形同步以及进行用户研究。受此启发，我们提出了一个新的评估框架，包括与三个维度（i）质量、（ii）自然度和（iii）同步相关的 8 个指标。在选择指标时，我们强调效率以及与人类偏好的一致性。基于此考虑，我们简化分析头部、嘴巴、眉毛的细粒度动态以及面部质量。我们对 17 个最先进模型生成的 85,000 个视频进行了广泛的实验，结果表明，虽然许多算法在唇形同步方面表现出色，但它们在生成表现力和无伪影细节方面面临着挑战。这些视频是根据我们策划的新颖的真实数据集生成的，以减轻训练数据的偏差。我们提出的基准框架旨在评估生成方法的改进。原始代码、数据集和排行榜将公开发布，并定期更新新方法，以反映该领域的进展。</li>
</ul>

<h3>Title: Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm</h3>
<ul>
<li><strong>Authors: </strong>Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04570">https://arxiv.org/abs/2511.04570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04570">https://arxiv.org/pdf/2511.04570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04570]] Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm(https://arxiv.org/abs/2511.04570)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>"Thinking with Text" and "Thinking with Images" paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce "Thinking with Video", a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2's performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions "thinking with video" as a unified multimodal reasoning paradigm.</li>
<li><strong>摘要：</strong>“用文本思考”和“用图像思考”范式显着提高了大语言模型（LLM）和视觉语言模型（VLM）的推理能力。然而，这些范式具有固有的局限性。 （1）图像仅捕捉单个时刻，无法表示动态过程或连续变化；（2）文本和视觉作为不同模态的分离，阻碍了统一的多模态理解和生成。为了克服这些限制，我们引入了“用视频思考”，这是一种利用视频生成模型（例如 Sora-2）在统一时间框架中桥接视觉和文本推理的新范式。为了支持这一探索，我们开发了视频思维基准（VideoThinkBench）。 VideoThinkBench 包含两个任务类别：(1) 以视觉为中心的任务（例如，眼球拼图），以及 (2) 以文本为中心的任务（例如，GSM8K、MMMU 的子集）。我们的评估表明 Sora-2 是一个有能力的推理者。在以视觉为中心的任务上，Sora-2 通常可以与最先进的 (SOTA) VLM 相媲美，甚至在一些任务上超过了 VLM，例如 Eyeballing Games。在以文本为中心的任务中，Sora-2 在 MATH 上实现了 92% 的准确率，在 MMMU 上实现了 75.53% 的准确率。此外，我们系统地分析了这些能力的来源。我们还发现自我一致性和情境学习可以提高 Sora-2 的性能。总之，我们的研究结果表明，视频生成模型是潜在的统一多模态理解和生成模型，将“用视频思考”定位为统一的多模态推理范式。</li>
</ul>

<h3>Title: UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Chen Shi, Shaoshuai Shi, Xiaoyang Lyu, Chunyang Liu, Kehua Sheng, Bo Zhang, Li Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04595">https://arxiv.org/abs/2511.04595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04595">https://arxiv.org/pdf/2511.04595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04595]] UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction(https://arxiv.org/abs/2511.04595)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Feed-forward 3D reconstruction for autonomous driving has advanced rapidly, yet existing methods struggle with the joint challenges of sparse, non-overlapping camera views and complex scene dynamics. We present UniSplat, a general feed-forward framework that learns robust dynamic scene reconstruction through unified latent spatio-temporal fusion. UniSplat constructs a 3D latent scaffold, a structured representation that captures geometric and semantic scene context by leveraging pretrained foundation models. To effectively integrate information across spatial views and temporal frames, we introduce an efficient fusion mechanism that operates directly within the 3D scaffold, enabling consistent spatio-temporal alignment. To ensure complete and detailed reconstructions, we design a dual-branch decoder that generates dynamic-aware Gaussians from the fused scaffold by combining point-anchored refinement with voxel-based generation, and maintain a persistent memory of static Gaussians to enable streaming scene completion beyond current camera coverage. Extensive experiments on real-world datasets demonstrate that UniSplat achieves state-of-the-art performance in novel view synthesis, while providing robust and high-quality renderings even for viewpoints outside the original camera coverage.</li>
<li><strong>摘要：</strong>自动驾驶的前馈 3D 重建进展迅速，但现有方法难以应对稀疏、不重叠的摄像机视图和复杂场景动态的共同挑战。我们提出了 UniSplat，一个通用的前馈框架，它通过统一的潜在时空融合来学习鲁棒的动态场景重建。 UniSplat 构建了一个 3D 潜在支架，这是一种通过利用预先训练的基础模型来捕获几何和语义场景上下文的结构化表示。为了有效地集成跨空间视图和时间框架的信息，我们引入了一种有效的融合机制，该机制直接在 3D 支架内运行，从而实现一致的时空对齐。为了确保完整和详细的重建，我们设计了一个双分支解码器，通过将点锚定细化与基于体素的生成相结合，从融合支架生成动态感知高斯，并保持静态高斯的持久记忆，以实现超出当前相机覆盖范围的流场景完成。对现实世界数据集的大量实验表明，UniSplat 在新颖的视图合成中实现了最先进的性能，同时即使对于原始相机覆盖范围之外的视点也能提供强大且高质量的渲染。</li>
</ul>

<h3>Title: NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Kylie Cancilla, Alexander Moore, Amar Saini, Carmen Carrano</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04628">https://arxiv.org/abs/2511.04628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04628">https://arxiv.org/pdf/2511.04628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04628]] NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment(https://arxiv.org/abs/2511.04628)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Video quality assessment (VQA) is vital for computer vision tasks, but existing approaches face major limitations: full-reference (FR) metrics require clean reference videos, and most no-reference (NR) models depend on training on costly human opinion labels. Moreover, most opinion-unaware NR methods are image-based, ignoring temporal context critical for video object detection. In this work, we present a scalable, streaming-based VQA model that is both no-reference and opinion-unaware. Our model leverages synthetic degradations of the DAVIS dataset, training a temporal-aware convolutional architecture to predict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without references at inference. We show that our streaming approach outperforms our own image-based baseline by generalizing across diverse degradations, underscoring the value of temporal modeling for scalable VQA in real-world vision systems. Additionally, we demonstrate that our model achieves higher correlation with full-reference metrics compared to BRISQUE, a widely-used opinion-aware image quality assessment baseline, validating the effectiveness of our temporal, opinion-unaware approach.</li>
<li><strong>摘要：</strong>视频质量评估 (VQA) 对于计算机视觉任务至关重要，但现有方法面临重大局限性：全参考 (FR) 指标需要干净的参考视频，而大多数无参考 (NR) 模型依赖于昂贵的人类意见标签的训练。此外，大多数无意见的 NR 方法都是基于图像的，忽略了对于视频对象检测至关重要的时间上下文。在这项工作中，我们提出了一个可扩展的、基于流的 VQA 模型，该模型既无参考又无意见。我们的模型利用 DAVIS 数据集的综合降级，训练时间感知卷积架构来直接从降级视频预测 FR 指标（LPIPS、PSNR、SSIM），无需推理参考。我们证明，我们的流方法通过泛化各种退化，优于我们自己的基于图像的基线，强调了现实世界视觉系统中可扩展 VQA 的时间建模的价值。此外，我们证明，与 BRISQUE（一种广泛使用的意见感知图像质量评估基线）相比，我们的模型与全参考指标具有更高的相关性，验证了我们的时间、意见感知方法的有效性。</li>
</ul>

<h3>Title: Efficient probabilistic surrogate modeling techniques for partially-observed large-scale dynamical systems</h3>
<ul>
<li><strong>Authors: </strong>Hans Harder, Abhijeet Vishwasrao, Luca Guastoni, Ricardo Vinuesa, Sebastian Peitz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04641">https://arxiv.org/abs/2511.04641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04641">https://arxiv.org/pdf/2511.04641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04641]] Efficient probabilistic surrogate modeling techniques for partially-observed large-scale dynamical systems(https://arxiv.org/abs/2511.04641)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper is concerned with probabilistic techniques for forecasting dynamical systems described by partial differential equations (such as, for example, the Navier-Stokes equations). In particular, it is investigating and comparing various extensions to the flow matching paradigm that reduce the number of sampling steps. In this regard, it compares direct distillation, progressive distillation, adversarial diffusion distillation, Wasserstein GANs and rectified flows. Moreover, experiments are conducted on a set of challenging systems. In particular, we also address the challenge of directly predicting 2D slices of large-scale 3D simulations, paving the way for efficient inflow generation for solvers.</li>
<li><strong>摘要：</strong>本文关注的是用于预测由偏微分方程（例如纳维-斯托克斯方程）描述的动力系统的概率技术。特别是，它正在研究和比较流匹配范例的各种扩展，以减少采样步骤的数量。在这方面，它比较了直接蒸馏、渐进蒸馏、对抗扩散蒸馏、Wasserstein GAN 和整流流。此外，实验是在一组具有挑战性的系统上进行的。特别是，我们还解决了直接预测大规模 3D 模拟的 2D 切片的挑战，为求解器高效生成流入铺平了道路。</li>
</ul>

<h3>Title: Forgetting is Everywhere</h3>
<ul>
<li><strong>Authors: </strong>Ben Sanati, Thomas L. Lee, Trevor McInroe, Aidan Scannell, Nikolay Malkin, David Abel, Amos Storkey</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04666">https://arxiv.org/abs/2511.04666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04666">https://arxiv.org/pdf/2511.04666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04666]] Forgetting is Everywhere(https://arxiv.org/abs/2511.04666)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A fundamental challenge in developing general learning algorithms is their tendency to forget past knowledge when adapting to new data. Addressing this problem requires a principled understanding of forgetting; yet, despite decades of study, no unified definition has emerged that provides insights into the underlying dynamics of learning. We propose an algorithm- and task-agnostic theory that characterises forgetting as a lack of self-consistency in a learner's predictive distribution over future experiences, manifesting as a loss of predictive information. Our theory naturally yields a general measure of an algorithm's propensity to forget. To validate the theory, we design a comprehensive set of experiments that span classification, regression, generative modelling, and reinforcement learning. We empirically demonstrate how forgetting is present across all learning settings and plays a significant role in determining learning efficiency. Together, these results establish a principled understanding of forgetting and lay the foundation for analysing and improving the information retention capabilities of general learning algorithms.</li>
<li><strong>摘要：</strong>开发通用学习算法的一个基本挑战是它们在适应新数据时容易忘记过去的知识。解决这个问题需要对遗忘有原则性的理解；然而，尽管经过数十年的研究，仍然没有出现统一的定义来深入了解学习的潜在动态。我们提出了一种与算法和任务无关的理论，该理论将遗忘描述为学习者对未来经历的预测分布缺乏自我一致性，表现为预测信息的丢失。我们的理论自然地给出了算法遗忘倾向的一般衡量标准。为了验证该理论，我们设计了一套全面的实验，涵盖分类、回归、生成建模和强化学习。我们凭经验证明遗忘是如何存在于所有学习环境中的，并在决定学习效率方面发挥着重要作用。总之，这些结果建立了对遗忘的原则性理解，并为分析和提高通用学习算法的信息保留能力奠定了基础。</li>
</ul>

<h3>Title: SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Ellis Brown, Arijit Ray, Ranjay Krishna, Ross Girshick, Rob Fergus, Saining Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04668">https://arxiv.org/abs/2511.04668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04668">https://arxiv.org/pdf/2511.04668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04668]] SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding(https://arxiv.org/abs/2511.04668)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite impressive high-level video comprehension, multimodal language models struggle with spatial reasoning across time and space. While current spatial training approaches rely on real-world video data, obtaining diverse footage with precise spatial annotations remains a bottleneck. To alleviate this bottleneck, we present SIMS-V -- a systematic data-generation framework that leverages the privileged information of 3D simulators to create spatially-rich video training data for multimodal language models. Using this framework, we investigate which properties of simulated data drive effective real-world transfer through systematic ablations of question types, mixes, and scales. We identify a minimal set of three question categories (metric measurement, perspective-dependent reasoning, and temporal tracking) that prove most effective for developing transferable spatial intelligence, outperforming comprehensive coverage despite using fewer question types. These insights enable highly efficient training: our 7B-parameter video LLM fine-tuned on just 25K simulated examples outperforms the larger 72B baseline and achieves competitive performance with proprietary models on rigorous real-world spatial reasoning benchmarks. Our approach demonstrates robust generalization, maintaining performance on general video understanding while showing substantial improvements on embodied and real-world spatial tasks.</li>
<li><strong>摘要：</strong>尽管高水平的视频理解能力令人印象深刻，但多模态语言模型在跨时间和空间的空间推理方面仍遇到困难。虽然当前的空间训练方法依赖于现实世界的视频数据，但获取具有精确空间注释的多样化镜头仍然是一个瓶颈。为了缓解这一瓶颈，我们提出了 SIMS-V——一种系统数据生成框架，它利用 3D 模拟器的特权信息为多模态语言模型创建空间丰富的视频训练数据。使用这个框架，我们通过问题类型、混合和规模的系统消融来研究模拟数据的哪些属性驱动有效的现实世界转移。我们确定了三个问题类别的最小集合（度量测量、视角依赖推理和时间跟踪），这些类别被证明对于开发可转移的空间智能最有效，尽管使用的问题类型较少，但其性能优于全面覆盖。这些见解可实现高效的训练：我们的 7B 参数视频 LLM 仅在 25K 个模拟示例上进行了微调，其性能优于更大的 72B 基线，并在严格的现实世界空间推理基准上与专有模型实现了具有竞争力的性能。我们的方法展示了强大的泛化能力，保持了一般视频理解的性能，同时在具体和现实世界的空间任务上显示出实质性的改进。</li>
</ul>

<h3>Title: InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinlai Liu, Jian Han, Bin Yan, Hui Wu, Fengda Zhu, Xing Wang, Yi Jiang, Bingyue Peng, Zehuan Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.04675">https://arxiv.org/abs/2511.04675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.04675">https://arxiv.org/pdf/2511.04675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.04675]] InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation(https://arxiv.org/abs/2511.04675)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce InfinityStar, a unified spacetime autoregressive framework for high-resolution image and dynamic video synthesis. Building on the recent success of autoregressive modeling in both vision and language, our purely discrete approach jointly captures spatial and temporal dependencies within a single architecture. This unified design naturally supports a variety of generation tasks such as text-to-image, text-to-video, image-to-video, and long interactive video synthesis via straightforward temporal autoregression. Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench, outperforming all autoregressive models by large margins, even surpassing some diffusion competitors like HunyuanVideo. Without extra optimizations, our model generates a 5s, 720p video approximately 10x faster than leading diffusion-based methods. To our knowledge, InfinityStar is the first discrete autoregressive video generator capable of producing industrial level 720p videos. We release all code and models to foster further research in efficient, high-quality video generation.</li>
<li><strong>摘要：</strong>我们介绍 InfinityStar，一个用于高分辨率图像和动态视频合成的统一时空自回归框架。基于最近在视觉和语言方面取得的自回归建模的成功，我们的纯离散方法联合捕获了单个架构内的空间和时间依赖性。这种统一的设计自然支持各种生成任务，例如文本到图像、文本到视频、图像到视频以及通过简单的时间自回归进行长交互式视频合成。大量实验表明，InfinityStar 在 VBench 上的得分为 83.74，大幅优于所有自回归模型，甚至超过了一些扩散竞争对手，例如 HunyuanVideo。无需额外优化，我们的模型生成 5 秒、720p 视频的速度比领先的基于扩散的方法快大约 10 倍。据我们所知，InfinityStar 是首款能够生成工业级 720p 视频的离散自回归视频生成器。我们发布所有代码和模型，以促进高效、高质量视频生成的进一步研究。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
