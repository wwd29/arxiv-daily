<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-28</h1>
<h3>Title: Livatar-1: Real-Time Talking Heads Generation with Tailored Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Haiyang Liu, Xiaolin Hong, Xuancheng Yang, Yudi Ruan, Xiang Lian, Michael Lingelbach, Hongwei Yi, Wei Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18649">https://arxiv.org/abs/2507.18649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18649">https://arxiv.org/pdf/2507.18649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18649]] Livatar-1: Real-Time Talking Heads Generation with Tailored Flow Matching(https://arxiv.org/abs/2507.18649)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present Livatar, a real-time audio-driven talking heads videos generation framework. Existing baselines suffer from limited lip-sync accuracy and long-term pose drift. We address these limitations with a flow matching based framework. Coupled with system optimizations, Livatar achieves competitive lip-sync quality with a 8.50 LipSync Confidence on the HDTF dataset, and reaches a throughput of 141 FPS with an end-to-end latency of 0.17s on a single A10 GPU. This makes high-fidelity avatars accessible to broader applications. Our project is available at this https URL with with examples at this https URL</li>
<li><strong>摘要：</strong>我们提出了Livatar，这是一个实时音频驱动的会说话的负责人视频生成框架。现有的基线遭受有限的唇部同步准确性和长期姿势漂移。我们通过基于流匹配的框架来解决这些限制。加上系统优化，利瓦塔尔（Livatar）在HDTF数据集上具有8.50 LIPS同步的信心，达到了竞争性的LIP-同步质量，并达到141 fps的吞吐量，端到端的延迟在单个A10 GPU上的端到端延迟为0.17。这使得高保真化的化身可以在更广泛的应用程序中访问。我们的项目可在此HTTPS URL上提供，并在此HTTPS URL上提供示例</li>
</ul>

<h3>Title: Features extraction for image identification using computer vision</h3>
<ul>
<li><strong>Authors: </strong>Venant Niyonkuru, Sylla Sekou, Jimmy Jackson Sinzinkayo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18650">https://arxiv.org/abs/2507.18650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18650">https://arxiv.org/pdf/2507.18650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18650]] Features extraction for image identification using computer vision(https://arxiv.org/abs/2507.18650)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study examines various feature extraction techniques in computer vision, the primary focus of which is on Vision Transformers (ViTs) and other approaches such as Generative Adversarial Networks (GANs), deep feature models, traditional approaches (SIFT, SURF, ORB), and non-contrastive and contrastive feature models. Emphasizing ViTs, the report summarizes their architecture, including patch embedding, positional encoding, and multi-head self-attention mechanisms with which they overperform conventional convolutional neural networks (CNNs). Experimental results determine the merits and limitations of both methods and their utilitarian applications in advancing computer vision.</li>
<li><strong>摘要：</strong>这项研究研究了计算机视觉中的各种特征提取技术，其主要重点是视觉变压器（VIT）以及其他方法，例如生成对抗网络（GAN），深度特征模型，传统方法（SIFT，SUFT，SURF，ORB）以及非对比度和对比的功能模型。该报告强调VIT，总结了它们的体系结构，包括贴片嵌入，位置编码以及多头发项机制，它们都表现出了传统的常规卷积神经网络（CNN）。实验结果决定了这两种方法及其在推动计算机视觉方面的功利应用的优点和局限性。</li>
</ul>

<h3>Title: Diffusion Models for Solving Inverse Problems via Posterior Sampling with Piecewise Guidance</h3>
<ul>
<li><strong>Authors: </strong>Saeed Mohseni-Sehdeh, Walid Saad, Kei Sakaguchi, Tao Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18654">https://arxiv.org/abs/2507.18654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18654">https://arxiv.org/pdf/2507.18654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18654]] Diffusion Models for Solving Inverse Problems via Posterior Sampling with Piecewise Guidance(https://arxiv.org/abs/2507.18654)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution</a></li>
<li><strong>Abstract: </strong>Diffusion models are powerful tools for sampling from high-dimensional distributions by progressively transforming pure noise into structured data through a denoising process. When equipped with a guidance mechanism, these models can also generate samples from conditional distributions. In this paper, a novel diffusion-based framework is introduced for solving inverse problems using a piecewise guidance scheme. The guidance term is defined as a piecewise function of the diffusion timestep, facilitating the use of different approximations during high-noise and low-noise phases. This design is shown to effectively balance computational efficiency with the accuracy of the guidance term. Unlike task-specific approaches that require retraining for each problem, the proposed method is problem-agnostic and readily adaptable to a variety of inverse problems. Additionally, it explicitly incorporates measurement noise into the reconstruction process. The effectiveness of the proposed framework is demonstrated through extensive experiments on image restoration tasks, specifically image inpainting and super-resolution. Using a class conditional diffusion model for recovery, compared to the \pgdm baseline, the proposed framework achieves a reduction in inference time of \(25\%\) for inpainting with both random and center masks, and \(23\%\) and \(24\%\) for \(4\times\) and \(8\times\) super-resolution tasks, respectively, while incurring only negligible loss in PSNR and SSIM.</li>
<li><strong>摘要：</strong>扩散模型是通过逐渐将纯噪声转换为结构化数据的强大工具，可通过降解过程进行取样。当配备指导机制时，这些模型还可以从条件分布中生成样品。在本文中，引入了一种基于扩散的新型框架，用于使用分段指导方案解决反问题。指导项被定义为扩散时间步的分段函数，从而促进了在高噪声和低噪声阶段使用不同近似值的使用。该设计显示可有效平衡计算效率与指导项的准确性。与需要针对每个问题进行重新训练的特定于任务的方法不同，所提出的方法是问题不合时宜的，并且很容易适应各种反问题。此外，它明确将测量噪声纳入重建过程中。通过对图像恢复任务的广泛实验，特别是图像介入和超分辨率，可以证明所提出的框架的有效性。与\ pgdm基线相比，使用有条件恢复的有条件扩散模型，所提出的框架的推理时间减少为\（25 \％\），用于与随机和中心掩码的内化，以及\（23 \％\）和\（24 \％\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \）和\（4 \ \）和\（4 \）和\（4 \）和\（4 \）和\（8虽然在PSNR和SSIM中只会造成可忽略的损失。</li>
</ul>

<h3>Title: Diffusion-FS: Multimodal Free-Space Prediction via Diffusion for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Keshav Gupta, Tejas S. Stanley, Pranjal Paul, Arun K. Singh, K. Madhava Krishna</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18763">https://arxiv.org/abs/2507.18763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18763">https://arxiv.org/pdf/2507.18763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18763]] Diffusion-FS: Multimodal Free-Space Prediction via Diffusion for Autonomous Driving(https://arxiv.org/abs/2507.18763)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Drivable Free-space prediction is a fundamental and crucial problem in autonomous driving. Recent works have addressed the problem by representing the entire non-obstacle road regions as the free-space. In contrast our aim is to estimate the driving corridors that are a navigable subset of the entire road region. Unfortunately, existing corridor estimation methods directly assume a BEV-centric representation, which is hard to obtain. In contrast, we frame drivable free-space corridor prediction as a pure image perception task, using only monocular camera input. However such a formulation poses several challenges as one doesn't have the corresponding data for such free-space corridor segments in the image. Consequently, we develop a novel self-supervised approach for free-space sample generation by leveraging future ego trajectories and front-view camera images, making the process of visual corridor estimation dependent on the ego trajectory. We then employ a diffusion process to model the distribution of such segments in the image. However, the existing binary mask-based representation for a segment poses many limitations. Therefore, we introduce ContourDiff, a specialized diffusion-based architecture that denoises over contour points rather than relying on binary mask representations, enabling structured and interpretable free-space predictions. We evaluate our approach qualitatively and quantitatively on both nuScenes and CARLA, demonstrating its effectiveness in accurately predicting safe multimodal navigable corridors in the image.</li>
<li><strong>摘要：</strong>可驱动的自由空间预测是自动驾驶中的一个基本和关键问题。最近的作品通过将整个非奥斯塔克斯公路区域表示为自由空间来解决了这个问题。相比之下，我们的目标是估算整个道路区域可导航子集的驾驶走廊。不幸的是，现有的走廊估计方法直接采用以BEV为中心的表示，很难获得。相比之下，我们仅使用单眼摄像机输入将可驱动的自由空间走廊预测作为纯图像感知任务。但是，这样的公式构成了几个挑战，因为一个挑战没有图像中此类自由空间走廊段的相应数据。因此，我们通过利用未来的自我轨迹和前视摄像头图像来开发一种新型的自我监督方法来生成自由空间样本，从而使视觉走廊估计的过程取决于自我轨迹。然后，我们采用扩散过程来对图像中此类段的分布进行建模。但是，现有的基于二进制蒙版的表示片段构成了许多局限性。因此，我们介绍了ContourDiff，这是一种基于专门的扩散架构，它可以通过轮廓点进行定位，而不是依靠二进制掩码表示形式，从而实现结构化和可解释的自由空间预测。我们对Nuscenes和Carla进行定性和定量评估方法，证明了其在准确预测图像中安全多模式可通道走廊方面的有效性。</li>
</ul>

<h3>Title: Even Faster Simulations with Flow Matching: A Study of Zero Degree Calorimeter Responses</h3>
<ul>
<li><strong>Authors: </strong>Maksymilian Wojnar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18811">https://arxiv.org/abs/2507.18811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18811">https://arxiv.org/pdf/2507.18811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18811]] Even Faster Simulations with Flow Matching: A Study of Zero Degree Calorimeter Responses(https://arxiv.org/abs/2507.18811)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative neural networks, particularly flow matching (FM), have enabled the generation of high-fidelity samples while significantly reducing computational costs. A promising application of these models is accelerating simulations in high-energy physics (HEP), helping research institutions meet their increasing computational demands. In this work, we leverage FM to develop surrogate models for fast simulations of zero degree calorimeters in the ALICE experiment. We present an effective training strategy that enables the training of fast generative models with an exceptionally low number of parameters. This approach achieves state-of-the-art simulation fidelity for both neutron (ZN) and proton (ZP) detectors, while offering substantial reductions in computational costs compared to existing methods. Our FM model achieves a Wasserstein distance of 1.27 for the ZN simulation with an inference time of 0.46 ms per sample, compared to the current best of 1.20 with an inference time of approximately 109 ms. The latent FM model further improves the inference speed, reducing the sampling time to 0.026 ms per sample, with a minimal trade-off in accuracy. Similarly, our approach achieves a Wasserstein distance of 1.30 for the ZP simulation, outperforming the current best of 2.08. The source code is available at this https URL.</li>
<li><strong>摘要：</strong>生成神经网络（尤其是流量匹配（FM））的最新进展已使高保真样本产生，同时大大降低了计算成本。这些模型的有希望的应用是加速模拟在高能物理学（HEP）中，帮助研究机构满足了他们日益增长的计算需求。在这项工作中，我们利用FM在爱丽丝实验中开发替代模型，以快速模拟零度量热量计。我们提出了一种有效的培训策略，该策略可以培训具有异常数量参数的快速生成模型。这种方法可实现中子（Zn）和质子（ZP）探测器的最新模拟保真度，同时与现有方法相比，计算成本大大降低。我们的FM模型在Zn模拟中达到了1.27的WASSERSTEIN距离，每样品的推理时间为0.46 ms，而当前最佳最佳1.20的推理时间约为109毫秒。潜在的FM模型进一步提高了推理速度，将采样时间降低到每个样品的0.026毫秒，精度的折衷最小。同样，对于ZP模拟，我们的方法达到了1.30的Wasserstein距离，表现优于2.08的电流最佳。源代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: RealDeal: Enhancing Realism and Details in Brain Image Generation via Image-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shen Zhu, Yinzhu Jin, Tyler Spears, Ifrah Zawar, P. Thomas Fletcher</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18830">https://arxiv.org/abs/2507.18830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18830">https://arxiv.org/pdf/2507.18830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18830]] RealDeal: Enhancing Realism and Details in Brain Image Generation via Image-to-Image Diffusion Models(https://arxiv.org/abs/2507.18830)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We propose image-to-image diffusion models that are designed to enhance the realism and details of generated brain images by introducing sharp edges, fine textures, subtle anatomical features, and imaging noise. Generative models have been widely adopted in the biomedical domain, especially in image generation applications. Latent diffusion models achieve state-of-the-art results in generating brain MRIs. However, due to latent compression, generated images from these models are overly smooth, lacking fine anatomical structures and scan acquisition noise that are typically seen in real images. This work formulates the realism enhancing and detail adding process as image-to-image diffusion models, which refines the quality of LDM-generated images. We employ commonly used metrics like FID and LPIPS for image realism assessment. Furthermore, we introduce new metrics to demonstrate the realism of images generated by RealDeal in terms of image noise distribution, sharpness, and texture.</li>
<li><strong>摘要：</strong>我们提出了图像到图像扩散模型，旨在通过引入锋利的边缘，细纹理，微妙的解剖特征和成像噪声来增强产生的大脑图像的现实主义和细节。生成模型已在生物医学领域中广泛采用，尤其是在图像生成应用中。潜在扩散模型实现最新的脑部MRI会产生最新的成绩。但是，由于潜在的压缩，这些模型中生成的图像过于平滑，缺乏精细的解剖结构和扫描习得噪声，这些噪声通常在真实图像中看到。这项工作为现实主义增强和细节添加过程作为图像到图像扩散模型，从而完善了LDM生成图像的质量。我们采用常用的指标，例如FID和LPIP进行图像现实主义评估。此外，我们介绍了新的指标，以证明Realdeal在图像噪声分布，清晰度和纹理方面产生的图像的现实性。</li>
</ul>

<h3>Title: Flow Stochastic Segmentation Networks</h3>
<ul>
<li><strong>Authors: </strong>Fabio De Sousa Ribeiro, Omar Todd, Charles Jones, Avinash Kori, Raghav Mehta, Ben Glocker</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18838">https://arxiv.org/abs/2507.18838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18838">https://arxiv.org/pdf/2507.18838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18838]] Flow Stochastic Segmentation Networks(https://arxiv.org/abs/2507.18838)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce the Flow Stochastic Segmentation Network (Flow-SSN), a generative segmentation model family featuring discrete-time autoregressive and modern continuous-time flow variants. We prove fundamental limitations of the low-rank parameterisation of previous methods and show that Flow-SSNs can estimate arbitrarily high-rank pixel-wise covariances without assuming the rank or storing the distributional parameters. Flow-SSNs are also more efficient to sample from than standard diffusion-based segmentation models, thanks to most of the model capacity being allocated to learning the base distribution of the flow, constituting an expressive prior. We apply Flow-SSNs to challenging medical imaging benchmarks and achieve state-of-the-art results. Code available: this https URL.</li>
<li><strong>摘要：</strong>我们介绍了Flow随机分割网络（Flow-SSN），这是一种生成分割模型家族，具有离散时间自动回归和现代连续时流量变体。我们证明了先前方法的低排名参数化的基本局限性，并表明Flow-SSN可以估计任意高级像素的协方差，而无需假设等级或存储分布参数。与基于标准扩散的分割模型相比，Flow-SSN的样本也更有效，这要归功于大多数模型能力分配给学习流的基本分布，并构成表达的先验。我们将Flow-SSN应用于挑战医学成像基准并实现最新结果。可用代码：此HTTPS URL。</li>
</ul>

<h3>Title: HQ-SMem: Video Segmentation and Tracking Using Memory Efficient Object Embedding With Selective Update and Self-Supervised Distillation Feedback</h3>
<ul>
<li><strong>Authors: </strong>Elham Soltani Kazemi, Imad Eddine Toubal, Gani Rahmon, Jaired Collins, K. Palaniappan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18921">https://arxiv.org/abs/2507.18921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18921">https://arxiv.org/pdf/2507.18921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18921]] HQ-SMem: Video Segmentation and Tracking Using Memory Efficient Object Embedding With Selective Update and Self-Supervised Distillation Feedback(https://arxiv.org/abs/2507.18921)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Video Object Segmentation (VOS) is foundational to numerous computer vision applications, including surveillance, autonomous driving, robotics and generative video editing. However, existing VOS models often struggle with precise mask delineation, deformable objects, topologically transforming objects, tracking drift and long video sequences. In this paper, we introduce HQ-SMem, for High Quality video segmentation and tracking using Smart Memory, a novel method that enhances the performance of VOS base models by addressing these limitations. Our approach incorporates three key innovations: (i) leveraging SAM with High-Quality masks (SAM-HQ) alongside appearance-based candidate-selection to refine coarse segmentation masks, resulting in improved object boundaries; (ii) implementing a dynamic smart memory mechanism that selectively stores relevant key frames while discarding redundant ones, thereby optimizing memory usage and processing efficiency for long-term videos; and (iii) dynamically updating the appearance model to effectively handle complex topological object variations and reduce drift throughout the video. These contributions mitigate several limitations of existing VOS models including, coarse segmentations that mix-in background pixels, fixed memory update schedules, brittleness to drift and occlusions, and prompt ambiguity issues associated with SAM. Extensive experiments conducted on multiple public datasets and state-of-the-art base trackers demonstrate that our method consistently ranks among the top two on VOTS and VOTSt 2024 datasets. Moreover, HQ-SMem sets new benchmarks on Long Video Dataset and LVOS, showcasing its effectiveness in challenging scenarios characterized by complex multi-object dynamics over extended temporal durations.</li>
<li><strong>摘要：</strong>视频对象细分（VOS）是众多计算机视觉应用的基础，包括监视，自动驾驶，机器人技术和生成视频编辑。但是，现有的VOS模型通常会在精确的面膜描述，可变形的对象，拓扑转换对象，跟踪漂移和长视频序列方面挣扎。在本文中，我们介绍了HQ-SMEM，用于使用Smart Memory进行高质量的视频细分和跟踪，这是一种新颖的方法，可通过解决这些限制来增强VOS基本模型的性能。我们的方法结合了三个关键的创新：（i）利用具有高质量面具（SAM-HQ）的SAM与基于外观的候选候选式选择以完善粗分割面罩，从而改善了对象边界； （ii）实施动态的智能内存机制，该机制在丢弃冗余范围的同时选择性地存储相关的关键帧，从而优化内存使用和长期视频的处理效率； （iii）动态更新外观模型，以有效处理复杂的拓扑对象变化并减少整个视频中的漂移。这些贡献减轻了现有的VOS模型的几个限制，包括混合背景像素的粗分段，固定的内存更新时间表，对漂移和遮挡的脆性以及引起与SAM相关的歧义问题。在多个公共数据集和最先进的基本跟踪器上进行的广泛实验表明，我们的方法始终在VOTS和FOTST 2024数据集中排名前两名。此外，HQ-SMEM在长视频数据集和LVO上设置了新的基准测试，展示了其在挑战性场景中的有效性，其特征是复杂的多对象动力学在延长的时间持续时间内。</li>
</ul>

<h3>Title: Gaussian Set Surface Reconstruction through Per-Gaussian Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zhentao Huang, Di Wu, Zhenbang He, Minglun Gong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18923">https://arxiv.org/abs/2507.18923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18923">https://arxiv.org/pdf/2507.18923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18923]] Gaussian Set Surface Reconstruction through Per-Gaussian Optimization(https://arxiv.org/abs/2507.18923)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting (3DGS) effectively synthesizes novel views through its flexible representation, yet fails to accurately reconstruct scene geometry. While modern variants like PGSR introduce additional losses to ensure proper depth and normal maps through Gaussian fusion, they still neglect individual placement optimization. This results in unevenly distributed Gaussians that deviate from the latent surface, complicating both reconstruction refinement and scene editing. Motivated by pioneering work on Point Set Surfaces, we propose Gaussian Set Surface Reconstruction (GSSR), a method designed to distribute Gaussians evenly along the latent surface while aligning their dominant normals with the surface normal. GSSR enforces fine-grained geometric alignment through a combination of pixel-level and Gaussian-level single-view normal consistency and multi-view photometric consistency, optimizing both local and global perspectives. To further refine the representation, we introduce an opacity regularization loss to eliminate redundant Gaussians and apply periodic depth- and normal-guided Gaussian reinitialization for a cleaner, more uniform spatial distribution. Our reconstruction results demonstrate significantly improved geometric precision in Gaussian placement, enabling intuitive scene editing and efficient generation of novel Gaussian-based 3D environments. Extensive experiments validate GSSR's effectiveness, showing enhanced geometric accuracy while preserving high-quality rendering performance.</li>
<li><strong>摘要：</strong>3D高斯脱落（3DGS）通过其灵活的表示有效地综合了新型视图，但无法准确重建场景几何形状。尽管像PGSR这样的现代变体会引入其他损失，以通过高斯融合来确保正确的深度和正常地图，但它们仍然忽略了个人放置优化。这会导致分布不均匀的高斯人偏离潜在表面，从而使重建的改进和场景编辑变得复杂。我们提出了高斯集固定表面重建（GSSR）的开创性工作，这是一种旨在沿潜在表面均匀分布高斯的方法，同时将其主要的正常质量与表面正常对齐。 GSSR通过像素级和高斯级单视图正常一致性和多视图光度一致性的组合来实施细粒的几何对齐方式，从而优化了局部和全局视角。为了进一步完善表示形式，我们引入了不透明度的正规化损失，以消除冗余高斯人，并施加周期性的深度和正常引导的高斯重新定性，以进行清洁剂，更均匀的空间分布。我们的重建结果表明，在高斯放置中，几何精度显着提高，从而实现了直观的场景编辑，并有效地生成了基于高斯的新型3D环境。广泛的实验验证了GSSR的有效性，显示出增强的几何精度，同时保持高质量的渲染性能。</li>
</ul>

<h3>Title: AEDR: Training-Free AI-Generated Image Attribution via Autoencoder Double-Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Chao Wang, Kejiang Chen, Zijin Yang, Yaofei Wang, Weiming Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18988">https://arxiv.org/abs/2507.18988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18988">https://arxiv.org/pdf/2507.18988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18988]] AEDR: Training-Free AI-Generated Image Attribution via Autoencoder Double-Reconstruction(https://arxiv.org/abs/2507.18988)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of image-generation technologies has made it possible for anyone to create photorealistic images using generative models, raising significant security concerns. To mitigate malicious use, tracing the origin of such images is essential. Reconstruction-based attribution methods offer a promising solution, but they often suffer from reduced accuracy and high computational costs when applied to state-of-the-art (SOTA) models. To address these challenges, we propose AEDR (AutoEncoder Double-Reconstruction), a novel training-free attribution method designed for generative models with continuous autoencoders. Unlike existing reconstruction-based approaches that rely on the value of a single reconstruction loss, AEDR performs two consecutive reconstructions using the model's autoencoder, and adopts the ratio of these two reconstruction losses as the attribution signal. This signal is further calibrated using the image homogeneity metric to improve accuracy, which inherently cancels out absolute biases caused by image complexity, with autoencoder-based reconstruction ensuring superior computational efficiency. Experiments on eight top latent diffusion models show that AEDR achieves 25.5% higher attribution accuracy than existing reconstruction-based methods, while requiring only 1% of the computational time.</li>
<li><strong>摘要：</strong>图像生成技术的快速发展使任何人都可以使用生成模型创建影像图像，从而引发了重大的安全问题。为了减轻恶意使用，追踪此类图像的起源至关重要。基于重建的归因方法提供了一种有希望的解决方案，但是当应用于最新模型（SOTA）模型时，它们通常会降低准确性和高计算成本。为了应对这些挑战，我们提出了AEDR（自动编码器双重重建），这是一种新型的无培训归因方法，专为具有连续自动编码器的生成模型而设计。与依赖单个重建损失值的现有基于重建的方法不同，AEDR使用模型的自动编码器执行两个连续的重建，并采用这两个重建损失的比率作为归因信号。使用图像均匀性度量来进一步校准该信号以提高准确性，从本质上消除了由图像复杂性引起的绝对偏差，并基于自动编码器的重建确保了卓越的计算效率。在八个顶级潜在扩散模型上的实验表明，与现有基于重建的方法相比，AEDR的归因精度高25.5％，而仅需要计算时间的1％。</li>
</ul>

<h3>Title: GENIAL: Generative Design Space Exploration via Network Inversion for Low Power Algorithmic Logic Units</h3>
<ul>
<li><strong>Authors: </strong>Maxence Bouvier, Ryan Amaudruz, Felix Arnold, Renzo Andri, Lukas Cavigelli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18989">https://arxiv.org/abs/2507.18989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18989">https://arxiv.org/pdf/2507.18989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18989]] GENIAL: Generative Design Space Exploration via Network Inversion for Low Power Algorithmic Logic Units(https://arxiv.org/abs/2507.18989)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>As AI workloads proliferate, optimizing arithmetic units is becoming increasingly important to reduce the footprint of digital systems. Conventional design flows, which often rely on manual or heuristics-based optimization, are limited in their ability to thoroughly explore the vast design space. In this paper, we introduce GENIAL, a machine learning-based framework for the automatic generation and optimization of arithmetic units, more specifically multipliers. At the core of GENIAL is a Transformer-based surrogate model trained in two stages, involving self-supervised pretraining followed by supervised finetuning, to robustly forecast key hardware metrics such as power and area from abstracted design representations. By inverting the surrogate model, GENIAL efficiently searches for new operand encodings that directly minimize power consumption in arithmetic units for specific input data distributions. Extensive experiments on large datasets demonstrate that GENIAL is consistently more sample efficient than other methods, and converges faster towards optimized designs. This enables to deploy a high-effort logic synthesis optimization flow in the loop, improving the accuracy of the surrogate model. Notably, GENIAL automatically discovers encodings that achieve up to 18% switching activity savings within multipliers on representative AI workloads compared with the conventional two's complement. We also demonstrate the versatility of our approach by achieving significant improvements on Finite State Machines, highlighting GENIAL's applicability for a wide spectrum of logic functions. Together, these advances mark a significant step toward automated Quality-of-Results-optimized combinational circuit generation for digital systems.</li>
<li><strong>摘要：</strong>随着AI的工作量扩散，优化算术单元对于减少数字系统的足迹变得越来越重要。传统的设计流通常依赖于手动或基于启发式的优化，其彻底探索庞大的设计空间的能力有限。在本文中，我们介绍了Ganial，这是一种基于机器学习的框架，用于自动生成和优化算术单元，更具体地说是乘数。 GANIAL的核心是基于变压器的替代模型，该模型在两个阶段进行了训练，涉及自我监督的预审议，然后进行监督的登录，以牢固地预测来自抽象设计表示的电源和区域，例如电源和区域。通过颠倒替代模型，Genial有效地搜索了新的操作数编码，这些编码直接最大程度地减少了算术单元中特定输入数据分布的功耗。大型数据集上的广泛实验表明，与其他方法相比，稳定的样本始终更有效，并且会更快地收敛到优化的设计。这使得可以在循环中部署高效率逻辑合成优化流，从而提高了替代模型的准确性。值得注意的是，与传统两者的补充相比，Genial会自动发现在代表性AI工作负载上节省多达18％的切换活动的编码。我们还通过对有限状态机器的重大改进来证明我们的方法的多功能性，从而强调了Genial对广泛逻辑功能的适用性。这些进步共同迈出了迈出的重要一步，迈出了数字系统的自动化质量优化的组合电路生成。</li>
</ul>

<h3>Title: GPSMamba: A Global Phase and Spectral Prompt-guided Mamba for Infrared Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Yongsong Huang, Tomo Miyazaki, Xiaofeng Liu, Shinichiro Omachi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.18998">https://arxiv.org/abs/2507.18998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.18998">https://arxiv.org/pdf/2507.18998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.18998]] GPSMamba: A Global Phase and Spectral Prompt-guided Mamba for Infrared Image Super-Resolution(https://arxiv.org/abs/2507.18998)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution</a></li>
<li><strong>Abstract: </strong>Infrared Image Super-Resolution (IRSR) is challenged by the low contrast and sparse textures of infrared data, requiring robust long-range modeling to maintain global coherence. While State-Space Models like Mamba offer proficiency in modeling long-range dependencies for this task, their inherent 1D causal scanning mechanism fragments the global context of 2D images, hindering fine-detail restoration. To address this, we propose Global Phase and Spectral Prompt-guided Mamba (GPSMamba), a framework that synergizes architectural guidance with non-causal supervision. First, our Adaptive Semantic-Frequency State Space Module (ASF-SSM) injects a fused semantic-frequency prompt directly into the Mamba block, integrating non-local context to guide reconstruction. Then, a novel Thermal-Spectral Attention and Phase Consistency Loss provides explicit, non-causal supervision to enforce global structural and spectral fidelity. By combining these two innovations, our work presents a systematic strategy to mitigate the limitations of causal modeling. Extensive experiments demonstrate that GPSMamba achieves state-of-the-art performance, validating our approach as a powerful new paradigm for infrared image restoration. Code is available at this https URL.</li>
<li><strong>摘要：</strong>红外图像超分辨率（IRSR）受到红外数据的较低对比度和稀疏纹理的挑战，需要坚固的远程建模才能维持全局相干性。尽管Mamba之类的状态空间模型提供了对此任务进行长期依赖性建模的熟练程度，但其固有的1D因果扫描机制片段片段片段构成了2D图像的全局上下文，从而阻碍了细节修复。为了解决这个问题，我们提出了全球阶段和频谱及时引导的Mamba（GPSMAMBA），该框架通过非伴侣监督协同建筑指导。首先，我们的自适应语义频率状态空间模块（ASF-SSM）将融合的语义频率提示直接注入Mamba块，从而集成了非本地上下文以指导重建。然后，一种新型的热光谱关注和相位一致性损失提供了明确的非毒药监督，以实施全球结构和光谱忠诚度。通过结合这两项创新，我们的工作提出了一种系统的策略来减轻因果建模的局限性。广泛的实验表明，GPSMAMBA实现了最先进的性能，从而证实了我们作为红外图像恢复的强大新范式的方法。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Enhancing Reward Models for High-quality Image Generation: Beyond Text-Image Alignment</h3>
<ul>
<li><strong>Authors: </strong>Ying Ba, Tianyu Zhang, Yalong Bai, Wenyi Mo, Tao Liang, Bing Su, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19002">https://arxiv.org/abs/2507.19002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19002">https://arxiv.org/pdf/2507.19002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19002]] Enhancing Reward Models for High-quality Image Generation: Beyond Text-Image Alignment(https://arxiv.org/abs/2507.19002)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Contemporary image generation systems have achieved high fidelity and superior aesthetic quality beyond basic text-image alignment. However, existing evaluation frameworks have failed to evolve in parallel. This study reveals that human preference reward models fine-tuned based on CLIP and BLIP architectures have inherent flaws: they inappropriately assign low scores to images with rich details and high aesthetic value, creating a significant discrepancy with actual human aesthetic preferences. To address this issue, we design a novel evaluation score, ICT (Image-Contained-Text) score, that achieves and surpasses the objectives of text-image alignment by assessing the degree to which images represent textual content. Building upon this foundation, we further train an HP (High-Preference) score model using solely the image modality to enhance image aesthetics and detail quality while maintaining text-image alignment. Experiments demonstrate that the proposed evaluation model improves scoring accuracy by over 10\% compared to existing methods, and achieves significant results in optimizing state-of-the-art text-to-image models. This research provides theoretical and empirical support for evolving image generation technology toward higher-order human aesthetic preferences. Code is available at this https URL.</li>
<li><strong>摘要：</strong>当代图像产生系统已经达到了高忠诚度和卓越的美学质量，而不是基本的文本图像对齐。但是，现有的评估框架未能并行发展。这项研究表明，基于剪辑和Blip体系结构进行微调微调的人类偏好奖励模型具有固有的缺陷：它们不适当地将低分分配给具有丰富细节和高审美价值的图像，从而与实际的人类美学偏好造成了显着差异。为了解决这个问题，我们设计了一个新颖的评估得分，即ICT（图像包含的文本）分数，通过评估图像代表文本内容的程度来实现和超越文本图像对齐的目标。在这个基础的基础上，我们仅使用图像模式进一步训练HP（高优先）得分模型，以增强图像美学和详细信息质量，同时保持文本图像对齐。实验表明，与现有方法相比，提出的评估模型将评分精度提高了10 \％以上，并且在优化最新的文本对图像模型方面取得了重大结果。这项研究为将图像生成技术发展为高阶人类美学偏好提供了理论和经验支持。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: A diffusion-based generative model for financial time series via geometric Brownian motion</h3>
<ul>
<li><strong>Authors: </strong>Gihun Kim, Sun-Yong Choi, Yeoneung Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19003">https://arxiv.org/abs/2507.19003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19003">https://arxiv.org/pdf/2507.19003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19003]] A diffusion-based generative model for financial time series via geometric Brownian motion(https://arxiv.org/abs/2507.19003)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose a novel diffusion-based generative framework for financial time series that incorporates geometric Brownian motion (GBM), the foundation of the Black--Scholes theory, into the forward noising process. Unlike standard score-based models that treat price trajectories as generic numerical sequences, our method injects noise proportionally to asset prices at each time step, reflecting the heteroskedasticity observed in financial time series. By accurately balancing the drift and diffusion terms, we show that the resulting log-price process reduces to a variance-exploding stochastic differential equation, aligning with the formulation in score-based generative models. The reverse-time generative process is trained via denoising score matching using a Transformer-based architecture adapted from the Conditional Score-based Diffusion Imputation (CSDI) framework. Empirical evaluations on historical stock data demonstrate that our model reproduces key stylized facts heavy-tailed return distributions, volatility clustering, and the leverage effect more realistically than conventional diffusion models.</li>
<li><strong>摘要：</strong>我们提出了一个新型的基于扩散的生成框架，用于财务时间序列，该框架结合了几何布朗运动（GBM），即黑色 -  choles理论的基础，纳入前向no脉过程。与将价格轨迹视为通用数值序列的标准分数模型不同，我们的方法在每个时间步骤中与资产价格成比例地注入噪音，反映了财务时间序列中观察到的异性恋性。通过准确平衡漂移和扩散项，我们表明所得的对数价格过程将减少到方差探索随机微分方程，与基于得分的生成模型中的公式保持一致。通过使用基于条件分数的扩散插补（CSDI）框架改编的基于变压器的体系结构，通过将分数匹配来训练反向时间生成过程。对历史库存数据的经验评估表明，与常规扩散模型相比，我们的模型重现了重型返回分布，波动率聚类和杠杆作用的重型返回分布，波动率群集和杠杆作用。</li>
</ul>

<h3>Title: MedIQA: A Scalable Foundation Model for Prompt-Driven Medical Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Siyi Xun, Yue Sun, Jingkun Chen, Zitong Yu, Tong Tong, Xiaohong Liu, Mingxiang Wu, Tao Tan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19004">https://arxiv.org/abs/2507.19004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19004">https://arxiv.org/pdf/2507.19004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19004]] MedIQA: A Scalable Foundation Model for Prompt-Driven Medical Image Quality Assessment(https://arxiv.org/abs/2507.19004)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Rapid advances in medical imaging technology underscore the critical need for precise and automated image quality assessment (IQA) to ensure diagnostic accuracy. Existing medical IQA methods, however, struggle to generalize across diverse modalities and clinical scenarios. In response, we introduce MedIQA, the first comprehensive foundation model for medical IQA, designed to handle variability in image dimensions, modalities, anatomical regions, and types. We developed a large-scale multi-modality dataset with plentiful manually annotated quality scores to support this. Our model integrates a salient slice assessment module to focus on diagnostically relevant regions feature retrieval and employs an automatic prompt strategy that aligns upstream physical parameter pre-training with downstream expert annotation fine-tuning. Extensive experiments demonstrate that MedIQA significantly outperforms baselines in multiple downstream tasks, establishing a scalable framework for medical IQA and advancing diagnostic workflows and clinical decision-making.</li>
<li><strong>摘要：</strong>医学成像技术的快速进步强调了对精确和自动化图像质量评估（IQA）的关键需求，以确保诊断准确性。但是，现有的医学IQA方法很难跨越各种方式和临床方案。作为回应，我们介绍了Mediqa，Mediqa是医学IQA的第一个综合基础模型，旨在处理图像维度，方式，解剖区域和类型的变异性。我们开发了一个大规模的多模式数据集，并具有丰富的手动注释质量分数来支持这一点。我们的模型集成了一个显着的切片评估模块，以专注于诊断相关的区域功能检索，并采用自动及时策略，该策略将上游物理参数预训练与下游专家注释微调相一致。广泛的实验表明，MEDIQA在多个下游任务中的表现明显优于基线，建立了可扩展的医学IQA框架，并推进了诊断工作流程和临床决策。</li>
</ul>

<h3>Title: A Survey of Multimodal Hallucination Evaluation and Detection</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Chen (1 and 2), Yuecong Min (1 and 2), Jie Zhang (1 and 2), Bei Yan (1 and 2), Jiahao Wang (3), Xiaozhen Wang (3), Shiguang Shan (1 and 2) ((1) State Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences (CAS) (2) University of Chinese Academy of Sciences (3) Trustworthy Technology and Engineering Laboratory, Huawei)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19024">https://arxiv.org/abs/2507.19024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19024">https://arxiv.org/pdf/2507.19024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19024]] A Survey of Multimodal Hallucination Evaluation and Detection(https://arxiv.org/abs/2507.19024)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multi-modal Large Language Models (MLLMs) have emerged as a powerful paradigm for integrating visual and textual information, supporting a wide range of multi-modal tasks. However, these models often suffer from hallucination, producing content that appears plausible but contradicts the input content or established world knowledge. This survey offers an in-depth review of hallucination evaluation benchmarks and detection methods across Image-to-Text (I2T) and Text-to-image (T2I) generation tasks. Specifically, we first propose a taxonomy of hallucination based on faithfulness and factuality, incorporating the common types of hallucinations observed in practice. Then we provide an overview of existing hallucination evaluation benchmarks for both T2I and I2T tasks, highlighting their construction process, evaluation objectives, and employed metrics. Furthermore, we summarize recent advances in hallucination detection methods, which aims to identify hallucinated content at the instance level and serve as a practical complement of benchmark-based evaluation. Finally, we highlight key limitations in current benchmarks and detection methods, and outline potential directions for future research.</li>
<li><strong>摘要：</strong>多模式大型语言模型（MLLM）已成为一种强大的范式，用于集成视觉和文本信息，支持广泛的多模式任务。但是，这些模型通常会遭受幻觉的困扰，产生似乎合理的内容，但与输入内容或既定的世界知识相矛盾。这项调查对图像到文本（I2T）和文本对图像（T2I）生成任务进行了幻觉评估基准和检测方法的深入审查。具体而言，我们首先提出了基于忠诚和事实的幻觉分类法，并结合了在实践中观察到的幻觉的常见类型。然后，我们概述了T2I和I2T任务的现有幻觉评估基准，强调了它们的施工过程，评估目标和所采用的指标。此外，我们总结了幻觉检测方法的最新进展，该方法旨在在实例级别识别幻觉内容，并作为基于基准评估的实际补充。最后，我们重点介绍了当前基准和检测方法的关键局限性，并概述了未来研究的潜在方向。</li>
</ul>

<h3>Title: A New One-Shot Federated Learning Framework for Medical Imaging Classification with Feature-Guided Rectified Flow and Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Yufei Ma, Hanwen Zhang, Qiya Yang, Guibo Luo, Yuesheng Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19045">https://arxiv.org/abs/2507.19045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19045">https://arxiv.org/pdf/2507.19045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19045]] A New One-Shot Federated Learning Framework for Medical Imaging Classification with Feature-Guided Rectified Flow and Knowledge Distillation(https://arxiv.org/abs/2507.19045)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In multi-center scenarios, One-Shot Federated Learning (OSFL) has attracted increasing attention due to its low communication overhead, requiring only a single round of transmission. However, existing generative model-based OSFL methods suffer from low training efficiency and potential privacy leakage in the healthcare domain. Additionally, achieving convergence within a single round of model aggregation is challenging under non-Independent and Identically Distributed (non-IID) data. To address these challenges, in this paper a modified OSFL framework is proposed, in which a new Feature-Guided Rectified Flow Model (FG-RF) and Dual-Layer Knowledge Distillation (DLKD) aggregation method are developed. FG-RF on the client side accelerates generative modeling in medical imaging scenarios while preserving privacy by synthesizing feature-level images rather than pixel-level images. To handle non-IID distributions, DLKD enables the global student model to simultaneously mimic the output logits and align the intermediate-layer features of client-side teacher models during aggregation. Experimental results on three non-IID medical imaging datasets show that our new framework and method outperform multi-round federated learning approaches, achieving up to 21.73% improvement, and exceeds the baseline FedISCA by an average of 21.75%. Furthermore, our experiments demonstrate that feature-level synthetic images significantly reduce privacy leakage risks compared to pixel-level synthetic images.</li>
<li><strong>摘要：</strong>在多中心方案中，由于其沟通开销低，一轮传输的沟通量较低，因此一轮的沟通量引起了越来越多的关注，仅需一轮传输而引起了人们的关注。但是，现有的基于生成模型的OSFL方法遭受了训练效率低下和医疗保健领域潜在的隐私泄漏。另外，在非独立和相同分布的（非IID）数据下，在单轮模型聚集中实现收敛性是有挑战性的。为了应对这些挑战，在本文中提出了一个修改的OSFL框架，其中开发了一种新的特征引导的整流流模型（FG-RF）和双层知识蒸馏（DLKD）聚合方法。客户端上的FG-RF在医学成像方案中加速了生成建模，同时通过合成特征级图像而不是像素级图像来保留隐私。为了处理非IID分布，DLKD使全球学生模型可以同时模拟输出逻辑，并在聚合过程中对客户端教师模型的中间层特征对齐。三个非IID医学成像数据集的实验结果表明，我们的新框架和方法的表现优于多轮联邦学习方法，提高了21.73％，并超过基线Fedisca的平均21.75％。此外，我们的实验表明，与像素级合成图像相比，特征级合成图像显着降低了隐私泄漏风险。</li>
</ul>

<h3>Title: Closing the Modality Gap for Mixed Modality Search</h3>
<ul>
<li><strong>Authors: </strong>Binxu Li, Yuhui Zhang, Xiaohan Wang, Weixin Liang, Ludwig Schmidt, Serena Yeung-Levy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19054">https://arxiv.org/abs/2507.19054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19054">https://arxiv.org/pdf/2507.19054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19054]] Closing the Modality Gap for Mixed Modality Search(https://arxiv.org/abs/2507.19054)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Mixed modality search -- retrieving information across a heterogeneous corpus composed of images, texts, and multimodal documents -- is an important yet underexplored real-world application. In this work, we investigate how contrastive vision-language models, such as CLIP, perform on the mixed modality search task. Our analysis reveals a critical limitation: these models exhibit a pronounced modality gap in the embedding space, where image and text embeddings form distinct clusters, leading to intra-modal ranking bias and inter-modal fusion failure. To address this issue, we propose GR-CLIP, a lightweight post-hoc calibration method that removes the modality gap in CLIP's embedding space. Evaluated on MixBench -- the first benchmark specifically designed for mixed modality search -- GR-CLIP improves NDCG@10 by up to 26 percentage points over CLIP, surpasses recent vision-language generative embedding models by 4 percentage points, while using 75x less compute.</li>
<li><strong>摘要：</strong>混合模态搜索 - 通过图像，文本和多模式文档组成的异质语料库检索信息 - 是一个重要但毫无疑问的现实世界应用程序。在这项工作中，我们调查了对比的视觉模型（例如剪辑）如何在混合模态搜索任务上执行。我们的分析揭示了一个关键的局限性：这些模型在嵌入空间中表现出明显的模态差距，其中图像和文本嵌入形成不同的簇，从而导致模式内排名偏置和模式间融合失败。为了解决这个问题，我们提出了GR-CLIP，这是一种轻巧的事后校准方法，可消除夹子嵌入空间中的模态差距。在Mixbench上进行评估 - 专门为混合方式搜索设计的第一个基准 -  GR-CLIP将NDCG@10提高到夹子上的26个百分点，超过了最近的Vision-Language生成嵌入模型，而使用75倍的计算。</li>
</ul>

<h3>Title: ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with Concept Relation Alignment</h3>
<ul>
<li><strong>Authors: </strong>Chong Xia, Shengjun Zhang, Fangfu Liu, Chang Liu, Khodchaphun Hirunyaratsameewong, Yueqi Duan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19058">https://arxiv.org/abs/2507.19058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19058">https://arxiv.org/pdf/2507.19058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19058]] ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with Concept Relation Alignment(https://arxiv.org/abs/2507.19058)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Perpetual 3D scene generation aims to produce long-range and coherent 3D view sequences, which is applicable for long-term video synthesis and 3D scene reconstruction. Existing methods follow a "navigate-and-imagine" fashion and rely on outpainting for successive view expansion. However, the generated view sequences suffer from semantic drift issue derived from the accumulated deviation of the outpainting module. To tackle this challenge, we propose ScenePainter, a new framework for semantically consistent 3D scene generation, which aligns the outpainter's scene-specific prior with the comprehension of the current scene. To be specific, we introduce a hierarchical graph structure dubbed SceneConceptGraph to construct relations among multi-level scene concepts, which directs the outpainter for consistent novel views and can be dynamically refined to enhance diversity. Extensive experiments demonstrate that our framework overcomes the semantic drift issue and generates more consistent and immersive 3D view sequences. Project Page: this https URL.</li>
<li><strong>摘要：</strong>永久3D场景生成旨在产生长期和连贯的3D视图序列，该序列适用于长期视频综合和3D场景重建。现有方法遵循“导航和象征”的时尚，并依赖于连续的视图扩展。但是，生成的视图序列遇到了源于支出模块的累积偏差而得出的语义漂移问题。为了应对这一挑战，我们提出了ScenePainter，这是一种用于语义上一致的3D场景生成的新框架，该框架与当前场景的理解相吻合。具体来说，我们引入了一个层次图结构，称为SceneConcepgraph，以在多级场景概念之间构建关系，该概念指导校长以保持一致的小说视图，并可以动态地完善以增强多样性。广泛的实验表明，我们的框架克服了语义漂移问题，并产生了更一致和沉浸式的3D视图序列。项目页面：此HTTPS URL。</li>
</ul>

<h3>Title: Cross-Subject Mind Decoding from Inaccurate Representations</h3>
<ul>
<li><strong>Authors: </strong>Yangyang Xu, Bangzhen Liu, Wenqi Shao, Yong Du, Shengfeng He, Tingting Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19071">https://arxiv.org/abs/2507.19071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19071">https://arxiv.org/pdf/2507.19071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19071]] Cross-Subject Mind Decoding from Inaccurate Representations(https://arxiv.org/abs/2507.19071)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Decoding stimulus images from fMRI signals has advanced with pre-trained generative models. However, existing methods struggle with cross-subject mappings due to cognitive variability and subject-specific differences. This challenge arises from sequential errors, where unidirectional mappings generate partially inaccurate representations that, when fed into diffusion models, accumulate errors and degrade reconstruction fidelity. To address this, we propose the Bidirectional Autoencoder Intertwining framework for accurate decoded representation prediction. Our approach unifies multiple subjects through a Subject Bias Modulation Module while leveraging bidirectional mapping to better capture data distributions for precise representation prediction. To further enhance fidelity when decoding representations into stimulus images, we introduce a Semantic Refinement Module to improve semantic representations and a Visual Coherence Module to mitigate the effects of inaccurate visual representations. Integrated with ControlNet and Stable Diffusion, our method outperforms state-of-the-art approaches on benchmark datasets in both qualitative and quantitative evaluations. Moreover, our framework exhibits strong adaptability to new subjects with minimal training samples.</li>
<li><strong>摘要：</strong>来自fMRI信号的解码刺激图像已通过预先训练的生成模型进行了进展。但是，由于认知变异性和特定于主题的差异，现有方法与跨主体映射困难。这一挑战源于顺序误差，其中单向映射会产生部分不准确的表示，当馈入扩散模型时，会累积误差并降低重建保真度。为了解决这个问题，我们提出了双向自动编码器交织框架以进行准确的解码表示预测。我们的方法通过主题偏差调制模块统一了多个受试者，同时利用双向映射以更好地捕获数据分布以进行精确表示预测。为了进一步增强在将表示形式解码为刺激图像时，我们介绍了一个语义改进模块，以改善语义表示和视觉连贯模块，以减轻不准确的视觉表示影响。与ControlNet和稳定扩散集成在一起，我们的方法在定性和定量评估中都优于基准数据集上的最先进方法。此外，我们的框架具有最少的培训样本对新主题的强大适应性。</li>
</ul>

<h3>Title: Clustering-Oriented Generative Attribute Graph Imputation</h3>
<ul>
<li><strong>Authors: </strong>Mulin Chen, Bocheng Wang, Jiaxin Zhong, Zongcheng Miao, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19085">https://arxiv.org/abs/2507.19085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19085">https://arxiv.org/pdf/2507.19085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19085]] Clustering-Oriented Generative Attribute Graph Imputation(https://arxiv.org/abs/2507.19085)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Attribute-missing graph clustering has emerged as a significant unsupervised task, where only attribute vectors of partial nodes are available and the graph structure is intact. The related models generally follow the two-step paradigm of imputation and refinement. However, most imputation approaches fail to capture class-relevant semantic information, leading to sub-optimal imputation for clustering. Moreover, existing refinement strategies optimize the learned embedding through graph reconstruction, while neglecting the fact that some attributes are uncorrelated with the graph. To remedy the problems, we establish the Clustering-oriented Generative Imputation with reliable Refinement (CGIR) model. Concretely, the subcluster distributions are estimated to reveal the class-specific characteristics precisely, and constrain the sampling space of the generative adversarial module, such that the imputation nodes are impelled to align with the correct clusters. Afterwards, multiple subclusters are merged to guide the proposed edge attention network, which identifies the edge-wise attributes for each class, so as to avoid the redundant attributes in graph reconstruction from disturbing the refinement of overall embedding. To sum up, CGIR splits attribute-missing graph clustering into the search and mergence of subclusters, which guides to implement node imputation and refinement within a unified framework. Extensive experiments prove the advantages of CGIR over state-of-the-art competitors.</li>
<li><strong>摘要：</strong>属性中误差图聚类已成为一项重要的无监督任务，其中仅可用部分节点的属性向量，并且图形结构是完整的。相关模型通常遵循插补和改进的两步范式。但是，大多数插补方法无法捕获相关的语义信息，从而导致了群集的次级插补。此外，现有的改进策略通过图形重建优化了学习的嵌入，同时忽略了某些属性与图形不相关的事实。为了解决这些问题，我们建立了面向聚类的生成归因（CGIR）模型。具体而言，估计亚集群分布可以精确地揭示特定于类的特征，并约束生成对抗模块的采样空间，以便促使插补节点与正确的簇保持一致。之后，合并了多个子频率以指导提出的边缘注意网络，该网络标识了每个类的边缘属性，以免避免图形重建中的冗余属性，从而扰乱整体嵌入的细化。总而言之，CGIR将属性放错的图形聚类分为子截图的搜索和合并，该图指导在统一的框架内实现节点推出和完善。广泛的实验证明了CGIR比最先进的竞争对手的优势。</li>
</ul>

<h3>Title: MedSymmFlow: Bridging Generative Modeling and Classification in Medical Imaging through Symmetrical Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Francisco Caetano, Lemar Abdi, Christiaan Viviers, Amaan Valiuddin, Fons van der Sommen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19098">https://arxiv.org/abs/2507.19098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19098">https://arxiv.org/pdf/2507.19098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19098]] MedSymmFlow: Bridging Generative Modeling and Classification in Medical Imaging through Symmetrical Flow Matching(https://arxiv.org/abs/2507.19098)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Reliable medical image classification requires accurate predictions and well-calibrated uncertainty estimates, especially in high-stakes clinical settings. This work presents MedSymmFlow, a generative-discriminative hybrid model built on Symmetrical Flow Matching, designed to unify classification, generation, and uncertainty quantification in medical imaging. MedSymmFlow leverages a latent-space formulation that scales to high-resolution inputs and introduces a semantic mask conditioning mechanism to enhance diagnostic relevance. Unlike standard discriminative models, it naturally estimates uncertainty through its generative sampling process. The model is evaluated on four MedMNIST datasets, covering a range of modalities and pathologies. The results show that MedSymmFlow matches or exceeds the performance of established baselines in classification accuracy and AUC, while also delivering reliable uncertainty estimates validated by performance improvements under selective prediction.</li>
<li><strong>摘要：</strong>可靠的医学图像分类需要准确的预测和良好的不确定性估计值，尤其是在高风险临床环境中。这项工作介绍了MedSymmflow，这是一种基于对称流量匹配的生成歧视混合模型，旨在统一医学成像中的分类，生成和不确定性定量。 MedSymmflow利用一种潜在的空间配方，该公式扩展到高分辨率输入并引入语义掩盖调理机制以增强诊断相关性。与标准判别模型不同，它通过其生成抽样过程自然估计不确定性。该模型在四个MedMnist数据集上进行了评估，涵盖了一系列方式和病理。结果表明，MedSymmflow在分类精度和AUC中匹配或超过已建立基线的性能，同时还提供了可靠的不确定性估计值，这些估计值通过选择性预测下的绩效改进验证。</li>
</ul>

<h3>Title: LISA: A Layer-wise Integration and Suppression Approach for Hallucination Mitigation in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhihui Guo, Xin Man, Hui Xu, Jie Shao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19110">https://arxiv.org/abs/2507.19110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19110">https://arxiv.org/pdf/2507.19110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19110]] LISA: A Layer-wise Integration and Suppression Approach for Hallucination Mitigation in Multimodal Large Language Models(https://arxiv.org/abs/2507.19110)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) excel in vision-language tasks such as image captioning but remain prone to object hallucinations, where they describe objects that do not appear in the image. To mitigate this, we propose \textbf{LISA}, a \textbf{L}ayer-wise \textbf{I}ntegration and \textbf{S}uppression \textbf{A}pproach that enhances generation consistency through hierarchical modulation and multi-layer fusion. LISA leverages the functional hierarchy within MLLMs, where shallow layers provide visual grounding, middle layers encode semantics, and deep layers tend to amplify spurious signals. First, zone-specific spectral modulation stabilizes attention by suppressing over-amplified activations in deeper layers while preserving alignment cues in earlier layers. Second, token-level logits from selected layers are fused via anchor-based routing, with token-wise anchor selection and soft logit fusion enabling adaptive integration during decoding. LISA is fully \textbf{plug-and-play} and can be seamlessly integrated into existing MLLMs, including Qwen2.5-VL. Experiments on multiple benchmarks show that LISA reduces hallucinations by up to 53.6\% in $\mathrm{CHAIR}_I$ and improves POPE F1 by 4.5\%, demonstrating strong generalization across models and tasks.</li>
<li><strong>摘要：</strong>多模式的大语言模型（MLLM）在视觉语言任务（例如图像字幕）中表现出色，但仍然容易出现对象幻觉，它们描述了图像中未出现的对象。为了减轻这种情况，我们提出\ textbf {lisa}，a \ textbf {l} ayer-wise \ textbf {i} ntegration和\ textbf {s} uppression \ textbf {a} a} pprouck procect procect in s texprouck cleact in s rounightical and ruseartial proceention contence contection is fenentions contection is s textbf {i} ntegration \ textbf {i}。 Lisa利用MLLM中的功能层次结构，其中浅层提供视觉接地，中间层编码语义，而深层倾向于放大虚假信号。首先，区域特异性光谱调制通过抑制更深层的过度放大激活，同时保留早期层的比对线索来稳定注意力。其次，来自选定层的令牌级逻辑通过基于锚的路由融合，具有令牌的锚定选择和软logit融合在解码过程中启用自适应积分。 LISA完全\ textbf {插件和播放}，可以无缝集成到现有的MLLM中，包括QWEN2.5-VL。多个基准测试的实验表明，丽莎在$ \ mathrm {cair} _i $中最多将幻觉降低了53.6 \％，并将教皇F1提高了4.5 \％，表明了跨模型和任务的强烈概括。</li>
</ul>

<h3>Title: Preserving Topological and Geometric Embeddings for Point Cloud Recovery</h3>
<ul>
<li><strong>Authors: </strong>Kaiyue Zhou, Zelong Tan, Hongxiao Wang, Ya-li Li, Shengjin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19121">https://arxiv.org/abs/2507.19121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19121">https://arxiv.org/pdf/2507.19121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19121]] Preserving Topological and Geometric Embeddings for Point Cloud Recovery(https://arxiv.org/abs/2507.19121)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation</a></li>
<li><strong>Abstract: </strong>Recovering point clouds involves the sequential process of sampling and restoration, yet existing methods struggle to effectively leverage both topological and geometric attributes. To address this, we propose an end-to-end architecture named \textbf{TopGeoFormer}, which maintains these critical features throughout the sampling and restoration phases. First, we revisit traditional feature extraction techniques to yield topological embedding using a continuous mapping of relative relationships between neighboring points, and integrate it in both phases for preserving the structure of the original space. Second, we propose the \textbf{InterTwining Attention} to fully merge topological and geometric embeddings, which queries shape with local awareness in both phases to form a learnable shape context facilitated with point-wise, point-shape-wise, and intra-shape features. Third, we introduce a full geometry loss and a topological constraint loss to optimize the embeddings in both Euclidean and topological spaces. The geometry loss uses inconsistent matching between coarse-to-fine generations and targets for reconstructing better geometric details, and the constraint loss limits embedding variances for better approximation of the topological space. In experiments, we comprehensively analyze the circumstances using the conventional and learning-based sampling/upsampling algorithms. The quantitative and qualitative results demonstrate that our method significantly outperforms existing sampling and recovery methods.</li>
<li><strong>摘要：</strong>恢复点云涉及采样和恢复的顺序过程，但现有的方法难以有效利用拓扑和几何属性。为了解决这个问题，我们提出了一个名为\ textbf {topgeoformer}的端到端体系结构，该体系结构在整个采样和恢复阶段中维护这些关键特征。首先，我们重新审视传统的特征提取技术，以使用相邻点之间的相对关系的连续映射产生拓扑嵌入，并将其集成在两个阶段中以保存原始空间的结构。其次，我们提出\ textbf {相互交织的注意}以完全合并拓扑和几何嵌入，它们在两个阶段都以局部意识来询问形状，以形成可学习的形状上下文，以方面的尖位，点形的良好，内部形状和形状的特征促进。第三，我们引入了完整的几何损失和拓扑约束损失，以优化欧几里得和拓扑空间中的嵌入。几何损失使用粗到1世代之间不一致的匹配来重建更好的几何细节，以及嵌入差异的约束损耗限制以更好地近似拓扑空间。在实验中，我们使用常规和基于学习的采样/UPS采样算法对情况进行了全面分析。定量和定性结果表明，我们的方法显着优于现有的抽样和恢复方法。</li>
</ul>

<h3>Title: Continual Learning-Based Unified Model for Unpaired Image Restoration Tasks</h3>
<ul>
<li><strong>Authors: </strong>Kotha Kartheek, Lingamaneni Gnanesh Chowdary, Snehasis Mukherjee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19184">https://arxiv.org/abs/2507.19184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19184">https://arxiv.org/pdf/2507.19184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19184]] Continual Learning-Based Unified Model for Unpaired Image Restoration Tasks(https://arxiv.org/abs/2507.19184)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Restoration of images contaminated by different adverse weather conditions such as fog, snow, and rain is a challenging task due to the varying nature of the weather conditions. Most of the existing methods focus on any one particular weather conditions. However, for applications such as autonomous driving, a unified model is necessary to perform restoration of corrupted images due to different weather conditions. We propose a continual learning approach to propose a unified framework for image restoration. The proposed framework integrates three key innovations: (1) Selective Kernel Fusion layers that dynamically combine global and local features for robust adaptive feature selection; (2) Elastic Weight Consolidation (EWC) to enable continual learning and mitigate catastrophic forgetting across multiple restoration tasks; and (3) a novel Cycle-Contrastive Loss that enhances feature discrimination while preserving semantic consistency during domain translation. Further, we propose an unpaired image restoration approach to reduce the dependance of the proposed approach on the training data. Extensive experiments on standard benchmark datasets for dehazing, desnowing and deraining tasks demonstrate significant improvements in PSNR, SSIM, and perceptual quality over the state-of-the-art.</li>
<li><strong>摘要：</strong>由于天气条件的不同性质，雾气，雪和雨等不同不利天气条件（例如雾，雪和雨水）污染的图像恢复是一项艰巨的任务。大多数现有方法都集中在任何一种特定的天气条件上。但是，对于诸如自动驾驶的应用程序，由于天气条件的不同，必须进行统一模型来恢复损坏的图像。我们提出了一种持续的学习方法，以提出一个统一的图像恢复框架。提出的框架集成了三个关键创新：（1）动态结合全局和局部特征以进行健壮自适应特征选择的选择性内核融合层； （2）弹性重量巩固（EWC）可以持续学习并减轻多个恢复任务的灾难性遗忘； （3）一种新型的周期对抗性损失，可以增强特征歧视，同时保持域翻译过程中的语义一致性。此外，我们提出了一种未配对的图像恢复方法，以减少拟议方法对培训数据的依赖性。在标准基准数据集上进行除去，否定和降低任务的广泛实验表明，PSNR，SSIM和知觉质量在最新的方面进行了显着改善。</li>
</ul>

<h3>Title: Reconstruct or Generate: Exploring the Spectrum of Generative Modeling for Cardiac MRI</h3>
<ul>
<li><strong>Authors: </strong>Niklas Bubeck, Yundi Zhang, Suprosanna Shit, Daniel Rueckert, Jiazhen Pan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19186">https://arxiv.org/abs/2507.19186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19186">https://arxiv.org/pdf/2507.19186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19186]] Reconstruct or Generate: Exploring the Spectrum of Generative Modeling for Cardiac MRI(https://arxiv.org/abs/2507.19186)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>In medical imaging, generative models are increasingly relied upon for two distinct but equally critical tasks: reconstruction, where the goal is to restore medical imaging (usually inverse problems like inpainting or superresolution), and generation, where synthetic data is created to augment datasets or carry out counterfactual analysis. Despite shared architecture and learning frameworks, they prioritize different goals: generation seeks high perceptual quality and diversity, while reconstruction focuses on data fidelity and faithfulness. In this work, we introduce a "generative model zoo" and systematically analyze how modern latent diffusion models and autoregressive models navigate the reconstruction-generation spectrum. We benchmark a suite of generative models across representative cardiac medical imaging tasks, focusing on image inpainting with varying masking ratios and sampling strategies, as well as unconditional image generation. Our findings show that diffusion models offer superior perceptual quality for unconditional generation but tend to hallucinate as masking ratios increase, whereas autoregressive models maintain stable perceptual performance across masking levels, albeit with generally lower fidelity.</li>
<li><strong>摘要：</strong>在医学成像中，生成模型越来越依赖于两个不同但同样至关重要的任务：重建，目的是恢复医学成像（通常是逆问题，例如涂料或上分辨率），以及在其中创建合成数据以增强数据集或进行反事实分析的生成。尽管共同的建筑和学习框架，但它们还优先考虑不同的目标：生成寻求高知名度和多样性，而重建则集中在数据保真度和忠诚上。在这项工作中，我们引入了一个“生成模型动物园”，并系统地分析了现代潜在扩散模型和自回归模型如何导航重建生成频谱。我们在代表性的心脏医学成像任务中基准了一套生成模型，重点是与不同的掩蔽率和采样策略以及无条件的图像生成的图像插图。我们的发现表明，扩散模型为无条件产生提供了卓越的感知质量，但随着掩蔽比的增加而倾向于幻觉，而自回归模型在跨掩蔽级别上保持稳定的感知性能，尽管通常具有较低的保真度。</li>
</ul>

<h3>Title: Joint Holistic and Lesion Controllable Mammogram Synthesis via Gated Conditional Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Xin Li, Kaixiang Yang, Qiang Li, Zhiwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19201">https://arxiv.org/abs/2507.19201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19201">https://arxiv.org/pdf/2507.19201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19201]] Joint Holistic and Lesion Controllable Mammogram Synthesis via Gated Conditional Diffusion Model(https://arxiv.org/abs/2507.19201)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Mammography is the most commonly used imaging modality for breast cancer screening, driving an increasing demand for deep-learning techniques to support large-scale analysis. However, the development of accurate and robust methods is often limited by insufficient data availability and a lack of diversity in lesion characteristics. While generative models offer a promising solution for data synthesis, current approaches often fail to adequately emphasize lesion-specific features and their relationships with surrounding tissues. In this paper, we propose Gated Conditional Diffusion Model (GCDM), a novel framework designed to jointly synthesize holistic mammogram images and localized lesions. GCDM is built upon a latent denoising diffusion framework, where the noised latent image is concatenated with a soft mask embedding that represents breast, lesion, and their transitional regions, ensuring anatomical coherence between them during the denoising process. To further emphasize lesion-specific features, GCDM incorporates a gated conditioning branch that guides the denoising process by dynamically selecting and fusing the most relevant radiomic and geometric properties of lesions, effectively capturing their interplay. Experimental results demonstrate that GCDM achieves precise control over small lesion areas while enhancing the realism and diversity of synthesized mammograms. These advancements position GCDM as a promising tool for clinical applications in mammogram synthesis. Our code is available at this https URL</li>
<li><strong>摘要：</strong>乳房X线摄影是用于乳腺癌筛查的最常用的成像方式，推动了对深度学习技术的不断增长的需求，以支持大规模分析。但是，准确和健壮方法的发展通常受到数据可用性不足和病变特征缺乏多样性的限制。尽管生成模型为数据合成提供了有希望的解决方案，但当前的方法通常无法充分强调特定病变特异性特征及其与周围组织的关系。在本文中，我们提出了封闭式条件扩散模型（GCDM），这是一个新型框架，旨在共同合成整体乳房X线照片图像和局部病变。 GCDM建立在潜在的deno的扩散框架上，其中噪声潜在图像与柔软的面具嵌入，代表乳房，病变及其过渡区域，以确保它们在脱糖过程中它们之间的解剖相干性。为了进一步强调病变特异性的特征，GCDM结合了一个封闭式条件分支，该分支通过动态选择和融合病变的最相关的放射线和几何特性，从而有效地捕获其相互作用。实验结果表明，GCDM可以精确控制小病变区域，同时增强了合成乳房X线照片的现实性和多样性。这些进步将GCDM定位为乳房X线照片合成中临床应用的有前途的工具。我们的代码可在此HTTPS URL上找到</li>
</ul>

<h3>Title: Dependency-aware synthetic tabular data generation</h3>
<ul>
<li><strong>Authors: </strong>Chaithra Umesh, Kristian Schultz, Manjunath Mahendra, Saptarshi Bej, Olaf Wolkenhauer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19211">https://arxiv.org/abs/2507.19211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19211">https://arxiv.org/pdf/2507.19211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19211]] Dependency-aware synthetic tabular data generation(https://arxiv.org/abs/2507.19211)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Synthetic tabular data is increasingly used in privacy-sensitive domains such as health care, but existing generative models often fail to preserve inter-attribute relationships. In particular, functional dependencies (FDs) and logical dependencies (LDs), which capture deterministic and rule-based associations between features, are rarely or often poorly retained in synthetic datasets. To address this research gap, we propose the Hierarchical Feature Generation Framework (HFGF) for synthetic tabular data generation. We created benchmark datasets with known dependencies to evaluate our proposed HFGF. The framework first generates independent features using any standard generative model, and then reconstructs dependent features based on predefined FD and LD rules. Our experiments on four benchmark datasets with varying sizes, feature imbalance, and dependency complexity demonstrate that HFGF improves the preservation of FDs and LDs across six generative models, including CTGAN, TVAE, and GReaT. Our findings demonstrate that HFGF can significantly enhance the structural fidelity and downstream utility of synthetic tabular data.</li>
<li><strong>摘要：</strong>综合表格数据越来越多地用于隐私敏感领域，例如医疗保健，但现有的生成模型通常无法保留属性间的关系。特别是，在合成数据集中捕获确定性和基于规则的关联的功能依赖性（FDS）和逻辑依赖（LDS）很少或通常在合成数据集中保留很少。为了解决这一研究差距，我们提出了用于合成表格数据生成的分层特征生成框架（HFGF）。我们创建了具有已知依赖关系的基准数据集，以评估我们提出的HFGF。该框架首先使用任何标准生成模型生成独立的功能，然后根据预定义的FD和LD规则重建相关功能。我们在四个具有不同尺寸，功能不平衡和依赖性复杂性的基准数据集上进行的实验表明，HFGF改善了六种生成模型（包括Ctgan，TVAE和Great）的FD和LDS的保存。我们的发现表明，HFGF可以显着增强合成表格数据的结构保真度和下游效用。</li>
</ul>

<h3>Title: A Markov Categorical Framework for Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yifan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19247">https://arxiv.org/abs/2507.19247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19247">https://arxiv.org/pdf/2507.19247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19247]] A Markov Categorical Framework for Language Modeling(https://arxiv.org/abs/2507.19247)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Auto-regressive language models factorize sequence probabilities and are trained by minimizing the negative log-likelihood (NLL) objective. While empirically powerful, a deep theoretical understanding of why this simple objective yields such versatile representations remains elusive. This work introduces a unifying analytical framework using Markov Categories (MCs) to deconstruct the AR generation process and the NLL objective. We model the single-step generation map as a composition of Markov kernels in the category Stoch. This compositional view, when enriched with statistical divergences, allows us to dissect information flow and learned geometry. Our framework makes three main contributions. First, we provide a formal, information-theoretic rationale for the success of modern speculative decoding methods like EAGLE, quantifying the information surplus in hidden states that these methods exploit. Second, we formalize how NLL minimization forces the model to learn not just the next token, but the data's intrinsic conditional stochasticity, a process we analyze using categorical entropy. Third, and most centrally, we prove that NLL training acts as an implicit form of spectral contrastive learning. By analyzing the information geometry of the model's prediction head, we show that NLL implicitly forces the learned representation space to align with the eigenspectrum of a predictive similarity operator, thereby learning a geometrically structured space without explicit contrastive pairs. This compositional and information-geometric perspective reveals the deep structural principles underlying the effectiveness of modern LMs. Project Page: this https URL</li>
<li><strong>摘要：</strong>自动回归语言模型分解序列概率，并通过最大程度地降低负模样（NLL）目标来训练。尽管经验强大，但对这个简单目标为什么产生这种多功能表示的原因有深刻的理论理解仍然难以捉摸。这项工作介绍了使用马尔可夫类别（MC）来解构AR生成过程和NLL目标的统一分析框架。我们将单步生成地图建模为Markov内核类别中的组成。当富集统计差异时，这种组成视图使我们能够剖析信息流和学习的几何形状。我们的框架做出了三个主要贡献。首先，我们为现代投机解码方法（例如Eagle）的成功提供了正式的信息理论理由，从而量化了这些方法利用这些方法的隐藏状态中的信息盈余。其次，我们正式化了NLL最小化如何迫使模型不仅要学习下一个令牌，还要学习数据的固有条件随机性，这是我们使用分类熵分析的过程。第三，也是大多数中央，我们证明了NLL培训是光谱对比学习的一种隐式形式。通过分析模型预测头的信息几何形状，我们表明，NLL隐含地迫使学习的表示空间与预测性相似性操作员的特征谱保持一致，从而学习一个几何结构的空间而没有明确的对比度。这种组成和信息几何的观点揭示了现代LMS有效性的基础的深层结构原理。项目页面：此HTTPS URL</li>
</ul>

<h3>Title: BridgeNet: A Unified Multimodal Framework for Bridging 2D and 3D Industrial Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>An Xiang, Zixuan Huang, Xitong Gao, Kejiang Ye, Cheng-zhong Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19253">https://arxiv.org/abs/2507.19253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19253">https://arxiv.org/pdf/2507.19253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19253]] BridgeNet: A Unified Multimodal Framework for Bridging 2D and 3D Industrial Anomaly Detection(https://arxiv.org/abs/2507.19253)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Industrial anomaly detection for 2D objects has gained significant attention and achieved progress in anomaly detection (AD) methods. However, identifying 3D depth anomalies using only 2D information is insufficient. Despite explicitly fusing depth information into RGB images or using point cloud backbone networks to extract depth features, both approaches struggle to adequately represent 3D information in multimodal scenarios due to the disparities among different modal information. Additionally, due to the scarcity of abnormal samples in industrial data, especially in multimodal scenarios, it is necessary to perform anomaly generation to simulate real-world abnormal samples. Therefore, we propose a novel unified multimodal anomaly detection framework to address these issues. Our contributions consist of 3 key aspects. (1) We extract visible depth information from 3D point cloud data simply and use 2D RGB images to represent appearance, which disentangles depth and appearance to support unified anomaly generation. (2) Benefiting from the flexible input representation, the proposed Multi-Scale Gaussian Anomaly Generator and Unified Texture Anomaly Generator can generate richer anomalies in RGB and depth. (3) All modules share parameters for both RGB and depth data, effectively bridging 2D and 3D anomaly detection. Subsequent modules can directly leverage features from both modalities without complex fusion. Experiments show our method outperforms state-of-the-art (SOTA) on MVTec-3D AD and Eyecandies datasets. Code available at: this https URL</li>
<li><strong>摘要：</strong>2D对象的工业异常检测已引起了人们的重大关注并取得了异常检测（AD）方法的进展。但是，仅使用2D信息识别3D深度异常是不够的。尽管明确将深度信息融合到RGB图像中或使用点云骨干网络提取深度特征，但由于不同模态信息之间的差异，这两种方法都难以充分地表示多模式场景中的3D信息。此外，由于工业数据中的样本异常稀缺，尤其是在多模式方案中，因此有必要进行异常生成以模拟现实世界中的异常样本。因此，我们提出了一个新型统一的多模式异常检测框架来解决这些问题。我们的贡献包括3个关键方面。 （1）我们简单地从3D点云数据中提取可见的深度信息，并使用2D RGB图像表示外观，从而将深度和外观删除以支持统一的异常生成。 （2）从灵活的输入表示中受益，提出的多尺度高斯异常发生器和统一纹理异常发生器可以在RGB和深度中产生更丰富的异常。 （3）所有模块共享RGB和深度数据的参数，有效地桥接了2D和3D异常检测。随后的模块可以直接利用两种模式的特征而无需复杂的融合。实验表明，我们的方法在MVTEC-3D AD和EYECONDIES数据集上优于最先进（SOTA）。代码可用：此HTTPS URL</li>
</ul>

<h3>Title: RemoteReasoner: Towards Unifying Geospatial Reasoning Workflow</h3>
<ul>
<li><strong>Authors: </strong>Liang Yao, Fan Liu, Hongbo Lu, Chuanyi Zhang, Rui Min, Shengxiang Xu, Shimin Di, Pai Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19280">https://arxiv.org/abs/2507.19280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19280">https://arxiv.org/pdf/2507.19280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19280]] RemoteReasoner: Towards Unifying Geospatial Reasoning Workflow(https://arxiv.org/abs/2507.19280)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Remote sensing imagery presents vast, inherently unstructured spatial data, demanding sophisticated reasoning to interpret complex user intents and contextual relationships beyond simple recognition tasks. In this paper, we aim to construct an Earth observation workflow to handle complex queries by reasoning about spatial context and user intent. As a reasoning workflow, it should be somewhat autonomous, where predefined ground-truth reasoning paths do not constrain the learning process. Furthermore, its architecture ought to be unified yet flexible, enabling the model to perform diverse reasoning tasks with distinct output formats through a single forward pass. Existing remote sensing approaches fail to address these requirements, as they rely on supervised fine-tuning paradigms that constrain the autonomy of reasoning. To this end, we propose RemoteReasoner, a flexible and robust workflow for remote sensing reasoning tasks. The design of RemoteReasoner integrates a multi-modal large language model (MLLM) for interpreting user instructions and localizing targets, together with task adaptation strategies that enable multi-granularity output generation. In contrast to existing methods, our framework is trained with reinforcement learning (RL) to endow the MLLM sufficient autonomy for precise reasoning. At the inference stage, our adaptation strategies enable diverse output formats at inference time without requiring task-specific decoders or further fine-tuning. Preliminary experiments demonstrated that RemoteReasoner achieves remarkable performance across multi-granularity reasoning tasks, including region-level and pixel-level. Additionally, our framework enables novel capabilities such as the contour extraction task beyond the reach of existing reasoning pipelines.</li>
<li><strong>摘要：</strong>遥感图像列出了庞大的，固有的非结构化空间数据，要求复杂的推理来解释复杂的用户意图和上下文关系，而不是简单的识别任务。在本文中，我们旨在通过有关空间上下文和用户意图来构建地球观察工作流程，以处理复杂的查询。作为推理工作流程，应该在某种程度上自主，而预定义的基础推理路径不会限制学习过程。此外，它的架构应该是统一但灵活的，使该模型能够通过单个前向传球执行具有不同输出格式的不同推理任务。现有的遥感方法无法解决这些要求，因为它们依赖于限制推理自治的监督微调范式。为此，我们提出了Enloterasoner，这是一种灵活而强大的工作流程，用于遥感推理任务。 Enlotereasoner的设计集成了一个多模式的大语言模型（MLLM），以解释用户指令和本地化目标，以及可以实现多粒度输出生成的任务适应策略。与现有方法相反，我们的框架接受了加固学习（RL）的培训，以赋予MLLM足够的自主权以进行精确推理。在推理阶段，我们的适应策略可以在推理时间进行多种输出格式，而无需特定于任务的解码器或进一步的微调。初步实验表明，远程探索者在包括区域级别和像素级在内的多个多界面推理任务中取得了卓越的性能。此外，我们的框架还可以实现新颖的功能，例如超出现有推理管道范围的轮廓提取任务。</li>
</ul>

<h3>Title: PINO: Person-Interaction Noise Optimization for Long-Duration and Customizable Motion Generation of Arbitrary-Sized Groups</h3>
<ul>
<li><strong>Authors: </strong>Sakuya Ota, Qing Yu, Kent Fujiwara, Satoshi Ikehata, Ikuro Sato</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19292">https://arxiv.org/abs/2507.19292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19292">https://arxiv.org/pdf/2507.19292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19292]] PINO: Person-Interaction Noise Optimization for Long-Duration and Customizable Motion Generation of Arbitrary-Sized Groups(https://arxiv.org/abs/2507.19292)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating realistic group interactions involving multiple characters remains challenging due to increasing complexity as group size expands. While existing conditional diffusion models incrementally generate motions by conditioning on previously generated characters, they rely on single shared prompts, limiting nuanced control and leading to overly simplified interactions. In this paper, we introduce Person-Interaction Noise Optimization (PINO), a novel, training-free framework designed for generating realistic and customizable interactions among groups of arbitrary size. PINO decomposes complex group interactions into semantically relevant pairwise interactions, and leverages pretrained two-person interaction diffusion models to incrementally compose group interactions. To ensure physical plausibility and avoid common artifacts such as overlapping or penetration between characters, PINO employs physics-based penalties during noise optimization. This approach allows precise user control over character orientation, speed, and spatial relationships without additional training. Comprehensive evaluations demonstrate that PINO generates visually realistic, physically coherent, and adaptable multi-person interactions suitable for diverse animation, gaming, and robotics applications.</li>
<li><strong>摘要：</strong>由于群体规模扩大，生成涉及多个字符的现实群体互动仍然具有挑战性。尽管现有的条件扩散模型通过根据先前生成的字符进行调节来逐渐产生动作，但它们依赖于单个共享提示，限制了细微的控制并导致过度简化的交互。在本文中，我们介绍了人际关系噪声优化（Pino），这是一个新颖的，无训练的框架，旨在在任意大小组之间产生现实和可定制的相互作用。 Pino将复杂的组相互作用分解为具有语义相关的成对相互作用，并利用鉴定的两人相互作用扩散模型来逐步构成组相互作用。为了确保身体上的合理性并避免诸如角色之间的重叠或渗透等常见的伪像，Pino在噪声优化期间采取了基于物理的惩罚。这种方法可以精确地控制字符取向，速度和空间关系，而无需其他培训。全面的评估表明，Pino在视觉上实现，身体连贯和适应性的多人相互作用，适用于各种动画，游戏和机器人技术应用。</li>
</ul>

<h3>Title: Doubling Your Data in Minutes: Ultra-fast Tabular Data Generation via LLM-Induced Dependency Graphs</h3>
<ul>
<li><strong>Authors: </strong>Shuo Yang, Zheyu Zhang, Bardh Prenkaj, Gjergji Kasneci</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19334">https://arxiv.org/abs/2507.19334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19334">https://arxiv.org/pdf/2507.19334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19334]] Doubling Your Data in Minutes: Ultra-fast Tabular Data Generation via LLM-Induced Dependency Graphs(https://arxiv.org/abs/2507.19334)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Tabular data is critical across diverse domains, yet high-quality datasets remain scarce due to privacy concerns and the cost of collection. Contemporary approaches adopt large language models (LLMs) for tabular augmentation, but exhibit two major limitations: (1) dense dependency modeling among tabular features that can introduce bias, and (2) high computational overhead in sampling. To address these issues, we propose SPADA for SPArse Dependency-driven Augmentation, a lightweight generative framework that explicitly captures sparse dependencies via an LLM-induced graph. We treat each feature as a node and synthesize values by traversing the graph, conditioning each feature solely on its parent nodes. We explore two synthesis strategies: a non-parametric method using Gaussian kernel density estimation, and a conditional normalizing flow model that learns invertible mappings for conditional density estimation. Experiments on four datasets show that SPADA reduces constraint violations by 4% compared to diffusion-based methods and accelerates generation by nearly 9,500 times over LLM-based baselines.</li>
<li><strong>摘要：</strong>表格数据在各个领域至关重要，但是由于隐私问题和收集成本，高质量的数据集仍然很少。当代方法采用大型语言模型（LLM）进行表格增强，但展示了两个主要局限性：（1）表格特征之间的密集依赖性建模，这些特征会引入偏见，以及（2）在抽样中的高计算开销。为了解决这些问题，我们建议SPADA稀疏依赖性驱动的增强，这是一种轻巧的生成框架，可通过LLM诱导的图明确捕获稀疏的依赖关系。我们将每个功能视为节点，并通过遍历图形，将每个功能仅在其父节点上调节。我们探讨了两种综合策略：使用高斯内核密度估计的一种非参数方法，以及一个有条件的归一化流模型，该模型了解有条件密度估计的可逆映射。四个数据集的实验表明，与基于扩散的方法相比，SPADA可将约束违规行为减少4％，并使基于LLM的基线的生成近9,500倍。</li>
</ul>

<h3>Title: Reconstruction of Sparse Urban Wireless Signals via Group Equivariant Non-Expansive Operators</h3>
<ul>
<li><strong>Authors: </strong>Lorenzo Mario Amorosa, Francesco Conti, Nicola Quercioli, Flavio Zabini, Tayebeh Lotfi Mahyari, Yiqun Ge, Patrizio Frosini</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19349">https://arxiv.org/abs/2507.19349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19349">https://arxiv.org/pdf/2507.19349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19349]] Reconstruction of Sparse Urban Wireless Signals via Group Equivariant Non-Expansive Operators(https://arxiv.org/abs/2507.19349)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In emerging communication systems such as sixth generation (6G) wireless networks, efficient resource management and service delivery rely on accurate knowledge of spatially-varying quantities like signal-to-interference-noise ratio (SINR) maps, which are costly to acquire at high resolution. This work explores the reconstruction of such spatial signals from sparse measurements using Group Equivariant Non-Expansive Operators (GENEOs), offering a low-complexity alternative to traditional neural networks. The concept of GENEO, which originated in topological data analysis (TDA), is a mathematical tool used in machine learning to represent agents modelled as functional operators acting on data while incorporating application-specific invariances. Leveraging these invariances reduces the number of parameters with respect to traditional neural networks and mitigates data scarcity by enforcing known algebraic and geometric constraints that reflect symmetries in the agents' actions. In this paper, we introduce a novel GENEO-based approach for SINR map reconstruction in urban wireless communication networks using extremely sparse sampling. We demonstrate that this mathematical framework achieves competitive performance compared to established methods. Our evaluation, conducted using both statistical and TDA metrics, highlights the advantages of our approach in accurately reconstructing spatial signals under severe data limitations on the number of samples.</li>
<li><strong>摘要：</strong>在新兴的通信系统（例如第六代（6G）无线网络）中，有效的资源管理和服务交付依赖于对空间变化数量的准确知识，例如信噪比 - 互发噪声比率（SINR）地图（SINR）地图，这是高分辨率以高昂的成本。这项工作探讨了使用群体模化的非企业运算符（Geneos）从稀疏测量中重建此类空间信号，并提供了传统神经网络的低复杂性替代方案。 Geneo的概念起源于拓扑数据分析（TDA），是一种数学工具，用于机器学习中，以代表建模为功能运算符的代理，同时纳入了应用程序特定于应用程序特定的侵入率。利用这些不传出的人数减少了有关传统神经网络的参数数量，并通过执行已知的代数和几何约束来减轻数据稀缺性，这些代数和几何约束反映了代理商的行为中的对称性。在本文中，我们介绍了一种基于基于基因的基于基因的新方法，用于使用极稀疏的采样来进行城市无线通信网络中的SINR地图重建方法。我们证明，与既定方法相比，这个数学框架可以达到竞争性能。我们使用统计和TDA指标进行的评估强调了我们方法在严重的数据限制下准确地重建空间信号的优势。</li>
</ul>

<h3>Title: SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic Coherence and Relevance Learning</h3>
<ul>
<li><strong>Authors: </strong>Lanmiao Liu, Esam Ghaleb, Aslı Özyürek, Zerrin Yumak</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19359">https://arxiv.org/abs/2507.19359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19359">https://arxiv.org/pdf/2507.19359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19359]] SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic Coherence and Relevance Learning(https://arxiv.org/abs/2507.19359)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Creating a virtual avatar with semantically coherent gestures that are aligned with speech is a challenging task. Existing gesture generation research mainly focused on generating rhythmic beat gestures, neglecting the semantic context of the gestures. In this paper, we propose a novel approach for semantic grounding in co-speech gesture generation that integrates semantic information at both fine-grained and global levels. Our approach starts with learning the motion prior through a vector-quantized variational autoencoder. Built on this model, a second-stage module is applied to automatically generate gestures from speech, text-based semantics and speaker identity that ensures consistency between the semantic relevance of generated gestures and co-occurring speech semantics through semantic coherence and relevance modules. Experimental results demonstrate that our approach enhances the realism and coherence of semantic gestures. Extensive experiments and user studies show that our method outperforms state-of-the-art approaches across two benchmarks in co-speech gesture generation in both objective and subjective metrics. The qualitative results of our model, code, dataset and pre-trained models can be viewed at this https URL.</li>
<li><strong>摘要：</strong>创建具有与语音一致的语义连贯性手势的虚拟化身是一项艰巨的任务。现有的手势生成研究主要集中于产生节奏的节奏手势，忽略了手势的语义背景。在本文中，我们提出了一种在共同语音发电中的语义基础的新方法，该方法将语义信息集成在细粒度和全球水平上。我们的方法始于通过矢量量化的变分自动编码器在先前学习运动。建立在该模型的基础上，将第二阶段模块应用于自动从语音，基于文本的语义和说话者的身份产生手势，以确保通过语义连贯性和相关性模块的语义相关性和相关语音语义的语义相关性与同时发生的语音语义之间的一致性。实验结果表明，我们的方法增强了语义手势的现实主义和连贯性。广泛的实验和用户研究表明，在客观和主观指标中，我们的方法在两个基准的两种基准中都优于两种基准的最先进方法。我们的模型，代码，数据集和预训练模型的定性结果可以在此HTTPS URL上查看。</li>
</ul>

<h3>Title: Counterfactual Explanations in Medical Imaging: Exploring SPN-Guided Latent Space Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Julia Siekiera, Stefan Kramer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19368">https://arxiv.org/abs/2507.19368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19368">https://arxiv.org/pdf/2507.19368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19368]] Counterfactual Explanations in Medical Imaging: Exploring SPN-Guided Latent Space Manipulation(https://arxiv.org/abs/2507.19368)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Artificial intelligence is increasingly leveraged across various domains to automate decision-making processes that significantly impact human lives. In medical image analysis, deep learning models have demonstrated remarkable performance. However, their inherent complexity makes them black box systems, raising concerns about reliability and interpretability. Counterfactual explanations provide comprehensible insights into decision processes by presenting hypothetical "what-if" scenarios that alter model classifications. By examining input alterations, counterfactual explanations provide patterns that influence the decision-making process. Despite their potential, generating plausible counterfactuals that adhere to similarity constraints providing human-interpretable explanations remains a challenge. In this paper, we investigate this challenge by a model-specific optimization approach. While deep generative models such as variational autoencoders (VAEs) exhibit significant generative power, probabilistic models like sum-product networks (SPNs) efficiently represent complex joint probability distributions. By modeling the likelihood of a semi-supervised VAE's latent space with an SPN, we leverage its dual role as both a latent space descriptor and a classifier for a given discrimination task. This formulation enables the optimization of latent space counterfactuals that are both close to the original data distribution and aligned with the target class distribution. We conduct experimental evaluation on the cheXpert dataset. To evaluate the effectiveness of the integration of SPNs, our SPN-guided latent space manipulation is compared against a neural network baseline. Additionally, the trade-off between latent variable regularization and counterfactual quality is analyzed.</li>
<li><strong>摘要：</strong>人工智能越来越多地在各个领域中利用，以使决策过程自动化，从而显着影响人类的生活。在医学图像分析中，深度学习模型表现出了出色的性能。但是，它们固有的复杂性使它们成为黑匣子系统，从而引起了人们对可靠性和解释性的关注。反事实解释通过呈现假设的“假设”场景来改变模型分类，从而为决策过程提供了可理解的见解。通过检查输入改变，反事实解释提供了影响决策过程的模式。尽管它们具有潜力，但产生合理的反事实，这些反事实是遵守相似性限制，提供人解释的解释仍然是一个挑战。在本文中，我们通过一种特定于模型的优化方法调查了这一挑战。诸如变分自动编码器（VAE）之类的深生成模型具有显着的生成能力，但概率模型（如Sum-rapopoduct Networks（SPN））有效地表示复杂的关节概率分布。通过对SPN进行半监督VAE的潜在空间的可能性建模，我们利用其双重作用作为潜在的空间描述符和分类器来执行给定的歧视任务。该公式可以优化潜在的空间反事实，既接近原始数据分布，又与目标类别分布保持一致。我们对CHEXPERT数据集进行了实验评估。为了评估SPN集成的有效性，将SPN引导的潜在空间操纵与神经网络基线进行了比较。此外，分析了潜在变量正则化和反事实质量之间的权衡。</li>
</ul>

<h3>Title: DEFNet: Multitasks-based Deep Evidential Fusion Network for Blind Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Yiwei Lou, Yuanpeng He, Rongchao Zhang, Yongzhi Cao, Hanpin Wang, Yu Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19418">https://arxiv.org/abs/2507.19418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19418">https://arxiv.org/pdf/2507.19418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19418]] DEFNet: Multitasks-based Deep Evidential Fusion Network for Blind Image Quality Assessment(https://arxiv.org/abs/2507.19418)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Blind image quality assessment (BIQA) methods often incorporate auxiliary tasks to improve performance. However, existing approaches face limitations due to insufficient integration and a lack of flexible uncertainty estimation, leading to suboptimal performance. To address these challenges, we propose a multitasks-based Deep Evidential Fusion Network (DEFNet) for BIQA, which performs multitask optimization with the assistance of scene and distortion type classification tasks. To achieve a more robust and reliable representation, we design a novel trustworthy information fusion strategy. It first combines diverse features and patterns across sub-regions to enhance information richness, and then performs local-global information fusion by balancing fine-grained details with coarse-grained context. Moreover, DEFNet exploits advanced uncertainty estimation technique inspired by evidential learning with the help of normal-inverse gamma distribution mixture. Extensive experiments on both synthetic and authentic distortion datasets demonstrate the effectiveness and robustness of the proposed framework. Additional evaluation and analysis are carried out to highlight its strong generalization capability and adaptability to previously unseen scenarios.</li>
<li><strong>摘要：</strong>盲图质量评估（BIQA）方法通常包含辅助任务以提高性能。但是，由于整合不足和缺乏灵活的不确定性估计，现有方法面临局限性，导致了次优的性能。为了应对这些挑战，我们为BIQA提出了一个基于多任务的深层证据融合网络（DEFNET），该网络在场景和失真类型分类任务的帮助下执行多任务优化。为了实现更强大和可靠的代表，我们设计了一种新颖的值得信赖的信息融合策略。它首先结合了各个区域的各种特征和模式，以增强信息丰富度，然后通过将细粒细节与粗粒的环境保持平衡，从而执行局部全球信息融合。此外，DeFnet利用了正常的持续伽马分布混合物，灵感来自证据学习的高级不确定性估计技术。对合成和真实失真数据集的广泛实验证明了所提出的框架的有效性和鲁棒性。进行其他评估和分析，以突出其强大的概括能力和对以前看不见的情况的适应性。</li>
</ul>

<h3>Title: HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars</h3>
<ul>
<li><strong>Authors: </strong>Byungjun Kim, Shunsuke Saito, Giljoo Nam, Tomas Simon, Jason Saragih, Hanbyul Joo, Junxuan Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.19481">https://arxiv.org/abs/2507.19481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.19481">https://arxiv.org/pdf/2507.19481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.19481]] HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars(https://arxiv.org/abs/2507.19481)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present a universal prior model for 3D head avatars with explicit hair compositionality. Existing approaches to build generalizable priors for 3D head avatars often adopt a holistic modeling approach, treating the face and hair as an inseparable entity. This overlooks the inherent compositionality of the human head, making it difficult for the model to naturally disentangle face and hair representations, especially when the dataset is limited. Furthermore, such holistic models struggle to support applications like 3D face and hairstyle swapping in a flexible and controllable manner. To address these challenges, we introduce a prior model that explicitly accounts for the compositionality of face and hair, learning their latent spaces separately. A key enabler of this approach is our synthetic hairless data creation pipeline, which removes hair from studio-captured datasets using estimated hairless geometry and texture derived from a diffusion prior. By leveraging a paired dataset of hair and hairless captures, we train disentangled prior models for face and hair, incorporating compositionality as an inductive bias to facilitate effective separation. Our model's inherent compositionality enables seamless transfer of face and hair components between avatars while preserving identity. Additionally, we demonstrate that our model can be fine-tuned in a few-shot manner using monocular captures to create high-fidelity, hair-compositional 3D head avatars for unseen subjects. These capabilities highlight the practical applicability of our approach in real-world scenarios, paving the way for flexible and expressive 3D avatar generation.</li>
<li><strong>摘要：</strong>我们为具有明确的头发组成性的3D头部化身提供了通用的先验模型。现有的方法来建造3D头像的可推广先验，通常采用整体建模方法，将面部和头发视为不可分割的实体。这忽略了人头的固有组成性，使模型很难自然地脱离面部和头发表示，尤其是在数据集受到限制的情况下。此外，这种整体模型努力以灵活且可控制的方式支持3D面和发型交换等应用。为了应对这些挑战，我们介绍了一个先前的模型，该模型明确说明了面部和头发的组成性，分别学习其潜在空间。这种方法的关键推动因素是我们的合成无毛数据创建管道，该管道使用估计的无毛几何形状和从扩散先验中得出的质地从录音室捕获的数据集中去除头发。通过利用配对的头发和无毛捕获的数据集，我们训练了面部和头发的先前模型，并将组合性作为感应性偏见，以促进有效的分离。我们的模型固有的组合性可以使头像之间的面部和头发组件无缝转移，同时保持身份。此外，我们证明了我们的模型可以使用单眼捕获以几次射击方式进行微调，从而为看不见的受试者创建高保真，头发组成3D头像。这些功能强调了我们方法在实际情况下的实际适用性，为灵活和表现力的3D头像生成铺平了道路。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
