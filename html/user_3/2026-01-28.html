<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-28</h1>
<h3>Title: VAE with Hyperspherical Coordinates: Improving Anomaly Detection from Hypervolume-Compressed Latent Space</h3>
<ul>
<li><strong>Authors: </strong>Alejandro Ascarate, Leo Lebrat, Rodrigo Santa Cruz, Clinton Fookes, Olivier Salvado</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18823">https://arxiv.org/abs/2601.18823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18823">https://arxiv.org/pdf/2601.18823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18823]] VAE with Hyperspherical Coordinates: Improving Anomaly Detection from Hypervolume-Compressed Latent Space(https://arxiv.org/abs/2601.18823)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Variational autoencoders (VAE) encode data into lower-dimensional latent vectors before decoding those vectors back to data. Once trained, one can hope to detect out-of-distribution (abnormal) latent vectors, but several issues arise when the latent space is high dimensional. This includes an exponential growth of the hypervolume with the dimension, which severely affects the generative capacity of the VAE. In this paper, we draw insights from high dimensional statistics: in these regimes, the latent vectors of a standard VAE are distributed on the `equators' of a hypersphere, challenging the detection of anomalies. We propose to formulate the latent variables of a VAE using hyperspherical coordinates, which allows compressing the latent vectors towards a given direction on the hypersphere, thereby allowing for a more expressive approximate posterior. We show that this improves both the fully unsupervised and OOD anomaly detection ability of the VAE, achieving the best performance on the datasets we considered, outperforming existing methods. For the unsupervised and OOD modalities, respectively, these are: i) detecting unusual landscape from the Mars Rover camera and unusual Galaxies from ground based imagery (complex, real world datasets); ii) standard benchmarks like Cifar10 and subsets of ImageNet as the in-distribution (ID) class.</li>
<li><strong>摘要：</strong>变分自动编码器 (VAE) 将数据编码为低维潜在向量，然后将这些向量解码回数据。一旦经过训练，人们就可以希望检测到分布外（异常）的潜在向量，但是当潜在空间是高维时，就会出现一些问题。这包括超体积随维度呈指数增长，这严重影响了 VAE 的生成能力。在本文中，我们从高维统计中得出见解：在这些情况下，标准 VAE 的潜在向量分布在超球面的“赤道”上，这对异常检测提出了挑战。我们建议使用超球面坐标来制定 VAE 的潜在变量，这允许将潜在向量压缩到超球面上的给定方向，从而允许更具表现力的近似后验。我们表明，这提高了 VAE 的完全无监督和 OOD 异常检测能力，在我们考虑的数据集上实现了最佳性能，优于现有方法。对于无监督和 OOD 模式，分别是： i) 从火星漫游者相机检测不寻常的景观，从地面图像（复杂的真实世界数据集）检测不寻常的星系； ii) Cifar10 等标准基准测试和 ImageNet 子集作为分布内 (ID) 类。</li>
</ul>

<h3>Title: Audio-Driven Talking Face Generation with Blink Embedding and Hash Grid Landmarks Encoding</h3>
<ul>
<li><strong>Authors: </strong>Yuhui Zhang, Hui Yu, Wei Liang, Sunjie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18849">https://arxiv.org/abs/2601.18849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18849">https://arxiv.org/pdf/2601.18849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18849]] Audio-Driven Talking Face Generation with Blink Embedding and Hash Grid Landmarks Encoding(https://arxiv.org/abs/2601.18849)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Dynamic Neural Radiance Fields (NeRF) have demonstrated considerable success in generating high-fidelity 3D models of talking portraits. Despite significant advancements in the rendering speed and generation quality, challenges persist in accurately and efficiently capturing mouth movements in talking portraits. To tackle this challenge, we propose an automatic method based on blink embedding and hash grid landmarks encoding in this study, which can substantially enhance the fidelity of talking faces. Specifically, we leverage facial features encoded as conditional features and integrate audio features as residual terms into our model through a Dynamic Landmark Transformer. Furthermore, we employ neural radiance fields to model the entire face, resulting in a lifelike face representation. Experimental evaluations have validated the superiority of our approach to existing methods.</li>
<li><strong>摘要：</strong>动态神经辐射场 (NeRF) 在生成说话肖像的高保真 3D 模型方面取得了相当大的成功。尽管渲染速度和生成质量取得了显着进步，但准确有效地捕捉说话肖像中的嘴部运动仍然存在挑战。为了应对这一挑战，我们在本研究中提出了一种基于眨眼嵌入和哈希网格地标编码的自动方法，该方法可以大大提高说话面孔的保真度。具体来说，我们利用编码为条件特征的面部特征，并通过动态地标变压器将音频特征作为残差项集成到我们的模型中。此外，我们采用神经辐射场对整个面部进行建模，从而产生逼真的面部表征。实验评估验证了我们的方法相对于现有方法的优越性。</li>
</ul>

<h3>Title: SelfieAvatar: Real-time Head Avatar reenactment from a Selfie Video</h3>
<ul>
<li><strong>Authors: </strong>Wei Liang, Hui Yu, Derui Ding, Rachael E. Jack, Philippe G. Schyns</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18851">https://arxiv.org/abs/2601.18851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18851">https://arxiv.org/pdf/2601.18851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18851]] SelfieAvatar: Real-time Head Avatar reenactment from a Selfie Video(https://arxiv.org/abs/2601.18851)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Head avatar reenactment focuses on creating animatable personal avatars from monocular videos, serving as a foundational element for applications like social signal understanding, gaming, human-machine interaction, and computer vision. Recent advances in 3D Morphable Model (3DMM)-based facial reconstruction methods have achieved remarkable high-fidelity face estimation. However, on the one hand, they struggle to capture the entire head, including non-facial regions and background details in real time, which is an essential aspect for producing realistic, high-fidelity head avatars. On the other hand, recent approaches leveraging generative adversarial networks (GANs) for head avatar generation from videos can achieve high-quality reenactments but encounter limitations in reproducing fine-grained head details, such as wrinkles and hair textures. In addition, existing methods generally rely on a large amount of training data, and rarely focus on using only a simple selfie video to achieve avatar reenactment. To address these challenges, this study introduces a method for detailed head avatar reenactment using a selfie video. The approach combines 3DMMs with a StyleGAN-based generator. A detailed reconstruction model is proposed, incorporating mixed loss functions for foreground reconstruction and avatar image generation during adversarial training to recover high-frequency details. Qualitative and quantitative evaluations on self-reenactment and cross-reenactment tasks demonstrate that the proposed method achieves superior head avatar reconstruction with rich and intricate textures compared to existing approaches.</li>
<li><strong>摘要：</strong>头部头像重现专注于从单眼视频创建可动画的个人头像，作为社交信号理解、游戏、人机交互和计算机视觉等应用的基础元素。基于 3D Morphable Model (3DMM) 的面部重建方法的最新进展已经实现了显着的高保真面部估计。然而，一方面，他们很难实时捕捉整个头部，包括非面部区域和背景细节，这是制作逼真、高保真头部头像的重要方面。另一方面，最近利用生成对抗网络（GAN）从视频生成头部头像的方法可以实现高质量的重演，但在再现细粒度头部细节（例如皱纹和头发纹理）方面遇到限制。此外，现有的方法一般依赖于大量的训练数据，很少专注于仅使用简单的自拍视频来实现头像重演。为了解决这些挑战，本研究介绍了一种使用自拍视频详细重现头部头像的方法。该方法将 3DMM 与基于 StyleGAN 的生成器相结合。提出了一种详细的重建模型，在对抗训练期间结合用于前景重建和头像图像生成的混合损失函数来恢复高频细节。对自我重演和交叉重演任务的定性和定量评估表明，与现有方法相比，所提出的方法可以实现具有丰富且复杂纹理的优质头部头像重建。</li>
</ul>

<h3>Title: RealStats: A Rigorous Real-Only Statistical Framework for Fake Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Haim Zisman, Uri Shaham</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18900">https://arxiv.org/abs/2601.18900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18900">https://arxiv.org/pdf/2601.18900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18900]] RealStats: A Rigorous Real-Only Statistical Framework for Fake Image Detection(https://arxiv.org/abs/2601.18900)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As generative models continue to evolve, detecting AI-generated images remains a critical challenge. While effective detection methods exist, they often lack formal interpretability and may rely on implicit assumptions about fake content, potentially limiting robustness to distributional shifts. In this work, we introduce a rigorous, statistically grounded framework for fake image detection that focuses on producing a probability score interpretable with respect to the real-image population. Our method leverages the strengths of multiple existing detectors by combining training-free statistics. We compute p-values over a range of test statistics and aggregate them using classical statistical ensembling to assess alignment with the unified real-image distribution. This framework is generic, flexible, and training-free, making it well-suited for robust fake image detection across diverse and evolving settings.</li>
<li><strong>摘要：</strong>随着生成模型的不断发展，检测人工智能生成的图像仍然是一个严峻的挑战。虽然存在有效的检测方法，但它们通常缺乏正式的可解释性，并且可能依赖于关于虚假内容的隐含假设，从而可能限制分布变化的稳健性。在这项工作中，我们引入了一个严格的、基于统计的虚假图像检测框架，该框架专注于生成可相对于真实图像群体进行解释的概率得分。我们的方法通过结合免训练统计数据来利用多个现有检测器的优势。我们计算一系列测试统计数据的 p 值，并使用经典统计集成来聚合它们，以评估与统一真实图像分布的对齐情况。该框架通用、灵活且无需训练，非常适合在不同和不断变化的环境中进行鲁棒的假图像检测。</li>
</ul>

<h3>Title: Pay Attention to Where You Look</h3>
<ul>
<li><strong>Authors: </strong>Alex Beriand, JhihYang Wu, Daniel Brignac, Natnael Daba, Abhijit Mahalanobis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18970">https://arxiv.org/abs/2601.18970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18970">https://arxiv.org/pdf/2601.18970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18970]] Pay Attention to Where You Look(https://arxiv.org/abs/2601.18970)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Novel view synthesis (NVS) has advanced with generative modeling, enabling photorealistic image generation. In few-shot NVS, where only a few input views are available, existing methods often assume equal importance for all input views relative to the target, leading to suboptimal results. We address this limitation by introducing a camera-weighting mechanism that adjusts the importance of source views based on their relevance to the target. We propose two approaches: a deterministic weighting scheme leveraging geometric properties like Euclidean distance and angular differences, and a cross-attention-based learning scheme that optimizes view weighting. Additionally, models can be further trained with our camera-weighting scheme to refine their understanding of view relevance and enhance synthesis quality. This mechanism is adaptable and can be integrated into various NVS algorithms, improving their ability to synthesize high-quality novel views. Our results demonstrate that adaptive view weighting enhances accuracy and realism, offering a promising direction for improving NVS.</li>
<li><strong>摘要：</strong>新颖的视图合成（NVS）在生成建模方面取得了进步，可以生成逼真的图像。在少样本 NVS 中，只有少数输入视图可用，现有方法通常假设所有输入视图相对于目标具有同等重要性，从而导致结果不理想。我们通过引入相机加权机制来解决这一限制，该机制根据源视图与目标的相关性来调整源视图的重要性。我们提出了两种方法：利用欧几里德距离和角度差异等几何特性的确定性加权方案，以及优化视图加权的基于交叉注意力的学习方案。此外，可以使用我们的相机加权方案进一步训练模型，以加深对视图相关性的理解并提高合成质量。该机制适应性强，可以集成到各种 NVS 算法中，提高其合成高质量新颖视图的能力。我们的结果表明，自适应视图加权提高了准确性和真实感，为改进 NVS 提供了一个有希望的方向。</li>
</ul>

<h3>Title: FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Wei Cao, Hao Zhang, Fengrui Tian, Yulun Wu, Yingying Li, Shenlong Wang, Ning Yu, Yaoyao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.18993">https://arxiv.org/abs/2601.18993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.18993">https://arxiv.org/pdf/2601.18993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.18993]] FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction(https://arxiv.org/abs/2601.18993)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Camera redirection aims to replay a dynamic scene from a single monocular video under a user-specified camera trajectory. However, large-angle redirection is inherently ill-posed: a monocular video captures only a narrow spatio-temporal view of a dynamic 3D scene, providing highly partial observations of the underlying 4D world. The key challenge is therefore to recover a complete and coherent representation from this limited input, with consistent geometry and motion. While recent diffusion-based methods achieve impressive results, they often break down under large-angle viewpoint changes far from the original trajectory, where missing visual grounding leads to severe geometric ambiguity and temporal inconsistency. To address this, we present FreeOrbit4D, an effective training-free framework that tackles this geometric ambiguity by recovering a geometry-complete 4D proxy as structural grounding for video generation. We obtain this proxy by decoupling foreground and background reconstructions: we unproject the monocular video into a static background and geometry-incomplete foreground point clouds in a unified global space, then leverage an object-centric multi-view diffusion model to synthesize multi-view images and reconstruct geometry-complete foreground point clouds in canonical object space. By aligning the canonical foreground point cloud to the global scene space via dense pixel-synchronized 3D--3D correspondences and projecting the geometry-complete 4D proxy onto target camera viewpoints, we provide geometric scaffolds that guide a conditional video diffusion model. Extensive experiments show that FreeOrbit4D produces more faithful redirected videos under challenging large-angle trajectories, and our geometry-complete 4D proxy further opens a potential avenue for practical applications such as edit propagation and 4D data generation. Project page and code will be released soon.</li>
<li><strong>摘要：</strong>摄像机重定向旨在在用户指定的摄像机轨迹下重播单个单目视频的动态场景。然而，大角度重定向本质上是不适定的：单目视频仅捕获动态 3D 场景的狭窄时空视图，提供对底层 4D 世界的高度局部观察。因此，关键的挑战是从有限的输入中恢复完整且连贯的表示，并具有一致的几何形状和运动。虽然最近基于扩散的方法取得了令人印象深刻的结果，但它们经常在远离原始轨迹的大角度视点变化下崩溃，缺少视觉基础会导致严重的几何模糊和时间不一致。为了解决这个问题，我们提出了 FreeOrbit4D，这是一个有效的免训练框架，它通过恢复几何完整的 4D 代理作为视频生成的结构基础来解决这种几何模糊性。我们通过解耦前景和背景重建来获得这个代理：我们将单目视频投影到统一的全局空间中的静态背景和几何不完整的前景点云中，然后利用以对象为中心的多视图扩散模型来合成多视图图像并在规范对象空间中重建几何完整的前景点云。通过密集的像素同步 3D--3D 对应将规范前景点云与全局场景空间对齐，并将几何完整的 4D 代理投影到目标摄像机视点上，我们提供了指导条件视频扩散模型的几何支架。大量实验表明，FreeOrbit4D 在具有挑战性的大角度轨迹下产生更忠实的重定向视频，并且我们的几何完整 4D 代理进一步为编辑传播和 4D 数据生成等实际应用开辟了潜在途径。项目页面和代码即将发布。</li>
</ul>

<h3>Title: NuiWorld: Exploring a Scalable Framework for End-to-End Controllable World Generation</h3>
<ul>
<li><strong>Authors: </strong>Han-Hung Lee, Cheng-Yu Yang, Yu-Lun Liu, Angel X. Chang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19048">https://arxiv.org/abs/2601.19048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19048">https://arxiv.org/pdf/2601.19048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19048]] NuiWorld: Exploring a Scalable Framework for End-to-End Controllable World Generation(https://arxiv.org/abs/2601.19048)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>World generation is a fundamental capability for applications like video games, simulation, and robotics. However, existing approaches face three main obstacles: controllability, scalability, and efficiency. End-to-end scene generation models have been limited by data scarcity. While object-centric generation approaches rely on fixed resolution representations, degrading fidelity for larger scenes. Training-free approaches, while flexible, are often slow and computationally expensive at inference time. We present NuiWorld, a framework that attempts to address these challenges. To overcome data scarcity, we propose a generative bootstrapping strategy that starts from a few input images. Leveraging recent 3D reconstruction and expandable scene generation techniques, we synthesize scenes of varying sizes and layouts, producing enough data to train an end-to-end model. Furthermore, our framework enables controllability through pseudo sketch labels, and demonstrates a degree of generalization to previously unseen sketches. Our approach represents scenes as a collection of variable scene chunks, which are compressed into a flattened vector-set representation. This significantly reduces the token length for large scenes, enabling consistent geometric fidelity across scenes sizes while improving training and inference efficiency.</li>
<li><strong>摘要：</strong>世界生成是视频游戏、模拟和机器人等应用程序的一项基本功能。然而，现有方法面临三个主要障碍：可控性、可扩展性和效率。端到端场景生成模型受到数据稀缺的限制。虽然以对象为中心的生成方法依赖于固定分辨率表示，但会降低较大场景的保真度。免训练方法虽然灵活，但在推理时通常很慢且计算成本昂贵。我们提出 NuiWorld，一个试图解决这些挑战的框架。为了克服数据稀缺的问题，我们提出了一种从一些输入图像开始的生成引导策略。利用最新的 3D 重建和可扩展场景生成技术，我们合成不同大小和布局的场景，生成足够的数据来训练端到端模型。此外，我们的框架通过伪草图标签实现了可控性，并展示了对以前未见过的草图的一定程度的泛化。我们的方法将场景表示为可变场景块的集合，这些场景块被压缩为扁平化的向量集表示。这显着减少了大型场景的令牌长度，实现了跨场景大小的一致的几何保真度，同时提高了训练和推理效率。</li>
</ul>

<h3>Title: Pixel-Grounded Retrieval for Knowledgeable Large Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Jeonghwan Kim, Renjie Tao, Sanat Sharma, Jiaqi Wang, Kai Sun, Zhaojiang Lin, Seungwhan Moon, Lambert Mathias, Anuj Kumar, Heng Ji, Xin Luna Dong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19060">https://arxiv.org/abs/2601.19060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19060">https://arxiv.org/pdf/2601.19060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19060]] Pixel-Grounded Retrieval for Knowledgeable Large Multimodal Models(https://arxiv.org/abs/2601.19060)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Visual Question Answering (VQA) often requires coupling fine-grained perception with factual knowledge beyond the input image. Prior multimodal Retrieval-Augmented Generation (MM-RAG) systems improve factual grounding but lack an internal policy for when and how to retrieve. We propose PixSearch, the first end-to-end Segmenting Large Multimodal Model (LMM) that unifies region-level perception and retrieval-augmented reasoning. During encoding, PixSearch emits <search> tokens to trigger retrieval, selects query modalities (text, image, or region), and generates pixel-level masks that directly serve as visual queries, eliminating the reliance on modular pipelines (detectors, segmenters, captioners, etc.). A two-stage supervised fine-tuning regimen with search-interleaved supervision teaches retrieval timing and query selection while preserving segmentation ability. On egocentric and entity-centric VQA benchmarks, PixSearch substantially improves factual consistency and generalization, yielding a 19.7% relative gain in accuracy on CRAG-MM compared to whole image retrieval, while retaining competitive reasoning performance on various VQA and text-only QA tasks.</li>
<li><strong>摘要：</strong>视觉问答（VQA）通常需要将细粒度感知与输入图像之外的事实知识结合起来。先前的多模式检索增强生成（MM-RAG）系统改善了事实基础，但缺乏何时以及如何检索的内部策略。我们提出了 PixSearch，这是第一个端到端分段大型多模态模型（LMM），它统一了区域级感知和检索增强推理。在编码过程中，PixSearch 发出 <search> 标记来触发检索，选择查询模式（文本、图像或区域），并生成直接用作视觉查询的像素级掩码，从而消除对模块化管道（检测器、分段器、字幕器等）的依赖。具有搜索交错监督的两阶段监督微调方案可以教授检索时间和查询选择，同时保留分段能力。在以自我为中心和以实体为中心的 VQA 基准上，PixSearch 显着提高了事实一致性和泛化性，与整个图像检索相比，CRAG-MM 的准确率相对提高了 19.7%，同时在各种 VQA 和纯文本 QA 任务上保持有竞争力的推理性能。</li>
</ul>

<h3>Title: Out-of-Distribution Generalization for Neural Physics Solvers</h3>
<ul>
<li><strong>Authors: </strong>Zhao Wei, Chin Chun Ooi, Jian Cheng Wong, Abhishek Gupta, Pao-Hsiung Chiu, Yew-Soon Ong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19091">https://arxiv.org/abs/2601.19091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19091">https://arxiv.org/pdf/2601.19091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19091]] Out-of-Distribution Generalization for Neural Physics Solvers(https://arxiv.org/abs/2601.19091)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Neural physics solvers are increasingly used in scientific discovery, given their potential for rapid in silico insights into physical, materials, or biological systems and their long-time evolution. However, poor generalization beyond their training support limits exploration of novel designs and long-time horizon predictions. We introduce NOVA, a route to generalizable neural physics solvers that can provide rapid, accurate solutions to scenarios even under distributional shifts in partial differential equation parameters, geometries and initial conditions. By learning physics-aligned representations from an initial sparse set of scenarios, NOVA consistently achieves 1-2 orders of magnitude lower out-of-distribution errors than data-driven baselines across complex, nonlinear problems including heat transfer, diffusion-reaction and fluid flow. We further showcase NOVA's dual impact on stabilizing long-time dynamical rollouts and improving generative design through application to the simulation of nonlinear Turing systems and fluidic chip optimization. Unlike neural physics solvers that are constrained to retrieval and/or emulation within an a priori space, NOVA enables reliable extrapolation beyond known regimes, a key capability given the need for exploration of novel hypothesis spaces in scientific discovery</li>
<li><strong>摘要：</strong>神经物理求解器越来越多地用于科学发现，因为它们具有快速了解物理、材料或生物系统及其长期演化的潜力。然而，超出训练支持的泛化能力较差，限制了对新颖设计和长期预测的探索。我们引入了 NOVA，这是一种通用神经物理求解器的途径，即使在偏微分方程参数、几何形状和初始条件发生分布变化的情况下，也可以为场景提供快速、准确的解决方案。通过从一组初始稀疏场景中学习物理对齐的表示，NOVA 在复杂的非线性问题（包括传热、扩散反应和流体流动）中始终实现比数据驱动基线低 1-2 个数量级的分布外误差。我们通过应用于非线性图灵系统的模拟和流体芯片优化，进一步展示了 NOVA 对稳定长期动态部署和改进生成设计的双重影响。与受限于先验空间内的检索和/或仿真的神经物理求解器不同，NOVA 能够实现超出已知范围的可靠外推，考虑到在科学发现中探索新假设空间的需要，这是一项关键功能</li>
</ul>

<h3>Title: TinyTorch: Building Machine Learning Systems from First Principles</h3>
<ul>
<li><strong>Authors: </strong>Vijay Janapa Reddi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19107">https://arxiv.org/abs/2601.19107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19107">https://arxiv.org/pdf/2601.19107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19107]] TinyTorch: Building Machine Learning Systems from First Principles(https://arxiv.org/abs/2601.19107)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Machine learning systems engineering requires a deep understanding of framework internals. Yet most current education separates algorithms from systems. Students learn gradient descent without measuring memory usage, and attention mechanisms without profiling computational cost. This split leaves graduates unprepared to debug real production failures and widens the gap between machine learning research and reliable deployment. We present TinyTorch, a 20 module curriculum in which students implement the core components of PyTorch, including tensors, autograd, optimizers, and neural networks, entirely in pure Python. The curriculum is built around three pedagogical principles. Progressive disclosure gradually introduces complexity as students build confidence. Systems first integration embeds memory and performance awareness from the very beginning. Historical milestone validation guides students to recreate key breakthroughs, from the Perceptron in 1958 to modern Transformers, using only code they have written themselves. TinyTorch requires only a laptop with 4GB of RAM and no GPU, making machine learning systems education accessible worldwide. Its goal is to prepare the next generation of AI engineers, practitioners who understand not only what machine learning systems do, but why they work and how to make them scale. The curriculum is available as open source at this http URL slash tinytorch.</li>
<li><strong>摘要：</strong>机器学习系统工程需要对框架内部结构有深入的了解。然而，当前大多数教育都将算法与系统分开。学生学习梯度下降而不测量内存使用情况，学习注意力机制而无需分析计算成本。这种分裂使得毕业生没有准备好调试实际的生产故障，并扩大了机器学习研究和可靠部署之间的差距。我们推出了 TinyTorch，这是一个包含 20 个模块的课程，学生可以完全使用纯 Python 实现 PyTorch 的核心组件，包括张量、自动求导、优化器和神经网络。该课程围绕三个教学原则构建。随着学生建立信心，渐进式披露逐渐引入复杂性。系统首先集成从一开始就嵌入了内存和性能意识。历史里程碑验证指导学生仅使用自己编写的代码来重新创造关键突破，从 1958 年的感知器到现代变形金刚。 TinyTorch 只需要一台具有 4GB RAM 的笔记本电脑，无需 GPU，从而使机器学习系统教育可以在全球范围内进行。其目标是培养下一代人工智能工程师和从业者，他们不仅了解机器学习系统的作用，还了解它们为何工作以及如何使其规模化。该课程可在http URL斜线tinytorch上以开源形式获得。</li>
</ul>

<h3>Title: GTFMN: Guided Texture and Feature Modulation Network for Low-Light Image Enhancement and Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Yongsong Huang, Tzu-Hsuan Peng, Tomo Miyazaki, Xiaofeng Liu, Chun-Ting Chou, Ai-Chun Pang, Shinichiro Omachi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19157">https://arxiv.org/abs/2601.19157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19157">https://arxiv.org/pdf/2601.19157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19157]] GTFMN: Guided Texture and Feature Modulation Network for Low-Light Image Enhancement and Super-Resolution(https://arxiv.org/abs/2601.19157)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution</a></li>
<li><strong>Abstract: </strong>Low-light image super-resolution (LLSR) is a challenging task due to the coupled degradation of low resolution and poor illumination. To address this, we propose the Guided Texture and Feature Modulation Network (GTFMN), a novel framework that decouples the LLSR task into two sub-problems: illumination estimation and texture restoration. First, our network employs a dedicated Illumination Stream whose purpose is to predict a spatially varying illumination map that accurately captures lighting distribution. Further, this map is utilized as an explicit guide within our novel Illumination Guided Modulation Block (IGM Block) to dynamically modulate features in the Texture Stream. This mechanism achieves spatially adaptive restoration, enabling the network to intensify enhancement in poorly lit regions while preserving details in well-exposed areas. Extensive experiments demonstrate that GTFMN achieves the best performance among competing methods on the OmniNormal5 and OmniNormal15 datasets, outperforming them in both quantitative metrics and visual quality.</li>
<li><strong>摘要：</strong>由于低分辨率和不良照明的耦合退化，低光图像超分辨率（LLSR）是一项具有挑战性的任务。为了解决这个问题，我们提出了引导纹理和特征调制网络（GTFMN），这是一种新颖的框架，它将 LLSR 任务解耦为两个子问题：光照估计和纹理恢复。首先，我们的网络采用专用的照明流，其目的是预测空间变化的照明图，以准确捕获照明分布。此外，该图被用作我们新颖的照明引导调制块（IGM 块）中的明确指南，以动态调制纹理流中的特征。这种机制实现了空间自适应恢复，使网络能够在光线不足的区域加强增强，同时保留曝光良好区域的细节。大量实验表明，GTFMN 在 OmniNormal5 和 OmniNormal15 数据集上的竞争方法中实现了最佳性能，在定量指标和视觉质量方面均优于它们。</li>
</ul>

<h3>Title: SNR-Edit: Structure-Aware Noise Rectification for Inversion-Free Flow-Based Editing</h3>
<ul>
<li><strong>Authors: </strong>Lifan Jiang, Boxi Wu, Yuhang Pei, Tianrun Wu, Yongyuan Chen, Yan Zhao, Shiyu Yu, Deng Cai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19180">https://arxiv.org/abs/2601.19180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19180">https://arxiv.org/pdf/2601.19180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19180]] SNR-Edit: Structure-Aware Noise Rectification for Inversion-Free Flow-Based Editing(https://arxiv.org/abs/2601.19180)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Inversion-free image editing using flow-based generative models challenges the prevailing inversion-based pipelines. However, existing approaches rely on fixed Gaussian noise to construct the source trajectory, leading to biased trajectory dynamics and causing structural degradation or quality loss. To address this, we introduce SNR-Edit, a training-free framework achieving faithful Latent Trajectory Correction via adaptive noise control. Mechanistically, SNR-Edit uses structure-aware noise rectification to inject segmentation constraints into the initial noise, anchoring the stochastic component of the source trajectory to the real image's implicit inversion position and reducing trajectory drift during source--target transport. This lightweight modification yields smoother latent trajectories and ensures high-fidelity structural preservation without requiring model tuning or inversion. Across SD3 and FLUX, evaluations on PIE-Bench and SNR-Bench show that SNR-Edit delivers performance on pixel-level metrics and VLM-based scoring, while adding only about 1s overhead per image.</li>
<li><strong>摘要：</strong>使用基于流的生成模型的无反演图像编辑挑战了流行的基于反演的管道。然而，现有方法依赖固定高斯噪声来构建源轨迹，导致轨迹动态偏差并导致结构退化或质量损失。为了解决这个问题，我们引入了 SNR-Edit，这是一种无需训练的框架，通过自适应噪声控制实现忠实的潜在轨迹校正。从机制上讲，SNR-Edit 使用结构感知噪声校正将分割约束注入初始噪声，将源轨迹的随机分量锚定到真实图像的隐式反转位置，并减少源-目标传输期间的轨迹漂移。这种轻量级的修改产生更平滑的潜在轨迹，并确保高保真结构保存，而不需要模型调整或反演。在 SD3 和 FLUX 中，PIE-Bench 和 SNR-Bench 的评估表明，SNR-Edit 在像素级指标和基于 VLM 的评分方面提供了性能，同时每个图像仅增加了约 1 秒的开销。</li>
</ul>

<h3>Title: Foresight Learning for SEC Risk Prediction</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Turtel, Paul Wilczewski, Danny Franklin, Kris Skotheim</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19189">https://arxiv.org/abs/2601.19189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19189">https://arxiv.org/pdf/2601.19189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19189]] Foresight Learning for SEC Risk Prediction(https://arxiv.org/abs/2601.19189)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Risk disclosures in SEC filings describe potential adverse events but rarely quantify their likelihood, limiting their usefulness for probabilistic analysis. A central obstacle is the absence of large-scale, risk-level supervision linking disclosed risks to realized outcomes. We introduce a fully automated data generation pipeline that converts qualitative SEC risk disclosures into temporally grounded supervision using only public data. For each filing, the pipeline generates firm-specific, time-bounded risk queries from the Risk Factors section and labels them by automatically resolving outcomes against subsequent disclosures. Using this dataset of risk queries and outcomes grounded in SEC filings, we train a compact large language model to estimate the probability that a disclosed risk will materialize within a specified horizon. Despite its modest size, the resulting model substantially improves over pretrained and heuristic baselines, and outperforms frontier general-purpose models, including GPT-5, on probabilistic accuracy and calibration. More broadly, this work demonstrates that Foresight Learning enables scalable and fully automated training of domain-specific expert models using only raw, chronological, in-domain text -- without proprietary data, external corpora, or manual annotation. The resulting models achieve frontier-level performance while remaining deployable on a single GPU. This result suggests a general pathway for learning calibrated, decision-relevant signals from naturally occurring enterprise documents. To support transparency and reproducibility, we open-source the evaluation dataset used in this study. Evaluation Data: this https URL Data Generation Platform: this https URL SDK: this https URL</li>
<li><strong>摘要：</strong>美国证券交易委员会文件中的风险披露描述了潜在的不良事件，但很少量化其可能性，限制了其对概率分析的有用性。一个主要障碍是缺乏将披露的风险与实现的结果联系起来的大规模风险级别监管。我们引入了完全自动化的数据生成管道，仅使用公共数据将定性 SEC 风险披露转换为基于时间的监管。对于每个文件，管道都会从风险因素部分生成公司特定的、有时限的风险查询，并通过根据后续披露自动解决结果来标记它们。使用基于 SEC 文件的风险查询和结果数据集，我们训练一个紧凑的大型语言模型来估计所披露的风险在指定范围内实现的概率。尽管其规模不大，但所得模型比预训练和启发式基线有了显着改进，并且在概率准确性和校准方面优于前沿通用模型（包括 GPT-5）。更广泛地说，这项工作表明，Foresight Learning 仅使用原始的、按时间顺序排列的域内文本即可对特定领域的专家模型进行可扩展且完全自动化的训练，无需专有数据、外部语料库或手动注释。生成的模型实现了前沿水平的性能，同时保持可在单个 GPU 上部署。这一结果提出了从自然发生的企业文档中学习校准的、与决策相关的信号的一般途径。为了支持透明度和可重复性，我们开源了本研究中使用的评估数据集。评估数据：此 https URL 数据生成平台：此 https URL SDK：此 https URL</li>
</ul>

<h3>Title: Towards Pixel-Level VLM Perception via Simple Points Prediction</h3>
<ul>
<li><strong>Authors: </strong>Tianhui Song, Haoyu Lu, Hao Yang, Lin Sui, Haoning Wu, Zaida Zhou, Zhiqi Huang, Yiping Bao, Y.Charles, Xinyu Zhou, Limin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19228">https://arxiv.org/abs/2601.19228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19228">https://arxiv.org/pdf/2601.19228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19228]] Towards Pixel-Level VLM Perception via Simple Points Prediction(https://arxiv.org/abs/2601.19228)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present SimpleSeg, a strikingly simple yet highly effective approach to endow Multimodal Large Language Models (MLLMs) with native pixel-level perception. Our method reframes segmentation as a simple sequence generation problem: the model directly predicts sequences of points (textual coordinates) delineating object boundaries, entirely within its language space. To achieve high fidelity, we introduce a two-stage SF$\to$RL training pipeline, where Reinforcement Learning with an IoU-based reward refines the point sequences to accurately match ground-truth contours. We find that the standard MLLM architecture possesses a strong, inherent capacity for low-level perception that can be unlocked without any specialized architecture. On segmentation benchmarks, SimpleSeg achieves performance that is comparable to, and often surpasses, methods relying on complex, task-specific designs. This work lays out that precise spatial understanding can emerge from simple point prediction, challenging the prevailing need for auxiliary components and paving the way for more unified and capable VLMs. Homepage: this https URL</li>
<li><strong>摘要：</strong>我们提出了 SimpleSeg，这是一种极其简单但非常有效的方法，可以赋予多模态大型语言模型 (MLLM) 原生像素级感知。我们的方法将分割重新构建为一个简单的序列生成问题：模型直接预测完全在其语言空间内描绘对象边界的点序列（文本坐标）。为了实现高保真度，我们引入了两阶段 SF$\to$RL 训练管道，其中基于 IoU 的奖励的强化学习细化点序列以准确匹配地面真实轮廓。我们发现标准 MLLM 架构拥有强大的、固有的低级感知能力，无需任何专门的架构即可解锁。在分割基准上，SimpleSeg 的性能可与依赖于复杂的特定任务设计的方法相媲美，甚至常常超越。这项工作表明，简单的点预测可以产生精确的空间理解，挑战对辅助组件的普遍需求，并为更统一和更强大的 VLM 铺平道路。主页：这个 https URL</li>
</ul>

<h3>Title: VC-Bench: Pioneering the Video Connecting Benchmark with a Dataset and Evaluation Metrics</h3>
<ul>
<li><strong>Authors: </strong>Zhiyu Yin, Zhipeng Liu, Kehai Chen, Lemao Liu, Jin Liu, Hong-Dong Li, Yang Xiang, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19236">https://arxiv.org/abs/2601.19236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19236">https://arxiv.org/pdf/2601.19236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19236]] VC-Bench: Pioneering the Video Connecting Benchmark with a Dataset and Evaluation Metrics(https://arxiv.org/abs/2601.19236)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While current video generation focuses on text or image conditions, practical applications like video editing and vlogging often need to seamlessly connect separate clips. In our work, we introduce Video Connecting, an innovative task that aims to generate smooth intermediate video content between given start and end clips. However, the absence of standardized evaluation benchmarks has hindered the development of this task. To bridge this gap, we proposed VC-Bench, a novel benchmark specifically designed for video connecting. It includes 1,579 high-quality videos collected from public platforms, covering 15 main categories and 72 subcategories to ensure diversity and structure. VC-Bench focuses on three core aspects: Video Quality Score VQS, Start-End Consistency Score SECS, and Transition Smoothness Score TSS. Together, they form a comprehensive framework that moves beyond conventional quality-only metrics. We evaluated multiple state-of-the-art video generation models on VC-Bench. Experimental results reveal significant limitations in maintaining start-end consistency and transition smoothness, leading to lower overall coherence and fluidity. We expect that VC-Bench will serve as a pioneering benchmark to inspire and guide future research in video connecting. The evaluation metrics and dataset are publicly available at: this https URL.</li>
<li><strong>摘要：</strong>虽然当前的视频生成侧重于文本或图像条件，但视频编辑和视频博客等实际应用通常需要无缝连接单独的剪辑。在我们的工作中，我们引入了视频连接，这是一项创新任务，旨在在给定的开始和结束剪辑之间生成流畅的中间视频内容。然而，标准化评价基准的缺乏阻碍了这项任务的开展。为了弥补这一差距，我们提出了 VC-Bench，这是一种专为视频连接设计的新颖基准。它包含从公共平台收集的1,579个高质量视频，涵盖15个大类和72个小类，以确保多样性和结构。 VC-Bench关注三个核心方面：视频质量得分VQS、开始-结束一致性得分SECS和过渡平滑度得分TSS。它们共同构成了一个全面的框架，超越了传统的仅质量指标。我们在 VC-Bench 上评估了多种最先进的视频生成模型。实验结果表明，在维持起始端一致性和过渡平滑度方面存在显着局限性，导致整体连贯性和流动性较低。我们期望 VC-Bench 将成为启发和指导未来视频连接研究的开创性基准。评估指标和数据集可在以下网址公开获取：此 https URL。</li>
</ul>

<h3>Title: LLM-Assisted Logic Rule Learning: Scaling Human Expertise for Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Haoting Zhang, Shekhar Jain</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19255">https://arxiv.org/abs/2601.19255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19255">https://arxiv.org/pdf/2601.19255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19255]] LLM-Assisted Logic Rule Learning: Scaling Human Expertise for Time Series Anomaly Detection(https://arxiv.org/abs/2601.19255)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection is critical for supply chain management to take proactive operations, but faces challenges: classical unsupervised anomaly detection based on exploiting data patterns often yields results misaligned with business requirements and domain knowledge, while manual expert analysis cannot scale to millions of products in the supply chain. We propose a framework that leverages large language models (LLMs) to systematically encode human expertise into interpretable, logic-based rules for detecting anomaly patterns in supply chain time series data. Our approach operates in three stages: 1) LLM-based labeling of training data instructed by domain knowledge, 2) automated generation and iterative improvements of symbolic rules through LLM-driven optimization, and 3) rule augmentation with business-relevant anomaly categories supported by LLMs to enhance interpretability. The experiment results showcase that our approach outperforms the unsupervised learning methods in both detection accuracy and interpretability. Furthermore, compared to direct LLM deployment for time series anomaly detection, our approach provides consistent, deterministic results with low computational latency and cost, making it ideal for production deployment. The proposed framework thus demonstrates how LLMs can bridge the gap between scalable automation and expert-driven decision-making in operational settings.</li>
<li><strong>摘要：</strong>时间序列异常检测对于供应链管理采取主动操作至关重要，但也面临挑战：基于利用数据模式的传统无监督异常检测往往会产生与业务需求和领域知识不相符的结果，而手动专家分析无法扩展到供应链中的数百万种产品。我们提出了一个框架，利用大型语言模型（LLM）将人类专业知识系统地编码为可解释的、基于逻辑的规则，以检测供应链时间序列数据中的异常模式。我们的方法分三个阶段进行：1）在领域知识指导下，基于 LLM 的训练数据标记；2）通过 LLM 驱动的优化自动生成和迭代改进符号规则；3）利用 LLM 支持的业务相关异常类别进行规则扩充，以增强可解释性。实验结果表明，我们的方法在检测准确性和可解释性方面都优于无监督学习方法。此外，与用于时间序列异常检测的直接 LLM 部署相比，我们的方法提供了一致的、确定性的结果，并且计算延迟和成本低，使其成为生产部署的理想选择。因此，拟议的框架展示了法学硕士如何弥合可扩展的自动化和操作环境中专家驱动的决策之间的差距。</li>
</ul>

<h3>Title: E-QRGMM: Efficient Generative Metamodeling for Covariate-Dependent Uncertainty Quantification</h3>
<ul>
<li><strong>Authors: </strong>Zhiyang Liang, Qingkai Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19256">https://arxiv.org/abs/2601.19256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19256">https://arxiv.org/pdf/2601.19256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19256]] E-QRGMM: Efficient Generative Metamodeling for Covariate-Dependent Uncertainty Quantification(https://arxiv.org/abs/2601.19256)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Covariate-dependent uncertainty quantification in simulation-based inference is crucial for high-stakes decision-making but remains challenging due to the limitations of existing methods such as conformal prediction and classical bootstrap, which struggle with covariate-specific conditioning. We propose Efficient Quantile-Regression-Based Generative Metamodeling (E-QRGMM), a novel framework that accelerates the quantile-regression-based generative metamodeling (QRGMM) approach by integrating cubic Hermite interpolation with gradient estimation. Theoretically, we show that E-QRGMM preserves the convergence rate of the original QRGMM while reducing grid complexity from $O(n^{1/2})$ to $O(n^{1/5})$ for the majority of quantile levels, thereby substantially improving computational efficiency. Empirically, E-QRGMM achieves a superior trade-off between distributional accuracy and training speed compared to both QRGMM and other advanced deep generative models on synthetic and practical datasets. Moreover, by enabling bootstrap-based construction of confidence intervals for arbitrary estimands of interest, E-QRGMM provides a practical solution for covariate-dependent uncertainty quantification.</li>
<li><strong>摘要：</strong>基于模拟的推理中依赖于协变量的不确定性量化对于高风险决策至关重要，但由于保形预测和经典引导等现有方法的局限性，这些方法仍然具有挑战性，这些方法与协变量特定的条件作斗争。我们提出了基于分位数回归的高效生成元建模（E-QRGMM），这是一种新颖的框架，通过将三次 Hermite 插值与梯度估计相结合来加速基于分位数回归的生成元建模（QRGMM）方法。理论上，我们表明 E-QRGMM 保留了原始 QRGMM 的收敛速度，同时将大多数分位数级别的网格复杂度从 $O(n^{1/2})$ 降低到 $O(n^{1/5})$，从而显着提高了计算效率。根据经验，与 QRGMM 和其他在合成和实际数据集上的高级深度生成模型相比，E-QRGMM 在分布精度和训练速度之间实现了出色的权衡。此外，通过为任意感兴趣的估计值启用基于自举的置信区间构建，E-QRGMM 为协变量相关的不确定性量化提供了实用的解决方案。</li>
</ul>

<h3>Title: Handcrafted Feature Fusion for Reliable Detection of AI-Generated Images</h3>
<ul>
<li><strong>Authors: </strong>Syed Mehedi Hasan Nirob, Moqsadur Rahman, Shamim Ehsan, Summit Haque</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19262">https://arxiv.org/abs/2601.19262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19262">https://arxiv.org/pdf/2601.19262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19262]] Handcrafted Feature Fusion for Reliable Detection of AI-Generated Images(https://arxiv.org/abs/2601.19262)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid progress of generative models has enabled the creation of highly realistic synthetic images, raising concerns about authenticity and trust in digital media. Detecting such fake content reliably is an urgent challenge. While deep learning approaches dominate current literature, handcrafted features remain attractive for their interpretability, efficiency, and generalizability. In this paper, we conduct a systematic evaluation of handcrafted descriptors, including raw pixels, color histograms, Discrete Cosine Transform (DCT), Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), Gray-Level Co-occurrence Matrix (GLCM), and wavelet features, on the CIFAKE dataset of real versus synthetic images. Using 50,000 training and 10,000 test samples, we benchmark seven classifiers ranging from Logistic Regression to advanced gradient-boosted ensembles (LightGBM, XGBoost, CatBoost). Results demonstrate that LightGBM consistently outperforms alternatives, achieving PR-AUC 0.9879, ROC-AUC 0.9878, F1 0.9447, and a Brier score of 0.0414 with mixed features, representing strong gains in calibration and discrimination over simpler descriptors. Across three configurations (baseline, advanced, mixed), performance improves monotonically, confirming that combining diverse handcrafted features yields substantial benefit. These findings highlight the continued relevance of carefully engineered features and ensemble learning for detecting synthetic images, particularly in contexts where interpretability and computational efficiency are critical.</li>
<li><strong>摘要：</strong>生成模型的快速进步使得能够创建高度逼真的合成图像，引发了人们对数字媒体的真实性和信任度的担忧。可靠地检测此类虚假内容是一项紧迫的挑战。虽然深度学习方法在当前文献中占主导地位，但手工制作的特征因其可解释性、效率和普遍性而仍然具有吸引力。在本文中，我们在真实与合成图像的 CIFAKE 数据集上对手工制作的描述符进行了系统评估，包括原始像素、颜色直方图、离散余弦变换 (DCT)、定向梯度直方图 (HOG)、局部二值模式 (LBP)、灰度共生矩阵 (GLCM) 和小波特征。使用 50,000 个训练样本和 10,000 个测试样本，我们对从逻辑回归到高级梯度增强集成（LightGBM、XGBoost、CatBoost）等七种分类器进行了基准测试。结果表明，LightGBM 始终优于替代方案，实现了 PR-AUC 0.9879、ROC-AUC 0.9878、F1 0.9447，混合特征的 Brier 分数为 0.0414，这表明在校准和区分较简单描述符方面取得了巨大进步。在三种配置（基线、高级、混合）中，性能单调提高，证实结合不同的手工功能会产生巨大的好处。这些发现强调了精心设计的特征和集成学习对于检测合成图像的持续相关性，特别是在可解释性和计算效率至关重要的情况下。</li>
</ul>

<h3>Title: Smoothing the Score Function for Generalization in Diffusion Models: An Optimization-based Explanation Framework</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Zhou, Jiawei Zhang, Stephen J. Wright</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19285">https://arxiv.org/abs/2601.19285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19285">https://arxiv.org/pdf/2601.19285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19285]] Smoothing the Score Function for Generalization in Diffusion Models: An Optimization-based Explanation Framework(https://arxiv.org/abs/2601.19285)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models achieve remarkable generation quality, yet face a fundamental challenge known as memorization, where generated samples can replicate training samples exactly. We develop a theoretical framework to explain this phenomenon by showing that the empirical score function (the score function corresponding to the empirical distribution) is a weighted sum of the score functions of Gaussian distributions, in which the weights are sharp softmax functions. This structure causes individual training samples to dominate the score function, resulting in sampling collapse. In practice, approximating the empirical score function with a neural network can partially alleviate this issue and improve generalization. Our theoretical framework explains why: In training, the neural network learns a smoother approximation of the weighted sum, allowing the sampling process to be influenced by local manifolds rather than single points. Leveraging this insight, we propose two novel methods to further enhance generalization: (1) Noise Unconditioning enables each training sample to adaptively determine its score function weight to increase the effect of more training samples, thereby preventing single-point dominance and mitigating collapse. (2) Temperature Smoothing introduces an explicit parameter to control the smoothness. By increasing the temperature in the softmax weights, we naturally reduce the dominance of any single training sample and mitigate memorization. Experiments across multiple datasets validate our theoretical analysis and demonstrate the effectiveness of the proposed methods in improving generalization while maintaining high generation quality.</li>
<li><strong>摘要：</strong>扩散模型实现了卓越的生成质量，但面临着称为记忆的基本挑战，其中生成的样本可以准确地复制训练样本。我们开发了一个理论框架来解释这种现象，通过证明经验得分函数（与经验分布相对应的得分函数）是高斯分布得分函数的加权和，其中权重是锐软最大函数。这种结构导致单个训练样本主导评分函数，导致采样崩溃。在实践中，用神经网络逼近经验得分函数可以部分缓解这个问题并提高泛化能力。我们的理论框架解释了原因：在训练中，神经网络学习加权和的更平滑的近似，从而允许采样过程受到局部流形而不是单个点的影响。利用这一见解，我们提出了两种新方法来进一步增强泛化能力：（1）噪声去调节使每个训练样本能够自适应地确定其评分函数权重，以增加更多训练样本的效果，从而防止单点优势并减轻崩溃。 (2)温度平滑引入了显式参数来控制平滑度。通过增加 softmax 权重的温度，我们自然会减少任何单个训练样本的主导地位并减轻记忆。跨多个数据集的实验验证了我们的理论分析，并证明了所提出的方法在提高泛化性同时保持高生成质量方面的有效性。</li>
</ul>

<h3>Title: LightSBB-M: Bridging Schrödinger and Bass for Generative Diffusion Modeling</h3>
<ul>
<li><strong>Authors: </strong>Alexandre Alouadi, Pierre Henry-Labordère, Grégoire Loeper, Othmane Mazhar, Huyên Pham, Nizar Touzi</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY, stat.CO, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19312">https://arxiv.org/abs/2601.19312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19312">https://arxiv.org/pdf/2601.19312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19312]] LightSBB-M: Bridging Schrödinger and Bass for Generative Diffusion Modeling(https://arxiv.org/abs/2601.19312)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The Schrodinger Bridge and Bass (SBB) formulation, which jointly controls drift and volatility, is an established extension of the classical Schrodinger Bridge (SB). Building on this framework, we introduce LightSBB-M, an algorithm that computes the optimal SBB transport plan in only a few iterations. The method exploits a dual representation of the SBB objective to obtain analytic expressions for the optimal drift and volatility, and it incorporates a tunable parameter beta greater than zero that interpolates between pure drift (the Schrodinger Bridge) and pure volatility (Bass martingale transport). We show that LightSBB-M achieves the lowest 2-Wasserstein distance on synthetic datasets against state-of-the-art SB and diffusion baselines with up to 32 percent improvement. We also illustrate the generative capability of the framework on an unpaired image-to-image translation task (adult to child faces in FFHQ). These findings demonstrate that LightSBB-M provides a scalable, high-fidelity SBB solver that outperforms existing SB and diffusion baselines across both synthetic and real-world generative tasks. The code is available at this https URL.</li>
<li><strong>摘要：</strong>薛定谔电桥和巴斯 (SBB) 公式共同控制漂移和波动，是经典薛定谔电桥 (SB) 的既定扩展。在此框架的基础上，我们引入了 LightSBB-M，这是一种只需几次迭代即可计算出最佳 SBB 传输计划的算法。该方法利用 SBB 目标的双重表示来获得最佳漂移和波动率的解析表达式，并且它包含一个大于零的可调参数 beta，该参数在纯漂移（薛定谔桥）和纯波动率（巴斯鞅输运）之间进行插值。我们表明，与最先进的 SB 和扩散基线相比，LightSBB-M 在合成数据集上实现了最低 2-Wasserstein 距离，提升幅度高达 32%。我们还说明了该框架在不成对的图像到图像转换任务（FFHQ 中成人脸到儿童脸）上的生成能力。这些研究结果表明，LightSBB-M 提供了一种可扩展的高保真 SBB 求解器，在合成任务和现实生成任务中均优于现有的 SB 和扩散基线。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection</h3>
<ul>
<li><strong>Authors: </strong>Quy-Anh Dang, Chris Ngo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19375">https://arxiv.org/abs/2601.19375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19375">https://arxiv.org/pdf/2601.19375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19375]] Selective Steering: Norm-Preserving Control Through Discriminative Layer Selection(https://arxiv.org/abs/2601.19375)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Despite significant progress in alignment, large language models (LLMs) remain vulnerable to adversarial attacks that elicit harmful behaviors. Activation steering techniques offer a promising inference-time intervention approach, but existing methods suffer from critical limitations: activation addition requires careful coefficient tuning and is sensitive to layer-specific norm variations, while directional ablation provides only binary control. Recent work on Angular Steering introduces continuous control via rotation in a 2D subspace, but its practical implementation violates norm preservation, causing distribution shift and generation collapse, particularly in models below 7B parameters. We propose Selective Steering, which addresses these limitations through two key innovations: (1) a mathematically rigorous norm-preserving rotation formulation that maintains activation distribution integrity, and (2) discriminative layer selection that applies steering only where feature representations exhibit opposite-signed class alignment. Experiments across nine models demonstrate that Selective Steering achieves 5.5x higher attack success rates than prior methods while maintaining zero perplexity violations and approximately 100\% capability retention on standard benchmarks. Our approach provides a principled, efficient framework for controllable and stable LLM behavior modification. Code: this https URL</li>
<li><strong>摘要：</strong>尽管在一致性方面取得了重大进展，但大型语言模型（LLM）仍然容易受到引发有害行为的对抗性攻击。激活引导技术提供了一种很有前景的推理时间干预方法，但现有方法存在严重的局限性：激活添加需要仔细的系数调整，并且对特定于层的范数变化敏感，而定向消融仅提供二元控制。最近有关角度转向的工作引入了通过 2D 子空间中的旋转进行连续控制，但其实际实现违反了范数保留，导致分布偏移和生成崩溃，特别是在低于 7B 参数的模型中。我们提出选择性转向，它通过两个关键创新解决这些限制：（1）数学上严格的保持范数旋转公式，保持激活分布的完整性，以及（2）判别层选择，仅在特征表示表现出相反符号类对齐的情况下应用转向。跨九个模型的实验表明，选择性引导的攻击成功率比以前的方法高出 5.5 倍，同时在标准基准上保持零困惑度违规和大约 100% 的能力保留。我们的方法为可控且稳定的法学硕士行为修改提供了一个有原则的、高效的框架。代码：这个https URL</li>
</ul>

<h3>Title: RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming</h3>
<ul>
<li><strong>Authors: </strong>Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen, Xiaopeng Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19433">https://arxiv.org/abs/2601.19433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19433">https://arxiv.org/pdf/2601.19433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19433]] RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming(https://arxiv.org/abs/2601.19433)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>从文本生成沉浸式 3D 场景是计算机视觉的核心任务，对于虚拟现实和游戏开发中的应用至关重要。尽管有望利用二维扩散先验，但现有方法存在空间盲性，并且依赖于无法利用显着对象之间的内部关系的预定义轨迹。因此，这些方法无法理解语义布局，从而阻止它们自适应地探索场景以推断被遮挡的内容。此外，当前的修复模型在 2D 图像空间中运行，难以合理地填补相机运动造成的漏洞。为了解决这些限制，我们提出了 RoamScene3D，这是一种弥合语义引导和空间生成之间差距的新颖框架。我们的方法推理对象之间的语义关系并产生一致且逼真的场景。具体来说，我们采用视觉语言模型（VLM）来构建对对象关系进行编码的场景图，引导相机感知显着的对象边界并规划自适应漫游轨迹。此外，为了减轻静态 2D 先验的局限性，我们引入了运动注入修复模型，该模型在集成真实相机轨迹的合成全景数据集上进行微调，使其适应相机运动。大量的实验表明，通过语义推理和几何约束，我们的方法在生成一致且逼真的场景方面显着优于最先进的方法。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: OSIRIS: Bridging Analog Circuit Design and Machine Learning with Scalable Dataset Generation</h3>
<ul>
<li><strong>Authors: </strong>Giuseppe Chiari, Michele Piccoli, Davide Zoni</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19439">https://arxiv.org/abs/2601.19439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19439">https://arxiv.org/pdf/2601.19439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19439]] OSIRIS: Bridging Analog Circuit Design and Machine Learning with Scalable Dataset Generation(https://arxiv.org/abs/2601.19439)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The automation of analog integrated circuit (IC) design remains a longstanding challenge, primarily due to the intricate interdependencies among physical layout, parasitic effects, and circuit-level performance. These interactions impose complex constraints that are difficult to accurately capture and optimize using conventional design methodologies. Although recent advances in machine learning (ML) have shown promise in automating specific stages of the analog design flow, the development of holistic, end-to-end frameworks that integrate these stages and iteratively refine layouts using post-layout, parasitic-aware performance feedback is still in its early stages. Furthermore, progress in this direction is hindered by the limited availability of open, high-quality datasets tailored to the analog domain, restricting both the benchmarking and the generalizability of ML-based techniques. To address these limitations, we present OSIRIS, a scalable dataset generation pipeline for analog IC design. OSIRIS systematically explores the design space of analog circuits while producing comprehensive performance metrics and metadata, thereby enabling ML-driven research in electronic design automation (EDA). In addition, we release a dataset consisting of 87,100 circuit variations generated with OSIRIS, accompanied by a reinforcement learning (RL)-based baseline method that exploits OSIRIS for analog design optimization.</li>
<li><strong>摘要：</strong>模拟集成电路 (IC) 设计的自动化仍然是一个长期存在的挑战，这主要是由于物理布局、寄生效应和电路级性能之间错综复杂的相互依赖性。这些相互作用施加了复杂的约束，使用传统的设计方法很难准确捕获和优化。尽管机器学习 (ML) 的最新进展在自动化模拟设计流程的特定阶段方面显示出了希望，但集成这些阶段并使用布局后、寄生感知性能反馈迭代地完善布局的整体端到端框架的开发仍处于早期阶段。此外，针对模拟领域定制的开放、高质量数据集的可用性有限，阻碍了这一方向的进展，限制了基于机器学习的技术的基准测试和通用性。为了解决这些限制，我们推出了 OSIRIS，这是一种用于模拟 IC 设计的可扩展数据集生成管道。 OSIRIS 系统地探索模拟电路的设计空间，同时生成全面的性能指标和元数据，从而实现电子设计自动化 (EDA) 中机器学习驱动的研究。此外，我们还发布了一个数据集，其中包含使用 OSIRIS 生成的 87,100 个电路变体，以及基于强化学习 (RL) 的基线方法，该方法利用 OSIRIS 进行模拟设计优化。</li>
</ul>

<h3>Title: Dynamic Worlds, Dynamic Humans: Generating Virtual Human-Scene Interaction Motion in Dynamic Scenes</h3>
<ul>
<li><strong>Authors: </strong>Yin Wang, Zhiying Leng, Haitian Liu, Frederick W. B. Li, Mu Li, Xiaohui Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19484">https://arxiv.org/abs/2601.19484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19484">https://arxiv.org/pdf/2601.19484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19484]] Dynamic Worlds, Dynamic Humans: Generating Virtual Human-Scene Interaction Motion in Dynamic Scenes(https://arxiv.org/abs/2601.19484)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Scenes are continuously undergoing dynamic changes in the real world. However, existing human-scene interaction generation methods typically treat the scene as static, which deviates from reality. Inspired by world models, we introduce Dyn-HSI, the first cognitive architecture for dynamic human-scene interaction, which endows virtual humans with three humanoid components. (1)Vision (human eyes): we equip the virtual human with a Dynamic Scene-Aware Navigation, which continuously perceives changes in the surrounding environment and adaptively predicts the next waypoint. (2)Memory (human brain): we equip the virtual human with a Hierarchical Experience Memory, which stores and updates experiential data accumulated during training. This allows the model to leverage prior knowledge during inference for context-aware motion priming, thereby enhancing both motion quality and generalization. (3) Control (human body): we equip the virtual human with Human-Scene Interaction Diffusion Model, which generates high-fidelity interaction motions conditioned on multimodal inputs. To evaluate performance in dynamic scenes, we extend the existing static human-scene interaction datasets to construct a dynamic benchmark, Dyn-Scenes. We conduct extensive qualitative and quantitative experiments to validate Dyn-HSI, showing that our method consistently outperforms existing approaches and generates high-quality human-scene interaction motions in both static and dynamic settings.</li>
<li><strong>摘要：</strong>现实世界中的场景不断发生动态变化。然而，现有的人景交互生成方法通常将场景视为静态的，这与现实存在偏差。受世界模型的启发，我们推出了 Dyn-HSI，这是第一个用于动态人景交互的认知架构，它赋予虚拟人类三个类人组件。 （1）视觉（人眼）：我们为虚拟人配备了动态场景感知导航，它不断感知周围环境的变化并自适应地预测下一个路径点。 （2）记忆（人脑）：我们为虚拟人配备了分层经验记忆，用于存储和更新训练过程中积累的经验数据。这使得模型能够在推理过程中利用先验知识来进行上下文感知运动启动，从而提高运动质量和泛化能力。 （3）控制（人体）：我们为虚拟人配备了人景交互扩散模型，该模型可以根据多模态输入生成高保真交互运动。为了评估动态场景中的性能，我们扩展了现有的静态人景交互数据集来构建动态基准 Dyn-Scenes。我们进行了广泛的定性和定量实验来验证 Dyn-HSI，表明我们的方法始终优于现有方法，并在静态和动态设置中生成高质量的人景交互运动。</li>
</ul>

<h3>Title: Entropy-Guided k-Guard Sampling for Long-Horizon Autoregressive Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yizhao Han, Tianxing Shi, Zhao Wang, Zifan Xu, Zhiyuan Pu, Mingxiao Li, Qian Zhang, Wei Yin, Xiao-Xiao Long</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19488">https://arxiv.org/abs/2601.19488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19488">https://arxiv.org/pdf/2601.19488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19488]] Entropy-Guided k-Guard Sampling for Long-Horizon Autoregressive Video Generation(https://arxiv.org/abs/2601.19488)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autoregressive (AR) architectures have achieved significant successes in LLMs, inspiring explorations for video generation. In LLMs, top-p/top-k sampling strategies work exceptionally well: language tokens have high semantic density and low redundancy, so a fixed size of token candidates already strikes a balance between semantic accuracy and generation diversity. In contrast, video tokens have low semantic density and high spatio-temporal redundancy. This mismatch makes static top-k/top-p strategies ineffective for video decoders: they either introduce unnecessary randomness for low-uncertainty regions (static backgrounds) or get stuck in early errors for high-uncertainty regions (foreground objects). Prediction errors will accumulate as more frames are generated and eventually severely degrade long-horizon quality. To address this, we propose Entropy-Guided k-Guard (ENkG) sampling, a simple yet effective strategy that adapts sampling to token-wise dispersion, quantified by the entropy of each token's predicted distribution. ENkG uses adaptive token candidate sizes: for low-entropy regions, it employs fewer candidates to suppress redundant noise and preserve structural integrity; for high-entropy regions, it uses more candidates to mitigate error compounding. ENkG is model-agnostic, training-free, and adds negligible overhead. Experiments demonstrate consistent improvements in perceptual quality and structural stability compared to static top-k/top-p strategies.</li>
<li><strong>摘要：</strong>自回归 (AR) 架构在法学硕士领域取得了重大成功，激发了对视频生成的探索。在法学硕士中，top-p/top-k 采样策略效果非常好：语言标记具有高语义密度和低冗余，因此固定大小的标记候选已经在语义准确性和生成多样性之间取得了平衡。相比之下，视频令牌具有低语义密度和高时空冗余。这种不匹配使得静态 top-k/top-p 策略对视频解码器无效：它们要么为低不确定性区域（静态背景）引入不必要的随机性，要么陷入高不确定性区域（前景对象）的早期错误。随着更多帧的生成，预测误差将会累积，并最终严重降低长期质量。为了解决这个问题，我们提出了熵引导 k-Guard (ENkG) 采样，这是一种简单而有效的策略，可以使采样适应令牌分散，并通过每个令牌预测分布的熵进行量化。 ENkG 使用自适应标记候选大小：对于低熵区域，它采用较少的候选来抑制冗余噪声并保持结构完整性；对于高熵区域，它使用更多候选来减轻错误复合。 ENkG 与模型无关，无需训练，并且增加的开销可以忽略不计。实验表明，与静态 top-k/top-p 策略相比，感知质量和结构稳定性得到了持续改进。</li>
</ul>

<h3>Title: Cortex-Grounded Diffusion Models for Brain Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Fabian Bongratz, Yitong Li, Sama Elbaroudy, Christian Wachinger</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19498">https://arxiv.org/abs/2601.19498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19498">https://arxiv.org/pdf/2601.19498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19498]] Cortex-Grounded Diffusion Models for Brain Image Generation(https://arxiv.org/abs/2601.19498)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Synthetic neuroimaging data can mitigate critical limitations of real-world datasets, including the scarcity of rare phenotypes, domain shifts across scanners, and insufficient longitudinal coverage. However, existing generative models largely rely on weak conditioning signals, such as labels or text, which lack anatomical grounding and often produce biologically implausible outputs. To this end, we introduce Cor2Vox, a cortex-grounded generative framework for brain magnetic resonance image (MRI) synthesis that ties image generation to continuous structural priors of the cerebral cortex. It leverages high-resolution cortical surfaces to guide a 3D shape-to-image Brownian bridge diffusion process, enabling topologically faithful synthesis and precise control over underlying anatomies. To support the generation of new, realistic brain shapes, we developed a large-scale statistical shape model of cortical morphology derived from over 33,000 UK Biobank scans. We validated the fidelity of Cor2Vox based on traditional image quality metrics, advanced cortical surface reconstruction, and whole-brain segmentation quality, outperforming many baseline methods. Across three applications, namely (i) anatomically consistent synthesis, (ii) simulation of progressive gray matter atrophy, and (iii) harmonization of in-house frontotemporal dementia scans with public datasets, Cor2Vox preserved fine-grained cortical morphology at the sub-voxel level, exhibiting remarkable robustness to variations in cortical geometry and disease phenotype without retraining.</li>
<li><strong>摘要：</strong>合成神经影像数据可以减轻现实世界数据集的关键局限性，包括稀有表型的稀缺、扫描仪之间的域转移以及纵向覆盖范围不足。然而，现有的生成模型很大程度上依赖于弱条件信号，例如标签或文本，这些信号缺乏解剖学基础，并且经常产生生物学上不可信的输出。为此，我们引入了 Cor2Vox，这是一种基于皮层的脑磁共振图像 (MRI) 合成生成框架，它将图像生成与大脑皮层的连续结构先验联系起来。它利用高分辨率皮质表面来引导 3D 形状到图像布朗桥扩散过程，从而实现拓扑忠实的合成和对底层解剖结构的精确控制。为了支持新的、真实的大脑形状的生成，我们开发了一个大规模的皮质形态统计形状模型，该模型源自超过 33,000 个英国生物银行扫描。我们基于传统图像质量指标、先进的皮层表面重建和全脑分割质量验证了 Cor2Vox 的保真度，其性能优于许多基线方法。在三个应用中，即（i）解剖学上一致的合成，（ii）进行性灰质萎缩的模拟，以及（iii）内部额颞叶痴呆扫描与公共数据集的协调，Cor2Vox 在亚体素水平上保留了细粒度的皮质形态，在无需重新训练的情况下对皮质几何和疾病表型的变化表现出显着的鲁棒性。</li>
</ul>

<h3>Title: Bridging Information Asymmetry: A Hierarchical Framework for Deterministic Blind Face Restoration</h3>
<ul>
<li><strong>Authors: </strong>Zhengjian Yao, Jiakui Hu, Kaiwen Li, Hangzhou He, Xinliang Zhang, Shuang Zeng, Lei Zhu, Yanye Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19506">https://arxiv.org/abs/2601.19506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19506">https://arxiv.org/pdf/2601.19506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19506]] Bridging Information Asymmetry: A Hierarchical Framework for Deterministic Blind Face Restoration(https://arxiv.org/abs/2601.19506)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation, generative</a></li>
<li><strong>Abstract: </strong>Blind face restoration remains a persistent challenge due to the inherent ill-posedness of reconstructing holistic structures from severely constrained observations. Current generative approaches, while capable of synthesizing realistic textures, often suffer from information asymmetry -- the intrinsic disparity between the information-sparse low quality inputs and the information-dense high quality outputs. This imbalance leads to a one-to-many mapping, where insufficient constraints result in stochastic uncertainty and hallucinatory artifacts. To bridge this gap, we present \textbf{Pref-Restore}, a hierarchical framework that integrates discrete semantic logic with continuous texture generation to achieve deterministic, preference-aligned restoration. Our methodology fundamentally addresses this information disparity through two complementary strategies: (1) Augmenting Input Density: We employ an auto-regressive integrator to reformulate textual instructions into dense latent queries, injecting high-level semantic stability to constrain the degraded signals; (2) Pruning Output Distribution: We pioneer the integration of on-policy reinforcement learning directly into the diffusion restoration loop. By transforming human preferences into differentiable constraints, we explicitly penalize stochastic deviations, thereby sharpening the posterior distribution toward the desired high-fidelity outcomes. Extensive experiments demonstrate that Pref-Restore achieves state-of-the-art performance across synthetic and real-world benchmarks. Furthermore, empirical analysis confirms that our preference-aligned strategy significantly reduces solution entropy, establishing a robust pathway toward reliable and deterministic blind restoration.</li>
<li><strong>摘要：</strong>由于从严重受限的观察中重建整体结构固有的不适定性，盲脸恢复仍然是一个持续的挑战。当前的生成方法虽然能够合成逼真的纹理，但经常遭受信息不对称的困扰——信息稀疏的低质量输入和信息密集的高质量输出之间的内在差异。这种不平衡导致一对多映射，其中约束不足会导致随机不确定性和幻觉伪像。为了弥补这一差距，我们提出了 \textbf{Pref-Restore}，这是一个分层框架，它将离散语义逻辑与连续纹理生成相集成，以实现确定性、偏好一致的恢复。我们的方法通过两种互补策略从根本上解决了这种信息差异：（1）增强输入密度：我们采用自回归积分器将文本指令重新构造为密集的潜在查询，注入高级语义稳定性以约束退化的信号； (2) 修剪输出分布：我们率先将同策略强化学习直接集成到扩散恢复循环中。通过将人类偏好转化为可微的约束，我们明确惩罚随机偏差，从而锐化后验分布以实现所需的高保真结果。大量实验表明，Pref-Restore 在综合基准和实际基准中实现了最先进的性能。此外，实证分析证实，我们的偏好一致策略显着降低了解熵，建立了一条通往可靠和确定性盲恢复的稳健途径。</li>
</ul>

<h3>Title: GenCP: Towards Generative Modeling Paradigm of Coupled Physics</h3>
<ul>
<li><strong>Authors: </strong>Tianrun Gao, Haoren Zheng, Wenhao Deng, Haodong Feng, Tao Zhang, Ruiqi Feng, Qianyi Chen, Tailin Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19541">https://arxiv.org/abs/2601.19541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19541">https://arxiv.org/pdf/2601.19541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19541]] GenCP: Towards Generative Modeling Paradigm of Coupled Physics(https://arxiv.org/abs/2601.19541)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Real-world physical systems are inherently complex, often involving the coupling of multiple physics, making their simulation both highly valuable and challenging. Many mainstream approaches face challenges when dealing with decoupled data. Besides, they also suffer from low efficiency and fidelity in strongly coupled spatio-temporal physical systems. Here we propose GenCP, a novel and elegant generative paradigm for coupled multiphysics simulation. By formulating coupled-physics modeling as a probability modeling problem, our key innovation is to integrate probability density evolution in generative modeling with iterative multiphysics coupling, thereby enabling training on data from decoupled simulation and inferring coupled physics during sampling. We also utilize operator-splitting theory in the space of probability evolution to establish error controllability guarantees for this "conditional-to-joint" sampling scheme. We evaluate our paradigm on a synthetic setting and three challenging multi-physics scenarios to demonstrate both principled insight and superior application performance of GenCP. Code is available at this repo: this http URL.</li>
<li><strong>摘要：</strong>现实世界的物理系统本质上是复杂的，通常涉及多个物理场的耦合，这使得它们的模拟既非常有价值又具有挑战性。许多主流方法在处理解耦数据时面临挑战。此外，它们还面临强耦合时空物理系统的低效率和保真度问题。在这里，我们提出了 GenCP，一种新颖而优雅的耦合多物理场仿真生成范例。通过将耦合物理建模表述为概率建模问题，我们的关键创新是将生成建模中的概率密度演化与迭代多物理场耦合相结合，从而能够对来自解耦模拟的数据进行训练并在采样过程中推断耦合物理。我们还利用概率演化空间中的算子分裂理论来为这种“条件联合”采样方案建立误差可控性保证。我们在合成环境和三个具有挑战性的多物理场景上评估我们的范例，以展示 GenCP 的原则性见解和卓越的应用性能。代码可在此存储库中找到：此 http URL。</li>
</ul>

<h3>Title: MaDiS: Taming Masked Diffusion Language Models for Sign Language Generation</h3>
<ul>
<li><strong>Authors: </strong>Ronglai Zuo, Rolandos Alexandros Potamias, Qi Sun, Evangelos Ververas, Jiankang Deng, Stefanos Zafeiriou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19577">https://arxiv.org/abs/2601.19577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19577">https://arxiv.org/pdf/2601.19577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19577]] MaDiS: Taming Masked Diffusion Language Models for Sign Language Generation(https://arxiv.org/abs/2601.19577)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Sign language generation (SLG) aims to translate written texts into expressive sign motions, bridging communication barriers for the Deaf and Hard-of-Hearing communities. Recent studies formulate SLG within the language modeling framework using autoregressive language models, which suffer from unidirectional context modeling and slow token-by-token inference. To address these limitations, we present MaDiS, a masked-diffusion-based language model for SLG that captures bidirectional dependencies and supports efficient parallel multi-token generation. We further introduce a tri-level cross-modal pretraining scheme that jointly learns from token-, latent-, and 3D physical-space objectives, leading to richer and more grounded sign representations. To accelerate model convergence in the fine-tuning stage, we design a novel unmasking strategy with temporal checkpoints, reducing the combinatorial complexity of unmasking orders by over $10^{41}$ times. In addition, a mixture-of-parts embedding layer is developed to effectively fuse information stored in different part-wise sign tokens through learnable gates and well-optimized codebooks. Extensive experiments on CSL-Daily, Phoenix-2014T, and How2Sign demonstrate that MaDiS achieves superior performance across multiple metrics, including DTW error and two newly introduced metrics, SiBLEU and SiCLIP, while reducing inference latency by nearly 30%. Code and models will be released on our project page.</li>
<li><strong>摘要：</strong>手语生成 (SLG) 旨在将书面文本翻译成富有表现力的手语动作，为聋哑人和听力障碍群体消除沟通障碍。最近的研究使用自回归语言模型在语言建模框架内制定 SLG，该模型存在单向上下文建模和缓慢的逐个标记推理的问题。为了解决这些限制，我们提出了 MaDiS，一种基于掩码扩散的 SLG 语言模型，它捕获双向依赖关系并支持高效的并行多令牌生成。我们进一步引入了一种三级跨模态预训练方案，该方案联合学习令牌、潜在和 3D 物理空间目标，从而获得更丰富、更扎实的符号表示。为了加速微调阶段的模型收敛，我们设计了一种带有时间检查点的新颖的揭露策略，将揭露顺序的组合复杂度降低了超过 $10^{41}$ 倍。此外，还开发了部分混合嵌入层，以通过可学习门和优化良好的码本有效地融合存储在不同部分符号令牌中的信息。 CSL-Daily、Phoenix-2014T 和 How2Sign 上的大量实验表明，MaDiS 在多个指标上实现了卓越的性能，包括 DTW 错误和两个新引入的指标 SiBLEU 和 SiCLIP，同时将推理延迟减少了近 30%。代码和模型将在我们的项目页面上发布。</li>
</ul>

<h3>Title: Localized Latent Editing for Dose-Response Modeling in Botulinum Toxin Injection Planning</h3>
<ul>
<li><strong>Authors: </strong>Estèphe Arnaud, Mohamed Daoudi, Pierre Guerreschi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19593">https://arxiv.org/abs/2601.19593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19593">https://arxiv.org/pdf/2601.19593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19593]] Localized Latent Editing for Dose-Response Modeling in Botulinum Toxin Injection Planning(https://arxiv.org/abs/2601.19593)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Botulinum toxin (Botox) injections are the gold standard for managing facial asymmetry and aesthetic rejuvenation, yet determining the optimal dosage remains largely intuitive, often leading to suboptimal outcomes. We propose a localized latent editing framework that simulates Botulinum Toxin injection effects for injection planning through dose-response modeling. Our key contribution is a Region-Specific Latent Axis Discovery method that learns localized muscle relaxation trajectories in StyleGAN2's latent space, enabling precise control over specific facial regions without global side effects. By correlating these localized latent trajectories with injected toxin units, we learn a predictive dose-response model. We rigorously compare two approaches: direct metric regression versus image-based generative simulation on a clinical dataset of N=360 images from 46 patients. On a hold-out test set, our framework demonstrates moderate-to-strong structural correlations for geometric asymmetry metrics, confirming that the generative model correctly captures the direction of morphological changes. While biological variability limits absolute precision, we introduce a hybrid "Human-in-the-Loop" workflow where clinicians interactively refine simulations, bridging the gap between pathological reconstruction and cosmetic planning.</li>
<li><strong>摘要：</strong>肉毒杆菌毒素 (Botox) 注射是治疗面部不对称和美容年轻化的黄金标准，但确定最佳剂量在很大程度上仍然是直观的，通常会导致次优的结果。我们提出了一种局部潜在编辑框架，可通过剂量反应模型模拟肉毒杆菌毒素注射效果，以制定注射计划。我们的主要贡献是一种区域特定的潜在轴发现方法，该方法可以学习 StyleGAN2 潜在空间中的局部肌肉松弛轨迹，从而能够精确控制特定的面部区域，而不会产生全局副作用。通过将这些局部潜在轨迹与注射的毒素单位相关联，我们学习了预测剂量反应模型。我们严格比较两种方法：直接度量回归与基于图像的生成模拟，在来自 46 名患者的 N=360 图像的临床数据集上进行。在保留测试集上，我们的框架展示了几何不对称度量的中等到强的结构相关性，证实生成模型正确地捕获了形态变化的方向。虽然生物变异性限制了绝对精度，但我们引入了混合“人机循环”工作流程，临床医生可以交互式地完善模拟，从而弥合病理重建和美容规划之间的差距。</li>
</ul>

<h3>Title: GMS-CAVP: Improving Audio-Video Correspondence with Multi-Scale Contrastive and Generative Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Shentong Mo, Zehua Chen, Jun Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19606">https://arxiv.org/abs/2601.19606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19606">https://arxiv.org/pdf/2601.19606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19606]] GMS-CAVP: Improving Audio-Video Correspondence with Multi-Scale Contrastive and Generative Pretraining(https://arxiv.org/abs/2601.19606)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in video-audio (V-A) understanding and generation have increasingly relied on joint V-A embeddings, which serve as the foundation for tasks such as cross-modal retrieval and generation. While prior methods like CAVP effectively model semantic and temporal correspondences between modalities using contrastive objectives, their performance remains suboptimal. A key limitation is the insufficient modeling of the dense, multi-scale nature of both video and audio signals, correspondences often span fine- to coarse-grained spatial-temporal structures, which are underutilized in existing frameworks. To this end, we propose GMS-CAVP, a novel framework that combines Multi-Scale Video-Audio Alignment and Multi-Scale Spatial-Temporal Diffusion-based pretraining objectives to enhance V-A correspondence modeling. First, GMS-CAVP introduces a multi-scale contrastive learning strategy that captures semantic and temporal relations across varying granularities. Second, we go beyond traditional contrastive learning by incorporating a diffusion-based generative objective, enabling modality translation and synthesis between video and audio. This unified discriminative-generative formulation facilitates deeper cross-modal understanding and paves the way for high-fidelity generation. Extensive experiments on VGGSound, AudioSet, and Panda70M demonstrate that GMS-CAVP outperforms previous methods in generation and retrieval.</li>
<li><strong>摘要：</strong>视频音频（V-A）理解和生成方面的最新进展越来越依赖于联合 V-A 嵌入，它是跨模态检索和生成等任务的基础。虽然 CAVP 等现有方法使用对比目标有效地模拟模态之间的语义和时间对应关系，但它们的性能仍然不理想。一个关键的限制是对视频和音频信号的密集、多尺度性质的建模不足，对应关系通常跨越细粒度到粗粒度的时空结构，而这些结构在现有框架中没有得到充分利用。为此，我们提出了 GMS-CAVP，这是一种结合多尺度视频音频对齐和基于多尺度时空扩散的预训练目标的新颖框架，以增强 V-A 对应建模。首先，GMS-CAVP 引入了一种多尺度对比学习策略，可以捕获不同粒度的语义和时间关系。其次，我们超越了传统的对比学习，结合了基于扩散的生成目标，实现了视频和音频之间的模态转换和合成。这种统一的判别生成公式促进了更深入的跨模式理解，并为高保真生成铺平了道路。在 VGGSound、AudioSet 和 Panda70M 上进行的大量实验表明，GMS-CAVP 在生成和检索方面优于以前的方法。</li>
</ul>

<h3>Title: Grasynda: Graph-based Synthetic Time Series Generation</h3>
<ul>
<li><strong>Authors: </strong>Luis Amorim, Moises Santos, Paulo J. Azevedo, Carlos Soares, Vitor Cerqueira</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19668">https://arxiv.org/abs/2601.19668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19668">https://arxiv.org/pdf/2601.19668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19668]] Grasynda: Graph-based Synthetic Time Series Generation(https://arxiv.org/abs/2601.19668)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Data augmentation is a crucial tool in time series forecasting, especially for deep learning architectures that require a large training sample size to generalize effectively. However, extensive datasets are not always available in real-world scenarios. Although many data augmentation methods exist, their limitations include the use of transformations that do not adequately preserve data properties. This paper introduces Grasynda, a novel graph-based approach for synthetic time series generation that: (1) converts univariate time series into a network structure using a graph representation, where each state is a node and each transition is represented as a directed edge; and (2) encodes their temporal dynamics in a transition probability matrix. We performed an extensive evaluation of Grasynda as a data augmentation method for time series forecasting. We use three neural network variations on six benchmark datasets. The results indicate that Grasynda consistently outperforms other time series data augmentation methods, including ones used in state-of-the-art time series foundation models. The method and all experiments are publicly available.</li>
<li><strong>摘要：</strong>数据增强是时间序列预测的重要工具，特别是对于需要大量训练样本才能有效泛化的深度学习架构。然而，在现实场景中并不总是可以获得广泛的数据集。尽管存在许多数据增强方法，但它们的局限性包括使用不能充分保留数据属性的转换。本文介绍了 Grasynda，一种基于图的合成时间序列生成方法，该方法：（1）使用图表示将单变量时间序列转换为网络结构，其中每个状态是一个节点，每个转换表示为有向边； (2) 将它们的时间动态编码到转移概率矩阵中。我们对 Grasynda 作为时间序列预测的数据增强方法进行了广泛的评估。我们在六个基准数据集上使用三种神经网络变体。结果表明，Grasynda 始终优于其他时间序列数据增强方法，包括在最先进的时间序列基础模型中使用的方法。该方法和所有实验都是公开的。</li>
</ul>

<h3>Title: ProToken: Token-Level Attribution for Federated Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Waris Gill, Ahmad Humayun, Ali Anwar, Muhammad Ali Gulzar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19672">https://arxiv.org/abs/2601.19672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19672">https://arxiv.org/pdf/2601.19672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19672]] ProToken: Token-Level Attribution for Federated Large Language Models(https://arxiv.org/abs/2601.19672)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables collaborative training of Large Language Models (LLMs) across distributed data sources while preserving privacy. However, when federated LLMs are deployed in critical applications, it remains unclear which client(s) contributed to specific generated responses, hindering debugging, malicious client identification, fair reward allocation, and trust verification. We present ProToken, a novel Provenance methodology for Token-level attribution in federated LLMs that addresses client attribution during autoregressive text generation while maintaining FL privacy constraints. ProToken leverages two key insights to enable provenance at each token: (1) transformer architectures concentrate task-specific signals in later blocks, enabling strategic layer selection for computational tractability, and (2) gradient-based relevance weighting filters out irrelevant neural activations, focusing attribution on neurons that directly influence token generation. We evaluate ProToken across 16 configurations spanning four LLM architectures (Gemma, Llama, Qwen, SmolLM) and four domains (medical, financial, mathematical, coding). ProToken achieves 98% average attribution accuracy in correctly localizing responsible client(s), and maintains high accuracy when the number of clients are scaled, validating its practical viability for real-world deployment settings.</li>
<li><strong>摘要：</strong>联合学习 (FL) 支持跨分布式数据源的大型语言模型 (LLM) 协作训练，同时保护隐私。然而，当联合 LLM 部署在关键应用程序中时，仍不清楚哪些客户端促成了特定的生成响应，从而阻碍了调试、恶意客户端识别、公平奖励分配和信任验证。我们提出了 ProToken，这是一种用于联合法学硕士中令牌级归因的新颖来源方法，可解决自回归文本生成过程中的客户归因问题，同时保持 FL 隐私约束。 ProToken 利用两个关键的见解来实现每个令牌的来源：（1）变压器架构将特定于任务的信号集中在后面的块中，从而实现计算易处理性的战略层选择，以及（2）基于梯度的相关权重过滤掉不相关的神经激活，将归因集中在直接影响令牌生成的神经元上。我们评估了涵盖四个 LLM 架构（Gemma、Llama、Qwen、SmolLM）和四个领域（医学、金融、数学、编码）的 16 种配置的 ProToken。 ProToken 在正确定位负责任的客户端方面实现了 98% 的平均归因准确度，并在扩大客户端数量时保持高精度，验证了其在实际部署设置中的实际可行性。</li>
</ul>

<h3>Title: Self-Supervised Weight Templates for Scalable Vision Model Initialization</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Xie, Fu Feng, Ruixiao Shi, Jing Wang, Yong Rui, Xin Geng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19694">https://arxiv.org/abs/2601.19694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19694">https://arxiv.org/pdf/2601.19694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19694]] Self-Supervised Weight Templates for Scalable Vision Model Initialization(https://arxiv.org/abs/2601.19694)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The increasing scale and complexity of modern model parameters underscore the importance of pre-trained models. However, deployment often demands architectures of varying sizes, exposing limitations of conventional pre-training and fine-tuning. To address this, we propose SWEET, a self-supervised framework that performs constraint-based pre-training to enable scalable initialization in vision tasks. Instead of pre-training a fixed-size model, we learn a shared weight template and size-specific weight scalers under Tucker-based factorization, which promotes modularity and supports flexible adaptation to architectures with varying depths and widths. Target models are subsequently initialized by composing and reweighting the template through lightweight weight scalers, whose parameters can be efficiently learned from minimal training data. To further enhance flexibility in width expansion, we introduce width-wise stochastic scaling, which regularizes the template along width-related dimensions and encourages robust, width-invariant representations for improved cross-width generalization. Extensive experiments on \textsc{classification}, \textsc{detection}, \textsc{segmentation} and \textsc{generation} tasks demonstrate the state-of-the-art performance of SWEET for initializing variable-sized vision models.</li>
<li><strong>摘要：</strong>现代模型参数的规模和复杂性不断增加，凸显了预训练模型的重要性。然而，部署通常需要不同规模的架构，从而暴露出传统预训练和微调的局限性。为了解决这个问题，我们提出了 SWEET，这是一种自我监督框架，它执行基于约束的预训练，以在视觉任务中实现可扩展的初始化。我们没有预先训练固定大小的模型，而是在基于 Tucker 的分解下学习共享权重模板和特定于大小的权重缩放器，这促进了模块化并支持灵活适应具有不同深度和宽度的架构。随后通过轻量级权重缩放器组合和重新加权模板来初始化目标模型，其参数可以从最少的训练数据中有效地学习。为了进一步增强宽度扩展的灵活性，我们引入了宽度方向随机缩放，它沿着与宽度相关的维度规范模板，并鼓励鲁棒的、宽度不变的表示以改进跨宽度泛化。对\textsc{分类}、\textsc{检测}、\textsc{分割}和\textsc{生成}任务的广泛实验证明了 SWEET 在初始化可变大小视觉模型方面的最先进性能。</li>
</ul>

<h3>Title: WaterClear-GS: Optical-Aware Gaussian Splatting for Underwater Reconstruction and Restoration</h3>
<ul>
<li><strong>Authors: </strong>Xinrui Zhang, Yufeng Wang, Shuangkang Fang, Zesheng Wang, Dacheng Qi, Wenrui Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19753">https://arxiv.org/abs/2601.19753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19753">https://arxiv.org/pdf/2601.19753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19753]] WaterClear-GS: Optical-Aware Gaussian Splatting for Underwater Reconstruction and Restoration(https://arxiv.org/abs/2601.19753)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Underwater 3D reconstruction and appearance restoration are hindered by the complex optical properties of water, such as wavelength-dependent attenuation and scattering. Existing Neural Radiance Fields (NeRF)-based methods struggle with slow rendering speeds and suboptimal color restoration, while 3D Gaussian Splatting (3DGS) inherently lacks the capability to model complex volumetric scattering effects. To address these issues, we introduce WaterClear-GS, the first pure 3DGS-based framework that explicitly integrates underwater optical properties of local attenuation and scattering into Gaussian primitives, eliminating the need for an auxiliary medium network. Our method employs a dual-branch optimization strategy to ensure underwater photometric consistency while naturally recovering water-free appearances. This strategy is enhanced by depth-guided geometry regularization and perception-driven image loss, together with exposure constraints, spatially-adaptive regularization, and physically guided spectral regularization, which collectively enforce local 3D coherence and maintain natural visual perception. Experiments on standard benchmarks and our newly collected dataset demonstrate that WaterClear-GS achieves outstanding performance on both novel view synthesis (NVS) and underwater image restoration (UIR) tasks, while maintaining real-time rendering. The code will be available at this https URL.</li>
<li><strong>摘要：</strong>水下 3D 重建和外观恢复受到水复杂光学特性的阻碍，例如波长相关的衰减和散射。现有的基于神经辐射场 (NeRF) 的方法面临渲染速度慢和颜色恢复欠佳的问题，而 3D 高斯泼溅 (3DGS) 本质上缺乏对复杂体积散射效应进行建模的能力。为了解决这些问题，我们引入了 WaterClear-GS，这是第一个基于纯 3DGS 的框架，它将局部衰减和散射的水下光学特性明确集成到高斯原语中，从而无需辅助介质网络。我们的方法采用双分支优化策略来确保水下光度一致性，同时自然恢复无水外观。该策略通过深度引导的几何正则化和感知驱动的图像丢失以及曝光约束、空间自适应正则化和物理引导的光谱正则化得到增强，它们共同增强了局部 3D 一致性并保持自然视觉感知。标准基准测试和我们新收集的数据集的实验表明，WaterClear-GS 在新颖的视图合成（NVS）和水下图像恢复（UIR）任务上均取得了出色的性能，同时保持了实时渲染。该代码将在此 https URL 中提供。</li>
</ul>

<h3>Title: GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance</h3>
<ul>
<li><strong>Authors: </strong>Haozhi Zhu, Miaomiao Zhao, Dingyao Liu, Runze Tian, Yan Zhang, Jie Guo, Fenggen Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19785">https://arxiv.org/abs/2601.19785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19785">https://arxiv.org/pdf/2601.19785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19785]] GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance(https://arxiv.org/abs/2601.19785)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D scene generation is a core technology for gaming, film/VFX, and VR/AR. Growing demand for rapid iteration, high-fidelity detail, and accessible content creation has further increased interest in this area. Existing methods broadly follow two paradigms - indirect 2D-to-3D reconstruction and direct 3D generation - but both are limited by weak structural modeling and heavy reliance on large-scale ground-truth supervision, often producing structural artifacts, geometric inconsistencies, and degraded high-frequency details in complex scenes. We propose GeoDiff3D, an efficient self-supervised framework that uses coarse geometry as a structural anchor and a geometry-constrained 2D diffusion model to provide texture-rich reference images. Importantly, GeoDiff3D does not require strict multi-view consistency of the diffusion-generated references and remains robust to the resulting noisy, inconsistent guidance. We further introduce voxel-aligned 3D feature aggregation and dual self-supervision to maintain scene coherence and fine details while substantially reducing dependence on labeled data. GeoDiff3D also trains with low computational cost and enables fast, high-quality 3D scene generation. Extensive experiments on challenging scenes show improved generalization and generation quality over existing baselines, offering a practical solution for accessible and efficient 3D scene construction.</li>
<li><strong>摘要：</strong>3D 场景生成是游戏、电影/VFX 和 VR/AR 的核心技术。对快速迭代、高保真细节和易于访问的内容创建的需求不断增长，进一步增加了人们对该领域的兴趣。现有方法大致遵循两种范式——间接 2D 到 3D 重建和直接 3D 生成——但这两种方法都受到结构建模较弱和严重依赖大规模地面实况监督的限制，通常会产生结构伪影、几何不一致以及复杂场景中高频细节的退化。我们提出了 GeoDiff3D，这是一种高效的自监督框架，它使用粗几何作为结构锚和几何约束的 2D 扩散模型来提供纹理丰富的参考图像。重要的是，GeoDiff3D 不要求扩散生成的参考具有严格的多视图一致性，并且对由此产生的噪声、不一致的指导保持鲁棒性。我们进一步引入体素对齐的 3D 特征聚合和双重自监督，以保持场景连贯性和精细细节，同时大幅减少对标记数据的依赖。 GeoDiff3D 还可以以较低的计算成本进行训练，并实现快速、高质量的 3D 场景生成。对具有挑战性的场景进行的大量实验表明，泛化能力和生成质量比现有基线有所提高，为可访问且高效的 3D 场景构建提供了实用的解决方案。</li>
</ul>

<h3>Title: Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals</h3>
<ul>
<li><strong>Authors: </strong>Octavio Pappalardo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19810">https://arxiv.org/abs/2601.19810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19810">https://arxiv.org/pdf/2601.19810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19810]] Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals(https://arxiv.org/abs/2601.19810)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Unsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks. A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals. The core challenge lies in how to effectively generate, select, and learn from such goals. Our focus is on broad distributions of downstream tasks where solving every task zero-shot is infeasible. Such settings naturally arise when the target tasks lie outside of the pre-training distribution or when their identities are unknown to the agent. In this work, we (i) optimize for efficient multi-episode exploration and adaptation within a meta-learning framework, and (ii) guide the training curriculum with evolving estimates of the agent's post-adaptation performance. We present ULEE, an unsupervised meta-learning method that combines an in-context learner with an adversarial goal-generation strategy that maintains training at the frontier of the agent's capabilities. On XLand-MiniGrid benchmarks, ULEE pre-training yields improved exploration and adaptation abilities that generalize to novel objectives, environment dynamics, and map structures. The resulting policy attains improved zero-shot and few-shot performance, and provides a strong initialization for longer fine-tuning processes. It outperforms learning from scratch, DIAYN pre-training, and alternative curricula.</li>
<li><strong>摘要：</strong>无监督预训练可以为强化学习代理配备先验知识，并加速下游任务的学习。一个基于人类发展的有前景的方向是研究通过设定和追求自己的目标来学习的智能体。核心挑战在于如何有效地生成、选择这些目标并从中学习。我们的重点是下游任务的广泛分布，其中零样本解决每个任务是不可行的。当目标任务位于预训练分布之外或者代理未知其身份时，这种设置自然会出现。在这项工作中，我们（i）在元学习框架内优化高效的多阶段探索和适应，以及（ii）通过对智能体适应后性能的不断变化的估计来指导培训课程。我们提出了 ULEE，这是一种无监督元学习方法，它将上下文学习器与对抗性目标生成策略相结合，该策略将训练保持在代理能力的前沿。在 XLand-MiniGrid 基准测试中，ULEE 预训练提高了探索和适应能力，可推广到新目标、环境动态和地图结构。由此产生的策略获得了改进的零样本和少样本性能，并为更长的微调过程提供了强大的初始化。它优于从头开始学习、DIAYN 预训练和替代课程。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
