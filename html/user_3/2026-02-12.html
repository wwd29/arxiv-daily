<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-12</h1>
<h3>Title: Large Language Models Predict Functional Outcomes after Acute Ischemic Stroke</h3>
<ul>
<li><strong>Authors: </strong>Anjali K. Kapoor (1), Anton Alyakin (1,2,3), Jin Vivian Lee (1,2,3), Eunice Yang (1,4), Annelene M. Schulze (1), Krithik Vishwanath (5), Jinseok Lee (2,6), Yindalon Aphinyanaphongs (7,8), Howard Riina (1,9), Jennifer A. Frontera (10), Eric Karl Oermann (1,2,8,11) ((1) Department of Neurosurgery, NYU Langone Health, New York, USA (2) Global AI Frontier Lab, New York University, Brooklyn, USA (3) Department of Neurosurgery, Washington University in Saint Louis, Saint Louis, USA (4) Columbia University Vagelos College of Physicians and Surgeons, New York, USA (5) Department of Aerospace Engineering and Engineering Mechanics, University of Texas at Austin, Austin, USA (6) Department of Biomedical Engineering, Kyung Hee University, Yongin, South Korea (7) Department of Population Health, NYU Langone Health, New York, USA (8) Division of Applied AI Technologies, NYU Langone Health, New York, USA (9) Department of Radiology, NYU Langone Health, New York, USA (10) Department of Neurology, NYU Langone Health, New York, USA (11) Center for Data Science, New York University, New York, USA)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10119">https://arxiv.org/abs/2602.10119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10119">https://arxiv.org/pdf/2602.10119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10119]] Large Language Models Predict Functional Outcomes after Acute Ischemic Stroke(https://arxiv.org/abs/2602.10119)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate prediction of functional outcomes after acute ischemic stroke can inform clinical decision-making and resource allocation. Prior work on modified Rankin Scale (mRS) prediction has relied primarily on structured variables (e.g., age, NIHSS) and conventional machine learning. The ability of large language models (LLMs) to infer future mRS scores directly from routine admission notes remains largely unexplored. We evaluated encoder (BERT, NYUTron) and generative (Llama-3.1-8B, MedGemma-4B) LLMs, in both frozen and fine-tuned settings, for discharge and 90-day mRS prediction using a large, real-world stroke registry. The discharge outcome dataset included 9,485 History and Physical notes and the 90-day outcome dataset included 1,898 notes from the NYU Langone Get With The Guidelines-Stroke registry (2016-2025). Data were temporally split with the most recent 12 months held out for testing. Performance was assessed using exact (7-class) mRS accuracy and binary functional outcome (mRS 0-2 vs. 3-6) accuracy and compared against established structured-data baselines incorporating NIHSS and age. Fine-tuned Llama achieved the highest performance, with 90-day exact mRS accuracy of 33.9% [95% CI, 27.9-39.9%] and binary accuracy of 76.3% [95% CI, 70.7-81.9%]. Discharge performance reached 42.0% [95% CI, 39.0-45.0%] exact accuracy and 75.0% [95% CI, 72.4-77.6%] binary accuracy. For 90-day prediction, Llama performed comparably to structured-data baselines. Fine-tuned LLMs can predict post-stroke functional outcomes from admission notes alone, achieving performance comparable to models requiring structured variable abstraction. Our findings support the development of text-based prognostic tools that integrate seamlessly into clinical workflows without manual data extraction.</li>
<li><strong>摘要：</strong>准确预测急性缺血性卒中后的功能结果可以为临床决策和资源分配提供信息。先前有关改进Rankin量表（mRS）预测的工作主要依赖于结构化变量（例如年龄、NIHSS）和传统机器学习。大型语言模型 (LLM) 直接从常规录取笔记推断未来 mRS 分数的能力在很大程度上尚未得到探索。我们在冻结和微调设置下评估了编码器（BERT、NYUTron）和生成式（Llama-3.1-8B、MedGemma-4B）LLM，以使用大型真实世界卒中注册表进行出院和 90 天 mRS 预测。出院结果数据集包括 9,485 条历史和身体记录，90 天结果数据集包括 1,898 条来自纽约大学朗格尼卒中指南登记处 (2016-2025) 的记录。数据按时间划分，保留最近 12 个月的数据进行测试。使用精确（7 级）mRS 准确性和二元功能结果（mRS 0-2 与 3-6）准确性评估表现，并与纳入 NIHSS 和年龄的既定结构化数据基线进行比较。经过微调的 Llama 实现了最高性能，90 天精确 mRS 准确度为 33.9% [95% CI, 27.9-39.9%]，二进制准确度为 76.3% [95% CI, 70.7-81.9%]。放电性能达到 42.0% [95% CI, 39.0-45.0%] 精确精度和 75.0% [95% CI, 72.4-77.6%] 二进制精度。对于 90 天预测，Llama 的表现与结构化数据基线相当。经过微调的法学硕士可以仅根据入院记录来预测中风后的功能结果，其性能可与需要结构化变量抽象的模型相媲美。我们的研究结果支持开发基于文本的预后工具，该工具可以无缝集成到临床工作流程中，而无需手动提取数据。</li>
</ul>

<h3>Title: ELROND: Exploring and decomposing intrinsic capabilities of diffusion models</h3>
<ul>
<li><strong>Authors: </strong>Paweł Skierś, Tomasz Trzciński, Kamil Deja</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10216">https://arxiv.org/abs/2602.10216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10216">https://arxiv.org/pdf/2602.10216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10216]] ELROND: Exploring and decomposing intrinsic capabilities of diffusion models(https://arxiv.org/abs/2602.10216)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A single text prompt passed to a diffusion model often yields a wide range of visual outputs determined solely by stochastic process, leaving users with no direct control over which specific semantic variations appear in the image. While existing unsupervised methods attempt to analyze these variations via output features, they omit the underlying generative process. In this work, we propose a framework to disentangle these semantic directions directly within the input embedding space. To that end, we collect a set of gradients obtained by backpropagating the differences between stochastic realizations of a fixed prompt that we later decompose into meaningful steering directions with either Principal Components Analysis or Sparse Autoencoder. Our approach yields three key contributions: (1) it isolates interpretable, steerable directions for precise, fine-grained control over a single concept; (2) it effectively mitigates mode collapse in distilled models by reintroducing lost diversity; and (3) it establishes a novel estimator for concept complexity under a specific model, based on the dimensionality of the discovered subspace.</li>
<li><strong>摘要：</strong>传递到扩散模型的单个文本提示通常会产生仅由随机过程决定的各种视觉输出，使用户无法直接控制图像中出现的特定语义变化。虽然现有的无监督方法试图通过输出特征来分析这些变化，但它们忽略了底层的生成过程。在这项工作中，我们提出了一个框架来直接在输入嵌入空间中解开这些语义方向。为此，我们收集了一组梯度，这些梯度是通过反向传播固定提示的随机实现之间的差异而获得的，随后我们使用主成分分析或稀疏自动编码器将其分解为有意义的转向方向。我们的方法产生了三个关键贡献：（1）它隔离了可解释、可操纵的方向，以对单个概念进行精确、细粒度的控制； （2）通过重新引入丢失的多样性，有效缓解蒸馏模型中的模式崩溃； (3)基于所发现的子空间的维数，在特定模型下建立了一种新颖的概念复杂性估计器。</li>
</ul>

<h3>Title: Temper-Then-Tilt: Principled Unlearning for Generative Models through Tempering and Classifier Guidance</h3>
<ul>
<li><strong>Authors: </strong>Jacob L. Block, Mehryar Mohri, Aryan Mokhtari, Sanjay Shakkottai</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10217">https://arxiv.org/abs/2602.10217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10217">https://arxiv.org/pdf/2602.10217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10217]] Temper-Then-Tilt: Principled Unlearning for Generative Models through Tempering and Classifier Guidance(https://arxiv.org/abs/2602.10217)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We study machine unlearning in large generative models by framing the task as density ratio estimation to a target distribution rather than supervised fine-tuning. While classifier guidance is a standard approach for approximating this ratio and can succeed in general, we show it can fail to faithfully unlearn with finite samples when the forget set represents a sharp, concentrated data distribution. To address this, we introduce Temper-Then-Tilt Unlearning (T3-Unlearning), which freezes the base model and applies a two-step inference procedure: (i) tempering the base distribution to flatten high-confidence spikes, and (ii) tilting the tempered distribution using a lightweight classifier trained to distinguish retain from forget samples. Our theoretical analysis provides finite-sample guarantees linking the surrogate classifier's risk to unlearning error, proving that tempering is necessary to successfully unlearn for concentrated distributions. Empirical evaluations on the TOFU benchmark show that T3-Unlearning improves forget quality and generative utility over existing baselines, while training only a fraction of the parameters with a minimal runtime.</li>
<li><strong>摘要：</strong>我们通过将任务框架为目标分布的密度比估计而不是监督微调来研究大型生成模型中的机器遗忘。虽然分类器指导是近似该比率的标准方法并且通常可以成功，但我们表明，当遗忘集代表尖锐、集中的数据分布时，它可能无法忠实地忘记有限样本。为了解决这个问题，我们引入了Temper-Then-Tilt Unlearning（T3-Unlearning），它冻结基础模型并应用两步推理过程：（i）调整基础分布以压平高置信度尖峰，以及（ii）使用经过训练以区分保留样本和遗忘样本的轻量级分类器倾斜调整分布。我们的理论分析提供了有限样本保证，将代理分类器的风险与遗忘错误联系起来，证明调节对于成功遗忘集中分布是必要的。对 TOFU 基准的实证评估表明，T3-Unlearning 比现有基线提高了遗忘质量和生成效用，同时以最短的运行时间仅训练一小部分参数。</li>
</ul>

<h3>Title: Self-Evolving Recommendation System: End-To-End Autonomous Model Optimization With LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Haochen Wang, Yi Wu, Daryl Chang, Li Wei, Lukasz Heldt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10226">https://arxiv.org/abs/2602.10226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10226">https://arxiv.org/pdf/2602.10226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10226]] Self-Evolving Recommendation System: End-To-End Autonomous Model Optimization With LLM Agents(https://arxiv.org/abs/2602.10226)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Optimizing large-scale machine learning systems, such as recommendation models for global video platforms, requires navigating a massive hyperparameter search space and, more critically, designing sophisticated optimizers, architectures, and reward functions to capture nuanced user behaviors. Achieving substantial improvements in these areas is a non-trivial task, traditionally relying on extensive manual iterations to test new hypotheses. We propose a self-evolving system that leverages Large Language Models (LLMs), specifically those from Google's Gemini family, to autonomously generate, train, and deploy high-performing, complex model changes within an end-to-end automated workflow. The self-evolving system is comprised of an Offline Agent (Inner Loop) that performs high-throughput hypothesis generation using proxy metrics, and an Online Agent (Outer Loop) that validates candidates against delayed north star business metrics in live production. Our agents act as specialized Machine Learning Engineers (MLEs): they exhibit deep reasoning capabilities, discovering novel improvements in optimization algorithms and model architecture, and formulating innovative reward functions that target long-term user engagement. The effectiveness of this approach is demonstrated through several successful production launches at YouTube, confirming that autonomous, LLM-driven evolution can surpass traditional engineering workflows in both development velocity and model performance.</li>
<li><strong>摘要：</strong>优化大规模机器学习系统（例如全球视频平台的推荐模型）需要浏览巨大的超参数搜索空间，更重要的是，需要设计复杂的优化器、架构和奖励函数来捕获细微的用户行为。在这些领域实现实质性改进是一项艰巨的任务，传统上依靠大量的手动迭代来测试新假设。我们提出了一种自我进化系统，利用大型语言模型（LLM），特别是来自 Google Gemini 系列的语言模型，在端到端自动化工作流程中自主生成、训练和部署高性能、复杂的模型更改。该自我进化系统由一个离线代理（内循环）和一个在线代理（外循环）组成，离线代理使用代理指标执行高吞吐量假设生成，在线代理根据实时生产中的延迟北极星业务指标验证候选者。我们的代理充当专业的机器学习工程师（MLE）：他们表现出深度推理能力，发现优化算法和模型架构的新颖改进，并制定针对长期用户参与的创新奖励函数。 YouTube 上的多次成功发布证明了这种方法的有效性，证实了法学硕士驱动的自主演进可以在开发速度和模型性能方面超越传统的工程工作流程。</li>
</ul>

<h3>Title: Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards</h3>
<ul>
<li><strong>Authors: </strong>Kirill Pavlenko, Alexander Golubev, Simon Karasik, Boris Yangel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10231">https://arxiv.org/abs/2602.10231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10231">https://arxiv.org/pdf/2602.10231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10231]] Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards(https://arxiv.org/abs/2602.10231)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Group Relative Policy Optimization (GRPO) assigns a single scalar advantage to all tokens in a completion. For structured generations with explicit segments and objectives, this couples unrelated reward signals across segments, leading to objective interference and misattributed credit. We propose Blockwise Advantage Estimation, a family of GRPO-compatible methods that assigns each objective its own advantage and applies it only to the tokens in the corresponding text block, reducing reliance on hand-designed scalar rewards and scaling naturally to additional objectives. A key challenge is estimating advantages for later blocks whose rewards are conditioned on sampled prefixes; standard unbiased approaches require expensive nested rollouts from intermediate states. Concretely, we introduce an Outcome-Conditioned Baseline that approximates intermediate state values using only within-group statistics by stratifying samples according to a prefix-derived intermediate outcome. On math tasks with uncertainty estimation, our method mitigates reward interference, is competitive with a state-of-the-art reward-designed approach, and preserves test-time gains from confidence-weighted ensembling. More broadly, it provides a modular recipe for optimizing sequential objectives in structured generations without additional rollouts.</li>
<li><strong>摘要：</strong>组相对策略优化 (GRPO) 为完成中的所有令牌分配单一标量优势。对于具有明确细分和目标的结构化一代，这会将跨细分的不相关奖励信号耦合起来，从而导致客观干扰和错误归因的信用。我们提出了 Blockwise Advantage Estimation，这是一系列与 GRPO 兼容的方法，它为每个目标分配自己的优势，并将其仅应用于相应文本块中的标记，减少对手工设计的标量奖励的依赖，并自然地扩展到其他目标。一个关键的挑战是估计后续区块的优势，其奖励取决于采样的前缀；标准的无偏方法需要从中间状态进行昂贵的嵌套部署。具体来说，我们引入了一个结果条件基线，该基线仅使用组内统计数据通过根据前缀导出的中间结果对样本进行分层来近似中间状态值。在具有不确定性估计的数学任务中，我们的方法减轻了奖励干扰，与最先进的奖励设计方法具有竞争力，并保留了置信加权集成带来的测试时间收益。更广泛地说，它提供了一个模块化方案，用于优化结构化生成中的顺序目标，而无需额外部署。</li>
</ul>

<h3>Title: XSPLAIN: XAI-enabling Splat-based Prototype Learning for Attribute-aware INterpretability</h3>
<ul>
<li><strong>Authors: </strong>Dominik Galus, Julia Farganus, Tymoteusz Zapala, Mikołaj Czachorowski, Piotr Borycki, Przemysław Spurek, Piotr Syga</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10239">https://arxiv.org/abs/2602.10239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10239">https://arxiv.org/pdf/2602.10239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10239]] XSPLAIN: XAI-enabling Splat-based Prototype Learning for Attribute-aware INterpretability(https://arxiv.org/abs/2602.10239)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting (3DGS) has rapidly become a standard for high-fidelity 3D reconstruction, yet its adoption in multiple critical domains is hindered by the lack of interpretability of the generation models as well as classification of the Splats. While explainability methods exist for other 3D representations, like point clouds, they typically rely on ambiguous saliency maps that fail to capture the volumetric coherence of Gaussian primitives. We introduce XSPLAIN, the first ante-hoc, prototype-based interpretability framework designed specifically for 3DGS classification. Our approach leverages a voxel-aggregated PointNet backbone and a novel, invertible orthogonal transformation that disentangles feature channels for interpretability while strictly preserving the original decision boundaries. Explanations are grounded in representative training examples, enabling intuitive ``this looks like that'' reasoning without any degradation in classification performance. A rigorous user study (N=51) demonstrates a decisive preference for our approach: participants selected XSPLAIN explanations 48.4\% of the time as the best, significantly outperforming baselines $(p<0.001)$, showing that XSPLAIN provides transparency and user trust. The source code for this work is available at: this https URL</li>
<li><strong>摘要：</strong>3D 高斯分布 (3DGS) 已迅速成为高保真 3D 重建的标准，但由于生成模型和 Splat 分类缺乏可解释性，其在多个关键领域的采用受到阻碍。虽然其他 3D 表示（例如点云）也存在可解释性方法，但它们通常依赖于模糊的显着图，而这些显着图无法捕获高斯基元的体积相干性。我们推出 XSPLAIN，这是第一个专门为 3DGS 分类设计的事前、基于原型的可解释性框架。我们的方法利用体素聚合的 PointNet 主干和新颖的可逆正交变换，可以解开特征通道以实现可解释性，同时严格保留原始决策边界。解释基于代表性的训练示例，可实现直观的“这看起来像那样”推理，而不会降低分类性能。严格的用户研究 (N=51) 表明了对我们方法的决定性偏好：参与者在 48.4\% 的情况下选择 XSPLAIN 解释为最佳解释，显着优于基线 $(p<0.001)$，表明 XSPLAIN 提供透明度和用户信任。这项工作的源代码位于：此 https URL</li>
</ul>

<h3>Title: ERGO: Excess-Risk-Guided Optimization for High-Fidelity Monocular 3D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Zehua Ma, Hanhui Li, Zhenyu Xie, Xiaonan Luo, Michael Kampffmeyer, Feng Gao, Xiaodan Liang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10278">https://arxiv.org/abs/2602.10278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10278">https://arxiv.org/pdf/2602.10278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10278]] ERGO: Excess-Risk-Guided Optimization for High-Fidelity Monocular 3D Gaussian Splatting(https://arxiv.org/abs/2602.10278)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generating 3D content from a single image remains a fundamentally challenging and ill-posed problem due to the inherent absence of geometric and textural information in occluded regions. While state-of-the-art generative models can synthesize auxiliary views to provide additional supervision, these views inevitably contain geometric inconsistencies and textural misalignments that propagate and amplify artifacts during 3D reconstruction. To effectively harness these imperfect supervisory signals, we propose an adaptive optimization framework guided by excess risk decomposition, termed ERGO. Specifically, ERGO decomposes the optimization losses in 3D Gaussian splatting into two components, i.e., excess risk that quantifies the suboptimality gap between current and optimal parameters, and Bayes error that models the irreducible noise inherent in synthesized views. This decomposition enables ERGO to dynamically estimate the view-specific excess risk and adaptively adjust loss weights during optimization. Furthermore, we introduce geometry-aware and texture-aware objectives that complement the excess-risk-derived weighting mechanism, establishing a synergistic global-local optimization paradigm. Consequently, ERGO demonstrates robustness against supervision noise while consistently enhancing both geometric fidelity and textural quality of the reconstructed 3D content. Extensive experiments on the Google Scanned Objects dataset and the OmniObject3D dataset demonstrate the superiority of ERGO over existing state-of-the-art methods.</li>
<li><strong>摘要：</strong>由于遮挡区域固有地缺乏几何和纹理信息，从单个图像生成 3D 内容仍然是一个根本性的挑战和不适定问题。虽然最先进的生成模型可以合成辅助视图来提供额外的监督，但这些视图不可避免地包含几何不一致和纹理错位，这些不一致和纹理错位会在 3D 重建过程中传播和放大伪影。为了有效地利用这些不完善的监管信号，我们提出了一种以过度风险分解为指导的自适应优化框架，称为 ERGO。具体来说，ERGO 将 3D 高斯分布中的优化损失分解为两个部分，即量化当前参数和最优参数之间的次优差距的超额风险，以及对合成视图中固有的不可约噪声进行建模的贝叶斯误差。这种分解使 ERGO 能够动态估计视图特定的超额风险，并在优化过程中自适应调整损失权重。此外，我们引入了几何感知和纹理感知目标，补充了过度风险衍生的加权机制，建立了协同的全局局部优化范例。因此，ERGO 展示了针对监督噪声的鲁棒性，同时持续增强重建 3D 内容的几何保真度和纹理质量。对 Google Scanned Objects 数据集和 OmniObject3D 数据集的大量实验证明了 ERGO 相对于现有最先进方法的优越性。</li>
</ul>

<h3>Title: ICODEN: Ordinary Differential Equation Neural Networks for Interval-Censored Data</h3>
<ul>
<li><strong>Authors: </strong>Haoling Wang, Lang Zeng, Tao Sun, Youngjoo Cho, Ying Ding</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10303">https://arxiv.org/abs/2602.10303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10303">https://arxiv.org/pdf/2602.10303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10303]] ICODEN: Ordinary Differential Equation Neural Networks for Interval-Censored Data(https://arxiv.org/abs/2602.10303)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Predicting time-to-event outcomes when event times are interval censored is challenging because the exact event time is unobserved. Many existing survival analysis approaches for interval-censored data rely on strong model assumptions or cannot handle high-dimensional predictors. We develop ICODEN, an ordinary differential equation-based neural network for interval-censored data that models the hazard function through deep neural networks and obtains the cumulative hazard by solving an ordinary differential equation. ICODEN does not require the proportional hazards assumption or a prespecified parametric form for the hazard function, thereby permitting flexible survival modeling. Across simulation settings with proportional or non-proportional hazards and both linear and nonlinear covariate effects, ICODEN consistently achieves satisfactory predictive accuracy and remains stable as the number of predictors increases. Applications to data from multiple phases of the Alzheimer's Disease Neuroimaging Initiative (ADNI) and to two Age-Related Eye Disease Studies (AREDS and AREDS2) for age-related macular degeneration (AMD) demonstrate ICODEN's robust prediction performance. In both applications, predicting time-to-AD or time-to-late AMD, ICODEN effectively uses hundreds to more than 1,000 SNPs and supports data-driven subgroup identification with differential progression risk profiles. These results establish ICODEN as a practical assumption-lean tool for prediction with interval-censored survival data in high-dimensional biomedical settings.</li>
<li><strong>摘要：</strong>当事件时间被间隔删失时，预测事件时间结果具有挑战性，因为确切的事件时间是无法观察到的。许多现有的区间删失数据生存分析方法依赖于强大的模型假设，或者无法处理高维预测变量。我们开发了 ICODEN，一种基于常微分方程的神经网络，用于区间删失数据，通过深度神经网络对风险函数进行建模，并通过求解常微分方程获得累积风险。 ICODEN 不需要比例风险假设或风险函数的预先指定参数形式，从而允许灵活的生存建模。在具有比例或非比例风险以及线性和非线性协变量效应的模拟设置中，ICODEN 始终达到令人满意的预测精度，并随着预测变量数量的增加保持稳定。对阿尔茨海默病神经影像计划 (ADNI) 多个阶段的数据的应用以及针对年龄相关性黄斑变性 (AMD) 的两项与年龄相关的眼病研究（AREDS 和 AREDS2）的数据证明了 ICODEN 强大的预测性能。在这两种预测 AD 时间或 AMD 晚期时间的应用中，ICODEN 有效地使用数百到 1,000 多个 SNP，并支持具有差异进展风险概况的数据驱动的亚组识别。这些结果使 ICODEN 成为一种实用的假设精益工具，用于在高维生物医学环境中使用区间删失生存数据进行预测。</li>
</ul>

<h3>Title: R2RAG-Flood: A reasoning-reinforced training-free retrieval augmentation generation framework for flood damage nowcasting</h3>
<ul>
<li><strong>Authors: </strong>Lipai Huang, Kai Yin, Chia-Fu Liu, Ali Mostafavi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10312">https://arxiv.org/abs/2602.10312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10312">https://arxiv.org/pdf/2602.10312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10312]] R2RAG-Flood: A reasoning-reinforced training-free retrieval augmentation generation framework for flood damage nowcasting(https://arxiv.org/abs/2602.10312)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>R2RAG-Flood is a reasoning-reinforced, training-free retrieval-augmented generation framework for post-storm property damage nowcasting. Building on an existing supervised tabular predictor, the framework constructs a reasoning-centric knowledge base composed of labeled tabular records, where each sample includes structured predictors, a compact natural language text-mode summary, and a model-generated reasoning trajectory. During inference, R2RAG-Flood issues context-augmented prompts that retrieve and condition on relevant reasoning trajectories from nearby geospatial neighbors and canonical class prototypes, enabling the large language model backbone to emulate and adapt prior reasoning rather than learn new task-specific parameters. Predictions follow a two-stage procedure that first determines property damage occurrence and then refines severity within a three-level Property Damage Extent categorization, with a conditional downgrade step to correct over-predicted severity. In a case study of Harris County, Texas at the 12-digit Hydrologic Unit Code scale, the supervised tabular baseline trained directly on structured predictors achieves 0.714 overall accuracy and 0.859 damage class accuracy for medium and high damage classes. Across seven large language model backbones, R2RAG-Flood attains 0.613 to 0.668 overall accuracy and 0.757 to 0.896 damage class accuracy, approaching the supervised baseline while additionally producing a structured rationale for each prediction. Using a severity-per-cost efficiency metric derived from API pricing and GPU instance costs, lightweight R2RAG-Flood variants demonstrate substantially higher efficiency than both the supervised tabular baseline and larger language models, while requiring no task-specific training or fine-tuning.</li>
<li><strong>摘要：</strong>R2RAG-Flood 是一种推理强化、免训练检索增强生成框架，用于风暴后财产损失临近预报。该框架以现有的监督表格预测器为基础，构建了一个由标记表格记录组成的以推理为中心的知识库，其中每个样本都包含结构化预测器、紧凑的自然语言文本模式摘要和模型生成的推理轨迹。在推理过程中，R2RAG-Flood 发出上下文增强提示，从附近的地理空间邻居和规范类原型检索相关推理轨迹并进行条件调节，使大型语言模型骨干能够模拟和适应先前的推理，而不是学习新的特定于任务的参数。预测遵循两阶段程序，首先确定财产损失的发生情况，然后在三级财产损失范围分类中细化严重性，并通过有条件的降级步骤来纠正过高预测的严重性。在德克萨斯州哈里斯县的 12 位水文单位代码规模的案例研究中，直接在结构化预测器上训练的监督表格基线实现了 0.714 的总体准确度和 0.859 的中等和高损害类别的损害类别准确度。在七个大型语言模型主干中，R2RAG-Flood 获得了 0.613 至 0.668 的整体准确度和 0.757 至 0.896 的损害类别准确度，接近监督基线，同时还为每个预测生成了结构化的基本原理。使用从 API 定价和 GPU 实例成本得出的按成本严重性效率指标，轻量级 R2RAG-Flood 变体表现出比受监督表格基线和更大语言模型高得多的效率，同时不需要特定于任务的训练或微调。</li>
</ul>

<h3>Title: Stop Training for the Worst: Progressive Unmasking Accelerates Masked Diffusion Training</h3>
<ul>
<li><strong>Authors: </strong>Jaeyeon Kim, Jonathan Geuter, David Alvarez-Melis, Sham Kakade, Sitan Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10314">https://arxiv.org/abs/2602.10314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10314">https://arxiv.org/pdf/2602.10314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10314]] Stop Training for the Worst: Progressive Unmasking Accelerates Masked Diffusion Training(https://arxiv.org/abs/2602.10314)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Masked Diffusion Models (MDMs) have emerged as a promising approach for generative modeling in discrete spaces. By generating sequences in any order and allowing for parallel decoding, they enable fast inference and strong performance on non-causal tasks. However, this flexibility comes with a training complexity trade-off: MDMs train on an exponentially large set of masking patterns, which is not only computationally expensive, but also creates a train--test mismatch between the random masks used in training and the highly structured masks induced by inference-time unmasking. In this work, we propose Progressive UnMAsking (PUMA), a simple modification of the forward masking process that aligns training-time and inference-time masking patterns, thereby focusing optimization on inference-aligned masks and speeding up training. Empirically, PUMA speeds up pretraining at the 125M scale by $\approx 2.5\times$ and offers complementary advantages on top of common recipes like autoregressive initialization. We open-source our codebase at this https URL.</li>
<li><strong>摘要：</strong>掩蔽扩散模型 (MDM) 已成为离散空间生成建模的一种有前景的方法。通过以任意顺序生成序列并允许并行解码，它们可以在非因果任务上实现快速推理和强大性能。然而，这种灵活性伴随着训练复杂性的权衡：MDM 在指数级大的掩码模式集上进行训练，这不仅计算成本高昂，而且还会在训练中使用的随机掩码与推理时间去掩码引起的高度结构化掩码之间产生训练-测试不匹配。在这项工作中，我们提出了渐进式取消屏蔽（PUMA），这是对前向屏蔽过程的简单修改，可以对齐训练时间和推理时间屏蔽模式，从而重点优化推理对齐掩模并加快训练速度。根据经验，PUMA 将 125M 规模的预训练速度提高了 $\大约 2.5\times$，并在自回归初始化等常见方法之上提供了互补的优势。我们在此 https URL 开源我们的代码库。</li>
</ul>

<h3>Title: Flow Matching with Uncertainty Quantification and Guidance</h3>
<ul>
<li><strong>Authors: </strong>Juyeop Han, Lukas Lao Beyer, Sertac Karaman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10326">https://arxiv.org/abs/2602.10326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10326">https://arxiv.org/pdf/2602.10326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10326]] Flow Matching with Uncertainty Quantification and Guidance(https://arxiv.org/abs/2602.10326)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Despite the remarkable success of sampling-based generative models such as flow matching, they can still produce samples of inconsistent or degraded quality. To assess sample reliability and generate higher-quality outputs, we propose uncertainty-aware flow matching (UA-Flow), a lightweight extension of flow matching that predicts the velocity field together with heteroscedastic uncertainty. UA-Flow estimates per-sample uncertainty by propagating velocity uncertainty through the flow dynamics. These uncertainty estimates act as a reliability signal for individual samples, and we further use them to steer generation via uncertainty-aware classifier guidance and classifier-free guidance. Experiments on image generation show that UA-Flow produces uncertainty signals more highly correlated with sample fidelity than baseline methods, and that uncertainty-guided sampling further improves generation quality.</li>
<li><strong>摘要：</strong>尽管基于采样的生成模型（例如流量匹配）取得了巨大成功，但它们仍然会产生质量不一致或质量下降的样本。为了评估样本可靠性并生成更高质量的输出，我们提出了不确定性感知流匹配（UA-Flow），这是流匹配的轻量级扩展，可以预测速度场和异方差不确定性。 UA-Flow 通过流动动力学传播速度不确定性来估计每个样本的不确定性。这些不确定性估计充当单个样本的可靠性信号，我们进一步使用它们通过不确定性感知分类器指导和无分类器指导来指导生成。图像生成实验表明，与基线方法相比，UA-Flow 产生的不确定性信号与样本保真度更加相关，并且不确定性引导采样进一步提高了生成质量。</li>
</ul>

<h3>Title: Conditional Uncertainty-Aware Political Deepfake Detection with Stochastic Convolutional Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Rafael-Petruţ Gardoş</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10343">https://arxiv.org/abs/2602.10343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10343">https://arxiv.org/pdf/2602.10343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10343]] Conditional Uncertainty-Aware Political Deepfake Detection with Stochastic Convolutional Neural Networks(https://arxiv.org/abs/2602.10343)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative image models have enabled the creation of highly realistic political deepfakes, posing risks to information integrity, public trust, and democratic processes. While automated deepfake detectors are increasingly deployed in moderation and investigative pipelines, most existing systems provide only point predictions and fail to indicate when outputs are unreliable, being an operationally critical limitation in high-stakes political contexts. This work investigates conditional, uncertainty-aware political deepfake detection using stochastic convolutional neural networks within an empirical, decision-oriented reliability framework. Rather than treating uncertainty as a purely Bayesian construct, it is evaluated through observable criteria, including calibration quality, proper scoring rules, and its alignment with prediction errors under both global and confidence-conditioned analyses. A politically focused binary image dataset is constructed via deterministic metadata filtering from a large public real-synthetic corpus. Two pretrained CNN backbones (ResNet-18 and EfficientNet-B4) are fully fine-tuned for classification. Deterministic inference is compared with single-pass stochastic prediction, Monte Carlo dropout with multiple forward passes, temperature scaling, and ensemble-based uncertainty surrogates. Evaluation reports ROC-AUC, thresholded confusion matrices, calibration metrics, and generator-disjoint out-of-distribution performance. Results demonstrate that calibrated probabilistic outputs and uncertainty estimates enable risk-aware moderation policies. A systematic confidence-band analysis further clarifies when uncertainty provides operational value beyond predicted confidence, delineating both the benefits and limitations of uncertainty-aware deepfake detection in political settings.</li>
<li><strong>摘要：</strong>生成图像模型的最新进展使得高度逼真的政治深度伪造成为可能，这给信息完整性、公众信任和民主进程带来了风险。虽然自动深度造假检测器越来越多地部署在审核和调查管道中，但大多数现有系统仅提供点预测，无法指示输出何时不可靠，这在高风险的政治背景下成为操作上的关键限制。这项工作研究了在经验性、面向决策的可靠性框架内使用随机卷积神经网络进行有条件的、不确定性感知的政治深度伪造检测。它不是将不确定性视为纯粹的贝叶斯构造，而是通过可观察的标准进行评估，包括校准质量、适当的评分规则及其与全局分析和置信条件分析下的预测误差的一致性。以政治为中心的二进制图像数据集是通过从大型公共真实合成语料库中进行确定性元数据过滤而构建的。两个预训练的 CNN 主干网络（ResNet-18 和 EfficientNet-B4）针对分类进行了全面微调。将确定性推理与单遍随机预测、多次前向传递的蒙特卡罗 dropout、温度缩放和基于集合的不确定性替代项进行比较。评估报告 ROC-AUC、阈值混淆矩阵、校准指标和生成器不相交的分布外性能。结果表明，经过校准的概率输出和不确定性估计可以实现风险意识调节政策。系统的置信带分析进一步阐明了不确定性何时提供超出预测置信度的操作价值，描绘了政治环境中不确定性感知深度伪造检测的好处和局限性。</li>
</ul>

<h3>Title: LightGTS-Cov: Covariate-Enhanced Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Yong Shang, Zhipeng Yao, Ning Jin, Xiangfei Qiu, Hui Zhang, Bin Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10412">https://arxiv.org/abs/2602.10412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10412">https://arxiv.org/pdf/2602.10412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10412]] LightGTS-Cov: Covariate-Enhanced Time Series Forecasting(https://arxiv.org/abs/2602.10412)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Time series foundation models are typically pre-trained on large, multi-source datasets; however, they often ignore exogenous covariates or incorporate them via simple concatenation with the target series, which limits their effectiveness in covariate-rich applications such as electricity price forecasting and renewable energy forecasting. We introduce LightGTS-Cov, a covariate-enhanced extension of LightGTS that preserves its lightweight, period-aware backbone while explicitly incorporating both past and future-known covariates. Built on a $\sim$1M-parameter LightGTS backbone, LightGTS-Cov adds only a $\sim$0.1M-parameter MLP plug-in that integrates time-aligned covariates into the target forecasts by residually refining the outputs of the decoding process. Across covariate-aware benchmarks on electricity price and energy generation datasets, LightGTS-Cov consistently outperforms LightGTS and achieves superior performance over other covariate-aware baselines under both settings, regardless of whether future-known covariates are provided. We further demonstrate its practical value in two real-world energy case applications: long-term photovoltaic power forecasting with future weather forecasts and day-ahead electricity price forecasting with weather and dispatch-plan covariates. Across both applications, LightGTS-Cov achieves strong forecasting accuracy and stable operational performance after deployment, validating its effectiveness in real-world industrial settings.</li>
<li><strong>摘要：</strong>时间序列基础模型通常在大型多源数据集上进行预训练；然而，它们经常忽略外生协变量或通过与目标序列的简单串联将其合并，这限制了它们在电价预测和可再生能源预测等协变量丰富的应用中的有效性。我们引入了 LightGTS-Cov，这是 LightGTS 的协变量增强扩展，它保留了其轻量级、周期感知的主干，同时明确地合并了过去和未来已知的协变量。 LightGTS-Cov 基于 $\sim$1M 参数的 LightGTS 主干网络构建，仅添加了 $\sim$0.1M 参数的 MLP 插件，该插件通过对解码过程的输出进行残余细化，将时间对齐的协变量集成到目标预测中。在电价和能源发电数据集的协变量感知基准中，无论是否提供未来已知的协变量，LightGTS-Cov 始终优于 LightGTS，并在两种设置下实现优于其他协变量感知基线的性能。我们进一步证明了其在两个现实世界能源案例应用中的实用价值：结合未来天气预报的长期光伏发电预测以及结合天气和调度计划协变量的日前电价预测。在这两种应用中，LightGTS-Cov 在部署后都实现了强大的预测准确性和稳定的运行性能，验证了其在现实工业环境中的有效性。</li>
</ul>

<h3>Title: Binary Flow Matching: Prediction-Loss Space Alignment for Robust Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiadong Hong, Lei Liu, Xinyu Bian, Wenjie Wang, Zhaoyang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, eess.IV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10420">https://arxiv.org/abs/2602.10420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10420">https://arxiv.org/pdf/2602.10420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10420]] Binary Flow Matching: Prediction-Loss Space Alignment for Robust Learning(https://arxiv.org/abs/2602.10420)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Flow matching has emerged as a powerful framework for generative modeling, with recent empirical successes highlighting the effectiveness of signal-space prediction ($x$-prediction). In this work, we investigate the transfer of this paradigm to binary manifolds, a fundamental setting for generative modeling of discrete data. While $x$-prediction remains effective, we identify a latent structural mismatch that arises when it is coupled with velocity-based objectives ($v$-loss), leading to a time-dependent singular weighting that amplifies gradient sensitivity to approximation errors. Motivated by this observation, we formalize prediction-loss alignment as a necessary condition for flow matching training. We prove that re-aligning the objective to the signal space ($x$-loss) eliminates the singular weighting, yielding uniformly bounded gradients and enabling robust training under uniform timestep sampling without reliance on heuristic schedules. Finally, with alignment secured, we examine design choices specific to binary data, revealing a topology-dependent distinction between probabilistic objectives (e.g., cross-entropy) and geometric losses (e.g., mean squared error). Together, these results provide theoretical foundations and practical guidelines for robust flow matching on binary -- and related discrete -- domains, positioning signal-space alignment as a key principle for robust diffusion learning.</li>
<li><strong>摘要：</strong>流匹配已成为生成建模的强大框架，最近的经验成功凸显了信号空间预测（$x$-预测）的有效性。在这项工作中，我们研究了这种范式到二元流形的转移，这是离散数据生成建模的基本设置。虽然$x$-预测仍然有效，但我们发现当它与基于速度的目标（$v$-loss）结合时会出现潜在的结构失配，从而导致依赖于时间的奇异权重，从而放大了对近似误差的梯度敏感性。受这一观察的启发，我们将预测损失对齐形式化为流匹配训练的必要条件。我们证明，将目标重新与信号空间（$x$-loss）对齐可以消除奇异权重，产生均匀有界的梯度，并在均匀时间步采样下实现稳健的训练，而无需依赖启发式计划。最后，在确保对齐的情况下，我们检查特定于二进制数据的设计选择，揭示概率目标（例如交叉熵）和几何损失（例如均方误差）之间依赖于拓扑的区别。总之，这些结果为二进制域和相关离散域上的鲁棒流匹配提供了理论基础和实践指南，将信号空间对齐定位为鲁棒扩散学习的关键原则。</li>
</ul>

<h3>Title: Breaking the Curse of Repulsion: Optimistic Distributionally Robust Policy Optimization for Off-Policy Generative Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Jie Jiang, Yusen Huo, Xiangxin Zhan, Changping Wang, Jun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10430">https://arxiv.org/abs/2602.10430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10430">https://arxiv.org/pdf/2602.10430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10430]] Breaking the Curse of Repulsion: Optimistic Distributionally Robust Policy Optimization for Off-Policy Generative Recommendation(https://arxiv.org/abs/2602.10430)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Policy-based Reinforcement Learning (RL) has established itself as the dominant paradigm in generative recommendation for optimizing sequential user interactions. However, when applied to offline historical logs, these methods suffer a critical failure: the dominance of low-quality data induces severe model collapse. We first establish the Divergence Theory of Repulsive Optimization, revealing that negative gradient updates inherently trigger exponential intensity explosion during off-policy training. This theory elucidates the inherent dilemma of existing methods, exposing their inability to reconcile variance reduction and noise imitation. To break this curse, we argue that the solution lies in rigorously identifying the latent high-quality distribution entangled within the noisy behavior policy. Accordingly, we reformulate the objective as an Optimistic Distributionally Robust Optimization (DRO) problem. Guided by this formulation, we propose Distributionally Robust Policy Optimization (DRPO). We prove that hard filtering is the exact solution to this DRO objective, enabling DRPO to optimally recover high-quality behaviors while strictly discarding divergence-inducing noise. Extensive experiments demonstrate that DRPO achieves state-of-the-art performance on mixed-quality recommendation benchmarks.</li>
<li><strong>摘要：</strong>基于策略的强化学习 (RL) 已成为优化顺序用户交互的生成推荐的主导范例。然而，当应用于离线历史日志时，这些方法会遭遇严重失败：低质量数据的主导导致严重的模型崩溃。我们首先建立了排斥优化的发散理论，揭示了负梯度更新本质上会在离策略训练期间触发指数强度爆炸。该理论阐明了现有方法的固有困境，暴露了它们无法协调方差减少和噪声模拟。为了打破这个诅咒，我们认为解决方案在于严格识别与噪声行为策略纠缠在一起的潜在高质量分布。因此，我们将目标重新表述为乐观分布鲁棒优化（DRO）问题。在此公式的指导下，我们提出了分布式鲁棒策略优化（DRPO）。我们证明硬过滤是实现该 DRO 目标的精确解决方案，使 DRPO 能够以最佳方式恢复高质量行为，同时严格丢弃引起发散的噪声。大量实验表明，DRPO 在混合质量推荐基准上实现了最先进的性能。</li>
</ul>

<h3>Title: A Multimodal Conditional Mixture Model with Distribution-Level Physics Priors</h3>
<ul>
<li><strong>Authors: </strong>Jinkyo Han, Bahador Bahmani</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10451">https://arxiv.org/abs/2602.10451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10451">https://arxiv.org/pdf/2602.10451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10451]] A Multimodal Conditional Mixture Model with Distribution-Level Physics Priors(https://arxiv.org/abs/2602.10451)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Many scientific and engineering systems exhibit intrinsically multimodal behavior arising from latent regime switching and non-unique physical mechanisms. In such settings, learning the full conditional distribution of admissible outcomes in a physically consistent and interpretable manner remains a challenge. While recent advances in machine learning have enabled powerful multimodal generative modeling, their integration with physics-constrained scientific modeling remains nontrivial, particularly when physical structure must be preserved or data are limited. This work develops a physics-informed multimodal conditional modeling framework based on mixture density representations. Mixture density networks (MDNs) provide an explicit and interpretable parameterization of multimodal conditional distributions. Physical knowledge is embedded through component-specific regularization terms that penalize violations of governing equations or physical laws. This formulation naturally accommodates non-uniqueness and stochasticity while remaining computationally efficient and amenable to conditioning on contextual inputs. The proposed framework is evaluated across a range of scientific problems in which multimodality arises from intrinsic physical mechanisms rather than observational noise, including bifurcation phenomena in nonlinear dynamical systems, stochastic partial differential equations, and atomistic-scale shock dynamics. In addition, the proposed method is compared with a conditional flow matching (CFM) model, a representative state-of-the-art generative modeling approach, demonstrating that MDNs can achieve competitive performance while offering a simpler and more interpretable formulation.</li>
<li><strong>摘要：</strong>许多科学和工程系统本质上表现出由潜在状态切换和非独特物理机制引起的多模态行为。在这种情况下，以物理上一致且可解释的方式学习可接受结果的完整条件分布仍然是一个挑战。虽然机器学习的最新进展已经实现了强大的多模态生成建模，但它们与物理约束的科学建模的集成仍然很重要，特别是当必须保留物理结构或数据有限时。这项工作开发了一种基于混合密度表示的物理信息多模态条件建模框架。混合密度网络 (MDN) 提供了多模态条件分布的显式且可解释的参数化。物理知识通过特定于组件的正则化项嵌入，这些正则化项对违反控制方程或物理定律的行为进行惩罚。这种公式自然地适应了非唯一性和随机性，同时保持计算效率并易于适应上下文输入。所提出的框架针对一系列科学问题进行了评估，其中多模态源于内在的物理机制而不是观测噪声，包括非线性动力系统中的分岔现象、随机偏微分方程和原子尺度的冲击动力学。此外，将所提出的方法与条件流匹配（CFM）模型（一种代表性的最先进的生成建模方法）进行了比较，证明 MDN 可以实现有竞争力的性能，同时提供更简单和更可解释的公式。</li>
</ul>

<h3>Title: Analyzing Fairness of Neural Network Prediction via Counterfactual Dataset Generation</h3>
<ul>
<li><strong>Authors: </strong>Brian Hyeongseok Kim, Jacqueline L. Mitchell, Chao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10457">https://arxiv.org/abs/2602.10457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10457">https://arxiv.org/pdf/2602.10457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10457]] Analyzing Fairness of Neural Network Prediction via Counterfactual Dataset Generation(https://arxiv.org/abs/2602.10457)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Interpreting the inference-time behavior of deep neural networks remains a challenging problem. Existing approaches to counterfactual explanation typically ask: What is the closest alternative input that would alter the model's prediction in a desired way? In contrast, we explore counterfactual datasets. Rather than perturbing the input, our method efficiently finds the closest alternative training dataset, one that differs from the original dataset by changing a few labels. Training a new model on this altered dataset can then lead to a different prediction of a given test instance. This perspective provides a new way to assess fairness by directly analyzing the influence of label bias on training and inference. Our approach can be characterized as probing whether a given prediction depends on biased labels. Since exhaustively enumerating all possible alternate datasets is infeasible, we develop analysis techniques that trace how bias in the training data may propagate through the learning algorithm to the trained network. Our method heuristically ranks and modifies the labels of a bounded number of training examples to construct a counterfactual dataset, retrains the model, and checks whether its prediction on a chosen test case changes. We evaluate our approach on feedforward neural networks across over 1100 test cases from 7 widely-used fairness datasets. Results show that it modifies only a small subset of training labels, highlighting its ability to pinpoint the critical training examples that drive prediction changes. Finally, we demonstrate how our counterfactual datasets reveal connections between training examples and test cases, offering an interpretable way to probe dataset bias.</li>
<li><strong>摘要：</strong>解释深度神经网络的推理时间行为仍然是一个具有挑战性的问题。现有的反事实解释方法通常会问：能够以所需方式改变模型预测的最接近的替代输入是什么？相反，我们探索反事实数据集。我们的方法不是扰乱输入，而是有效地找到最接近的替代训练数据集，该数据集通过更改一些标签而与原始数据集不同。在这个改变的数据集上训练新模型可以导致对给定测试实例的不同预测。这一视角通过直接分析标签偏差对训练和推理的影响，提供了一种评估公平性的新方法。我们的方法可以被描述为探测给定的预测是否依赖于有偏见的标签。由于详尽地枚举所有可能的替代数据集是不可行的，因此我们开发了分析技术来跟踪训练数据中的偏差如何通过学习算法传播到经过训练的网络。我们的方法启发式地对有限数量的训练示例的标签进行排名和修改，以构建反事实数据集，重新训练模型，并检查其对所选测试用例的预测是否发生变化。我们通过来自 7 个广泛使用的公平性数据集的 1100 多个测试用例来评估我们的前馈神经网络方法。结果表明，它只修改了训练标签的一小部分，突显了它精确定位驱动预测变化的关键训练示例的能力。最后，我们演示了反事实数据集如何揭示训练示例和测试用例之间的联系，提供一种可解释的方法来探测数据集偏差。</li>
</ul>

<h3>Title: Driving Reaction Trajectories via Latent Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Yili Shen, Xiangliang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10476">https://arxiv.org/abs/2602.10476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10476">https://arxiv.org/pdf/2602.10476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10476]] Driving Reaction Trajectories via Latent Flow Matching(https://arxiv.org/abs/2602.10476)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in reaction prediction have achieved near-saturated accuracy on standard benchmarks (e.g., USPTO), yet most state-of-the-art models formulate the task as a one-shot mapping from reactants to products, offering limited insight into the underlying reaction process. Procedural alternatives introduce stepwise generation but often rely on mechanism-specific supervision, discrete symbolic edits, and computationally expensive inference. In this work, we propose LatentRxnFlow, a new reaction prediction paradigm that models reactions as continuous latent trajectories anchored at the thermodynamic product state. Built on Conditional Flow Matching, our approach learns time-dependent latent dynamics directly from standard reactant-product pairs, without requiring mechanistic annotations or curated intermediate labels. While LatentRxnFlow achieves state-of-the-art performance on USPTO benchmarks, more importantly, the continuous formulation exposes the full generative trajectory, enabling trajectory-level diagnostics that are difficult to realize with discrete or one-shot models. We show that latent trajectory analysis allows us to localize and characterize failure modes and to mitigate certain errors via gated inference. Furthermore, geometric properties of the learned trajectories provide an intrinsic signal of epistemic uncertainty, helping prioritize reliably predictable reaction outcomes and flag ambiguous cases for additional validation. Overall, LatentRxnFlow combines strong predictive accuracy with improved transparency, diagnosability, and uncertainty awareness, moving reaction prediction toward more trustworthy deployment in high-throughput discovery workflows.</li>
<li><strong>摘要：</strong>反应预测的最新进展已在标准基准（例如美国专利商标局）上实现了接近饱和的准确性，但大多数最先进的模型将任务制定为从反应物到产物的一次性映射，对潜在反应过程的了解有限。程序替代方案引入了逐步生成，但通常依赖于特定于机制的监督、离散符号编辑和计算成本高昂的推理。在这项工作中，我们提出了 LatentRxnFlow，一种新的反应预测范式，它将反应建模为锚定于热力学产物状态的连续潜在轨迹。我们的方法基于条件流匹配，直接从标准反应物-产物对学习时间相关的潜在动态，无需机械注释或策划的中间标签。虽然 LatentRxnFlow 在 USPTO 基准上实现了最先进的性能，但更重要的是，连续公式暴露了完整的生成轨迹，从而实现了离散或一次性模型难以实现的轨迹级诊断。我们表明，潜在轨迹分析使我们能够定位和表征故障模式，并通过门控推理来减轻某些错误。此外，学习轨迹的几何特性提供了认知不确定性的内在信号，有助于优先考虑可靠预测的反应结果，并标记不明确的情况以进行额外验证。总体而言，LatentRxnFlow 将强大的预测准确性与改进的透明度、可诊断性和不确定性意识相结合，将反应预测推向高通量发现工作流程中更值得信赖的部署。</li>
</ul>

<h3>Title: Learning Structure-Semantic Evolution Trajectories for Graph Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Wei Chen, Xingyu Guo, Shuang Li, Yan Zhong, Zhao Zhang, Fuzhen Zhuang, Hongrui Liu, Libang Zhang, Guo Ye, Huimei He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10506">https://arxiv.org/abs/2602.10506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10506">https://arxiv.org/pdf/2602.10506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10506]] Learning Structure-Semantic Evolution Trajectories for Graph Domain Adaptation(https://arxiv.org/abs/2602.10506)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Graph Domain Adaptation (GDA) aims to bridge distribution shifts between domains by transferring knowledge from well-labeled source graphs to given unlabeled target graphs. One promising recent approach addresses graph transfer by discretizing the adaptation process, typically through the construction of intermediate graphs or stepwise alignment procedures. However, such discrete strategies often fail in real-world scenarios, where graph structures evolve continuously and nonlinearly, making it difficult for fixed-step alignment to approximate the actual transformation process. To address these limitations, we propose \textbf{DiffGDA}, a \textbf{Diff}usion-based \textbf{GDA} method that models the domain adaptation process as a continuous-time generative process. We formulate the evolution from source to target graphs using stochastic differential equations (SDEs), enabling the joint modeling of structural and semantic transitions. To guide this evolution, a domain-aware network is introduced to steer the generative process toward the target domain, encouraging the diffusion trajectory to follow an optimal adaptation path. We theoretically show that the diffusion process converges to the optimal solution bridging the source and target domains in the latent space. Extensive experiments on 14 graph transfer tasks across 8 real-world datasets demonstrate DiffGDA consistently outperforms state-of-the-art baselines.</li>
<li><strong>摘要：</strong>图域适应（GDA）旨在通过将知识从标记良好的源图转移到给定的未标记目标图来桥接域之间的分布变化。最近一种有前途的方法通过离散化适应过程来解决图转移问题，通常是通过构建中间图或逐步对齐过程。然而，这种离散策略在现实场景中经常会失败，因为图结构连续且非线性地演化，使得固定步长对齐很难逼近实际的变换过程。为了解决这些限制，我们提出了 \textbf{DiffGDA}，这是一种基于 \textbf{Diff}usion 的 \textbf{GDA} 方法，它将域适应过程建模为连续时间生成过程。我们使用随机微分方程（SDE）制定从源图到目标图的演化，从而实现结构和语义转换的联合建模。为了指导这种演变，引入了领域感知网络来引导生成过程朝向目标领域，鼓励扩散轨迹遵循最佳适应路径。我们从理论上证明，扩散过程收敛到在潜在空间中桥接源域和目标域的最佳解决方案。对 8 个真实数据集的 14 个图传输任务进行的广泛实验表明，DiffGDA 始终优于最先进的基线。</li>
</ul>

<h3>Title: 3DXTalker: Unifying Identity, Lip Sync, Emotion, and Spatial Dynamics in Expressive 3D Talking Avatars</h3>
<ul>
<li><strong>Authors: </strong>Zhongju Wang, Zhenhong Sun, Beier Wang, Yifu Wang, Daoyi Dong, Huadong Mo, Hongdong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10516">https://arxiv.org/abs/2602.10516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10516">https://arxiv.org/pdf/2602.10516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10516]] 3DXTalker: Unifying Identity, Lip Sync, Emotion, and Spatial Dynamics in Expressive 3D Talking Avatars(https://arxiv.org/abs/2602.10516)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Audio-driven 3D talking avatar generation is increasingly important in virtual communication, digital humans, and interactive media, where avatars must preserve identity, synchronize lip motion with speech, express emotion, and exhibit lifelike spatial dynamics, collectively defining a broader objective of expressivity. However, achieving this remains challenging due to insufficient training data with limited subject identities, narrow audio representations, and restricted explicit controllability. In this paper, we propose 3DXTalker, an expressive 3D talking avatar through data-curated identity modeling, audio-rich representations, and spatial dynamics controllability. 3DXTalker enables scalable identity modeling via 2D-to-3D data curation pipeline and disentangled representations, alleviating data scarcity and improving identity generalization. Then, we introduce frame-wise amplitude and emotional cues beyond standard speech embeddings, ensuring superior lip synchronization and nuanced expression modulation. These cues are unified by a flow-matching-based transformer for coherent facial dynamics. Moreover, 3DXTalker also enables natural head-pose motion generation while supporting stylized control via prompt-based conditioning. Extensive experiments show that 3DXTalker integrates lip synchronization, emotional expression, and head-pose dynamics within a unified framework, achieves superior performance in 3D talking avatar generation.</li>
<li><strong>摘要：</strong>音频驱动的 3D 说话化身生成在虚拟通信、数字人类和交互式媒体中变得越来越重要，其中化身必须保持身份、使嘴唇运动与语音同步、表达情感并展现逼真的空间动态，共同定义更广泛的表现力目标。然而，由于训练数据不足、受试者身份有限、音频表示狭窄以及显式可控性有限，实现这一目标仍然具有挑战性。在本文中，我们提出了 3DXTalker，这是一种通过数据管理的身份建模、丰富的音频表示和空间动态可控性实现的富有表现力的 3D 说话化身。 3DXTalker 通过 2D 到 3D 数据管理管道和解开的表示实现可扩展的身份建模，从而缓解数据稀缺并提高身份泛化。然后，我们引入了标准语音嵌入之外的逐帧幅度和情感线索，确保卓越的唇形同步和细致入微的表达调制。这些线索通过基于流匹配的转换器进行统一，以实现连贯的面部动态。此外，3DXTalker 还可以生成自然的头部姿势运动，同时通过基于提示的调节支持风格化控制。大量实验表明，3DXTalker将唇形同步、情绪表达和头部姿势动态集成在一个统一的框架内，在3D说话头像生成方面取得了优异的性能。</li>
</ul>

<h3>Title: Prioritize the Process, Not Just the Outcome: Rewarding Latent Thought Trajectories Improves Reasoning in Looped Language Models</h3>
<ul>
<li><strong>Authors: </strong>Williams Jonathan, Tureci Esin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10520">https://arxiv.org/abs/2602.10520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10520">https://arxiv.org/pdf/2602.10520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10520]] Prioritize the Process, Not Just the Outcome: Rewarding Latent Thought Trajectories Improves Reasoning in Looped Language Models(https://arxiv.org/abs/2602.10520)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Looped Language Models (LoopLMs) perform multi-step latent reasoning prior to token generation and outperform conventional LLMs on reasoning benchmarks at smaller parameter budgets. However, attempts to further improve LoopLM reasoning with reinforcement learning have failed - standard objectives such as Group Relative Policy Optimization (GRPO) only assign credit to the final latent state, creating a fundamental mismatch with the model's internal computation. To resolve this, we introduce RLTT (Reward Latent Thought Trajectories), a reinforcement learning framework which distributes reward across the full latent reasoning trajectory. RLTT provides dense, trajectory-level credit assignment without relying on external verifiers and can directly replace GRPO with negligible overhead. Across extensive experiments with Ouro-2.6B-Thinking under identical training and inference conditions, RLTT yields substantial improvements over GRPO on challenging mathematical reasoning benchmarks, improving accuracy by +14.4% on MATH-500, +16.6% on AIME24, and +10.0% on BeyondAIME. Despite being trained exclusively on mathematics, RLTT also transfers effectively to non-mathematical reasoning benchmarks, demonstrating the effectiveness of trajectory-level credit assignment for reinforcement learning in LoopLMs.</li>
<li><strong>摘要：</strong>循环语言模型 (LoopLM) 在令牌生成之前执行多步潜在推理，并且在较小参数预算的推理基准上优于传统的 LLM。然而，通过强化学习进一步改进 LoopLM 推理的尝试已经失败——组相对策略优化 (GRPO) 等标准目标仅将功劳分配给最终的潜在状态，从而与模型的内部计算产生根本性的不匹配。为了解决这个问题，我们引入了 RLTT（奖励潜在思维轨迹），这是一种强化学习框架，可以在整个潜在推理轨迹上分配奖励。 RLTT 提供密集的轨迹级信用分配，无需依赖外部验证者，并且可以直接替换 GRPO，开销可以忽略不计。在相同的训练和推理条件下使用 Ouro-2.6B-Thinking 进行的大量实验中，RLTT 在具有挑战性的数学推理基准上比 GRPO 有了显着的改进，在 MATH-500 上提高了 14.4% 的准确度，在 AIME24 上提高了 16.6%，在 BeyondAIME 上提高了 10.0%。尽管仅接受数学训练，RLTT 也可以有效地转移到非数学推理基准，证明了 LoopLM 中强化学习的轨迹级学分分配的有效性。</li>
</ul>

<h3>Title: Bridging the Compression-Precision Paradox: A Hybrid Architecture for Clinical EEG Report Generation with Guaranteed Measurement Accuracy</h3>
<ul>
<li><strong>Authors: </strong>Wuyang Zhang, Zhen Luo, Chuqiao Gu, Jianming Ma, Yebo Cao, Wangming Yuan, Yinzhi Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10544">https://arxiv.org/abs/2602.10544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10544">https://arxiv.org/pdf/2602.10544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10544]] Bridging the Compression-Precision Paradox: A Hybrid Architecture for Clinical EEG Report Generation with Guaranteed Measurement Accuracy(https://arxiv.org/abs/2602.10544)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Automated EEG monitoring requires clinician-level precision for seizure detection and reporting. Clinical EEG recordings exceed LLM context windows, requiring extreme compression (400:1+ ratios) that destroys fine-grained temporal precision. A 0.5 Hz error distinguishes absence epilepsy from Lennox-Gastaut syndrome. LLMs lack inherent time-series comprehension and rely on statistical associations from compressed representations. This dual limitation causes systems to hallucinate clinically incorrect measurement values. We separate measurement extraction from text generation. Our hybrid architecture computes exact clinical values via signal processing before compression, employs a cross-modal bridge for EEG-to-language translation, and uses parameter-efficient fine-tuning with constrained decoding around frozen slots. Multirate sampling maintains long-range context while preserving event-level precision. Evaluation on TUH and CHB-MIT datasets achieves 60% fewer false alarms, 50% faster detection, and sub-clinical measurement precision. This is the first system guaranteeing clinical measurement accuracy in automated EEG reports.</li>
<li><strong>摘要：</strong>自动脑电图监测需要临床医生级别的癫痫检测和报告精度。临床脑电图记录超过了 LLM 上下文窗口，需要极端压缩（400:1+ 比率），这会破坏细粒度的时间精度。 0.5 Hz 的误差可区分失神性癫痫和 Lennox-Gastaut 综合征。法学硕士缺乏固有的时间序列理解，依赖于压缩表示的统计关联。这种双重限制导致系统产生临床上不正确的测量值的幻觉。我们将测量提取与文本生成分开。我们的混合架构在压缩前通过信号处理计算精确的临床值，采用跨模式桥进行脑电图到语言的翻译，并使用参数高效的微调和围绕冻结槽的受限解码。多速率采样可维护远程上下文，同时保留事件级精度。对 TUH 和 CHB-MIT 数据集的评估实现了误报减少 60%、检测速度提高 50% 以及亚临床测量精度。这是第一个保证自动脑电图报告中临床测量准确性的系统。</li>
</ul>

<h3>Title: RealHD: A High-Quality Dataset for Robust Detection of State-of-the-Art AI-Generated Images</h3>
<ul>
<li><strong>Authors: </strong>Hanzhe Yu, Yun Ye, Jintao Rong, Qi Xuan, Chen Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10546">https://arxiv.org/abs/2602.10546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10546">https://arxiv.org/pdf/2602.10546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10546]] RealHD: A High-Quality Dataset for Robust Detection of State-of-the-Art AI-Generated Images(https://arxiv.org/abs/2602.10546)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of generative AI has raised concerns about the authenticity of digital images, as highly realistic fake images can now be generated at low cost, potentially increasing societal risks. In response, several datasets have been established to train detection models aimed at distinguishing AI-generated images from real ones. However, existing datasets suffer from limited generalization, low image quality, overly simple prompts, and insufficient image diversity. To address these limitations, we propose a high-quality, large-scale dataset comprising over 730,000 images across multiple categories, including both real and AI-generated images. The generated images are synthesized via state-of-the-art methods, including text-to-image generation (guided by over 10,000 carefully designed prompts), image inpainting, image refinement, and face swapping. Each generated image is annotated with its generation method and category. Inpainting images further include binary masks to indicate inpainted regions, providing rich metadata for analysis. Compared to existing datasets, detection models trained on our dataset demonstrate superior generalization capabilities. Our dataset not only serves as a strong benchmark for evaluating detection methods but also contributes to advancing the robustness of AI-generated image detection techniques. Building upon this, we propose a lightweight detection method based on image noise entropy, which transforms the original image into an entropy tensor of Non-Local Means (NLM) noise before classification. Extensive experiments demonstrate that models trained on our dataset achieve strong generalization, and our method delivers competitive performance, establishing a solid baseline for future research. The dataset and source code are publicly available at this https URL.</li>
<li><strong>摘要：</strong>生成式人工智能的快速发展引起了人们对数字图像真实性的担忧，因为现在可以以低成本生成高度逼真的虚假图像，这可能会增加社会风险。为此，已经建立了几个数据集来训练检测模型，旨在区分人工智能生成的图像和真实图像。然而，现有数据集存在泛化能力有限、图像质量低、提示过于简单、图像多样性不足等问题。为了解决这些限制，我们提出了一个高质量、大规模的数据集，其中包含多个类别的 730,000 多张图像，包括真实图像和人工智能生成的图像。生成的图像是通过最先进的方法合成的，包括文本到图像生成（由超过 10,000 个精心设计的提示引导）、图像修复、图像细化和面部交换。每个生成的图像都注释有其生成方法和类别。修复图像还包括用于指示修复区域的二进制掩码，为分析提供丰富的元数据。与现有数据集相比，在我们的数据集上训练的检测模型表现出卓越的泛化能力。我们的数据集不仅可以作为评估检测方法的强大基准，还有助于提高人工智能生成的图像检测技术的稳健性。在此基础上，我们提出了一种基于图像噪声熵的轻量级检测方法，该方法在分类之前将原始图像转换为非局部均值（NLM）噪声的熵张量。大量的实验表明，在我们的数据集上训练的模型具有很强的泛化性，并且我们的方法提供了具有竞争力的性能，为未来的研究奠定了坚实的基线。数据集和源代码可通过此 https URL 公开获取。</li>
</ul>

<h3>Title: Enhancing Underwater Images via Adaptive Semantic-aware Codebook Learning</h3>
<ul>
<li><strong>Authors: </strong>Bosen Lin, Feng Gao, Yanwei Yu, Junyu Dong, Qian Du</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10586">https://arxiv.org/abs/2602.10586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10586">https://arxiv.org/pdf/2602.10586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10586]] Enhancing Underwater Images via Adaptive Semantic-aware Codebook Learning(https://arxiv.org/abs/2602.10586)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Underwater Image Enhancement (UIE) is an ill-posed problem where natural clean references are not available, and the degradation levels vary significantly across semantic regions. Existing UIE methods treat images with a single global model and ignore the inconsistent degradation of different scene components. This oversight leads to significant color distortions and loss of fine details in heterogeneous underwater scenes, especially where degradation varies significantly across different image regions. Therefore, we propose SUCode (Semantic-aware Underwater Codebook Network), which achieves adaptive UIE from semantic-aware discrete codebook representation. Compared with one-shot codebook-based methods, SUCode exploits semantic-aware, pixel-level codebook representation tailored to heterogeneous underwater degradation. A three-stage training paradigm is employed to represent raw underwater image features to avoid pseudo ground-truth contamination. Gated Channel Attention Module (GCAM) and Frequency-Aware Feature Fusion (FAFF) jointly integrate channel and frequency cues for faithful color restoration and texture recovery. Extensive experiments on multiple benchmarks demonstrate that SUCode achieves state-of-the-art performance, outperforming recent UIE methods on both reference and no-reference metrics. The code will be made public available at this https URL.</li>
<li><strong>摘要：</strong>水下图像增强（UIE）是一个不适定问题，其中自然干净的参考不可用，并且不同语义区域的退化水平差异很大。现有的 UIE 方法使用单个全局模型处理图像，并忽略不同场景组件的不一致退化。这种疏忽会导致异构水下场景中显着的颜色失真和精细细节的丢失，特别是在不同图像区域的退化差异显着的情况下。因此，我们提出了 SUCode（语义感知水下码本网络），它从语义感知离散码本表示实现自适应 UIE。与基于一次性码本的方法相比，SUCode 利用针对异构水下退化量身定制的语义感知、像素级码本表示。采用三阶段训练范例来表示原始水下图像特征，以避免伪地面真实污染。门控通道注意模块 (GCAM) 和频率感知特征融合 (FAFF) 联合集成通道和频率线索，以实现忠实的色彩恢复和纹理恢复。对多个基准的大量实验表明，SUCode 实现了最先进的性能，在参考和无参考指标上均优于最新的 UIE 方法。该代码将在此 https URL 上公开。</li>
</ul>

<h3>Title: dnaHNet: A Scalable and Hierarchical Foundation Model for Genomic Sequence Learning</h3>
<ul>
<li><strong>Authors: </strong>Arnav Shah, Junzhe Li, Parsa Idehpour, Adibvafa Fallahpour, Brandon Wang, Sukjun Hwang, Bo Wang, Patrick D. Hsu, Hani Goodarzi, Albert Gu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10603">https://arxiv.org/abs/2602.10603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10603">https://arxiv.org/pdf/2602.10603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10603]] dnaHNet: A Scalable and Hierarchical Foundation Model for Genomic Sequence Learning(https://arxiv.org/abs/2602.10603)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Genomic foundation models have the potential to decode DNA syntax, yet face a fundamental tradeoff in their input representation. Standard fixed-vocabulary tokenizers fragment biologically meaningful motifs such as codons and regulatory elements, while nucleotide-level models preserve biological coherence but incur prohibitive computational costs for long contexts. We introduce dnaHNet, a state-of-the-art tokenizer-free autoregressive model that segments and models genomic sequences end-to-end. Using a differentiable dynamic chunking mechanism, dnaHNet compresses raw nucleotides into latent tokens adaptively, balancing compression with predictive accuracy. Pretrained on prokaryotic genomes, dnaHNet outperforms leading architectures including StripedHyena2 in scaling and efficiency. This recursive chunking yields quadratic FLOP reductions, enabling $>3 \times$ inference speedup over Transformers. On zero-shot tasks, dnaHNet achieves superior performance in predicting protein variant fitness and gene essentiality, while automatically discovering hierarchical biological structures without supervision. These results establish dnaHNet as a scalable, interpretable framework for next-generation genomic modeling.</li>
<li><strong>摘要：</strong>基因组基础模型具有解码 DNA 语法的潜力，但在输入表示方面面临着根本性的权衡。标准的固定词汇分词器将具有生物学意义的基序（例如密码子和调控元件）片段化，而核苷酸水平模型保留了生物一致性，但在长上下文中会产生高昂的计算成本。我们引入了 dnaHNet，这是一种最先进的无分词器的自回归模型，可以端到端地对基因组序列进行分段和建模。 dnaHNet 使用可微的动态分块机制，自适应地将原始核苷酸压缩为潜在标记，平衡压缩与预测准确性。 dnaHNet 在原核基因组上进行预训练，在扩展性和效率方面优于包括 StripedHyena2 在内的领先架构。这种递归分块可减少二次方的 FLOP，从而使推理速度比 Transformer 提高 >3 倍。在零样本任务中，dnaHNet 在预测蛋白质变异适应性和基因必要性方面实现了卓越的性能，同时在无需监督的情况下自动发现分层生物结构。这些结果将 dnaHNet 确立为下一代基因组建模的可扩展、可解释的框架。</li>
</ul>

<h3>Title: Mitigating Reward Hacking in RLHF via Bayesian Non-negative Reward Modeling</h3>
<ul>
<li><strong>Authors: </strong>Zhibin Duan, Guowei Rong, Zhuo Li, Bo Chen, Mingyuan Zhou, Dandan Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10623">https://arxiv.org/abs/2602.10623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10623">https://arxiv.org/pdf/2602.10623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10623]] Mitigating Reward Hacking in RLHF via Bayesian Non-negative Reward Modeling(https://arxiv.org/abs/2602.10623)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reward models learned from human preferences are central to aligning large language models (LLMs) via reinforcement learning from human feedback, yet they are often vulnerable to reward hacking due to noisy annotations and systematic biases such as response length or style. We propose Bayesian Non-Negative Reward Model (BNRM), a principled reward modeling framework that integrates non-negative factor analysis into Bradley-Terry (BT) preference model. BNRM represents rewards through a sparse, non-negative latent factor generative process that operates at two complementary levels: instance-specific latent variables induce disentangled reward representations, while sparsity over global latent factors acts as an implicit debiasing mechanism that suppresses spurious correlations. Together, this disentanglement-then-debiasing structure enables robust uncertainty-aware reward learning. To scale BNRM to modern LLMs, we develop an amortized variational inference network conditioned on deep model representations, allowing efficient end-to-end training. Extensive empirical results demonstrate that BNRM substantially mitigates reward over-optimization, improves robustness under distribution shifts, and yields more interpretable reward decompositions than strong baselines.</li>
<li><strong>摘要：</strong>从人类偏好中学习的奖励模型对于通过人类反馈的强化学习来调整大型语言模型（LLM）至关重要，但由于嘈杂的注释和系统偏差（例如响应长度或风格），它们通常很容易受到奖励黑客攻击。我们提出贝叶斯非负奖励模型（BNRM），这是一种原则奖励建模框架，它将非负因素分析集成到 Bradley-Terry（BT）偏好模型中。 BNRM 通过稀疏的、非负的潜在因子生成过程来表示奖励，该过程在两个互补的层面上运行：特定于实例的潜在变量引起解缠结的奖励表示，而全局潜在因子的稀疏性充当隐式去偏机制，抑制虚假相关性。总之，这种解开纠缠然后消除偏差的结构可以实现强大的不确定性感知奖励学习。为了将 BNRM 扩展到现代法学硕士，我们开发了一个以深度模型表示为条件的摊销变分推理网络，从而实现高效的端到端训练。大量的实证结果表明，BNRM 大大减轻了奖励过度优化，提高了分布变化下的鲁棒性，并产生比强基线更可解释的奖励分解。</li>
</ul>

<h3>Title: Eliminating VAE for Fast and High-Resolution Generative Detail Restoration</h3>
<ul>
<li><strong>Authors: </strong>Yan Wang, Shijie Zhao, Junlin Li, Li Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10630">https://arxiv.org/abs/2602.10630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10630">https://arxiv.org/pdf/2602.10630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10630]] Eliminating VAE for Fast and High-Resolution Generative Detail Restoration(https://arxiv.org/abs/2602.10630)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have attained remarkable breakthroughs in the real-world super-resolution (SR) task, albeit at slow inference and high demand on devices. To accelerate inference, recent works like GenDR adopt step distillation to minimize the step number to one. However, the memory boundary still restricts the maximum processing size, necessitating tile-by-tile restoration of high-resolution images. Through profiling the pipeline, we pinpoint that the variational auto-encoder (VAE) is the bottleneck of latency and memory. To completely solve the problem, we leverage pixel-(un)shuffle operations to eliminate the VAE, reversing the latent-based GenDR to pixel-space GenDR-Pix. However, upscale with x8 pixelshuffle may induce artifacts of repeated patterns. To alleviate the distortion, we propose a multi-stage adversarial distillation to progressively remove the encoder and decoder. Specifically, we utilize generative features from the previous stage models to guide adversarial discrimination. Moreover, we propose random padding to augment generative features and avoid discriminator collapse. We also introduce a masked Fourier space loss to penalize the outliers of amplitude. To improve inference performance, we empirically integrate a padding-based self-ensemble with classifier-free guidance to improve inference scaling. Experimental results show that GenDR-Pix performs 2.8x acceleration and 60% memory-saving compared to GenDR with negligible visual degradation, surpassing other one-step diffusion SR. Against all odds, GenDR-Pix can restore 4K image in only 1 second and 6GB.</li>
<li><strong>摘要：</strong>尽管推理速度慢且对设备的要求很高，但扩散模型在现实世界的超分辨率（SR）任务中取得了显着的突破。为了加速推理，GenDR 等最近的工作采用了步骤蒸馏，将步骤数最小化为 1。然而，内存边界仍然限制最大处理大小，需要逐块恢复高分辨率图像。通过对管道进行分析，我们发现变分自动编码器（VAE）是延迟和内存的瓶颈。为了彻底解决这个问题，我们利用像素（非）洗牌操作来消除 VAE，将基于潜在的 GenDR 反转为像素空间 GenDR-Pix。然而，x8 Pixelshuffle 的高档可能会导致重复图案的伪影。为了减轻失真，我们提出了多阶段对抗性蒸馏来逐步删除编码器和解码器。具体来说，我们利用前一阶段模型的生成特征来指导对抗性歧视。此外，我们提出随机填充来增强生成特征并避免鉴别器崩溃。我们还引入了掩蔽傅立叶空间损失来惩罚幅度的异常值。为了提高推理性能，我们根据经验将基于填充的自集成与无分类器指导相结合，以提高推理规模。实验结果表明，与 GenDR 相比，GenDR-Pix 的加速速度提高了 2.8 倍，节省了 60% 的内存，视觉退化可以忽略不计，超越了其他一步扩散 SR。尽管困难重重，GenDR-Pix 仍能在 1 秒内恢复 4K 图像，容量为 6GB。</li>
</ul>

<h3>Title: Generative clinical time series models trained on moderate amounts of patient data are privacy preserving</h3>
<ul>
<li><strong>Authors: </strong>Rustam Zhumagambetov, Niklas Giesa, Sebastian D. Boie, Stefan Haufe</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10631">https://arxiv.org/abs/2602.10631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10631">https://arxiv.org/pdf/2602.10631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10631]] Generative clinical time series models trained on moderate amounts of patient data are privacy preserving(https://arxiv.org/abs/2602.10631)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Sharing medical data for machine learning model training purposes is often impossible due to the risk of disclosing identifying information about individual patients. Synthetic data produced by generative artificial intelligence (genAI) models trained on real data is often seen as one possible solution to comply with privacy regulations. While powerful genAI models for heterogeneous hospital time series have recently been introduced, such modeling does not guarantee privacy protection, as the generated data may still reveal identifying information about individuals in the models' training cohort. Applying established privacy mechanisms to generative time series models, however, proves challenging as post-hoc data anonymization through k-anonymization or similar techniques is limited, while model-centered privacy mechanisms that implement differential privacy (DP) may lead to unstable training, compromising the utility of generated data. Given these known limitations, privacy audits for generative time series models are currently indispensable regardless of the concrete privacy mechanisms applied to models and/or data. In this work, we use a battery of established privacy attacks to audit state-of-the-art hospital time series models, trained on the public MIMIC-IV dataset, with respect to privacy preservation. Furthermore, the eICU dataset was used to mount a privacy attack against the synthetic data generator trained on the MIMIC-IV dataset. Results show that established privacy attacks are ineffective against generated multivariate clinical time series when synthetic data generators are trained on large enough training datasets. Furthermore, we discuss how the use of existing DP mechanisms for these synthetic data generators would not bring desired improvement in privacy, but only a decrease in utility for machine learning prediction tasks.</li>
<li><strong>摘要：</strong>由于存在泄露个体患者识别信息的风险，出于机器学习模型训练目的共享医疗数据通常是不可能的。由基于真实数据训练的生成人工智能 (genAI) 模型生成的合成数据通常被视为遵守隐私法规的一种可能的解决方案。虽然最近推出了针对异构医院时间序列的强大 genAI 模型，但这种建模并不能保证隐私保护，因为生成的数据仍然可能会泄露模型训练队列中个体的识别信息。然而，将已建立的隐私机制应用于生成时间序列模型具有挑战性，因为通过 k-匿名化或类似技术进行的事后数据匿名化是有限的，而实现差分隐私 (DP) 的以模型为中心的隐私机制可能会导致训练不稳定，从而损害生成数据的效用。鉴于这些已知的限制，无论应用于模型和/或数据的具体隐私机制如何，生成时间序列模型的隐私审计目前都是必不可少的。在这项工作中，我们使用一系列已建立的隐私攻击来审核最先进的医院时间序列模型，并在公共 MIMIC-IV 数据集上进行训练，以保护隐私。此外，eICU 数据集还被用来对在 MIMIC-IV 数据集上训练的合成数据生成器发起隐私攻击。结果表明，当合成数据生成器在足够大的训练数据集上进行训练时，已建立的隐私攻击对于生成的多变量临床时间序列无效。此外，我们还讨论了这些合成数据生成器使用现有的 DP 机制不会带来预期的隐私改进，而只会降低机器学习预测任务的效用。</li>
</ul>

<h3>Title: Coarse-Grained Boltzmann Generators</h3>
<ul>
<li><strong>Authors: </strong>Weilong Chen, Bojun Zhao, Jan Eckwert, Julija Zavadlav</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.stat-mech, physics.chem-ph, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10637">https://arxiv.org/abs/2602.10637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10637">https://arxiv.org/pdf/2602.10637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10637]] Coarse-Grained Boltzmann Generators(https://arxiv.org/abs/2602.10637)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Sampling equilibrium molecular configurations from the Boltzmann distribution is a longstanding challenge. Boltzmann Generators (BGs) address this by combining exact-likelihood generative models with importance sampling, but their practical scalability is limited. Meanwhile, coarse-grained surrogates enable the modeling of larger systems by reducing effective dimensionality, yet often lack the reweighting process required to ensure asymptotically correct statistics. In this work, we propose Coarse-Grained Boltzmann Generators (CG-BGs), a principled framework that unifies scalable reduced-order modeling with the exactness of importance sampling. CG-BGs act in a coarse-grained coordinate space, using a learned potential of mean force (PMF) to reweight samples generated by a flow-based model. Crucially, we show that this PMF can be efficiently learned from rapidly converged data via force matching. Our results demonstrate that CG-BGs faithfully capture complex interactions mediated by explicit solvent within highly reduced representations, establishing a scalable pathway for the unbiased sampling of larger molecular systems.</li>
<li><strong>摘要：</strong>从玻尔兹曼分布中采样平衡分子构型是一个长期存在的挑战。玻尔兹曼生成器（BG）通过将精确似然生成模型与重要性采样相结合来解决这个问题，但它们的实际可扩展性有限。同时，粗粒度代理可以通过降低有效维度来对更大的系统进行建模，但通常缺乏确保渐近正确统计数据所需的重新加权过程。在这项工作中，我们提出了粗粒度玻尔兹曼生成器（CG-BG），这是一个原则框架，它将可扩展的降阶建模与重要性采样的准确性结合起来。 CG-BG 在粗粒度坐标空间中起作用，使用学习到的平均力 (PMF) 势来重新加权基于流的模型生成的样本。至关重要的是，我们表明可以通过力匹配从快速收敛的数据中有效地学习该 PMF。我们的结果表明，CG-BG 在高度简化的表示中忠实地捕获了由显式溶剂介导的复杂相互作用，为较大分子系统的无偏采样建立了可扩展的途径。</li>
</ul>

<h3>Title: VideoSTF: Stress-Testing Output Repetition in Video Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Cao, Wei Song, Shangzhi Xu, Jingling Xue, Jin Song Dong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10639">https://arxiv.org/abs/2602.10639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10639">https://arxiv.org/pdf/2602.10639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10639]] VideoSTF: Stress-Testing Output Repetition in Video Large Language Models(https://arxiv.org/abs/2602.10639)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video Large Language Models (VideoLLMs) have recently achieved strong performance in video understanding tasks. However, we identify a previously underexplored generation failure: severe output repetition, where models degenerate into self-reinforcing loops of repeated phrases or sentences. This failure mode is not captured by existing VideoLLM benchmarks, which focus primarily on task accuracy and factual correctness. We introduce VideoSTF, the first framework for systematically measuring and stress-testing output repetition in VideoLLMs. VideoSTF formalizes repetition using three complementary n-gram-based metrics and provides a standardized testbed of 10,000 diverse videos together with a library of controlled temporal transformations. Using VideoSTF, we conduct pervasive testing, temporal stress testing, and adversarial exploitation across 10 advanced VideoLLMs. We find that output repetition is widespread and, critically, highly sensitive to temporal perturbations of video inputs. Moreover, we show that simple temporal transformations can efficiently induce repetitive degeneration in a black-box setting, exposing output repetition as an exploitable security vulnerability. Our results reveal output repetition as a fundamental stability issue in modern VideoLLMs and motivate stability-aware evaluation for video-language systems. Our evaluation code and scripts are available at: this https URL.</li>
<li><strong>摘要：</strong>视频大语言模型（VideoLLM）最近在视频理解任务中取得了强劲的性能。然而，我们发现了一个先前未被充分探索的生成失败：严重的输出重复，其中模型退化为重复短语或句子的自我强化循环。现有的 VideoLLM 基准测试未捕获此故障模式，该基准测试主要关注任务准确性和事实正确性。我们介绍 VideoSTF，这是第一个在 VideoLLM 中系统测量和压力测试输出重复的框架。 VideoSTF 使用三个互补的基于 n-gram 的指标来形式化重复，并提供包含 10,000 个不同视频的标准化测试床以及受控时间转换库。使用 VideoSTF，我们在 10 个高级 VideoLLM 中进行普遍测试、时间压力测试和对抗性利用。我们发现输出重复很普遍，而且至关重要的是，它对视频输入的时间扰动高度敏感。此外，我们表明简单的时间变换可以有效地在黑盒设置中引起重复退化，从而将输出重复暴露为可利用的安全漏洞。我们的结果表明，输出重复是现代 VideoLLM 中的一个基本稳定性问题，并激发了对视频语言系统的稳定性感知评估。我们的评估代码和脚本可从以下位置获取：此 https URL。</li>
</ul>

<h3>Title: Evaluation metrics for temporal preservation in synthetic longitudinal patient data</h3>
<ul>
<li><strong>Authors: </strong>Katariina Perkonoja, Parisa Movahedi, Antti Airola, Kari Auranen, Joni Virta</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10643">https://arxiv.org/abs/2602.10643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10643">https://arxiv.org/pdf/2602.10643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10643]] Evaluation metrics for temporal preservation in synthetic longitudinal patient data(https://arxiv.org/abs/2602.10643)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study introduces a set of metrics for evaluating temporal preservation in synthetic longitudinal patient data, defined as artificially generated data that mimic real patients' repeated measurements over time. The proposed metrics assess how synthetic data reproduces key temporal characteristics, categorized into marginal, covariance, individual-level and measurement structures. We show that strong marginal-level resemblance may conceal distortions in covariance and disruptions in individual-level trajectories. Temporal preservation is influenced by factors such as original data quality, measurement frequency, and preprocessing strategies, including binning, variable encoding and precision. Variables with sparse or highly irregular measurement times provide limited information for learning temporal dependencies, resulting in reduced resemblance between the synthetic and original data. No single metric adequately captures temporal preservation; instead, a multidimensional evaluation across all characteristics provides a more comprehensive assessment of synthetic data quality. Overall, the proposed metrics clarify how and why temporal structures are preserved or degraded, enabling more reliable evaluation and improvement of generative models and supporting the creation of temporally realistic synthetic longitudinal patient data.</li>
<li><strong>摘要：</strong>本研究引入了一组用于评估合成纵向患者数据的时间保存的指标，这些数据被定义为模仿真实患者随时间重复测量的人工生成的数据。提出的指标评估合成数据如何再现关键时间特征，分为边际、协方差、个体水平和测量结构。我们表明，强烈的边际水平相似性可能掩盖了协方差的扭曲和个体水平轨迹的破坏。时间保存受到原始数据质量、测量频率和预处理策略（包括分箱、变量编码和精度）等因素的影响。测量时间稀疏或高度不规则的变量为学习时间依赖性提供的信息有限，导致合成数据与原始数据之间的相似性降低。没有任何单一指标能够充分捕捉时间保存情况；相反，对所有特征进行多维评估可以对合成数据质量进行更全面的评估。总体而言，所提出的指标阐明了时间结构如何以及为何被保留或退化，从而能够更可靠地评估和改进生成模型，并支持创建时间上真实的合成纵向患者数据。</li>
</ul>

<h3>Title: Multimodal Priors-Augmented Text-Driven 3D Human-Object Interaction Generation</h3>
<ul>
<li><strong>Authors: </strong>Yin Wang, Ziyao Zhang, Zhiying Leng, Haitian Liu, Frederick W. B. Li, Mu Li, Xiaohui Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10659">https://arxiv.org/abs/2602.10659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10659">https://arxiv.org/pdf/2602.10659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10659]] Multimodal Priors-Augmented Text-Driven 3D Human-Object Interaction Generation(https://arxiv.org/abs/2602.10659)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We address the challenging task of text-driven 3D human-object interaction (HOI) motion generation. Existing methods primarily rely on a direct text-to-HOI mapping, which suffers from three key limitations due to the significant cross-modality gap: (Q1) sub-optimal human motion, (Q2) unnatural object motion, and (Q3) weak interaction between humans and objects. To address these challenges, we propose MP-HOI, a novel framework grounded in four core insights: (1) Multimodal Data Priors: We leverage multimodal data (text, image, pose/object) from large multimodal models as priors to guide HOI generation, which tackles Q1 and Q2 in data modeling. (2) Enhanced Object Representation: We improve existing object representations by incorporating geometric keypoints, contact features, and dynamic properties, enabling expressive object representations, which tackles Q2 in data representation. (3) Multimodal-Aware Mixture-of-Experts (MoE) Model: We propose a modality-aware MoE model for effective multimodal feature fusion paradigm, which tackles Q1 and Q2 in feature fusion. (4) Cascaded Diffusion with Interaction Supervision: We design a cascaded diffusion framework that progressively refines human-object interaction features under dedicated supervision, which tackles Q3 in interaction refinement. Comprehensive experiments demonstrate that MP-HOI outperforms existing approaches in generating high-fidelity and fine-grained HOI motions.</li>
<li><strong>摘要：</strong>我们解决文本驱动的 3D 人机交互 (HOI) 运动生成这一具有挑战性的任务。现有方法主要依赖于直接文本到 HOI 的映射，由于存在显着的跨模态差距，该方法受到三个关键限制：（Q1）次优的人体运动，（Q2）不自然的物体运动，以及（Q3）人类和物体之间的弱交互。为了应对这些挑战，我们提出了 MP-HOI，这是一个基于四个核心见解的新颖框架：（1）多模态数据先验：我们利用大型多模态模型中的多模态数据（文本、图像、姿势/对象）作为先验来指导 HOI 生成，从而解决数据建模中的 Q1 和 Q2 问题。 （2）增强的对象表示：我们通过合并几何关键点、接触特征和动态属性来改进现有的对象表示，从而实现富有表现力的对象表示，从而解决了数据表示中的问题2。 （3）多模态感知混合专家（MoE）模型：我们提出了一种用于有效多模态特征融合范式的模态感知MoE模型，该模型解决了特征融合中的Q1和Q2问题。 （4）具有交互监督的级联扩散：我们设计了一个级联扩散框架，在专门的监督下逐步细化人与物体的交互特征，解决了交互细化中的问题3。综合实验表明，MP-HOI 在生成高保真和细粒度 HOI 运动方面优于现有方法。</li>
</ul>

<h3>Title: Dynamic Frequency Modulation for Controllable Text-driven Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Tiandong Shi, Ling Zhao, Ji Qi, Jiayi Ma, Chengli Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10662">https://arxiv.org/abs/2602.10662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10662">https://arxiv.org/pdf/2602.10662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10662]] Dynamic Frequency Modulation for Controllable Text-driven Image Generation(https://arxiv.org/abs/2602.10662)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The success of text-guided diffusion models has established a new image generation paradigm driven by the iterative refinement of text prompts. However, modifying the original text prompt to achieve the expected semantic adjustments often results in unintended global structure changes that disrupt user intent. Existing methods rely on empirical feature map selection for intervention, whose performance heavily depends on appropriate selection, leading to suboptimal stability. This paper tries to solve the aforementioned problem from a frequency perspective and analyzes the impact of the frequency spectrum of noisy latent variables on the hierarchical emergence of the structure framework and fine-grained textures during the generation process. We find that lower-frequency components are primarily responsible for establishing the structure framework in the early generation stage. Their influence diminishes over time, giving way to higher-frequency components that synthesize fine-grained textures. In light of this, we propose a training-free frequency modulation method utilizing a frequency-dependent weighting function with dynamic decay. This method maintains the structure framework consistency while permitting targeted semantic modifications. By directly manipulating the noisy latent variable, the proposed method avoids the empirical selection of internal feature maps. Extensive experiments demonstrate that the proposed method significantly outperforms current state-of-the-art methods, achieving an effective balance between preserving structure and enabling semantic updates.</li>
<li><strong>摘要：</strong>文本引导扩散模型的成功建立了由文本提示的迭代细化驱动的新图像生成范式。然而，修改原始文本提示以实现预期的语义调整通常会导致意外的全局结构更改，从而扰乱用户意图。现有方法依赖于经验特征图选择进行干预，其性能在很大程度上取决于适当的选择，导致稳定性不佳。本文试图从频率角度解决上述问题，并分析生成过程中噪声潜变量的频谱对结构框架和细粒度纹理的分层出现的影响。我们发现低频组件主要负责建立早期生成阶段的结构框架。它们的影响随着时间的推移而减弱，让位于合成细粒度纹理的高频组件。鉴于此，我们提出了一种利用具有动态衰减的频率相关加权函数的免训练频率调制方法。该方法保持结构框架的一致性，同时允许有针对性的语义修改。通过直接操纵噪声潜变量，所提出的方法避免了内部特征图的经验选择。大量的实验表明，所提出的方法显着优于当前最先进的方法，在保留结构和实现语义更新之间实现了有效的平衡。</li>
</ul>

<h3>Title: TwiFF (Think With Future Frames): A Large-Scale Dataset for Dynamic Visual Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Junhua Liu, Zhangcheng Wang, Zhike Han, Ningli Wang, Guotao Liang, Kun Kuang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10675">https://arxiv.org/abs/2602.10675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10675">https://arxiv.org/pdf/2602.10675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10675]] TwiFF (Think With Future Frames): A Large-Scale Dataset for Dynamic Visual Reasoning(https://arxiv.org/abs/2602.10675)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Visual Chain-of-Thought (VCoT) has emerged as a promising paradigm for enhancing multimodal reasoning by integrating visual perception into intermediate reasoning steps. However, existing VCoT approaches are largely confined to static scenarios and struggle to capture the temporal dynamics essential for tasks such as instruction, prediction, and camera motion. To bridge this gap, we propose TwiFF-2.7M, the first large-scale, temporally grounded VCoT dataset derived from $2.7$ million video clips, explicitly designed for dynamic visual question and answer. Accompanying this, we introduce TwiFF-Bench, a high-quality evaluation benchmark of $1,078$ samples that assesses both the plausibility of reasoning trajectories and the correctness of final answers in open-ended dynamic settings. Building on these foundations, we propose the TwiFF model, a unified modal that synergistically leverages pre-trained video generation and image comprehension capabilities to produce temporally coherent visual reasoning cues-iteratively generating future action frames and textual reasoning. Extensive experiments demonstrate that TwiFF significantly outperforms existing VCoT methods and Textual Chain-of-Thought baselines on dynamic reasoning tasks, which fully validates the effectiveness for visual question answering in dynamic scenarios. Our code and data is available at this https URL.</li>
<li><strong>摘要：</strong>视觉思维链（VCoT）已成为一种有前途的范式，通过将视觉感知集成到中间推理步骤来增强多模态推理。然而，现有的 VCoT 方法主要局限于静态场景，难以捕捉指令、预测和相机运动等任务所必需的时间动态。为了弥补这一差距，我们提出了 TwiFF-2.7M，这是第一个大规模、基于时间的 VCoT 数据集，源自价值 270 万美元的视频剪辑，专门为动态视觉问答而设计。与此同时，我们推出了 TwiFF-Bench，这是一个包含 1,078 美元样本的高质量评估基准，用于评估开放式动态设置中推理轨迹的合理性和最终答案的正确性。在此基础上，我们提出了 TwiFF 模型，这是一种统一模式，协同利用预先训练的视频生成和图像理解功能来产生时间连贯的视觉推理线索，迭代地生成未来的动作框架和文本推理。大量实验表明，TwiFF 在动态推理任务上显着优于现有的 VCoT 方法和文本思维链基线，充分验证了动态场景下视觉问答的有效性。我们的代码和数据可以通过此 https URL 获取。</li>
</ul>

<h3>Title: OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL</h3>
<ul>
<li><strong>Authors: </strong>Jinjie Shen, Jing Wu, Yaxiong Wang, Lechao Cheng, Shengeng Tang, Tianrui Hui, Nan Pu, Zhun Zhong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10687">https://arxiv.org/abs/2602.10687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10687">https://arxiv.org/pdf/2602.10687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10687]] OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL(https://arxiv.org/abs/2602.10687)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing forgery detection methods are often limited to uni-modal or bi-modal settings, failing to handle the interleaved text, images, and videos prevalent in real-world misinformation. To bridge this gap, this paper targets to develop a unified framework for omnibus vision-language forgery detection and grounding. In this unified setting, the {interplay} between diverse modalities and the dual requirements of simultaneous detection and localization pose a critical ``difficulty bias`` problem: the simpler veracity classification task tends to dominate the gradients, leading to suboptimal performance in fine-grained grounding during multi-task optimization. To address this challenge, we propose \textbf{OmniVL-Guard}, a balanced reinforcement learning framework for omnibus vision-language forgery detection and grounding. Particularly, OmniVL-Guard comprises two core designs: Self-Evolving CoT Generatio and Adaptive Reward Scaling Policy Optimization (ARSPO). {Self-Evolving CoT Generation} synthesizes high-quality reasoning paths, effectively overcoming the cold-start challenge. Building upon this, {Adaptive Reward Scaling Policy Optimization (ARSPO)} dynamically modulates reward scales and task weights, ensuring a balanced joint optimization. Extensive experiments demonstrate that OmniVL-Guard significantly outperforms state-of-the-art methods and exhibits zero-shot robust generalization across out-of-domain scenarios.</li>
<li><strong>摘要：</strong>现有的伪造检测方法通常仅限于单模态或双模态设置，无法处理现实世界错误信息中普遍存在的交错文本、图像和视频。为了弥补这一差距，本文的目标是开发一个用于综合视觉语言伪造检测和基础的统一框架。在这种统一的设置中，不同模态之间的相互作用以及同时检测和定位的双重要求提出了一个关键的“难度偏差”问题：更简单的准确性分类任务往往会主导梯度，导致多任务优化期间细粒度基础的性能不佳。为了应对这一挑战，我们提出了 \textbf{OmniVL-Guard}，这是一个用于综合视觉语言伪造检测和基础的平衡强化学习框架。特别是，OmniVL-Guard包含两个核心设计：自我进化CoT生成和自适应奖励缩放策略优化（ARSPO）。 {Self-Evolving CoT Generation}综合高质量推理路径，有效克服冷启动挑战。在此基础上，{自适应奖励缩放策略优化（ARSPO）}动态调整奖励规模和任务权重，确保平衡的联合优化。大量实验表明，OmniVL-Guard 的性能显着优于最先进的方法，并在域外场景中展现出零样本的鲁棒泛化能力。</li>
</ul>

<h3>Title: SnapMLA: Efficient Long-Context MLA Decoding via Hardware-Aware FP8 Quantized Pipelining</h3>
<ul>
<li><strong>Authors: </strong>Yifan Zhang, Zunhai Su, Shuhao Hu, Rui Yang, Wei Wu, Yulei Qian, Yuchen Xie, Xunliang Cai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10718">https://arxiv.org/abs/2602.10718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10718">https://arxiv.org/pdf/2602.10718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10718]] SnapMLA: Efficient Long-Context MLA Decoding via Hardware-Aware FP8 Quantized Pipelining(https://arxiv.org/abs/2602.10718)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While FP8 attention has shown substantial promise in innovations like FlashAttention-3, its integration into the decoding phase of the DeepSeek Multi-head Latent Attention (MLA) architecture presents notable challenges. These challenges include numerical heterogeneity arising from the decoupling of positional embeddings, misalignment of quantization scales in FP8 PV GEMM, and the need for optimized system-level support. In this paper, we introduce SnapMLA, an FP8 MLA decoding framework optimized to improve long-context efficiency through the following hardware-aware algorithm-kernel co-optimization techniques: (i) RoPE-Aware Per-Token KV Quantization, where the RoPE part is maintained in high precision, motivated by our comprehensive analysis of the heterogeneous quantization sensitivity inherent to the MLA KV cache. Furthermore, per-token granularity is employed to align with the autoregressive decoding process and maintain quantization accuracy. (ii) Quantized PV Computation Pipeline Reconstruction, which resolves the misalignment of quantization scale in FP8 PV computation stemming from the shared KV structure of the MLA KV cache. (iii) End-to-End Dataflow Optimization, where we establish an efficient data read-and-write workflow using specialized kernels, ensuring efficient data flow and performance gains. Extensive experiments on state-of-the-art MLA LLMs show that SnapMLA achieves up to a 1.91x improvement in throughput, with negligible risk of performance degradation in challenging long-context tasks, including mathematical reasoning and code generation benchmarks. Code is available at this https URL.</li>
<li><strong>摘要：</strong>虽然 FP8 注意力在 FlashAttention-3 等创新中显示出了巨大的前景，但将其集成到 DeepSeek 多头潜在注意力 (MLA) 架构的解码阶段却带来了显着的挑战。这些挑战包括位置嵌入解耦产生的数值异质性、FP8 PV GEMM 中量化尺度的错位以及优化系统级支持的需求。在本文中，我们介绍了 SnapMLA，这是一种经过优化的 FP8 MLA 解码框架，通过以下硬件感知算法-内核协同优化技术来提高长上下文效率：（i）RoPE 感知每令牌 KV 量化，其中 RoPE 部分保持高精度，其动机是我们对 MLA KV 缓存固有的异构量化敏感性的全面分析。此外，每个令牌的粒度用于与自回归解码过程保持一致并保持量化精度。 (ii)量化PV计算管道重构，解决了由于MLA KV缓存共享KV结构导致的FP8 PV计算中量化尺度错位的问题。 (iii) 端到端数据流优化，我们使用专用内核建立高效的数据读写工作流程，确保高效的数据流和性能提升。对最先进的 MLA LLM 进行的大量实验表明，SnapMLA 的吞吐量提高了 1.91 倍，在具有挑战性的长上下文任务（包括数学推理和代码生成基准）中，性能下降的风险可以忽略不计。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: Ecological mapping with geospatial foundation models</h3>
<ul>
<li><strong>Authors: </strong>Craig Mahlasi, Gciniwe S. Baloyi, Zaheed Gaffoor, Levente Klein, Anne Jones, Etienne Vos, Michal Muszynski, Geoffrey Dawson, Campbell Watson</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10720">https://arxiv.org/abs/2602.10720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10720">https://arxiv.org/pdf/2602.10720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10720]] Ecological mapping with geospatial foundation models(https://arxiv.org/abs/2602.10720)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Geospatial foundation models (GFMs) are a fast-emerging paradigm for various geospatial tasks, such as ecological mapping. However, the utility of GFMs has not been fully explored for high-value use cases. This study aims to explore the utility, challenges and opportunities associated with the application of GFMs for ecological uses. In this regard, we fine-tune several pretrained AI models, namely, Prithvi-E0-2.0 and TerraMind, across three use cases, and compare this with a baseline ResNet-101 model. Firstly, we demonstrate TerraMind's LULC generation capabilities. Lastly, we explore the utility of the GFMs in forest functional trait mapping and peatlands detection. In all experiments, the GFMs outperform the baseline ResNet models. In general TerraMind marginally outperforms Prithvi. However, with additional modalities TerraMind significantly outperforms the baseline ResNet and Prithvi models. Nonetheless, consideration should be given to the divergence of input data from pretrained modalities. We note that these models would benefit from higher resolution and more accurate labels, especially for use cases where pixel-level dynamics need to be mapped.</li>
<li><strong>摘要：</strong>地理空间基础模型（GFM）是一种快速出现的范例，适用于生态测绘等各种地理空间任务。然而，GFM 在高价值用例中的实用性尚未得到充分探索。本研究旨在探讨 GFM 在生态用途中的应用的效用、挑战和机遇。在这方面，我们在三个用例中微调了几个预训练的 AI 模型，即 Prithvi-E0-2.0 和 TerraMind，并将其与基线 ResNet-101 模型进行比较。首先，我们展示了 TerraMind 的 LULC 生成能力。最后，我们探讨了 GFM 在森林功能性状绘图和泥炭地检测中的效用。在所有实验中，GFM 均优于基线 ResNet 模型。总体而言，TerraMind 的表现略胜于 Prithvi。然而，通过其他模式，TerraMind 显着优于基线 ResNet 和 Prithvi 模型。尽管如此，应考虑输入数据与预训练模式的差异。我们注意到，这些模型将受益于更高分辨率和更准确的标签，特别是对于需要映射像素级动态的用例。</li>
</ul>

<h3>Title: A Diffusion-Based Generative Prior Approach to Sparse-view Computed Tomography</h3>
<ul>
<li><strong>Authors: </strong>Davide Evangelista, Pasquale Cascarano, Elena Loli Piccolomini</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10722">https://arxiv.org/abs/2602.10722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10722">https://arxiv.org/pdf/2602.10722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10722]] A Diffusion-Based Generative Prior Approach to Sparse-view Computed Tomography(https://arxiv.org/abs/2602.10722)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The reconstruction of X-rays CT images from sparse or limited-angle geometries is a highly challenging task. The lack of data typically results in artifacts in the reconstructed image and may even lead to object distortions. For this reason, the use of deep generative models in this context has great interest and potential success. In the Deep Generative Prior (DGP) framework, the use of diffusion-based generative models is combined with an iterative optimization algorithm for the reconstruction of CT images from sinograms acquired under sparse geometries, to maintain the explainability of a model-based approach while introducing the generative power of a neural network. There are therefore several aspects that can be further investigated within these frameworks to improve reconstruction quality, such as image generation, the model, and the iterative algorithm used to solve the minimization problem, for which we propose modifications with respect to existing approaches. The results obtained even under highly sparse geometries are very promising, although further research is clearly needed in this direction.</li>
<li><strong>摘要：</strong>从稀疏或有限角度几何形状重建 X 射线 CT 图像是一项极具挑战性的任务。数据的缺乏通常会导致重建图像中出现伪影，甚至可能导致对象失真。因此，在这种情况下使用深度生成模型具有很大的兴趣和潜在的成功。在深度生成先验（DGP）框架中，使用基于扩散的生成模型与迭代优化算法相结合，用于根据稀疏几何形状下获取的正弦图重建 CT 图像，以保持基于模型的方法的可解释性，同时引入神经网络的生成能力。因此，可以在这些框架内进一步研究几个方面，以提高重建质量，例如图像生成、模型和用于解决最小化问题的迭代算法，为此我们建议对现有方法进行修改。即使在高度稀疏的几何形状下获得的结果也非常有希望，尽管在这个方向上显然需要进一步的研究。</li>
</ul>

<h3>Title: Self-Supervised Image Super-Resolution Quality Assessment based on Content-Free Multi-Model Oriented Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Kian Majlessi, Amir Masoud Soltani, Mohammad Ebrahim Mahdavi, Aurelien Gourrier, Peyman Adibi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10744">https://arxiv.org/abs/2602.10744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10744">https://arxiv.org/pdf/2602.10744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10744]] Self-Supervised Image Super-Resolution Quality Assessment based on Content-Free Multi-Model Oriented Representation Learning(https://arxiv.org/abs/2602.10744)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, quality assessment</a></li>
<li><strong>Abstract: </strong>Super-resolution (SR) applied to real-world low-resolution (LR) images often results in complex, irregular degradations that stem from the inherent complexity of natural scene acquisition. In contrast to SR artifacts arising from synthetic LR images created under well-defined scenarios, those distortions are highly unpredictable and vary significantly across different real-life contexts. Consequently, assessing the quality of SR images (SR-IQA) obtained from realistic LR, remains a challenging and underexplored problem. In this work, we introduce a no-reference SR-IQA approach tailored for such highly ill-posed realistic settings. The proposed method enables domain-adaptive IQA for real-world SR applications, particularly in data-scarce domains. We hypothesize that degradations in super-resolved images are strongly dependent on the underlying SR algorithms, rather than being solely determined by image content. To this end, we introduce a self-supervised learning (SSL) strategy that first pretrains multiple SR model oriented representations in a pretext stage. Our contrastive learning framework forms positive pairs from images produced by the same SR model and negative pairs from those generated by different methods, independent of image content. The proposed approach S3 RIQA, further incorporates targeted preprocessing to extract complementary quality information and an auxiliary task to better handle the various degradation profiles associated with different SR scaling factors. To this end, we constructed a new dataset, SRMORSS, to support unsupervised pretext training; it includes a wide range of SR algorithms applied to numerous real LR images, which addresses a gap in existing datasets. Experiments on real SR-IQA benchmarks demonstrate that S3 RIQA consistently outperforms most state-of-the-art relevant metrics.</li>
<li><strong>摘要：</strong>将超分辨率 (SR) 应用于现实世界的低分辨率 (LR) 图像通常会导致复杂的、不规则的降级，这是由于自然场景采集的固有复杂性造成的。与在明确场景下创建的合成 LR 图像产生的 SR 伪像相比，这些失真是高度不可预测的，并且在不同的现实生活环境中存在显着差异。因此，评估从现实 LR 获得的 SR 图像 (SR-IQA) 的质量仍然是一个具有挑战性且尚未得到充分探索的问题。在这项工作中，我们引入了一种针对这种高度不适定的现实环境量身定制的无参考 SR-IQA 方法。所提出的方法能够为现实世界的 SR 应用提供域自适应 IQA，特别是在数据稀缺领域。我们假设超分辨率图像的退化很大程度上取决于底层的 SR 算法，而不是仅仅由图像内容决定。为此，我们引入了一种自监督学习（SSL）策略，该策略首先在借口阶段预训练多个面向 SR 模型的表示。我们的对比学习框架从相同 SR 模型生成的图像中形成正对，从不同方法生成的图像中形成负对，与图像内容无关。所提出的方法 S3 RIQA 进一步结合了有针对性的预处理以提取补充质量信息和辅助任务以更好地处理与不同 SR 缩放因子相关的各种退化概况。为此，我们构建了一个新的数据集SRMORSS，以支持无监督借口训练；它包括应用于大量真实 LR 图像的广泛 SR 算法，弥补了现有数据集的空白。对真实 SR-IQA 基准测试的实验表明，S3 RIQA 始终优于大多数最先进的相关指标。</li>
</ul>

<h3>Title: Predicting integers from continuous parameters</h3>
<ul>
<li><strong>Authors: </strong>Bas Maat, Peter Bloem</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10751">https://arxiv.org/abs/2602.10751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10751">https://arxiv.org/pdf/2602.10751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10751]] Predicting integers from continuous parameters(https://arxiv.org/abs/2602.10751)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We study the problem of predicting numeric labels that are constrained to the integers or to a subrange of the integers. For example, the number of up-votes on social media posts, or the number of bicycles available at a public rental station. While it is possible to model these as continuous values, and to apply traditional regression, this approach changes the underlying distribution on the labels from discrete to continuous. Discrete distributions have certain benefits, which leads us to the question whether such integer labels can be modeled directly by a discrete distribution, whose parameters are predicted from the features of a given instance. Moreover, we focus on the use case of output distributions of neural networks, which adds the requirement that the parameters of the distribution be continuous so that backpropagation and gradient descent may be used to learn the weights of the network. We investigate several options for such distributions, some existing and some novel, and test them on a range of tasks, including tabular learning, sequential prediction and image generation. We find that overall the best performance comes from two distributions: Bitwise, which represents the target integer in bits and places a Bernoulli distribution on each, and a discrete analogue of the Laplace distribution, which uses a distribution with exponentially decaying tails around a continuous mean.</li>
<li><strong>摘要：</strong>我们研究预测受限于整数或整数子范围的数字标签的问题。例如，社交媒体帖子上的点赞数，或者公共租赁站的可用自行车数量。虽然可以将它们建模为连续值并应用传统回归，但这种方法将标签上的基础分布从离散更改为连续。离散分布有一定的好处，这让我们产生这样的问题：是否可以通过离散分布直接对此类整数标签进行建模，离散分布的参数是根据给定实例的特征预测的。此外，我们关注神经网络输出分布的用例，这增加了分布参数连续的要求，以便可以使用反向传播和梯度下降来学习网络的权重。我们研究了此类分布的几种选择，一些是现有的，一些是新颖的，并在一系列任务上测试它们，包括表格学习、顺序预测和图像生成。我们发现，总体而言，最佳性能来自两种分布：按位，它以位为单位表示目标整数，并在每个分布上放置伯努利分布，以及拉普拉斯分布的离散模拟，它使用围绕连续均值呈指数衰减尾部的分布。</li>
</ul>

<h3>Title: Dual-End Consistency Model</h3>
<ul>
<li><strong>Authors: </strong>Linwei Dong, Ruoyu Guo, Ge Bai, Zehuan Yuan, Yawei Luo, Changqing Zou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10764">https://arxiv.org/abs/2602.10764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10764">https://arxiv.org/pdf/2602.10764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10764]] Dual-End Consistency Model(https://arxiv.org/abs/2602.10764)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The slow iterative sampling nature remains a major bottleneck for the practical deployment of diffusion and flow-based generative models. While consistency models (CMs) represent a state-of-the-art distillation-based approach for efficient generation, their large-scale application is still limited by two key issues: training instability and inflexible sampling. Existing methods seek to mitigate these problems through architectural adjustments or regularized objectives, yet overlook the critical reliance on trajectory selection. In this work, we first conduct an analysis on these two limitations: training instability originates from loss divergence induced by unstable self-supervised term, whereas sampling inflexibility arises from error accumulation. Based on these insights and analysis, we propose the Dual-End Consistency Model (DE-CM) that selects vital sub-trajectory clusters to achieve stable and effective training. DE-CM decomposes the PF-ODE trajectory and selects three critical sub-trajectories as optimization targets. Specifically, our approach leverages continuous-time CMs objectives to achieve few-step distillation and utilizes flow matching as a boundary regularizer to stabilize the training process. Furthermore, we propose a novel noise-to-noisy (N2N) mapping that can map noise to any point, thereby alleviating the error accumulation in the first step. Extensive experimental results show the effectiveness of our method: it achieves a state-of-the-art FID score of 1.70 in one-step generation on the ImageNet 256x256 dataset, outperforming existing CM-based one-step approaches.</li>
<li><strong>摘要：</strong>缓慢的迭代采样性质仍然是扩散和基于流的生成模型的实际部署的主要瓶颈。虽然一致性模型（CM）代表了一种最先进的基于蒸馏的高效生成方法，但其大规模应用仍然受到两个关键问题的限制：训练不稳定和采样不灵活。现有方法试图通过架构调整或规范化目标来缓解这些问题，但忽视了对轨迹选择的关键依赖。在这项工作中，我们首先对这两个局限性进行了分析：训练的不稳定性源于不稳定的自监督项引起的损失发散，而采样的不灵活性则源于误差累积。基于这些见解和分析，我们提出了双端一致性模型（DE-CM），该模型选择重要的子轨迹簇来实现稳定有效的训练。 DE-CM分解PF-ODE轨迹并选择三个关键子轨迹作为优化目标。具体来说，我们的方法利用连续时间 CM 目标来实现几步蒸馏，并利用流匹配作为边界正则化器来稳定训练过程。此外，我们提出了一种新颖的噪声到噪声（N2N）映射，可以将噪声映射到任何点，从而减轻第一步中的误差累积。大量的实验结果表明了我们方法的有效性：它在 ImageNet 256x256 数据集上的一步生成中达到了 1.70 的最先进的 FID 分数，优于现有的基于 CM 的一步方法。</li>
</ul>

<h3>Title: Transport, Don't Generate: Deterministic Geometric Flows for Combinatorial Optimization</h3>
<ul>
<li><strong>Authors: </strong>Benjy Friedmann, Nadav Dym</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10794">https://arxiv.org/abs/2602.10794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10794">https://arxiv.org/pdf/2602.10794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10794]] Transport, Don't Generate: Deterministic Geometric Flows for Combinatorial Optimization(https://arxiv.org/abs/2602.10794)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in Neural Combinatorial Optimization (NCO) have been dominated by diffusion models that treat the Euclidean Traveling Salesman Problem (TSP) as a stochastic $N \times N$ heatmap generation task. In this paper, we propose CycFlow, a framework that replaces iterative edge denoising with deterministic point transport. CycFlow learns an instance-conditioned vector field that continuously transports input 2D coordinates to a canonical circular arrangement, where the optimal tour is recovered from this $2N$ dimensional representation via angular sorting. By leveraging data-dependent flow matching, we bypass the quadratic bottleneck of edge scoring in favor of linear coordinate dynamics. This paradigm shift accelerates solving speed by up to three orders of magnitude compared to state-of-the-art diffusion baselines, while maintaining competitive optimality gaps.</li>
<li><strong>摘要：</strong>神经组合优化 (NCO) 的最新进展主要由扩散模型主导，该模型将欧几里得旅行商问题 (TSP) 视为随机 $N \times N$ 热图生成任务。在本文中，我们提出了 CycFlow，一个用确定性点传输代替迭代边缘去噪的框架。 CycFlow 学习一个实例条件向量场，该向量场连续地将输入的 2D 坐标传输到规范的圆形排列，其中通过角度排序从这个 $2N$ 维度表示中恢复最佳游览。通过利用数据相关的流匹配，我们绕过了边缘评分的二次瓶颈，有利于线性坐标动态。与最先进的扩散基线相比，这种范式转变将求解速度加快了三个数量级，同时保持了竞争性最优差距。</li>
</ul>

<h3>Title: PRISM: Parallel Residual Iterative Sequence Model</h3>
<ul>
<li><strong>Authors: </strong>Jie Jiang, Ke Cheng, Xin Xu, Mengyang Pang, Tianhao Lu, Jiaheng Li, Yue Liu, Yuan Wang, Jun Zhang, Huan Yu, Zhouchen Lin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10796">https://arxiv.org/abs/2602.10796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10796">https://arxiv.org/pdf/2602.10796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10796]] PRISM: Parallel Residual Iterative Sequence Model(https://arxiv.org/abs/2602.10796)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative sequence modeling faces a fundamental tension between the expressivity of Transformers and the efficiency of linear sequence models. Existing efficient architectures are theoretically bounded by shallow, single-step linear updates, while powerful iterative methods like Test-Time Training (TTT) break hardware parallelism due to state-dependent gradients. We propose PRISM (Parallel Residual Iterative Sequence Model) to resolve this tension. PRISM introduces a solver-inspired inductive bias that captures key structural properties of multi-step refinement in a parallelizable form. We employ a Write-Forget Decoupling strategy that isolates non-linearity within the injection operator. To bypass the serial dependency of explicit solvers, PRISM utilizes a two-stage proxy architecture: a short-convolution anchors the initial residual using local history energy, while a learned predictor estimates the refinement updates directly from the input. This design distills structural patterns associated with iterative correction into a parallelizable feedforward operator. Theoretically, we prove that this formulation achieves Rank-$L$ accumulation, structurally expanding the update manifold beyond the single-step Rank-$1$ bottleneck. Empirically, it achieves comparable performance to explicit optimization methods while achieving 174x higher throughput.</li>
<li><strong>摘要：</strong>生成序列建模面临 Transformer 的表达能力和线性序列模型的效率之间的根本张力。现有的高效架构理论上受到浅层单步线性更新的限制，而测试时训练 (TTT) 等强大的迭代方法由于状态相关的梯度而打破了硬件并行性。我们提出 PRISM（并行残差迭代序列模型）来解决这种紧张关系。 PRISM 引入了受求解器启发的归纳偏置，以可并行的形式捕获多步细化的关键结构属性。我们采用 Write-Forget 解耦策略来隔离注入运算符内的非线性。为了绕过显式求解器的串行依赖性，PRISM 采用两阶段代理架构：短卷积使用局部历史能量锚定初始残差，而学习预测器直接从输入估计细化更新。该设计将与迭代校正相关的结构模式提炼为可并行前馈算子。理论上，我们证明该公式实现了 Rank-$L$ 累积，在结构上扩展了更新流形，超越了单步 Rank-$1$ 瓶颈。根据经验，它实现了与显式优化方法相当的性能，同时吞吐量提高了 174 倍。</li>
</ul>

<h3>Title: DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories</h3>
<ul>
<li><strong>Authors: </strong>Chenlong Deng, Mengjie Deng, Junjie Wu, Dun Zeng, Teng Wang, Qingsong Xie, Jiadeng Huang, Shengjie Ma, Changwang Zhang, Zhaoxiang Wang, Jun Wang, Yutao Zhu, Zhicheng Dou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10809">https://arxiv.org/abs/2602.10809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10809">https://arxiv.org/pdf/2602.10809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10809]] DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories(https://arxiv.org/abs/2602.10809)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Existing multimodal retrieval systems excel at semantic matching but implicitly assume that query-image relevance can be measured in isolation. This paradigm overlooks the rich dependencies inherent in realistic visual streams, where information is distributed across temporal sequences rather than confined to single snapshots. To bridge this gap, we introduce DeepImageSearch, a novel agentic paradigm that reformulates image retrieval as an autonomous exploration task. Models must plan and perform multi-step reasoning over raw visual histories to locate targets based on implicit contextual cues. We construct DISBench, a challenging benchmark built on interconnected visual data. To address the scalability challenge of creating context-dependent queries, we propose a human-model collaborative pipeline that employs vision-language models to mine latent spatiotemporal associations, effectively offloading intensive context discovery before human verification. Furthermore, we build a robust baseline using a modular agent framework equipped with fine-grained tools and a dual-memory system for long-horizon navigation. Extensive experiments demonstrate that DISBench poses significant challenges to state-of-the-art models, highlighting the necessity of incorporating agentic reasoning into next-generation retrieval systems.</li>
<li><strong>摘要：</strong>现有的多模态检索系统擅长语义匹配，但隐含地假设查询图像相关性可以单独测量。这种范例忽略了现实视觉流中固有的丰富依赖关系，其中信息分布在时间序列上，而不是局限于单个快照。为了弥补这一差距，我们引入了 DeepImageSearch，这是一种新颖的代理范式，它将图像检索重新定义为一项自主探索任务。模型必须对原始视觉历史进行规划和执行多步骤推理，以根据隐式上下文线索定位目标。我们构建了 DISBench，这是一个基于互连视觉数据的具有挑战性的基准。为了解决创建上下文相关查询的可扩展性挑战，我们提出了一种人类模型协作管道，该管道采用视觉语言模型来挖掘潜在的时空关联，从而在人类验证之前有效地卸载密集的上下文发现。此外，我们使用配备细粒度工具的模块化代理框架和用于长视野导航的双内存系统构建了强大的基线。大量实验表明 DISBench 对最先进的模型提出了重大挑战，凸显了将代理推理纳入下一代检索系统的必要性。</li>
</ul>

<h3>Title: Flow caching for autoregressive video generation</h3>
<ul>
<li><strong>Authors: </strong>Yuexiao Ma, Xuzhe Zheng, Jing Xu, Xiwei Xu, Feng Ling, Xiawu Zheng, Huafeng Kuang, Huixia Li, Xing Wang, Xuefeng Xiao, Fei Chao, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10825">https://arxiv.org/abs/2602.10825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10825">https://arxiv.org/pdf/2602.10825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10825]] Flow caching for autoregressive video generation(https://arxiv.org/abs/2602.10825)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Autoregressive models, often built on Transformer architectures, represent a powerful paradigm for generating ultra-long videos by synthesizing content in sequential chunks. However, this sequential generation process is notoriously slow. While caching strategies have proven effective for accelerating traditional video diffusion models, existing methods assume uniform denoising across all frames-an assumption that breaks down in autoregressive models where different video chunks exhibit varying similarity patterns at identical timesteps. In this paper, we present FlowCache, the first caching framework specifically designed for autoregressive video generation. Our key insight is that each video chunk should maintain independent caching policies, allowing fine-grained control over which chunks require recomputation at each timestep. We introduce a chunkwise caching strategy that dynamically adapts to the unique denoising characteristics of each chunk, complemented by a joint importance-redundancy optimized KV cache compression mechanism that maintains fixed memory bounds while preserving generation quality. Our method achieves remarkable speedups of 2.38 times on MAGI-1 and 6.7 times on SkyReels-V2, with negligible quality degradation (VBench: 0.87 increase and 0.79 decrease respectively). These results demonstrate that FlowCache successfully unlocks the potential of autoregressive models for real-time, ultra-long video generation-establishing a new benchmark for efficient video synthesis at scale. The code is available at this https URL.</li>
<li><strong>摘要：</strong>自回归模型通常建立在 Transformer 架构之上，代表了通过合成连续块中的内容来生成超长视频的强大范例。然而，这种顺序生成过程是出了名的慢。虽然缓存策略已被证明对于加速传统视频扩散模型是有效的，但现有方法假设所有帧都采用统一的去噪——这种假设在自回归模型中被打破，其中不同的视频块在相同的时间步长表现出不同的相似性模式。在本文中，我们介绍了 FlowCache，这是第一个专为自回归视频生成而设计的缓存框架。我们的主要见解是每个视频块应该维护独立的缓存策略，从而可以对每个时间步需要重新计算的块进行细粒度控制。我们引入了一种分块缓存策略，该策略动态适应每个块的独特去噪特性，并辅以联合重要性冗余优化的 KV 缓存压缩机制，该机制在保持固定内存边界的同时保持生成质量。我们的方法在 MAGI-1 上实现了 2.38 倍的显着加速，在 SkyReels-V2 上实现了 6.7 倍的显着加速，而质量下降可以忽略不计（VBench：分别增加 0.87 倍和减少 0.79 倍）。这些结果表明，FlowCache 成功释放了自回归模型在实时、超长视频生成方面的潜力，为大规模高效视频合成建立了新基准。该代码可从此 https URL 获取。</li>
</ul>

<h3>Title: SimuScene: Training and Benchmarking Code Generation to Simulate Physical Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Yanan Wang, Renxi Wang, Yongxin Wang, Xuezhi Liang, Fajri Koto, Timothy Baldwin, Xiaodan Liang, Haonan Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10840">https://arxiv.org/abs/2602.10840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10840">https://arxiv.org/pdf/2602.10840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10840]] SimuScene: Training and Benchmarking Code Generation to Simulate Physical Scenarios(https://arxiv.org/abs/2602.10840)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been extensively studied for tasks like math competitions, complex coding, and scientific reasoning, yet their ability to accurately represent and simulate physical scenarios via code remains underexplored. We propose SimuScene, the first systematic study that trains and evaluates LLMs on simulating physical scenarios across five physics domains and 52 physical concepts. We build an automatic pipeline to collect data, with human verification to ensure quality. The final dataset contains 7,659 physical scenarios with 334 human-verified examples as the test set. We evaluated 10 contemporary LLMs and found that even the strongest model achieves only a 21.5% pass rate, demonstrating the difficulty of the task. Finally, we introduce a reinforcement learning pipeline with visual rewards that uses a vision-language model as a judge to train textual models. Experiments show that training with our data improves physical simulation via code while substantially enhancing general code generation performance.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已针对数学竞赛、复杂编码和科学推理等任务进行了广泛研究，但它们通过代码准确表示和模拟物理场景的能力仍未得到充分开发。我们提出了 SimuScene，这是第一个系统研究，旨在训练和评估法学硕士模拟跨五个物理领域和 52 个物理概念的物理场景。我们建立了一个自动管道来收集数据，并通过人工验证来确保质量。最终数据集包含 7,659 个物理场景，其中有 334 个经过人工验证的示例作为测试集。我们评估了 10 位当代法学硕士，发现即使是最强的模型也只能达到 21.5% 的通过率，可见任务的难度。最后，我们引入了带有视觉奖励的强化学习管道，它使用视觉语言模型作为判断来训练文本模型。实验表明，使用我们的数据进行训练可以通过代码改进物理模拟，同时显着提高一般代码生成性能。</li>
</ul>

<h3>Title: Chart Specification: Structural Representations for Incentivizing VLM Reasoning in Chart-to-Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Minggui He, Mingchen Dai, Jian Zhang, Yilun Liu, Shimin Tao, Pufan Zeng, Osamu Yoshie, Yuya Ieiri</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10880">https://arxiv.org/abs/2602.10880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10880">https://arxiv.org/pdf/2602.10880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10880]] Chart Specification: Structural Representations for Incentivizing VLM Reasoning in Chart-to-Code Generation(https://arxiv.org/abs/2602.10880)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have shown promise in generating plotting code from chart images, yet achieving structural fidelity remains challenging. Existing approaches largely rely on supervised fine-tuning, encouraging surface-level token imitation rather than faithful modeling of underlying chart structure, which often leads to hallucinated or semantically inconsistent outputs. We propose Chart Specification, a structured intermediate representation that shifts training from text imitation to semantically grounded supervision. Chart Specification filters syntactic noise to construct a structurally balanced training set and supports a Spec-Align Reward that provides fine-grained, verifiable feedback on structural correctness, enabling reinforcement learning to enforce consistent plotting logic. Experiments on three public benchmarks show that our method consistently outperforms prior approaches. With only 3K training samples, we achieve strong data efficiency, surpassing leading baselines by up to 61.7% on complex benchmarks, and scaling to 4K samples establishes new state-of-the-art results across all evaluated metrics. Overall, our results demonstrate that precise structural supervision offers an efficient pathway to high-fidelity chart-to-code generation. Code and dataset are available at: this https URL</li>
<li><strong>摘要：</strong>视觉语言模型 (VLM) 在从图表图像生成绘图代码方面显示出了前景，但实现结构保真度仍然具有挑战性。现有的方法在很大程度上依赖于监督微调，鼓励表面级别的令牌模仿，而不是对底层图表结构的忠实建模，这通常会导致幻觉或语义不一致的输出。我们提出了图表规范，一种结构化的中间表示，将训练从文本模仿转变为基于语义的监督。图表规范过滤语法噪音以构建结构平衡的训练集，并支持规范对齐奖励，该奖励提供有关结构正确性的细粒度、可验证的反馈，使强化学习能够强制执行一致的绘图逻辑。对三个公共基准的实验表明，我们的方法始终优于先前的方法。仅通过 3K 训练样本，我们就实现了强大的数据效率，在复杂的基准测试中超出了领先基线高达 61.7%，并且扩展到 4K 样本在所有评估指标中建立了新的最先进的结果。总的来说，我们的结果表明，精确的结构监督为高保真图表到代码生成提供了一条有效的途径。代码和数据集可在以下位置获得：此 https URL</li>
</ul>

<h3>Title: CMAD: Cooperative Multi-Agent Diffusion via Stochastic Optimal Control</h3>
<ul>
<li><strong>Authors: </strong>Riccardo Barbano, Alexander Denker, Zeljko Kereta, Runchang Li, Francisco Vargas</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10933">https://arxiv.org/abs/2602.10933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10933">https://arxiv.org/pdf/2602.10933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10933]] CMAD: Cooperative Multi-Agent Diffusion via Stochastic Optimal Control(https://arxiv.org/abs/2602.10933)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation, generative</a></li>
<li><strong>Abstract: </strong>Continuous-time generative models have achieved remarkable success in image restoration and synthesis. However, controlling the composition of multiple pre-trained models remains an open challenge. Current approaches largely treat composition as an algebraic composition of probability densities, such as via products or mixtures of experts. This perspective assumes the target distribution is known explicitly, which is almost never the case. In this work, we propose a different paradigm that formulates compositional generation as a cooperative Stochastic Optimal Control problem. Rather than combining probability densities, we treat pre-trained diffusion models as interacting agents whose diffusion trajectories are jointly steered, via optimal control, toward a shared objective defined on their aggregated output. We validate our framework on conditional MNIST generation and compare it against a naive inference-time DPS-style baseline replacing learned cooperative control with per-step gradient guidance.</li>
<li><strong>摘要：</strong>连续时间生成模型在图像恢复和合成方面取得了显着的成功。然而，控制多个预训练模型的组成仍然是一个开放的挑战。当前的方法主要将组合视为概率密度的代数组合，例如通过乘积或专家的组合。这种观点假设目标分布是明确已知的，但事实几乎从未如此。在这项工作中，我们提出了一种不同的范式，将组合生成表述为协作随机最优控制问题。我们不是将概率密度结合起来，而是将预先训练的扩散模型视为交互代理，其扩散轨迹通过最优控制共同引导，以实现根据其聚合输出定义的共享目标。我们在条件 MNIST 生成上验证了我们的框架，并将其与朴素推理时间 DPS 风格的基线进行比较，用每步梯度指导取代学习的合作控制。</li>
</ul>

<h3>Title: Stochastic Parroting in Temporal Attention -- Regulating the Diagonal Sink</h3>
<ul>
<li><strong>Authors: </strong>Victoria Hankemeier, Malte Hankemeier</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10956">https://arxiv.org/abs/2602.10956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10956">https://arxiv.org/pdf/2602.10956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10956]] Stochastic Parroting in Temporal Attention -- Regulating the Diagonal Sink(https://arxiv.org/abs/2602.10956)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Spatio-temporal models analyze spatial structures and temporal dynamics, which makes them prone to information degeneration among space and time. Prior literature has demonstrated that over-squashing in causal attention or temporal convolutions creates a bias on the first tokens. To analyze whether such a bias is present in temporal attention mechanisms, we derive sensitivity bounds on the expected value of the Jacobian of a temporal attention layer. We theoretically show how off-diagonal attention scores depend on the sequence length, and that temporal attention matrices suffer a diagonal attention sink. We suggest regularization methods, and experimentally demonstrate their effectiveness.</li>
<li><strong>摘要：</strong>时空模型分析空间结构和时间动态，这使得它们容易出现空间和时间之间的信息退化。先前的文献已经证明，因果注意力或时间卷积的过度压缩会对第一个标记产生偏差。为了分析时间注意机制中是否存在这种偏差，我们推导了时间注意层的雅可比行列式的期望值的敏感性界限。我们从理论上证明了非对角注意力分数如何取决于序列长度，并且时间注意力矩阵遭受对角注意力下沉。我们提出正则化方法，并通过实验证明其有效性。</li>
</ul>

<h3>Title: Sample Efficient Generative Molecular Optimization with Joint Self-Improvement</h3>
<ul>
<li><strong>Authors: </strong>Serra Korkmaz, Adam Izdebski, Jonathan Pirnay, Rasmus Møller-Larsen, Michal Kmicikiewicz, Pankhil Gawade, Dominik G. Grimm, Ewa Szczurek</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10984">https://arxiv.org/abs/2602.10984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10984">https://arxiv.org/pdf/2602.10984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10984]] Sample Efficient Generative Molecular Optimization with Joint Self-Improvement(https://arxiv.org/abs/2602.10984)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative molecular optimization aims to design molecules with properties surpassing those of existing compounds. However, such candidates are rare and expensive to evaluate, yielding sample efficiency essential. Additionally, surrogate models introduced to predict molecule evaluations, suffer from distribution shift as optimization drives candidates increasingly out-of-distribution. To address these challenges, we introduce Joint Self-Improvement, which benefits from (i) a joint generative-predictive model and (ii) a self-improving sampling scheme. The former aligns the generator with the surrogate, alleviating distribution shift, while the latter biases the generative part of the joint model using the predictive one to efficiently generate optimized molecules at inference-time. Experiments across offline and online molecular optimization benchmarks demonstrate that Joint Self-Improvement outperforms state-of-the-art methods under limited evaluation budgets.</li>
<li><strong>摘要：</strong>生成分子优化旨在设计性能超越现有化合物的分子。然而，此类候选者很少见，评估成本也很高，因此样本效率至关重要。此外，为预测分子评估而引入的替代模型会受到分布变化的影响，因为优化导致候选者越来越脱离分布。为了应对这些挑战，我们引入了联合自我改进，它受益于（i）联合生成预测模型和（ii）自我改进抽样方案。前者将生成器与代理对齐，减轻分布偏移，而后者使用预测模型对联合模型的生成部分进行偏置，以在推理时有效地生成优化的分子。离线和在线分子优化基准的实验表明，在有限的评估预算下，联合自我改进优于最先进的方法。</li>
</ul>

<h3>Title: TVCACHE: A Stateful Tool-Value Cache for Post-Training LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Vijaya Kumar, Bhaskar Kataria, Byungsoo Oh, Emaad Manzoor, Rachee Singh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.10986">https://arxiv.org/abs/2602.10986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.10986">https://arxiv.org/pdf/2602.10986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.10986]] TVCACHE: A Stateful Tool-Value Cache for Post-Training LLM Agents(https://arxiv.org/abs/2602.10986)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In RL post-training of LLM agents, calls to external tools take several seconds or even minutes, leaving allocated GPUs idle and inflating post-training time and cost. While many tool invocations repeat across parallel rollouts and could in principle be cached, naively caching their outputs for reuse is incorrect since tool outputs depend on the environment state induced by prior agent interactions. We present TVCACHE, a stateful tool-value cache for LLM agent post-training. TVCACHE maintains a tree of observed tool-call sequences and performs longest-prefix matching for cache lookups: a hit occurs only when the agent's full tool history matches a previously executed sequence, guaranteeing identical environment state. On three diverse workloads-terminal-based tasks, SQL generation, and video understanding. TVCACHE achieves cache hit rates of up to 70% and reduces median tool call execution time by up to 6.9X, with no degradation in post-training reward accumulation.</li>
<li><strong>摘要：</strong>在 LLM 代理的强化学习后期训练中，调用外部工具需要几秒钟甚至几分钟的时间，导致分配的 GPU 闲置，并增加训练后的时间和成本。虽然许多工具调用在并行部署中重复，并且原则上可以缓存，但天真地缓存其输出以供重用是不正确的，因为工具输出取决于先前代理交互引起的环境状态。我们推出了 TVCACHE，这是一种用于 LLM 代理后期训练的有状态工具值缓存。 TVCACHE 维护观察到的工具调用序列树，并执行缓存查找的最长前缀匹配：仅当代理的完整工具历史记录与先前执行的序列匹配时才会发生命中，从而保证相同的环境状态。关于三种不同的工作负载——基于终端的任务、SQL 生成和视频理解。 TVCACHE 实现了高达 70% 的缓存命中率，并将工具调用执行时间中位数减少了 6.9 倍，并且训练后奖励积累没有下降。</li>
</ul>

<h3>Title: First International StepUP Competition for Biometric Footstep Recognition: Methods, Results and Remaining Challenges</h3>
<ul>
<li><strong>Authors: </strong>Robyn Larracy, Eve MacDonald, Angkoon Phinyomark, Saeid Rezaei, Mahdi Laghaei, Ali Hajighasem, Aaron Tabor, Erik Scheme</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11086">https://arxiv.org/abs/2602.11086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11086">https://arxiv.org/pdf/2602.11086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11086]] First International StepUP Competition for Biometric Footstep Recognition: Methods, Results and Remaining Challenges(https://arxiv.org/abs/2602.11086)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Biometric footstep recognition, based on a person's unique pressure patterns under their feet during walking, is an emerging field with growing applications in security and safety. However, progress in this area has been limited by the lack of large, diverse datasets necessary to address critical challenges such as generalization to new users and robustness to shifts in factors like footwear or walking speed. The recent release of the UNB StepUP-P150 dataset, the largest and most comprehensive collection of high-resolution footstep pressure recordings to date, opens new opportunities for addressing these challenges through deep learning. To mark this milestone, the First International StepUP Competition for Biometric Footstep Recognition was launched. Competitors were tasked with developing robust recognition models using the StepUP-P150 dataset that were then evaluated on a separate, dedicated test set designed to assess verification performance under challenging variations, given limited and relatively homogeneous reference data. The competition attracted global participation, with 23 registered teams from academia and industry. The top-performing team, Saeid_UCC, achieved the best equal error rate (EER) of 10.77% using a generative reward machine (GRM) optimization strategy. Overall, the competition showcased strong solutions, but persistent challenges in generalizing to unfamiliar footwear highlight a critical area for future work.</li>
<li><strong>摘要：</strong>生物足迹识别基于人行走时脚下独特的压力模式，是一个新兴领域，在安防领域的应用不断增长。然而，由于缺乏解决关键挑战所需的大型、多样化的数据集，例如对新用户的泛化以及对鞋类或步行速度等因素变化的鲁棒性，该领域的进展受到限制。最近发布的 UNB StepUP-P150 数据集是迄今为止最大、最全面的高分辨率脚步压力记录集合，为通过深度学习应对这些挑战提供了新的机会。为了纪念这一里程碑，首届国际脚步生物识别 StepUP 竞赛启动了。参赛者的任务是使用 StepUP-P150 数据集开发强大的识别模型，然后在单独的专用测试集上进行评估，该测试集旨在评估具有挑战性的变化下的验证性能，并给出有限且相对同质的参考数据。本次大赛吸引了全球的参与，共有来自学术界和工业界的23支参赛队伍报名。表现最好的团队 Saeid_UCC 使用生成奖励机 (GRM) 优化策略实现了 10.77% 的最佳等错误率 (EER)。总体而言，比赛展示了强大的解决方案，但在推广到不熟悉的鞋类方面持续存在的挑战凸显了未来工作的关键领域。</li>
</ul>

<h3>Title: MerLin: A Discovery Engine for Photonic and Hybrid Quantum Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Cassandre Notton, Benjamin Stott, Philippe Schoeb, Anthony Walsh, Grégoire Leboucher, Vincent Espitalier, Vassilis Apostolou, Louis-Félix Vigneux, Alexia Salavrakos, Jean Senellart</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.PL, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11092">https://arxiv.org/abs/2602.11092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11092">https://arxiv.org/pdf/2602.11092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11092]] MerLin: A Discovery Engine for Photonic and Hybrid Quantum Machine Learning(https://arxiv.org/abs/2602.11092)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Identifying where quantum models may offer practical benefits in near term quantum machine learning (QML) requires moving beyond isolated algorithmic proposals toward systematic and empirical exploration across models, datasets, and hardware constraints. We introduce MerLin, an open source framework designed as a discovery engine for photonic and hybrid quantum machine learning. MerLin integrates optimized strong simulation of linear optical circuits into standard PyTorch and scikit learn workflows, enabling end to end differentiable training of quantum layers. MerLin is designed around systematic benchmarking and reproducibility. As an initial contribution, we reproduce eighteen state of the art photonic and hybrid QML works spanning kernel methods, reservoir computing, convolutional and recurrent architectures, generative models, and modern training paradigms. These reproductions are released as reusable, modular experiments that can be directly extended and adapted, establishing a shared experimental baseline consistent with empirical benchmarking methodologies widely adopted in modern artificial intelligence. By embedding photonic quantum models within established machine learning ecosystems, MerLin allows practitioners to leverage existing tooling for ablation studies, cross modality comparisons, and hybrid classical quantum workflows. The framework already implements hardware aware features, allowing tests on available quantum hardware while enabling exploration beyond its current capabilities, positioning MerLin as a future proof co design tool linking algorithms, benchmarks, and hardware.</li>
<li><strong>摘要：</strong>确定量子模型在哪些方面可以在近期量子机器学习（QML）中提供实际好处，需要超越孤立的算法建议，转向跨模型、数据集和硬件限制的系统和实证探索。我们介绍 MerLin，这是一个开源框架，旨在作为光子和混合量子机器学习的发现引擎。 MerLin 将线性光学电路的优化强模拟集成到标准 PyTorch 和 scikit learn 工作流程中，从而实现量子层的端到端可微分训练。 MerLin 是围绕系统基准测试和可重复性而设计的。作为最初的贡献，我们重现了 18 个最先进的光子和混合 QML 作品，涵盖核方法、存储计算、卷积和循环架构、生成模型和现代训练范例。这些复制品作为可重复使用的模块化实验发布，可以直接扩展和调整，建立与现代人工智能广泛采用的经验基准测试方法一致的共享实验基线。通过将光子量子模型嵌入已建立的机器学习生态系统中，MerLin 允许从业者利用现有工具进行消融研究、跨模态比较和混合经典量子工作流程。该框架已经实现了硬件感知功能，允许在可用的量子硬件上进行测试，同时能够进行超出其当前功能的探索，将 MerLin 定位为链接算法、基准测试和硬件的面向未来的协同设计工具。</li>
</ul>

<h3>Title: FastFlow: Accelerating The Generative Flow Matching Models with Bandit Inference</h3>
<ul>
<li><strong>Authors: </strong>Divya Jyoti Bajpai, Dhruv Bhardwaj, Soumya Roy, Tejas Duseja, Harsh Agarwal, Aashay Sandansing, Manjesh Kumar Hanawal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11105">https://arxiv.org/abs/2602.11105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11105">https://arxiv.org/pdf/2602.11105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11105]] FastFlow: Accelerating The Generative Flow Matching Models with Bandit Inference(https://arxiv.org/abs/2602.11105)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Flow-matching models deliver state-of-the-art fidelity in image and video generation, but the inherent sequential denoising process renders them slower. Existing acceleration methods like distillation, trajectory truncation, and consistency approaches are static, require retraining, and often fail to generalize across tasks. We propose FastFlow, a plug-and-play adaptive inference framework that accelerates generation in flow matching models. FastFlow identifies denoising steps that produce only minor adjustments to the denoising path and approximates them without using the full neural network models used for velocity predictions. The approximation utilizes finite-difference velocity estimates from prior predictions to efficiently extrapolate future states, enabling faster advancements along the denoising path at zero compute cost. This enables skipping computation at intermediary steps. We model the decision of how many steps to safely skip before requiring a full model computation as a multi-armed bandit problem. The bandit learns the optimal skips to balance speed with performance. FastFlow integrates seamlessly with existing pipelines and generalizes across image generation, video generation, and editing tasks. Experiments demonstrate a speedup of over 2.6x while maintaining high-quality outputs. The source code for this work can be found at this https URL.</li>
<li><strong>摘要：</strong>流匹配模型在图像和视频生成中提供最先进的保真度，但固有的顺序去噪过程使它们速度变慢。现有的加速方法（例如蒸馏、轨迹截断和一致性方法）是静态的，需要重新训练，并且通常无法跨任务泛化。我们提出了 FastFlow，一种即插即用的自适应推理框架，可加速流匹配模型的生成。 FastFlow 识别仅对去噪路径产生微小调整的去噪步骤，并在不使用用于速度预测的完整神经网络模型的情况下对其进行近似。该近似利用先前预测的有限差分速度估计来有效地推断未来状态，从而以零计算成本沿着去噪路径实现更快的进展。这使得能够跳过中间步骤的计算。我们将在需要完整模型计算之前安全跳过多少步骤的决策建模为多臂老虎机问题。老虎机学习最佳跳跃以平衡速度与性能。 FastFlow 与现有管道无缝集成，并可泛化图像生成、视频生成和编辑任务。实验表明，在保持高质量输出的同时，速度提高了 2.6 倍以上。这项工作的源代码可以在此 https URL 中找到。</li>
</ul>

<h3>Title: HairWeaver: Few-Shot Photorealistic Hair Motion Synthesis with Sim-to-Real Guided Video Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Di Chang, Ji Hou, Aljaz Bozic, Assaf Neuberger, Felix Juefei-Xu, Olivier Maury, Gene Wei-Chin Lin, Tuur Stuyck, Doug Roble, Mohammad Soleymani, Stephane Grabli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11117">https://arxiv.org/abs/2602.11117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11117">https://arxiv.org/pdf/2602.11117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11117]] HairWeaver: Few-Shot Photorealistic Hair Motion Synthesis with Sim-to-Real Guided Video Diffusion(https://arxiv.org/abs/2602.11117)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present HairWeaver, a diffusion-based pipeline that animates a single human image with realistic and expressive hair dynamics. While existing methods successfully control body pose, they lack specific control over hair, and as a result, fail to capture the intricate hair motions, resulting in stiff and unrealistic animations. HairWeaver overcomes this limitation using two specialized modules: a Motion-Context-LoRA to integrate motion conditions and a Sim2Real-Domain-LoRA to preserve the subject's photoreal appearance across different data domains. These lightweight components are designed to guide a video diffusion backbone while maintaining its core generative capabilities. By training on a specialized dataset of dynamic human motion generated from a CG simulator, HairWeaver affords fine control over hair motion and ultimately learns to produce highly realistic hair that responds naturally to movement. Comprehensive evaluations demonstrate that our approach sets a new state of the art, producing lifelike human hair animations with dynamic details.</li>
<li><strong>摘要：</strong>我们推出了 HairWeaver，这是一种基于扩散的管道，可以通过逼真且富有表现力的头发动态来对单个人类图像进行动画处理。虽然现有方法成功地控制了身体姿势，但它们缺乏对头发的具体控制，因此无法捕捉复杂的头发运动，导致动画僵硬且不切实际。 HairWeaver 使用两个专用模块克服了这一限制：一个用于集成运动条件的 Motion-Context-LoRA，另一个是 Sim2Real-Domain-LoRA，用于在不同数据域中保留主体的真实外观。这些轻量级组件旨在指导视频传播主干，同时保持其核心生成功能。通过对 CG 模拟器生成的动态人体运动的专门数据集进行训练，HairWeaver 可以对头发运动进行精细控制，并最终学会生成对运动做出自然响应的高度逼真的头发。综合评估表明，我们的方法树立了新的技术水平，可以制作具有动态细节的逼真的人发动画。</li>
</ul>

<h3>Title: From Natural Language to Materials Discovery:The Materials Knowledge Navigation Agent</h3>
<ul>
<li><strong>Authors: </strong>Genmao Zhuang, Amir Barati Farimani</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11123">https://arxiv.org/abs/2602.11123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11123">https://arxiv.org/pdf/2602.11123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11123]] From Natural Language to Materials Discovery:The Materials Knowledge Navigation Agent(https://arxiv.org/abs/2602.11123)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accelerating the discovery of high-performance materials remains a central challenge across energy, electronics, and aerospace technologies, where traditional workflows depend heavily on expert intuition and computationally expensive simulations. Here we introduce the Materials Knowledge Navigation Agent (MKNA), a language-driven system that translates natural-language scientific intent into executable actions for database retrieval, property prediction, structure generation, and stability evaluation. Beyond automating tool invocation, MKNA autonomously extracts quantitative thresholds and chemically meaningful design motifs from literature and database evidence, enabling data-grounded hypothesis formation. Applied to the search for high-Debye-temperature ceramics, the agent identifies a literature-supported screening criterion (Theta_D > 800 K), rediscovers canonical ultra-stiff materials such as diamond, SiC, SiN, and BeO, and proposes thermodynamically stable, previously unreported Be-C-rich compounds that populate the sparsely explored 1500-1700 K regime. These results demonstrate that MKNA not only finds stable candidates but also reconstructs interpretable design heuristics, establishing a generalizable platform for autonomous, language-guided materials exploration.</li>
<li><strong>摘要：</strong>加速高性能材料的发现仍然是能源、电子和航空航天技术领域的核心挑战，这些技术的传统工作流程在很大程度上依赖于专家的直觉和计算成本高昂的模拟。在这里，我们介绍材料知识导航代理（MKNA），这是一种语言驱动的系统，可将自然语言的科学意图转化为可执行的操作，用于数据库检索、属性预测、结构生成和稳定性评估。除了自动化工具调用之外，MKNA 还可以从文献和数据库证据中自主提取定量阈值和具有化学意义的设计主题，从而能够形成基于数据的假设。应用于寻找高德拜温度陶瓷时，该代理确定了文献支持的筛选标准（Theta_D > 800 K），重新发现了典型的超硬材料，例如金刚石、SiC、SiN 和 BeO，并提出了热力学稳定、先前未报道的富含 Be-C 的化合物，这些化合物填充了很少探索的 1500-1700 K 范围。这些结果表明，MKNA 不仅找到了稳定的候选者，而且还重建了可解释的设计启发式，为自主的、语言引导的材料探索建立了一个通用平台。</li>
</ul>

<h3>Title: The Offline-Frontier Shift: Diagnosing Distributional Limits in Generative Multi-Objective Optimization</h3>
<ul>
<li><strong>Authors: </strong>Stephanie Holly, Alexandru-Ciprian Zăvoianu, Siegfried Silber, Sepp Hochreiter, Werner Zellinger</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11126">https://arxiv.org/abs/2602.11126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11126">https://arxiv.org/pdf/2602.11126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11126]] The Offline-Frontier Shift: Diagnosing Distributional Limits in Generative Multi-Objective Optimization(https://arxiv.org/abs/2602.11126)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Offline multi-objective optimization (MOO) aims to recover Pareto-optimal designs given a finite, static dataset. Recent generative approaches, including diffusion models, show strong performance under hypervolume, yet their behavior under other established MOO metrics is less understood. We show that generative methods systematically underperform evolutionary alternatives with respect to other metrics, such as generational distance. We relate this failure mode to the offline-frontier shift, i.e., the displacement of the offline dataset from the Pareto front, which acts as a fundamental limitation in offline MOO. We argue that overcoming this limitation requires out-of-distribution sampling in objective space (via an integral probability metric) and empirically observe that generative methods remain conservatively close to the offline objective distribution. Our results position offline MOO as a distribution-shift--limited problem and provide a diagnostic lens for understanding when and why generative optimization methods fail.</li>
<li><strong>摘要：</strong>离线多目标优化 (MOO) 旨在在给定有限静态数据集的情况下恢复帕累托最优设计。最近的生成方法（包括扩散模型）在超容量下表现出强大的性能，但它们在其他已建立的 MOO 指标下的行为却知之甚少。我们表明，就其他指标（例如代际距离）而言，生成方法的表现系统地低于进化替代方法。我们将这种故障模式与离线前沿偏移联系起来，即离线数据集从帕累托前沿的位移，这是离线 MOO 的基本限制。我们认为，克服这一限制需要在客观空间中进行分布外采样（通过积分概率度量），并根据经验观察生成方法仍然保守地接近离线目标分布。我们的结果将离线 MOO 定位为分布转移受限问题，并为理解生成优化方法何时以及为何失败提供了诊断视角。</li>
</ul>

<h3>Title: Just on Time: Token-Level Early Stopping for Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zahar Kohut, Severyn Shykula, Dmytro Khamula, Mykola Vysotskyi, Taras Rumezhak, Volodymyr Karpiv</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11133">https://arxiv.org/abs/2602.11133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11133">https://arxiv.org/pdf/2602.11133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11133]] Just on Time: Token-Level Early Stopping for Diffusion Language Models(https://arxiv.org/abs/2602.11133)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion language models generate text through iterative refinement, a process that is often computationally inefficient because many tokens reach stability long before the final denoising step. We introduce a training-free, token-level early stopping approach that identifies convergence independently at each position. Our method leverages lightweight signals derived from the model's predictions and local context to dynamically determine when individual tokens can be finalized. This yields adaptive per-token freezing without task-specific fine-tuning, substantially reducing the total number of diffusion steps required. Across diverse benchmarks, spanning mathematical reasoning, general question answering, and scientific understanding, our approach achieves state-of-the-art efficiency gains while preserving generation quality.</li>
<li><strong>摘要：</strong>扩散语言模型通过迭代细化生成文本，这一过程通常计算效率低下，因为许多标记在最终去噪步骤之前很久就达到了稳定。我们引入了一种免训练的令牌级早期停止方法，该方法可以独立识别每个位置的收敛性。我们的方法利用从模型的预测和本地上下文中得出的轻量级信号来动态确定各个令牌何时可以最终确定。这会产生自适应的每个令牌冻结，而无需针对特定任务进行微调，从而大大减少所需的扩散步骤总数。跨越数学推理、一般问题解答和科学理解等不同的基准，我们的方法在保持发电质量的同时实现了最先进的效率增益。</li>
</ul>

<h3>Title: TabICLv2: A better, faster, scalable, and open tabular foundation model</h3>
<ul>
<li><strong>Authors: </strong>Jingang Qu, David Holzmüller, Gaël Varoquaux, Marine Le Morvan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11139">https://arxiv.org/abs/2602.11139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11139">https://arxiv.org/pdf/2602.11139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11139]] TabICLv2: A better, faster, scalable, and open tabular foundation model(https://arxiv.org/abs/2602.11139)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Tabular foundation models, such as TabPFNv2 and TabICL, have recently dethroned gradient-boosted trees at the top of predictive benchmarks, demonstrating the value of in-context learning for tabular data. We introduce TabICLv2, a new state-of-the-art foundation model for regression and classification built on three pillars: (1) a novel synthetic data generation engine designed for high pretraining diversity; (2) various architectural innovations, including a new scalable softmax in attention improving generalization to larger datasets without prohibitive long-sequence pretraining; and (3) optimized pretraining protocols, notably replacing AdamW with the Muon optimizer. On the TabArena and TALENT benchmarks, TabICLv2 without any tuning surpasses the performance of the current state of the art, RealTabPFN-2.5 (hyperparameter-tuned, ensembled, and fine-tuned on real data). With only moderate pretraining compute, TabICLv2 generalizes effectively to million-scale datasets under 50GB GPU memory while being markedly faster than RealTabPFN-2.5. We provide extensive ablation studies to quantify these contributions and commit to open research by first releasing inference code and model weights at this https URL, with synthetic data engine and pretraining code to follow.</li>
<li><strong>摘要：</strong>表格基础模型（例如 TabPFNv2 和 TabICL）最近取代了梯度提升树，成为预测基准的榜首，展示了表格数据上下文学习的价值。我们推出了 TabICLv2，这是一种基于三个支柱的新型最先进的回归和分类基础模型：（1）专为高预训练多样性而设计的新型合成数据生成引擎； (2) 各种架构创新，包括新的可扩展的 softmax 注意力机制，可改善对更大数据集的泛化，而无需进行令人望而却步的长序列预训练； (3) 优化预训练协议，特别是用 Muon 优化器替换 AdamW。在 TabArena 和 TALENT 基准测试中，无需任何调整的 TabICLv2 就超越了当前最先进的 RealTabPFN-2.5（对真实数据进行超参数调整、集成和微调）的性能。只需适度的预训练计算，TabICLv2 就可以有效地推广到 50GB GPU 内存下的百万级数据集，同时速度明显快于 RealTabPFN-2.5。我们提供广泛的消融研究来量化这些贡献，并致力于开放研究，首先在此 https URL 上发布推理代码和模型权重，随后提供合成数据引擎和预训练代码。</li>
</ul>

<h3>Title: GENIUS: Generative Fluid Intelligence Evaluation Suite</h3>
<ul>
<li><strong>Authors: </strong>Ruichuan An, Sihan Yang, Ziyu Guo, Wei Dai, Zijun Shen, Haodong Li, Renrui Zhang, Xinyu Wei, Guopeng Li, Wenshan Wu, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.11144">https://arxiv.org/abs/2602.11144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.11144">https://arxiv.org/pdf/2602.11144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.11144]] GENIUS: Generative Fluid Intelligence Evaluation Suite(https://arxiv.org/abs/2602.11144)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess $\textit{Crystallized Intelligence}$, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks $\textit{Generative Fluid Intelligence (GFI)}$: the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce $\textbf{GENIUS}$ ($\textbf{GEN}$ Fluid $\textbf{I}$ntelligence Eval$\textbf{U}$ation $\textbf{S}$uite). We formalize $\textit{GFI}$ as a synthesis of three primitives. These include $\textit{Inducing Implicit Patterns}$ (e.g., inferring personalized visual preferences), $\textit{Executing Ad-hoc Constraints}$ (e.g., visualizing abstract metaphors), and $\textit{Adapting to Contextual Knowledge}$ (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, $\textbf{GENIUS}$ establishes a rigorous standard for $\textit{GFI}$, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: $\href{this https URL}{this https URL}$.</li>
<li><strong>摘要：</strong>统一多模态模型（UMM）在视觉生成方面取得了显着的进步。然而，现有的基准主要评估$\textit{结晶智能}$，它依赖于回忆积累的知识和学习的模式。这种关注忽略了$\textit{生成流体智能（GFI）}$：归纳模式、通过约束进行推理以及动态适应新场景的能力。为了严格评估这种能力，我们引入了$\textbf{GENIUS}$ ($\textbf{GEN}$ Fluid $\textbf{I}$ntelligence Eval$\textbf{U}$ation $\textbf{S}$uite)。我们将 $\textit{GFI}$ 形式化为三个原语的综合。其中包括$\textit{诱导隐式模式}$（例如，推断个性化视觉偏好）、$\textit{执行临时约束}$（例如，可视化抽象隐喻）和$\textit{适应上下文知识}$（例如，模拟反直觉物理）。总的来说，这些原语挑战模型来解决完全基于直接上下文的问题。我们对 12 个代表性模型的系统评估揭示了这些任务的显着性能缺陷。至关重要的是，我们的诊断分析理清了这些故障模式。它表明缺陷源于有限的情境理解，而不是内在的生成能力不足。为了弥补这一差距，我们提出了一种无需培训的注意力干预策略。最终，$\textbf{GENIUS}$ 为 $\textit{GFI}$ 建立了严格的标准，引导该领域超越知识利用，走向动态、通用推理。我们的数据集和代码将发布在：$\href{this https URL}{this https URL}$。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
