<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-16</h1>
<h3>Title: The 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real): Methods and Results</h3>
<ul>
<li><strong>Authors: </strong>Qiuyu Chen, Xin Jin, Yue Song, Xihui Liu, Shuai Yang, Tao Yang, Ziqiang Li, Jianguo Huang, Yuntao Wei, Ba'ao Xie, Nicu Sebe, Wenjun (Kevin)Zeng, Jooyeol Yun, Davide Abati, Mohamed Omran, Jaegul Choo, Amir Habibian, Auke Wiggers, Masato Kobayashi, Ning Ding, Toru Tamaki, Marzieh Gheisari, Auguste Genovesio, Yuheng Chen, Dingkun Liu, Xinyao Yang, Xinping Xu, Baicheng Chen, Dongrui Wu, Junhao Geng, Lexiang Lv, Jianxin Lin, Hanzhe Liang, Jie Zhou, Xuanxin Chen, Jinbao Wang, Can Gao, Zhangyi Wang, Zongze Li, Bihan Wen, Yixin Gao, Xiaohan Pan, Xin Li, Zhibo Chen, Baorui Peng, Zhongming Chen, Haoran Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10463">https://arxiv.org/abs/2509.10463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10463">https://arxiv.org/pdf/2509.10463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10463]] The 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real): Methods and Results(https://arxiv.org/abs/2509.10463)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper reviews the 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real), held in conjunction with ICCV 2025. The workshop aimed to bridge the gap between the theoretical promise of Disentangled Representation Learning (DRL) and its application in realistic scenarios, moving beyond synthetic benchmarks. DRL4Real focused on evaluating DRL methods in practical applications such as controllable generation, exploring advancements in model robustness, interpretability, and generalization. The workshop accepted 9 papers covering a broad range of topics, including the integration of novel inductive biases (e.g., language), the application of diffusion models to DRL, 3D-aware disentanglement, and the expansion of DRL into specialized domains like autonomous driving and EEG analysis. This summary details the workshop's objectives, the themes of the accepted papers, and provides an overview of the methodologies proposed by the authors.</li>
<li><strong>摘要：</strong>本文回顾了与ICCV 2025合作举行的关于可控生成的第1个国际代表性学习（DRL4REAL）。该讲习班旨在弥合分解代表学习的理论承诺（DRL）之间的差距（DRL）及其在现实场景中的应用，超越合成基础。 DRL4REAL的重点是评估可控生成等实际应用中的DRL方法，探索模型鲁棒性，可解释性和概括方面的进步。该研讨会接受了9篇论文，其中涵盖了广泛的主题，包括整合新型的电感偏见（例如语言），扩散模型在DRL中的应用，3D感知的分解以及将DRL扩展到专用领域（如自主驱动和EEG分析）等专业域。该摘要详细介绍了研讨会的目标，即接受论文的主题，并概述了作者提出的方法。</li>
</ul>

<h3>Title: Exploring Multi-view Symbolic Regression methods in physical sciences</h3>
<ul>
<li><strong>Authors: </strong>Etienne Russeil, Fabrício Olivetti de França, Konstantin Malanchev, Guillaume Moinard, Maxime Cherrey</a></li>
<li><strong>Subjects: </strong>cs.LG, astro-ph.GA, astro-ph.IM, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10500">https://arxiv.org/abs/2509.10500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10500">https://arxiv.org/pdf/2509.10500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10500]] Exploring Multi-view Symbolic Regression methods in physical sciences(https://arxiv.org/abs/2509.10500)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Describing the world behavior through mathematical functions help scientists to achieve a better understanding of the inner mechanisms of different phenomena. Traditionally, this is done by deriving new equations from first principles and careful observations. A modern alternative is to automate part of this process with symbolic regression (SR). The SR algorithms search for a function that adequately fits the observed data while trying to enforce sparsity, in the hopes of generating an interpretable equation. A particularly interesting extension to these algorithms is the Multi-view Symbolic Regression (MvSR). It searches for a parametric function capable of describing multiple datasets generated by the same phenomena, which helps to mitigate the common problems of overfitting and data scarcity. Recently, multiple implementations added support to MvSR with small differences between them. In this paper, we test and compare MvSR as supported in Operon, PySR, phy-SO, and eggp, in different real-world datasets. We show that they all often achieve good accuracy while proposing solutions with only few free parameters. However, we find that certain features enable a more frequent generation of better models. We conclude by providing guidelines for future MvSR developments.</li>
<li><strong>摘要：</strong>通过数学功能来描述世界行为，有助于科学家更好地了解不同现象的内部机制。传统上，这是通过从第一原则和仔细观察的新方程来得出的。一种现代的选择是通过符号回归（SR）自动化该过程的一部分。 SR算法在试图实施稀疏性的同时，搜索一个充分适合观察到的数据的函数，以期生成可解释的方程。对这些算法的一个特别有趣的扩展是多视图符号回归（MVSR）。它搜索了能够描述相同现象生成的多个数据集的参数函数，这有助于减轻过度拟合和数据稀缺性的常见问题。最近，多个实现增加了对MVSR的支持，它们之间存在很小的差异。在本文中，我们在不同的现实世界数据集中测试和比较操纵子，PYSR，PHY-SO和EGGP中支持的MVSR。我们表明，他们通常都可以实现良好的准确性，同时只有很少的免费参数提出解决方案。但是，我们发现某些功能可以使更频繁地生成更好的模型。我们通过为未来的MVSR发展提供准则来结束。</li>
</ul>

<h3>Title: The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback</h3>
<ul>
<li><strong>Authors: </strong>Sai Teja Reddy Adapala</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10509">https://arxiv.org/abs/2509.10509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10509">https://arxiv.org/pdf/2509.10509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10509]] The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback(https://arxiv.org/abs/2509.10509)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The stability of recursively trained large language models (LLMs) is a foundational problem for AI safety. Prevailing theory predicts model collapse, a progressive degradation when models are trained on their own output. We challenge this narrative by introducing a selective feedback mechanism. Contrary to expectation, instead of merely slowing decay, our experiments provide strong evidence that this pressure reverses it, inducing a statistically significant performance improvement in a Gemma 2B model on a complex summarization task. We name this phenomenon the Anti-Ouroboros Effect. We contrast this with a foundational experiment using a simple classifier, where the theoretical degenerative loop was validated, highlighting the unique dynamics of high-dimensional models. Our findings establish that systemic resilience can be an emergent property of LLMs under simple selection pressure, suggesting a powerful and scalable principle for developing safer and more robust AI systems. Across five generations, a quality-filtered condition improved by 6.6% in ROUGE-L F1 score, whereas an unfiltered control degraded by 3.5% and a random-filter control degraded by 4.2%</li>
<li><strong>摘要：</strong>经过递归训练的大语言模型（LLM）的稳定性是AI安全的基本问题。盛行的理论预测模型崩溃，这是当模型受到自己的产出训练时的进行性降解。我们通过引入选择性反馈机制来挑战这种叙述。与期望相反，我们的实验不仅减慢了衰减，还提供了有力的证据表明，这种压力会逆转衰减，从而在复杂的汇总任务中引起了Gemma 2b模型的统计学意义性能改善。我们将这种现象命名为抗iroboros效应。我们将其与使用简单分类器的基础实验进行对比，在该实验中，理论退化环经过验证，突出了高维模型的独特动力学。我们的发现表明，在简单的选择压力下，系统的弹性可能是LLM的新兴属性，这表明了一个有力且可扩展的原则，用于开发更安全，更健壮的AI系统。在五代人中，我们胭脂-l F1分数的质量过滤条件提高了6.6％，而未经过滤的控制降低了3.5％，随机过滤器控制降低了4.2％</li>
</ul>

<h3>Title: Multimodal Deep Learning for ATCO Command Lifecycle Modeling and Workload Prediction</h3>
<ul>
<li><strong>Authors: </strong>Kaizhen Tan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10522">https://arxiv.org/abs/2509.10522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10522">https://arxiv.org/pdf/2509.10522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10522]] Multimodal Deep Learning for ATCO Command Lifecycle Modeling and Workload Prediction(https://arxiv.org/abs/2509.10522)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Air traffic controllers (ATCOs) issue high-intensity voice commands in dense airspace, where accurate workload modeling is critical for safety and efficiency. This paper proposes a multimodal deep learning framework that integrates structured data, trajectory sequences, and image features to estimate two key parameters in the ATCO command lifecycle: the time offset between a command and the resulting aircraft maneuver, and the command duration. A high-quality dataset was constructed, with maneuver points detected using sliding window and histogram-based methods. A CNN-Transformer ensemble model was developed for accurate, generalizable, and interpretable predictions. By linking trajectories to voice commands, this work offers the first model of its kind to support intelligent command generation and provides practical value for workload assessment, staffing, and scheduling.</li>
<li><strong>摘要：</strong>空中空间中的空中流量控制器（ATCO）发出高强度的语音命令，在该空域中，准确的工作负载建模对于安全性和效率至关重要。本文提出了一个多模式深度学习框架，该框架集成了结构化数据，轨迹序列和图像功能，以估算ATCO命令命令生命周期中的两个关键参数：命令和由此产生的飞机操纵和命令持续时间之间的时间偏移。构建了一个高质量的数据集，使用滑动窗口和基于直方图的方法检测到机动点。开发了CNN转换器集合模型，以进行准确，可推广和可解释的预测。通过将轨迹链接到语音命令，这项工作提供了同类的第一个模型，以支持智能命令生成，并为工作负载评估，人员配备和调度提供实用价值。</li>
</ul>

<h3>Title: Mitigating Catastrophic Forgetting and Mode Collapse in Text-to-Image Diffusion via Latent Replay</h3>
<ul>
<li><strong>Authors: </strong>Aoi Otani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10529">https://arxiv.org/abs/2509.10529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10529">https://arxiv.org/pdf/2509.10529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10529]] Mitigating Catastrophic Forgetting and Mode Collapse in Text-to-Image Diffusion via Latent Replay(https://arxiv.org/abs/2509.10529)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Continual learning -- the ability to acquire knowledge incrementally without forgetting previous skills -- is fundamental to natural intelligence. While the human brain excels at this, artificial neural networks struggle with "catastrophic forgetting," where learning new tasks erases previously acquired knowledge. This challenge is particularly severe for text-to-image diffusion models, which generate images from textual prompts. Additionally, these models face "mode collapse," where their outputs become increasingly repetitive over time. To address these challenges, we apply Latent Replay, a neuroscience-inspired approach, to diffusion models. Traditional replay methods mitigate forgetting by storing and revisiting past examples, typically requiring large collections of images. Latent Replay instead retains only compact, high-level feature representations extracted from the model's internal architecture. This mirrors the hippocampal process of storing neural activity patterns rather than raw sensory inputs, reducing memory usage while preserving critical information. Through experiments with five sequentially learned visual concepts, we demonstrate that Latent Replay significantly outperforms existing methods in maintaining model versatility. After learning all concepts, our approach retained 77.59% Image Alignment (IA) on the earliest concept, 14% higher than baseline methods, while maintaining diverse outputs. Surprisingly, random selection of stored latent examples outperforms similarity-based strategies. Our findings suggest that Latent Replay enables efficient continual learning for generative AI models, paving the way for personalized text-to-image models that evolve with user needs without excessive computational costs.</li>
<li><strong>摘要：</strong>持续学习 - 能够逐步获取知识而不忘记以前的技能的能力 - 是自然智力的基础。尽管人类大脑在这方面表现出色，但人工神经网络在“灾难性遗忘”中挣扎，其中学习新任务消除了先前获得的知识。对于文本到图像扩散模型，这一挑战尤为严重，该模型从文本提示中产生图像。此外，这些模型将面临“模式崩溃”，随着时间的流逝，它们的输出变得越来越重复。为了应对这些挑战，我们将潜在的重播（一种由神经科学启发的方法）应用于扩散模型。传统的重播方法通过存储和重新审视过去的示例来减轻遗忘，通常需要大量图像。相反，潜在的重播仅保留从模型的内部体系结构中提取的紧凑，高级功能表示。这反映了存储神经活动模式而不是原始感觉输入的海马过程，从而减少了记忆使用情况，同时保留关键信息。通过使用五个依次学习的视觉概念的实验，我们证明了潜在的重播在维持模型多功能性方面显着优于现有方法。在学习了所有概念之后，我们的方法在最早的概念上保留了77.59％的图像对齐（IA），比基线方法高14％，同时保持多样化的产出。令人惊讶的是，存储的潜在示例的随机选择优于基于相似性的策略。我们的发现表明，潜在重播可以为生成AI模型有效地持续学习，为个性化的文本到图像模型铺平了道路，这些模型随着用户需求而没有过多的计算成本而发展。</li>
</ul>

<h3>Title: Semantic-guided LoRA Parameters Generation</h3>
<ul>
<li><strong>Authors: </strong>Miaoge Li, Yang Chen, Zhijie Rao, Can Jiang, Jingcai Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10535">https://arxiv.org/abs/2509.10535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10535">https://arxiv.org/pdf/2509.10535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10535]] Semantic-guided LoRA Parameters Generation(https://arxiv.org/abs/2509.10535)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) has demonstrated strong generalization capabilities across a variety of tasks for efficiently fine-tuning AI models, especially on resource-constrained edges. However, in real-world applications, edge users often exhibit task-specific preferences that are difficult to handle with a unified model trained under a closed-world assumption, and the challenge may further increase when there are significant domain shifts between training and deployment. Meanwhile, retraining/fine-tuning models for each user is also impractical due to its cost-intensive nature and privacy concerns over raw data utilization from edges. To address these challenges, we propose Semantic-guided LoRA Parameter Generation (SG-LoRA), the first of its kind framework to efficiently produce user-specific LoRA parameters without any additional training on user tasks or access to user-specific data. Concretely, SG-LoRA uses task descriptions as the semantic bridge, measuring their proximity to a set of known expert tasks in a shared embedding space. Based on this semantic guidance, it models the target task's LoRA parameter distribution to generate high-performing parameters for novel tasks. SG-LoRA enables the real-time construction of LoRA models aligned with individual intents by distilling knowledge from prominent LoRA experts and, meanwhile, offering a privacy-preserving solution for personalized model adaptation in a novel zero-shot open-world setting proposed in this work. Extensive experiments on multiple challenging tasks confirm the superior performance and remarkable adaptability of SG-LoRA. Code is available at this https URL.</li>
<li><strong>摘要：</strong>低级适应性（LORA）表现出了各种任务的强大概括能力，以有效地微调AI模型，尤其是在资源受限的边缘上。但是，在现实世界中，Edge用户经常表现出特定于任务的偏好，这些偏好很难通过在封闭世界假设下训练的统一模型来处理，并且当培训和部署之间存在重大域名时，挑战可能会进一步增加。同时，由于其成本密集的性质和对原始数据利用率的经济密集性和隐私问题，每个用户的再训练/微调模型也是不切实际的。为了应对这些挑战，我们提出了语义引导的Lora参数生成（SG-LORA），这是有效生产特定用户特定用户的Lora参数的第一个同类框架，而无需对用户任务或访问用户特定数据的任何其他培训。具体而言，SG-Lora使用任务描述作为语义桥，测量了它们与共享嵌入空间中一组已知专家任务的接近性。基于此语义指导，它将目标任务的LORA参数分布建模，以生成新任务的高性能参数。 SG-Lora可以通过从著名的Lora专家中提取知识来实时构建与个人意图的实时构建，同时，在这项工作中提出的新颖的零照片开放世界设置中，为个性化模型适应提供了隐私的解决方案。关于多个具有挑战性任务的广泛实验证实了SG-Lora的出色表现和显着的适应性。代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: DualAlign: Generating Clinically Grounded Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Rumeng Li, Xun Wang, Hong Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10538">https://arxiv.org/abs/2509.10538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10538">https://arxiv.org/pdf/2509.10538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10538]] DualAlign: Generating Clinically Grounded Synthetic Data(https://arxiv.org/abs/2509.10538)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Synthetic clinical data are increasingly important for advancing AI in healthcare, given strict privacy constraints on real-world EHRs, limited availability of annotated rare-condition data, and systemic biases in observational datasets. While large language models (LLMs) can generate fluent clinical text, producing synthetic data that is both realistic and clinically meaningful remains challenging. We introduce DualAlign, a framework that enhances statistical fidelity and clinical plausibility through dual alignment: (1) statistical alignment, which conditions generation on patient demographics and risk factors; and (2) semantic alignment, which incorporates real-world symptom trajectories to guide content generation. Using Alzheimer's disease (AD) as a case study, DualAlign produces context-grounded symptom-level sentences that better reflect real-world clinical documentation. Fine-tuning an LLaMA 3.1-8B model with a combination of DualAlign-generated and human-annotated data yields substantial performance gains over models trained on gold data alone or unguided synthetic baselines. While DualAlign does not fully capture longitudinal complexity, it offers a practical approach for generating clinically grounded, privacy-preserving synthetic data to support low-resource clinical text analysis.</li>
<li><strong>摘要：</strong>鉴于对现实世界EHR的严格隐私限制，带注释的稀有条件数据的有限可用性以及观察数据集中的全身偏见，综合临床数据对于推进医疗保健中的AI越来越重要。尽管大型语言模型（LLM）可以产生流利的临床文本，而产生既现实又有意义的合成数据仍然具有挑战性。我们介绍了Dualalign，该框架通过双重对齐来增强统计保真度和临床合理性：（1）统计一致性，该统计对齐对患者人口统计学和风险因素产生产生； （2）语义对齐，它结合了现实世界中的症状轨迹，以指导内容产生。 DualAlign使用阿尔茨海默氏病（AD）作为案例研究，产生了上下文接地的症状级句子，可以更好地反映现实世界中的临床文献。通过双重生成和人类通知的数据组合，对Llama 3.1-8B模型进行微调，可在单独使用金数据或无指导的合成基础的模型上获得可观的性能增长。尽管DualAlign并未完全捕获纵向复杂性，但它提供了一种实用的方法来生成临床扎根的隐私合成数据，以支持低资源的临床文本分析。</li>
</ul>

<h3>Title: pySigLib - Fast Signature-Based Computations on CPU and GPU</h3>
<ul>
<li><strong>Authors: </strong>Daniil Shmelev, Cristopher Salvi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MS, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10613">https://arxiv.org/abs/2509.10613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10613">https://arxiv.org/pdf/2509.10613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10613]] pySigLib - Fast Signature-Based Computations on CPU and GPU(https://arxiv.org/abs/2509.10613)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Signature-based methods have recently gained significant traction in machine learning for sequential data. In particular, signature kernels have emerged as powerful discriminators and training losses for generative models on time-series, notably in quantitative finance. However, existing implementations do not scale to the dataset sizes and sequence lengths encountered in practice. We present pySigLib, a high-performance Python library offering optimised implementations of signatures and signature kernels on CPU and GPU, fully compatible with PyTorch's automatic differentiation. Beyond an efficient software stack for large-scale signature-based computation, we introduce a novel differentiation scheme for signature kernels that delivers accurate gradients at a fraction of the runtime of existing libraries.</li>
<li><strong>摘要：</strong>基于签名的方法最近在机器学习中获得了序列数据的显着牵引力。特别是，签名内核已成为有力的歧视者和时间序列上的生成模型的培训损失，尤其是在定量融资中。但是，现有的实现并未扩展到实践中遇到的数据集大小和序列长度。我们提出了Pysiglib，这是一个高性能的Python库，可在CPU和GPU上提供优化的签名和签名内核实现，与Pytorch的自动分化完全兼容。除了用于大规模签名计算的有效软件堆栈外，我们还为签名内核引入了一种新颖的差异化方案，该方案在现有库的运行时提供了准确的梯度。</li>
</ul>

<h3>Title: Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses</h3>
<ul>
<li><strong>Authors: </strong>Emily Kaczmarek, Justin Szeto, Brennan Nichyporuk, Tal Arbel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10620">https://arxiv.org/abs/2509.10620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10620">https://arxiv.org/pdf/2509.10620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10620]] Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses(https://arxiv.org/abs/2509.10620)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>3D structural Magnetic Resonance Imaging (MRI) brain scans are commonly acquired in clinical settings to monitor a wide range of neurological conditions, including neurodegenerative disorders and stroke. While deep learning models have shown promising results analyzing 3D MRI across a number of brain imaging tasks, most are highly tailored for specific tasks with limited labeled data, and are not able to generalize across tasks and/or populations. The development of self-supervised learning (SSL) has enabled the creation of large medical foundation models that leverage diverse, unlabeled datasets ranging from healthy to diseased data, showing significant success in 2D medical imaging applications. However, even the very few foundation models for 3D brain MRI that have been developed remain limited in resolution, scope, or accessibility. In this work, we present a general, high-resolution SimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on 18,759 patients (44,958 scans) from 11 publicly available datasets spanning diverse neurological diseases. We compare our model to Masked Autoencoders (MAE), as well as two supervised baselines, on four diverse downstream prediction tasks in both in-distribution and out-of-distribution settings. Our fine-tuned SimCLR model outperforms all other models across all tasks. Notably, our model still achieves superior performance when fine-tuned using only 20% of labeled training samples for predicting Alzheimer's disease. We use publicly available code and data, and release our trained model at this https URL, contributing a broadly applicable and accessible foundation model for clinical brain MRI analysis.</li>
<li><strong>摘要：</strong>3D结构磁共振成像（MRI）脑扫描通常在临床环境中获得，以监测广泛的神经系统疾病，包括神经退行性疾病和中风。虽然深度学习模型已显示出令人鼓舞的结果，分析了许多大脑成像任务中的3D MRI，但大多数人都针对具有有限标记数据的特定任务量身定制，并且无法跨任务和/或人群概括。自我监督学习（SSL）的发展使得创建了大型医学基础模型，该模型利用了从健康的数据到患病数据的多样化，未标记的数据集，在2D医学成像应用中取得了巨大成功。但是，即使是开发的3D大脑MRI的基础模型也很少，在分辨率，范围或可访问性方面仍然有限。在这项工作中，我们为3D大脑结构MRI提供了一个普通高分辨率的SIMCLR SSL基础模型，该模型已从11个公开可用的数据集中对18,759例患者（44,958次扫描）进行了培训，这些数据集涉及多种神经系统疾病。我们将模型与掩盖自动编码器（MAE）以及两个有监督的基线进行比较，在分发和分发设置中的四个不同的下游预测任务上。我们的微调SIMCLR模型在所有任务上都优于所有其他模型。值得注意的是，当仅使用20％的标记训练样本来预测阿尔茨海默氏病时，我们的模型仍然可以实现出色的性能。我们使用公开可用的代码和数据，并在此HTTPS URL上发布了我们训练有素的模型，为临床大脑MRI分析提供了广泛且可访问的基础模型。</li>
</ul>

<h3>Title: Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Hao Zhang, Chun-Han Yao, Simon Donné, Narendra Ahuja, Varun Jampani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10687">https://arxiv.org/abs/2509.10687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10687">https://arxiv.org/pdf/2509.10687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10687]] Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation(https://arxiv.org/abs/2509.10687)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present Stable Part Diffusion 4D (SP4D), a framework for generating paired RGB and kinematic part videos from monocular inputs. Unlike conventional part segmentation methods that rely on appearance-based semantic cues, SP4D learns to produce kinematic parts - structural components aligned with object articulation and consistent across views and time. SP4D adopts a dual-branch diffusion model that jointly synthesizes RGB frames and corresponding part segmentation maps. To simplify the architecture and flexibly enable different part counts, we introduce a spatial color encoding scheme that maps part masks to continuous RGB-like images. This encoding allows the segmentation branch to share the latent VAE from the RGB branch, while enabling part segmentation to be recovered via straightforward post-processing. A Bidirectional Diffusion Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a contrastive part consistency loss to promote spatial and temporal alignment of part predictions. We demonstrate that the generated 2D part maps can be lifted to 3D to derive skeletal structures and harmonic skinning weights with few manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K, a curated dataset of over 20K rigged objects selected and processed from Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part video sequences. Experiments show that SP4D generalizes strongly to diverse scenarios, including real-world videos, novel generated objects, and rare articulated poses, producing kinematic-aware outputs suitable for downstream animation and motion-related tasks.</li>
<li><strong>摘要：</strong>我们提出稳定的零件扩散4D（SP4D），这是一种从单眼输入中生成配对的RGB和运动学零件视频的框架。与依赖于基于外观的语义提示的常规部分分割方法不同，SP4D学会了生产运动学零件 - 结构成分与对象表达相一致，并且在视图和时间之间保持一致。 SP4D采用了双分支扩散模型，该模型共同合成RGB框架和相应的零件分割图。为了简化体系结构并灵活地启用不同的零件计数，我们引入了一个空间颜色编码方案，该方案将部分掩盖映射到连续类似RGB的图像。该编码允许分割分支从RGB分支共享潜在的VAE，同时可以通过直接的后处理恢复零件分割。双向扩散融合（Bidifuse）模块增强了跨分支的一致性，并得到了对比度部分一致性损失的支持，以促进零件预测的空间和时间对齐。我们证明，生成的2D部分地图可以提起到3D，以得出骨骼结构和谐波皮肤重量，几乎没有手动调整。为了训练和评估SP4D，我们构建了KinematicParts20K，这是一个从Objaverse XL中选择和处理的20K索具对象的策划数据集（Deitke等，2023），每个数据集与多视图RGB和部分视频序列配对。实验表明，SP4D强烈概括到各种情况下，包括现实世界的视频，新颖的物体和稀有的铰接式姿势，产生适用于下游动画和与运动相关的任务的运动学感知输出。</li>
</ul>

<h3>Title: Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Yi-Ruei Liu, You-Zhe Xie, Yu-Hsiang Hsu, I-Sheng Fang, Yu-Lun Liu, Jun-Cheng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10759">https://arxiv.org/abs/2509.10759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10759">https://arxiv.org/pdf/2509.10759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10759]] Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation(https://arxiv.org/abs/2509.10759)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Common computer vision systems typically assume ideal pinhole cameras but fail when facing real-world camera effects such as fisheye distortion and rolling shutter, mainly due to the lack of learning from training data with camera effects. Existing data generation approaches suffer from either high costs, sim-to-real gaps or fail to accurately model camera effects. To address this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage pipeline that combines 4D Gaussian Splatting with physically-based ray tracing for camera effect simulation. Given multi-view videos, 4D-GRT first reconstructs dynamic scenes, then applies ray tracing to generate videos with controllable, physically accurate camera effects. 4D-GRT achieves the fastest rendering speed while performing better or comparable rendering quality compared to existing baselines. Additionally, we construct eight synthetic dynamic scenes in indoor environments across four camera effects as a benchmark to evaluate generated videos with camera effects.</li>
<li><strong>摘要：</strong>常见的计算机视觉系统通常假设理想的针孔摄像机，但在面对现实世界的相机效果（例如Fisheye扭曲和滚动快门）时失败，这主要是由于缺乏从具有相机效果的训练数据中学习的。现有的数据生成方法遭受了高成本，SIM到空白差距或无法准确对相机效应进行建模。为了解决这一瓶颈，我们提出了4D高斯射线跟踪（4D-Grt），这是一种新型的两阶段管道，将4D高斯分裂与基于物理的射线跟踪结合在一起，以进行相机效应模拟。给定多视频视频，4D-GRT首先重建动态场景，然后应用射线跟踪来生成具有可控，物理上准确的相机效果的视频。与现有基线相比，4D-GRT的渲染速度达到了最快的渲染速度。此外，我们在室内环境中构建八个综合动态场景，跨四个相机效果，作为评估具有相机效果的生成视频的基准。</li>
</ul>

<h3>Title: GoldenTransformer: A Modular Fault Injection Framework for Transformer Robustness Research</h3>
<ul>
<li><strong>Authors: </strong>Luke Howard</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10790">https://arxiv.org/abs/2509.10790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10790">https://arxiv.org/pdf/2509.10790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10790]] GoldenTransformer: A Modular Fault Injection Framework for Transformer Robustness Research(https://arxiv.org/abs/2509.10790)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Transformers have become the foundation for a wide range of state--of--the--art models across natural language processing, computer vision, and other machine learning domains. Despite their widespread deployment, the robustness of these models under fault conditions remains underexplored. We present GoldenTransformer, a modular and extensible fault injection framework designed to evaluate the resiliency of Large Language Models to induced hardware faults. GoldenTransformer offers a unified Python-based platform for injecting diverse classes of faults--such as weight corruption, activation injections, and attention--level disruptions--into pretrained transformer--based models. Inspired by the GoldenEye simulator for DNNs, our framework focuses on the unique challenges of working with large transformer architectures, including considerations such as structural complexity, latent dependencies, and nonuniform layer definitions. GoldenTransformer is built atop PyTorch and HuggingFace Transformers, and it supports experiment reproducibility, metric logging, and visualization out of the box. We detail the technical design and use of GoldenTransformer and demonstrate through several example experiments on classification and generation tasks. By enabling controlled injection of faults at multiple logical and structural points in a transformer, GoldenTransformer offers researchers and practitioners a valuable tool for model robustness analysis and for guiding dependable system design in real-world LLM applications.</li>
<li><strong>摘要：</strong>变形金刚已成为自然语言处理，计算机视觉和其他机器学习领域的广泛状态的基础。尽管它们广泛地部署，但这些模型在断层条件下的鲁棒性仍然没有得到充实。我们提出了GoldentRansformer，这是一种模块化且可扩展的故障注入框架，旨在评估大语言模型对诱导硬件故障的弹性。 GoldentRansformer提供了一个基于Python的平台，用于注入各种故障类别 - 例如，重量损坏，激活注射和注意力（级别的破坏） - 基于预测的基于变压器的模型。受DNN的Goldeneye模拟器的启发，我们的框架着眼于使用大型变压器体系结构的独特挑战，包括结构复杂性，潜在依赖项和非均匀层定义等考虑因素。 GoldentRansformer是在Pytorch和Huggingface Transformers顶上建造的，它支持实验可重复性，度量记录和可视化的开箱即用。我们详细介绍了GoldentRansformer的技术设计和使用，并通过几个有关分类和发电任务的示例实验进行了演示。通过在变压器中的多个逻辑和结构点上启用故障的受控注入，GoldentRansformer为研究人员和从业人员提供了模型鲁棒性分析以及指导现实世界中LLM应用中可靠的系统设计的宝贵工具。</li>
</ul>

<h3>Title: InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts</h3>
<ul>
<li><strong>Authors: </strong>Weipeng Zhong, Peizhou Cao, Yichen Jin, Li Luo, Wenzhe Cai, Jingli Lin, Hanqing Wang, Zhaoyang Lyu, Tai Wang, Bo Dai, Xudong Xu, Jiangmiao Pang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10813">https://arxiv.org/abs/2509.10813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10813">https://arxiv.org/pdf/2509.10813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10813]] InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts(https://arxiv.org/abs/2509.10813)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The advancement of Embodied AI heavily relies on large-scale, simulatable 3D scene datasets characterized by scene diversity and realistic layouts. However, existing datasets typically suffer from limitations in data scale or diversity, sanitized layouts lacking small items, and severe object collisions. To address these shortcomings, we introduce \textbf{InternScenes}, a novel large-scale simulatable indoor scene dataset comprising approximately 40,000 diverse scenes by integrating three disparate scene sources, real-world scans, procedurally generated scenes, and designer-created scenes, including 1.96M 3D objects and covering 15 common scene types and 288 object classes. We particularly preserve massive small items in the scenes, resulting in realistic and complex layouts with an average of 41.5 objects per region. Our comprehensive data processing pipeline ensures simulatability by creating real-to-sim replicas for real-world scans, enhances interactivity by incorporating interactive objects into these scenes, and resolves object collisions by physical simulations. We demonstrate the value of InternScenes with two benchmark applications: scene layout generation and point-goal navigation. Both show the new challenges posed by the complex and realistic layouts. More importantly, InternScenes paves the way for scaling up the model training for both tasks, making the generation and navigation in such complex scenes possible. We commit to open-sourcing the data, models, and benchmarks to benefit the whole community.</li>
<li><strong>摘要：</strong>体现AI的进步在很大程度上依赖于以场景多样性和现实布局为特征的大规模，可模拟的3D场景数据集。但是，现有的数据集通常会遭受数据量表或多样性的局限性，缺乏小项目的消毒布局以及严重的对象碰撞。为了解决这些缺点，我们介绍了\ textbf {InternScenes}，这是一个新颖的大规模模拟室内场景数据集，包括大约40,000个不同的场景，通过整合了三个不同的场景来源，现实世界中的扫描，实际生成的场景，以及设计师创建的场景以及158 dd的对象和覆盖对象。我们特别在场景中保留大量小项目，从而导致每个区域平均41.5个物体进行现实和复杂的布局。我们的全面数据处理管道通过为真实世界扫描创建真实到SIM的复制品，通过将交互式对象纳入这些场景来增强交互性，并通过物理模拟解决对象碰撞来确保相互作用。我们通过两个基准应用程序演示了实习生的价值：场景布局生成和点目标导航。两者都表明了复杂和现实的布局带来的新挑战。更重要的是，InternScenes为对这两个任务的模型培训扩展铺平了道路，从而使在如此复杂的场景中产生和导航成为可能。我们致力于开放数据，模型和基准，以使整个社区受益。</li>
</ul>

<h3>Title: CogGNN: Cognitive Graph Neural Networks in Generative Connectomics</h3>
<ul>
<li><strong>Authors: </strong>Mayssa Soussia, Yijun Lin, Mohamed Ali Mahjoub, Islem Rekik</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10864">https://arxiv.org/abs/2509.10864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10864">https://arxiv.org/pdf/2509.10864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10864]] CogGNN: Cognitive Graph Neural Networks in Generative Connectomics(https://arxiv.org/abs/2509.10864)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>Generative learning has advanced network neuroscience, enabling tasks like graph super-resolution, temporal graph prediction, and multimodal brain graph fusion. However, current methods, mainly based on graph neural networks (GNNs), focus solely on structural and topological properties, neglecting cognitive traits. To address this, we introduce the first cognified generative model, CogGNN, which endows GNNs with cognitive capabilities (e.g., visual memory) to generate brain networks that preserve cognitive features. While broadly applicable, we present CogGNN, a specific variant designed to integrate visual input, a key factor in brain functions like pattern recognition and memory recall. As a proof of concept, we use our model to learn connectional brain templates (CBTs), population-level fingerprints from multi-view brain networks. Unlike prior work that overlooks cognitive properties, CogGNN generates CBTs that are both cognitively and structurally meaningful. Our contributions are: (i) a novel cognition-aware generative model with a visual-memory-based loss; (ii) a CBT-learning framework with a co-optimization strategy to yield well-centered, discriminative, cognitively enhanced templates. Extensive experiments show that CogGNN outperforms state-of-the-art methods, establishing a strong foundation for cognitively grounded brain network modeling.</li>
<li><strong>摘要：</strong>生成学习具有先进的网络神经科学，可以实现图形超分辨率，时间图预测和多模式脑图融合等任务。但是，当前的方法主要基于图形神经网络（GNN），仅着眼于结构和拓扑特性，忽略了认知特征。为了解决这个问题，我们介绍了第一个认知的生成模型COGGNN，该模型赋予GNNS具有认知能力（例如，视觉记忆），以生成保留认知特征的脑网络。虽然广泛适用，但我们提出了COGGNN，这是一种旨在整合视觉输入的特定变体，这是大脑函数诸如模式识别和记忆回忆之类的关键因素。作为概念证明，我们使用模型来学习连接脑模板（CBT），来自多视图大脑网络的人群级指纹。与忽略认知特性的先前工作不同，CogGNN会产生在认知和结构上有意义的CBT。我们的贡献是：（i）具有基于视觉记忆损失的新型认知感知生成模型； （ii）具有合作策略的CBT学习框架，以产生以良好为中心，歧视性，认知增强的模板。广泛的实验表明，COGGNN的表现要优于最先进的方法，为认知上扎根的脑网络建模建立了坚实的基础。</li>
</ul>

<h3>Title: Total Variation Subgradient Guided Image Fusion for Dual-Camera CASSI System</h3>
<ul>
<li><strong>Authors: </strong>Weiqiang Zhao, Tianzhu Liu, Yuzhe Gui, Yanfeng Gu</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.optics</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10897">https://arxiv.org/abs/2509.10897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10897">https://arxiv.org/pdf/2509.10897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10897]] Total Variation Subgradient Guided Image Fusion for Dual-Camera CASSI System(https://arxiv.org/abs/2509.10897)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Spectral imaging technology has long-faced fundamental challenges in balancing spectral, spatial, and temporal reso- lutions. While compressive sensing-based Coded Aperture Snapshot Spectral Imaging (CASSI) mitigates this trade-off through optical encoding, high compression ratios result in ill-posed reconstruction problems. Traditional model-based methods exhibit limited performance due to reliance on handcrafted inherent image priors, while deep learning approaches are constrained by their black-box nature, which compromises physical interpretability. To address these limitations, we propose a dual-camera CASSI reconstruction framework that integrates total variation (TV) subgradient theory. By es- tablishing an end-to-end SD-CASSI mathematical model, we reduce the computational complexity of solving the inverse problem and provide a mathematically well-founded framework for analyzing multi-camera systems. A dynamic regular- ization strategy is introduced, incorporating normalized gradient constraints from RGB/panchromatic-derived reference images, which constructs a TV subgradient similarity function with strict convex optimization guarantees. Leveraging spatial priors from auxiliary cameras, an adaptive reference generation and updating mechanism is designed to provide subgradient guidance. Experimental results demonstrate that the proposed method effectively preserves spatial-spectral structural consistency. The theoretical framework establishes an interpretable mathematical foundation for computational spectral imaging, demonstrating robust performance across diverse reconstruction scenarios. The source code is available at this https URL.</li>
<li><strong>摘要：</strong>光谱成像技术在平衡频谱，空间和时间重新分配方面面临着长期的基本挑战。尽管基于压缩感应的编码光圈快照光谱成像（CASSI）可以通过光学编码来减轻这种权衡，但高压缩比导致了不足的重建问题。传统的基于模型的方法由于依赖手工制作的固有图像先验而表现出有限的性能，而深度学习方法受其黑盒子的限制，这会损害物理性的可解释性。为了解决这些局限性，我们提出了一个双相机CASSI重建框架，该框架整合了总变化（TV）亚级别理论。通过建立端到端SD-CASSI数学模型，我们降低了解决反问题的计算复杂性，并提供了一个数学上有充分根据的框架来分析多相机系统。引入了动态的常规ization策略，并结合了RGB/Panchromatic衍生的参考图像的归一化梯度约束，该图像构建了电视亚级别相似性​​功能，并具有严格的凸优化保证。自适应参考生成和更新机制利用辅助摄像机的空间先验旨在提供亚级别的指导。实验结果表明，所提出的方法有效地保留了空间光谱结构的一致性。理论框架为计算光谱成像建立了可解释的数学基础，证明了各种重建方案的稳健性能。源代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: ToMA: Token Merge with Attention for Image Generation with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Wenbo Lu, Shaoyi Zheng, Yuxuan Xia, Shengjie Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10918">https://arxiv.org/abs/2509.10918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10918">https://arxiv.org/pdf/2509.10918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10918]] ToMA: Token Merge with Attention for Image Generation with Diffusion Models(https://arxiv.org/abs/2509.10918)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models excel in high-fidelity image generation but face scalability limits due to transformers' quadratic attention complexity. Plug-and-play token reduction methods like ToMeSD and ToFu reduce FLOPs by merging redundant tokens in generated images but rely on GPU-inefficient operations (e.g., sorting, scattered writes), introducing overheads that negate theoretical speedups when paired with optimized attention implementations (e.g., FlashAttention). To bridge this gap, we propose Token Merge with Attention (ToMA), an off-the-shelf method that redesigns token reduction for GPU-aligned efficiency, with three key contributions: 1) a reformulation of token merge as a submodular optimization problem to select diverse tokens; 2) merge/unmerge as an attention-like linear transformation via GPU-friendly matrix operations; and 3) exploiting latent locality and sequential redundancy (pattern reuse) to minimize overhead. ToMA reduces SDXL/Flux generation latency by 24%/23%, respectively (with DINO $\Delta < 0.07$), outperforming prior methods. This work bridges the gap between theoretical and practical efficiency for transformers in diffusion.</li>
<li><strong>摘要：</strong>扩散模型在高保真图像产生中表现出色，但由于变压器的二次注意复杂性而引起的面部可伸缩性限制。通过在生成的图像中合并冗余令牌，但依赖于GPU-Inleff的操作（例如，排序，分散的写入），引入否定理论加速的间接费用（例如，与优化的注意力实现（例如，闪存），否定了否定理论加速的开销（例如，闪存）。为了弥合这一差距，我们建议代币与注意力（TOMA）合并，这是一种现成的方法，可以重新设计为GPU一致性效率的代币减少，并具有三个关键贡献：1）将令牌合并作为supperuliblemular Optimation Impartization问题的重新汇总，以选择多样的代币； 2）通过对GPU友好的矩阵操作合并/解开作为注意力的线性转换； 3）利用潜在位置和顺序冗余（模式重复使用）以最大程度地减少开销。 Toma分别将SDXL/Flux的产生潜伏期减少了24％/23％（使用Dino $ \ delta <0.07 $），表现优于先验方法。这项工作弥合了变压器扩散的理论和实践效率之间的差距。</li>
</ul>

<h3>Title: Simulating Sinogram-Domain Motion and Correcting Image-Domain Artifacts Using Deep Learning in HR-pQCT Bone Imaging</h3>
<ul>
<li><strong>Authors: </strong>Farhan Sadik, Christopher L. Newman, Stuart J. Warden, Rachel K. Surowiec</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10961">https://arxiv.org/abs/2509.10961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10961">https://arxiv.org/pdf/2509.10961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10961]] Simulating Sinogram-Domain Motion and Correcting Image-Domain Artifacts Using Deep Learning in HR-pQCT Bone Imaging(https://arxiv.org/abs/2509.10961)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Rigid-motion artifacts, such as cortical bone streaking and trabecular smearing, hinder in vivo assessment of bone microstructures in high-resolution peripheral quantitative computed tomography (HR-pQCT). Despite various motion grading techniques, no motion correction methods exist due to the lack of standardized degradation models. We optimize a conventional sinogram-based method to simulate motion artifacts in HR-pQCT images, creating paired datasets of motion-corrupted images and their corresponding ground truth, which enables seamless integration into supervised learning frameworks for motion correction. As such, we propose an Edge-enhanced Self-attention Wasserstein Generative Adversarial Network with Gradient Penalty (ESWGAN-GP) to address motion artifacts in both simulated (source) and real-world (target) datasets. The model incorporates edge-enhancing skip connections to preserve trabecular edges and self-attention mechanisms to capture long-range dependencies, facilitating motion correction. A visual geometry group (VGG)-based perceptual loss is used to reconstruct fine micro-structural features. The ESWGAN-GP achieves a mean signal-to-noise ratio (SNR) of 26.78, structural similarity index measure (SSIM) of 0.81, and visual information fidelity (VIF) of 0.76 for the source dataset, while showing improved performance on the target dataset with an SNR of 29.31, SSIM of 0.87, and VIF of 0.81. The proposed methods address a simplified representation of real-world motion that may not fully capture the complexity of in vivo motion artifacts. Nevertheless, because motion artifacts present one of the foremost challenges to more widespread adoption of this modality, these methods represent an important initial step toward implementing deep learning-based motion correction in HR-pQCT.</li>
<li><strong>摘要：</strong>刚性动物（例如皮质骨条纹和小梁涂抹），在高分辨率外周定量计算机断层扫描（HR-PQCT）中对骨微结构的体内评估。尽管有各种运动分级技术，但由于缺乏标准化的降解模型，尚无运动校正方法。我们优化了一种基于正弦图的传统方法，以模拟HR-PQCT图像中的运动伪像，创建运动腐败图像的配对数据集及其相应的地面真理，从而使无缝集成到有监督的学习框架中进行运动校正。因此，我们提出了一个具有梯度惩罚（ESWGAN-GP）的边缘增强的自我注意的瓦斯汀生成对抗网络，以解决模拟（源）和现实世界（目标）数据集中的运动伪像。该模型结合了边缘增强的跳过连接，以保留小梁边缘和自我发项机制，以捕获远程依赖性，从而促进运动校正。基于视觉几何组（VGG）的感知损失用于重建精细的微结构特征。 ESWGAN-GP的平均信噪比（SNR）为26.78，结构相似性指数量度（SSIM）为0.81，源数据集的视觉信息保真度（VIF）为0.76，同时显示出目标数据集的改进性能，其SNR的SNR为29.31，SSSIM为29.31，SSIM，为0.87，和VIF，和VIF。提出的方法解决了现实世界运动的简化表示，该表示可能无法完全捕获体内运动伪像的复杂性。然而，由于运动伪影提出了更广泛地采用这种模式的首要挑战之一，因此这些方法代表了在HR-PQCT中实施基于深度学习的运动校正的重要第一步。</li>
</ul>

<h3>Title: PHLoRA: data-free Post-hoc Low-Rank Adapter extraction from full-rank checkpoint</h3>
<ul>
<li><strong>Authors: </strong>Bhoomit Vasani, Jack FitzGerald, Anjie Fang, Sushmit Vaish</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10971">https://arxiv.org/abs/2509.10971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10971">https://arxiv.org/pdf/2509.10971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10971]] PHLoRA: data-free Post-hoc Low-Rank Adapter extraction from full-rank checkpoint(https://arxiv.org/abs/2509.10971)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce PHLoRA (Pronounced "flora"). (Post-hoc LoRA), a simple yet powerful method to extract low-rank adaptation adapters from full-rank fine-tuned models without requiring access to training data or gradients. By computing the low-rank decomposition of weight differences between a base model and its fine-tuned counterpart, our method reconstructs adapter modules that can be merged or dynamically routed at inference time via S-LoRA, or served in scalable, industry settings using platforms like NVIDIA NIM. This approach amortizes latency overhead across requests and yields substantial cost savings. Unlike prior work that trains each adapter explicitly, our approach decouples fine-tuning from adapter generation, allowing adapter extraction from existing full-rank models or third-party checkpoints. Experiments on text, image, and video benchmarks using the Amazon Nova model family demonstrate that extracted adapters preserve high energy from the full weight delta, can be pruned safely, and yield negligible degradation in downstream task performance when re-merged. Overall, PHLoRA provides a practical path for making all existing full-rank checkpoints adapter-ready, democratizing scalable inference for all models.</li>
<li><strong>摘要：</strong>我们介绍菲洛拉（发音为“植物”）。 （事后洛拉），这是一种简单而强大的方法，是从全级微调模型中提取低级别适应适配器而无需访问培训数据或梯度的方法。通过计算基本模型与其微调对应物之间的重量差异的低排放分解，我们的方法重建了可以通过S-Lora在推理时合并或动态路由的适配器模块，或使用NVIDIA NIM等平台进行可扩展的行业设置。这种方法将跨请求的延迟开销摊销，并节省大量成本。与先前对每个适配器进行训练的工作不同，我们的方法与适配器生成的微调脱离，从而可以从现有的全等级型号或第三方检查点中提取适配器。使用Amazon Nova模型家族进行文本，图像和视频基准测试的实验表明，提取的适配器可以安全地修剪全部重量，可以安全地修剪，并在重新合并时在下游任务性能中可忽略不计的退化。总体而言，菲洛拉（Phlora）提供了一条实用的途径，可以使所有现有的全级检查站适配器准备就绪，对所有模型的可扩展推断进行民主化。</li>
</ul>

<h3>Title: Decoupling Search and Learning in Neural Net Training</h3>
<ul>
<li><strong>Authors: </strong>Akshay Vegesna, Samip Dahal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10973">https://arxiv.org/abs/2509.10973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10973">https://arxiv.org/pdf/2509.10973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10973]] Decoupling Search and Learning in Neural Net Training(https://arxiv.org/abs/2509.10973)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Gradient descent typically converges to a single minimum of the training loss without mechanisms to explore alternative minima that may generalize better. Searching for diverse minima directly in high-dimensional parameter space is generally intractable. To address this, we propose a framework that performs training in two distinct phases: search in a tractable representation space (the space of intermediate activations) to find diverse representational solutions, and gradient-based learning in parameter space by regressing to those searched representations. Through evolutionary search, we discover representational solutions whose fitness and diversity scale with compute--larger populations and more generations produce better and more varied solutions. These representations prove to be learnable: networks trained by regressing to searched representations approach SGD's performance on MNIST, CIFAR-10, and CIFAR-100. Performance improves with search compute up to saturation. The resulting models differ qualitatively from networks trained with gradient descent, following different representational trajectories during training. This work demonstrates how future training algorithms could overcome gradient descent's exploratory limitations by decoupling search in representation space from efficient gradient-based learning in parameter space.</li>
<li><strong>摘要：</strong>梯度下降通常会收敛到训练损失的最小值，而没有机制来探索可以更好地推广的替代最小值。在高维参数空间中直接搜索各种最小值通常是棘手的。为了解决这个问题，我们提出了一个框架，该框架在两个不同的阶段进行培训：在可拖动的表示空间（中间激活空间）中搜索，以找到各种表示的表示解决方案，并通过回归到搜索的表示中，以在参数空间中进行基于梯度的学习。通过进化搜索，我们发现了代表性解决方案，其适应性和多样性量表具有计算人群（少量人群）和更多的世代产生更好，更多样化的解决方案。这些表示形式被证明是可学习的：通过回归搜索表示的培训的网络方法SGD在MNIST，CIFAR-10和CIFAR-100上的性能。搜索计算最高饱和度可以提高性能。所得模型在训练过程中遵循不同的代表性轨迹，与接受梯度下降训练的网络差异。这项工作表明，未来的培训算法如何通过将表示空间中的搜索搜索从参数空间中的有效基于梯度的学习中解散，从而克服梯度下降的探索性限制。</li>
</ul>

<h3>Title: TrueSkin: Towards Fair and Accurate Skin Tone Recognition and Generation</h3>
<ul>
<li><strong>Authors: </strong>Haoming Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10980">https://arxiv.org/abs/2509.10980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10980">https://arxiv.org/pdf/2509.10980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10980]] TrueSkin: Towards Fair and Accurate Skin Tone Recognition and Generation(https://arxiv.org/abs/2509.10980)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Skin tone recognition and generation play important roles in model fairness, healthcare, and generative AI, yet they remain challenging due to the lack of comprehensive datasets and robust methodologies. Compared to other human image analysis tasks, state-of-the-art large multimodal models (LMMs) and image generation models struggle to recognize and synthesize skin tones accurately. To address this, we introduce TrueSkin, a dataset with 7299 images systematically categorized into 6 classes, collected under diverse lighting conditions, camera angles, and capture settings. Using TrueSkin, we benchmark existing recognition and generation approaches, revealing substantial biases: LMMs tend to misclassify intermediate skin tones as lighter ones, whereas generative models struggle to accurately produce specified skin tones when influenced by inherent biases from unrelated attributes in the prompts, such as hairstyle or environmental context. We further demonstrate that training a recognition model on TrueSkin improves classification accuracy by more than 20\% compared to LMMs and conventional approaches, and fine-tuning with TrueSkin significantly improves skin tone fidelity in image generation models. Our findings highlight the need for comprehensive datasets like TrueSkin, which not only serves as a benchmark for evaluating existing models but also provides a valuable training resource to enhance fairness and accuracy in skin tone recognition and generation tasks.</li>
<li><strong>摘要：</strong>肤色识别和发电在模型公平，医疗保健和生成AI中起着重要作用，但是由于缺乏全面的数据集和强大的方法，它们仍然具有挑战性。与其他人类图像分析任务相比，最先进的大型多模型模型（LMM）和图像生成模型难以准确识别和合成肤色。为了解决这个问题，我们介绍了Trueskin，这是一个具有7299张图像系统分类为6个类的数据集，在不同的照明条件，摄像机角度和捕获设置下收集。使用Trueskin，我们基于现有的识别和发电方法，揭示了实质性的偏见：LMM倾向于将中间的肤色误认为较轻，而生成模型则难以准确地产生指定的肤色，而当在提示中（例如发型或环境环境）中受到无关属性的固有偏见，例如发型或环境环境。我们进一步证明，与LMM和常规方法相比，对Trueskin的识别模型提高了20 \％的分类精度，并且对Trueskin的微调显着改善了图像生成模型中的肤色忠诚度。我们的发现强调了对Trueskin等综合数据集的需求，Trueskin不仅是评估现有模型的基准，而且还提供了有价值的培训资源，以增强肤色识别和发电任务的公平性和准确性。</li>
</ul>

<h3>Title: An Advanced Convolutional Neural Network for Bearing Fault Diagnosis under Limited Data</h3>
<ul>
<li><strong>Authors: </strong>Shengke Sun, Shuzhen Han, Ziqian Luan, Xinghao Qin, Jiao Yin, Zhanshan Zhao, Jinli Cao, Hua Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11053">https://arxiv.org/abs/2509.11053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11053">https://arxiv.org/pdf/2509.11053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11053]] An Advanced Convolutional Neural Network for Bearing Fault Diagnosis under Limited Data(https://arxiv.org/abs/2509.11053)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the area of bearing fault diagnosis, deep learning (DL) methods have been widely used recently. However, due to the high cost or privacy concerns, high-quality labeled data are scarce in real world scenarios. While few-shot learning has shown promise in addressing data scarcity, existing methods still face significant limitations in this domain. Traditional data augmentation techniques often suffer from mode collapse and generate low-quality samples that fail to capture the diversity of bearing fault patterns. Moreover, conventional convolutional neural networks (CNNs) with local receptive fields makes them inadequate for extracting global features from complex vibration signals. Additionally, existing methods fail to model the intricate relationships between limited training samples. To solve these problems, we propose an advanced data augmentation and contrastive fourier convolution framework (DAC-FCF) for bearing fault diagnosis under limited data. Firstly, a novel conditional consistent latent representation and reconstruction generative adversarial network (CCLR-GAN) is proposed to generate more diverse data. Secondly, a contrastive learning based joint optimization mechanism is utilized to better model the relations between the available training data. Finally, we propose a 1D fourier convolution neural network (1D-FCNN) to achieve a global-aware of the input data. Experiments demonstrate that DAC-FCF achieves significant improvements, outperforming baselines by up to 32\% on case western reserve university (CWRU) dataset and 10\% on a self-collected test bench. Extensive ablation experiments prove the effectiveness of the proposed components. Thus, the proposed DAC-FCF offers a promising solution for bearing fault diagnosis under limited data.</li>
<li><strong>摘要：</strong>在轴承断层诊断的领域，最近已广泛使用了深度学习（DL）方法。但是，由于较高的成本或隐私问题，在现实世界中，高质量的标签数据很少。尽管很少有学习的学习在解决数据稀缺方面显示出希望，但现有方法仍然面临着该领域的重大局限性。传统的数据增强技术通常会遭受模式崩溃的影响，并产生低质量的样品，无法捕获轴承断层模式的多样性。此外，具有局部接受场的常规卷积神经网络（CNN）使它们无法从复杂的振动信号中提取全局特征。此外，现有方法无法建模有限培训样本之间的复杂关系。为了解决这些问题，我们提出了一个高级数据增强和对比度傅里叶卷积框架（DAC-FCF），用于在有限的数据下进行轴承故障诊断。首先，提出了一种新颖的条件一致的潜在表示和重建生成对抗网络（CCLR-GAN）来生成更多样化的数据。其次，利用基于对比的基于学习的关节优化机制来更好地模拟可用培训数据之间的关系。最后，我们提出了一个1D傅里叶卷积神经网络（1D-FCNN），以实现对输入数据的全球意识。实验表明，DAC-FCF在Case Western Reserve University（CWRU）数据集中最高32 \％，在自我收集的测试台上优于32 \％。广泛的消融实验证明了所提出的组件的有效性。因此，拟议的DAC-FCF在有限的数据下为轴承断层诊断提供了有希望的解决方案。</li>
</ul>

<h3>Title: Mars Traversability Prediction: A Multi-modal Self-supervised Approach for Costmap Generation</h3>
<ul>
<li><strong>Authors: </strong>Zongwu Xie, Kaijie Yun, Yang Liu, Yiming Ji, Han Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11082">https://arxiv.org/abs/2509.11082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11082">https://arxiv.org/pdf/2509.11082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11082]] Mars Traversability Prediction: A Multi-modal Self-supervised Approach for Costmap Generation(https://arxiv.org/abs/2509.11082)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present a robust multi-modal framework for predicting traversability costmaps for planetary rovers. Our model fuses camera and LiDAR data to produce a bird's-eye-view (BEV) terrain costmap, trained self-supervised using IMU-derived labels. Key updates include a DINOv3-based image encoder, FiLM-based sensor fusion, and an optimization loss combining Huber and smoothness terms. Experimental ablations (removing image color, occluding inputs, adding noise) show only minor changes in MAE/MSE (e.g. MAE increases from ~0.0775 to 0.0915 when LiDAR is sparsified), indicating that geometry dominates the learned cost and the model is highly robust. We attribute the small performance differences to the IMU labeling primarily reflecting terrain geometry rather than semantics and to limited data diversity. Unlike prior work claiming large gains, we emphasize our contributions: (1) a high-fidelity, reproducible simulation environment; (2) a self-supervised IMU-based labeling pipeline; and (3) a strong multi-modal BEV costmap prediction model. We discuss limitations and future work such as domain generalization and dataset expansion.</li>
<li><strong>摘要：</strong>我们提出了一个强大的多模式框架，用于预测行星漫游者的遍历性成本图。我们的模型将摄像头和LiDAR数据融合，以生成鸟眼的视线（BEV）地形成本映射，并使用IMU衍生标签进行了训练有素的自我监督。关键更新包括基于Dinov3的图像编码器，基于胶片的传感器融合以及结合Huber和平滑度项的优化损失。实验消融（去除图像颜色，阻塞输入，添加噪声）仅显示MAE/MSE的较小变化（例如MAE从〜0.0775增加到0.0915，当Lidar被稀疏时），这表明几何形状占主导地位，并且该模型高度强大。我们将较小的性能差异归因于IMU标签，主要反映地形几何形状，而不是语义和有限的数据多样性。与先前声称巨大收益的工作不同，我们强调了我们的贡献：（1）高保真，可重复的模拟环境； （2）基于IMU的自制标签管道； （3）强大的多模式BEV CostMap预测模型。我们讨论局限性和未来工作，例如域概括和数据集扩展。</li>
</ul>

<h3>Title: PanoLora: Bridging Perspective and Panoramic Video Generation with LoRA Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Dong, Yuyang Yin, Yuqi Li, Eric Li, Hao-Xiang Guo, Yikai Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11092">https://arxiv.org/abs/2509.11092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11092">https://arxiv.org/pdf/2509.11092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11092]] PanoLora: Bridging Perspective and Panoramic Video Generation with LoRA Adaptation(https://arxiv.org/abs/2509.11092)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating high-quality 360° panoramic videos remains a significant challenge due to the fundamental differences between panoramic and traditional perspective-view projections. While perspective videos rely on a single viewpoint with a limited field of view, panoramic content requires rendering the full surrounding environment, making it difficult for standard video generation models to adapt. Existing solutions often introduce complex architectures or large-scale training, leading to inefficiency and suboptimal results. Motivated by the success of Low-Rank Adaptation (LoRA) in style transfer tasks, we propose treating panoramic video generation as an adaptation problem from perspective views. Through theoretical analysis, we demonstrate that LoRA can effectively model the transformation between these projections when its rank exceeds the degrees of freedom in the task. Our approach efficiently fine-tunes a pretrained video diffusion model using only approximately 1,000 videos while achieving high-quality panoramic generation. Experimental results demonstrate that our method maintains proper projection geometry and surpasses previous state-of-the-art approaches in visual quality, left-right consistency, and motion diversity.</li>
<li><strong>摘要：</strong>由于全景和传统视角预测之间的根本差异，生成高质量的360°全景视频仍然是一个重大挑战。虽然透视视频依赖于具有有限视野的单个观点，但全景内容需要呈现完整的环境，这使得标准视频生成模型很难适应。现有的解决方案通常会引入复杂的体系结构或大规模培训，从而导致效率低下和次优效果。从风格转移任务中，低级适应（LORA）的成功激励，我们提出将全景视频生成作为适应性问题从角度看待。通过理论分析，我们证明洛拉可以有效地模拟这些预测超过任务自由度时的转换。我们的方法在获得高质量的全景生成的同时，仅使用大约1,000个视频来有效地使用大约1,000个视频进行预处理的视频扩散模型。实验结果表明，我们的方法保持正确的投影几何形状，并超过了以前的视觉质量，左右一致性和运动多样性的最先进方法。</li>
</ul>

<h3>Title: SMILE: A Super-resolution Guided Multi-task Learning Method for Hyperspectral Unmixing</h3>
<ul>
<li><strong>Authors: </strong>Ruiying Li, Bin Pan, Qiaoying Qu, Xia Xu, Zhenwei Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11093">https://arxiv.org/abs/2509.11093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11093">https://arxiv.org/pdf/2509.11093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11093]] SMILE: A Super-resolution Guided Multi-task Learning Method for Hyperspectral Unmixing(https://arxiv.org/abs/2509.11093)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>The performance of hyperspectral unmixing may be constrained by low spatial resolution, which can be enhanced using super-resolution in a multitask learning way. However, integrating super-resolution and unmixing directly may suffer two challenges: Task affinity is not verified, and the convergence of unmixing is not guaranteed. To address the above issues, in this paper, we provide theoretical analysis and propose super-resolution guided multi-task learning method for hyperspectral unmixing (SMILE). The provided theoretical analysis validates feasibility of multitask learning way and verifies task affinity, which consists of relationship and existence theorems by proving the positive guidance of super-resolution. The proposed framework generalizes positive information from super-resolution to unmixing by learning both shared and specific representations. Moreover, to guarantee the convergence, we provide the accessibility theorem by proving the optimal solution of unmixing. The major contributions of SMILE include providing progressive theoretical support, and designing a new framework for unmixing under the guidance of super-resolution. Our experiments on both synthetic and real datasets have substantiate the usefulness of our work.</li>
<li><strong>摘要：</strong>高光谱脉冲的性能可能会受到低空间分辨率的限制，可以使用多任务学习方式使用超分辨率来增强这种分辨率。但是，整合超分辨率并直接混合可能会遇到两个挑战：未验证任务亲和力，并且无法保证Unmixing的收敛性。为了解决上述问题，在本文中，我们提供了理论分析，并提出了高分辨率指导的多任务学习方法，以实现高光谱脉冲（Smile）。提供的理论分析验证了多任务学习方式的可行性，并验证任务亲和力，其中包括关系和存在定理，通过证明超分辨率的积极指导。所提出的框架通过学习共享和特定表示形式来概括从超分辨率到UNBING的积极信息。此外，为了确保收敛性，我们通过证明未混合的最佳解决方案来提供可访问性定理。微笑的主要贡献包括提供渐进的理论支持，并在超分辨率的指导下设计一个新的框架，以拆开。我们对合成数据集和真实数据集的实验证实了我们工作的有用性。</li>
</ul>

<h3>Title: Filling the Gaps: A Multitask Hybrid Multiscale Generative Framework for Missing Modality in Remote Sensing Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Nhi Kieu, Kien Nguyen, Arnold Wiliem, Clinton Fookes, Sridha Sridharan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11102">https://arxiv.org/abs/2509.11102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11102">https://arxiv.org/pdf/2509.11102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11102]] Filling the Gaps: A Multitask Hybrid Multiscale Generative Framework for Missing Modality in Remote Sensing Semantic Segmentation(https://arxiv.org/abs/2509.11102)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multimodal learning has shown significant performance boost compared to ordinary unimodal models across various domains. However, in real-world scenarios, multimodal signals are susceptible to missing because of sensor failures and adverse weather conditions, which drastically deteriorates models' operation and performance. Generative models such as AutoEncoder (AE) and Generative Adversarial Network (GAN) are intuitive solutions aiming to reconstruct missing modality from available ones. Yet, their efficacy in remote sensing semantic segmentation remains underexplored. In this paper, we first examine the limitations of existing generative approaches in handling the heterogeneity of multimodal remote sensing data. They inadequately capture semantic context in complex scenes with large intra-class and small inter-class variation. In addition, traditional generative models are susceptible to heavy dependence on the dominant modality, introducing bias that affects model robustness under missing modality conditions. To tackle these limitations, we propose a novel Generative-Enhanced MultiModal learning Network (GEMMNet) with three key components: (1) Hybrid Feature Extractor (HyFEx) to effectively learn modality-specific representations, (2) Hybrid Fusion with Multiscale Awareness (HyFMA) to capture modality-synergistic semantic context across scales and (3) Complementary Loss (CoLoss) scheme to alleviate the inherent bias by encouraging consistency across modalities and tasks. Our method, GEMMNet, outperforms both generative baselines AE, cGAN (conditional GAN), and state-of-the-art non-generative approaches - mmformer and shaspec - on two challenging semantic segmentation remote sensing datasets (Vaihingen and Potsdam). Source code is made available.</li>
<li><strong>摘要：</strong>与跨各个领域的普通单峰模型相比，多模式学习显示出明显的性能提升。但是，在实际情况下，由于传感器故障和不利的天气条件，多模式信号容易丢失，这极大地恶化了模型的操作和性能。诸如自动编码器（AE）和生成对抗网络（GAN）之类的生成模型是直观的解决方案，旨在重建可用模态的缺失模式。然而，它们在遥感语义分段中的功效仍然没有得到充实。在本文中，我们首先研究了处理多模式遥感数据异质性时现有生成方法的局限性。他们在具有较大的级别和小类间变化的复杂场景中捕获语义上下文。此外，传统的生成模型易受对主要模式的严重依赖，引入了在缺失的模态条件下影响模型鲁棒性的偏见。 To tackle these limitations, we propose a novel Generative-Enhanced MultiModal learning Network (GEMMNet) with three key components: (1) Hybrid Feature Extractor (HyFEx) to effectively learn modality-specific representations, (2) Hybrid Fusion with Multiscale Awareness (HyFMA) to capture modality-synergistic semantic context across scales and (3) Complementary Loss (CoLoss) scheme to alleviate the inherent通过鼓励方式和任务的一致性来偏见。在两个具有挑战性的语义分段遥感数据集（Vaihingenen and Potsdam）上，我们的方法（Gemmnet）优于生成基线AE，CGAN（有条件的GAN）和最先进的非代方法-Mmmforment和ShaSpec。源代码可用。</li>
</ul>

<h3>Title: WildSmoke: Ready-to-Use Dynamic 3D Smoke Assets from a Single Video in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Yuqiu Liu, Jialin Song, Manolis Savva, Wuyang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11114">https://arxiv.org/abs/2509.11114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11114">https://arxiv.org/pdf/2509.11114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11114]] WildSmoke: Ready-to-Use Dynamic 3D Smoke Assets from a Single Video in the Wild(https://arxiv.org/abs/2509.11114)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose a pipeline to extract and reconstruct dynamic 3D smoke assets from a single in-the-wild video, and further integrate interactive simulation for smoke design and editing. Recent developments in 3D vision have significantly improved reconstructing and rendering fluid dynamics, supporting realistic and temporally consistent view synthesis. However, current fluid reconstructions rely heavily on carefully controlled clean lab environments, whereas real-world videos captured in the wild are largely underexplored. We pinpoint three key challenges of reconstructing smoke in real-world videos and design targeted techniques, including smoke extraction with background removal, initialization of smoke particles and camera poses, and inferring multi-view videos. Our method not only outperforms previous reconstruction and generation methods with high-quality smoke reconstructions (+2.22 average PSNR on wild videos), but also enables diverse and realistic editing of fluid dynamics by simulating our smoke assets. We provide our models, data, and 4D smoke assets at [this https URL](this https URL).</li>
<li><strong>摘要：</strong>我们提出了一条管道，以从单个野外视频中提取和重建动态3D烟雾资产，并进一步整合烟雾设计和编辑的交互式仿真。 3D视力的最新发展显着改善了重建和渲染流体动力学，支持现实且在时间上一致的视图综合。但是，当前的流体重建在很大程度上依赖于精心控制的清洁实验室环境，而在野外捕获的现实世界的视频很大程度上却没有被逐渐解散。我们指出了在实际视频和设计目标技术中重建烟雾的三个关键挑战，包括烟气清除，烟雾颗粒和相机姿势的初始化以及推断多视频视频的初始化。我们的方法不仅要优于先前的重建和发电方法，并具有高质量的烟雾重建（+2.22野生视频中的平均PSNR），而且还可以通过模拟我们的烟雾资产来对流体动力学进行多样化和现实的编辑。我们在[此HTTPS URL]（此HTTPS URL）上提供模型，数据和4D烟雾资产。</li>
</ul>

<h3>Title: Feature Space Topology Control via Hopkins Loss</h3>
<ul>
<li><strong>Authors: </strong>Einari Vaaras, Manu Airaksinen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11154">https://arxiv.org/abs/2509.11154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11154">https://arxiv.org/pdf/2509.11154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11154]] Feature Space Topology Control via Hopkins Loss(https://arxiv.org/abs/2509.11154)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Feature space topology refers to the organization of samples within the feature space. Modifying this topology can be beneficial in machine learning applications, including dimensionality reduction, generative modeling, transfer learning, and robustness to adversarial attacks. This paper introduces a novel loss function, Hopkins loss, which leverages the Hopkins statistic to enforce a desired feature space topology, which is in contrast to existing topology-related methods that aim to preserve input feature topology. We evaluate the effectiveness of Hopkins loss on speech, text, and image data in two scenarios: classification and dimensionality reduction using nonlinear bottleneck autoencoders. Our experiments show that integrating Hopkins loss into classification or dimensionality reduction has only a small impact on classification performance while providing the benefit of modifying feature topology.</li>
<li><strong>摘要：</strong>特征空间拓扑是指特征空间内的样本组织。修改此拓扑可以对机器学习应用程序有益，包括降低维度，生成建模，转移学习和对对抗性攻击的鲁棒性。本文介绍了一种新颖的损失功能，即霍普金斯损失，该功能利用霍普金斯统计数据来强制执行所需的特征空间拓扑，这与现有与拓扑相关的方法形成鲜明对比，该方法旨在保留输入特征拓扑。我们在两种情况下评估了霍普金斯对语音，文本和图像数据损失的有效性：使用非线性瓶颈自动编码器降低分类和维度。我们的实验表明，将霍普金斯损失整合到分类或降低维度的情况下对分类性能的影响很小，同时提供了修改特征拓扑的好处。</li>
</ul>

<h3>Title: GK-SMOTE: A Hyperparameter-free Noise-Resilient Gaussian KDE-Based Oversampling Approach</h3>
<ul>
<li><strong>Authors: </strong>Mahabubur Rahman Miraj, Hongyu Huang, Ting Yang, Jinxue Zhao, Nankun Mu, Xinyu Lei</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11163">https://arxiv.org/abs/2509.11163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11163">https://arxiv.org/pdf/2509.11163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11163]] GK-SMOTE: A Hyperparameter-free Noise-Resilient Gaussian KDE-Based Oversampling Approach(https://arxiv.org/abs/2509.11163)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Imbalanced classification is a significant challenge in machine learning, especially in critical applications like medical diagnosis, fraud detection, and cybersecurity. Traditional oversampling techniques, such as SMOTE, often fail to handle label noise and complex data distributions, leading to reduced classification accuracy. In this paper, we propose GK-SMOTE, a hyperparameter-free, noise-resilient extension of SMOTE, built on Gaussian Kernel Density Estimation (KDE). GK-SMOTE enhances class separability by generating synthetic samples in high-density minority regions, while effectively avoiding noisy or ambiguous areas. This self-adaptive approach uses Gaussian KDE to differentiate between safe and noisy regions, ensuring more accurate sample generation without requiring extensive parameter tuning. Our extensive experiments on diverse binary classification datasets demonstrate that GK-SMOTE outperforms existing state-of-the-art oversampling techniques across key evaluation metrics, including MCC, Balanced Accuracy, and AUPRC. The proposed method offers a robust, efficient solution for imbalanced classification tasks, especially in noisy data environments, making it an attractive choice for real-world applications.</li>
<li><strong>摘要：</strong>分类不平衡是机器学习的重大挑战，尤其是在医学诊断，欺诈检测和网络安全等关键应用中。传统的过采样技术（例如Smote）通常无法处理标签噪声和复杂的数据分布，从而降低了分类精度。在本文中，我们提出了基于高斯核密度估计（KDE）的GK-Smote，这是Smote的无高参数，弹性扩展。 GK-Smote通过在高密度少数地区产生合成样本来增强类可分离性，同时有效地避免了嘈杂或模棱两可的区域。这种自适应方法使用高斯KDE来区分安全和嘈杂区域，从而在不需要大量参数调整的情况下确保更准确的样品生成。我们对各种二进制分类数据集进行的广泛实验表明，GK-Smote在关键评估指标（包括MCC，平衡的准确性和AUPRC）上优于现有的最新过度采样技术。所提出的方法为分类任务提供了强大，有效的解决方案，尤其是在嘈杂的数据环境中，使其成为现实世界应用程序的吸引人选择。</li>
</ul>

<h3>Title: Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation for Causal Inference in Traffic</h3>
<ul>
<li><strong>Authors: </strong>Waikit Xiu, Qiang Lu, Xiying Li, Chen Hu, Shengbo Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11165">https://arxiv.org/abs/2509.11165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11165">https://arxiv.org/pdf/2509.11165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11165]] Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation for Causal Inference in Traffic(https://arxiv.org/abs/2509.11165)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As intelligent transportation systems advance, traffic video understanding plays an increasingly pivotal role in comprehensive scene perception and causal analysis. Yet, existing approaches face notable challenges in accurately modeling spatiotemporal causality and integrating domain-specific knowledge, limiting their effectiveness in complex scenarios. To address these limitations, we propose Traffic-MLLM, a multimodal large language model tailored for fine-grained traffic analysis. Built on the Qwen2.5-VL backbone, our model leverages high-quality traffic-specific multimodal datasets and uses Low-Rank Adaptation (LoRA) for lightweight fine-tuning, significantly enhancing its capacity to model continuous spatiotemporal features in video sequences. Furthermore, we introduce an innovative knowledge prompting module fusing Chain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG), enabling precise injection of detailed traffic regulations and domain knowledge into the inference process. This design markedly boosts the model's logical reasoning and knowledge adaptation capabilities. Experimental results on TrafficQA and DriveQA benchmarks show Traffic-MLLM achieves state-of-the-art performance, validating its superior ability to process multimodal traffic data. It also exhibits remarkable zero-shot reasoning and cross-scenario generalization capabilities.</li>
<li><strong>摘要：</strong>随着智能运输系统的发展，交通视频理解在综合场景感知和因果分析中起着越来越重要的作用。然而，现有方法在准确地建模时空因果关系和整合特定于领域的知识方面面临着明显的挑战，从而限制了它们在复杂场景中的有效性。为了解决这些限制，我们提出了量身定制的用于细粒度流量分析的多模式大语言模型。基于QWEN2.5-VL主链，我们的模型利用了高质量的特定于特定于交通的多模式数据集，并使用低级适应（LORA）进行轻质微调，从而显着增强了其在视频序列中建模连续时空特征的能力。此外，我们引入了一种创新的知识，促使模块融合了三链（COT）推理（COT）推理，并带有检索功能增强的一代（RAG），从而可以精确地注入详细的交通法规和域知识，以进入推理过程。这种设计显着提高了模型的逻辑推理和知识适应能力。交通QA和DriveQA基准测试的实验结果表明，流量-MLLM实现了最先进的性能，从而验证了其处理多模式流量数据的卓越能力。它还表现出显着的零射线推理和跨阵营的概括能力。</li>
</ul>

<h3>Title: Scaling Up Forest Vision with Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Yihang She, Andrew Blake, David Coomes, Srinivasan Keshav</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11201">https://arxiv.org/abs/2509.11201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11201">https://arxiv.org/pdf/2509.11201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11201]] Scaling Up Forest Vision with Synthetic Data(https://arxiv.org/abs/2509.11201)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accurate tree segmentation is a key step in extracting individual tree metrics from forest laser scans, and is essential to understanding ecosystem functions in carbon cycling and beyond. Over the past decade, tree segmentation algorithms have advanced rapidly due to developments in AI. However existing, public, 3D forest datasets are not large enough to build robust tree segmentation systems. Motivated by the success of synthetic data in other domains such as self-driving, we investigate whether similar approaches can help with tree segmentation. In place of expensive field data collection and annotation, we use synthetic data during pretraining, and then require only minimal, real forest plot annotation for fine-tuning. We have developed a new synthetic data generation pipeline to do this for forest vision tasks, integrating advances in game-engines with physics-based LiDAR simulation. As a result, we have produced a comprehensive, diverse, annotated 3D forest dataset on an unprecedented scale. Extensive experiments with a state-of-the-art tree segmentation algorithm and a popular real dataset show that our synthetic data can substantially reduce the need for labelled real data. After fine-tuning on just a single, real, forest plot of less than 0.1 hectare, the pretrained model achieves segmentations that are competitive with a model trained on the full scale real data. We have also identified critical factors for successful use of synthetic data: physics, diversity, and scale, paving the way for more robust 3D forest vision systems in the future. Our data generation pipeline and the resulting dataset are available at this https URL.</li>
<li><strong>摘要：</strong>准确的树分割是从森林激光扫描中提取单个树指标的关键步骤，对于理解碳循环及其他地区的生态系统功能至关重要。在过去的十年中，由于AI的发展，树木分割算法已迅速发展。但是，现有的公共3D森林数据集还不够大，无法构建强大的树细分系统。由于自动驾驶等其他领域中合成数据的成功的动机，我们研究了类似的方法是否可以帮助树分割。代替昂贵的现场数据收集和注释，我们在预处理过程中使用合成数据，然后仅需要微小的森林图注释来进行微调。我们已经开发了一种新的合成数据生成管道来实现森林视觉任务，将游戏引擎的进步与基于物理的LiDAR模拟整合在一起。结果，我们以前所未有的规模生产了一个全面，多样的注释的3D森林数据集。使用最先进的树分割算法和流行的真实数据集进行的广泛实验表明，我们的合成数据可以大大减少对实际数据的需求。在仅对一个小于0.1公顷的单一真实的森林图进行微调后，验证的模型就可以通过对完整规模的真实数据进行训练的模型来实现竞争性的分割。我们还确定了成功使用合成数据的关键因素：物理，多样性和规模，为将来的3D森林视觉系统铺平了道路。我们的数据生成管道和所得数据集可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Beyond Sliders: Mastering the Art of Diffusion-based Image Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Yufei Tang, Daiheng Gao, Pingyu Wu, Wenbo Zhou, Bang Zhang, Weiming Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11213">https://arxiv.org/abs/2509.11213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11213">https://arxiv.org/pdf/2509.11213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11213]] Beyond Sliders: Mastering the Art of Diffusion-based Image Manipulation(https://arxiv.org/abs/2509.11213)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In the realm of image generation, the quest for realism and customization has never been more pressing. While existing methods like concept sliders have made strides, they often falter when it comes to no-AIGC images, particularly images captured in real world settings. To bridge this gap, we introduce Beyond Sliders, an innovative framework that integrates GANs and diffusion models to facilitate sophisticated image manipulation across diverse image categories. Improved upon concept sliders, our method refines the image through fine grained guidance both textual and visual in an adversarial manner, leading to a marked enhancement in image quality and realism. Extensive experimental validation confirms the robustness and versatility of Beyond Sliders across a spectrum of applications.</li>
<li><strong>摘要：</strong>在图像生成的领域中，对现实主义和定制的追求从未如此紧迫。尽管概念滑块之类的现有方法取得了长足的进步，但在无AIGC图像方面，它们通常会步履蹒跚，尤其是在现实世界中捕获的图像。为了弥合这一差距，我们介绍了超越滑块，这是一个创新的框架，该框架集成了gan和扩散模型，以促进各种图像类别的复杂图像操纵。通过对概念滑块进行了改进，我们的方法通过对抗性方式通过细粒度和视觉方式来完善图像，从而导致图像质量和现实主义的明显增强。广泛的实验验证证实了超越滑块跨应用程序的鲁棒性和多功能性。</li>
</ul>

<h3>Title: Synthetic Dataset Evaluation Based on Generalized Cross Validation</h3>
<ul>
<li><strong>Authors: </strong>Zhihang Song, Dingyi Yao, Ruibo Ming, Lihui Peng, Danya Yao, Yi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11273">https://arxiv.org/abs/2509.11273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11273">https://arxiv.org/pdf/2509.11273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11273]] Synthetic Dataset Evaluation Based on Generalized Cross Validation(https://arxiv.org/abs/2509.11273)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of synthetic dataset generation techniques, evaluating the quality of synthetic data has become a critical research focus. Robust evaluation not only drives innovations in data generation methods but also guides researchers in optimizing the utilization of these synthetic resources. However, current evaluation studies for synthetic datasets remain limited, lacking a universally accepted standard framework. To address this, this paper proposes a novel evaluation framework integrating generalized cross-validation experiments and domain transfer learning principles, enabling generalizable and comparable assessments of synthetic dataset quality. The framework involves training task-specific models (e.g., YOLOv5s) on both synthetic datasets and multiple real-world benchmarks (e.g., KITTI, BDD100K), forming a cross-performance matrix. Following normalization, a Generalized Cross-Validation (GCV) Matrix is constructed to quantify domain transferability. The framework introduces two key metrics. One measures the simulation quality by quantifying the similarity between synthetic data and real-world datasets, while another evaluates the transfer quality by assessing the diversity and coverage of synthetic data across various real-world scenarios. Experimental validation on Virtual KITTI demonstrates the effectiveness of our proposed framework and metrics in assessing synthetic data fidelity. This scalable and quantifiable evaluation solution overcomes traditional limitations, providing a principled approach to guide synthetic dataset optimization in artificial intelligence research.</li>
<li><strong>摘要：</strong>随着合成数据集生成技术的快速发展，评估合成数据的质量已成为关键的研究重点。强大的评估不仅可以推动数据生成方法的创新，还可以指导研究人员优化这些合成资源的利用。但是，当前针对合成数据集的评估研究仍然有限，缺乏普遍接受的标准框架。为了解决这个问题，本文提出了一个新的评估框架，该框架整合了广义的交叉验证实验和域转移学习原理，从而实现了合成数据集质量的可概括和可比评估。该框架涉及合成数据集和多个现实世界基准（例如Kitti，bdd100k）上的培训特定模型（例如，Yolov5s），形成了交叉表现矩阵。归一化后，构建了广义交叉验证（GCV）矩阵以量化域的可传递性。该框架引入了两个关键指标。一个人通过量化综合数据和现实世界数据集之间的相似性来衡量模拟质量，而另一个通过评估各种现实世界情景中合成数据的多样性和覆盖范围来评估传输质量。对虚拟Kitti的实验验证证明了我们提出的框架和指标在评估合成数据保真度中的有效性。这种可扩展且可量化的评估解决方案克服了传统的局限性，提供了一种原则性的方法来指导人工智能研究中的合成数据集优化。</li>
</ul>

<h3>Title: PINGS: Physics-Informed Neural Network for Fast Generative Sampling</h3>
<ul>
<li><strong>Authors: </strong>Achmad Ardani Prasha, Clavino Ourizqi Rachmadi, Muhamad Fauzan Ibnu Syahlan, Naufal Rahfi Anugerah, Nanda Garin Raditya, Putri Amelia, Sabrina Laila Mutiara, Hilman Syachr Ramadhan</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11284">https://arxiv.org/abs/2509.11284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11284">https://arxiv.org/pdf/2509.11284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11284]] PINGS: Physics-Informed Neural Network for Fast Generative Sampling(https://arxiv.org/abs/2509.11284)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce PINGS (Physics-Informed Neural Network for Fast Generative Sampling), a framework that amortizes diffusion sampling by training a physics-informed network to approximate reverse-time probability-flow dynamics, reducing sampling to a single forward pass (NFE = 1). As a proof of concept, we learn a direct map from a 3D standard normal to a non-Gaussian Gaussian Mixture Model (GMM). PINGS preserves the target's distributional structure (multi-bandwidth kernel $MMD^2 = 1.88 \times 10^{-2}$ with small errors in mean, covariance, skewness, and excess kurtosis) and achieves constant-time generation: $10^4$ samples in $16.54 \pm 0.56$ millisecond on an RTX 3090, versus 468-843 millisecond for DPM-Solver (10/20) and 960 millisecond for DDIM (50) under matched conditions. We also sanity-check the PINN/automatic-differentiation pipeline on a damped harmonic oscillator, obtaining MSEs down to $\mathcal{O}(10^{-5})$. Compared to fast but iterative ODE solvers and direct-map families (Flow, Rectified-Flow, Consistency), PINGS frames generative sampling as a PINN-style residual problem with endpoint anchoring, yielding a white-box, differentiable map with NFE = 1. These proof-of-concept results position PINGS as a promising route to fast, function-based generative sampling with potential extensions to scientific simulation (e.g., fast calorimetry).</li>
<li><strong>摘要：</strong>我们介绍了PINGS（物理知识的神经网络，用于快速生成采样），该框架通过训练物理信息信息网络来分配扩散采样，以近似反向时概率流动性，从而将采样减少到单个正向通行证（NFE = 1）。作为概念的证明，我们从3D标准的直率到非高斯高斯混合物模型（GMM）学习了直接地图。 PINGS preserves the target's distributional structure (multi-bandwidth kernel $MMD^2 = 1.88 \times 10^{-2}$ with small errors in mean, covariance, skewness, and excess kurtosis) and achieves constant-time generation: $10^4$ samples in $16.54 \pm 0.56$ millisecond on an RTX 3090, versus 468-843在匹配的条件下，DPM溶剂剂（10/20）的毫秒（10/20）和960毫秒（50）。我们还理智地检查了抑制谐波振荡器上的Pinn/自动分化管道，将MSE降低到$ \ Mathcal {O}（10^{ -  5}）$。与快速但迭代的ODE求解器和直接映射家族（流量，整流流，一致性）相比，ping框架生成抽样作为带有端点锚固的Pinn式残留问题，产生了一个白色盒，具有NFE = 1的白色框，可区分的映射。这些证明的结果证明结果将结果置于基于快速的生成型（Extient of Extife simiption），以便于快速，可实现的型号（e量热法）。</li>
</ul>

<h3>Title: Toward Next-generation Medical Vision Backbones: Modeling Finer-grained Long-range Visual Dependency</h3>
<ul>
<li><strong>Authors: </strong>Mingyuan Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11328">https://arxiv.org/abs/2509.11328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11328">https://arxiv.org/pdf/2509.11328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11328]] Toward Next-generation Medical Vision Backbones: Modeling Finer-grained Long-range Visual Dependency(https://arxiv.org/abs/2509.11328)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Medical Image Computing (MIC) is a broad research topic covering both pixel-wise (e.g., segmentation, registration) and image-wise (e.g., classification, regression) vision tasks. Effective analysis demands models that capture both global long-range context and local subtle visual characteristics, necessitating fine-grained long-range visual dependency modeling. Compared to Convolutional Neural Networks (CNNs) that are limited by intrinsic locality, transformers excel at long-range modeling; however, due to the high computational loads of self-attention, transformers typically cannot process high-resolution features (e.g., full-scale image features before downsampling or patch embedding) and thus face difficulties in modeling fine-grained dependency among subtle medical image details. Concurrently, Multi-layer Perceptron (MLP)-based visual models are recognized as computation/memory-efficient alternatives in modeling long-range visual dependency but have yet to be widely investigated in the MIC community. This doctoral research advances deep learning-based MIC by investigating effective long-range visual dependency modeling. It first presents innovative use of transformers for both pixel- and image-wise medical vision tasks. The focus then shifts to MLPs, pioneeringly developing MLP-based visual models to capture fine-grained long-range visual dependency in medical images. Extensive experiments confirm the critical role of long-range dependency modeling in MIC and reveal a key finding: MLPs provide feasibility in modeling finer-grained long-range dependency among higher-resolution medical features containing enriched anatomical/pathological details. This finding establishes MLPs as a superior paradigm over transformers/CNNs, consistently enhancing performance across various medical vision tasks and paving the way for next-generation medical vision backbones.</li>
<li><strong>摘要：</strong>医学图像计算（MIC）是一个广泛的研究主题，涵盖了像素（例如，分段，注册）和图像（例如分类，回归）视觉任务的广泛研究主题。有效的分析需要捕获全球远程上下文和局部微妙视觉特征的模型，需要精细粒度的远程视觉依赖性建模。与受内在位置限制的卷积神经网络（CNN）相比，变形金刚在远程建模时表现出色。但是，由于自我注意力的较高计算载荷，变压器通常无法处理高分辨率特征（例如，在下采样或贴片嵌入之前的全尺度图像特征），因此在对细微的医疗图像详细信息中对细粒度依赖性进行建模时面临困难。同时，基于多层的感知器（MLP）的视觉模型被认为是对长距离视觉依赖性建模的计算/记忆效率替代方法，但尚未在MIC社区进行广泛研究。这项博士研究通过研究有效的远程视觉依赖性建模来推动基于学习的麦克风。它首先提出了对像素和图像医学视觉任务的创新使用。然后，焦点转移到MLP，开发基于MLP的视觉模型，以捕获医学图像中细粒的长距离视觉依赖性。广泛的实验证实了远程依赖模型在MIC中的关键作用，并揭示了一个关键发现：MLP提供了建模更粒度的长期依赖性的可行性。这一发现确立了MLP比变形金刚/CNN的优越范式，从而不断提高各种医疗视觉任务的性能，并为下一代医学视觉骨架铺平了道路。</li>
</ul>

<h3>Title: GLaVE-Cap: Global-Local Aligned Video Captioning with Vision Expert Integration</h3>
<ul>
<li><strong>Authors: </strong>Wan Xu, Feng Zhu, Yihan Zeng, Yuanfan Guo, Ming Liu, Hang Xu, Wangmeng Zuo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11360">https://arxiv.org/abs/2509.11360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11360">https://arxiv.org/pdf/2509.11360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11360]] GLaVE-Cap: Global-Local Aligned Video Captioning with Vision Expert Integration(https://arxiv.org/abs/2509.11360)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Video detailed captioning aims to generate comprehensive video descriptions to facilitate video understanding. Recently, most efforts in the video detailed captioning community have been made towards a local-to-global paradigm, which first generates local captions from video clips and then summarizes them into a global caption. However, we find this paradigm leads to less detailed and contextual-inconsistent captions, which can be attributed to (1) no mechanism to ensure fine-grained captions, and (2) weak interaction between local and global captions. To remedy the above two issues, we propose GLaVE-Cap, a Global-Local aligned framework with Vision Expert integration for Captioning, which consists of two core modules: TrackFusion enables comprehensive local caption generation, by leveraging vision experts to acquire cross-frame visual prompts, coupled with a dual-stream structure; while CaptionBridge establishes a local-global interaction, by using global context to guide local captioning, and adaptively summarizing local captions into a coherent global caption. Besides, we construct GLaVE-Bench, a comprehensive video captioning benchmark featuring 5X more queries per video than existing benchmarks, covering diverse visual dimensions to facilitate reliable evaluation. We further provide a training dataset GLaVE-1.2M containing 16K high-quality fine-grained video captions and 1.2M related question-answer pairs. Extensive experiments on four benchmarks show that our GLaVE-Cap achieves state-of-the-art performance. Besides, the ablation studies and student model analyses further validate the effectiveness of the proposed modules and the contribution of GLaVE-1.2M to the video understanding community. The source code, model weights, benchmark, and dataset will be open-sourced.</li>
<li><strong>摘要：</strong>视频详细字幕旨在生成全面的视频描述以促进视频理解。最近，视频中的大多数努力详细介绍了字幕社区，这是对局部到全球范式的，该范式首先从视频剪辑中生成本地字幕，然后将它们汇总为全球标题。但是，我们发现此范式导致较不详细和上下文的字幕，这可以归因于（1）没有确保细粒字幕的机制，以及（2）本地字幕和全球字幕之间的较弱的相互作用。为了纠正上述两个问题，我们提出了Glave-CAP，这是一个全球本地对齐的框架，具有视觉专家的字幕集成，该框架由两个核心模块组成：TrackFusion可以通过利用视觉专家来获取跨框架视觉提示，并与双流结构相结合；尽管Captionbridge建立了本地全球互动，但使用全球上下文来指导本地字幕，并将本地字幕汇总到连贯的全球字幕中。此外，我们构建了Glave Bench，这是一个全面的视频字幕标题，每个视频的查询比现有基准测试多5倍，涵盖了各种视觉维度，以促进可靠的评估。我们进一步提供了一个培训数据集glave-1.2m，其中包含16K高质量的高粒度视频字幕和120万相关的问题 - 答案对。对四个基准测试的广泛实验表明，我们的魅力帽可以达到最新的性能。此外，消融研究和学生模型分析进一步验证了所提出的模块的有效性以及Glave-1.2m对视频理解社区的贡献。源代码，模型权重，基准和数据集将被开源。</li>
</ul>

<h3>Title: Decoding Musical Origins: Distinguishing Human and AI Composers</h3>
<ul>
<li><strong>Authors: </strong>Cheng-Yang Tsai, Tzu-Wei Huang, Shao-Yu Wei, Guan-Wei Chen, Hung-Ying Chu, Yu-Cheng Lin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11369">https://arxiv.org/abs/2509.11369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11369">https://arxiv.org/pdf/2509.11369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11369]] Decoding Musical Origins: Distinguishing Human and AI Composers(https://arxiv.org/abs/2509.11369)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of Large Language Models (LLMs), AI-driven music generation has become a vibrant and fruitful area of research. However, the representation of musical data remains a significant challenge. To address this, a novel, machine-learning-friendly music notation system, YNote, was developed. This study leverages YNote to train an effective classification model capable of distinguishing whether a piece of music was composed by a human (Native), a rule-based algorithm (Algorithm Generated), or an LLM (LLM Generated). We frame this as a text classification problem, applying the Term Frequency-Inverse Document Frequency (TF-IDF) algorithm to extract structural features from YNote sequences and using the Synthetic Minority Over-sampling Technique (SMOTE) to address data imbalance. The resulting model achieves an accuracy of 98.25%, successfully demonstrating that YNote retains sufficient stylistic information for analysis. More importantly, the model can identify the unique " technological fingerprints " left by different AI generation techniques, providing a powerful tool for tracing the origins of AI-generated content.</li>
<li><strong>摘要：</strong>随着大语言模型（LLM）的快速发展，AI驱动的音乐发电已成为一个充满活力和富有成果的研究领域。但是，音乐数据的表示仍然是一个重大挑战。为了解决这个问题，开发了一种新颖的，机器学习的音乐符号系统Ynote。这项研究利用Ynote来训练有效的分类模型，能够区分音乐是由人（本地），基于规则的算法（生成的算法）或LLM（LLM生成）组成的。我们将其构架为文本分类问题，应用术语频率插图频率（TF-IDF）算法从Ynote序列中提取结构特征，并使用合成少数群体过采样技术（SMOTE）来解决数据不平衡。最终的模型的准确性为98.25％，成功地证明Ynote保留了足够的风格信息进行分析。更重要的是，该模型可以识别不同AI生成技术留下的独特的“技术指纹”，从而为追踪AI生成的内容的起源提供了强大的工具。</li>
</ul>

<h3>Title: Intelligent Reservoir Decision Support: An Integrated Framework Combining Large Language Models, Advanced Prompt Engineering, and Multimodal Data Fusion for Real-Time Petroleum Operations</h3>
<ul>
<li><strong>Authors: </strong>Seyed Kourosh Mahjour, Seyed Saman Mahjour</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11376">https://arxiv.org/abs/2509.11376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11376">https://arxiv.org/pdf/2509.11376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11376]] Intelligent Reservoir Decision Support: An Integrated Framework Combining Large Language Models, Advanced Prompt Engineering, and Multimodal Data Fusion for Real-Time Petroleum Operations(https://arxiv.org/abs/2509.11376)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The petroleum industry faces unprecedented challenges in reservoir management, requiring rapid integration of complex multimodal datasets for real-time decision support. This study presents a novel integrated framework combining state-of-the-art large language models (GPT-4o, Claude 4 Sonnet, Gemini 2.5 Pro) with advanced prompt engineering techniques and multimodal data fusion for comprehensive reservoir analysis. The framework implements domain-specific retrieval-augmented generation (RAG) with over 50,000 petroleum engineering documents, chain-of-thought reasoning, and few-shot learning for rapid field adaptation. Multimodal integration processes seismic interpretations, well logs, and production data through specialized AI models with vision transformers. Field validation across 15 diverse reservoir environments demonstrates exceptional performance: 94.2% reservoir characterization accuracy, 87.6% production forecasting precision, and 91.4% well placement optimization success rate. The system achieves sub-second response times while maintaining 96.2% safety reliability with no high-risk incidents during evaluation. Economic analysis reveals 62-78% cost reductions (mean 72%) relative to traditional methods with 8-month payback period. Few-shot learning reduces field adaptation time by 72%, while automated prompt optimization achieves 89% improvement in reasoning quality. The framework processed real-time data streams with 96.2% anomaly detection accuracy and reduced environmental incidents by 45%. We provide detailed experimental protocols, baseline comparisons, ablation studies, and statistical significance testing to ensure reproducibility. This research demonstrates practical integration of cutting-edge AI technologies with petroleum domain expertise for enhanced operational efficiency, safety, and economic performance.</li>
<li><strong>摘要：</strong>石油行业在储层管理方面面临着前所未有的挑战，需要快速整合复杂的多模式数据集以实时决策支持。这项研究提出了一个新颖的集成框架，结合了最先进的大语言模型（GPT-4O，Claude 4 Sonnet，Gemini 2.5 Pro）与先进的及时工程技术和多模式数据融合，以进行全面的储层分析。该框架具有特定于域特定的检索功能生成（RAG），其中有50,000多种石油工程文档，经过想象的推理和几乎没有用于快速现场适应的射击学习。多模式集成过程通过具有视觉变压器的专业AI模型来进行地震解释，井原木和生产数据。跨15种不同储层环境的现场验证表明了出色的性能：94.2％的储层表征准确性，87.6％的生产预测精度和91.4％的井位置优化成功率。该系统达到了下一步的响应时间，同时保持96.2％的安全可靠性，而在评估过程中没有高风险事件。经济分析显示，相对于8个月回报期的传统方法，成本降低了62-78％（平均72％）。很少有学习的学习可以将现场适应时间减少72％，而自动化及时优化的推理质量提高了89％。该框架以96.2％的异常检测准确性处理了实时数据流，并将环境事件降低了45％。我们提供详细的实验方案，基线比较，消融研究和统计显着性测试，以确保可重复性。这项研究表明，尖端AI技术与石油领域专业知识的实用整合，以提高运营效率，安全性和经济绩效。</li>
</ul>

<h3>Title: No Modality Left Behind: Dynamic Model Generation for Incomplete Medical Data</h3>
<ul>
<li><strong>Authors: </strong>Christoph Fürböck, Paul Weiser, Branko Mitic, Philipp Seeböck, Thomas Helbich, Georg Langs</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11406">https://arxiv.org/abs/2509.11406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11406">https://arxiv.org/pdf/2509.11406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11406]] No Modality Left Behind: Dynamic Model Generation for Incomplete Medical Data(https://arxiv.org/abs/2509.11406)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In real world clinical environments, training and applying deep learning models on multi-modal medical imaging data often struggles with partially incomplete data. Standard approaches either discard missing samples, require imputation or repurpose dropout learning schemes, limiting robustness and generalizability. To address this, we propose a hypernetwork-based method that dynamically generates task-specific classification models conditioned on the set of available modalities. Instead of training a fixed model, a hypernetwork learns to predict the parameters of a task model adapted to available modalities, enabling training and inference on all samples, regardless of completeness. We compare this approach with (1) models trained only on complete data, (2) state of the art channel dropout methods, and (3) an imputation-based method, using artificially incomplete datasets to systematically analyze robustness to missing modalities. Results demonstrate superior adaptability of our method, outperforming state of the art approaches with an absolute increase in accuracy of up to 8% when trained on a dataset with 25% completeness (75% of training data with missing modalities). By enabling a single model to generalize across all modality configurations, our approach provides an efficient solution for real-world multi-modal medical data analysis.</li>
<li><strong>摘要：</strong>在现实世界的临床环境中，对多模式医学成像数据进行培训和应用深度学习模型通常与部分不完整的数据斗争。标准方法要么丢弃丢失的样本，需要插补或重新使用辍学的学习方案，从而限制了鲁棒性和概括性。为了解决这个问题，我们提出了一种基于超网络的方法，该方法动态生成以可用模式为条件的特定任务分类模型。 HyperNetwork没有训练固定模型，而是学会了预测适合可用方式的任务模型的参数，从而对所有样品进行培训和推断，无论其完整性如何。我们将这种方法与仅在完整数据，（2）最新渠道辍学方法和（3）基于插补的方法的（1）模型进行比较的模型，使用人工不完整的数据集将基于插补的方法进行系统地分析缺失模态的鲁棒性。结果表明，我们的方法的卓越适应性，超过了艺术的方法，当在具有25％完整性的数据集中接受培训时，准确性的绝对提高到8％（75％的培训数据中有75％的模式缺失）。通过启用单个模型在所有模态配置中概括，我们的方法为现实世界多模式医学数据分析提供了有效的解决方案。</li>
</ul>

<h3>Title: MVQA-68K: A Multi-dimensional and Causally-annotated Dataset with Quality Interpretability for Video Assessment</h3>
<ul>
<li><strong>Authors: </strong>Yanyun Pu, Kehan Li, Zeyi Huang, Zhijie Zhong, Kaixiang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11589">https://arxiv.org/abs/2509.11589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11589">https://arxiv.org/pdf/2509.11589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11589]] MVQA-68K: A Multi-dimensional and Causally-annotated Dataset with Quality Interpretability for Video Assessment(https://arxiv.org/abs/2509.11589)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, quality assessment</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of video generation models such as Sora, video quality assessment (VQA) is becoming increasingly crucial for selecting high-quality videos from large-scale datasets used in pre-training. Traditional VQA methods, typically producing single numerical scores, often lack comprehensiveness and interpretability. To address these challenges, we introduce MVQA-68K, a novel multi-dimensional VQA dataset comprising over 68,000 carefully annotated videos, covering seven essential quality dimensions: overall aesthetics, camera movement, dynamic degree, texture detail, composition, visual quality, and factual consistency. Each annotation includes detailed chain-of-thought reasoning to facilitate interpretability and comprehensive understanding. Extensive experiments demonstrate that MVQA-68K significantly enhances the performance of various multimodal large language models (MLLMs) on the VQA task, achieving state-of-the-art results not only on our internal test set (Fig.1) but also on public benchmarks including LSVQ-test, LSVQ-1080p, and LIVE-VQC. Meantime, incorporating explicit reasoning process during VQA training substantially boosts the zero-shot generalization. Code and dataset will be available at github: this https URL</li>
<li><strong>摘要：</strong>随着诸如Sora之类的视频生成模型的快速发展，视频质量评估（VQA）对于从预训练中使用的大型数据集中选择高质量的视频变得越来越重要。传统的VQA方法通常会产生单个数值分数，通常缺乏全面性和解释性。为了应对这些挑战，我们介绍了MVQA-68K，这是一种新型的多维VQA数据集，其中包括68,000多个精心注释的视频，涵盖了七个基本质量维度：整体美学，相机运动，动态程度，文本，文本，质地细节，组成，视觉质量和事实一致性。每个注释都包括详细的经过思考的推理，以促进解释性和全面的理解。广泛的实验表明，MVQA-68K显着提高了VQA任务上各种多模式大型语言模型（MLLM）的性能，不仅在我们的内部测试集（图1）上实现了最先进的结果（图1），还可以在包括LSVQ-TEST，LSVQ-TEST，LSVQ-1080P和LIVE-VQC上进行公共基准。同时，在VQA培训期间合并显式推理过程大大提高了零击的概括。代码和数据集将在GitHub上提供：此HTTPS URL</li>
</ul>

<h3>Title: Disentangling Content from Style to Overcome Shortcut Learning: A Hybrid Generative-Discriminative Learning Framework</h3>
<ul>
<li><strong>Authors: </strong>Siming Fu, Sijun Dong, Xiaoliang Meng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11598">https://arxiv.org/abs/2509.11598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11598">https://arxiv.org/pdf/2509.11598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11598]] Disentangling Content from Style to Overcome Shortcut Learning: A Hybrid Generative-Discriminative Learning Framework(https://arxiv.org/abs/2509.11598)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Despite the remarkable success of Self-Supervised Learning (SSL), its generalization is fundamentally hindered by Shortcut Learning, where models exploit superficial features like texture instead of intrinsic structure. We experimentally verify this flaw within the generative paradigm (e.g., MAE) and argue it is a systemic issue also affecting discriminative methods, identifying it as the root cause of their failure on unseen domains. While existing methods often tackle this at a surface level by aligning or separating domain-specific features, they fail to alter the underlying learning mechanism that fosters shortcut dependency. To address this at its core, we propose HyGDL (Hybrid Generative-Discriminative Learning Framework), a hybrid framework that achieves explicit content-style disentanglement. Our approach is guided by the Invariance Pre-training Principle: forcing a model to learn an invariant essence by systematically varying a bias (e.g., style) at the input while keeping the supervision signal constant. HyGDL operates on a single encoder and analytically defines style as the component of a representation that is orthogonal to its style-invariant content, derived via vector projection.</li>
<li><strong>摘要：</strong>尽管自我监督的学习取得了显着的成功（SSL），但它的概括从根本上受到了捷径学习的阻碍，在这种学习中，模型利用了浅表特征，例如纹理而不是内在的结构。我们在实验中验证了生成范式（例如MAE）中的这一缺陷，并认为这是一个系统性的问题，也影响了歧视方法，将其识别为它们在看不见领域失败的根本原因。尽管现有方法通常通过对齐或分离特定于域的特征来在表面水平上解决此问题，但它们无法改变培养快捷方式依赖性的基本学习机制。为了解决此问题，我们提出了HYGDL（混合生成歧义学习框架），这是一个实现明确内容式拆除的混合框架。我们的方法以不变性预训练原理为指导：强迫模型通过系统地在输入处系统地改变偏见（例如样式），同时使监督信号保持恒定。 HYGDL在单个编码器上运行，并分析将样式定义为与其样式不变内容正交的表示形式的组成部分，该内容通过向量投影得出。</li>
</ul>

<h3>Title: A Controllable 3D Deepfake Generation Framework with Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Wending Liu, Siyun Liang, Huy H. Nguyen, Isao Echizen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11624">https://arxiv.org/abs/2509.11624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11624">https://arxiv.org/pdf/2509.11624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11624]] A Controllable 3D Deepfake Generation Framework with Gaussian Splatting(https://arxiv.org/abs/2509.11624)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose a novel 3D deepfake generation framework based on 3D Gaussian Splatting that enables realistic, identity-preserving face swapping and reenactment in a fully controllable 3D space. Compared to conventional 2D deepfake approaches that suffer from geometric inconsistencies and limited generalization to novel view, our method combines a parametric head model with dynamic Gaussian representations to support multi-view consistent rendering, precise expression control, and seamless background integration. To address editing challenges in point-based representations, we explicitly separate the head and background Gaussians and use pre-trained 2D guidance to optimize the facial region across views. We further introduce a repair module to enhance visual consistency under extreme poses and expressions. Experiments on NeRSemble and additional evaluation videos demonstrate that our method achieves comparable performance to state-of-the-art 2D approaches in identity preservation, as well as pose and expression consistency, while significantly outperforming them in multi-view rendering quality and 3D consistency. Our approach bridges the gap between 3D modeling and deepfake synthesis, enabling new directions for scene-aware, controllable, and immersive visual forgeries, revealing the threat that emerging 3D Gaussian Splatting technique could be used for manipulation attacks.</li>
<li><strong>摘要：</strong>我们提出了一个基于3D高斯脱落的新型3D深泡产生框架，该框架可以在完全可控制的3D空间中实现现实，具有身份的面部交换和重新制定。与传统的2D深泡方法相比，几何不一致且对新观点的概括有限，我们的方法将参数头模型与动态高斯表示相结合，以支持多视图一致的渲染，精确的表达控制，精确的表达控制和无缝背景集成。为了解决基于点的表示中的编辑挑战，我们明确地将头部和背景高斯人分开，并使用预先训练的2D指南来优化面部区域。我们进一步引入了一个维修模块，以提高在极端姿势和表达式下的视觉一致性。关于Nersemble和其他评估视频的实验表明，我们的方法在身份保存方面的最先进的2D方法以及姿势和表达一致性可以达到可比的性能，同时在多视图渲染质量和3D一致性方面显着优于它们。我们的方法弥合了3D建模和深泡合成之间的差距，为场景吸引，可控和沉浸式视觉伪造提供了新的方向，从而揭示了新兴的3D高斯分裂技术可以用于操纵攻击的威胁。</li>
</ul>

<h3>Title: SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Fei Ren, Shaobo Wang, Kaixin Li, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11628">https://arxiv.org/abs/2509.11628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11628">https://arxiv.org/pdf/2509.11628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11628]] SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching(https://arxiv.org/abs/2509.11628)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models have revolutionized high-fidelity image and video synthesis, yet their computational demands remain prohibitive for real-time applications. These models face two fundamental challenges: strict temporal dependencies preventing parallelization, and computationally intensive forward passes required at each denoising step. Drawing inspiration from speculative decoding in large language models, we present SpeCa, a novel 'Forecast-then-verify' acceleration framework that effectively addresses both limitations. SpeCa's core innovation lies in introducing Speculative Sampling to diffusion models, predicting intermediate features for subsequent timesteps based on fully computed reference timesteps. Our approach implements a parameter-free verification mechanism that efficiently evaluates prediction reliability, enabling real-time decisions to accept or reject each prediction while incurring negligible computational overhead. Furthermore, SpeCa introduces sample-adaptive computation allocation that dynamically modulates resources based on generation complexity, allocating reduced computation for simpler samples while preserving intensive processing for complex instances. Experiments demonstrate 6.34x acceleration on FLUX with minimal quality degradation (5.5% drop), 7.3x speedup on DiT while preserving generation fidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The verification mechanism incurs minimal overhead (1.67%-3.5% of full inference costs), establishing a new paradigm for efficient diffusion model inference while maintaining generation quality even at aggressive acceleration ratios. Our codes have been released in Github: \textbf{this https URL}</li>
<li><strong>摘要：</strong>扩散模型已彻底改变了高保真图像和视频综合，但是它们的计算需求对于实时应用仍然令人难以置信。这些模型面临两个基本的挑战：严格的时间依赖性阻止并行化，以及在每个降级步骤中所需的计算密集远期通行证。我们提出了从大语模型中的投机解码中汲取灵感，我们提出了一种新颖的“预测 - 验证”加速框架，可有效解决这两个局限性。 Speca的核心创新在于将投机采样引入扩散模型，从而根据完全计算的参考时间段来预测后续时间段的中间特征。我们的方法实现了一种无参数验证机制，该机制有效地评估了预测可靠性，从而使实时决策能够接受或拒绝每个预测，同时产生可忽略的计算开销。此外，SpecA引入了样本自适应计算分配，该计算分配了基于生成复杂性的动态调节资源，为简单样本分配减少的计算，同时为复杂实例保留密集处理。实验表明，通量的加速度为6.34倍，质量降解最小（下降5.5％），DIT的7.3倍加速，同时保留发电的忠诚度，而Hunyuanvideo的Vbench得分为79.84％。验证机制会产生最小的开销（占全部推理成本的1.67％-3.5％），为有效的扩散模型推断建立了新的范式，同时即使在积极的加速比下也保持发电质量。我们的代码已在github中发布：\ textbf {this https url}</li>
</ul>

<h3>Title: WeatherBench: A Real-World Benchmark Dataset for All-in-One Adverse Weather Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Qiyuan Guan, Qianfeng Yang, Xiang Chen, Tianyu Song, Guiyue Jin, Jiyu Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11642">https://arxiv.org/abs/2509.11642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11642">https://arxiv.org/pdf/2509.11642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11642]] WeatherBench: A Real-World Benchmark Dataset for All-in-One Adverse Weather Image Restoration(https://arxiv.org/abs/2509.11642)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Existing all-in-one image restoration approaches, which aim to handle multiple weather degradations within a single framework, are predominantly trained and evaluated using mixed single-weather synthetic datasets. However, these datasets often differ significantly in resolution, style, and domain characteristics, leading to substantial domain gaps that hinder the development and fair evaluation of unified models. Furthermore, the lack of a large-scale, real-world all-in-one weather restoration dataset remains a critical bottleneck in advancing this field. To address these limitations, we present a real-world all-in-one adverse weather image restoration benchmark dataset, which contains image pairs captured under various weather conditions, including rain, snow, and haze, as well as diverse outdoor scenes and illumination settings. The resulting dataset provides precisely aligned degraded and clean images, enabling supervised learning and rigorous evaluation. We conduct comprehensive experiments by benchmarking a variety of task-specific, task-general, and all-in-one restoration methods on our dataset. Our dataset offers a valuable foundation for advancing robust and practical all-in-one image restoration in real-world scenarios. The dataset has been publicly released and is available at this https URL.</li>
<li><strong>摘要：</strong>现有的多合一图像恢复方法，旨在处理单个框架内多种天气降解，主要使用混合的单天合成数据集对训练和评估。但是，这些数据集在分辨率，样式和域特征方面通常差异很大，从而导致较大的领域差距阻碍了统一模型的开发和公平评估。此外，缺乏大规模的，现实世界中多合一的天气恢复数据集仍然是前进这一领域的关键瓶颈。为了解决这些限制，我们提出了一个真实世界中多合一的不利天气图像恢复基准数据集，其中包含在各种天气条件下捕获的图像对，包括雨，雪和雾霾，以及各种户外场景和照明设置。最终的数据集提供了精确的降级和干净的图像，从而实现了监督的学习和严格的评估。我们通过基准在数据集上的各种特定于任务，任务和多合一的恢复方法来进行全面的实验。我们的数据集为在现实情况下推进强大而实用的多合一图像修复的宝贵基础。该数据集已公开发布，可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Joint-octamamba:an octa joint segmentation network based on feature enhanced mamba</h3>
<ul>
<li><strong>Authors: </strong>Chuang Liu, Nan Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11649">https://arxiv.org/abs/2509.11649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11649">https://arxiv.org/pdf/2509.11649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11649]] Joint-octamamba:an octa joint segmentation network based on feature enhanced mamba(https://arxiv.org/abs/2509.11649)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>OCTA is a crucial non-invasive imaging technique for diagnosing and monitoring retinal diseases like diabetic retinopathy, age-related macular degeneration, and glaucoma. Current 2D-based methods for retinal vessel (RV) segmentation offer insufficient accuracy. To address this, we propose RVMamba, a novel architecture integrating multiple feature extraction modules with the Mamba state-space model. Moreover, existing joint segmentation models for OCTA data exhibit performance imbalance between different tasks. To simultaneously improve the segmentation of the foveal avascular zone (FAZ) and mitigate this imbalance, we introduce FAZMamba and a unified Joint-OCTAMamba framework. Experimental results on the OCTA-500 dataset demonstrate that Joint-OCTAMamba outperforms existing models across evaluation this http URL code is available at this https URL.</li>
<li><strong>摘要：</strong>OCTA是一种至关重要的非侵入性成像技术，用于诊断和监测视网膜疾病，例如糖尿病性视网膜病，与年龄相关的黄斑变性和青光眼。当前基于2D的视网膜血管（RV）分割方法提供了不足的精度。为了解决这个问题，我们提出了RVMamba，这是一种新颖的体系结构，将多个特征提取模块与Mamba状态空间模型集成在一起。此外，现有的八八个数据的联合细分模型在不同任务之间表现出性能不平衡。为了同时改善动脉凹陷区（FAZ）的分割并减轻这种不平衡，我们引入了Fazmamba和统一的联合octamamba框架。 Octa-500数据集的实验结果表明，在此HTTPS URL上可用http URL代码在评估中胜过现有模型。</li>
</ul>

<h3>Title: DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained Dirty Tableware Recognition</h3>
<ul>
<li><strong>Authors: </strong>Lifei Hao, Yue Cheng, Baoqi Huang, Bing Jia, Xuandong Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11661">https://arxiv.org/abs/2509.11661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11661">https://arxiv.org/pdf/2509.11661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11661]] DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained Dirty Tableware Recognition(https://arxiv.org/abs/2509.11661)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Intelligent tableware cleaning is a critical application in food safety and smart homes, but existing methods are limited by coarse-grained classification and scarcity of few-shot data, making it difficult to meet industrialization requirements. We propose DTGen, a few-shot data augmentation scheme based on generative diffusion models, specifically designed for fine-grained dirty tableware recognition. DTGen achieves efficient domain specialization through LoRA, generates diverse dirty images via structured prompts, and ensures data quality through CLIP-based cross-modal filtering. Under extremely limited real few-shot conditions, DTGen can synthesize virtually unlimited high-quality samples, significantly improving classifier performance and supporting fine-grained dirty tableware recognition. We further elaborate on lightweight deployment strategies, promising to transfer DTGen's benefits to embedded dishwashers and integrate with cleaning programs to intelligently regulate energy consumption and detergent usage. Research results demonstrate that DTGen not only validates the value of generative AI in few-shot industrial vision but also provides a feasible deployment path for automated tableware cleaning and food safety monitoring.</li>
<li><strong>摘要：</strong>智能餐具清洁是食品安全和智能家居中的关键应用，但是现有方法受到粗粒分类和少量数据的稀缺的限制，因此很难满足工业化要求。我们建议DTGEN，这是基于生成扩散模型的一些摄像数据增强方案，该方案专门为细粒度的肮脏餐具识别而设计。 DTGEN通过LORA实现有效的域专业化，通过结构化提示生成各种肮脏的图像，并通过基于夹子的跨模式过滤确保数据质量。在极有限的实际几次条件下，DTGEN可以合成几乎无限的高质量样品，从而显着提高分类器的性能并支持细颗粒的脏桌件识别。我们进一步详细介绍了轻型部署策略，承诺将DTGEN的好处转移到嵌入式洗碗机上，并与清洁计划集成，以智能调节能源消耗和清洁剂的用法。研究结果表明，DTGEN不仅在几乎没有工业视野中验证了生成AI的价值，而且还为自动餐具清洁和食品安全监控提​​供了可行的部署路径。</li>
</ul>

<h3>Title: Microsurgical Instrument Segmentation for Robot-Assisted Surgery</h3>
<ul>
<li><strong>Authors: </strong>Tae Kyeong Jeong, Garam Kim, Juyoun Park</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11727">https://arxiv.org/abs/2509.11727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11727">https://arxiv.org/pdf/2509.11727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11727]] Microsurgical Instrument Segmentation for Robot-Assisted Surgery(https://arxiv.org/abs/2509.11727)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of thin structures is critical for microsurgical scene understanding but remains challenging due to resolution loss, low contrast, and class imbalance. We propose Microsurgery Instrument Segmentation for Robotic Assistance(MISRA), a segmentation framework that augments RGB input with luminance channels, integrates skip attention to preserve elongated features, and employs an Iterative Feedback Module(IFM) for continuity restoration across multiple passes. In addition, we introduce a dedicated microsurgical dataset with fine-grained annotations of surgical instruments including thin objects, providing a benchmark for robust evaluation Dataset available at this https URL. Experiments demonstrate that MISRA achieves competitive performance, improving the mean class IoU by 5.37% over competing methods, while delivering more stable predictions at instrument contacts and overlaps. These results position MISRA as a promising step toward reliable scene parsing for computer-assisted and robotic microsurgery.</li>
<li><strong>摘要：</strong>薄结构的准确分割对于显微外科现场的理解至关重要，但由于分辨率损失，低对比度和阶级失衡，仍然具有挑战性。我们提出了用于机器人援助（MISRA）的显微外科手术仪器分割，这是一个分割框架，将RGB输入与亮度通道增加，集成了Skip注意以保持延长特征，并采用了迭代反馈模块（IFM），以跨多个通过。此外，我们还引入了一个专用的微表外科数据集，其中包含包括薄物体在内的手术仪器的细粒度注释，为此HTTPS URL提供了可用的可靠评估数据集的基准。实验表明，Misra实现了竞争性能，将平均级别的IOU提高了5.37％，而竞争方法则在仪器接触和重叠时提供了更稳定的预测。这些结果将Misra定位为朝着可靠的场景解析计算机辅助和机器人微型外科手术的有前途的一步。</li>
</ul>

<h3>Title: A Fully Open and Generalizable Foundation Model for Ultrasound Clinical Applications</h3>
<ul>
<li><strong>Authors: </strong>Hongyuan Zhang, Yuheng Wu, Mingyang Zhao, Zhiwei Chen, Rebecca Li, Fei Zhu, Haohan Zhao, Xiaohua Yuan, Meng Yang, Chunli Qiu, Xiang Cong, Haiyan Chen, Lina Luan, Randolph H.L. Wong, Huai Liao, Colin A Graham, Shi Chang, Guowei Tao, Dong Yi, Zhen Lei, Nassir Navab, Sebastien Ourselin, Jiebo Luo, Hongbin Liu, Gaofeng Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11752">https://arxiv.org/abs/2509.11752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11752">https://arxiv.org/pdf/2509.11752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11752]] A Fully Open and Generalizable Foundation Model for Ultrasound Clinical Applications(https://arxiv.org/abs/2509.11752)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) that can effectively learn ultrasound representations by integrating multi-source data holds significant promise for advancing clinical care. However, the scarcity of large labeled datasets in real-world clinical environments and the limited generalizability of task-specific models have hindered the development of generalizable clinical AI models for ultrasound applications. In this study, we present EchoCare, a novel ultrasound foundation model for generalist clinical use, developed via self-supervised learning on our curated, publicly available, large-scale dataset EchoCareData. EchoCareData comprises 4.5 million ultrasound images, sourced from over 23 countries across 5 continents and acquired via a diverse range of distinct imaging devices, thus encompassing global cohorts that are multi-center, multi-device, and multi-ethnic. Unlike prior studies that adopt off-the-shelf vision foundation model architectures, we introduce a hierarchical classifier into EchoCare to enable joint learning of pixel-level and representation-level features, capturing both global anatomical contexts and local ultrasound characteristics. With minimal training, EchoCare outperforms state-of-the-art comparison models across 10 representative ultrasound benchmarks of varying diagnostic difficulties, spanning disease diagnosis, lesion segmentation, organ detection, landmark prediction, quantitative regression, imaging enhancement and report generation. The code and pretrained model are publicly released, rendering EchoCare accessible for fine-tuning and local adaptation, supporting extensibility to additional applications. EchoCare provides a fully open and generalizable foundation model to boost the development of AI technologies for diverse clinical ultrasound applications.</li>
<li><strong>摘要：</strong>可以通过整合多源数据有效地学习超声表示的人工智能（AI）对推进临床护理具有重要的希望。但是，在现实世界中临床环境中大型标记的数据集的稀缺性以及特定于任务模型的有限概括性妨碍了用于超声应用程序的可推广临床AI模型的开发。在这项研究中，我们提出了Echocare，这是一种新型的通才临床使用超声基础模型，它是通过我们精心策划的，公开的，大规模数据集Echocaredata开发的。 Echocaredata包含450万张超声图像，这些图像来自5个大洲的23个国家，并通过各种不同的成像设备获取，从而涵盖了多中心，多设备和多种族的全球同伙。与采用现成的视觉基础模型体系结构的先前研究不同，我们将分层分类器引入Echocare，以使像素级和代表级特征联合学习，从而捕获全球解剖环境和局部超声特征。通过最少的培训，Echocare在10个代表性的超声基准的最先进的比较模型中，跨越了诊断困难，疾病诊断，病变分割，器官检测，地标预测，定量回归，成像增强和报告产生。该代码和预估计的模型将公开发布，可访问可访问以进行微调和本地改编，从而支持对其他应用程序的可扩展性。 Echocare提供了一个完全开放且可推广的基础模型，以增强用于多种临床超声应用的AI技术的开发。</li>
</ul>

<h3>Title: SA-UNetv2: Rethinking Spatial Attention U-Net for Retinal Vessel Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Changlu Guo, Anders Nymark Christensen, Anders Bjorholm Dahl, Yugen Yi, Morten Rieger Hannemose</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11774">https://arxiv.org/abs/2509.11774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11774">https://arxiv.org/pdf/2509.11774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11774]] SA-UNetv2: Rethinking Spatial Attention U-Net for Retinal Vessel Segmentation(https://arxiv.org/abs/2509.11774)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Retinal vessel segmentation is essential for early diagnosis of diseases such as diabetic retinopathy, hypertension, and neurodegenerative disorders. Although SA-UNet introduces spatial attention in the bottleneck, it underuses attention in skip connections and does not address the severe foreground-background imbalance. We propose SA-UNetv2, a lightweight model that injects cross-scale spatial attention into all skip connections to strengthen multi-scale feature fusion and adopts a weighted Binary Cross-Entropy (BCE) plus Matthews Correlation Coefficient (MCC) loss to improve robustness to class imbalance. On the public DRIVE and STARE datasets, SA-UNetv2 achieves state-of-the-art performance with only 1.2MB memory and 0.26M parameters (less than 50% of SA-UNet), and 1 second CPU inference on 592 x 592 x 3 images, demonstrating strong efficiency and deployability in resource-constrained, CPU-only settings.</li>
<li><strong>摘要：</strong>视网膜血管分割对于早期诊断糖尿病性视网膜病，高血压和神经退行性疾病等疾病至关重要。尽管Sa-Unet在瓶颈中引入了空间关注，但它在跳过连接中的注意力还不足，并且无法解决严重的前景不平衡。我们提出了SA-UNETV2，这是一种轻巧的模型，将跨尺度空间关注注入所有跳过连接中，以增强多尺度功能融合，并采用加权二进制跨嵌段（BCE）以及Matthews相关系数（MCC）以提高稳健性，以提高稳健性。在公共驱动器和凝视数据集上，SA-UNETV2仅具有1.2MB内存和0.26万参数（少于SA-UNET的50％），在592 x 592 x 3图像上进行1秒的CPU推断，表明资源限制，CPU-CPU-CPU-CPU-CPU的强大效率和可部署性。</li>
</ul>

<h3>Title: Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Tim Lebailly, Vijay Veerabadran, Satwik Kottur, Karl Ridgeway, Michael Louis Iuzzolino</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11840">https://arxiv.org/abs/2509.11840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11840">https://arxiv.org/pdf/2509.11840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11840]] Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation(https://arxiv.org/abs/2509.11840)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative vision-language models (VLMs) exhibit strong high-level image understanding but lack spatially dense alignment between vision and language modalities, as our findings indicate. Orthogonal to advancements in generative VLMs, another line of research has focused on representation learning for vision-language alignment, targeting zero-shot inference for dense tasks like segmentation. In this work, we bridge these two directions by densely aligning images with synthetic descriptions generated by VLMs. Synthetic captions are inexpensive, scalable, and easy to generate, making them an excellent source of high-level semantic understanding for dense alignment methods. Empirically, our approach outperforms prior work on standard zero-shot open-vocabulary segmentation benchmarks/datasets, while also being more data-efficient.</li>
<li><strong>摘要：</strong>正如我们的发现所示，生成视觉模型（VLM）表现出强烈的高级图像理解，但视觉和语言方式之间缺乏空间密集的对准。与生成VLMS的进步正交，另一项研究重点是表示视觉对齐的表示学习，以针对诸如细分之类的密集任务的零击性推断。在这项工作中，我们通过用VLMS产生的合成描述密集对准图像来桥接这两个方向。合成字幕廉价，可扩展且易于生成，使其成为密集比对方法的高级语义理解的绝佳来源。从经验上讲，我们的方法在标准零射击开放式唱片分割基准/数据集上的先前工作优于先前的工作，同时也提高了数据效率。</li>
</ul>

<h3>Title: Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting Using Weighted Prompt Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Sofia Jamil, Kotla Sai Charan, Sriparna Saha, Koustava Goswami, K J Joseph</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11878">https://arxiv.org/abs/2509.11878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11878">https://arxiv.org/pdf/2509.11878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11878]] Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting Using Weighted Prompt Manipulation(https://arxiv.org/abs/2509.11878)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Poetry is an expressive form of art that invites multiple interpretations, as readers often bring their own emotions, experiences, and cultural backgrounds into their understanding of a poem. Recognizing this, we aim to generate images for poems and improve these images in a zero-shot setting, enabling audiences to modify images as per their requirements. To achieve this, we introduce a novel Weighted Prompt Manipulation (WPM) technique, which systematically modifies attention weights and text embeddings within diffusion models. By dynamically adjusting the importance of specific words, WPM enhances or suppresses their influence in the final generated image, leading to semantically richer and more contextually accurate visualizations. Our approach exploits diffusion models and large language models (LLMs) such as GPT in conjunction with existing poetry datasets, ensuring a comprehensive and structured methodology for improved image generation in the literary domain. To the best of our knowledge, this is the first attempt at integrating weighted prompt manipulation for enhancing imagery in poetic language.</li>
<li><strong>摘要：</strong>诗歌是一种富有表现力的艺术形式，它引起了多种解释，因为读者经常将自己的情感，经验和文化背景带入对诗歌的理解中。认识到这一点，我们旨在为诗歌生成图像，并在零拍设置中改进这些图像，使观众能够根据其要求修改图像。为了实现这一目标，我们引入了一种新型的加权及时操纵（WPM）技术，该技术会系统地修改扩散模型中的注意力重量和文本嵌入。通过动态调整特定单词的重要性，WPM可以增强或抑制其在最终生成的图像中的影响，从而导致语义上更丰富，更上下文准确的可视化。我们的方法利用扩散模型和大型语言模型（LLM）（例如GPT）与现有诗歌数据集结合使用，确保了一种全面且结构化的方法，以改善文学领域的图像生成。据我们所知，这是首次尝试整合加权及时操纵以增强诗意语言图像的尝试。</li>
</ul>

<h3>Title: Graph Algorithm Unrolling with Douglas-Rachford Iterations for Image Interpolation with Guaranteed Initialization</h3>
<ul>
<li><strong>Authors: </strong>Xue Zhang, Bingshuo Hu, Gene Cheung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11926">https://arxiv.org/abs/2509.11926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11926">https://arxiv.org/pdf/2509.11926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11926]] Graph Algorithm Unrolling with Douglas-Rachford Iterations for Image Interpolation with Guaranteed Initialization(https://arxiv.org/abs/2509.11926)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Conventional deep neural nets (DNNs) initialize network parameters at random and then optimize each one via stochastic gradient descent (SGD), resulting in substantial risk of poor-performing local this http URL on the image interpolation problem and leveraging a recent theorem that maps a (pseudo-)linear interpolator {\Theta} to a directed graph filter that is a solution to a MAP problem regularized with a graph shift variation (GSV) prior, we first initialize a directed graph adjacency matrix A based on a known interpolator {\Theta}, establishing a baseline this http URL, towards further gain, we learn perturbation matrices P and P(2) from data to augment A, whose restoration effects are implemented via Douglas-Rachford (DR) iterations, which we unroll into a lightweight interpretable neural this http URL results demonstrate state-of-the-art image interpolation results, while drastically reducing network parameters.</li>
<li><strong>摘要：</strong>传统的深神经网（DNNS）随机初始化网络参数，然后通过随机梯度下降（SGD）优化每个参数，从而导致在图像插值问题上进行本地http url的稳定性不佳的很大风险 regularized with a graph shift variation (GSV) prior, we first initialize a directed graph adjacency matrix A based on a known interpolator {\Theta}, establishing a baseline this http URL, towards further gain, we learn perturbation matrices P and P(2) from data to augment A, whose restoration effects are implemented via Douglas-Rachford (DR) iterations, which we unroll into a lightweight可解释的神经此HTTP URL结果证明了最新的图像插值结果，同时大大降低了网络参数。</li>
</ul>

<h3>Title: Sphere-GAN: a GAN-based Approach for Saliency Estimation in 360° Videos</h3>
<ul>
<li><strong>Authors: </strong>Mahmoud Z. A. Wahba, Sara Baldoni, Federica Battisti</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11948">https://arxiv.org/abs/2509.11948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11948">https://arxiv.org/pdf/2509.11948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11948]] Sphere-GAN: a GAN-based Approach for Saliency Estimation in 360° Videos(https://arxiv.org/abs/2509.11948)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The recent success of immersive applications is pushing the research community to define new approaches to process 360° images and videos and optimize their transmission. Among these, saliency estimation provides a powerful tool that can be used to identify visually relevant areas and, consequently, adapt processing algorithms. Although saliency estimation has been widely investigated for 2D content, very few algorithms have been proposed for 360° saliency estimation. Towards this goal, we introduce Sphere-GAN, a saliency detection model for 360° videos that leverages a Generative Adversarial Network with spherical convolutions. Extensive experiments were conducted using a public 360° video saliency dataset, and the results demonstrate that Sphere-GAN outperforms state-of-the-art models in accurately predicting saliency maps.</li>
<li><strong>摘要：</strong>沉浸式应用程序的最新成功是促使研究社区定义了处理360°图像和视频并优化其传输的新方法。其中，显着性估计提供了一个强大的工具，可用于识别视觉相关领域，从而适应处理算法。尽管已经广泛研究了2D含量的显着性估计，但是对于360°显着性估计，很少提出算法。为了实现这一目标，我们介绍了Sphere-Gan，这是一种针对360°视频的显着性检测模型，该视频利用了具有球形卷积的生成对抗网络。使用公共360°视频显着数据集进行了广泛的实验，结果表明，在准确预测显着性图的情况下，球体距离的表现优于最先进的模型。</li>
</ul>

<h3>Title: Learning to Generate 4D LiDAR Sequences</h3>
<ul>
<li><strong>Authors: </strong>Ao Liang, Youquan Liu, Yu Yang, Dongyue Lu, Linfeng Li, Lingdong Kong, Huaici Zhao, Wei Tsang Ooi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11959">https://arxiv.org/abs/2509.11959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11959">https://arxiv.org/pdf/2509.11959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11959]] Learning to Generate 4D LiDAR Sequences(https://arxiv.org/abs/2509.11959)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>While generative world models have advanced video and occupancy-based data synthesis, LiDAR generation remains underexplored despite its importance for accurate 3D perception. Extending generation to 4D LiDAR data introduces challenges in controllability, temporal stability, and evaluation. We present LiDARCrafter, a unified framework that converts free-form language into editable LiDAR sequences. Instructions are parsed into ego-centric scene graphs, which a tri-branch diffusion model transforms into object layouts, trajectories, and shapes. A range-image diffusion model generates the initial scan, and an autoregressive module extends it into a temporally coherent sequence. The explicit layout design further supports object-level editing, such as insertion or relocation. To enable fair assessment, we provide EvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On nuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and temporal consistency, offering a foundation for LiDAR-based simulation and data augmentation.</li>
<li><strong>摘要：</strong>尽管生成世界模型具有先进的视频和基于占用的数据综合，但LiDAR的生成仍然毫无争议，尽管它对于准确的3D感知而言重要性。扩展到4D LIDAR数据引入了可控性，时间稳定性和评估方面的挑战。我们提出了Lidarcrafter，这是一个统一的框架，将自由形式的语言转换为可编辑的激光雷达序列。指令解析为以自我为中心的场景图，三支分支扩散模型将其转换为对象布局，轨迹和形状。范围图像扩散模型会生成初始扫描，并且自回归模块将其扩展到时间连贯的序列中。显式布局设计进一步支持对象级编辑，例如插入或搬迁。为了实现公平评估，我们提供了跨场景，跨越场景，对象和序列级别的指标的基准。在Nuscenes上，Lidarcrafter实现了最先进的保真度，可控性和时间一致性，为基于激光雷达的模拟和数据增强提供了基础。</li>
</ul>

<h3>Title: Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on Security and Robustness</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Fu, Yan Ren, Finn Carter, Chenyue Wen, Le Ku, Daheng Yu, Emily Davis, Bo Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12024">https://arxiv.org/abs/2509.12024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12024">https://arxiv.org/pdf/2509.12024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12024]] Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on Security and Robustness(https://arxiv.org/abs/2509.12024)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved unprecedented success in image generation but pose increasing risks in terms of privacy, fairness, and security. A growing demand exists to \emph{erase} sensitive or harmful concepts (e.g., NSFW content, private individuals, artistic styles) from these models while preserving their overall generative capabilities. We introduce \textbf{SCORE} (Secure and Concept-Oriented Robust Erasure), a novel framework for robust concept removal in diffusion models. SCORE formulates concept erasure as an \emph{adversarial independence} problem, theoretically guaranteeing that the model's outputs become statistically independent of the erased concept. Unlike prior heuristic methods, SCORE minimizes the mutual information between a target concept and generated outputs, yielding provable erasure guarantees. We provide formal proofs establishing convergence properties and derive upper bounds on residual concept leakage. Empirically, we evaluate SCORE on Stable Diffusion and FLUX across four challenging benchmarks: object erasure, NSFW removal, celebrity face suppression, and artistic style unlearning. SCORE consistently outperforms state-of-the-art methods including EraseAnything, ANT, MACE, ESD, and UCE, achieving up to \textbf{12.5\%} higher erasure efficacy while maintaining comparable or superior image quality. By integrating adversarial optimization, trajectory consistency, and saliency-driven fine-tuning, SCORE sets a new standard for secure and robust concept erasure in diffusion models.</li>
<li><strong>摘要：</strong>扩散模型在图像产生中取得了前所未有的成功，但在隐私，公平和安全方面构成了增加的风险。 \ emph {erase}敏感或有害的概念（例如，NSFW内容，私人个人，艺术风格）的需求不断增长，同时保留其整体生成能力。我们介绍了\ textbf {score}（安全和概念上的稳健擦除），这是一个在扩散模型中进行稳健概念删除的新颖框架。分数将概念擦除作为\ emph {对抗独立}问题，从理论上保证模型的输出在统计上独立于删除的概念。与先前的启发式方法不同，得分可以最大程度地减少目标概念和生成的产出之间的相互信息，从而产生可证明的擦除保证。我们提供正式的证据，以建立收敛属性并在残留概念泄漏方面得出上限。从经验上讲，我们评估了四个具有挑战性的基准的稳定扩散和通量的得分：对象擦除，NSFW删除，名人面部抑制和艺术风格，而没有学习。得分始终优于最先进的方法，包括Eraseanything，Ant，Mace，ESD和UCE，最多可实现\ TextBf {12.5 \％}，同时保持可比较或出色的图像质量。通过整合对抗性优化，轨迹一致性和显着驱动的微调，得分为扩散模型中的安全且可靠的概念擦除设定了新的标准。</li>
</ul>

<h3>Title: RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Zilong Zhang, Chujie Qin, Chunle Guo, Yong Zhang, Chao Xue, Ming-Ming Cheng, Chongyi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12039">https://arxiv.org/abs/2509.12039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12039">https://arxiv.org/pdf/2509.12039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12039]] RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration(https://arxiv.org/abs/2509.12039)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation, generative</a></li>
<li><strong>Abstract: </strong>This work presents Robust Representation Learning via Adaptive Mask (RAM++), a two-stage framework for all-in-one image restoration. RAM++ integrates high-level semantic understanding with low-level texture generation to achieve content-oriented robust restoration. It addresses the limitations of existing degradation-oriented methods in extreme scenarios (e.g., degradations strongly coupled with image structures). RAM++ also mitigates common challenges such as unbalanced performance across tasks, overfitting to seen degradations, and weak generalization to unseen ones through three key designs: 1) Adaptive Semantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-level masks to semantically rich and textured regions. This design enables the network to learn both generative priors and image content priors from various degradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuning strategy that adjusts the layers with higher contributions to bridge the integrity gap between masked pretraining and full-image fine-tuning while retaining learned priors. 3) Robust Feature Regularization (RFR): a strategy that leverages DINOv2's semantically consistent and degradation-invariant representations, together with efficient feature fusion, to achieve faithful and semantically coherent restoration. With these designs, RAM++ achieves robust, well-balanced, and state-of-the-art performance across seen, unseen, extreme, and mixed degradations. Our code and model will be released at this https URL</li>
<li><strong>摘要：</strong>这项工作通过自适应面具（RAM ++）提供了强大的表示学习，这是一个多合一图像恢复的两阶段框架。 RAM ++将高级语义理解与低级纹理产生相结合，以实现面向内容的稳健恢复。它解决了在极端情况下现有面向降解方法的局限性（例如，降解与图像结构强烈结合）。 RAM ++还缓解了常见的挑战，例如跨任务不平衡的性能，过度适应可见的降解，并且通过三个关键设计而无法看到较弱的概括：1）适用于语义上的语义级别的策略，将语义级别的口罩应用于语义上富含语义和纹理的区域。该设计使网络能够从各种降解中学习生成的先验和图像内容先验。 2）掩盖属性电导（MAC）：一种选择性的微调策略，该策略在保留学习的先验者的同时，在掩盖掩盖预训练和完整的微型调查的同时，以更高的贡献来调整层较高的层面。 3）鲁棒特征正则化（RFR）：一种利用Dinov2的语义一致和降解不变表示的策略，以及有效的特征融合，以实现忠实而语义上的连贯的恢复。通过这些设计，RAM ++在看到，看不见，极端和混合降解的情况下实现了强大，平衡和最先进的性能。我们的代码和模型将在此HTTPS URL上发布</li>
</ul>

<h3>Title: Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking</h3>
<ul>
<li><strong>Authors: </strong>Zirui Zheng, Takashi Isobe, Tong Shen, Xu Jia, Jianbin Zhao, Xiaomin Li, Mengmeng Ge, Baolu Li, Qinghe Wang, Dong Li, Dong Zhou, Yunzhi Zhuge, Huchuan Lu, Emad Barsoum</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12046">https://arxiv.org/abs/2509.12046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12046">https://arxiv.org/pdf/2509.12046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12046]] Layout-Conditioned Autoregressive Text-to-Image Generation via Structured Masking(https://arxiv.org/abs/2509.12046)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While autoregressive (AR) models have demonstrated remarkable success in image generation, extending them to layout-conditioned generation remains challenging due to the sparse nature of layout conditions and the risk of feature entanglement. We present Structured Masking for AR-based Layout-to-Image (SMARLI), a novel framework for layoutto-image generation that effectively integrates spatial layout constraints into AR-based image generation. To equip AR model with layout control, a specially designed structured masking strategy is applied to attention computation to govern the interaction among the global prompt, layout, and image tokens. This design prevents mis-association between different regions and their descriptions while enabling sufficient injection of layout constraints into the generation process. To further enhance generation quality and layout accuracy, we incorporate Group Relative Policy Optimization (GRPO) based post-training scheme with specially designed layout reward functions for next-set-based AR models. Experimental results demonstrate that SMARLI is able to seamlessly integrate layout tokens with text and image tokens without compromising generation quality. It achieves superior layoutaware control while maintaining the structural simplicity and generation efficiency of AR models.</li>
<li><strong>摘要：</strong>尽管自回归（AR）模型在图像生成中表现出了很大的成功，但由于布局条件的稀疏性质和特征纠缠的风险，将它们扩展到布局条件的生成仍然具有挑战性。我们为基于AR的布局到图像（SMARLI）提供了结构化掩蔽，这是一个新型的布局到图像生成的框架，可有效地将空间布局约束集成到基于AR的图像生成中。为了为AR模型配备布局控制，将特殊设计的结构化掩蔽策略应用于注意力计算，以控制全局提示，布局和图像令牌之间的相互作用。这种设计阻止了不同区域及其描述之间的错误关联，同时可以将布局约束充分注入生成过程。为了进一步提高发电质量和布局的精度，我们将基于群体的培训后方案（GRPO）结合起来，并为隔壁的AR模型提供了专门设计的布局奖励功能。实验结果表明，Smarli能够无缝地将布局令牌与文本和图像令牌整合在一起，而不会损害生成质量。它在保持AR模型的结构简单性和产生效率的同时，可以实现出色的Layoutaware控制。</li>
</ul>

<h3>Title: AvatarSync: Rethinking Talking-Head Animation through Autoregressive Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Deng, Xiuyang Wu, Hai-Tao Zheng, Suiyang Zhang, Yi He, Yuxing Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12052">https://arxiv.org/abs/2509.12052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12052">https://arxiv.org/pdf/2509.12052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12052]] AvatarSync: Rethinking Talking-Head Animation through Autoregressive Perspective(https://arxiv.org/abs/2509.12052)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Existing talking-head animation approaches based on Generative Adversarial Networks (GANs) or diffusion models often suffer from inter-frame flicker, identity drift, and slow inference. These limitations inherent to their video generation pipelines restrict their suitability for applications. To address this, we introduce AvatarSync, an autoregressive framework on phoneme representations that generates realistic and controllable talking-head animations from a single reference image, driven directly text or audio input. In addition, AvatarSync adopts a two-stage generation strategy, decoupling semantic modeling from visual dynamics, which is a deliberate "Divide and Conquer" design. The first stage, Facial Keyframe Generation (FKG), focuses on phoneme-level semantic representation by leveraging the many-to-one mapping from text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to anchor abstract phonemes to character-level units. Combined with a customized Text-Frame Causal Attention Mask, the keyframes are generated. The second stage, inter-frame interpolation, emphasizes temporal coherence and visual smoothness. We introduce a timestamp-aware adaptive strategy based on a selective state space model, enabling efficient bidirectional context reasoning. To support deployment, we optimize the inference pipeline to reduce latency without compromising visual fidelity. Extensive experiments show that AvatarSync outperforms existing talking-head animation methods in visual fidelity, temporal consistency, and computational efficiency, providing a scalable and controllable solution.</li>
<li><strong>摘要：</strong>现有的谈话头动画方法基于生成对抗网络（GAN）或扩散模型通常会遭受框架间的闪烁，身份漂移和缓慢的推理。其视频生成管道固有的这些限制限制了它们对应用的适用性。为了解决这个问题，我们介绍了Avatarsync，这是一种对音素表示的自回归框架，该框架从单个参考图像，驱动的直接文本或音频输入中生成真实且可控制的说话头动画。此外，Avatarsync采用了两阶段的一代策略，将语义建模与视觉动力学解耦，这是故意的“分裂和征服”设计。第一阶段是面部钥匙帧生成（FKG），通过利用从文本或音频到音素的多对一映射到音素来重点介绍音素级别的语义表示。构建音素到视觉映射是为了将抽象音素锚定在字符级别的单位上。结合定制的文本框架因果注意面具，生成了关键框架。第二阶段的框架间插值强调了时间连贯性和视觉平滑度。我们引入了基于选择性状态空间模型的时间戳感知的自适应策略，从而实现了有效的双向上下文推理。为了支持部署，我们优化推理管道以减少延迟而不损害视觉保真度。广泛的实验表明，Avatarsync在视觉保真度，时间一致性和计算效率方面的表现优于现有的说话头动画方法，提供了可扩展可控制的解决方案。</li>
</ul>

<h3>Title: Open-ended Hierarchical Streaming Video Understanding with Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hyolim Kang, Yunsu Park, Youngbeom Yoo, Yeeun Choi, Seon Joo Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12145">https://arxiv.org/abs/2509.12145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12145">https://arxiv.org/pdf/2509.12145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12145]] Open-ended Hierarchical Streaming Video Understanding with Vision Language Models(https://arxiv.org/abs/2509.12145)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce Hierarchical Streaming Video Understanding, a task that combines online temporal action localization with free-form description generation. Given the scarcity of datasets with hierarchical and fine-grained temporal annotations, we demonstrate that LLMs can effectively group atomic actions into higher-level events, enriching existing datasets. We then propose OpenHOUSE (Open-ended Hierarchical Online Understanding System for Events), which extends streaming action perception beyond action classification. OpenHOUSE features a specialized streaming module that accurately detects boundaries between closely adjacent actions, nearly doubling the performance of direct extensions of existing methods. We envision the future of streaming action perception in the integration of powerful generative models, with OpenHOUSE representing a key step in that direction.</li>
<li><strong>摘要：</strong>我们介绍了分层流视频理解，该任务将在线时间动作本地化与自由形式描述生成结合在一起。鉴于数据集缺乏层次结构和细粒的时间注释，我们证明LLM可以有效地将原子动作分组为高级事件，从而丰富现有的数据集。然后，我们建议开放式房屋（事件的开放式层次结构在线理解系统），该系统将流动动作感知扩展到行动分类之外。 OpenHouse具有一个专门的流媒体模块，该模块可以准确检测紧密相邻动作之间的边界，几乎使现有方法的直接扩展的性能翻了一番。我们设想了在强大的生成模型集成中流式动作感知的未来，开放式房屋代表了朝着该方向朝着关键的一步。</li>
</ul>

<h3>Title: Multi Anatomy X-Ray Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Nishank Singla, Krisztian Koos, Farzin Haddadpour, Amin Honarmandi Shandiz, Lovish Chum, Xiaojian Xu, Qing Jin, Erhan Bas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12146">https://arxiv.org/abs/2509.12146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12146">https://arxiv.org/pdf/2509.12146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12146]] Multi Anatomy X-Ray Foundation Model(https://arxiv.org/abs/2509.12146)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation models are limited to chest anatomy and fail to generalize across broader clinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray foundation model using self-supervised learning on a large, private dataset of 1.15 million images spanning diverse anatomical regions and evaluated across 12 datasets and 20 downstream tasks, including classification, retrieval, segmentation, localization, visual grounding, and report generation. XR-0 achieves state-of-the-art performance on most multi-anatomy tasks and remains competitive on chest-specific benchmarks. Our results demonstrate that anatomical diversity and supervision are critical for building robust, general-purpose medical vision models, paving the way for scalable and adaptable AI systems in radiology.</li>
<li><strong>摘要：</strong>X射线成像在放射学上是无处不在的，但是大多数现有的AI基础模型仅限于胸部解剖结构，并且无法跨越更广泛的临床任务。在这项工作中，我们介绍了XR-0，这是多种解剖学X射线基础模型，使用自我监督的学习在一个大型的私人数据集中，该数据集的115万张图像涵盖了各种解剖区域，并在12个数据集和20个下游任务中进行了评估，包括分类，检索，检索，分割，分割，本地化，本地化，视觉地面和报告生成。 XR-0可以在大多数多门术任务上实现最先进的表现，并且在特定于胸部的基准上保持竞争力。我们的结果表明，解剖学多样性和监督对于构建强大的通用医学视觉模型至关重要，为放射学中的可扩展和适应性AI系统铺平了道路。</li>
</ul>

<h3>Title: All that structure matches does not glitter</h3>
<ul>
<li><strong>Authors: </strong>Maya M. Martirossyan, Thomas Egg, Philipp Hoellmer, George Karypis, Mark Transtrum, Adrian Roitberg, Mingjie Liu, Richard G. Hennig, Ellad B. Tadmor, Stefano Martiniani</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12178">https://arxiv.org/abs/2509.12178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12178">https://arxiv.org/pdf/2509.12178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12178]] All that structure matches does not glitter(https://arxiv.org/abs/2509.12178)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models for materials, especially inorganic crystals, hold potential to transform the theoretical prediction of novel compounds and structures. Advancement in this field depends critically on robust benchmarks and minimal, information-rich datasets that enable meaningful model evaluation. This paper critically examines common datasets and reported metrics for a crystal structure prediction task$\unicode{x2014}$generating the most likely structures given the chemical composition of a material. We focus on three key issues: First, materials datasets should contain unique crystal structures; for example, we show that the widely-utilized carbon-24 dataset only contains $\approx$40% unique structures. Second, materials datasets should not be split randomly if polymorphs of many different compositions are numerous, which we find to be the case for the perov-5 dataset. Third, benchmarks can mislead if used uncritically, e.g., reporting a match rate metric without considering the structural variety exhibited by identical building blocks. To address these oft-overlooked issues, we introduce several fixes. We provide revised versions of the carbon-24 dataset: one with duplicates removed, one deduplicated and split by number of atoms $N$, and two containing only identical structures but with different unit cells. We also propose a new split for the perov-5 dataset which ensures polymorphs are grouped within each split subset, setting a more sensible standard for benchmarking model performance. Finally, we present METRe and cRMSE, new model evaluation metrics that can correct existing issues with the match rate metric.</li>
<li><strong>摘要：</strong>材料的生成模型，尤其是无机晶体，具有改变新型化合物和结构的理论预测的潜力。该领域的进步取决于强大的基准测试和最小，信息丰富的数据集，从而实现有意义的模型评估。本文对晶体结构预测任务$ \ unicode {x2014} $生成最有可能的结构的晶体结构预测任务的通用数据集和报告的指标。我们关注三个关键问题：首先，材料数据集应包含独特的晶体结构；例如，我们表明，广泛利用的碳-24数据集仅包含$ \ $ 40％的独特结构。其次，如果许多不同组合物的多晶型物众多，我们发现perov-5数据集是这种情况，则不应随机分配材料数据集。第三，如果不严格地使用基准，例如报告匹配率指标，而无需考虑相同的构建块所表现出的结构品种。为了解决这些经常被忽视的问题，我们介绍了几个修复程序。我们提供了碳-24数据集的修订版：一个删除了重复项，一个是根据原子$ n $的数量进行了重复的并划分的，而两个仅包含相同的结构，但具有不同的单位单元。我们还为PEROV-5数据集提出了一个新的拆分，以确保将多晶型物分组为每个拆分子集中，从而为基准测试模型性能设定了更明智的标准。最后，我们提出了仪表和CRMSE，即新的模型评估指标，可以纠正匹配率指标的现有问题。</li>
</ul>

<h3>Title: OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhou, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Haoyu Guo, Zizun Li, Kaijing Ma, Xinyue Li, Yating Wang, Haoyi Zhu, Mingyu Liu, Dingning Liu, Jiange Yang, Zhoujie Fu, Junyi Chen, Chunhua Shen, Jiangmiao Pang, Kaipeng Zhang, Tong He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12201">https://arxiv.org/abs/2509.12201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12201">https://arxiv.org/pdf/2509.12201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12201]] OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling(https://arxiv.org/abs/2509.12201)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The field of 4D world modeling - aiming to jointly capture spatial geometry and temporal dynamics - has witnessed remarkable progress in recent years, driven by advances in large-scale generative models and multimodal learning. However, the development of truly general 4D world models remains fundamentally constrained by the availability of high-quality data. Existing datasets and benchmarks often lack the dynamic complexity, multi-domain diversity, and spatial-temporal annotations required to support key tasks such as 4D geometric reconstruction, future prediction, and camera-control video generation. To address this gap, we introduce OmniWorld, a large-scale, multi-domain, multi-modal dataset specifically designed for 4D world modeling. OmniWorld consists of a newly collected OmniWorld-Game dataset and several curated public datasets spanning diverse domains. Compared with existing synthetic datasets, OmniWorld-Game provides richer modality coverage, larger scale, and more realistic dynamic interactions. Based on this dataset, we establish a challenging benchmark that exposes the limitations of current state-of-the-art (SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning existing SOTA methods on OmniWorld leads to significant performance gains across 4D reconstruction and video generation tasks, strongly validating OmniWorld as a powerful resource for training and evaluation. We envision OmniWorld as a catalyst for accelerating the development of general-purpose 4D world models, ultimately advancing machines' holistic understanding of the physical world.</li>
<li><strong>摘要：</strong>4D世界建模的领域 - 旨在共同捕获空间几何形状和时间动态 - 近年来，在大规模生成模型和多模式学习的进步驱动下，近年来取得了显着的进步。但是，真正一般的4D世界模型的发展基本上仍然受高质量数据的可用性的限制。现有的数据集和基准通常缺乏支持关键任务（例如4D几何重建，未来预测和摄像机对照视频的生成）所需的动态复杂性，多域多样性和时空注释。为了解决这一差距，我们介绍了Omniworld，这是一种专门为4D世界建模设计的大型，多域的多模式数据集。 OmniWorld由新收集的OmniWorld游戏数据集和几个跨越不同域的策划的公共数据集组成。与现有的合成数据集相比，OmniWorld-Game提供了更丰富的方式覆盖范围，更大的规模和更现实的动态交互。基于此数据集，我们建立了一个具有挑战性的基准测试，该基准揭示了对复杂4D环境建模时最新方法（SOTA）方法的局限性。此外，对OmniWorld的现有SOTA方法进行了微调，从而在4D重建和视频生成任务中取得了显着的性能提高，从而强烈验证了OmniWorld，作为培训和评估的有力资源。我们设想Omniworld是加速通用4D世界模型的发展的催化剂，最终促进了机器对物理世界的整体理解。</li>
</ul>

<h3>Title: LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence</h3>
<ul>
<li><strong>Authors: </strong>Zixin Yin, Xili Dai, Duomin Wang, Xianfang Zeng, Lionel M. Ni, Gang Yu, Heung-Yeung Shum</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12203">https://arxiv.org/abs/2509.12203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12203">https://arxiv.org/pdf/2509.12203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12203]] LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence(https://arxiv.org/abs/2509.12203)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The reliance on implicit point matching via attention has become a core bottleneck in drag-based editing, resulting in a fundamental compromise on weakened inversion strength and costly test-time optimization (TTO). This compromise severely limits the generative capabilities of diffusion models, suppressing high-fidelity inpainting and text-guided creation. In this paper, we introduce LazyDrag, the first drag-based image editing method for Multi-Modal Diffusion Transformers, which directly eliminates the reliance on implicit point matching. In concrete terms, our method generates an explicit correspondence map from user drag inputs as a reliable reference to boost the attention control. This reliable reference opens the potential for a stable full-strength inversion process, which is the first in the drag-based editing task. It obviates the necessity for TTO and unlocks the generative capability of models. Therefore, LazyDrag naturally unifies precise geometric control with text guidance, enabling complex edits that were previously out of reach: opening the mouth of a dog and inpainting its interior, generating new objects like a ``tennis ball'', or for ambiguous drags, making context-aware changes like moving a hand into a pocket. Additionally, LazyDrag supports multi-round workflows with simultaneous move and scale operations. Evaluated on the DragBench, our method outperforms baselines in drag accuracy and perceptual quality, as validated by VIEScore and human evaluation. LazyDrag not only establishes new state-of-the-art performance, but also paves a new way to editing paradigms.</li>
<li><strong>摘要：</strong>通过注意力匹配隐式点的依赖已成为基于阻力的编辑中的核心瓶颈，从而对反转强度和昂贵的测试时间优化（TTO）产生了基本妥协。这种损害严重限制了扩散模型的生成能力，从而抑制了高保真介绍和文本引导的创造。在本文中，我们介绍了LazyDrag，这是用于多模式扩散变压器的第一个基于阻力的图像编辑方法，该方法直接消除了对隐式点匹配的依赖。用具体的术语，我们的方法从用户拖动输入中生成一个明确的对应图，作为可靠的引用以提高注意力控制。此可靠的参考为稳定的全强度反演过程打开了潜力，这是基于阻力的编辑任务中的第一个。它消除了TTO的必要性并解锁了模型的生成能力。因此，Lazydrag自然会通过文本指导统一精确的几何控制，从而实现了以前无法触及的复杂编辑：张开狗的口并介绍其内部装饰，产生了“网球球”（例如``网球球'''的新物体，或者为“含糊不清”的拖曳，使上下文感知到了上下文的变化，例如将手移入口袋中。此外，Lazydrag支持同时移动和规模操作的多轮工作流程。在Dragbench上进行了评估，我们的方法以阻力准确性和感知质量优于基准，这是通过Viescore和人类评估验证的。 Lazydrag不仅建立了新的最先进的性能，而且还为编辑范式铺平了一种新的方式。</li>
</ul>

<h3>Title: Character-Centric Understanding of Animated Movies</h3>
<ul>
<li><strong>Authors: </strong>Zhongrui Gui, Junyu Xie, Tengda Han, Weidi Xie, Andrew Zisserman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12204">https://arxiv.org/abs/2509.12204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12204">https://arxiv.org/pdf/2509.12204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12204]] Character-Centric Understanding of Animated Movies(https://arxiv.org/abs/2509.12204)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Animated movies are captivating for their unique character designs and imaginative storytelling, yet they pose significant challenges for existing recognition systems. Unlike the consistent visual patterns detected by conventional face recognition methods, animated characters exhibit extreme diversity in their appearance, motion, and deformation. In this work, we propose an audio-visual pipeline to enable automatic and robust animated character recognition, and thereby enhance character-centric understanding of animated movies. Central to our approach is the automatic construction of an audio-visual character bank from online sources. This bank contains both visual exemplars and voice (audio) samples for each character, enabling subsequent multi-modal character recognition despite long-tailed appearance distributions. Building on accurate character recognition, we explore two downstream applications: Audio Description (AD) generation for visually impaired audiences, and character-aware subtitling for the hearing impaired. To support research in this domain, we introduce CMD-AM, a new dataset of 75 animated movies with comprehensive annotations. Our character-centric pipeline demonstrates significant improvements in both accessibility and narrative comprehension for animated content over prior face-detection-based approaches. For the code and dataset, visit this https URL.</li>
<li><strong>摘要：</strong>动画电影因其独特的角色设计和富有想象力的讲故事而着迷，但它们对现有识别系统构成了重大挑战。与常规面部识别方法检测到的一致的视觉模式不同，动画角色的外观，运动和变形表现出极端的多样性。在这项工作中，我们提出了一条视听管道，以实现自动和健壮的动画角色识别，从而增强以角色为中心的动画电影的理解。我们方法的核心是从在线来源自动构造视听角色库。该银行包含每个字符的视觉示例和语音样本（音频）样本，尽管长期尾巴的外观分布，但可以随后的多模式字符识别。在准确的角色识别的基础上，我们探索了两个下游应用程序：视觉障碍受众的音频描述（AD）生成，以及对听力受损的角色引人注目的副标题。为了支持该领域的研究，我们介绍了CMD-AM，这是一个新的75部动画电影的新数据集，其中包含全面注释。我们以角色为中心的管道表明，在基于面对面检测的方法上，动画内容的可访问性和叙事理解能力都显着改善。对于代码和数据集，请访问此HTTPS URL。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
