<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-07</h1>
<h3>Title: Adversarial training with restricted data manipulation</h3>
<ul>
<li><strong>Authors: </strong>David Benfield, Stefano Coniglio, Phan Tu Vuong, Alain Zemkoho</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03254">https://arxiv.org/abs/2510.03254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03254">https://arxiv.org/pdf/2510.03254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03254]] Adversarial training with restricted data manipulation(https://arxiv.org/abs/2510.03254)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Adversarial machine learning concerns situations in which learners face attacks from active adversaries. Such scenarios arise in applications such as spam email filtering, malware detection and fake image generation, where security methods must be actively updated to keep up with the everimproving generation of malicious data. Pessimistic Bilevel optimisation has been shown to be an effective method of training resilient classifiers against such adversaries. By modelling these scenarios as a game between the learner and the adversary, we anticipate how the adversary will modify their data and then train a resilient classifier accordingly. However, since existing pessimistic bilevel approaches feature an unrestricted adversary, the model is vulnerable to becoming overly pessimistic and unrealistic. When finding the optimal solution that defeats the classifier, it is possible that the adversary's data becomes nonsensical and loses its intended nature. Such an adversary will not properly reflect reality, and consequently, will lead to poor classifier performance when implemented on real-world data. By constructing a constrained pessimistic bilevel optimisation model, we restrict the adversary's movements and identify a solution that better reflects reality. We demonstrate through experiments that this model performs, on average, better than the existing approach.</li>
<li><strong>摘要：</strong>对手的机器学习涉及学习者面临活跃对手的攻击的情况。这些方案出现在诸如垃圾邮件过滤，恶意软件检测和虚假图像生成之类的应用中，必须积极更新安全方法，以跟上恶意数据的Everimproved Generation。悲观的二线优化已被证明是针对此类对手训练弹性分类器的有效方法。通过将这些方案建模为学习者和对手之间的游戏，我们预计对手将如何修改其数据，然后相应地训练弹性分类器。但是，由于现有的悲观双层方法具有不受限制的对手，因此该模型容易变得过于悲观和不现实。当找到打败分类器的最佳解决方案时，对手的数据可能会变得荒谬并失去其预期的性质。这样的对手将无法正确反映现实，因此，在实际数据上实施时，分类器的性能差。通过构建一个受约束的悲观双层优化模型，我们限制了对手的运动，并确定了更好地反映现实的解决方案。我们通过实验证明，该模型平均表现要比现有方法更好。</li>
</ul>

<h3>Title: SciTS: Scientific Time Series Understanding and Generation with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Wen Wu, Ziyang Zhang, Liwei Liu, Xuenan Xu, Junlin Liu, Ke Fan, Qitan Lv, Jimin Zhuang, Chen Zhang, Zheqi Yuan, Siyuan Hou, Tianyi Lin, Kai Chen, Bowen Zhou, Chao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03255">https://arxiv.org/abs/2510.03255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03255">https://arxiv.org/pdf/2510.03255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03255]] SciTS: Scientific Time Series Understanding and Generation with LLMs(https://arxiv.org/abs/2510.03255)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The scientific reasoning ability of large language models (LLMs) has recently attracted significant attention. Time series, as a fundamental modality in scientific data, presents unique challenges that are often overlooked in current multimodal LLMs, which either encode numerical sequences as text or convert them into images. Such approaches may be insufficient for comprehensive scientific time series understanding and generation. Existing unified time series models typically specialise in either forecasting or analysis, and their effectiveness on non-periodic, heterogeneous scientific signals remains unclear. To address these gaps, we introduce SciTS, a benchmark spanning 12 scientific domains and 43 tasks, with over 50k+ instances, both univariate and multivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz in frequency. We benchmark 17 models, including text-only LLMs, multimodal LLMs, and unified time series models, and find that general-purpose LLMs exhibit stronger generalisability than specialised time series models, while representing time series as text or images limits their performance due to excessively long sequences and loss of numerical precision, respectively. We then introduce TimeOmni, a framework that equips LLMs with the ability to understand and generate time series while remaining compatible with general-purpose LLM training. This work fills a gap in both dedicated benchmarks and modelling frameworks for scientific time series, paving the way for LLMs to understand and generate complex temporal scientific data.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）的科学推理能力最近引起了极大的关注。时间序列是科学数据中的基本方式，提出了独特的挑战，这些挑战通常在当前的多模式LLM中被忽略，这些挑战要么将数值序列编码为文本，要么将它们转换为图像。这种方法可能不足以用于全面的科学时间序列的理解和产生。现有的统一时间序列模型通常专门研究预测或分析，并且它们对非周期性，异质科学信号的有效性尚不清楚。为了解决这些差距，我们介绍了SCITS，一个基准，跨越了12个科学域和43个任务，其中超过50k+实例，即单变量和多变量信号，范围从$ 10^0 $到$ 10^7 $长至10^7 $，频率最高为10〜MHz。我们基准了17个模型，包括仅文本LLM，多模式LLM和统一的时间序列模型，发现通用用途LLMS比专业时间序列模型表现出更强的通用性，而将时间序列表示为文本或图像，由于其性能过多，因此分别限制了其性能，并且分别是数字精确的损失。然后，我们引入Timeomni，该框架使LLMS具有理解和生成时间序列的能力，同时与通用LLM培训保持兼容。这项工作填补了科学时间序列的专用基准和建模框架的空白，为LLMS铺平了理解和生成复杂的时间科学数据的道路。</li>
</ul>

<h3>Title: Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models</h3>
<ul>
<li><strong>Authors: </strong>Agnieszka Polowczyk, Alicja Polowczyk, Joanna Waczyńska, Piotr Borycki, Przemysław Spurek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03263">https://arxiv.org/abs/2510.03263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03263">https://arxiv.org/pdf/2510.03263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03263]] Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models(https://arxiv.org/abs/2510.03263)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The impressive capability of modern text-to-image models to generate realistic visuals has come with a serious drawback: they can be misused to create harmful, deceptive or unlawful content. This has accelerated the push for machine unlearning. This new field seeks to selectively remove specific knowledge from a model's training data without causing a drop in its overall performance. However, it turns out that actually forgetting a given concept is an extremely difficult task. Models exposed to attacks using adversarial prompts show the ability to generate so-called unlearned concepts, which can be not only harmful but also illegal. In this paper, we present considerations regarding the ability of models to forget and recall knowledge, introducing the Memory Self-Regeneration task. Furthermore, we present MemoRa strategy, which we consider to be a regenerative approach supporting the effective recovery of previously lost knowledge. Moreover, we propose that robustness in knowledge retrieval is a crucial yet underexplored evaluation measure for developing more robust and effective unlearning techniques. Finally, we demonstrate that forgetting occurs in two distinct ways: short-term, where concepts can be quickly recalled, and long-term, where recovery is more challenging.</li>
<li><strong>摘要：</strong>现代文本到图像模型产生现实视觉效果的令人印象深刻的能力是一个严重的缺点：它们可能会被滥用以创造有害，欺骗性或非法内容。这加速了机器学习的推动力。这个新领域试图从模型的培训数据中选择性地删除特定知识，而不会导致其整体性能下降。但是，事实证明，实际忘记给定概念是一项极其艰巨的任务。使用对抗性提示暴露于攻击的模型表明能够产生所谓的未经学习的概念，这不仅有害，而且是非法的。在本文中，我们提出了有关模型忘记和回忆知识并引入内存自我恢复任务的能力的考虑。此外，我们提出了备忘录策略，我们认为这是一种再生方法，支持了先前丢失的知识的有效恢复。此外，我们建议知识检索中的鲁棒性是一种至关重要但毫无疑问的评估措施，用于开发更健壮和有效的学习技术。最后，我们证明忘记以两种不同的方式发生：短期，可以快速召回概念，并且长期恢复更具挑战性。</li>
</ul>

<h3>Title: Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tianao Zhang, Zhiteng Li, Xianglong Yan, Haotong Qin, Yong Guo, Yulun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03274">https://arxiv.org/abs/2510.03274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03274">https://arxiv.org/pdf/2510.03274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03274]] Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models(https://arxiv.org/abs/2510.03274)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion large language models (dLLMs), which offer bidirectional context and flexible masked-denoising generation, are emerging as a compelling alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model sizes continue to grow, motivating weight compression for deployment. Although post-training quantization (PTQ) is effective for AR LLMs, directly transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the fully visible signals assumed by standard PTQ methods, we introduce Masked Calibration Simulation (MCS) to align calibration with the timestep-dependent masking, which yields more reliable calibrations. Moreover, we propose a Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight representations via an optimization algorithm. It performs iterative approximation guided by our simulated calibration data. In addition, under a strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a sensitivity-based precision allocation scheme that adaptively assigns bit width across channel groups. When restricted to 2-bit precision, Quant-dLLM consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer PTQ methods on dLLMs. The code and models will be available at: this https URL.</li>
<li><strong>摘要：</strong>扩散的大语言模型（DLLM）提供了双向上下文和灵活的掩盖式变化，它已成为自回归（AR）LLMS的引人注目的替代方案。但是，像AR LLM一样，它们的模型大小继续增长，激发了部署的重量压缩。尽管训练后量化（PTQ）对AR LLM有效，但直接以2位转移到DLLM会导致性能不令人满意。为了应对这些挑战，我们提出了Quant-DLLM，这是一种针对DLLMS量身定制的超低位PTQ框架。由于DLLMS中的蒙版降解激活与标准PTQ方法假设的完全可见信号不同，因此我们引入掩盖校准模拟（MCS），以使校准与时间段依赖性掩盖相结合，从而产生更可靠的校准。此外，我们提出了一个数据感知的任何阶量化器（DAQ），该量化器（DAQ）通过优化算法学习超低重量表示。它执行以我们的模拟校准数据为指导的迭代近似。此外，在严格的2位预算下，我们引入了自适应块混合精度（ABMP），这是一种基于灵敏度的精度分配方案，可自适应地分配跨通道组的位宽度。当限制在2位精度时，Quant-DLLM始终达到的精度比DLLMS上的最新AR-TRANSFER PTQ方法更高。代码和模型将可用：此HTTPS URL。</li>
</ul>

<h3>Title: Why mask diffusion does not work</h3>
<ul>
<li><strong>Authors: </strong>Haocheng Sun, Cynthia Xin Wen, Edward Hong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03289">https://arxiv.org/abs/2510.03289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03289">https://arxiv.org/pdf/2510.03289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03289]] Why mask diffusion does not work(https://arxiv.org/abs/2510.03289)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The main advantages of diffusion language models over autoregressive (AR) models lie in their ability to support parallel generation and bidirectional attention, enabling a more controllable generation process. In recent years, open-source mask diffusion language models have emerged, most of which are based on a variant known as absorbing diffusion. However, this paper demonstrates why mask diffusion faces inherent difficulties in achieving parallel generation and bidirectional attention. We also propose the most effective training and inference strategies for mask diffusion.</li>
<li><strong>摘要：</strong>扩散语言模型的主要优点比自回旋（AR）模型支持其支持并行生成和双向关注的能力，从而实现了更可控制的生成过程。近年来，已经出现了开源面膜扩散语言模型，其中大多数基于一种称为吸收扩散的变体。但是，本文说明了为什么掩盖扩散在实现并行产生和双向关注方面面临固有的困难。我们还提出了掩盖扩散的最有效的培训和推理策略。</li>
</ul>

<h3>Title: Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data</h3>
<ul>
<li><strong>Authors: </strong>Doğanay Demir, İlknur Durgar Elkahlout</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03292">https://arxiv.org/abs/2510.03292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03292">https://arxiv.org/pdf/2510.03292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03292]] Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data(https://arxiv.org/abs/2510.03292)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In an era dominated by video content, understanding its structure and dynamics has become increasingly important. This paper presents a hybrid framework that combines a distributed multi-GPU inference system with an interactive visualization platform for analyzing celebrity dynamics in video episodes. The inference framework efficiently processes large volumes of video data by leveraging optimized ONNX models, heterogeneous batch inference, and high-throughput parallelism, ensuring scalable generation of timestamped appearance records. These records are then transformed into a comprehensive suite of visualizations, including appearance frequency charts, duration analyses, pie charts, co-appearance matrices, network graphs, stacked area charts, seasonal comparisons, and heatmaps. Together, these visualizations provide multi-dimensional insights into video content, revealing patterns in celebrity prominence, screen-time distribution, temporal dynamics, co-appearance relationships, and intensity across episodes and seasons. The interactive nature of the system allows users to dynamically explore data, identify key moments, and uncover evolving relationships between individuals. By bridging distributed recognition with structured, visually-driven analytics, this work enables new possibilities for entertainment analytics, content creation strategies, and audience engagement studies.</li>
<li><strong>摘要：</strong>在视频内容主导的时代，了解其结构和动态变得越来越重要。本文提出了一种混合框架，将分布式多 GPU 推理系统与交互式可视化平台相结合，用于分析视频剧集中的名人动态。推理框架利用优化的 ONNX 模型、异构批量推理和高吞吐量并行性，高效处理大量视频数据，确保可扩展地生成带时间戳的外观记录。然后，这些记录被转换为一套全面的可视化效果，包括出现频率图、持续时间分析、饼图、共现矩阵、网络图、堆积面积图、季节比较和热图。这些可视化效果共同提供了对视频内容的多维洞察，揭示了名人知名度、屏幕时间分布、时间动态、共同出现关系以及跨剧集和季节的强度模式。系统的交互特性允许用户动态探索数据、识别关键时刻并揭示个体之间不断变化的关系。通过将分布式识别与结构化、视觉驱动的分析结合起来，这项工作为娱乐分析、内容创建策略和受众参与度研究带来了新的可能性。</li>
</ul>

<h3>Title: Multimodal Arabic Captioning with Interpretable Visual Concept Integration</h3>
<ul>
<li><strong>Authors: </strong>Passant Elchafei, Amany Fashwan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03295">https://arxiv.org/abs/2510.03295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03295">https://arxiv.org/pdf/2510.03295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03295]] Multimodal Arabic Captioning with Interpretable Visual Concept Integration(https://arxiv.org/abs/2510.03295)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present VLCAP, an Arabic image captioning framework that integrates CLIP-based visual label retrieval with multimodal text generation. Rather than relying solely on end-to-end captioning, VLCAP grounds generation in interpretable Arabic visual concepts extracted with three multilingual encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label retrieval. A hybrid vocabulary is built from training captions and enriched with about 21K general domain labels translated from the Visual Genome dataset, covering objects, attributes, and scenes. The top-k retrieved labels are transformed into fluent Arabic prompts and passed along with the original image to vision-language models. In the second stage, we tested Qwen-VL and Gemini Pro Vision for caption generation, resulting in six encoder-decoder configurations. The results show that mCLIP + Gemini Pro Vision achieved the best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL obtained the highest LLM-judge score (36.33%). This interpretable pipeline enables culturally coherent and contextually accurate Arabic captions.</li>
<li><strong>摘要：</strong>我们提出了VLCAP，这是一种阿拉伯图像字幕框架，将基于夹子的视觉标签检索与多模式文本生成集成在一起。 VLCAP的基础不仅依赖于端到端的字幕，不如用三个多语言编码器Mclip，Araclip和Jina V4提取的可解释的阿拉伯视觉概念中的VLCAP代码生成，分别对标签检索进行了评估。混合词汇是由训练标题构建的，并具有大约21k的一般域标签，这些域名从视觉基因组数据集中翻译而成，涵盖对象，属性和场景。 TOP-K检索的标签被转换为流利的阿拉伯语提示，并与原始图像一起传递到视觉模型。在第二阶段，我们测试了QWEN-VL和Gemini Pro Vision的字幕生成，从而产生了六个编码器码头配置。结果表明，McLip + Gemini Pro Vision获得了最佳的BLEU-1（5.34％）和余弦相似性（60.01％），而Araclip + Qwen-Vl获得了最高的LLM法官得分（36.33％）。这种可解释的管道可以使文化连贯且上下文准确的阿拉伯语标题。</li>
</ul>

<h3>Title: Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Daiheng Gao, Nanxiang Jiang, Andi Zhang, Shilin Lu, Yufei Tang, Wenbo Zhou, Weiming Zhang, Zhaoxin Fan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03302">https://arxiv.org/abs/2510.03302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03302">https://arxiv.org/pdf/2510.03302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03302]] Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models(https://arxiv.org/abs/2510.03302)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Concept erasure techniques have been widely deployed in T2I diffusion models to prevent inappropriate content generation for safety and copyright considerations. However, as models evolve to next-generation architectures like Flux, established erasure methods (\textit{e.g.}, ESD, UCE, AC) exhibit degraded effectiveness, raising questions about their true mechanisms. Through systematic analysis, we reveal that concept erasure creates only an illusion of ``amnesia": rather than genuine forgetting, these methods bias sampling trajectories away from target concepts, making the erasure fundamentally reversible. This insight motivates the need to distinguish superficial safety from genuine concept removal. In this work, we propose \textbf{RevAm} (\underline{Rev}oking \underline{Am}nesia), an RL-based trajectory optimization framework that resurrects erased concepts by dynamically steering the denoising process without modifying model weights. By adapting Group Relative Policy Optimization (GRPO) to diffusion models, RevAm explores diverse recovery trajectories through trajectory-level rewards, overcoming local optima that limit existing methods. Extensive experiments demonstrate that RevAm achieves superior concept resurrection fidelity while reducing computational time by 10$\times$, exposing critical vulnerabilities in current safety mechanisms and underscoring the need for more robust erasure techniques beyond trajectory manipulation.</li>
<li><strong>摘要：</strong>概念擦除技术已被广泛部署在T2I扩散模型中，以防止因安全性和版权考虑而产生不适当的内容。但是，随着模型的发展到诸如通量之类的下一代体系结构，建立的擦除方法（\ textit {e.g。}，ESD，UCE，AC）表现出降级的有效性，提出了有关其真实机制的问题。通过系统的分析，我们揭示了概念擦除仅产生``健忘症''的幻象：这些方法不是真正的遗忘，而是偏向采样轨迹与目标概念的偏见，从而使擦除从根本上可逆。这种洞察力的需求是促使您在这项工作中区分出来的概念。 \下划线{am} nesia），一种基于RL的轨迹优化框架，通过在不修改模型权重的情况下动态地转向deno的过程，从而使组相对策略优化（grpo）通过范围探索轨迹恢复范围，从而将相对的恢复范围限制在范围内。概念复活的保真度在将计算时间减少10 $ \ times $的同时，暴露了当前安全机制中的关键漏洞，并强调了对超出轨迹操作的更强大擦除技术的需求。</li>
</ul>

<h3>Title: A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety</h3>
<ul>
<li><strong>Authors: </strong>Shucheng Zhang, Yan Shi, Bingzhang Wang, Yuang Zhang, Muhammad Monjurul Karim, Kehua Chen, Chenxi Liu, Mehrdad Nasri, Yinhai Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03314">https://arxiv.org/abs/2510.03314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03314">https://arxiv.org/pdf/2510.03314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03314]] A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety(https://arxiv.org/abs/2510.03314)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and cyclists, remains a critical global challenge, as conventional infrastructure-based measures often prove inadequate in dynamic urban environments. Recent advances in artificial intelligence (AI), particularly in visual perception and reasoning, open new opportunities for proactive and context-aware VRU protection. However, existing surveys on AI applications for VRUs predominantly focus on detection, offering limited coverage of other vision-based tasks that are essential for comprehensive VRU understanding and protection. This paper presents a state-of-the-art review of recent progress in camera-based AI sensing systems for VRU safety, with an emphasis on developments from the past five years and emerging research trends. We systematically examine four core tasks, namely detection and classification, tracking and reidentification, trajectory prediction, and intent recognition and prediction, which together form the backbone of AI-empowered proactive solutions for VRU protection in intelligent transportation systems. To guide future research, we highlight four major open challenges from the perspectives of data, model, and deployment. By linking advances in visual AI with practical considerations for real-world implementation, this survey aims to provide a foundational reference for the development of next-generation sensing systems to enhance VRU safety.</li>
<li><strong>摘要：</strong>确保行人和骑自行车者等脆弱的道路使用者（VRU）的安全仍然是一个至关重要的全球挑战，因为在动态的城市环境中，基于常规的基础设施措施通常证明是不足的。人工智能（AI）的最新进展，尤其是在视觉感知和推理方面，开放了积极主动和背景感知的VRU保护的新机会。但是，对VRU的AI应用程序的现有调查主要集中在检测上，提供了对其他基于视觉的任务的有限覆盖范围，这对于全面的VRU理解和保护至关重要。本文介绍了对基于摄像机的AI传感系统的最新进展的最新评论，以实现VRU安全性，重点是过去五年来的发展以及新兴的研究趋势。我们系统地检查了四个核心任务，即检测和分类，跟踪和重新识别，轨迹预测以及意图识别和预测，它们共同构成了智能运输系统中VRU保护的AI授权主动解决方案的骨干。为了指导未来的研究，我们从数据，模型和部署的角度重点介绍了四个主要的开放挑战。通过将视觉AI的进步与实际实施的实际考虑，该调查旨在为开发下一代传感系统的开发提供基础参考，以增强VRU安全性。</li>
</ul>

<h3>Title: Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Kabil, Ghada Khoriba, Mina Yousef, Essam A. Rashed</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03318">https://arxiv.org/abs/2510.03318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03318">https://arxiv.org/pdf/2510.03318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03318]] Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications(https://arxiv.org/abs/2510.03318)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Medical Image Segmentation (MIS) stands as a cornerstone in medical image analysis, playing a pivotal role in precise diagnostics, treatment planning, and monitoring of various medical conditions. This paper presents a comprehensive and systematic survey of MIS methodologies, bridging the gap between traditional image processing techniques and modern deep learning approaches. The survey encompasses thresholding, edge detection, region-based segmentation, clustering algorithms, and model-based techniques while also delving into state-of-the-art deep learning architectures such as Convolutional Neural Networks (CNNs), Fully Convolutional Networks (FCNs), and the widely adopted U-Net and its variants. Moreover, integrating attention mechanisms, semi-supervised learning, generative adversarial networks (GANs), and Transformer-based models is thoroughly explored. In addition to covering established methods, this survey highlights emerging trends, including hybrid architectures, cross-modality learning, federated and distributed learning frameworks, and active learning strategies, which aim to address challenges such as limited labeled datasets, computational complexity, and model generalizability across diverse imaging modalities. Furthermore, a specialized case study on lumbar spine segmentation is presented, offering insights into the challenges and advancements in this relatively underexplored anatomical region. Despite significant progress in the field, critical challenges persist, including dataset bias, domain adaptation, interpretability of deep learning models, and integration into real-world clinical workflows.</li>
<li><strong>摘要：</strong>医学图像分割（MIS）是医学图像分析中的基石，在精确诊断，治疗计划和各种医疗状况的监测中起着关键作用。本文介绍了对误差方法的全面，系统的调查，弥合了传统图像处理技术与现代深度学习方法之间的差距。该调查包括阈值，边缘检测，基于区域的分割，聚类算法和基于模型的技术，同时还深入研究了最先进的深度学习体系结构，例如卷积神经网络（CNN），完全卷积网络（FCN），以及广泛采用的U-NET及其广泛使用的U-NET及其变异。此外，彻底探索了集成注意机制，半监督学习，生成对抗网络（GAN）和基于变压器的模型。除了涵盖既定方法外，该调查还强调了新兴趋势，包括混合体系结构，跨模式学习，联合和分布式学习框架以及主动学习策略，这些策略旨在解决诸如有限的标记数据集，计算复杂性以及跨不同成像模态的有限标记数据集，计算复杂性以及模型的通用性。此外，还提出了一项有关腰椎分割的专门案例研究，从而有见识这个相对不受欢迎的解剖区域的挑战和进步。尽管该领域取得了重大进展，但关键挑战仍然存在，包括数据集偏见，域适应性，深度学习模型的解释性以及整合到现实世界中的临床工作流程中。</li>
</ul>

<h3>Title: Fast frequency reconstruction using Deep Learning for event recognition in ring laser data</h3>
<ul>
<li><strong>Authors: </strong>Giuseppe Di Somma, Giorgio Carelli, Angela D.V. Di Virgilio, Francesco Fuso, Enrico Maccioni, Paolo Marsili</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph, physics.data-an, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03325">https://arxiv.org/abs/2510.03325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03325">https://arxiv.org/pdf/2510.03325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03325]] Fast frequency reconstruction using Deep Learning for event recognition in ring laser data(https://arxiv.org/abs/2510.03325)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The reconstruction of a frequency with minimal delay from a sinusoidal signal is a common task in several fields; for example Ring Laser Gyroscopes, since their output signal is a beat frequency. While conventional methods require several seconds of data, we present a neural network approach capable of reconstructing frequencies of several hundred Hertz within approximately 10 milliseconds. This enables rapid trigger generation. The method outperforms standard Fourier-based techniques, improving frequency estimation precision by a factor of 2 in the operational range of GINGERINO, our Ring Laser Gyroscope.\\ In addition to fast frequency estimation, we introduce an automated classification framework to identify physical disturbances in the signal, such as laser instabilities and seismic events, achieving accuracy rates between 99\% and 100\% on independent test datasets for the seismic class. These results mark a step forward in integrating artificial intelligence into signal analysis for geophysical applications.</li>
<li><strong>摘要：</strong>从正弦信号中延迟最小的频率的重建是几个领域的常见任务。例如，由于它们的输出信号为节拍频率，因此环形激光陀螺仪。尽管常规方法需要几秒钟的数据，但我们提出了一种能够在大约10毫秒内重建数百个HERTZ的神经网络方法。这可以快速触发生成。该方法的表现优于基于傅立叶的标准技术，在Gingerino的操作范围内提高了频率估计精度2倍，我们的环激光陀螺仪。除了快速频率估计外，我们还引入了一个自动化分类框架，以识别信号中的物理干扰，以确定激光率和SEISIS率的速率，如激光效率和SEISIS速度，并实现99级的速率。地震阶级。这些结果标志着将人工智能整合到地球物理应用的信号分析中迈出的一步。</li>
</ul>

<h3>Title: Matching the Optimal Denoiser in Point Cloud Diffusion with (Improved) Rotational Alignment</h3>
<ul>
<li><strong>Authors: </strong>Ameya Daigavane, YuQing Xie, Bodhi P. Vani, Saeed Saremi, Joseph Kleinhenz, Tess Smidt</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03335">https://arxiv.org/abs/2510.03335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03335">https://arxiv.org/pdf/2510.03335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03335]] Matching the Optimal Denoiser in Point Cloud Diffusion with (Improved) Rotational Alignment(https://arxiv.org/abs/2510.03335)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are a popular class of generative models trained to reverse a noising process starting from a target data distribution. Training a diffusion model consists of learning how to denoise noisy samples at different noise levels. When training diffusion models for point clouds such as molecules and proteins, there is often no canonical orientation that can be assigned. To capture this symmetry, the true data samples are often augmented by transforming them with random rotations sampled uniformly over $SO(3)$. Then, the denoised predictions are often rotationally aligned via the Kabsch-Umeyama algorithm to the ground truth samples before computing the loss. However, the effect of this alignment step has not been well studied. Here, we show that the optimal denoiser can be expressed in terms of a matrix Fisher distribution over $SO(3)$. Alignment corresponds to sampling the mode of this distribution, and turns out to be the zeroth order approximation for small noise levels, explaining its effectiveness. We build on this perspective to derive better approximators to the optimal denoiser in the limit of small noise. Our experiments highlight that alignment is often a `good enough' approximation for the noise levels that matter most for training diffusion models.</li>
<li><strong>摘要：</strong>扩散模型是一类流行的生成模型类别，该模型训练了从目标数据分布开始的尖叫过程。训练一个扩散模型包括学习如何在不同的噪声水平下定位嘈杂的样品。当训练分子和蛋白质等点云的扩散模型时，通常没有可以分配的规范取向。为了捕获这种对称性，通常通过将它们随机旋转在$上均匀地采样（3）$的随机旋转来扩大真实的数据样本。然后，在计算损失之前，经常通过kabsch-umeyama算法与地面真相样本旋转对齐。但是，该对齐步骤的效果尚未得到很好的研究。在这里，我们表明，最佳Denoiser可以用$ $（3）$的矩阵Fisher分布来表示。对齐对应于对该分布的模式进行采样，事实证明是小噪声水平的零阶近似值，从而解释了其有效性。我们以这种观点为基础，以在小噪声的极限下为最佳DeOiser提供更好的近似值。我们的实验表明，对齐通常是“足够好”的近似值，对于训练扩散模型最重要的噪声水平。</li>
</ul>

<h3>Title: OpusAnimation: Code-Based Dynamic Chart Generation</h3>
<ul>
<li><strong>Authors: </strong>Bozheng Li, Miao Yang, Zhenhan Chen, Jiawang Cao, Mushui Liu, Yi Lu, Yongliang Wu, Bin Zhang, Yangguang Ji, Licheng Tang, Jay Wu, Wenbo Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03341">https://arxiv.org/abs/2510.03341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03341">https://arxiv.org/pdf/2510.03341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03341]] OpusAnimation: Code-Based Dynamic Chart Generation(https://arxiv.org/abs/2510.03341)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Dynamic Chart Generation (DCG) involves producing code-rendered animated visualizations as charts. While recent advances in multi-modal large language models (MLLMs) have significantly improved their capability on static chart generation and comprehension, MLLMs' potential for handling dynamic chart generation and understanding remains underexplored. To bridge this research gap, we introduce DCG-Bench (Dynamic Chart Generation Benchmark), the first benchmark evaluating MLLM's capability on dynamic chart generation tasks from three dimensions: Simple Text-to-Chart, Detailed Text-to-Chart, and Video-to-Chart tasks. We construct DCG-8K, a high-quality DCG dataset with annotations covering instruction-code-video triplets and QA pairs for both code and video evaluation. Based on DCG-8K, we explored a two-stage training recipe, proposing Joint-Code-Visual Reward for group relative policy optimization to construct expert MLLM Qwen2.5-VL-DCG-3B for the DCG task. Our benchmarking result reveals shortcomings of existing MLLMs in the visual-to-chart task, and our model beats the best open-sourced MLLM with an average 8.31% performance gain across three tasks, and shows on par performance against proprietary models with only 3B parameters, proving the effectiveness of our training recipe. Our code and dataset will be publicly available.</li>
<li><strong>摘要：</strong>动态图表生成（DCG）涉及生成代码渲染的动画可视化作为图表。尽管多模式大语言模型（MLLM）的最新进展已显着提高了其在静态图表生成和理解方面的能力，但MLLM的处理动态图表生成和理解的潜力仍然没有得到充实。为了弥合这一研究差距，我们介绍了DCG基础台（动态图表生成基准），这是第一个基准测试，该基准评估了MLLM从三个维度上的动态图表生成任务上的能力：简单的文本到创建，详细的，详细的文本到创建和视频到审视任务。我们构建了DCG-8K，这是一个高质量的DCG数据集，带有注释，涵盖了代码和视频评估的指令代码-Video三胞胎和QA对。基于DCG-8K，我们探索了一个两阶段的培训配方，提出了针对小组相对政策优化的联合代码 - 视觉奖励，以构建DCG任务的专家MLLM QWEN2.5-VL-DCG-3B。我们的基准测试结果揭示了视觉到创建任务中现有MLLM的缺点，我们的模型击败了最佳的开源MLLM，在三个任务中平均绩效增长了8.31％，并且仅针对只有3B参数的专有模型表明了PAR性能，证明了我们的培训配方的有效性。我们的代码和数据集将公开可用。</li>
</ul>

<h3>Title: Inference-Time Search using Side Information for Diffusion-based Image Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Mahdi Farahbakhsh, Vishnu Teja Kunde, Dileep Kalathil, Krishna Narayanan, Jean-Francois Chamberland</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03352">https://arxiv.org/abs/2510.03352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03352">https://arxiv.org/pdf/2510.03352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03352]] Inference-Time Search using Side Information for Diffusion-based Image Reconstruction(https://arxiv.org/abs/2510.03352)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as powerful priors for solving inverse problems. However, existing approaches typically overlook side information that could significantly improve reconstruction quality, especially in severely ill-posed settings. In this work, we propose a novel inference-time search algorithm that guides the sampling process using the side information in a manner that balances exploration and exploitation. This enables more accurate and reliable reconstructions, providing an alternative to the gradient-based guidance that is prone to reward-hacking artifacts. Our approach can be seamlessly integrated into a wide range of existing diffusion-based image reconstruction pipelines. Through extensive experiments on a number of inverse problems, such as box inpainting, super-resolution, and various deblurring tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that our approach consistently improves the qualitative and quantitative performance of diffusion-based image reconstruction algorithms. We also show the superior performance of our approach with respect to other baselines, including reward gradient-based guidance algorithms. The code is available at \href{this https URL}{this repository}.</li>
<li><strong>摘要：</strong>扩散模型已成为解决反问题的强大先验。但是，现有方法通常忽略了侧面信息，这些信息可能会大大提高重建质量，尤其是在严重不良的环境中。在这项工作中，我们提出了一种新颖的推理时间搜索算法，该算法以平衡探索和开发的方式使用侧面信息来指导采样过程。这使得更准确，更可靠的重建是基于梯度的指导的替代方法，该指导容易奖励黑客工件。我们的方法可以无缝集成到广泛的基于扩散的图像重建管道中。通过对许多反问题的广泛实验，例如盒装，超分辨率以及各种脱蓝色任务，包括运动，高斯，非线性和盲目的脱毛，我们表明我们的方法一致地改善了基于扩散的图像图像构造算法的定性和定量性能。我们还展示了我们在其他基线方面的出色表现，包括基于奖励梯度的指导算法。该代码可在\ href {此https url} {此存储库}中获得。</li>
</ul>

<h3>Title: Provenance Networks: End-to-End Exemplar-Based Explainability</h3>
<ul>
<li><strong>Authors: </strong>Ali Kayyam, Anusha Madan Gopal, M. Anthony Lewis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03361">https://arxiv.org/abs/2510.03361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03361">https://arxiv.org/pdf/2510.03361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03361]] Provenance Networks: End-to-End Exemplar-Based Explainability(https://arxiv.org/abs/2510.03361)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We introduce provenance networks, a novel class of neural models designed to provide end-to-end, training-data-driven explainability. Unlike conventional post-hoc methods, provenance networks learn to link each prediction directly to its supporting training examples as part of the model's normal operation, embedding interpretability into the architecture itself. Conceptually, the model operates similarly to a learned KNN, where each output is justified by concrete exemplars weighted by relevance in the feature space. This approach facilitates systematic investigations of the trade-off between memorization and generalization, enables verification of whether a given input was included in the training set, aids in the detection of mislabeled or anomalous data points, enhances resilience to input perturbations, and supports the identification of similar inputs contributing to the generation of a new data point. By jointly optimizing the primary task and the explainability objective, provenance networks offer insights into model behavior that traditional deep networks cannot provide. While the model introduces additional computational cost and currently scales to moderately sized datasets, it provides a complementary approach to existing explainability techniques. In particular, it addresses critical challenges in modern deep learning, including model opaqueness, hallucination, and the assignment of credit to data contributors, thereby improving transparency, robustness, and trustworthiness in neural models.</li>
<li><strong>摘要：</strong>我们介绍了出处网络，这是一种新颖的神经模型类别，旨在提供端到端，训练DATA驱动的解释性。与传统的事后方法不同，出处网络学会将每个预测直接链接到其支持培训示例，作为模型正常操作的一部分，将可解释性嵌入体系结构本身中。从概念上讲，该模型的运行方式与学习的KNN相似，在该KNN中，每个输出都是通过特征空间中相关性加权的具体示例来证明的。这种方法促进了对记忆和泛化之间权衡的系统研究，可以验证培训集中是否包括给定输入，有助于检测错误标记或异常数据点，增强对输入扰动的弹性，并支持有助于产生新数据点的类似输入的识别。通过共同优化主要任务和解释性目标，出处网络提供了传统深层网络无法提供的模型行为的见解。虽然该模型将额外的计算成本和当前扩展到适度大小的数据集时，但它为现有的解释性技术提供了互补的方法。特别是，它解决了现代深度学习中的关键挑战，包括模型不透明，幻觉以及将信贷分配给数据贡献者，从而提高了神经模型中的透明度，稳健性和可信度。</li>
</ul>

<h3>Title: Diffusion-Based, Data-Assimilation-Enabled Super-Resolution of Hub-height Winds</h3>
<ul>
<li><strong>Authors: </strong>Xiaolong Ma, Xu Dong, Ashley Tarrant, Lei Yang, Rao Kotamarthi, Jiali Wang, Feng Yan, Rajkumar Kettimuthu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03364">https://arxiv.org/abs/2510.03364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03364">https://arxiv.org/pdf/2510.03364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03364]] Diffusion-Based, Data-Assimilation-Enabled Super-Resolution of Hub-height Winds(https://arxiv.org/abs/2510.03364)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>High-quality observations of hub-height winds are valuable but sparse in space and time. Simulations are widely available on regular grids but are generally biased and too coarse to inform wind-farm siting or to assess extreme-weather-related risks (e.g., gusts) at infrastructure scales. To fully utilize both data types for generating high-quality, high-resolution hub-height wind speeds (tens to ~100m above ground), this study introduces WindSR, a diffusion model with data assimilation for super-resolution downscaling of hub-height winds. WindSR integrates sparse observational data with simulation fields during downscaling using state-of-the-art diffusion models. A dynamic-radius blending method is introduced to merge observations with simulations, providing conditioning for the diffusion process. Terrain information is incorporated during both training and inference to account for its role as a key driver of winds. Evaluated against convolutional-neural-network and generative-adversarial-network baselines, WindSR outperforms them in both downscaling efficiency and accuracy. Our data assimilation reduces WindSR's model bias by approximately 20% relative to independent observations.</li>
<li><strong>摘要：</strong>高质量观察到集线器高风是有价值的，但时空稀疏。仿真在常规网格上广泛使用，但通常有偏见且太粗糙，无法告知风向选址或评估基础设施尺度上与高温相关的风险（例如，阵风）。为了充分利用两种数据类型来生成高质量的高分辨率集线器高风速（地面以上〜100m），这项研究引入了WINDSR，这是一个扩散模型，该模型具有数据同化，用于超级分辨率降低集线器高度风。 WINDSR使用最先进的扩散模型将稀疏的观测数据与模拟场整合在一起。引入了一种动态 - 拉迪乌斯混合方法，以与模拟合并观察，为扩散过程提供条件。在培训和推断期间，都将地形信息纳入了其作为风的主要驱动力的作用。 Windsr评估了反卷积神经网络和生成对抗网络基线的评估，在降低效率和准确性方面都优于它们。相对于独立观察，我们的数据同化将Windsr的模型偏置减少了约20％。</li>
</ul>

<h3>Title: Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Renrong Shao, Wei Zhang, Jun wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03375">https://arxiv.org/abs/2510.03375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03375">https://arxiv.org/pdf/2510.03375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03375]] Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation(https://arxiv.org/abs/2510.03375)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Data-free knowledge distillation~(DFKD) is an effective manner to solve model compression and transmission restrictions while retaining privacy protection, which has attracted extensive attention in recent years. Currently, the majority of existing methods utilize a generator to synthesize images to support the distillation. Although the current methods have achieved great success, there are still many issues to be explored. Firstly, the outstanding performance of supervised learning in deep learning drives us to explore a pseudo-supervised paradigm on DFKD. Secondly, current synthesized methods cannot distinguish the distributions of different categories of samples, thus producing ambiguous samples that may lead to an incorrect evaluation by the teacher. Besides, current methods cannot optimize the category-wise diversity samples, which will hinder the student model learning from diverse samples and further achieving better performance. In this paper, to address the above limitations, we propose a novel learning paradigm, i.e., conditional pseudo-supervised contrast for data-free knowledge distillation~(CPSC-DFKD). The primary innovations of CPSC-DFKD are: (1) introducing a conditional generative adversarial network to synthesize category-specific diverse images for pseudo-supervised learning, (2) improving the modules of the generator to distinguish the distributions of different categories, and (3) proposing pseudo-supervised contrastive learning based on teacher and student views to enhance diversity. Comprehensive experiments on three commonly-used datasets validate the performance lift of both the student and generator brought by CPSC-DFKD. The code is available at this https URL</li>
<li><strong>摘要：</strong>无数据知识蒸馏〜（DFKD）是解决模型压缩和传输限制的同时保留隐私保护的有效方式，近年来引起了广泛的关注。当前，大多数现有方法都利用发电机合成图像来支持蒸馏。尽管当前的方法取得了巨大的成功，但仍有许多问题要探讨。首先，深度学习中有监督的学习的出色表现驱使我们探索了DFKD上的伪监督范式。其次，当前合成的方法无法区分不同类别的样本的分布，从而产生模棱两可的样本，这可能会导致教师的评估不正确。此外，当前的方法无法优化类别的多样性样本，这将阻碍学生模型从不同样本中学习并进一步实现更好的性能。在本文中，为了解决上述局限性，我们提出了一种新颖的学习范式，即无数据知识蒸馏〜（CPSC-DFKD）的有条件伪监督的对比度。 CPSC-DFKD的主要创新是：（1）引入条件生成的对抗网络，以合成类别的特定类别的不同图像，以进行伪文明的学习，（2）为了区​​分不同类别的生成器模块，以改善不同类别的分布，以及（3）提出基于多样化的教师的教师和对相反的教师的意见。在三个常用数据集上进行的全面实验验证了CPSC-DFKD带来的学生和发电机的性能提升。该代码可在此HTTPS URL上找到</li>
</ul>

<h3>Title: Visual Language Model as a Judge for Object Detection in Industrial Diagrams</h3>
<ul>
<li><strong>Authors: </strong>Sanjukta Ghosh</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03376">https://arxiv.org/abs/2510.03376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03376">https://arxiv.org/pdf/2510.03376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03376]] Visual Language Model as a Judge for Object Detection in Industrial Diagrams(https://arxiv.org/abs/2510.03376)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Industrial diagrams such as piping and instrumentation diagrams (P&IDs) are essential for the design, operation, and maintenance of industrial plants. Converting these diagrams into digital form is an important step toward building digital twins and enabling intelligent industrial automation. A central challenge in this digitalization process is accurate object detection. Although recent advances have significantly improved object detection algorithms, there remains a lack of methods to automatically evaluate the quality of their outputs. This paper addresses this gap by introducing a framework that employs Visual Language Models (VLMs) to assess object detection results and guide their refinement. The approach exploits the multimodal capabilities of VLMs to identify missing or inconsistent detections, thereby enabling automated quality assessment and improving overall detection performance on complex industrial diagrams.</li>
<li><strong>摘要：</strong>管道和仪器图（P＆ID）等工业图对于工厂的设计，操作和维护至关重要。将这些图转换为数字形式是迈向建立数字双胞胎并实现智能工业自动化的重要一步。在这个数字化过程中的一个核心挑战是准确的对象检测。尽管最近的进步显着改善了对象检测算法，但仍缺乏自动评估其产出质量的方法。本文通过引入采用视觉语言模型（VLM）来评估对象检测结果并指导其完善的框架来解决这一差距。该方法利用了VLM的多模式能力来识别缺失或不一致的检测，从而实现了自动化的质量评估并提高了复杂工业图的总体检测性能。</li>
</ul>

<h3>Title: From Scope to Script: An Automated Report Generation Model for Gastrointestinal Endoscopy</h3>
<ul>
<li><strong>Authors: </strong>Evandros Kaklamanos, Kristjana Kristinsdottir, Jonathan Huang, Dustin Carlson, Rajesh Keswani, John Pandolfino, Mozziyar Etemadi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03543">https://arxiv.org/abs/2510.03543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03543">https://arxiv.org/pdf/2510.03543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03543]] From Scope to Script: An Automated Report Generation Model for Gastrointestinal Endoscopy(https://arxiv.org/abs/2510.03543)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Endoscopic procedures such as esophagogastroduodenoscopy (EGD) and colonoscopy play a critical role in diagnosing and managing gastrointestinal (GI) disorders. However, the documentation burden associated with these procedures place significant strain on gastroenterologists, contributing to inefficiencies in clinical workflows and physician burnout. To address this challenge, we propose a novel automated report generation model that leverages a transformer-based vision encoder and text decoder within a two-stage training framework. In the first stage, both components are pre-trained on image/text caption pairs to capture generalized vision-language features, followed by fine-tuning on images/report pairs to generate clinically meaningful findings. Our approach not only streamlines the documentation process but also holds promise for reducing physician workload and improving patient care.</li>
<li><strong>摘要：</strong>内窥镜手术（例如食管胃构镜检查（EGD）和结肠镜检查）在诊断和管理胃肠道（GI）疾病中起着至关重要的作用。但是，与这些程序相关的文档负担对胃肠病学家造成了重大压力，这导致临床工作流程和医师倦怠的效率低下。为了应对这一挑战，我们提出了一种新颖的自动报告生成模型，该模型在两个阶段的培训框架内利用基于变压器的视觉编码器和文本解码器。在第一阶段，这两个组件均在图像/文本字幕对中进行了预训练，以捕获广义视力语言特征，然后对图像/报告对进行微调，以生成临床意义的发现。我们的方法不仅简化了文档过程，而且还具有减少医师工作量和改善患者护理的希望。</li>
</ul>

<h3>Title: Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing</h3>
<ul>
<li><strong>Authors: </strong>Danial Samadi Vahdati, Tai Duc Nguyen, Ekta Prashnani, Koki Nagano, David Luebke, Orazio Gallo, Matthew Stamm</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03548">https://arxiv.org/abs/2510.03548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03548">https://arxiv.org/pdf/2510.03548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03548]] Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing(https://arxiv.org/abs/2510.03548)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>AI-based talking-head videoconferencing systems reduce bandwidth by sending a compact pose-expression latent and re-synthesizing RGB at the receiver, but this latent can be puppeteered, letting an attacker hijack a victim's likeness in real time. Because every frame is synthetic, deepfake and synthetic video detectors fail outright. To address this security problem, we exploit a key observation: the pose-expression latent inherently contains biometric information of the driving identity. Therefore, we introduce the first biometric leakage defense without ever looking at the reconstructed RGB video: a pose-conditioned, large-margin contrastive encoder that isolates persistent identity cues inside the transmitted latent while cancelling transient pose and expression. A simple cosine test on this disentangled embedding flags illicit identity swaps as the video is rendered. Our experiments on multiple talking-head generation models show that our method consistently outperforms existing puppeteering defenses, operates in real-time, and shows strong generalization to out-of-distribution scenarios.</li>
<li><strong>摘要：</strong>基于人工智能的说话头视频会议系统通过向接收器发送紧凑的姿势表达潜在和重新合成的RGB来减少带宽，但是可以伪造这种潜在的RGB，让攻击者实时劫持受害者的​​相似之处。因为每个框架都是合成的，所以深层和合成视频探测器完全失败。为了解决这个安全问题，我们利用一个关键观察：姿势表达潜在固有地包含驾驶身份的生物特征识别信息。因此，我们介绍了第一个生物识别泄漏防御，而没有查看重建的RGB视频：姿势条件，大幅度的对比度编码器，该视频隔离了传播潜在的持续性身份线索，同时取消了短暂的姿势和表达。在视频呈现时，对此截断的嵌入旗的简单余弦测试是非法的身份互换。我们在多个谈话头生成模型上的实验表明，我们的方法始终优于现有的伪造防御，实时运行，并显示出对分布外情景的强烈概括。</li>
</ul>

<h3>Title: Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!</h3>
<ul>
<li><strong>Authors: </strong>Junbao Zhou, Yuan Zhou, Kesen Zhao, Qingshan Xu, Beier Zhu, Richang Hong, Hanwang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03550">https://arxiv.org/abs/2510.03550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03550">https://arxiv.org/pdf/2510.03550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03550]] Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!(https://arxiv.org/abs/2510.03550)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Achieving streaming, fine-grained control over the outputs of autoregressive video diffusion models remains challenging, making it difficult to ensure that they consistently align with user expectations. To bridge this gap, we propose \textbf{stReaming drag-oriEnted interactiVe vidEo manipuLation (REVEL)}, a new task that enables users to modify generated videos \emph{anytime} on \emph{anything} via fine-grained, interactive drag. Beyond DragVideo and SG-I2V, REVEL unifies drag-style video manipulation as editing and animating video frames with both supporting user-specified translation, deformation, and rotation effects, making drag operations versatile. In resolving REVEL, we observe: \emph{i}) drag-induced perturbations accumulate in latent space, causing severe latent distribution drift that halts the drag process; \emph{ii}) streaming drag is easily disturbed by context frames, thereby yielding visually unnatural outcomes. We thus propose a training-free approach, \textbf{DragStream}, comprising: \emph{i}) an adaptive distribution self-rectification strategy that leverages neighboring frames' statistics to effectively constrain the drift of latent embeddings; \emph{ii}) a spatial-frequency selective optimization mechanism, allowing the model to fully exploit contextual information while mitigating its interference via selectively propagating visual cues along generation. Our method can be seamlessly integrated into existing autoregressive video diffusion models, and extensive experiments firmly demonstrate the effectiveness of our DragStream.</li>
<li><strong>摘要：</strong>实现对自回归视频扩散模型输出的流媒体，细粒度的控制仍然具有挑战性，因此很难确保它们始终如一地与用户期望保持一致。为了弥合这一差距，我们建议\ textbf {流向拖放的交互式视频操作（revel）}，这是一个新任务，使用户能够通过细粒度，交互式拖动在\ emph {nothing}上修改生成的视频\ emph {anytime}。除了DragVideo和SG-I2V之外，Revel还将拖动视频操纵统一为编辑和动画视频帧，并具有支持用户指定的翻译，变形和旋转效果，从而使拖动操作多功能。在解决狂欢时，我们观察到：\ emph {i}）阻力诱导的扰动积累在潜在空间中，导致严重的潜在分布漂移阻止了阻力过程； \ emph {ii}）流式阻力很容易被上下文帧打扰，从而产生了视觉上不自然的结果。因此，我们提出了一种无训练方法，\ textbf {dragstream}，包括：\ emph {i}）一种自适应分布的自我分配策略，该策略利用相邻框架的统计数据有效地限制了潜在嵌入的漂移； \ emph {ii}）一种空间频率选择性优化机制，允许模型完全利用上下文信息，同时通过选择性地传播沿一代生成的视觉提示来减轻其干扰。我们的方法可以无缝集成到现有的自动回归视频扩散模型中，并且广泛的实验牢固地证明了我们的拖网的有效性。</li>
</ul>

<h3>Title: Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs</h3>
<ul>
<li><strong>Authors: </strong>Fatmazohra Rezkellah, Ramzi Dakhmouche</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR, cs.CY, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03567">https://arxiv.org/abs/2510.03567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03567">https://arxiv.org/pdf/2510.03567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03567]] Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs(https://arxiv.org/abs/2510.03567)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>With the increasing adoption of Large Language Models (LLMs), more customization is needed to ensure privacy-preserving and safe generation. We address this objective from two critical aspects: unlearning of sensitive information and robustness to jail-breaking attacks. We investigate various constrained optimization formulations that address both aspects in a \emph{unified manner}, by finding the smallest possible interventions on LLM weights that either make a given vocabulary set unreachable or embed the LLM with robustness to tailored attacks by shifting part of the weights to a \emph{safer} region. Beyond unifying two key properties, this approach contrasts with previous work in that it doesn't require an oracle classifier that is typically not available or represents a computational overhead. Surprisingly, we find that the simplest point-wise constraint-based intervention we propose leads to better performance than max-min interventions, while having a lower computational cost. Comparison against state-of-the-art defense methods demonstrates superior performance of the proposed approach.</li>
<li><strong>摘要：</strong>随着大型语言模型（LLM）的越来越多，需要更多的自定义来确保保护隐私和安全的生成。我们从两个关键方面解决了这一目标：对敏感信息的学习和牢固的攻击。我们通过在LLM重量上找到最小的可能干预措施来研究各种受约束的优化公式，这些配方以\ emph {unified Manigy}来解决这两个方面，这些干预措施使给定的词汇集无法通过将重量的一部分转移到\ emph {Safer {Safer}区域，以使给定的词汇集无法实现LLM，或者以固定的态度嵌入LLM。除了统一两个关键属性外，这种方法与以前的工作形成对比，因为它不需要通常不可用或代表计算开销的Oracle分类器。令人惊讶的是，我们发现，我们提出的最简单的基于点约束的干预措施可导致比Max-Min干预措施更好，同时具有较低的计算成本。与最先进的防御方法的比较证明了拟议方法的出色表现。</li>
</ul>

<h3>Title: Longitudinal Flow Matching for Trajectory Modeling</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Mohaiminul Islam, Thijs P. Kuipers, Sharvaree Vadgama, Coen de Vente, Afsana Khan, Clara I. Sánchez, Erik J. Bekkers</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03569">https://arxiv.org/abs/2510.03569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03569">https://arxiv.org/pdf/2510.03569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03569]] Longitudinal Flow Matching for Trajectory Modeling(https://arxiv.org/abs/2510.03569)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models for sequential data often struggle with sparsely sampled and high-dimensional trajectories, typically reducing the learning of dynamics to pairwise transitions. We propose \textit{Interpolative Multi-Marginal Flow Matching} (IMMFM), a framework that learns continuous stochastic dynamics jointly consistent with multiple observed time points. IMMFM employs a piecewise-quadratic interpolation path as a smooth target for flow matching and jointly optimizes drift and a data-driven diffusion coefficient, supported by a theoretical condition for stable learning. This design captures intrinsic stochasticity, handles irregular sparse sampling, and yields subject-specific trajectories. Experiments on synthetic benchmarks and real-world longitudinal neuroimaging datasets show that IMMFM outperforms existing methods in both forecasting accuracy and further downstream tasks.</li>
<li><strong>摘要：</strong>顺序数据的生成模型通常会在稀疏采样和高维轨迹上遇到困难，通常会减少对成对过渡的动力学学习。我们提出\ textIt {插值多 - 边际流匹配}（IMMFM），该框架学习连续随机动力学与多个观察到的时间点共同一致。 IMMFM采用分段 - 季节插值路径作为流动匹配的平滑目标，并共同优化了漂移和数据驱动的扩散系数，并受到稳定学习的理论条件的支持。该设计捕获了内在的随机性，处理不规则的稀疏采样，并产生特定于主题的轨迹。关于合成基准和现实世界纵向神经影像学数据集的实验表明，IMMFM在预测准确性和进一步的下游任务中都胜过现有方法。</li>
</ul>

<h3>Title: Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL</h3>
<ul>
<li><strong>Authors: </strong>Ruitao Wu, Yifan Zhao, Guangyao Chen, Jia Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03608">https://arxiv.org/abs/2510.03608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03608">https://arxiv.org/pdf/2510.03608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03608]] Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL(https://arxiv.org/abs/2510.03608)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially learn new classes from minimal examples without forgetting prior knowledge, a task complicated by the stability-plasticity dilemma and data scarcity. Current FSCIL methods often struggle with generalization due to their reliance on limited datasets. While diffusion models offer a path for data augmentation, their direct application can lead to semantic misalignment or ineffective guidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel framework that establishes a mutual boosting loop between diffusion model and FSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a dynamic, multi-faceted reward function derived from the classifier's state directs the diffusion model. This reward system operates at two levels: the feature level ensures semantic coherence and diversity using prototype-anchored maximum mean discrepancy and dimension-wise variance matching, while the logits level promotes exploratory image generation and enhances inter-class discriminability through confidence recalibration and cross-session confusion-aware mechanisms. This co-evolutionary process, where generated images refine the classifier and an improved classifier state yields better reward signals, demonstrably achieves state-of-the-art performance on FSCIL benchmarks, significantly enhancing both knowledge retention and new class learning.</li>
<li><strong>摘要：</strong>很少有类型的课堂学习（FSCIL）挑战模型，以从最小示例中依次学习新类，而不会忘记先验知识，这一任务因稳定性 - 塑性困境和数据稀缺而变得复杂。当前的FSCIL方法由于依赖有限的数据集而经常在概括方面遇到困难。尽管扩散模型为数据增强提供了一条途径，但它们的直接应用可能导致语义错位或无效的指导。本文介绍了扩散分类器协同作用（DC），这是一个新型框架，该框架在扩散模型和FSCIL分类器之间建立了相互提升的循环。 DC采用了奖励一致的学习策略，其中一种动态的，多方面的奖励函数从分类器的状态引导，指导扩散模型。该奖励系统在两个层面上运行：功能级别可确保使用原型锚定的最大平均差异和尺寸差异匹配的原型固定性和多样性，而logits级别则可以通过置信度重新校准和交叉衰退的混淆机制来促进探索性图像的产生并增强阶层间的辨别力。这个共同进化的过程，生成的图像完善分类器和改进的分类器状态可产生更好的奖励信号，可以明显地在FSCIL基准上实现最先进的表现，从而显着增强知识保留和新的类学习。</li>
</ul>

<h3>Title: Neural Bayesian Filtering</h3>
<ul>
<li><strong>Authors: </strong>Christopher Solinas, Radovan Haluska, David Sychrovsky, Finbarr Timbers, Nolan Bard, Michael Buro, Martin Schmid, Nathan R. Sturtevant, Michael Bowling</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03614">https://arxiv.org/abs/2510.03614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03614">https://arxiv.org/pdf/2510.03614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03614]] Neural Bayesian Filtering(https://arxiv.org/abs/2510.03614)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present Neural Bayesian Filtering (NBF), an algorithm for maintaining distributions over hidden states, called beliefs, in partially observable systems. NBF is trained to find a good latent representation of the beliefs induced by a task. It maps beliefs to fixed-length embedding vectors, which condition generative models for sampling. During filtering, particle-style updates compute posteriors in this embedding space using incoming observations and the environment's dynamics. NBF combines the computational efficiency of classical filters with the expressiveness of deep generative models - tracking rapidly shifting, multimodal beliefs while mitigating the risk of particle impoverishment. We validate NBF in state estimation tasks in three partially observable environments.</li>
<li><strong>摘要：</strong>我们提出了神经贝叶斯过滤（NBF），这是一种用于在某些可观察到的系统中，用于维持隐藏状态（称为信念）的分布的算法。 NBF经过训练，可以找到由任务引起的信念的良好潜在表现。它将信念映射到固定长度嵌入向量，该载体是为采样的生成模型。在过滤过程中，粒子式更新使用传入的观测值和环境的动力学来计算此嵌入空间中的后代。 NBF结合了经典过滤器的计算效率和深层生成模型的表现力 - 跟踪迅速转移，多模式信念，同时减轻粒子贫困的风险。我们在三个部分可观察到的环境中验证NBF的状态估计任务。</li>
</ul>

<h3>Title: From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Ali Azizpour, Reza Ramezanpour, Ashutosh Sabharwal, Santiago Segarra</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03690">https://arxiv.org/abs/2510.03690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03690">https://arxiv.org/pdf/2510.03690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03690]] From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning(https://arxiv.org/abs/2510.03690)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Real-world graph datasets often consist of mixtures of populations, where graphs are generated from multiple distinct underlying distributions. However, modern representation learning approaches, such as graph contrastive learning (GCL) and augmentation methods like Mixup, typically overlook this mixture structure. In this work, we propose a unified framework that explicitly models data as a mixture of underlying probabilistic graph generative models represented by graphons. To characterize these graphons, we leverage graph moments (motif densities) to cluster graphs arising from the same model. This enables us to disentangle the mixture components and identify their distinct generative mechanisms. This model-aware partitioning benefits two key graph learning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a data augmentation technique that interpolates in a semantically valid space guided by the estimated graphons, instead of assuming a single graphon per class. 2) For GCL, it enables model-adaptive and principled augmentations. Additionally, by introducing a new model-aware objective, our proposed approach (termed MGCL) improves negative sampling by restricting negatives to graphs from other models. We establish a key theoretical guarantee: a novel, tighter bound showing that graphs sampled from graphons with small cut distance will have similar motif densities with high probability. Extensive experiments on benchmark datasets demonstrate strong empirical performance. In unsupervised learning, MGCL achieves state-of-the-art results, obtaining the top average rank across eight datasets. In supervised learning, GMAM consistently outperforms existing strategies, achieving new state-of-the-art accuracy in 6 out of 7 datasets.</li>
<li><strong>摘要：</strong>现实世界图数据集通常由种群的混合物组成，其中图是从多个不同的基础分布生成的。但是，现代表示学习方法，例如图形对比学习（GCL）和混合诸如混合的增强方法，通常忽略了这种混合结构。在这项工作中，我们提出了一个统一的框架，该框架将数据显式建模为图形代表的潜在概率图生成模型的混合物。为了表征这些图形，我们利用图形矩（基序密度）来群集图形，该图形由相同的模型产生。这使我们能够解散混合物成分并确定其独特的生成机制。这种模型感知的分区受益于两个关键的图形学习任务：1）它可以实现Graphon-Mixture-Awaire Mixup（GMAM），这是一种数据增强技术，该技术在由估计的图形指导的语义有效空间中插值，而不是假设每个班级都有一个图形子。 2）对于GCL，它可以实现模型自动和原则性的增强。此外，通过引入一个新的模型感知目标，我们提出的方法（称为MGCL）通过将负面因素限制为其他模型的图来改善负面采样。我们建立了一个关键的理论保证：一种新颖的，更紧密的结合，表明从较小切割距离的图形中采样的图形将具有相似的基序密度，概率很高。基准数据集上的广泛实验表现出强烈的经验性能。在无监督的学习中，MGCL取得了最新的结果，在八个数据集中获得了最高的平均等级。在监督的学习中，GMAM始终胜过现有的策略，在7个数据集中有6个实现了新的最先进的准确性。</li>
</ul>

<h3>Title: Merge and Guide: Unifying Model Merging and Guided Decoding for Controllable Multi-Objective Generation</h3>
<ul>
<li><strong>Authors: </strong>Guofu Xie, Chen Zhang, Xiao Zhang, Yunsheng Shi, Ting Yao, Jun Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03782">https://arxiv.org/abs/2510.03782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03782">https://arxiv.org/pdf/2510.03782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03782]] Merge and Guide: Unifying Model Merging and Guided Decoding for Controllable Multi-Objective Generation(https://arxiv.org/abs/2510.03782)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Adapting to diverse user needs at test time is a key challenge in controllable multi-objective generation. Existing methods are insufficient: merging-based approaches provide indirect, suboptimal control at the parameter level, often disregarding the impacts of multiple objectives. While decoding-based guidance is more direct, it typically requires aggregating logits from multiple expert models, incurring significant space overhead and relying heavily on individual model capacity. To address these issues, we introduce Merge-And-GuidE (MAGE), a two-stage framework that leverages model merging for guided decoding. We first identify a critical compatibility problem between the guidance and base models. In Stage 1, MAGE resolves this by dynamically constructing a more robust base model, merging a series of backbone models that account for multiple objectives. In Stage 2, we merge explicit and implicit value models into a unified guidance proxy, which then steers the decoding of the base model from Stage 1. Our analysis empirically validates Linear Mode Connectivity (LMC) in value models, explores the relationship between model merging and prediction ensembling, and demonstrates the enhanced controllability afforded by our approach. Extensive experiments show that our method outperforms existing approaches, achieving superior controllability, Pareto-optimal performance, and enhanced adaptability.</li>
<li><strong>摘要：</strong>在测试时间适应多样化的用户需求是可控多目标生成的关键挑战。现有方法不足：基于合并的方法在参数级别提供了间接的，次优的控制，通常会忽略多个目标的影响。尽管基于解码的指导更为直接，但通常需要从多个专家模型中汇总逻辑，从而产生了大量空间的开销并严重依赖单个模型容量。为了解决这些问题，我们介绍了合并和指南（MAGE），这是一个两阶段的框架，利用模型合并进行指导解码。我们首先确定指导和基本模型之间的关键兼容性问题。在第1阶段，法师通过动态构建更健壮的基本模型，合并一系列说明多个目标的骨干模型来解决此问题。在第2阶段，我们将明确和隐式价值模型合并到统一的指导代理中，然后引导基本模型从阶段1中解码。我们的分析在价值模型中经验验证了线性模式连接性（LMC），探索了模型合并和预测之间的关系，并证明了我们的方法可提供的增强的可控性。广泛的实验表明，我们的方法的表现优于现有方法，实现了卓越的可控性，最佳的性能和增强的适应性。</li>
</ul>

<h3>Title: Curriculum-Augmented GFlowNets For mRNA Sequence Generation</h3>
<ul>
<li><strong>Authors: </strong>Aya Laajil, Abduragim Shtanchaev, Sajan Muhammad, Eric Moulines, Salem Lahlou</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03811">https://arxiv.org/abs/2510.03811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03811">https://arxiv.org/pdf/2510.03811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03811]] Curriculum-Augmented GFlowNets For mRNA Sequence Generation(https://arxiv.org/abs/2510.03811)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Designing mRNA sequences is a major challenge in developing next-generation therapeutics, since it involves exploring a vast space of possible nucleotide combinations while optimizing sequence properties like stability, translation efficiency, and protein expression. While Generative Flow Networks are promising for this task, their training is hindered by sparse, long-horizon rewards and multi-objective trade-offs. We propose Curriculum-Augmented GFlowNets (CAGFN), which integrate curriculum learning with multi-objective GFlowNets to generate de novo mRNA sequences. CAGFN integrates a length-based curriculum that progressively adapts the maximum sequence length guiding exploration from easier to harder subproblems. We also provide a new mRNA design environment for GFlowNets which, given a target protein sequence and a combination of biological objectives, allows for the training of models that generate plausible mRNA candidates. This provides a biologically motivated setting for applying and advancing GFlowNets in therapeutic sequence design. On different mRNA design tasks, CAGFN improves Pareto performance and biological plausibility, while maintaining diversity. Moreover, CAGFN reaches higher-quality solutions faster than a GFlowNet trained with random sequence sampling (no curriculum), and enables generalization to out-of-distribution sequences.</li>
<li><strong>摘要：</strong>设计mRNA序列是开发下一代疗法的主要挑战，因为它涉及探索大量可能的核苷酸组合，同时优化序列特性，例如稳定性，翻译效率和蛋白质表达。尽管生成流网络对此任务有希望，但他们的培训受到稀疏，长期奖励和多目标权衡的阻碍。我们提出了课程提升的Gflownets（CAGFN），将课程学习与多目标Gflownets整合起来，以生成从头mRNA序列。 CAGFN集成了一个基于长度的课程，该课程逐渐使最大序列长度从易于到硬的子问题进行调整。我们还为Gflownets提供了一种新的mRNA设计环境，鉴于靶蛋白序列和生物学目标的组合，可以训练产生合理的MRNA候选物的模型。这为在治疗序列设计中应用和推进Gflownets提供了一个以生物学动机的设置。在不同的mRNA设计任务上，CAGFN可以提高帕累托的性能和生物学上的合理性，同时保持多样性。此外，CAGFN比接受随机序列采样（无课程）训练的GFLOWNET更快地达到较高的解决方案，并使概括能够概括分布序列。</li>
</ul>

<h3>Title: Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation</h3>
<ul>
<li><strong>Authors: </strong>Venkata Narendra Kotyada, Revanth Eranki, Nagesh Bhattu Sristy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03821">https://arxiv.org/abs/2510.03821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03821">https://arxiv.org/pdf/2510.03821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03821]] Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation(https://arxiv.org/abs/2510.03821)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Unpaired image-to-image translation involves learning mappings between source domain and target domain in the absence of aligned or corresponding samples. Score based diffusion models have demonstrated state-of-the-art performance in generative tasks. Their ability to approximate complex data distributions through stochastic differential equations (SDEs) enables them to generate high-fidelity and diverse outputs, making them particularly well-suited for unpaired I2I settings. In parallel, contrastive learning provides a powerful framework for learning semantic similarities without the need for explicit supervision or paired data. By pulling together representations of semantically similar samples and pushing apart dissimilar ones, contrastive methods are inherently aligned with the objectives of unpaired translation. Its ability to selectively enforce semantic consistency at the feature level makes contrastive learning particularly effective for guiding generation in unpaired scenarios. In this work, we propose a time-dependent contrastive learning approach where a model is trained with SimCLR by considering an image and its domain invarient feature as a positive pair, enabling the preservation of domain-invariant features and the discarding of domain-specific ones. The learned contrastive model then guides the inference of a pretrained SDE for the I2I translation task. We empirically compare Contrastive-SDE with several baselines across three common unpaired I2I tasks, using four metrics for evaluation. Constrastive-SDE achieves comparable results to the state-of-the-art on several metrics. Furthermore, we observe that our model converges significantly faster and requires no label supervision or classifier training, making it a more efficient alternative for this task.</li>
<li><strong>摘要：</strong>不成对的图像到图像的转换涉及在没有对齐或对应样本的情况下学习源域和目标域之间的映射。基于分数的扩散模型在生成任务中展示了最先进的性能。它们能够通过随机微分方程 (SDE) 近似复杂的数据分布，从而能够生成高保真度和多样化的输出，从而特别适合不配对的 I2I 设置。同时，对比学习提供了一个强大的框架来学习语义相似性，而无需显式监督或配对数据。通过将语义相似样本的表示放在一起并分开不同的样本，对比方法本质上与不配对翻译的目标保持一致。它能够在特征级别选择性地强制语义一致性，使得对比学习对于指导不配对场景中的生成特别有效。在这项工作中，我们提出了一种与时间相关的对比学习方法，其中通过将图像及其域不变特征视为正对，使用 SimCLR 来训练模型，从而能够保留域不变特征并丢弃特定于域的特征。然后，学习到的对比模型指导 I2I 翻译任务的预训练 SDE 的推理。我们根据经验将 Contrastive-SDE 与三个常见的不配对 I2I 任务的多个基线进行比较，使用四个指标进行评估。 Contrastive-SDE 在多个指标上取得了与最新技术相当的结果。此外，我们观察到我们的模型收敛速度明显更快，并且不需要标签监督或分类器训练，这使其成为该任务的更有效替代方案。</li>
</ul>

<h3>Title: Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Pranav Sharma, Shivank Garg, Durga Toshniwal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03840">https://arxiv.org/abs/2510.03840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03840">https://arxiv.org/pdf/2510.03840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03840]] Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models(https://arxiv.org/abs/2510.03840)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in image generation models have led to models that produce synthetic images that are increasingly difficult for standard AI detectors to identify, even though they often remain distinguishable by humans. To identify this discrepancy, we introduce \textbf{Mirage}, a curated dataset comprising a diverse range of AI-generated images exhibiting visible artifacts, where current state-of-the-art detection methods largely fail. Furthermore, we investigate whether Large Vision-Language Models (LVLMs), which are increasingly employed as substitutes for human judgment in various tasks, can be leveraged for explainable AI image detection. Our experiments on both Mirage and existing benchmark datasets demonstrate that while LVLMs are highly effective at detecting AI-generated images with visible artifacts, their performance declines when confronted with images lacking such cues.</li>
<li><strong>摘要：</strong>图像产生模型的最新进展导致了产生合成图像的模型，这些图像越来越难以标准AI检测器识别，即使它们通常仍然可以被人类区分开。为了确定这种差异，我们介绍了\ textbf {mirage}，这是一个策划的数据集，其中包含各种AI生成的图像，展示了可见的伪影，其中当前的最新检测方法在很大程度上失败了。此外，我们调查了越来越多地用作各种任务中人类判断的大型视觉模型（LVLM），可以利用可解释的AI图像检测。我们对Mirage和现有基准数据集的实验表明，尽管LVLM在检测具有可见伪影的AI生成的图像方面非常有效，但在面对缺乏此类提示的图像时，它们的性能下降。</li>
</ul>

<h3>Title: UGround: Towards Unified Visual Grounding with Unrolled Transformers</h3>
<ul>
<li><strong>Authors: </strong>Rui Qian, Xin Yin, Chuanhang Deng, Zhiyuan Peng, Jian Xiong, Wei Zhai, Dejing Dou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03853">https://arxiv.org/abs/2510.03853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03853">https://arxiv.org/pdf/2510.03853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03853]] UGround: Towards Unified Visual Grounding with Unrolled Transformers(https://arxiv.org/abs/2510.03853)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present UGround, a \textbf{U}nified visual \textbf{Ground}ing paradigm that dynamically selects intermediate layers across \textbf{U}nrolled transformers as ``mask as prompt'', diverging from the prevailing pipeline that leverages the fixed last hidden layer as ``\texttt{<SEG>} as prompt''. UGround addresses two primary challenges posed by the prevailing paradigm: (1) its reliance on the fixed last hidden layer, which sequentially amplifies cumulative errors arising from layer-by-layer propagation without intermediate correction, and (2) its use of \texttt{<SEG>} as a prompt, which implicitly projects textual embeddings into visual space without explicit spatial cues (\eg, coordinates). Central to UGround is Policy-Prompted Masking, which comprises two key components: Stochastic Skip Connection (SSC) and Mask as Prompt (MasP). SSC is a reinforcement learning policy that, via stochastic sampling, allows each \texttt{<SEG>} token to slide across unrolled transformer layers, enabling dynamic layer selection at which it connects to the vision model (\eg, SAM) in a skip-connection fashion. Given the selected hidden layer, MasP uses the similarity map derived from the \texttt{<SEG>} token and image tokens as a soft logit mask to prompt SAM for mask generation, offering explicit spatial cues through its activation regions. To validate the effectiveness of UGround, we, for the first time, have unified visual grounding within a single framework from an attribute perspective, spanning from traditional refer expression segmentation to newly proposed reasoning segmentation, single-target to multi-target, positive query to false premise (empty target). All codes and models are publicly available at \href{this https URL}{this https URL}.</li>
<li><strong>摘要：</strong>We present UGround, a \textbf{U}nified visual \textbf{Ground}ing paradigm that dynamically selects intermediate layers across \textbf{U}nrolled transformers as ``mask as prompt'', diverging from the prevailing pipeline that leverages the fixed last hidden layer as ``\texttt{<SEG>} as prompt''. Uground解决了普遍的范式提出的两个主要挑战：（1）其对固定的最后一个隐藏层的依赖，该层依次放大累积错误引起的累积错误，而无需中间校正，（2）它对\ texttt {<seg>}的使用迅速地启动了，该空间是迅速的，即迅速启动的，该空间是迅速的，即启动的范围，该范围迅速地将其视为启动（<seg>}，这是一个隐含的太空，它的实图（<seg>}的实例均可使用（<seg>}），该空间（<seg>}的效果（<seg>}）的使用（<seg>}，这是一个隐含的太空（<seg>}）坐标）。 Uground的核心是政策促进的掩蔽，其中包括两个关键组成部分：随机跳过连接（SSC）和掩码作为提示（MASP）。 SSC是一项强化学习政策，通过随机抽样，允许每个\ texttt {<seg>}令牌跨过展开的变压器层滑动，从而使动态层选择在跳过连接方式中连接到视觉模型（\ eg，sam）。给定选定的隐藏层，MASP使用从\ texttt {<seg>}代币和图像令牌作为软logit掩码的相似性图来提示SAM进行掩码生成，从而通过其激活区域提供明确的空间提示。为了验证Uground的有效性，我们首次从属性的角度在单个框架内建立了统一的视觉接地，从传统的参考表达分割到新提出的推理分割，单目标，单目标到多目标，积极的查询，正面查询到虚假的前提（空目标）。所有代码和模型均可在\ href {this HTTPS url} {此https url}上公开可用。</li>
</ul>

<h3>Title: SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Nikolaos Kaparinos, Vasileios Mezaris</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03870">https://arxiv.org/abs/2510.03870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03870">https://arxiv.org/pdf/2510.03870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03870]] SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks(https://arxiv.org/abs/2510.03870)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution, generative</a></li>
<li><strong>Abstract: </strong>Generative Adversarial Networks (GANs) achieve excellent performance in generative tasks, such as image super-resolution, but their computational requirements make difficult their deployment on resource-constrained devices. While knowledge distillation is a promising research direction for GAN compression, effectively training a smaller student generator is challenging due to the capacity mismatch between the student generator and the teacher discriminator. In this work, we propose Student Discriminator Assisted Knowledge Distillation (SDAKD), a novel GAN distillation methodology that introduces a student discriminator to mitigate this capacity mismatch. SDAKD follows a three-stage training strategy, and integrates an adapted feature map distillation approach in its last two training stages. We evaluated SDAKD on two well-performing super-resolution GANs, GCFSR and Real-ESRGAN. Our experiments demonstrate consistent improvements over the baselines and SOTA GAN knowledge distillation methods. The SDAKD source code will be made openly available upon acceptance of the paper.</li>
<li><strong>摘要：</strong>生成对抗网络（GAN）在生成任务（例如图像超分辨率）中实现了出色的性能，但是它们的计算要求使其在资源受限设备上的部署很难。尽管知识蒸馏是GAN压缩的有前途的研究方向，但由于学生发生器和教师歧视者之间的能力不匹配，因此有效地培训了较小的学生发生器的挑战。在这项工作中，我们提出了学生歧视者协助知识蒸馏（SDAKD），这是一种新颖的GAN蒸馏方法，它引入了学生歧视者来减轻这种能力不匹配。 SDAKD遵循三阶段的训练策略，并在其最后两个训练阶段中整合了适用的功能地图蒸馏方法。我们评估了两个表现出色的超分辨率GAN，GCFSR和Real-esrgan的SDAKD。我们的实验表明，对基准和SOTA GAN知识蒸馏方法的一致改进。接受纸张后，将公开提供SDAKD源代码。</li>
</ul>

<h3>Title: DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human</h3>
<ul>
<li><strong>Authors: </strong>Yunhao Li, Sijing Wu, Yucheng Zhu, Huiyu Duan, Zicheng Zhang, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03874">https://arxiv.org/abs/2510.03874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03874">https://arxiv.org/pdf/2510.03874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03874]] DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human(https://arxiv.org/abs/2510.03874)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, quality assessment</a></li>
<li><strong>Abstract: </strong>With the rapid development of 3D scanning and reconstruction technologies, dynamic digital human avatars based on 4D meshes have become increasingly popular. A high-precision dynamic digital human avatar can be applied to various fields such as game production, animation generation, and remote immersive communication. However, these 4D human avatar meshes are prone to being degraded by various types of noise during the processes of collection, compression, and transmission, thereby affecting the viewing experience of users. In light of this fact, quality assessment of dynamic 4D digital humans becomes increasingly important. In this paper, we first propose a large-scale dynamic digital human quality assessment dataset, DHQA-4D, which contains 32 high-quality real-scanned 4D human mesh sequences, 1920 distorted textured 4D human meshes degraded by 11 textured distortions, as well as their corresponding textured and non-textured mean opinion scores (MOSs). Equipped with DHQA-4D dataset, we analyze the influence of different types of distortion on human perception for textured dynamic 4D meshes and non-textured dynamic 4D meshes. Additionally, we propose DynaMesh-Rater, a novel large multimodal model (LMM) based approach that is able to assess both textured 4D meshes and non-textured 4D meshes. Concretely, DynaMesh-Rater elaborately extracts multi-dimensional features, including visual features from a projected 2D video, motion features from cropped video clips, and geometry features from the 4D human mesh to provide comprehensive quality-related information. Then we utilize a LMM model to integrate the multi-dimensional features and conduct a LoRA-based instruction tuning technique to teach the LMM model to predict the quality scores. Extensive experimental results on the DHQA-4D dataset demonstrate the superiority of our DynaMesh-Rater method over previous quality assessment methods.</li>
<li><strong>摘要：</strong>随着3D扫描和重建技术的快速发展，基于4D网格的动态数字人体化身变得越来越流行。高精度动态数字人体化身可以应用于游戏制作，动画生成和远程沉浸式沟通等各个领域。但是，这些4D人类的头像网眼容易在收集，压缩和传输过程中被各种类型的噪声降解，从而影响用户的观看体验。鉴于这一事实，动态4D数字人类的质量评估变得越来越重要。在本文中，我们首先提出了一个大型动态数字人体质量评估数据集DHQA-4D，其中包含32个高质量的实范围的4D人网序，1920年，纹理的4D人类网格被扭曲的纹理4D人类网格降低了11个纹理畸变，以及相应的纹理和非纹理和非纹理和非态度的意见，以及均值的scors（Moss）（Moss）。配备了DHQA-4D数据集，我们分析了不同类型的失真对纹理动态4D网格和非纹理动态4D网格的影响的影响。此外，我们提出了一种基于新型的大型多模型（LMM）方法Dynamesh-Rater，能够评估纹理的4D网格和非纹理的4D网格。具体的，Dynamesh-Far的详细提取了多维功能，包括预计2D视频的视觉特征，裁剪视频剪辑的运动功能以及4D人网格的几何特征，以提供全面的质量相关信息。然后，我们利用LMM模型整合了多维特征，并进行基于LORA的指令调整技术来教授LMM模型以预测质量得分。 DHQA-4D数据集的广泛实验结果证明了我们的Dynamesh-Rater方法比以前的质量评估方法的优越性。</li>
</ul>

<h3>Title: Exploring Instruction Data Quality for Explainable Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Yunhao Li, Sijing Wu, Huiyu Duan, Yucheng Zhu, Qi Jia, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03880">https://arxiv.org/abs/2510.03880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03880">https://arxiv.org/pdf/2510.03880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03880]] Exploring Instruction Data Quality for Explainable Image Quality Assessment(https://arxiv.org/abs/2510.03880)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>In recent years, with the rapid development of powerful multimodal large language models (MLLMs), explainable image quality assessment (IQA) has gradually become popular, aiming at providing quality-related descriptions and answers of images. To achieve this goal, recent methods seek to construct a large-scale instruction tuning dataset to empower the MLLM with quality perception ability following the well-known scaling law. However, a large amount of instruction tuning data may cause substantial computational costs and redundant data, which in turn will cause harm to the performance of the model. To cope with this problem, in this paper, we challenge the scaling law and systematically investigate the role of data quality of the instruction tuning dataset for explainable IQA. Using a powerful pre-trained MLLM, we first investigate the changes in model performance after fine-tuning with different sizes of instruction tuning data. We find that selecting a subset of the data set randomly using an appropriate ratio can even lead to better results than training with the entire instruction tuning dataset, demonstrating the redundancy of current explainable IQA instruction tuning data. Beyond randomly sampling a subset, we propose a clustering-based data selection framework with three stages: clustering feature extraction, cluster quota allocation, and cluster sampling strategy. Then we systematically analyze the choices of each stage and propose a simple but efficient data selection method IQA-Select for explainable IQA. The experimental results demonstrate that IQA-Select can achieve 102.1% and 103.7% performance of full fine-tuning using only 10% selected data in Q-Bench and AesBench respectively, significantly reducing computational costs while achieving better performance.</li>
<li><strong>摘要：</strong>近年来，随着强大的多模态大语言模型（MLLM）的快速发展，可解释的图像质量评估（IQA）逐渐流行，旨在提供与图像质量相关的描述和答案。为了实现这一目标，最近的方法试图构建一个大规模的指令调整数据集，以使 MLLM 遵循众所周知的缩放定律具有质量感知能力。然而，大量的指令调优数据可能会导致大量的计算成本和冗余数据，进而对模型的性能造成损害。为了解决这个问题，在本文中，我们挑战缩放定律并系统地研究指令调整数据集的数据质量对于可解释的 IQA 的作用。使用强大的预训练 MLLM，我们首先研究使用不同大小的指令调优数据进行微调后模型性能的变化。我们发现，使用适当的比率随机选择数据集的子集甚至可以比使用整个指令调优数据集进行训练获得更好的结果，这证明了当前可解释的 IQA 指令调优数据的冗余性。除了随机采样子集之外，我们还提出了一个基于聚类的数据选择框架，分为三个阶段：聚类特征提取、聚类配额分配和聚类采样策略。然后我们系统地分析了每个阶段的选择，并提出了一种简单但有效的数据选择方法IQA-Select，用于可解释的IQA。实验结果表明，IQA-Select 在 Q-Bench 和 AesBench 中仅使用 10% 的选定数据即可分别实现 102.1% 和 103.7% 的完全微调性能，显着降低了计算成本，同时获得了更好的性能。</li>
</ul>

<h3>Title: BONSAI: Structure-exploiting robust Bayesian optimization for networked black-box systems under uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Akshay Kudva, Joel A. Paulson</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03893">https://arxiv.org/abs/2510.03893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03893">https://arxiv.org/pdf/2510.03893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03893]] BONSAI: Structure-exploiting robust Bayesian optimization for networked black-box systems under uncertainty(https://arxiv.org/abs/2510.03893)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Optimal design under uncertainty remains a fundamental challenge in advancing reliable, next-generation process systems. Robust optimization (RO) offers a principled approach by safeguarding against worst-case scenarios across a range of uncertain parameters. However, traditional RO methods typically require known problem structure, which limits their applicability to high-fidelity simulation environments. To overcome these limitations, recent work has explored robust Bayesian optimization (RBO) as a flexible alternative that can accommodate expensive, black-box objectives. Existing RBO methods, however, generally ignore available structural information and struggle to scale to high-dimensional settings. In this work, we introduce BONSAI (Bayesian Optimization of Network Systems under uncertAInty), a new RBO framework that leverages partial structural knowledge commonly available in simulation-based models. Instead of treating the objective as a monolithic black box, BONSAI represents it as a directed graph of interconnected white- and black-box components, allowing the algorithm to utilize intermediate information within the optimization process. We further propose a scalable Thompson sampling-based acquisition function tailored to the structured RO setting, which can be efficiently optimized using gradient-based methods. We evaluate BONSAI across a diverse set of synthetic and real-world case studies, including applications in process systems engineering. Compared to existing simulation-based RO algorithms, BONSAI consistently delivers more sample-efficient and higher-quality robust solutions, highlighting its practical advantages for uncertainty-aware design in complex engineering systems.</li>
<li><strong>摘要：</strong>不确定性下的最佳设计仍然是推进可靠的下一代流程系统的基本挑战。强大的优化（RO）通过在一系列不确定的参数中保护最坏情况，提供了一种原则性的方法。但是，传统的RO方法通常需要已知的问题结构，这将其适用性限制在高保真模拟环境中。为了克服这些局限性，最近的工作探索了强大的贝叶斯优化（RBO）作为一种灵活的替代方案，可以适应昂贵的黑盒目标。但是，现有的RBO方法通常忽略了可用的结构信息，而难以扩展到高维设置。在这项工作中，我们介绍了盆景（在不确定性下对网络系统的贝叶斯优化），这是一个新的RBO框架，利用基于模拟模型中常见的部分结构知识。盆景不是将目标视为单片黑匣子，而是将其表示为互连的白色和黑色框组件的有向图，允许该算法在优化过程中利用中间信息。我们进一步提出了一个可扩展的基于汤普森采样的采集功能，该功能量身定制为结构化RO设置，可以使用基于梯度的方法有效地优化该功能。我们在各种合成和现实世界中的案例研究中评估盆景，包括过程系统工程中的应用。与现有的基于仿真的RO算法相比，盆景始终提供更有效和更高质量的健壮解决方案，从而强调了其在复杂工程系统中不确定性意识设计的实际优势。</li>
</ul>

<h3>Title: Zero-Shot Fine-Grained Image Classification Using Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Md. Atabuzzaman, Andrew Zhang, Chris Thomas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03903">https://arxiv.org/abs/2510.03903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03903">https://arxiv.org/pdf/2510.03903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03903]] Zero-Shot Fine-Grained Image Classification Using Large Vision-Language Models(https://arxiv.org/abs/2510.03903)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) have demonstrated impressive performance on vision-language reasoning tasks. However, their potential for zero-shot fine-grained image classification, a challenging task requiring precise differentiation between visually similar categories, remains underexplored. We present a novel method that transforms zero-shot fine-grained image classification into a visual question-answering framework, leveraging LVLMs' comprehensive understanding capabilities rather than relying on direct class name generation. We enhance model performance through a novel attention intervention technique. We also address a key limitation in existing datasets by developing more comprehensive and precise class description benchmarks. We validate the effectiveness of our method through extensive experimentation across multiple fine-grained image classification benchmarks. Our proposed method consistently outperforms the current state-of-the-art (SOTA) approach, demonstrating both the effectiveness of our method and the broader potential of LVLMs for zero-shot fine-grained classification tasks. Code and Datasets: this https URL</li>
<li><strong>摘要：</strong>大型视觉模型（LVLM）在视觉推理任务上表现出了令人印象深刻的表现。但是，它们进行零拍的细粒图像分类的潜力，这是一项具有挑战性的任务，需要在视觉上相似的类别之间进行精确的区分，但仍未得到充实。我们提出了一种新颖的方法，该方法将零拍的细颗粒图像分类转换为视觉提问框架，利用LVLMS的全面理解能力，而不是依靠直接的班级名称生成。我们通过新颖的注意力干预技术增强了模型性能。我们还通过开发更全面，更精确的类描述基准来解决现有数据集中的关键限制。我们通过对多个细粒图像分类基准进行广泛的实验来验证方法的有效性。我们提出的方法始终优于当前的最新方法（SOTA）方法，既证明了我们方法的有效性，又证明了LVLM对零摄像的细粒分类任务的更广泛潜力。代码和数据集：此HTTPS URL</li>
</ul>

<h3>Title: From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance</h3>
<ul>
<li><strong>Authors: </strong>Ardalan Aryashad, Parsa Razmara, Amin Mahjoub, Seyedarmin Azizi, Mahdi Salmani, Arad Firouzkouhi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03906">https://arxiv.org/abs/2510.03906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03906">https://arxiv.org/pdf/2510.03906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03906]] From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance(https://arxiv.org/abs/2510.03906)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Autonomous driving perception systems are particularly vulnerable in foggy conditions, where light scattering reduces contrast and obscures fine details critical for safe operation. While numerous defogging methods exist-from handcrafted filters to learned restoration models-improvements in image fidelity do not consistently translate into better downstream detection and segmentation. Moreover, prior evaluations often rely on synthetic data, leaving questions about real-world transferability. We present a structured empirical study that benchmarks a comprehensive set of pipelines, including (i) classical filters, (ii) modern defogging networks, (iii) chained variants (filter$\rightarrow$model, model$\rightarrow$filter), and (iv) prompt-driven visual--language image editing models (VLM) applied directly to foggy images. Using Foggy Cityscapes, we assess both image quality and downstream performance on object detection (mAP) and segmentation (PQ, RQ, SQ). Our analysis reveals when defogging helps, when chaining yields synergy or degradation, and how VLM-based editors compare to dedicated approaches. In addition, we evaluate qualitative rubric-based scores from a VLM judge and quantify their alignment with task metrics, showing strong correlations with mAP. Together, these results establish a transparent, task-oriented benchmark for defogging methods and highlight the conditions under which preprocessing genuinely improves autonomous perception in adverse weather.</li>
<li><strong>摘要：</strong>自主驾驶感知系统在有雾的条件下特别容易受到影响，在雾气条件下，光散射减少了对比度，并掩盖了对安全操作至关重要的细节。虽然众多的脱节方法存在于手工制作的过滤器中，以了解图像保真度中的恢复模型 - 并不能始终转化为更好的下游检测和分割。此外，先前的评估通常依赖于合成数据，留下有关现实世界可传输性的问题。我们提出了一项结构化的经验研究，该研究基准了一组全面的管道，包括（i）经典过滤器，（ii）现代脱落网络，（iii）链式变体（滤镜$ \ rightarrow $型号，型号$ \ rightarrow $ filter）和（iv）及时驱动的视觉图像编辑模型（vlme）的迅速驱动式驱动式驱动式型号的应用程序。使用有雾的城市景观，我们在对象检测（MAP）和分段（PQ，RQ，SQ）上评估图像质量和下游性能。我们的分析揭示了何时会有所帮助，链接会产生协同作用或退化，以及基于VLM的编辑者与专用方法的比较。此外，我们从VLM法官中评估了基于定性的标题分数，并量化了其与任务指标的一致性，显示与MAP的密切相关。这些结果共同建立了一种透明的，面向任务的基准，用于融化方法，并突出了预处理真正改善不利天气中自主感知的条件。</li>
</ul>

<h3>Title: Generating Human Motion Videos using a Cascaded Text-to-Video Framework</h3>
<ul>
<li><strong>Authors: </strong>Hyelin Nam, Hyojun Go, Byeongjun Park, Byung-Hoon Kim, Hyungjin Chung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03909">https://arxiv.org/abs/2510.03909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03909">https://arxiv.org/pdf/2510.03909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03909]] Generating Human Motion Videos using a Cascaded Text-to-Video Framework(https://arxiv.org/abs/2510.03909)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Human video generation is becoming an increasingly important task with broad applications in graphics, entertainment, and embodied AI. Despite the rapid progress of video diffusion models (VDMs), their use for general-purpose human video generation remains underexplored, with most works constrained to image-to-video setups or narrow domains like dance videos. In this work, we propose CAMEO, a cascaded framework for general human motion video generation. It seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs, mitigating suboptimal factors that may arise in this process across both training and inference through carefully designed components. Specifically, we analyze and prepare both textual prompts and visual conditions to effectively train the VDM, ensuring robust alignment between motion descriptions, conditioning signals, and the generated videos. Furthermore, we introduce a camera-aware conditioning module that connects the two stages, automatically selecting viewpoints aligned with the input text to enhance coherence and reduce manual intervention. We demonstrate the effectiveness of our approach on both the MovieGen benchmark and a newly introduced benchmark tailored to the T2M-VDM combination, while highlighting its versatility across diverse use cases.</li>
<li><strong>摘要：</strong>通过在图形，娱乐和体现的AI中进行广泛应用，人类视频的生成正成为越来越重要的任务。尽管视频扩散模型（VDM）取得了迅速的进展，但它们用于通用人类视频生成的使用仍未得到充满激光，大多数作品都限制在图像到视频设置或诸如舞蹈视频之类的狭窄域。在这项工作中，我们提出了Cameo，这是一个级联人类运动视频的级联框架。它无缝地桥接文本到动作（T2M）模型和条件VDM，从而减轻了通过精心设计的组件在训练和推理过程中可能出现的次优因子。具体来说，我们分析和准备文本提示和视觉条件，以有效地训练VDM，确保运动描述，调理信号和生成的视频之间的稳健对齐。此外，我们引入了一个连接两个阶段的相机感知调节模块，自动选择与输入文本对齐的观点，以增强连贯性并减少手动干预。我们证明了我们的方法对MovieGen基准和针对T2M-VDM组合量身定制的新推出的基准的有效性，同时突出了其在各种用例中的多功能性。</li>
</ul>

<h3>Title: Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition</h3>
<ul>
<li><strong>Authors: </strong>Arushi Dashore, Aryan Anumala, Emily Hui, Olivia Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03921">https://arxiv.org/abs/2510.03921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03921">https://arxiv.org/pdf/2510.03921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03921]] Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition(https://arxiv.org/abs/2510.03921)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Automated tennis stroke analysis has advanced significantly with the integration of biomechanical motion cues alongside deep learning techniques, enhancing stroke classification accuracy and player performance evaluation. Despite these advancements, existing systems often fail to connect biomechanical insights with actionable language feedback that is both accessible and meaningful to players and coaches. This research project addresses this gap by developing a novel framework that extracts key biomechanical features (such as joint angles, limb velocities, and kinetic chain patterns) from motion data using Convolutional Neural Network Long Short-Term Memory (CNN-LSTM)-based models. These features are analyzed for relationships influencing stroke effectiveness and injury risk, forming the basis for feedback generation using large language models (LLMs). Leveraging the THETIS dataset and feature extraction techniques, our approach aims to produce feedback that is technically accurate, biomechanically grounded, and actionable for end-users. The experimental setup evaluates this framework on classification performance and interpretability, bridging the gap between explainable AI and sports biomechanics.</li>
<li><strong>摘要：</strong>随着生物力学运动线索与深度学习技术的整合，增强了中风分类精度和玩家绩效评估，自动网球中风分析已取得了显着提高。尽管有这些进步，但现有系统通常无法将生物力学见解与可行的语言反馈联系起来，这对玩家和教练既可以访问又有意义。该研究项目通过开发一个新的框架来解决这一差距，该框架使用卷积神经网络长短期记忆（CNN-LSTM）模型从运动数据中提取关键的生物力学特征（例如关节角度，肢体速度和动力学链模式）。分析了影响中风效率和伤害风险的关系，从而为使用大语言模型（LLM）构成了基础。我们的方法利用Thetis数据集并具有提取技术，旨在产生技术准确，生物力学基础且对最终用户可行的反馈。实验设置评估了有关分类性能和解释性的框架，弥合了可解释的AI和运动生物力学之间的差距。</li>
</ul>

<h3>Title: On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection</h3>
<ul>
<li><strong>Authors: </strong>Weiqing He, Xiang Li, Tianqi Shang, Li Shen, Weijie Su, Qi Long</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03944">https://arxiv.org/abs/2510.03944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03944">https://arxiv.org/pdf/2510.03944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03944]] On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection(https://arxiv.org/abs/2510.03944)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) raise concerns about content authenticity and integrity because they can generate human-like text at scale. Text watermarks, which embed detectable statistical signals into generated text, offer a provable way to verify content origin. Many detection methods rely on pivotal statistics that are i.i.d. under human-written text, making goodness-of-fit (GoF) tests a natural tool for watermark detection. However, GoF tests remain largely underexplored in this setting. In this paper, we systematically evaluate eight GoF tests across three popular watermarking schemes, using three open-source LLMs, two datasets, various generation temperatures, and multiple post-editing methods. We find that general GoF tests can improve both the detection power and robustness of watermark detectors. Notably, we observe that text repetition, common in low-temperature settings, gives GoF tests a unique advantage not exploited by existing methods. Our results highlight that classic GoF tests are a simple yet powerful and underused tool for watermark detection in LLMs.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）引起了人们对内容真实性和完整性的担忧，因为它们可以大规模生成类似人类的文本。将可检测到的统计信号嵌入生成的文本中的文本水印提供了一种可证明的方法来验证内容来源。许多检测方法依赖于I.I.D的关键统计数据。在人工写的文本下，使拟合优度（GOF）测试是水印检测的天然工具。但是，在这种情况下，GOF测试在很大程度上仍未得到充实。在本文中，我们使用三个开源LLM，两个数据集，各种生成温度和多种后编辑方法，系统地评估了三个流行的水印方案中的八次GOF测试。我们发现一般的GOF测试可以提高水印探测器的检测能力和鲁棒性。值得注意的是，我们观察到，在低温设置中常见的文本重复使GOF测试具有不被现有方法利用的独特优势。我们的结果凸显了经典的GOF测试是一种简单但功能强大且没有用的工具，用于LLMS中的水印检测。</li>
</ul>

<h3>Title: Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Yaxin Hou, Bo Han, Yuheng Jia, Hui Liu, Junhui Hou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03993">https://arxiv.org/abs/2510.03993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03993">https://arxiv.org/pdf/2510.03993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03993]] Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning(https://arxiv.org/abs/2510.03993)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Current long-tailed semi-supervised learning methods assume that labeled data exhibit a long-tailed distribution, and unlabeled data adhere to a typical predefined distribution (i.e., long-tailed, uniform, or inverse long-tailed). However, the distribution of the unlabeled data is generally unknown and may follow an arbitrary distribution. To tackle this challenge, we propose a Controllable Pseudo-label Generation (CPG) framework, expanding the labeled dataset with the progressively identified reliable pseudo-labels from the unlabeled dataset and training the model on the updated labeled dataset with a known distribution, making it unaffected by the unlabeled data distribution. Specifically, CPG operates through a controllable self-reinforcing optimization cycle: (i) at each training step, our dynamic controllable filtering mechanism selectively incorporates reliable pseudo-labels from the unlabeled dataset into the labeled dataset, ensuring that the updated labeled dataset follows a known distribution; (ii) we then construct a Bayes-optimal classifier using logit adjustment based on the updated labeled data distribution; (iii) this improved classifier subsequently helps identify more reliable pseudo-labels in the next training step. We further theoretically prove that this optimization cycle can significantly reduce the generalization error under some conditions. Additionally, we propose a class-aware adaptive augmentation module to further improve the representation of minority classes, and an auxiliary branch to maximize data utilization by leveraging all labeled and unlabeled samples. Comprehensive evaluations on various commonly used benchmark datasets show that CPG achieves consistent improvements, surpassing state-of-the-art methods by up to \textbf{15.97\%} in accuracy. The code is available at this https URL.</li>
<li><strong>摘要：</strong>当前的长尾半监督学习方法假定标记的数据表现出长尾巴分布，并且未标记的数据遵循典型的预定义分布（即长尾巴，统一或倒数型）。但是，未标记数据的分布通常未知，可能遵循任意分布。为了应对这一挑战，我们提出了一个可控的伪标签生成（CPG）框架，从未标记的数据集中逐渐识别出可靠的可靠伪标记，并在已知的已知分布中扩展了可靠的伪标记，并在已知的已知分布中培训了该模型，从而使该模型未被无标记的数据分配所影响。具体而言，CPG通过可控的自我增强优化周期运行：（i）在每个训练步骤中，我们的动态可控过滤机制选择性地将来自未标记的数据集中的可靠伪标记合并到标记的数据集中，确保更新的标记数据集遵循已知的分布； （ii）然后，我们使用基于更新的标记数据分布的logit调整来构建一个贝叶斯最佳分类器； （iii）此改进的分类器随后有助于在下一个训练步骤中确定更可靠的伪标签。从理论上讲，我们进一步证明了这种优化周期可以在某些条件下显着减少概括误差。此外，我们提出了一个班级感知的自适应增强模块，以进一步改善少数群体的表示，并通过利用所有标记和未标记的样本来提高辅助分支来最大化数据利用。对各种常用基准数据集的全面评估表明，CPG可以通过准确性最多\ textbf {15.97 \％}实现一致的改进，超过了最新方法。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models</h3>
<ul>
<li><strong>Authors: </strong>Hao Wu, Yuan Gao, Xingjian Shi, Shuaipeng Li, Fan Xu, Fan Zhang, Zhihong Zhu, Weiyan Wang, Xiao Luo, Kun Wang, Xian Wu, Xiaomeng Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04020">https://arxiv.org/abs/2510.04020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04020">https://arxiv.org/pdf/2510.04020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04020]] Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models(https://arxiv.org/abs/2510.04020)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To address the dual challenges of inherent stochasticity and non-differentiable metrics in physical spatiotemporal forecasting, we propose Spatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in Model-Based Reinforcement Learning. SFP constructs a novel Generative World Model to simulate diverse, high-fidelity future states, enabling an "imagination-based" environmental simulation. Within this framework, a base forecasting model acts as an agent, guided by a beam search-based planning algorithm that leverages non-differentiable domain metrics as reward signals to explore high-return future sequences. These identified high-reward candidates then serve as pseudo-labels to continuously optimize the agent's policy through iterative self-training, significantly reducing prediction error and demonstrating exceptional performance on critical domain metrics like capturing extreme events.</li>
<li><strong>摘要：</strong>为了应对物理时空预测中固有的随机性和非差异指标的双重挑战，我们提出了时空预测作为计划（SFP），这是一种基于模型的增强学习学习的新范式。 SFP构建了一种新颖的生成世界模型，以模拟多样化的高保真未来状态，从而实现了“基于想象力的”环境模拟。在此框架内，基本的预测模型充当代理，在基于光束搜索的计划算法的指导下，该算法利用非不同的域指标作为奖励信号来探索高返回未来的序列。然后，这些确定的高回报候选者是伪标记，通过迭代自我训练，不断地优化代理商的策略，大大减少了预测误差，并在关键领域的指标上表现出卓越的性能，例如捕获极端事件。</li>
</ul>

<h3>Title: MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Pan, Yucheng Lu, Han Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04057">https://arxiv.org/abs/2510.04057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04057">https://arxiv.org/pdf/2510.04057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04057]] MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation(https://arxiv.org/abs/2510.04057)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present MetaFind, a scene-aware tri-modal compositional retrieval framework designed to enhance scene generation in the metaverse by retrieving 3D assets from large-scale repositories. MetaFind addresses two core challenges: (i) inconsistent asset retrieval that overlooks spatial, semantic, and stylistic constraints, and (ii) the absence of a standardized retrieval paradigm specifically tailored for 3D asset retrieval, as existing approaches mainly rely on general-purpose 3D shape representation models. Our key innovation is a flexible retrieval mechanism that supports arbitrary combinations of text, image, and 3D modalities as queries, enhancing spatial reasoning and style consistency by jointly modeling object-level features (including appearance) and scene-level layout structures. Methodologically, MetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that captures spatial relationships and object appearance features, ensuring retrieved 3D assets are contextually and stylistically coherent with the existing scene, regardless of coordinate frame transformations. The framework supports iterative scene construction by continuously adapting retrieval results to current scene updates. Empirical evaluations demonstrate the improved spatial and stylistic consistency of MetaFind in various retrieval tasks compared to baseline methods.</li>
<li><strong>摘要：</strong>我们提出了Metafind，这是一种场景感知的三模式构图检索框架，旨在通过从大规模存储库中检索3D资产来增强元视频的产生。 Metafind解决了两个核心挑战：（i）忽略空间，语义和风格约束的不一致的资产检索，以及（ii）缺乏专门针对3D资产检索而定制的标准化检索范式，因为现有方法主要依赖于General-Purepose 3D形状表示模型，因此现有方法主要依靠现有方法。我们的关键创新是一种灵活的检索机制，它支持文本，图像和3D模式的任意组合作为查询，通过共同建模对象级特征（包括外观）和场景级别的布局结构来增强空间推理和样式一致性。从方法上讲，Metafind引入了一个插件的播放等效布局编码器ESSGNN，该布局ESSGNN捕获空间关系和对象外观特征，从而确保检索到3D资产在上下文和风格上与现有场景相干，而与坐标帧转换无关。该框架通过将检索结果不断地调整为当前场景更新来支持迭代场景构建。与基线方法相比，经验评估表明，在各种检索任务中，元方面的空间和风格一致性提高了。</li>
</ul>

<h3>Title: Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models under Data Constraints</h3>
<ul>
<li><strong>Authors: </strong>Subhodip Panda, MS Varun, Shreyans Jain, Sarthak Kumar Maharana, Prathosh A.P</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04058">https://arxiv.org/abs/2510.04058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04058">https://arxiv.org/pdf/2510.04058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04058]] Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models under Data Constraints(https://arxiv.org/abs/2510.04058)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>For a responsible and safe deployment of diffusion models in various domains, regulating the generated outputs from these models is desirable because such models could generate undesired, violent, and obscene outputs. To tackle this problem, recent works use machine unlearning methodology to forget training data points containing these undesired features from pre-trained generative models. However, these methods proved to be ineffective in data-constrained settings where the whole training dataset is inaccessible. Thus, the principal objective of this work is to propose a machine unlearning methodology that can prevent the generation of outputs containing undesired features from a pre-trained diffusion model in such a data-constrained setting. Our proposed method, termed as Variational Diffusion Unlearning (VDU), is a computationally efficient method that only requires access to a subset of training data containing undesired features. Our approach is inspired by the variational inference framework with the objective of minimizing a loss function consisting of two terms: plasticity inducer and stability regularizer. Plasticity inducer reduces the log-likelihood of the undesired training data points, while the stability regularizer, essential for preventing loss of image generation quality, regularizes the model in parameter space. We validate the effectiveness of our method through comprehensive experiments for both class unlearning and feature unlearning. For class unlearning, we unlearn some user-identified classes from MNIST, CIFAR-10, and tinyImageNet datasets from a pre-trained unconditional denoising diffusion probabilistic model (DDPM). Similarly, for feature unlearning, we unlearn the generation of certain high-level features from a pre-trained Stable Diffusion model</li>
<li><strong>摘要：</strong>对于在各个域中扩散模型的负责任且安全的部署，可以调节这些模型产生的输出，因为这样的模型可能会产生不希望的，暴力和淫秽的输出。为了解决这个问题，最近的作品使用机器学习方法来忘记培训数据点，其中包含预先训练的生成模型中这些不希望的功能。但是，这些方法被证明在整个培训数据集无法访问的数据约束设置中无效。因此，这项工作的主要目的是提出一种机器学习方法，该方法可以防止在这种数据约束设置中产生来自预训练的扩散模型中不希望的特征的输出。我们提出的方法，称为变分扩散（VDU），是一种计算有效的方法，仅需要访问包含不希望特征的训练数据的子集。我们的方法的灵感来自各种推理框架的启发，目的是最大程度地减少由两个术语组成的损失函数：可塑性诱导剂和稳定性正常化程序。可塑性诱导剂降低了不需要的训练数据点的对数可能性，而稳定性适用器对于防止图像产生质量的损失至关重要，在参数空间中正规化该模型。我们通过全面的实验验证了我们方法的有效性，该实验既不是阶级学习和特征学习的。对于课堂学习，我们从MNIST，CIFAR-10和Tinyimagenet数据集中从预先训练的无条件denoising扩散概率模型（DDPM）中取下了一些用户识别的类。同样，对于特征学习，我们从预先训练的稳定扩散模型中学到了某些高级特征的产生</li>
</ul>

<h3>Title: Diffusion Low Rank Hybrid Reconstruction for Sparse View Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Zongyin Deng, Qing Zhou, Yuhao Fang, Zijian Wang, Yao Lu, Ye Zhang, Chun Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04069">https://arxiv.org/abs/2510.04069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04069">https://arxiv.org/pdf/2510.04069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04069]] Diffusion Low Rank Hybrid Reconstruction for Sparse View Medical Imaging(https://arxiv.org/abs/2510.04069)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This work presents TV-LoRA, a novel method for low-dose sparse-view CT reconstruction that combines a diffusion generative prior (NCSN++ with SDE modeling) and multi-regularization constraints, including anisotropic TV and nuclear norm (LoRA), within an ADMM framework. To address ill-posedness and texture loss under extremely sparse views, TV-LoRA integrates generative and physical constraints, and utilizes a 2D slice-based strategy with FFT acceleration and tensor-parallel optimization for efficient inference. Experiments on AAPM-2016, CTHD, and LIDC datasets with $N_{\mathrm{view}}=8,4,2$ show that TV-LoRA consistently surpasses benchmarks in SSIM, texture recovery, edge clarity, and artifact suppression, demonstrating strong robustness and generalizability. Ablation studies confirm the complementary effects of LoRA regularization and diffusion priors, while the FFT-PCG module provides a speedup. Overall, Diffusion + TV-LoRA achieves high-fidelity, efficient 3D CT reconstruction and broad clinical applicability in low-dose, sparse-sampling scenarios.</li>
<li><strong>摘要：</strong>这项工作介绍了TV-Lora，这是一种用于低剂量稀疏视图CT重建的新方法，将扩散生成的先验（NCSN ++与SDE建模）和多指导约束结合在一起，包括各向异性电视和核norm（LORA）。为了解决极度稀疏视图下的不良性和纹理损失，TV-Lora集成了生成和物理约束，并利用了基于2D Slice的策略，具有FFT加速度以及张量 - 平行优化的优化，以进行有效的推理。具有$ n _ {\ mathrm {view}} = 8,4,2 $的AAPM-2016，CTHD和LIDC数据集的实验表明，TV-Lora在SSIM，纹理恢复，边缘澄清和刻痕抑制，强有力的稳固性和一般性和一般性和一般性和一般性和一般性上都始终超过基准测试。消融研究证实了洛拉正则化和扩散先验的互补作用，而FFT-PCG模块提供了加速。总体而言，扩散 + TV-Lora在低剂量，稀疏抽样的情况下实现了高保真，有效的3D CT重建和广泛的临床适用性。</li>
</ul>

<h3>Title: Rethinking Consistent Multi-Label Classification under Inexact Supervision</h3>
<ul>
<li><strong>Authors: </strong>Wei Wang, Tianhao Ma, Ming-Kun Xie, Gang Niu, Masashi Sugiyama</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04091">https://arxiv.org/abs/2510.04091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04091">https://arxiv.org/pdf/2510.04091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04091]] Rethinking Consistent Multi-Label Classification under Inexact Supervision(https://arxiv.org/abs/2510.04091)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Partial multi-label learning and complementary multi-label learning are two popular weakly supervised multi-label classification paradigms that aim to alleviate the high annotation costs of collecting precisely annotated multi-label data. In partial multi-label learning, each instance is annotated with a candidate label set, among which only some labels are relevant; in complementary multi-label learning, each instance is annotated with complementary labels indicating the classes to which the instance does not belong. Existing consistent approaches for the two paradigms either require accurate estimation of the generation process of candidate or complementary labels or assume a uniform distribution to eliminate the estimation problem. However, both conditions are usually difficult to satisfy in real-world scenarios. In this paper, we propose consistent approaches that do not rely on the aforementioned conditions to handle both problems in a unified way. Specifically, we propose two unbiased risk estimators based on first- and second-order strategies. Theoretically, we prove consistency w.r.t. two widely used multi-label classification evaluation metrics and derive convergence rates for the estimation errors of the proposed risk estimators. Empirically, extensive experimental results validate the effectiveness of our proposed approaches against state-of-the-art methods.</li>
<li><strong>摘要：</strong>部分多标签学习和互补的多标签学习是两个流行的弱监督多标签分类范式，旨在减轻收集精确注释的多标签数据的高注释成本。在部分多标签学习中，每个实例都用候选标签集注释，其中只有一些标签是相关的。在互补的多标签学习中，每个实例都用补充标签注释，以指示该实例不属于的类。现有的两个范式的一致方法要么需要准确估算候选者的生成过程或互补标签，要么假设均匀分布以消除估计问题。但是，在实际情况下，这两种情况通常都难以满足。在本文中，我们提出了一致的方法，这些方法不依赖上述条件以统一的方式处理这两个问题。具体而言，我们根据一阶和二阶策略提出了两个公正的风险估计器。从理论上讲，我们证明了一致性W.R.T.两个广泛使用的多标签分类评估指标，并得出拟议风险估计器的估计误差的收敛率。从经验上讲，广泛的实验结果证明了我们提出的方法针对最新方法的有效性。</li>
</ul>

<h3>Title: Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws</h3>
<ul>
<li><strong>Authors: </strong>Ramzi Dakhmouche, Hossein Gorji</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04102">https://arxiv.org/abs/2510.04102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04102">https://arxiv.org/pdf/2510.04102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04102]] Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws(https://arxiv.org/abs/2510.04102)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Motivated by the remarkable success of Foundation Models (FMs) in language modeling, there has been growing interest in developing FMs for time series prediction, given the transformative power such models hold for science and engineering. This culminated in significant success of FMs in short-range forecasting settings. However, extrapolation or long-range forecasting remains elusive for FMs, which struggle to outperform even simple baselines. This contrasts with physical laws which have strong extrapolation properties, and raises the question of the fundamental difference between the structure of neural networks and physical laws. In this work, we identify and formalize a fundamental property characterizing the ability of statistical learning models to predict more accurately outside of their training domain, hence explaining performance deterioration for deep learning models in extrapolation settings. In addition to a theoretical analysis, we present empirical results showcasing the implications of this property on current deep learning architectures. Our results not only clarify the root causes of the extrapolation gap but also suggest directions for designing next-generation forecasting models capable of mastering extrapolation.</li>
<li><strong>摘要：</strong>由于基础模型（FMS）在语言建模中的显着成功，因此，鉴于这种模型对科学和工程的变革力量，对时间序列预测的FMS开发越来越兴趣。这最终导致了FMS在短期预测环境中的显着成功。但是，对于FMS而言，推断或远程预测仍然难以捉摸，即使是简单的基准，这些FM都难以超越。这与具有强大外推特性的物理定律形成对比，并提出了一个关于神经网络和物理定律结构之间基本差异的问题。在这项工作中，我们确定并形式化了一个基本属性，该属性表征了统计学习模型在其训练领域之外更准确预测的能力，从而解释了外推设置中深度学习模型的性能恶化。除了进行理论分析外，我们提出了经验结果，展示了该特性对当前深度学习体系结构的影响。我们的结果不仅阐明了外推差距的根本原因，而且还提出了设计能够掌握外推的下一代预测模型的方向。</li>
</ul>

<h3>Title: Can Linear Probes Measure LLM Uncertainty?</h3>
<ul>
<li><strong>Authors: </strong>Ramzi Dakhmouche, Adrien Letellier, Hossein Gorji</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04108">https://arxiv.org/abs/2510.04108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04108">https://arxiv.org/pdf/2510.04108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04108]] Can Linear Probes Measure LLM Uncertainty?(https://arxiv.org/abs/2510.04108)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Effective Uncertainty Quantification (UQ) represents a key aspect for reliable deployment of Large Language Models (LLMs) in automated decision-making and beyond. Yet, for LLM generation with multiple choice structure, the state-of-the-art in UQ is still dominated by the naive baseline given by the maximum softmax score. To address this shortcoming, we demonstrate that taking a principled approach via Bayesian statistics leads to improved performance despite leveraging the simplest possible model, namely linear regression. More precisely, we propose to train multiple Bayesian linear models, each predicting the output of a layer given the output of the previous one. Based on the obtained layer-level posterior distributions, we infer the global uncertainty level of the LLM by identifying a sparse combination of distributional features, leading to an efficient UQ scheme. Numerical experiments on various LLMs show consistent improvement over state-of-the-art baselines.</li>
<li><strong>摘要：</strong>有效的不确定性量化（UQ）代表了自动决策及其他方面可靠部署大语言模型（LLM）的关键方面。然而，对于具有多项选择结构的LLM生成，UQ中的最先进仍由最大软磁得分给出的天真基线主导。为了解决这一缺点，我们证明，尽管利用了最简单的模型，即线性回归，但通过贝叶斯统计进行有原则的方法会提高性能。更确切地说，我们建议训练多个贝叶斯线性模型，每种模型都可以预测前一个输出的层的输出。基于获得的层级后验分布，我们通过识别分布特征的稀疏组合来推断LLM的全局不确定性水平，从而导致有效的UQ方案。在各种LLMS上进行的数值实验表现出比最新基线的一致性改进。</li>
</ul>

<h3>Title: Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Seunghyun Lee, Tae-Kyun Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04125">https://arxiv.org/abs/2510.04125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04125">https://arxiv.org/pdf/2510.04125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04125]] Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation(https://arxiv.org/abs/2510.04125)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Latest diffusion models have shown promising results in category-level 6D object pose estimation by modeling the conditional pose distribution with depth image input. The existing methods, however, suffer from slow convergence during training, learning its encoder with the diffusion denoising network in end-to-end fashion, and require an additional network that evaluates sampled pose hypotheses to filter out low-quality pose candidates. In this paper, we propose a novel pipeline that tackles these limitations by two key components. First, the proposed method pretrains the encoder with the direct pose regression head, and jointly learns the networks via the regression head and the denoising diffusion head, significantly accelerating training convergence while achieving higher accuracy. Second, sampling guidance via time-dependent score scaling is proposed s.t. the exploration-exploitation trade-off is effectively taken, eliminating the need for the additional evaluation network. The sampling guidance maintains multi-modal characteristics of symmetric objects at early denoising steps while ensuring high-quality pose generation at final steps. Extensive experiments on multiple benchmarks including REAL275, HouseCat6D, and ROPE, demonstrate that the proposed method, simple yet effective, achieves state-of-the-art accuracies even with single-pose inference, while being more efficient in both training and inference.</li>
<li><strong>摘要：</strong>最新的扩散模型通过用深度图像输入对条件姿势分布进行建模，在类别级别6D对象姿势估计中显示出令人鼓舞的结果。但是，现有方法在训练过程中遇到了缓慢的收敛性，以端到端方式学习其编码器，并需要一个额外的网络来评估采样的姿势假设以滤除低质量的姿势候选者。在本文中，我们提出了一条新型的管道，该管道通过两个关键组成部分来应对这些局限性。首先，提出的方法从直接姿势回归头预处理编码器，并通过回归头和脱氧扩散头共同学习网络，从而显着加速了训练收敛的同时，同时达到了更高的准确性。其次，提出了通过时间依赖性得分缩放的抽样引导。有效地采取了探索 - 开发权的权衡，从而消除了对额外评估网络的需求。该采样指南在早期的剥离步骤中保持对称对象的多模式特征，同时确保最终步骤的高质量姿势产生。对包括Real275，HouseCat6d和Rope在内的多个基准测试的广泛实验表明，即使是单个货物推断，提出的简单而有效的方法也可以实现最先进的精度，同时在训练和推理方面都更有效。</li>
</ul>

<h3>Title: Automating construction safety inspections using a multi-modal vision-language RAG framework</h3>
<ul>
<li><strong>Authors: </strong>Chenxin Wang, Elyas Asadi Shamsabadi, Zhaohui Chen, Luming Shen, Alireza Ahmadian Fard Fini, Daniel Dias-da-Costa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04145">https://arxiv.org/abs/2510.04145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04145">https://arxiv.org/pdf/2510.04145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04145]] Automating construction safety inspections using a multi-modal vision-language RAG framework(https://arxiv.org/abs/2510.04145)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Conventional construction safety inspection methods are often inefficient as they require navigating through large volume of information. Recent advances in large vision-language models (LVLMs) provide opportunities to automate safety inspections through enhanced visual and linguistic understanding. However, existing applications face limitations including irrelevant or unspecific responses, restricted modal inputs and hallucinations. Utilisation of Large Language Models (LLMs) for this purpose is constrained by availability of training data and frequently lack real-time adaptability. This study introduces SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG) framework for automating construction safety inspection reports by integrating visual and audio inputs. Using real-world data, SiteShield outperformed unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04, precision of 0.76, and recall of 0.96. The findings indicate that SiteShield offers a novel pathway to enhance information retrieval and efficiency in generating safety reports.</li>
<li><strong>摘要：</strong>常规的施工安全检查方法通常效率低下，因为它们需要通过大量信息导航。大型视觉模型（LVLM）的最新进展为通过增强的视觉和语言理解提供了自动化安全检查的机会。但是，现有的应用程序面临的局限性包括无关紧要的或非特定的响应，限制的模态输入和幻觉。用于此目的的大型语言模型（LLM）的利用受培训数据的可用性限制，并且经常缺乏实时适应性。这项研究介绍了SiteShield，这是一种基于多模式LVLM的检索效果生成（RAG）框架，用于通过集成视觉和音频输入来自动化施工安全检查报告。使用现实世界数据，SiteShield的表现优于单峰LLM，而没有rag，而F1得分为0.82，锤损损失为0.04，精度为0.76，回忆为0.96。研究结果表明，SiteShield提供了一种新的途径，以增强信息检索和效率生成安全报告。</li>
</ul>

<h3>Title: BLADE: Bias-Linked Adaptive DEbiasing</h3>
<ul>
<li><strong>Authors: </strong>Piyush Arora, Navlika Singh, Vasubhya Diwan, Pratik Mazumder</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04174">https://arxiv.org/abs/2510.04174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04174">https://arxiv.org/pdf/2510.04174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04174]] BLADE: Bias-Linked Adaptive DEbiasing(https://arxiv.org/abs/2510.04174)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Neural networks have revolutionized numerous fields, yet they remain vulnerable to a critical flaw: the tendency to learn implicit biases, spurious correlations between certain attributes and target labels in training data. These biases are often more prevalent and easier to learn, causing models to rely on superficial patterns rather than task-relevant features necessary for generalization. Existing methods typically rely on strong assumptions, such as prior knowledge of these biases or access to bias-conflicting samples, i.e., samples that contradict spurious correlations and counterbalance bias-aligned samples, samples that conform to these spurious correlations. However, such assumptions are often impractical in real-world settings. We propose BLADE ({B}ias-{L}inked {A}daptive {DE}biasing), a generative debiasing framework that requires no prior knowledge of bias or bias-conflicting samples. BLADE first trains a generative model to translate images across bias domains while preserving task-relevant features. Then, it adaptively refines each image with its synthetic counterpart based on the image's susceptibility to bias. To encourage robust representations, BLADE aligns an image with its bias-translated synthetic counterpart that shares task-relevant features but differs in bias, while misaligning it with samples sharing the same bias. We evaluate BLADE on multiple benchmark datasets and show that it significantly outperforms state-of-the-art methods. Notably, it exceeds the closest baseline by an absolute margin of around 18% on the corrupted CIFAR-10 dataset under the worst group setting, establishing a new benchmark in bias mitigation and demonstrating its potential for developing more robust deep learning models without explicit supervision.</li>
<li><strong>摘要：</strong>神经网络已经彻底改变了许多领域，但它们仍然容易受到关键缺陷的影响：学习隐性偏见，某些属性和培训数据中的目标标签之间的虚假相关性的趋势。这些偏见通常更为普遍，更易于学习，导致模型依赖于表面模式，而不是与任务相关的特征。现有方法通常依赖于强烈的假设，例如对这些偏见的先验知识或访问偏见的样本，即与伪造相关性和平衡偏见的样本相矛盾的样本，符合这些虚假相关性的样本。但是，在现实世界中，这种假设通常是不切实际的。我们提出了刀片（{b} ias- {l}墨水{a} daptive {de}偏见），这是一个生成的偏见框架，不需要先验了解偏见或偏见冲突的样本。 Blade首先训练生成模型，以在保留与任务相关的功能的同时，将图像转换为跨偏置域。然后，它根据图像对偏差的敏感性而适应其合成对应物的每个图像。为了鼓励强大的表示形式，Blade将图像与其偏置翻译的合成对应物保持一致，该合成对应物具有与任务相关的功能，但偏见的不同之处，同时将其与共享相同偏见的样本错误对齐。我们评估了多个基准数据集上的刀片，并表明它的表现明显优于最先进的方法。值得注意的是，在最差的组设置下，它超过了损坏的CIFAR-10数据集的最接近基线约18％，建立了缓解偏见的新基准，并证明了其在没有明确监督的情况下开发更强大的深度学习模型的潜力。</li>
</ul>

<h3>Title: World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Moo Hyun Son, Jintaek Oh, Sun Bin Mun, Jaechul Roh, Sehyun Choi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04201">https://arxiv.org/abs/2510.04201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04201">https://arxiv.org/pdf/2510.04201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04201]] World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge(https://arxiv.org/abs/2510.04201)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>While text-to-image (T2I) models can synthesize high-quality images, their performance degrades significantly when prompted with novel or out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We introduce World-To-Image, a novel framework that bridges this gap by empowering T2I generation with agent-driven world knowledge. We design an agent that dynamically searches the web to retrieve images for concepts unknown to the base model. This information is then used to perform multimodal prompt optimization, steering powerful generative backbones toward an accurate synthesis. Critically, our evaluation goes beyond traditional metrics, utilizing modern assessments like LLMGrader and ImageReward to measure true semantic fidelity. Our experiments show that World-To-Image substantially outperforms state-of-the-art methods in both semantic alignment and visual aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated NICE benchmark. Our framework achieves these results with high efficiency in less than three iterations, paving the way for T2I systems that can better reflect the ever-changing real world. Our demo code is available here\footnote{this https URL}.</li>
<li><strong>摘要：</strong>虽然文本对图像（T2I）模型可以合成高质量的图像，但由于固有的知识截止，在使用新颖或分布式（OOD）实体提示时，其性能会大大降低。我们介绍了世界到图像，这是一个新颖的框架，通过赋予T2I代理的世界知识来弥补这一差距。我们设计了一个动态搜索Web的代理，以检索基本模型未知的概念的图像。然后，该信息用于执行多模式提示优化，将强大的生成骨架转向准确的合成。至关重要的是，我们的评估超越了传统指标，利用诸如llmgrader和ImageRARD之类的现代评估来衡量真正的语义忠诚。我们的实验表明，世界到图像在语义一致性和视觉美学方面都大大优于最先进的方法，在我们精心策划的良好基准测试中，准确性至预测提高了8.1％。我们的框架在不到三个迭代中以高效率实现了这些结果，为T2I系统铺平了道路，这些系统可以更好地反映不断变化的现实世界。我们的演示代码可在此处提供\ footNote {this HTTPS URL}。</li>
</ul>

<h3>Title: MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering</h3>
<ul>
<li><strong>Authors: </strong>Lixuan He, Shikang Zheng, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04220">https://arxiv.org/abs/2510.04220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04220">https://arxiv.org/pdf/2510.04220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04220]] MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering(https://arxiv.org/abs/2510.04220)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Autoregressive (AR) models have shown great promise in image generation, yet they face a fundamental inefficiency stemming from their core component: a vast, unstructured vocabulary of visual tokens. This conventional approach treats tokens as a flat vocabulary, disregarding the intrinsic structure of the token embedding space where proximity often correlates with semantic similarity. This oversight results in a highly complex prediction task, which hinders training efficiency and limits final generation quality. To resolve this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled framework that constructs a hierarchical semantic tree directly from the codebook's intrinsic structure. MASC employs a novel geometry-aware distance metric and a density-driven agglomerative construction to model the underlying manifold of the token embeddings. By transforming the flat, high-dimensional prediction task into a structured, hierarchical one, MASC introduces a beneficial inductive bias that significantly simplifies the learning problem for the AR model. MASC is designed as a plug-and-play module, and our extensive experiments validate its effectiveness: it accelerates training by up to 57% and significantly improves generation quality, reducing the FID of LlamaGen-XL from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly competitive with state-of-the-art methods, establishing that structuring the prediction space is as crucial as architectural innovation for scalable generative modeling.</li>
<li><strong>摘要：</strong>自动回归（AR）模型在图像产生方面表现出了巨大的希望，但是它们面临着源于其核心组成部分的基本效率低下的效率：视觉令牌的庞大，非结构化的词汇。这种常规的方法将令牌视为平坦的词汇，无视令牌嵌入空间的内在结构，在该空间中，近端通常与语义相似性相关。这种监督导致了一项高度复杂的预测任务，这会阻碍培训效率并限制了最终一代质量。为了解决这个问题，我们提出了与歧管一致的语义聚类（MASC），这是一个原则上的框架，该框架直接从代码书的内在结构中构造了层次结构的语义树。 MASC采用一种新颖的几何感知距离度量和密度驱动的团聚结构来对令牌嵌入的基础歧管进行建模。通过将平坦的高维预测任务转换为结构化的层次结构，MASC引入了有益的感应偏见，可以显着简化AR模型的学习问题。 MASC被设计为插件模块，我们的广泛实验验证了其有效性：它可以将训练提高高达57％，并显着提高了发电质量，从而将Llamagen-XL的FID从2.87降低到2.58。 MASC将现有的AR框架提升为具有最先进方法的高度竞争力，并确定构造预测空间与可扩展生成建模的建筑创新至关重要。</li>
</ul>

<h3>Title: Scaling Sequence-to-Sequence Generative Neural Rendering</h3>
<ul>
<li><strong>Authors: </strong>Shikun Liu, Kam Woh Ng, Wonbong Jang, Jiadong Guo, Junlin Han, Haozhe Liu, Yiannis Douratsos, Juan C. Pérez, Zijian Zhou, Chi Phung, Tao Xiang, Juan-Manuel Pérez-Rúa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04236">https://arxiv.org/abs/2510.04236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04236">https://arxiv.org/pdf/2510.04236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04236]] Scaling Sequence-to-Sequence Generative Neural Rendering(https://arxiv.org/abs/2510.04236)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present Kaleido, a family of generative models designed for photorealistic, unified object- and scene-level neural rendering. Kaleido operates on the principle that 3D can be regarded as a specialised sub-domain of video, expressed purely as a sequence-to-sequence image synthesis task. Through a systemic study of scaling sequence-to-sequence generative neural rendering, we introduce key architectural innovations that enable our model to: i) perform generative view synthesis without explicit 3D representations; ii) generate any number of 6-DoF target views conditioned on any number of reference views via a masked autoregressive framework; and iii) seamlessly unify 3D and video modelling within a single decoder-only rectified flow transformer. Within this unified framework, Kaleido leverages large-scale video data for pre-training, which significantly improves spatial consistency and reduces reliance on scarce, camera-labelled 3D datasets -- all without any architectural modifications. Kaleido sets a new state-of-the-art on a range of view synthesis benchmarks. Its zero-shot performance substantially outperforms other generative methods in few-view settings, and, for the first time, matches the quality of per-scene optimisation methods in many-view settings.</li>
<li><strong>摘要：</strong>我们推出了 Kaleido，这是一个为照片级真实感、统一的对象和场景级神经渲染而设计的生成模型系列。 Kaleido 的运行原理是，3D 可以被视为视频的专门子域，纯粹表示为序列到序列的图像合成任务。通过对缩放序列到序列生成神经渲染的系统研究，我们引入了关键的架构创新，使我们的模型能够：i）在没有显式 3D 表示的情况下执行生成视图合成； ii) 通过屏蔽自回归框架生成任意数量的 6-DoF 目标视图，以任意数量的参考视图为条件； iii) 在单个仅解码器整流流量变压器中无缝统一 3D 和视频建模。在这个统一的框架内，Kaleido 利用大规模视频数据进行预训练，这显着提高了空间一致性并减少了对稀缺的、带有相机标签的 3D 数据集的依赖 - 所有这些都无需进行任何架构修改。 Kaleido 在一系列视图合成基准上树立了新的最先进水平。其零样本性能在少视图设置中大大优于其他生成方法，并且首次与多视图设置中每场景优化方法的质量相匹配。</li>
</ul>

<h3>Title: ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation</h3>
<ul>
<li><strong>Authors: </strong>Jay Zhangjie Wu, Xuanchi Ren, Tianchang Shen, Tianshi Cao, Kai He, Yifan Lu, Ruiyuan Gao, Enze Xie, Shiyi Lan, Jose M. Alvarez, Jun Gao, Sanja Fidler, Zian Wang, Huan Ling</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04290">https://arxiv.org/abs/2510.04290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04290">https://arxiv.org/pdf/2510.04290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04290]] ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation(https://arxiv.org/abs/2510.04290)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in large generative models have significantly advanced image editing and in-context image generation, yet a critical gap remains in ensuring physical consistency, where edited objects must remain coherent. This capability is especially vital for world simulation related tasks. In this paper, we present ChronoEdit, a framework that reframes image editing as a video generation problem. First, ChronoEdit treats the input and edited images as the first and last frames of a video, allowing it to leverage large pretrained video generative models that capture not only object appearance but also the implicit physics of motion and interaction through learned temporal consistency. Second, ChronoEdit introduces a temporal reasoning stage that explicitly performs editing at inference time. Under this setting, the target frame is jointly denoised with reasoning tokens to imagine a plausible editing trajectory that constrains the solution space to physically viable transformations. The reasoning tokens are then dropped after a few steps to avoid the high computational cost of rendering a full video. To validate ChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for contexts that require physical consistency, and demonstrate that ChronoEdit surpasses state-of-the-art baselines in both visual fidelity and physical plausibility. Code and models for both the 14B and 2B variants of ChronoEdit will be released on the project page: this https URL</li>
<li><strong>摘要：</strong>大型生成模型的最新进展具有显着高级的图像编辑和内在图像的生成，但是在确保物理一致性方面仍然存在一个关键的差距，其中编辑的对象必须保持一致。这种功能对于与世界模拟相关的任务尤其重要。在本文中，我们介绍了Chronoedit，该框架将图像编辑重新编辑为视频生成问题。首先，Chronoedit将输入和编辑的图像视为视频的第一个和最后一个框架，使其能够利用大型预审预周化的视频生成模型，这些模型不仅可以捕获对象的外观，还可以通过学习的时间一致性来捕获运动和交互的隐式物理。其次，Chronoedit引入了一个时间推理阶段，该阶段在推理时间明确执行编辑。在此设置下，目标框架与推理令牌共同剥夺，以想象一个合理的编辑轨迹，该轨迹将解决方案空间限制为物理可行的变换。然后在几个步骤后删除推理令牌，以避免呈现完整视频的高计算成本。为了验证Chronoedit，我们介绍了PBENCH-EDIT，这是需要物理一致性的上下文的图像推出对的新基准，并证明Chronoedit在视觉保真度和物理合理性中都超过了最先进的基线。 ChronoIt的14B和2B变体的代码和模型将在项目页面上发布：此HTTPS URL</li>
</ul>

<h3>Title: GenAR: Next-Scale Autoregressive Generation for Spatial Gene Expression Prediction</h3>
<ul>
<li><strong>Authors: </strong>Jiarui Ouyang, Yihui Wang, Yihang Gao, Yingxue Xu, Shu Yang, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04315">https://arxiv.org/abs/2510.04315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04315">https://arxiv.org/pdf/2510.04315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04315]] GenAR: Next-Scale Autoregressive Generation for Spatial Gene Expression Prediction(https://arxiv.org/abs/2510.04315)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Spatial Transcriptomics (ST) offers spatially resolved gene expression but remains costly. Predicting expression directly from widely available Hematoxylin and Eosin (H&E) stained images presents a cost-effective alternative. However, most computational approaches (i) predict each gene independently, overlooking co-expression structure, and (ii) cast the task as continuous regression despite expression being discrete counts. This mismatch can yield biologically implausible outputs and complicate downstream analyses. We introduce GenAR, a multi-scale autoregressive framework that refines predictions from coarse to fine. GenAR clusters genes into hierarchical groups to expose cross-gene dependencies, models expression as codebook-free discrete token generation to directly predict raw counts, and conditions decoding on fused histological and spatial embeddings. From an information-theoretic perspective, the discrete formulation avoids log-induced biases and the coarse-to-fine factorization aligns with a principled conditional decomposition. Extensive experimental results on four Spatial Transcriptomics datasets across different tissue types demonstrate that GenAR achieves state-of-the-art performance, offering potential implications for precision medicine and cost-effective molecular profiling. Code is publicly available at this https URL.</li>
<li><strong>摘要：</strong>空间转录组学（ST）提供了空间分辨的基因表达，但仍然昂贵。直接从广泛可用的苏木精和曙红（H＆E）染色图像中预测表达，这是一种具有成本效益的替代方法。但是，大多数计算方法（i）独立预测了每个基因，忽略共表达结构，（ii）尽管表达为离散计数，但将任务视为连续回归。这种不匹配可以产生生物学上令人难以置信的输出，并使下游分析复杂化。我们介绍了Genar，这是一种多尺度自动回归框架，可完善从粗到细的预测。 Genar将基因群群中群体组合到跨基因依赖性，模型作为无代码的离散代币产生，以直接预测原始计数，并在融合的组织学和空间嵌入中解码的条件。从信息理论的角度来看，离散的公式避免了对数诱导的偏见，而粗到最细分的分解与有原则的条件分解一致。对不同组织类型的四个空间转录组数据集的广泛实验结果表明，Genar实现了最新的性能，从而对精确药物和具有成本效益的分子分析具有潜在的影响。代码在此HTTPS URL上公开可用。</li>
</ul>

<h3>Title: DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks</h3>
<ul>
<li><strong>Authors: </strong>Nghiem T. Diep, Hien Dang, Tuan Truong, Tan Dinh, Huy Nguyen, Nhat Ho</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04331">https://arxiv.org/abs/2510.04331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04331">https://arxiv.org/pdf/2510.04331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04331]] DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks(https://arxiv.org/abs/2510.04331)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Parameter-efficient fine-tuning (PEFT) methods have become the standard paradigm for adapting large-scale models. Among these techniques, Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the learning capacity and training stability of the vanilla Low-Rank Adaptation (LoRA) method by explicitly decomposing pre-trained weights into magnitude and directional components. In this work, we propose DoRAN, a new variant of DoRA designed to further stabilize training and boost the sample efficiency of DoRA. Our approach includes two key stages: (i) injecting noise into the denominator of DoRA's weight decomposition, which serves as an adaptive regularizer to mitigate instabilities; and (ii) replacing static low-rank matrices with auxiliary networks that generate them dynamically, enabling parameter coupling across layers and yielding better sample efficiency in both theory and practice. Comprehensive experiments on vision and language benchmarks show that DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These results underscore the effectiveness of combining stabilization through noise-based regularization with network-based parameter generation, offering a promising direction for robust and efficient fine-tuning of foundation models.</li>
<li><strong>摘要：</strong>参数有效的微调（PEFT）方法已成为适应大型模型的标准范式。在这些技术中，已经证明，重量分解的低排名适应性（DORA）可以通过将预训练的重量显式分解为大小和方向成分，从而提高了香草低级适应方法（LORA）方法的学习能力和训练稳定性。在这项工作中，我们提出了Doran，这是一种新的Dora变体，旨在进一步稳定训练并提高Dora的样品效率。我们的方法包括两个关键阶段：（i）将噪声注入Dora重量分解的分母，该分解是可减轻不稳定性的自适应正规化器； （ii）用辅助网络替换静态低级别矩阵，该辅助网络动态生成它们，从而使参数偶联跨层耦合，并在理论和实践中产生更好的样品效率。关于视觉和语言基准的全面实验表明，多兰一贯的表现要优于洛拉，多拉和其他PEFT基线。这些结果强调了通过基于噪声的正则化与基于网络的参数生成结合稳定的有效性，为基础模型的强大和有效微调提供了有希望的方向。</li>
</ul>

<h3>Title: Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies</h3>
<ul>
<li><strong>Authors: </strong>G. Niklas Noren, Eva-Lisa Meldau, Johan Ellenius</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04341">https://arxiv.org/abs/2510.04341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04341">https://arxiv.org/pdf/2510.04341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04341]] Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies(https://arxiv.org/abs/2510.04341)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Many high-stakes AI applications target low-prevalence events, where apparent accuracy can conceal limited real-world value. Relevant AI models range from expert-defined rules and traditional machine learning to generative LLMs constrained for classification. We outline key considerations for critical appraisal of AI in rare-event recognition, including problem framing and test set design, prevalence-aware statistical evaluation, robustness assessment, and integration into human workflows. In addition, we propose an approach to structured case-level examination (SCLE), to complement statistical performance evaluation, and a comprehensive checklist to guide procurement or development of AI models for rare-event recognition. We instantiate the framework in pharmacovigilance, drawing on three studies: rule-based retrieval of pregnancy-related reports; duplicate detection combining machine learning with probabilistic record linkage; and automated redaction of person names using an LLM. We highlight pitfalls specific to the rare-event setting including optimism from unrealistic class balance and lack of difficult positive controls in test sets - and show how cost-sensitive targets align model performance with operational value. While grounded in pharmacovigilance practice, the principles generalize to domains where positives are scarce and error costs may be asymmetric.</li>
<li><strong>摘要：</strong>许多高风险的AI应用程序针对的是低贫困事件，明显的准确性可以掩盖有限的现实价值。相关的AI模型范围从专家定义的规则和传统的机器学习到限制分类的生成LLM。我们概述了在罕见的识别中对AI进行批判性评估的主要考虑因素，包括问题框架和测试集设计，意识到患病意识的统计评估，鲁棒性评估以及整合到人类工作流程中。此外，我们提出了一种结构化病例级检查（SCLE）的方法，以补充统计绩效评估以及一份全面的清单，以指导或开发AI模型的采购或开发，以实现罕见的事实识别。我们实例化了药物保护框架的框架，借鉴了三项研究：基于规则的与妊娠相关报告的检索；重复检测将机器学习与概率记录链接结合在一起；并使用LLM自动重新修订人名称。我们重点介绍了罕见事件设置的特定陷阱，包括对不切实际的阶级平衡的乐观情绪以及在测试集中缺乏困难的积极对照 - 并显示成本敏感的目标如何使模型性能与操作价值相结合。虽然基于药物保护实践，但原则将其推广到差异很少，错误成本可能不对称。</li>
</ul>

<h3>Title: Quantifying Ambiguity in Categorical Annotations: A Measure and Statistical Inference Framework</h3>
<ul>
<li><strong>Authors: </strong>Christopher Klugmann, Daniel Kondermann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04366">https://arxiv.org/abs/2510.04366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04366">https://arxiv.org/pdf/2510.04366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04366]] Quantifying Ambiguity in Categorical Annotations: A Measure and Statistical Inference Framework(https://arxiv.org/abs/2510.04366)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Human-generated categorical annotations frequently produce empirical response distributions (soft labels) that reflect ambiguity rather than simple annotator error. We introduce an ambiguity measure that maps a discrete response distribution to a scalar in the unit interval, designed to quantify aleatoric uncertainty in categorical tasks. The measure bears a close relationship to quadratic entropy (Gini-style impurity) but departs from those indices by treating an explicit "can't solve" category asymmetrically, thereby separating uncertainty arising from class-level indistinguishability from uncertainty due to explicit unresolvability. We analyze the measure's formal properties and contrast its behavior with a representative ambiguity measure from the literature. Moving beyond description, we develop statistical tools for inference: we propose frequentist point estimators for population ambiguity and derive the Bayesian posterior over ambiguity induced by Dirichlet priors on the underlying probability vector, providing a principled account of epistemic uncertainty. Numerical examples illustrate estimation, calibration, and practical use for dataset-quality assessment and downstream machine-learning workflows.</li>
<li><strong>摘要：</strong>人类生成的分类注释经常产生经验响应分布（软标签），反映出歧义而不是简单的注释误差。我们介绍了一个模棱两可的度量，该度量将离散响应分布映射到单位间隔中的标量，旨在量化分类任务中的不确定性。该措施与二次熵（Gini式杂质）有着密切的关系，但是通过不对称地处理明确的“无法解决”类别，从而偏离了这些指标，从而将不确定性与由于明显的不受欢迎所致的不确定性产生的不确定性而分离出不确定性。我们分析了该度量的形式特性，并将其行为与文献的代表性歧义度量进行了对比。除了描述之外，我们开发了用于推断的统计工具：我们提出了频繁的点估计量，以使人口模棱两可，并在基本概率向量对Dirichlet先验引起的歧义性歧义中得出了贝叶斯后部，从而提供了认识论不确定性的原则上的说法。数值示例说明了数据集质量评估和下游机器学习工作流程的估计，校准和实际用途。</li>
</ul>

<h3>Title: MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator</h3>
<ul>
<li><strong>Authors: </strong>Xuehai He, Shijie Zhou, Thivyanth Venkateswaran, Kaizhi Zheng, Ziyu Wan, Achuta Kadambi, Xin Eric Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04390">https://arxiv.org/abs/2510.04390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04390">https://arxiv.org/pdf/2510.04390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04390]] MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator(https://arxiv.org/abs/2510.04390)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>World models that support controllable and editable spatiotemporal environments are valuable for robotics, enabling scalable training data, repro ducible evaluation, and flexible task design. While recent text-to-video models generate realistic dynam ics, they are constrained to 2D views and offer limited interaction. We introduce MorphoSim, a language guided framework that generates 4D scenes with multi-view consistency and object-level controls. From natural language instructions, MorphoSim produces dynamic environments where objects can be directed, recolored, or removed, and scenes can be observed from arbitrary viewpoints. The framework integrates trajectory-guided generation with feature field dis tillation, allowing edits to be applied interactively without full re-generation. Experiments show that Mor phoSim maintains high scene fidelity while enabling controllability and editability. The code is available at this https URL.</li>
<li><strong>摘要：</strong>支持可控且可编辑的时空环境的世界模型对于机器人技术很有价值，启用可扩展培训数据，可重复可辨认的评估和灵活的任务设计。尽管最近的文本对视频模型产生了现实的动态IC，但它们被限制为2D视图并提供有限的交互作用。我们介绍了MorphoSim，这是一个语言指导的框架，该框架生成具有多视图一致性和对象级控件的4D场景。从自然语言指示中，morphosim产生动态环境，可以在其中指导，重新上色或删除对象，并且可以从任意观点观察到场景。该框架将轨迹引导的生成与特征田间耕作集成在一起，从而使编辑可以在不完全再生的情况下进行交互应用。实验表明，MOR Phosim在实现可控性和编辑性的同时保持了高场景的保真度。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: CodeFormer++: Blind Face Restoration Using Deformable Registration and Deep Metric Learning</h3>
<ul>
<li><strong>Authors: </strong>Venkata Bharath Reddy Reddem, Akshay P Sarashetti, Ranjith Merugu, Amit Satish Unde</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04410">https://arxiv.org/abs/2510.04410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04410">https://arxiv.org/pdf/2510.04410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04410]] CodeFormer++: Blind Face Restoration Using Deformable Registration and Deep Metric Learning(https://arxiv.org/abs/2510.04410)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation, generative</a></li>
<li><strong>Abstract: </strong>Blind face restoration (BFR) has attracted increasing attention with the rise of generative methods. Most existing approaches integrate generative priors into the restoration pro- cess, aiming to jointly address facial detail generation and identity preservation. However, these methods often suffer from a trade-off between visual quality and identity fidelity, leading to either identity distortion or suboptimal degradation removal. In this paper, we present CodeFormer++, a novel framework that maximizes the utility of generative priors for high-quality face restoration while preserving identity. We decompose BFR into three sub-tasks: (i) identity- preserving face restoration, (ii) high-quality face generation, and (iii) dynamic fusion of identity features with realistic texture details. Our method makes three key contributions: (1) a learning-based deformable face registration module that semantically aligns generated and restored faces; (2) a texture guided restoration network to dynamically extract and transfer the texture of generated face to boost the quality of identity-preserving restored face; and (3) the integration of deep metric learning for BFR with the generation of informative positive and hard negative samples to better fuse identity- preserving and generative features. Extensive experiments on real-world and synthetic datasets demonstrate that, the pro- posed CodeFormer++ achieves superior performance in terms of both visual fidelity and identity consistency.</li>
<li><strong>摘要：</strong>随着生成方法的兴起，盲面修复（BFR）引起了人们越来越多的关注。大多数现有方法都将生成先验纳入恢复过程，旨在共同解决面部细节的生成和身份保护。但是，这些方法通常会在视觉质量和身份保真度之间进行权衡，从而导致身份失真或次优降解的去除。在本文中，我们提出了CodeFormer ++，这是一个新颖的框架，可最大程度地提高生成先验的效用，同时保持身份。我们将BFR分解为三个子任务：（i）身份 - 保持面部修复，（ii）高质量的面部生成，以及（iii）具有逼真的纹理细节的身份特征的动态融合。我们的方法做出了三个关键的贡献：（1）基于学习的可变形面式注册模块，该模块将语义上的生成和修复的面对面对齐； （2）质地引导的恢复网络，以动态提取和转移生成的面部的纹理，以提高具有身份的恢复面部的质量； （3）将BFR的深度度量学习与提供信息丰富的正面和硬性负面样本的产生，以更好地融合身份 - 保留和生成特征。关于现实世界和合成数据集的广泛实验表明，根据视觉效率和身份一致性，构成的代码形式++在较高的性能上都达到了卓越的性能。</li>
</ul>

<h3>Title: REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization</h3>
<ul>
<li><strong>Authors: </strong>Qiyuan He, Yicong Li, Haotian Ye, Jinghao Wang, Xinyao Liao, Pheng-Ann Heng, Stefano Ermon, James Zou, Angela Yao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04450">https://arxiv.org/abs/2510.04450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04450">https://arxiv.org/pdf/2510.04450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04450]] REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization(https://arxiv.org/abs/2510.04450)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Visual autoregressive (AR) generation offers a promising path toward unifying vision and language models, yet its performance remains suboptimal against diffusion models. Prior work often attributes this gap to tokenizer limitations and rasterization ordering. In this work, we identify a core bottleneck from the perspective of generator-tokenizer inconsistency, i.e., the AR-generated tokens may not be well-decoded by the tokenizer. To address this, we propose reAR, a simple training strategy introducing a token-wise regularization objective: when predicting the next token, the causal transformer is also trained to recover the visual embedding of the current token and predict the embedding of the target token under a noisy context. It requires no changes to the tokenizer, generation order, inference pipeline, or external models. Despite its simplicity, reAR substantially improves performance. On ImageNet, it reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard rasterization-based tokenizer. When applied to advanced tokenizers, it achieves a gFID of 1.42 with only 177M parameters, matching the performance with larger state-of-the-art diffusion models (675M).</li>
<li><strong>摘要：</strong>视觉自我回归（AR）的一代为统一视觉和语言模型提供了有希望的途径，但其性能仍然在扩散模型上占优势。先前的工作通常将这一差距归因于令牌限制和栅格化排序。在这项工作中，我们从发电机 -  tokenizer不一致的角度识别出核心瓶颈，即，AR生成的代币可能不会被令牌剂固定。为了解决这个问题，我们提出了后方，这是一种简单的训练策略，引入了令牌正规化目标：预测下一个令牌时，还对因果变压器进行了训练，以恢复当前令牌的视觉嵌入并预测目标令牌在嘈杂的环境下的嵌入。它不需要更改令牌，生成顺序，推理管道或外部模型。尽管它很简单，但后方仍大大提高了性能。在ImageNet上，使用基于标准的栅格化令牌将GFID从3.02降低到1.86，并提高到316.9。当应用于高级引导者时，它仅使用1.47亿参数实现1.42的GFID，与较大的最新扩散模型（675m）匹配性能。</li>
</ul>

<h3>Title: TBStar-Edit: From Image Editing Pattern Shifting to Consistency Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Hao Fang, Zechao Zhan, Weixin Feng, Ziwei Huang, XuBin Li, Tiezheng Ge</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04483">https://arxiv.org/abs/2510.04483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04483">https://arxiv.org/pdf/2510.04483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04483]] TBStar-Edit: From Image Editing Pattern Shifting to Consistency Enhancement(https://arxiv.org/abs/2510.04483)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent advances in image generation and editing technologies have enabled state-of-the-art models to achieve impressive results in general domains. However, when applied to e-commerce scenarios, these general models often encounter consistency limitations. To address this challenge, we introduce TBStar-Edit, an new image editing model tailored for the e-commerce domain. Through rigorous data engineering, model architecture design and training strategy, TBStar-Edit achieves precise and high-fidelity image editing while maintaining the integrity of product appearance and layout. Specifically, for data engineering, we establish a comprehensive data construction pipeline, encompassing data collection, construction, filtering, and augmentation, to acquire high-quality, instruction-following, and strongly consistent editing data to support model training. For model architecture design, we design a hierarchical model framework consisting of a base model, pattern shifting modules, and consistency enhancement modules. For model training, we adopt a two-stage training strategy to enhance the consistency preservation: first stage for editing pattern shifting, and second stage for consistency enhancement. Each stage involves training different modules with separate datasets. Finally, we conduct extensive evaluations of TBStar-Edit on a self-proposed e-commerce benchmark, and the results demonstrate that TBStar-Edit outperforms existing general-domain editing models in both objective metrics (VIE Score) and subjective user preference.</li>
<li><strong>摘要：</strong>图像生成和编辑技术的最新进展使最先进的模型能够在一般领域中取得令人印象深刻的结果。但是，当应用于电子商务方案时，这些通用模型通常会遇到一致性限制。为了应对这一挑战，我们介绍了TBSTAR-EDIT，这是一种针对电子商务领域量身定制的新图像编辑模型。通过严格的数据工程，模型架构设计和培训策略，TBSTAR-EDIT可以在保持产品外观和布局的完整性时实现精确且高保真的图像编辑。具体来说，对于数据工程，我们建立了一条全面的数据构建管道，包括数据收集，构建，过滤和增强，以获取高质量，指导遵循的高质量，并强烈一致的编辑数据以支持模型培训。对于模型体系结构设计，我们设计了一个分层模型框架，该模型框架由基本模型，模式变化模块和一致性增强模块组成。对于模型培训，我们采用了两阶段的训练策略来增强一致性保护：编辑模式变化的第一阶段，以及增强一致性的第二阶段。每个阶段都涉及使用单独的数据集培训不同的模块。最后，我们对TBSTAR-EDIT进行了广泛的评估，对自称的电子商务基准进行了广泛的评估，结果表明，TBSTAR-EDIT在客观指标（VIE分数）和主观用户偏好中都优于现有的通用域编辑模型。</li>
</ul>

<h3>Title: Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zijing Hu, Yunze Tong, Fengda Zhang, Junkun Yuan, Jun Xiao, Kun Kuang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04504">https://arxiv.org/abs/2510.04504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04504">https://arxiv.org/pdf/2510.04504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04504]] Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation(https://arxiv.org/abs/2510.04504)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved impressive results in generating high-quality images. Yet, they often struggle to faithfully align the generated images with the input prompts. This limitation arises from synchronous denoising, where all pixels simultaneously evolve from random noise to clear images. As a result, during generation, the prompt-related regions can only reference the unrelated regions at the same noise level, failing to obtain clear context and ultimately impairing text-to-image alignment. To address this issue, we propose asynchronous diffusion models -- a novel framework that allocates distinct timesteps to different pixels and reformulates the pixel-wise denoising process. By dynamically modulating the timestep schedules of individual pixels, prompt-related regions are denoised more gradually than unrelated regions, thereby allowing them to leverage clearer inter-pixel context. Consequently, these prompt-related regions achieve better alignment in the final images. Extensive experiments demonstrate that our asynchronous diffusion models can significantly improve text-to-image alignment across diverse prompts. The code repository for this work is available at this https URL.</li>
<li><strong>摘要：</strong>扩散模型在产生高质量图像方面取得了令人印象深刻的结果。然而，他们经常努力将生成的图像与输入提示保持一致。这种限制是由同步的denoising产生的，其中所有像素同时从随机噪声到清除图像。结果，在生成期间，与迅速相关的区域只能在相同的噪声级别引用无关区域，未能获得清晰的上下文，最终会损害文本对图像的一致性。为了解决这个问题，我们提出了异步扩散模型 - 一个新颖的框架，将不同的时间段分配给不同的像素并重新制定了像素的脱氧过程。通过动态调节单个像素的时间步度计划，与无关区域相比，与迅速相关的区域的逐渐逐渐变形，从而使它们能够利用更清晰的像素间上下文。因此，这些迅速相关的区域在最终图像中实现了更好的对齐。广泛的实验表明，我们的异步扩散模型可以显着改善各种提示之间的文本对象。此工作的代码存储库可在此HTTPS URL上获得。</li>
</ul>

<h3>Title: Real-time Prediction of Urban Sound Propagation with Conditioned Normalizing Flows</h3>
<ul>
<li><strong>Authors: </strong>Achim Eckerle, Martin Spitznagel, Janis Keuper</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04510">https://arxiv.org/abs/2510.04510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04510">https://arxiv.org/pdf/2510.04510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04510]] Real-time Prediction of Urban Sound Propagation with Conditioned Normalizing Flows(https://arxiv.org/abs/2510.04510)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accurate and fast urban noise prediction is pivotal for public health and for regulatory workflows in cities, where the Environmental Noise Directive mandates regular strategic noise maps and action plans, often needed in permission workflows, right-of-way allocation, and construction scheduling. Physics-based solvers are too slow for such time-critical, iterative "what-if" studies. We evaluate conditional Normalizing Flows (Full-Glow) for generating for generating standards-compliant urban sound-pressure maps from 2D urban layouts in real time per 256x256 map on a single RTX 4090), enabling interactive exploration directly on commodity hardware. On datasets covering Baseline, Diffraction, and Reflection regimes, our model accelerates map generation by >2000 times over a reference solver while improving NLoS accuracy by up to 24% versus prior deep models; in Baseline NLoS we reach 0.65 dB MAE with high structural fidelity. The model reproduces diffraction and interference patterns and supports instant recomputation under source or geometry changes, making it a practical engine for urban planning, compliance mapping, and operations (e.g., temporary road closures, night-work variance assessments).</li>
<li><strong>摘要：</strong>准确而快速的城市噪音预测对于公共卫生和城市的监管工作流程至关重要，在该城市中，环境噪声指令要求定期的战略噪声图和行动计划，通常需要在许可工作流程，道右分配和施工计划中。基于物理的求解器对于这种关键时期，迭代的“ what-if”研究的速度太慢。我们评估有条件的归一化流（全磁），以生成与单个RTX 4090上的256x256地图实时从2D城市布局生成符合标准的城市音压图），从而使商品硬件上的交互式探索直接启用交互式探索。在涵盖基线，衍射和反射状态的数据集上，我们的模型在参考求解器上加速了地图生成> 2000倍，同时将NLOS准确性提高了24％，而不是先前的深层模型。在基线NLOS中，我们以高结构保真度达到0.65 dB MAE。该模型重现了衍射和干扰模式，并支持在源或几何变化下的即时重新计算，使其成为城市规划，合规映射和操作的实用引擎（例如，临时道路封闭，夜间工作差异评估）。</li>
</ul>

<h3>Title: Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yisen Gao, Xingcheng Fu, Qingyun Sun, Jianxin Li, Xianxian Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04522">https://arxiv.org/abs/2510.04522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04522">https://arxiv.org/pdf/2510.04522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04522]] Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction(https://arxiv.org/abs/2510.04522)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Graph diffusion models have made significant progress in learning structured graph data and have demonstrated strong potential for predictive tasks. Existing approaches typically embed node, edge, and graph-level features into a unified latent space, modeling prediction tasks including classification and regression as a form of conditional generation. However, due to the non-Euclidean nature of graph data, features of different curvatures are entangled in the same latent space without releasing their geometric potential. To address this issue, we aim to construt an ideal Riemannian diffusion model to capture distinct manifold signatures of complex graph data and learn their distribution. This goal faces two challenges: numerical instability caused by exponential mapping during the encoding proces and manifold deviation during diffusion generation. To address these challenges, we propose GeoMancer: a novel Riemannian graph diffusion framework for both generation and prediction tasks. To mitigate numerical instability, we replace exponential mapping with an isometric-invariant Riemannian gyrokernel approach and decouple multi-level features onto their respective task-specific manifolds to learn optimal representations. To address manifold deviation, we introduce a manifold-constrained diffusion method and a self-guided strategy for unconditional generation, ensuring that the generated data remains aligned with the manifold signature. Extensive experiments validate the effectiveness of our approach, demonstrating superior performance across a variety of tasks.</li>
<li><strong>摘要：</strong>图扩散模型在学习结构化图数据方面取得了重大进展，并展示了预测任务的强大潜力。现有方法通常将节点、边缘和图级特征嵌入到统一的潜在空间中，对包括分类和回归在内的预测任务进行建模，作为条件生成的一种形式。然而，由于图数据的非欧几里得性质，不同曲率的特征纠缠在同一潜在空间中，而没有释放其几何潜力。为了解决这个问题，我们的目标是构建一个理想的黎曼扩散模型来捕获复杂图数据的不同流形特征并学习它们的分布。这一目标面临两个挑战：编码过程中指数映射引起的数值不稳定和扩散生成过程中的流形偏差。为了应对这些挑战，我们提出了 GeoMancer：一种用于生成和预测任务的新型黎曼图扩散框架。为了减轻数值不稳定性，我们用等距不变黎曼陀螺核​​方法替换指数映射，并将多级特征解耦到各自的特定任务流形上以学习最佳表示。为了解决流形偏差，我们引入了流形约束扩散方法和无条件生成的自引导策略，确保生成的数据与流形签名保持一致。大量的实验验证了我们方法的有效性，展示了在各种任务中的卓越性能。</li>
</ul>

<h3>Title: TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling</h3>
<ul>
<li><strong>Authors: </strong>Hyunmin Cho, Donghoon Ahn, Susung Hong, Jee Eun Kim, Seungryong Kim, Kyong Hwan Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04533">https://arxiv.org/abs/2510.04533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04533">https://arxiv.org/pdf/2510.04533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04533]] TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling(https://arxiv.org/abs/2510.04533)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent diffusion models achieve the state-of-the-art performance in image generation, but often suffer from semantic inconsistencies or hallucinations. While various inference-time guidance methods can enhance generation, they often operate indirectly by relying on external signals or architectural modifications, which introduces additional computational overhead. In this paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and direct guidance method that operates solely on trajectory signals without modifying the underlying diffusion model. TAG leverages an intermediate sample as a projection basis and amplifies the tangential components of the estimated scores with respect to this basis to correct the sampling trajectory. We formalize this guidance process by leveraging a first-order Taylor expansion, which demonstrates that amplifying the tangential component steers the state toward higher-probability regions, thereby reducing inconsistencies and enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module that improves diffusion sampling fidelity with minimal computational addition, offering a new perspective on diffusion guidance.</li>
<li><strong>摘要：</strong>最近的扩散模型在图像产生中实现了最新的性能，但通常会遭受语义上的不一致或幻觉的困扰。尽管各种推理时间指导方法可以增强产生，但它们通常通过依靠外部信号或体系结构修改而间接运行，这引入了其他计算开销。在本文中，我们提出了切向扩增指导（TAG），这是一种更有效，更直接的指导方法，仅在轨迹信号上运行，而无需修改潜在的扩散模型。 TAG利用中间样品作为投影基础，并放大相对于此基础的估计分数的切向成分以纠正采样轨迹。我们通过利用一阶泰勒扩展来形式化这一指导过程，该过程表明，放大切线组件会引导状态朝着更高的概率区域，从而降低不一致并提高样本质量。 TAG是一种插件，体系结构 - 敏捷的模块，可通过最小的计算添加来改善扩散采样保真度，并提供有关扩散指南的新观点。</li>
</ul>

<h3>Title: LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Haoqiang Kang, Yizhe Zhang, Nikki Lijing Kuang, Nicklas Majamaki, Navdeep Jaitly, Yi-An Ma, Lianhui Qin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04573">https://arxiv.org/abs/2510.04573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04573">https://arxiv.org/pdf/2510.04573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04573]] LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning(https://arxiv.org/abs/2510.04573)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate their reasoning ability through chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may limit the ability to revisit and refine earlier tokens in a holistic manner, which can also lead to inefficient exploration for diverse solutions. In this paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning framework that unifies the expressiveness of continuous latent representation with the iterative refinement capabilities of latent diffusion models for an existing LLM. We first construct a structured latent reasoning space using a Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of thought tokens, preserving semantic information and interpretability while offering compact but expressive representations. Subsequently, we utilize a latent diffusion model that learns to denoise a block of latent thought tokens with a blockwise bidirectional attention mask, enabling longer horizon and iterative refinement with adaptive test-time compute. This design allows efficient parallel generation of diverse reasoning trajectories, allowing the model to plan and revise the reasoning process holistically. We conduct evaluations on a suite of mathematical reasoning and planning benchmarks. Empirical results show that LaDiR consistently improves accuracy, diversity, and interpretability over existing autoregressive, diffusion-based, and latent reasoning methods, revealing a new paradigm for text reasoning with latent diffusion.</li>
<li><strong>摘要：</strong>大型语言模型（LLMS）通过经营链（COT）生成展示了其推理能力。但是，LLM的自回归解码可能会限制以整体方式重新访问和改进较早令牌的能力，这也可能导致各种解决方案的效率低下。在本文中，我们提出了LADIR（潜在扩散推理器），这是一个新型的推理框架，将连续潜在表示的表现力与现有LLM的潜扩散模型的迭代完善能力统一性。我们首先使用差异自动编码器（VAE）构建一个结构化的潜在推理空间，该空间将文本推理步骤编码为思想令牌的块，在提供紧凑但表达性表示的同时保留语义信息和解释性。随后，我们利用了一个潜在扩散模型，该模型学会了用块双向注意性掩码来降低潜在的思想令牌，从而可以使用自适应测试时间计算，从而更长的地平线和迭代的细化。这种设计允许有效地平行生成各种推理轨迹，从而使模型可以整体计划和修改推理过程。我们对数学推理和计划基准的一套进行评估。经验结果表明，LADIR始终提高了与现有自回归，基于扩散和潜在推理方法的准确性，多样性和解释性，从而揭示了一种新的范式，该范式具有潜在扩散的文本推理。</li>
</ul>

<h3>Title: SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator</h3>
<ul>
<li><strong>Authors: </strong>Yuhta Takida, Satoshi Hayakawa, Takashi Shibuya, Masaaki Imaizumi, Naoki Murata, Bac Nguyen, Toshimitsu Uesaka, Chieh-Hsin Lai, Yuki Mitsufuji</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04576">https://arxiv.org/abs/2510.04576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04576">https://arxiv.org/pdf/2510.04576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04576]] SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator(https://arxiv.org/abs/2510.04576)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Deep generative models have made significant advances in generating complex content, yet conditional generation remains a fundamental challenge. Existing conditional generative adversarial networks often struggle to balance the dual objectives of assessing authenticity and conditional alignment of input samples within their conditional discriminators. To address this, we propose a novel discriminator design that integrates three key capabilities: unconditional discrimination, matching-aware supervision to enhance alignment sensitivity, and adaptive weighting to dynamically balance all objectives. Specifically, we introduce Sum of Naturalness and Alignment (SONA), which employs separate projections for naturalness (authenticity) and alignment in the final layer with an inductive bias, supported by dedicated objective functions and an adaptive weighting mechanism. Extensive experiments on class-conditional generation tasks show that \ours achieves superior sample quality and conditional alignment compared to state-of-the-art methods. Furthermore, we demonstrate its effectiveness in text-to-image generation, confirming the versatility and robustness of our approach.</li>
<li><strong>摘要：</strong>深层生成模型在产生复杂的内容方面取得了重大进步，但有条件的生成仍然是一个基本挑战。现有的条件生成对抗网络通常很难平衡其有条件歧视者中输入样本的真实性和条件对齐的双重目标。为了解决这个问题，我们提出了一种集成了三个关键功能的新颖歧视器设计：无条件歧视，匹配感知的监督以增强对齐敏感性和适应性加权，以动态平衡所有目标。具体而言，我们介绍了自然和对齐的总和（SONA），该总和在最终层中采用了自然性（真实性）和对齐方式，并具有感应性偏见，并由专用的目标函数和适应性的加权机制支持。关于课堂条件生成任务的广泛实验表明，与最先进的方法相比，\我们的样本质量和条件对齐能力优越。此外，我们证明了它在文本到图像生成中的有效性，证实了我们方法的多功能性和鲁棒性。</li>
</ul>

<h3>Title: Improved probabilistic regression using diffusion models</h3>
<ul>
<li><strong>Authors: </strong>Carlo Kneissl, Christopher Bülte, Philipp Scholl, Gitta Kutyniok</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04583">https://arxiv.org/abs/2510.04583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04583">https://arxiv.org/pdf/2510.04583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04583]] Improved probabilistic regression using diffusion models(https://arxiv.org/abs/2510.04583)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Probabilistic regression models the entire predictive distribution of a response variable, offering richer insights than classical point estimates and directly allowing for uncertainty quantification. While diffusion-based generative models have shown remarkable success in generating complex, high-dimensional data, their usage in general regression tasks often lacks uncertainty-related evaluation and remains limited to domain-specific applications. We propose a novel diffusion-based framework for probabilistic regression that learns predictive distributions in a nonparametric way. More specifically, we propose to model the full distribution of the diffusion noise, enabling adaptation to diverse tasks and enhanced uncertainty quantification. We investigate different noise parameterizations, analyze their trade-offs, and evaluate our framework across a broad range of regression tasks, covering low- and high-dimensional settings. For several experiments, our approach shows superior performance against existing baselines, while delivering calibrated uncertainty estimates, demonstrating its versatility as a tool for probabilistic prediction.</li>
<li><strong>摘要：</strong>概率回归对响应变量的整个预测分布进行建模，比经典点估计提供了更丰富的见解，并直接允许进行不确定性定量。尽管基于扩散的生成模型在生成复杂的高维数据方面取得了显着成功，但它们在一般回归任务中的用法通常缺乏与不确定性相关的评估，并且仍然限于特定于领域的应用。我们为概率回归提出了一个新颖的基于扩散的框架，该框架以非参数方式学习预测分布。更具体地说，我们建议对扩散噪声的完整分布进行建模，从而适应各种任务并增强不确定性定量。我们研究了不同的噪声参数化，分析其权衡，并在各种回归任务中评估我们的框架，涵盖低维度和高维度。对于几个实验，我们的方法显示出针对现有基线的卓越性能，同时进行了校准的不确定性估计，证明了其多功能性是概率预测的工具。</li>
</ul>

<h3>Title: Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong, Vamsidhar Kamanuru, Jay Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, Urmish Thakker, James Zou, Kunle Olukotun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04618">https://arxiv.org/abs/2510.04618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04618">https://arxiv.org/pdf/2510.04618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04618]] Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models(https://arxiv.org/abs/2510.04618)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) applications such as agents and domain-specific reasoning increasingly rely on context adaptation -- modifying inputs with instructions, strategies, or evidence, rather than weight updates. Prior approaches improve usability but often suffer from brevity bias, which drops domain insights for concise summaries, and from context collapse, where iterative rewriting erodes details over time. Building on the adaptive memory introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context Engineering), a framework that treats contexts as evolving playbooks that accumulate, refine, and organize strategies through a modular process of generation, reflection, and curation. ACE prevents collapse with structured, incremental updates that preserve detailed knowledge and scale with long-context models. Across agent and domain-specific benchmarks, ACE optimizes contexts both offline (e.g., system prompts) and online (e.g., agent memory), consistently outperforming strong baselines: +10.6% on agents and +8.6% on finance, while significantly reducing adaptation latency and rollout cost. Notably, ACE could adapt effectively without labeled supervision and instead by leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches the top-ranked production-level agent on the overall average and surpasses it on the harder test-challenge split, despite using a smaller open-source model. These results show that comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）应用程序（例如代理和特定于领域的推理）越来越依赖上下文适应 - 通过说明，策略或证据修改输入，而不是重量更新。先前的方法可改善可用性，但通常会遭受简洁的偏见，从而使域的洞察力简化摘要，并且从上下文崩溃中，迭代重写会随着时间的流逝而侵蚀细节。在动态备忘单引入的自适应记忆的基础上，我们介绍了ACE（代理上下文工程），该框架将上下文视为不断发展的剧本，这些剧本通过模块化的生成，反射，策划和策划来积累，完善和组织策略。 ACE可以通过结构化的增量更新来防止崩溃，这些更新可通过长篇文本模型保留详细的知识和规模。跨代理和域特异性基准，ACE优化了离线（例如，系统提示）和在线（例如代理存储器）的上下文，始终优于强大的基准： +10.6％的代理商和 +8.6％的资金，同时显着降低了适应延迟和推广成本。值得注意的是，ACE可以在没有标记的监督的情况下有效地适应，而是利用自然执行反馈。在AppWorld排行榜上，ACE在总体平均水平上匹配了排名最高的生产级别的代理商，尽管使用了较小的开源型号，但在更硬的测试挑战分配中超过了它。这些结果表明，全面，不断发展的环境可实现低开销的可扩展，高效和自我改善的LLM系统。</li>
</ul>

<h3>Title: Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI</h3>
<ul>
<li><strong>Authors: </strong>Youngjoon Lee, Seongmin Cho, Yehhyun Jo, Jinu Gong, Hyunjoo Jenny Lee, Joonhyuk Kang</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04622">https://arxiv.org/abs/2510.04622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04622">https://arxiv.org/pdf/2510.04622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04622]] Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI(https://arxiv.org/abs/2510.04622)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The limited data availability due to strict privacy regulations and significant resource demands severely constrains biomedical time-series AI development, which creates a critical gap between data requirements and accessibility. Synthetic data generation presents a promising solution by producing artificial datasets that maintain the statistical properties of real biomedical time-series data without compromising patient confidentiality. We propose a framework for synthetic biomedical time-series data generation based on advanced forecasting models that accurately replicates complex electrophysiological signals such as EEG and EMG with high fidelity. These synthetic datasets preserve essential temporal and spectral properties of real data, which enables robust analysis while effectively addressing data scarcity and privacy challenges. Our evaluations across multiple subjects demonstrate that the generated synthetic data can serve as an effective substitute for real data and also significantly boost AI model performance. The approach maintains critical biomedical features while provides high scalability for various applications and integrates seamlessly into open-source repositories, substantially expanding resources for AI-driven biomedical research.</li>
<li><strong>摘要：</strong>由于严格的隐私法规和大量资源的要求，数据可用性有限，严重限制了生物医学时间序列的AI开发，这在数据要求和可访问性之间造成了危险的差距。合成数据生成通过生成人工数据集来提出一个有希望的解决方案，从而在不损害患者机密性的情况下保持真实生物医学时间序列数据的统计特性。我们提出了一个基于先进的预测模型的合成生物医学时间序列数据生成的框架，该模型可以准确地复制具有高富度性的复杂电生理信号，例如EEG和EMG。这些合成数据集保留了真实数据的基本时间和光谱特性，从而可以有效地解决数据稀缺和隐私挑战。我们对多个受试者进行的评估表明，生成的合成数据可以作为真实数据的有效替代品，并显着提高AI模型性能。该方法保持关键的生物医学特征，同时为各种应用提供高可扩展性，并无缝地集成到开源存储库中，从而大大扩展了AI驱动的生物医学研究的资源。</li>
</ul>

<h3>Title: SFANet: Spatial-Frequency Attention Network for Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Vrushank Ahire, Aniruddh Muley, Shivam Zample, Siddharth Verma, Pranav Menon, Surbhi Madan, Abhinav Dhall</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04630">https://arxiv.org/abs/2510.04630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04630">https://arxiv.org/pdf/2510.04630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04630]] SFANet: Spatial-Frequency Attention Network for Deepfake Detection(https://arxiv.org/abs/2510.04630)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Detecting manipulated media has now become a pressing issue with the recent rise of deepfakes. Most existing approaches fail to generalize across diverse datasets and generation techniques. We thus propose a novel ensemble framework, combining the strengths of transformer-based architectures, such as Swin Transformers and ViTs, and texture-based methods, to achieve better detection accuracy and robustness. Our method introduces innovative data-splitting, sequential training, frequency splitting, patch-based attention, and face segmentation techniques to handle dataset imbalances, enhance high-impact regions (e.g., eyes and mouth), and improve generalization. Our model achieves state-of-the-art performance when tested on the DFWild-Cup dataset, a diverse subset of eight deepfake datasets. The ensemble benefits from the complementarity of these approaches, with transformers excelling in global feature extraction and texturebased methods providing interpretability. This work demonstrates that hybrid models can effectively address the evolving challenges of deepfake detection, offering a robust solution for real-world applications.</li>
<li><strong>摘要：</strong>随着深层爆发的兴起，发现操纵媒体已成为一个紧迫的问题。大多数现有的方法无法推广到各种数据集和发电技术中。因此，我们提出了一个新颖的集合框架，结合了基于变形金刚的体系结构的优势，例如SWIN变压器和VIT，以及基于纹理的方法，以实现更好的检测准确性和鲁棒性。我们的方法介绍了创新的数据分解，顺序训练，频率分裂，基于补丁的注意力以及面部分割技术，以处理数据集失衡，增强高影响力区域（例如，眼睛和口腔）并改善概括。当在DFWILD-CUP数据集上测试时，我们的模型可实现最先进的性能，这是八个DeepFake数据集的各种子集。合奏从这些方法的互补性中受益，在全球特征提取和基于质地的方法方面，变形金刚提供了可解释性。这项工作表明，混合模型可以有效地应对深泡检测的不断发展的挑战，从而为现实世界应用提供了强大的解决方案。</li>
</ul>

<h3>Title: Predictive Feature Caching for Training-free Acceleration of Molecular Geometry Generation</h3>
<ul>
<li><strong>Authors: </strong>Johanna Sommer, John Rachwan, Nils Fleischmann, Stephan Günnemann, Bertrand Charpentier</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04646">https://arxiv.org/abs/2510.04646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04646">https://arxiv.org/pdf/2510.04646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04646]] Predictive Feature Caching for Training-free Acceleration of Molecular Geometry Generation(https://arxiv.org/abs/2510.04646)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Flow matching models generate high-fidelity molecular geometries but incur significant computational costs during inference, requiring hundreds of network evaluations. This inference overhead becomes the primary bottleneck when such models are employed in practice to sample large numbers of molecular candidates. This work discusses a training-free caching strategy that accelerates molecular geometry generation by predicting intermediate hidden states across solver steps. The proposed method operates directly on the SE(3)-equivariant backbone, is compatible with pretrained models, and is orthogonal to existing training-based accelerations and system-level optimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching achieves a twofold reduction in wall-clock inference time at matched sample quality and a speedup of up to 3x compared to the base model with minimal sample quality degradation. Because these gains compound with other optimizations, applying caching alongside other general, lossless optimizations yield as much as a 7x speedup.</li>
<li><strong>摘要：</strong>流匹配模型会产生高保真分子几何形状，但在推断过程中产生了明显的计算成本，需要数百个网络评估。当在实践中使用此类模型来采样大量分子候选者时，这种推断的开销成为主要瓶颈。这项工作讨论了一种无训练的缓存策略，该策略通过在求解器步骤中预测中间隐藏状态来加速分子几何的产生。所提出的方法直接在SE（3） - 等级骨架上运行，与预验证的模型兼容，并且与现有的基于训练的加速度和系统级别的优化均正交。 Geom-Prugs数据集的实验表明，与基本模型相比，缓存的壁式推理时间降低了双重降低，高达3倍的壁锁推理时间降低了最小的样品质量降解。因为这些具有其他优化的增益，因此将缓存与其他一般的无损优化一起应用，而无效的优化产生了多达7倍的速度。</li>
</ul>

<h3>Title: ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Foivos Paraperas Papantoniou, Stefanos Zafeiriou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04706">https://arxiv.org/abs/2510.04706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04706">https://arxiv.org/pdf/2510.04706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04706]] ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion(https://arxiv.org/abs/2510.04706)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Human-centric generative models designed for AI-driven storytelling must bring together two core capabilities: identity consistency and precise control over human performance. While recent diffusion-based approaches have made significant progress in maintaining facial identity, achieving fine-grained expression control without compromising identity remains challenging. In this work, we present a diffusion-based framework that faithfully reimagines any subject under any particular facial expression. Building on an ID-consistent face foundation model, we adopt a compositional design featuring an expression cross-attention module guided by FLAME blendshape parameters for explicit control. Trained on a diverse mixture of image and video data rich in expressive variation, our adapter generalizes beyond basic emotions to subtle micro-expressions and expressive transitions, overlooked by prior works. In addition, a pluggable Reference Adapter enables expression editing in real images by transferring the appearance from a reference frame during synthesis. Extensive quantitative and qualitative evaluations show that our model outperforms existing methods in tailored and identity-consistent expression generation. Code and models can be found at this https URL.</li>
<li><strong>摘要：</strong>为AI驱动的讲故事而设计的以人为中心的生成模型必须汇总两个核心能力：身份一致性和对人类绩效的精确控制。尽管最近的基于扩散的方法在维持面部身份方面取得了重大进展，但在不损害身份的情况下实现细粒度的表达控制仍然具有挑战性。在这项工作中，我们提出了一个基于扩散的框架，该框架忠实地重新构想了任何特定面部表达下的任何主题。在ID一致的面部基础模型的基础上，我们采用了一种构图设计，该设计由Flame Blendshape参数引导的表达交叉注意模块，以进行显式控制。经过培训的图像和视频数据具有丰富的表达性变化的混合物，我们的适配器超出了基本情绪，从而通过先前的作品忽略了微妙的微表达和表达过渡。此外，可插入的参考适配器可以通过在合成过程中从参考框中传输外观来实现真实图像的表达编辑。广泛的定量和定性评估表明，我们的模型在量身定制和身份符合表达生成中的现有方法优于现有方法。代码和模型可以在此HTTPS URL上找到。</li>
</ul>

<h3>Title: ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Luo Cheng, Song Siyang, Yan Siyuan, Yu Zhen, Ge Zongyuan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04712">https://arxiv.org/abs/2510.04712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04712">https://arxiv.org/pdf/2510.04712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04712]] ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model(https://arxiv.org/abs/2510.04712)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The automatic generation of diverse and human-like facial reactions in dyadic dialogue remains a critical challenge for human-computer interaction systems. Existing methods fail to model the stochasticity and dynamics inherent in real human reactions. To address this, we propose ReactDiff, a novel temporal diffusion framework for generating diverse facial reactions that are appropriate for responding to any given dialogue context. Our key insight is that plausible human reactions demonstrate smoothness, and coherence over time, and conform to constraints imposed by human facial anatomy. To achieve this, ReactDiff incorporates two vital priors (spatio-temporal facial kinematics) into the diffusion process: i) temporal facial behavioral kinematics and ii) facial action unit dependencies. These two constraints guide the model toward realistic human reaction manifolds, avoiding visually unrealistic jitters, unstable transitions, unnatural expressions, and other artifacts. Extensive experiments on the REACT2024 dataset demonstrate that our approach not only achieves state-of-the-art reaction quality but also excels in diversity and reaction appropriateness.</li>
<li><strong>摘要：</strong>在二元对话中自动产生的不同和类似人类的面部反应仍然是人类计算机相互作用系统的关键挑战。现有方法无法模拟实际人类反应中固有的随机性和动力学。为了解决这个问题，我们提出了ReactDiff，这是一个新型的时间扩散框架，用于产生适合于对任何给定的对话环境做出反应的各种面部反应。我们的关键见解是，合理的人类反应表现出光滑度和一致性，并且符合人面部解剖结构所施加的约束。为了实现这一目标，ReactDiff将两个重要的先验（时空面部运动学）融入了扩散过程中：i）暂时面部行为行为运动学和ii）面部作用单位依赖性。这两个约束指导该模型朝着现实的人类反应歧管，避免了视觉上不切实际的烦恼，不稳定的过渡，不自然的表达和其他人工制品。对React2024数据集的广泛实验表明，我们的方法不仅达到了最先进的反应质量，而且在多样性和反应适当性方面也表现出色。</li>
</ul>

<h3>Title: ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs</h3>
<ul>
<li><strong>Authors: </strong>Wonjun Kang, Kevin Galim, Seunghyuk Oh, Minjae Lee, Yuchen Zeng, Shuibai Zhang, Coleman Hooper, Yuezhou Hu, Hyung Il Koo, Nam Ik Cho, Kangwook Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04767">https://arxiv.org/abs/2510.04767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04767">https://arxiv.org/pdf/2510.04767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04767]] ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs(https://arxiv.org/abs/2510.04767)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While most autoregressive LLMs are constrained to one-by-one decoding, diffusion LLMs (dLLMs) have attracted growing interest for their potential to dramatically accelerate inference through parallel decoding. Despite this promise, the conditional independence assumption in dLLMs causes parallel decoding to ignore token dependencies, inevitably degrading generation quality when these dependencies are strong. However, existing works largely overlook these inherent challenges, and evaluations on standard benchmarks (e.g., math and coding) are not sufficient to capture the quality degradation caused by parallel decoding. To address this gap, we first provide an information-theoretic analysis of parallel decoding. We then conduct case studies on analytically tractable synthetic list operations from both data distribution and decoding strategy perspectives, offering quantitative insights that highlight the fundamental limitations of parallel decoding. Building on these insights, we propose ParallelBench, the first benchmark specifically designed for dLLMs, featuring realistic tasks that are trivial for humans and autoregressive LLMs yet exceptionally challenging for dLLMs under parallel decoding. Using ParallelBench, we systematically analyze both dLLMs and autoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can suffer dramatic quality degradation in real-world scenarios, and (ii) current parallel decoding strategies struggle to adapt their degree of parallelism based on task difficulty, thus failing to achieve meaningful speedup without compromising quality. Our findings underscore the pressing need for innovative decoding methods that can overcome the current speed-quality trade-off. We release our benchmark to help accelerate the development of truly efficient dLLMs.</li>
<li><strong>摘要：</strong>尽管大多数自​​回旋的LLM都限制在一对一解码上，但扩散LLM（DLLM）吸引了人们日益增长的兴趣，因为它们的潜力通过平行解码来显着加速推断。尽管有这样的承诺，但DLLMS中的条件独立性假设导致平行解码忽略令牌依赖性，当这些依赖性强劲时，不可避免地会降低产生质量。但是，现有作品在很大程度上忽略了这些固有的挑战，并且对标准基准测试（例如，数学和编码）的评估不足以捕获并行解码引起的质量退化。为了解决这一差距，我们首先提供了平行解码的信息理论分析。然后，我们从数据分布和解码策略的角度进行了有关可分析可处理的合成列表操作的案例研究，提供了定量见解，突出了并行解码的基本局限性。在这些见解的基础上，我们提出了ParelallBench，这是专门为DLLM设计的第一个基准测试标准，其中包含对人类和自回归LLM的逼真的任务，但对并行解码下的DLLM来说非常具有挑战性。使用平行平台，我们系统地分析了DLLM和自回旋的LLMS，表明：（i）在现实情况下，在平行解码下的DLLM可以遭受巨大的质量降解，以及（ii）当前的平行解码策略难以根据任务难以实现意义上的快速效果，以适应其基于任务的平行程度。我们的发现强调了可以克服当前速度质量权衡的创新解码方法的迫切需求。我们发布我们的基准，以帮助加速真正有效的DLLM的发展。</li>
</ul>

<h3>Title: Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning</h3>
<ul>
<li><strong>Authors: </strong>Xiaomeng Fan, Yuchuan Mao, Zhi Gao, Yuwei Wu, Jin Chen, Yunde Jia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04770">https://arxiv.org/abs/2510.04770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04770">https://arxiv.org/pdf/2510.04770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04770]] Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning(https://arxiv.org/abs/2510.04770)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Open-vocabulary learning requires modeling the data distribution in open environments, which consists of both seen-class and unseen-class data. Existing methods estimate the distribution in open environments using seen-class data, where the absence of unseen classes makes the estimation error inherently unidentifiable. Intuitively, learning beyond the seen classes is crucial for distribution estimation to bound the estimation error. We theoretically demonstrate that the distribution can be effectively estimated by generating unseen-class data, through which the estimation error is upper-bounded. Building on this theoretical insight, we propose a novel open-vocabulary learning method, which generates unseen-class data for estimating the distribution in open environments. The method consists of a class-domain-wise data generation pipeline and a distribution alignment algorithm. The data generation pipeline generates unseen-class data under the guidance of a hierarchical semantic tree and domain information inferred from the seen-class data, facilitating accurate distribution estimation. With the generated data, the distribution alignment algorithm estimates and maximizes the posterior probability to enhance generalization in open-vocabulary learning. Extensive experiments on $11$ datasets demonstrate that our method outperforms baseline approaches by up to $14\%$, highlighting its effectiveness and superiority.</li>
<li><strong>摘要：</strong>开放式学习需要在开放环境中对数据分布进行建模，该数据分布由可见级和看不见的级别数据组成。现有方法使用可见的类数据估算开放环境中的分布，在这些数据中，缺乏看不见的类使估计误差本质上无法识别。直观地，学习超出所见类的学习对于分布估计以绑定估计误差至关重要。从理论上讲，我们可以通过生成看不见的级别数据来有效地估算分布，通过该数据估计误差在上限。在理论上的洞察力的基础上，我们提出了一种新型的开放式摄影学习方法，该方法生成了看不见的级别数据，用于估计开放环境中的分布。该方法由类域的数据生成管道和分布比对算法组成。数据生成管道在层次结构的语义树和域信息的指导下生成了看不见的级别数据，从可见级数据中推断出，从而促进了准确的分布估计。借助生成的数据，分布对准算法估计并最大化后验概率增强开放式摄取的学习中的概括。 $ 11 $数据集的广泛实验表明，我们的方法的表现优于基线方法，最高$ 14 \％$，强调了其有效性和优势。</li>
</ul>

<h3>Title: DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Qi Li, Shuwen Qiu, Julien Han, Xingzi Xu, Mehmet Saygin Seyfioglu, Kee Kiat Koo, Karim Bouyarmane</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04797">https://arxiv.org/abs/2510.04797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04797">https://arxiv.org/pdf/2510.04797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04797]] DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing(https://arxiv.org/abs/2510.04797)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The rapid growth of e-commerce has intensified the demand for Virtual Try-On (VTO) technologies, enabling customers to realistically visualize products overlaid on their own images. Despite recent advances, existing VTO models face challenges with fine-grained detail preservation, robustness to real-world imagery, efficient sampling, image editing capabilities, and generalization across diverse product categories. In this paper, we present DiT-VTON, a novel VTO framework that leverages a Diffusion Transformer (DiT), renowned for its performance on text-conditioned image generation, adapted here for the image-conditioned VTO task. We systematically explore multiple DiT configurations, including in-context token concatenation, channel concatenation, and ControlNet integration, to determine the best setup for VTO image conditioning. To enhance robustness, we train the model on an expanded dataset encompassing varied backgrounds, unstructured references, and non-garment categories, demonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also redefines the VTO task beyond garment try-on, offering a versatile Virtual Try-All (VTA) solution capable of handling a wide range of product categories and supporting advanced image editing functionalities such as pose preservation, localized editing, texture transfer, and object-level customization. Experimental results show that our model surpasses state-of-the-art methods on VITON-HD, achieving superior detail preservation and robustness without reliance on additional condition encoders. It also outperforms models with VTA and image editing capabilities on a diverse dataset spanning thousands of product categories.</li>
<li><strong>摘要：</strong>电子商务的快速增长加剧了对虚拟试验技术（VTO）技术的需求，使客户能够实际可视化在自己的图像上覆盖的产品。尽管有最近的进步，现有的VTO模型将面临挑战，细节保存，对现实世界图像的稳健性，有效的采样，图像编辑功能以及跨不同产品类别的概括。在本文中，我们提出了Dit-Vton，这是一个新颖的VTO框架，它利用扩散变压器（DIT），以其在文本条件的图像生成上的性能而闻名，此处适用于图像条件的VTO任务。我们系统地探索了多个DIT配置，包括在上下文令牌串联，通道串联和ControlNet集成中，以确定用于VTO图像调节的最佳设置。为了增强鲁棒性，我们在扩展的数据集上训练该模型，包括各种背景，非结构化的参考和非议程类别，证明了数据扩展对VTO适应性的好处。 DIT-VTON还重新定义了服装之外的VTO任务，它提供了能够处理广泛的产品类别并支持高级图像编辑功能（例如姿势保存，局部编辑，纹理传输，纹理传输和对象级别的自定义）的多功能虚拟试验（VTA）解决方案。实验结果表明，我们的模型超过了Viton-HD上的最新方法，实现了优越的细节保存和鲁棒性，而无需依赖其他条件编码。它还超过了具有VTA和图像编辑功能的模型，该模型涵盖了数千种产品类别。</li>
</ul>

<h3>Title: Synthesising Counterfactual Explanations via Label-Conditional Gaussian Mixture Variational Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Junqi Jiang, Francesco Leofante, Antonio Rago, Francesca Toni</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04855">https://arxiv.org/abs/2510.04855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04855">https://arxiv.org/pdf/2510.04855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04855]] Synthesising Counterfactual Explanations via Label-Conditional Gaussian Mixture Variational Autoencoders(https://arxiv.org/abs/2510.04855)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Counterfactual explanations (CEs) provide recourse recommendations for individuals affected by algorithmic decisions. A key challenge is generating CEs that are robust against various perturbation types (e.g. input and model perturbations) while simultaneously satisfying other desirable properties. These include plausibility, ensuring CEs reside on the data manifold, and diversity, providing multiple distinct recourse options for single inputs. Existing methods, however, mostly struggle to address these multifaceted requirements in a unified, model-agnostic manner. We address these limitations by proposing a novel generative framework. First, we introduce the Label-conditional Gaussian Mixture Variational Autoencoder (L-GMVAE), a model trained to learn a structured latent space where each class label is represented by a set of Gaussian components with diverse, prototypical centroids. Building on this, we present LAPACE (LAtent PAth Counterfactual Explanations), a model-agnostic algorithm that synthesises entire paths of CE points by interpolating from inputs' latent representations to those learned latent centroids. This approach inherently ensures robustness to input changes, as all paths for a given target class converge to the same fixed centroids. Furthermore, the generated paths provide a spectrum of recourse options, allowing users to navigate the trade-off between proximity and plausibility while also encouraging robustness against model changes. In addition, user-specified actionability constraints can also be easily incorporated via lightweight gradient optimisation through the L-GMVAE's decoder. Comprehensive experiments show that LAPACE is computationally efficient and achieves competitive performance across eight quantitative metrics.</li>
<li><strong>摘要：</strong>反事实解释（CES）为受算法决定影响的个人提供了建议。一个关键挑战是生成与各种扰动类型（例如输入和模型扰动）同时满足其他期望的属性的CE。这些包括合理性，确保CES驻留在数据歧管上和多样性上，为单个输入提供了多种不同的追索权。但是，现有的方法主要努力以统一的模型不可静止的方式解决这些多方面的要求。我们通过提出一个新颖的生成框架来解决这些局限性。首先，我们介绍了标签条件高斯混合物变化自动编码器（L-GMVAE），该模型训练有素，可以学习一个结构化的潜在空间，其中每个类标签由一组具有多样性的原型质心的高斯组件表示。在此基础上，我们提出了Lapace（潜在路径反事实解释），这是一种模型不合时宜的算法，通过从输入的潜在表示到那些学到的潜在质心来融合CE点的整个路径。这种方法固有地确保了输入变化的鲁棒性，因为给定目标类别类别的所有路径都会收敛到相同的固定质心。此外，生成的路径提供了一系列追索性选项，使用户可以在接近度和合理性之间进行权衡，同时也鼓励对模型更改的鲁棒性。此外，还可以通过L-GMVAE的解码器轻松地优化用户指定的可操作约束。综合实验表明，林帕斯在计算上是有效的，并且在八个定量指标中实现了竞争性能。</li>
</ul>

<h3>Title: μDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy</h3>
<ul>
<li><strong>Authors: </strong>Elena Corbetta, Thomas Bocklitz</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.data-an, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04859">https://arxiv.org/abs/2510.04859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04859">https://arxiv.org/pdf/2510.04859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04859]] μDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy(https://arxiv.org/abs/2510.04859)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Optical microscopy is one of the most widely used techniques in research studies for life sciences and biomedicine. These applications require reliable experimental pipelines to extract valuable knowledge from the measured samples and must be supported by image quality assessment (IQA) to ensure correct processing and analysis of the image data. IQA methods are implemented with variable complexity. However, while most quality metrics have a straightforward implementation, they might be time consuming and computationally expensive when evaluating a large dataset. In addition, quality metrics are often designed for well-defined image features and may be unstable for images out of the ideal domain. To overcome these limitations, recent works have proposed deep learning-based IQA methods, which can provide superior performance, increased generalizability and fast prediction. Our method, named $\mathrm{\mu}$DeepIQA, is inspired by previous studies and applies a deep convolutional neural network designed for IQA on natural images to optical microscopy measurements. We retrained the same architecture to predict individual quality metrics and global quality scores for optical microscopy data. The resulting models provide fast and stable predictions of image quality by generalizing quality estimation even outside the ideal range of standard methods. In addition, $\mathrm{\mu}$DeepIQA provides patch-wise prediction of image quality and can be used to visualize spatially varying quality in a single image. Our study demonstrates that optical microscopy-based studies can benefit from the generalizability of deep learning models due to their stable performance in the presence of outliers, the ability to assess small image patches, and rapid predictions.</li>
<li><strong>摘要：</strong>光学显微镜是生命科学和生物医学研究研究中最广泛使用的技术之一。这些应用需要可靠的实验管道来从测量样本中提取有价值的知识，并且必须通过图像质量评估（IQA）支持，以确保对图像数据的正确处理和分析。 IQA方法是具有可变复杂性的。但是，尽管大多数质量指标具有直接的实现，但评估大型数据集时它们可能会耗时且计算昂贵。此外，质量指标通常是针对明确定义的图像特征而设计的，并且对于理想域中的图像可能不稳定。为了克服这些局限性，最近的作品提出了基于深度学习的IQA方法，可以提供卓越的性能，提高概括性和快速预测。我们的方法被称为$ \ mathrm {\ mu} $ deepiqa，灵感来自先前的研究，并应用了一个为IQA设计的深卷积神经网络，该网络旨在IQA上的自然图像进行光学显微镜测量。我们进行了相同的体系结构，以预测光学显微镜数据的个人质量指标和全球质量得分。最终的模型通过概括质量估计，即使在理想的标准方法范围之外，可以通过推广质量估计来快速，稳定。此外，$ \ mathrm {\ mu} $ deepiqa提供了图像质量的补丁预测，可用于在单个图像中可视化空间变化的质量。我们的研究表明，基于光学显微镜的研究可以从深度学习模型的普遍性中受益，因为它们在存在异常值的存在下，评估小图像斑块的能力和快速预测的能力。</li>
</ul>

<h3>Title: In-Field Mapping of Grape Yield and Quality with Illumination-Invariant Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Ciem Cornelissen, Sander De Coninck, Axel Willekens, Sam Leroux, Pieter Simoens</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04864">https://arxiv.org/abs/2510.04864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04864">https://arxiv.org/pdf/2510.04864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04864]] In-Field Mapping of Grape Yield and Quality with Illumination-Invariant Deep Learning(https://arxiv.org/abs/2510.04864)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>This paper presents an end-to-end, IoT-enabled robotic system for the non-destructive, real-time, and spatially-resolved mapping of grape yield and quality (Brix, Acidity) in vineyards. The system features a comprehensive analytical pipeline that integrates two key modules: a high-performance model for grape bunch detection and weight estimation, and a novel deep learning framework for quality assessment from hyperspectral (HSI) data. A critical barrier to in-field HSI is the ``domain shift" caused by variable illumination. To overcome this, our quality assessment is powered by the Light-Invariant Spectral Autoencoder (LISA), a domain-adversarial framework that learns illumination-invariant features from uncalibrated data. We validated the system's robustness on a purpose-built HSI dataset spanning three distinct illumination domains: controlled artificial lighting (lab), and variable natural sunlight captured in the morning and afternoon. Results show the complete pipeline achieves a recall (0.82) for bunch detection and a $R^2$ (0.76) for weight prediction, while the LISA module improves quality prediction generalization by over 20% compared to the baselines. By combining these robust modules, the system successfully generates high-resolution, georeferenced data of both grape yield and quality, providing actionable, data-driven insights for precision viticulture.</li>
<li><strong>摘要：</strong>本文介绍了一个端到端，具有物联网的机器人系统，用于葡萄园中葡萄产量和质量（brix，酸度）的非破坏性，实时和空间分辨的映射。该系统具有一条全面的分析管道，该管道整合了两个关键模块：用于葡萄束检测和重量估计的高性能模型，以及从高光谱（HSI）数据中进行质量评估的新型深度学习框架。现场HSI的关键障碍是由可变照明引起的``域转移''。为了克服这一点，我们的质量评估由轻不变的光谱自动编码器（LISA）提供支持，这是一种域名 - 逆转面框架，该域是从无效的数据中学习的三个不同的数据。域：在早晨和下午捕获的人工照明（实验室）和可变的自然阳光表明，完整的管道可召回（0.82），以进行束检测，而$ r^2 $（0.76）用于重量预测，而Lisa模块将质量通用性提高了20％的质量数据。葡萄的产量和质量，为精确葡萄栽培提供了可行的数据驱动的见解。</li>
</ul>

<h3>Title: Flow-Matching Based Refiner for Molecular Conformer Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiangyang Xu, Hongyang Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04878">https://arxiv.org/abs/2510.04878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04878">https://arxiv.org/pdf/2510.04878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04878]] Flow-Matching Based Refiner for Molecular Conformer Generation(https://arxiv.org/abs/2510.04878)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Low-energy molecular conformers generation (MCG) is a foundational yet challenging problem in drug discovery. Denoising-based methods include diffusion and flow-matching methods that learn mappings from a simple base distribution to the molecular conformer distribution. However, these approaches often suffer from error accumulation during sampling, especially in the low SNR steps, which are hard to train. To address these challenges, we propose a flow-matching refiner for the MCG task. The proposed method initializes sampling from mixed-quality outputs produced by upstream denoising models and reschedules the noise scale to bypass the low-SNR phase, thereby improving sample quality. On the GEOM-QM9 and GEOM-Drugs benchmark datasets, the generator-refiner pipeline improves quality with fewer total denoising steps while preserving diversity.</li>
<li><strong>摘要：</strong>低能分子构象产生（MCG）是药物发现中的基础但具有挑战性的问题。基于脱氧的方法包括扩散和流匹配方法，这些方法从简单的基础分布到分子构象异构体分布学习映射。但是，这些方法通常会在抽样过程中累积错误，尤其是在很难训练的低SNR步骤中。为了应对这些挑战，我们为MCG任务提出了一个匹配炼油仪。所提出的方法初始化了来自上游DeNo型模型产生的混合质量输出的采样，并重新安排了噪声量表以绕过低SNR相，从而提高了样品质量。在GEOM-QM9和GEOM-DRUGS基准数据集上，发电机 - 填料管道通过更少的总降级步骤提高质量，同时保持多样性。</li>
</ul>

<h3>Title: Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Xin Li, Kaixiang Yang, Qiang Li, Zhiwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04947">https://arxiv.org/abs/2510.04947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04947">https://arxiv.org/pdf/2510.04947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04947]] Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion(https://arxiv.org/abs/2510.04947)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Dual-view mammography, including craniocaudal (CC) and mediolateral oblique (MLO) projections, offers complementary anatomical views crucial for breast cancer diagnosis. However, in real-world clinical workflows, one view may be missing, corrupted, or degraded due to acquisition errors or compression artifacts, limiting the effectiveness of downstream analysis. View-to-view translation can help recover missing views and improve lesion alignment. Unlike natural images, this task in mammography is highly challenging due to large non-rigid deformations and severe tissue overlap in X-ray projections, which obscure pixel-level correspondences. In this paper, we propose Column-Aware and Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view translation framework based on conditional diffusion model. To address cross-view structural misalignment, we first design a column-aware cross-attention mechanism that leverages the geometric property that anatomically corresponding regions tend to lie in similar column positions across views. A Gaussian-decayed bias is applied to emphasize local column-wise correlations while suppressing distant mismatches. Furthermore, we introduce an implicit 3D structure reconstruction module that back-projects noisy 2D latents into a coarse 3D feature volume based on breast-view projection geometry. The reconstructed 3D structure is refined and injected into the denoising UNet to guide cross-view generation with enhanced anatomical awareness. Extensive experiments demonstrate that CA3D-Diff achieves superior performance in bidirectional tasks, outperforming state-of-the-art methods in visual fidelity and structural consistency. Furthermore, the synthesized views effectively improve single-view malignancy classification in screening settings, demonstrating the practical value of our method in real-world diagnostics.</li>
<li><strong>摘要：</strong>双视乳房X线摄影，包括颅底（CC）和中外侧倾斜（MLO）预测，提供了对乳腺癌诊断至关重要的互补解剖学观点。但是，在现实世界中的临床工作流程中，由于获得错误或压缩工件，可能缺少，损坏或退化，从而限制了下游分析的有效性。视图对视图翻译可以帮助恢复缺失的视图并改善病变对齐。与自然图像不同，乳房X线摄影中的这项任务由于较大的非刚性变形而高度挑战，并且X射线投影中的严重组织重叠，这使像素级的对应关系掩盖了。在本文中，我们提出了基于条件扩散模型的新型双向乳房X线照片翻译框架，这是一种新型的双向乳房X线图翻译框架。为了解决跨视图的结构错位，我们首先设计了一种柱状跨注意机制，该机制利用了几何特性，该几何特性在解剖学上对应的区域往往位于跨视图的相似柱位置。在抑制遥远的不匹配的同时，应用了高斯污点的偏见来强调局部柱的相关性。此外，我们引入了一个隐式的3D结构重建模块，该模块将嘈杂的2D潜在潜在的背景标记为基于乳房视图投影几何形状的粗3D特征体积。重建的3D结构进行了完善，并注入了denoising UNET，以指导跨视图生成具有增强的解剖学意识。广泛的实验表明，CA3D-DIFF在双向任务中取得了出色的表现，在视觉效果和结构一致性方面表现优于最先进的方法。此外，合成的观点有效地改善了筛选设置中的单视性恶性分类，证明了我们在现实世界诊断中方法的实际价值。</li>
</ul>

<h3>Title: SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Théophane Vallaeys, Jakob Verbeek, Matthieu Cord</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04961">https://arxiv.org/abs/2510.04961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04961">https://arxiv.org/pdf/2510.04961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04961]] SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization(https://arxiv.org/abs/2510.04961)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Tokenizers are a key component of state-of-the-art generative image models, extracting the most important features from the signal while reducing data dimension and redundancy. Most current tokenizers are based on KL-regularized variational autoencoders (KL-VAE), trained with reconstruction, perceptual and adversarial losses. Diffusion decoders have been proposed as a more principled alternative to model the distribution over images conditioned on the latent. However, matching the performance of KL-VAE still requires adversarial losses, as well as a higher decoding time due to iterative sampling. To address these limitations, we introduce a new pixel diffusion decoder architecture for improved scaling and training stability, benefiting from transformer components and GAN-free training. We use distillation to replicate the performance of the diffusion decoder in an efficient single-step decoder. This makes SSDD the first diffusion decoder optimized for single-step reconstruction trained without adversarial losses, reaching higher reconstruction quality and faster sampling than KL-VAE. In particular, SSDD improves reconstruction FID from $0.87$ to $0.50$ with $1.4\times$ higher throughput and preserve generation quality of DiTs with $3.8\times$ faster sampling. As such, SSDD can be used as a drop-in replacement for KL-VAE, and for building higher-quality and faster generative models.</li>
<li><strong>摘要：</strong>令牌器是最先进的生成图像模型的关键组成部分，从信号中提取最重要的特征，同时降低数据维度和冗余。大多数当前的标记器基于KL调查的变异自动编码器（KL-VAE），经过重建，感知和对抗性损失训练。已经提出扩散解码器作为更有原则的替代方案，以模拟在潜在条件下的图像上的分布。但是，匹配KL-VAE的性能仍然需要对抗性损失，并且由于迭代采样而导致更高的解码时间。为了解决这些局限性，我们引入了一种新的像素扩散解码器体系结构，以改善缩放和训练稳定性，从而受益于变压器组件和无GAN训练。我们使用蒸馏来复制有效的单步解码器中扩散解码器的性能。这使得SSDD成为针对未经对抗损失的单步重建优化的第一个扩散解码器，比KL-VAE获得了更高的重建质量和更快的采样速度。特别是，SSDD将重建FID从$ 0.87 $提高到$ 0.50 $，$ 1.4 \ times $ $ $ $ $ $ $ $ $ $ $ $ 3.8 \ $ 3.8 \ times $ agter $ abter $。因此，SSDD可以用作KL-VAE的倒入替代品，以及构建更高质量和更快的生成型模型。</li>
</ul>

<h3>Title: Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns</h3>
<ul>
<li><strong>Authors: </strong>Nabil Daiyan, Md Rakibul Haque</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05015">https://arxiv.org/abs/2510.05015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05015">https://arxiv.org/pdf/2510.05015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05015]] Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns(https://arxiv.org/abs/2510.05015)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Parkinson's disease (PD) is a progressive neurodegenerative condition characterized by the death of dopaminergic neurons, leading to various movement disorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects, yet traditional diagnostic methods are often cumbersome and costly. In this study, a machine learning-based approach is proposed using hand-drawn spiral and wave images as potential biomarkers for PD detection. Our methodology leverages convolutional neural networks (CNNs), transfer learning, and attention mechanisms to improve model performance and resilience against overfitting. To enhance the diversity and richness of both spiral and wave categories, the training dataset undergoes augmentation to increase the number of images. The proposed architecture comprises three phases: utilizing pre-trained CNNs, incorporating custom convolutional layers, and ensemble voting. Employing hard voting further enhances performance by aggregating predictions from multiple models. Experimental results show promising accuracy rates. For spiral images, weighted average precision, recall, and F1-score are 90%, and for wave images, they are 96.67%. After combining the predictions through ensemble hard voting, the overall accuracy is 93.3%. These findings underscore the potential of machine learning in early PD diagnosis, offering a non-invasive and cost-effective solution to improve patient outcomes.</li>
<li><strong>摘要：</strong>帕金森氏病（PD）是一种进行性神经退行性疾病，其特征是多巴胺能神经元死亡，导致各种运动障碍症状。 PD的早期诊断对于预防不良影响至关重要，但是传统的诊断方法通常繁琐且昂贵。在这项研究中，提出了一种基于机器学习的方法，该方法是使用手绘螺旋和波浪图像作为PD检测的潜在生物标志物。我们的方法论利用卷积神经网络（CNN），转移学习和注意机制来提高模型性能和弹性，以防止过度拟合。为了增强螺旋和波动类别的多样性和丰富性，训练数据集经历了增加的图像数量。拟议的建筑包括三个阶段：利用预先培训的CNN，结合自定义卷积层和合奏投票。使用硬投票进一步通过汇总多个模型的预测来进一步提高绩效。实验结果显示出有希望的准确率。对于螺旋图像，加权平均精度，召回和F1得分为90％，对于波浪图像，它们为96.67％。通过集合硬投票结合预测后，总体准确性为93.3％。这些发现强调了在早期PD诊断中机器学习的潜力，提供了一种无创和具有成本效益的解决方案，以改善患者的预后。</li>
</ul>

<h3>Title: Graph-Aware Diffusion for Signal Generation</h3>
<ul>
<li><strong>Authors: </strong>Sergio Rozada, Vimal K. B., Andrea Cavallo, Antonio G. Marques, Hadi Jamali-Rad, Elvin Isufi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05036">https://arxiv.org/abs/2510.05036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05036">https://arxiv.org/pdf/2510.05036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05036]] Graph-Aware Diffusion for Signal Generation(https://arxiv.org/abs/2510.05036)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We study the problem of generating graph signals from unknown distributions defined over given graphs, relevant to domains such as recommender systems or sensor networks. Our approach builds on generative diffusion models, which are well established in vision and graph generation but remain underexplored for graph signals. Existing methods lack generality, either ignoring the graph structure in the forward process or designing graph-aware mechanisms tailored to specific domains. We adopt a forward process that incorporates the graph through the heat equation. Rather than relying on the standard formulation, we consider a time-warped coefficient to mitigate the exponential decay of the drift term, yielding a graph-aware generative diffusion model (GAD). We analyze its forward dynamics, proving convergence to a Gaussian Markov random field with covariance parametrized by the graph Laplacian, and interpret the backward dynamics as a sequence of graph-signal denoising problems. Finally, we demonstrate the advantages of GAD on synthetic data, real traffic speed measurements, and a temperature sensor network.</li>
<li><strong>摘要：</strong>我们研究了从定义的图形上定义的未知分布的图形信号的问题，这些图形与推荐系统或传感器网络等域相关。我们的方法建立在生成扩散模型的基础上，这些模型在视觉和图形生成中已经很好地建立，但仍未在图形信号中进行。现有方法缺乏通用性，要么忽略向前过程中的图形结构，要么设计针对特定域的图形感知机制。我们采用了通过热方程组合图的正向过程。我们不依赖标准公式，而是考虑一个时间绘制的系数来减轻漂移项的指数衰减，从而产生图形感知的生成扩散模型（GAD）。我们分析了其正向动力学，证明了与图形拉普拉斯（Graph laplacian）参数的协方差的收敛到高斯马尔可夫随机场，并将后退动力学解释为一系列图形信号denoising问题。最后，我们证明了GAD在合成数据，实际交通速度测量和温度传感器网络上的优势。</li>
</ul>

<h3>Title: Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts</h3>
<ul>
<li><strong>Authors: </strong>Jihoon Lee, Hoyeon Moon, Kevin Zhai, Arun Kumar Chithanar, Anit Kumar Sahu, Soummya Kar, Chul Lee, Souradip Chakraborty, Amrit Singh Bedi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05040">https://arxiv.org/abs/2510.05040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05040">https://arxiv.org/pdf/2510.05040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05040]] Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts(https://arxiv.org/abs/2510.05040)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion-based large language models (dLLMs) are trained flexibly to model extreme dependence in the data distribution; however, how to best utilize this information at inference time remains an open problem. In this work, we uncover an interesting property of these models: dLLMs trained on textual data implicitly learn a mixture of semi-autoregressive experts, where different generation orders reveal different specialized behaviors. We show that committing to any single, fixed inference time schedule, a common practice, collapses performance by failing to leverage this latent ensemble. To address this, we introduce HEX (Hidden semiautoregressive EXperts for test-time scaling), a training-free inference method that ensembles across heterogeneous block schedules. By doing a majority vote over diverse block-sized generation paths, HEX robustly avoids failure modes associated with any single fixed schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to 3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and specialized fine-tuned methods like GRPO, without additional training. HEX even yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%. Our results establish a new paradigm for test-time scaling in diffusion-based LLMs (dLLMs), revealing that the sequence in which masking is performed plays a critical role in determining performance during inference.</li>
<li><strong>摘要：</strong>基于扩散的大语言模型（DLLM）经过灵活的训练，以模拟数据分布中的极端依赖性；但是，如何在推理时间最好地利用此信息仍然是一个空旷的问题。在这项工作中，我们揭示了这些模型的有趣属性：经过文本数据训练的DLLM隐含地学习了半自动回归专家的混合，其中不同的一代订单揭示了不同的专业行为。我们表明，致力于任何单一的固定推理时间表，一种常见的做法，通过未能利用这种潜在的合奏来崩溃。为了解决这个问题，我们介绍了十六进制（隐藏的半急路预向专家进行测试时间缩放），这是一种无训练的推理方法，跨越了异质块计划。通过对各种块大小的一代途径进行多数投票，十六进制可避免与任何固定时间表相关的失败模式。在推理基准（例如GSM8K）上，它可提高准确性高达3.56倍（从24.72％到88.10％），在未经其他培训的情况下，表现优于TOP-K边距推断和GRPO等专业的微调方法，例如GRPO。十六进制甚至可以从16.40％到40.00％的数学基准增长，ARC-C的科学推理从54.18％至87.80％，而真实的Fultullqa从28.36％到57.46％。我们的结果为基于扩散的LLM（DLLM）中测试时间缩放的新范式建立了新的范式，表明执行掩盖的序列在确定推理过程中的性能中起着至关重要的作用。</li>
</ul>

<h3>Title: No-reference Quality Assessment of Contrast-distorted Images using Contrast-enhanced Pseudo Reference</h3>
<ul>
<li><strong>Authors: </strong>Mohammad-Ali Mahmoudpour, Saeed Mahmoudpour</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05053">https://arxiv.org/abs/2510.05053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05053">https://arxiv.org/pdf/2510.05053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05053]] No-reference Quality Assessment of Contrast-distorted Images using Contrast-enhanced Pseudo Reference(https://arxiv.org/abs/2510.05053)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, quality assessment</a></li>
<li><strong>Abstract: </strong>Contrast change is an important factor that affects the quality of images. During image capturing, unfavorable lighting conditions can cause contrast change and visual quality loss. While various methods have been proposed to assess the quality of images under different distortions such as blur and noise, contrast distortion has been largely overlooked as its visual impact and properties are different from other conventional types of distortions. In this paper, we propose a no-reference image quality assessment (NR-IQA) metric for contrast-distorted images. Using a set of contrast enhancement algorithms, we aim to generate pseudo-reference images that are visually close to the actual reference image, such that the NR problem is transformed to a Full-reference (FR) assessment with higher accuracy. To this end, a large dataset of contrast-enhanced images is produced to train a classification network that can select the most suitable contrast enhancement algorithm based on image content and distortion for pseudo-reference image generation. Finally, the evaluation is performed in the FR manner to assess the quality difference between the contrast-enhanced (pseudoreference) and degraded images. Performance evaluation of the proposed method on three databases containing contrast distortions (CCID2014, TID2013, and CSIQ), indicates the promising performance of the proposed method.</li>
<li><strong>摘要：</strong>对比变化是影响图像质量的重要因素。在捕获图像期间，不利的照明条件会导致对比度变化和视觉质量损失。尽管已经提出了各种方法来评估不同扭曲（例如模糊和噪声）下图像的质量，但由于其视觉影响和属性与其他常规类型的扭曲不同，因此对比损坏在很大程度上被忽略了。在本文中，我们建议对对比度图像进行无参考图像质量评估（NR-IQA）度量。使用一组对比度增强算法，我们旨在生成视觉上接近实际参考图像的伪参考图像，从而使NR问题以更高的精度转换为全参考（FR）评估。为此，生产了大量的对比增强图像的数据集来训练一个可以根据图像内容和伪参考图像生成的图像内容和失真来选择最合适的对比度增强算法的分类网络。最后，以FR的方式进行评估，以评估对比度增强（伪率）和降解图像之间的质量差异。在包含对比畸变（CCID2014，TID2013和CSIQ）的三个数据库上提出的方法的性能评估，表明该方法的有希望的性能。</li>
</ul>

<h3>Title: Modeling Student Learning with 3.8 Million Program Traces</h3>
<ul>
<li><strong>Authors: </strong>Alexis Ross, Megha Srivastava, Jeremiah Blanchard, Jacob Andreas</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05056">https://arxiv.org/abs/2510.05056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05056">https://arxiv.org/pdf/2510.05056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05056]] Modeling Student Learning with 3.8 Million Program Traces(https://arxiv.org/abs/2510.05056)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As programmers write code, they often edit and retry multiple times, creating rich "interaction traces" that reveal how they approach coding tasks and provide clues about their level of skill development. For novice programmers in particular, these traces reflect the diverse reasoning processes they employ to code, such as exploratory behavior to understand how a programming concept works, re-strategizing in response to bugs, and personalizing stylistic choices. In this work, we explore what can be learned from training language models on such reasoning traces: not just about code, but about coders, and particularly students learning to program. We introduce a dataset of over 3.8 million programming reasoning traces from users of Pencil Code, a free online educational platform used by students to learn simple programming concepts. Compared to models trained only on final programs or synthetically-generated traces, we find that models trained on real traces are stronger at modeling diverse student behavior. Through both behavioral and probing analyses, we also find that many properties of code traces, such as goal backtracking or number of comments, can be predicted from learned representations of the students who write them. Building on this result, we show that we can help students recover from mistakes by steering code generation models to identify a sequence of edits that will results in more correct code while remaining close to the original student's style. Together, our results suggest that many properties of code are properties of individual students and that training on edit traces can lead to models that are more steerable, more predictive of student behavior while programming, and better at generating programs in their final states. Code and data is available at this https URL</li>
<li><strong>摘要：</strong>当程序员编写代码时，他们经常多次编辑和重试，创建丰富的“交互跟踪”，以揭示他们如何处理编码任务并提供有关其技能开发水平的线索。特别是对于新手程序员而言，这些痕迹反映了他们为编码所采用的各种推理过程，例如探索性行为，以了解编程概念如何工作，重新进行响应错误以及个性化样式选择。在这项工作中，我们探讨了从培训语言模型上可以从此类推理轨迹中学到的知识：不仅是关于代码，还涉及编码人员，尤其是学习编程的学生。我们从铅笔代码的用户中介绍了一个超过380万个编程推理痕迹的数据集，铅笔代码的用户是一个免费的在线教育平台，用于学习简单的编程概念。与仅在最终程序或合成生成的痕迹上训练的模型相比，我们发现在真实痕迹上训练的模型在对多样化的学生行为进行建模方面更强。通过行为和探测分析，我们还发现，可以从撰写它们的学生的学会来预测代码迹线的许多属性，例如目标回溯或评论数量。在此结果的基础上，我们表明，我们可以通过转向代码生成模型来帮助学生从错误中恢复，以确定一系列编辑，这些序列将导致更正确的代码，同时保持与原始学生的样式。总之，我们的结果表明，代码的许多属性是个别学生的属性，并且编辑轨迹的培训可以导致模型更可进入，在编程时更加可预测学生的行为，并且更好地在其最终状态下生成程序。代码和数据可在此HTTPS URL上找到</li>
</ul>

<h3>Title: MICROTRIPS: MICRO-geography TRavel Intelligence and Pattern Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Yangyang Wang, Tayo Fabusuyi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05080">https://arxiv.org/abs/2510.05080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05080">https://arxiv.org/pdf/2510.05080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05080]] MICROTRIPS: MICRO-geography TRavel Intelligence and Pattern Synthesis(https://arxiv.org/abs/2510.05080)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This study presents a novel small-area estimation framework to enhance urban transportation planning through detailed characterization of travel behavior. Our approach improves on the four-step travel model by employing publicly available microdata files and machine learning methods to predict travel behavior for a representative, synthetic population at small geographic areas. This approach enables high-resolution estimation of trip generation, trip distribution, mode choice, and route assignment. Validation using ACS/PUMS work-commute datasets demonstrates that our framework achieves higher accuracy compared to conventional approaches. The resulting granular insights enable the tailoring of interventions to address localized situations and support a range of policy applications and targeted interventions, including the optimal placement of micro-fulfillment centers, effective curb-space management, and the design of more inclusive transportation solutions particularly for vulnerable communities.</li>
<li><strong>摘要：</strong>这项研究提出了一个新型的小区域估计框架，以通过详细的旅行行为表征来增强城市运输计划。我们的方法通过采用公开可用的微数据文件和机器学习方法来改善四步旅行模型，以预测小地理区域中代表性的合成人群的旅行行为。这种方法可以对旅行，跳闸分布，模式选择和路线分配的高分辨率估计。使用ACS/PUMS的验证工作 - 交换数据集表明，与常规方法相比，我们的框架具有更高的准确性。由此产生的颗粒状见解使干预措施可以针对本地情况进行裁缝，并支持一系列政策应用和有针对性的干预措施，包括最佳的微型填充中心，有效的缘距离管理，以及对更具包容性的运输解决方案的设计，尤其是针对弱势社区。</li>
</ul>

<h3>Title: Factuality Matters: When Image Generation and Editing Meet Structured Visuals</h3>
<ul>
<li><strong>Authors: </strong>Le Zhuo, Songhao Han, Yuandong Pu, Boxiang Qiu, Sayak Paul, Yue Liao, Yihao Liu, Jie Shao, Xi Chen, Si Liu, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05091">https://arxiv.org/abs/2510.05091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05091">https://arxiv.org/pdf/2510.05091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05091]] Factuality Matters: When Image Generation and Editing Meet Structured Visuals(https://arxiv.org/abs/2510.05091)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>While modern visual generation models excel at creating aesthetically pleasing natural images, they struggle with producing or editing structured visuals like charts, diagrams, and mathematical figures, which demand composition planning, text rendering, and multimodal reasoning for factual fidelity. To address this, we present the first comprehensive, systematic investigation of this domain, encompassing data construction, model training, and an evaluation benchmark. First, we construct a large-scale dataset of 1.3 million high-quality structured image pairs derived from executable drawing programs and augmented with chain-of-thought reasoning annotations. Building on it, we train a unified model that integrates a VLM with FLUX.1 Kontext via a lightweight connector for enhanced multimodal understanding. A three-stage training curriculum enables progressive feature alignment, knowledge infusion, and reasoning-augmented generation, further boosted by an external reasoner at inference time. Finally, we introduce StructBench, a novel benchmark for generation and editing with over 1,700 challenging instances, and an accompanying evaluation metric, StructScore, which employs a multi-round Q\&A protocol to assess fine-grained factual accuracy. Evaluations of 15 models reveal that even leading closed-source systems remain far from satisfactory. Our model attains strong editing performance, and inference-time reasoning yields consistent gains across diverse architectures. By releasing the dataset, model, and benchmark, we aim to advance unified multimodal foundations for structured visuals.</li>
<li><strong>摘要：</strong>尽管现代视觉生成模型在创建美学上令人愉悦的自然图像方面表现出色，但它们在制作或编辑结构化视觉效果（例如图表，图表和数学人物）方面努力，这些视觉效果需要组成计划，文本渲染以及多模式推理，以实现事实忠诚。为了解决这个问题，我们介绍了该领域的首次全面，系统的研究，其中包括数据构建，模型培训和评估基准。首先，我们构建了一个大规模数据集，该数据集由130万个高质量的结构图像对构建，这些图像对源自可执行的绘图程序，并通过经过思考的推理注释进行了增强。在其上，我们训练一个统一的模型，该模型将VLM与Flux集成在一起。1通过轻质连接器Kontext，以增强多模式的理解。一个三阶段的培训课程可以使渐进的特征对齐，知识输液和推理提升的生成，并在推理时外部推理者进一步提高。最后，我们介绍了结构基台，这是一种新颖的基准，用于生成和编辑，具有1,700多个具有挑战性的实例，以及随附的评估度量标准，即结构Score，它采用了多轮Q \＆A协议来评估细粒度的事实准确性。对15个模型的评估表明，即使是领先的封闭源系统也远非令人满意。我们的模型达到了强大的编辑性能，推理时间推理可以在各种体系结构之间产生一致的收益。通过释放数据集，模型和基准，我们旨在推动统一的多模式基础的结构化视觉效果。</li>
</ul>

<h3>Title: Character Mixing for Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Tingting Liao, Chongjian Ge, Guangyi Liu, Hao Li, Yi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05093">https://arxiv.org/abs/2510.05093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05093">https://arxiv.org/pdf/2510.05093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05093]] Character Mixing for Video Generation(https://arxiv.org/abs/2510.05093)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where characters interact naturally across different worlds? We study inter-character interaction in text-to-video generation, where the key challenge is to preserve each character's identity and behaviors while enabling coherent cross-context interaction. This is difficult because characters may never have coexisted and because mixing styles often causes style delusion, where realistic characters appear cartoonish or vice versa. We introduce a framework that tackles these issues with Cross-Character Embedding (CCE), which learns identity and behavioral logic across multimodal sources, and Cross-Character Augmentation (CCA), which enriches training with synthetic co-existence and mixed-style data. Together, these techniques allow natural interactions between previously uncoexistent characters without losing stylistic fidelity. Experiments on a curated benchmark of cartoons and live-action series with 10 characters show clear improvements in identity preservation, interaction quality, and robustness to style delusion, enabling new forms of generative this http URL results and videos are available on our project page: this https URL.</li>
<li><strong>摘要：</strong>想象一下，憨豆先生走进《汤姆和杰瑞》中，我们能否生成角色在不同世界中自然互动的视频？我们研究文本到视频生成中的角色间交互，其中的关键挑战是保留每个角色的身份和行为，同时实现连贯的跨上下文交互。这很困难，因为角色可能永远不会共存，而且混合风格常常会导致风格错觉，即现实的角色显得卡通化，反之亦然。我们引入了一个框架，通过跨字符嵌入（CCE）和跨字符增强（CCA）来解决这些问题，跨字符嵌入（CCE）可以跨多模式源学习身份和行为逻辑，而跨字符增强（CCA）可以通过合成共存和混合风格数据丰富训练。这些技术共同实现了以前不共存的角色之间的自然互动，而不会失去风格保真度。对包含 10 个角色的卡通和真人连续剧的策划基准进行的实验表明，在身份保留、交互质量和对风格错觉的鲁棒性方面有明显的改进，从而实现了新形式的生成此 http URL 结果和视频，可在我们的项目页面上找到：此 https URL。</li>
</ul>

<h3>Title: VChain: Chain-of-Visual-Thought for Reasoning in Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Huang, Ning Yu, Gordon Chen, Haonan Qiu, Paul Debevec, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05094">https://arxiv.org/abs/2510.05094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05094">https://arxiv.org/pdf/2510.05094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05094]] VChain: Chain-of-Visual-Thought for Reasoning in Video Generation(https://arxiv.org/abs/2510.05094)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recent video generation models can produce smooth and visually appealing clips, but they often struggle to synthesize complex dynamics with a coherent chain of consequences. Accurately modeling visual outcomes and state transitions over time remains a core challenge. In contrast, large language and multimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and future prediction capabilities. To bridge these strengths, we introduce VChain, a novel inference-time chain-of-visual-thought framework that injects visual reasoning signals from multimodal models into video generation. Specifically, VChain contains a dedicated pipeline that leverages large multimodal models to generate a sparse set of critical keyframes as snapshots, which are then used to guide the sparse inference-time tuning of a pre-trained video generator only at these key moments. Our approach is tuning-efficient, introduces minimal overhead and avoids dense supervision. Extensive experiments on complex, multi-step scenarios show that VChain significantly enhances the quality of generated videos.</li>
<li><strong>摘要：</strong>最近的视频生成模型可以产生光滑且视觉上吸引人的剪辑，但它们常常难以通过一致的后果链综合复杂的动态。随着时间的流逝，准确地对视觉结果和状态转变进行建模仍然是一个核心挑战。相反，大语言和多模型模型（例如GPT-4O）具有强大的视觉状态推理和未来的预测能力。为了弥合这些优势，我们介绍了Vchain，这是一种新颖的推理式思想链框架，将视觉推理信号从多模型模型注入视频生成。具体而言，VCHAIN包含一条专用管道，该管道利用大型的多模型模型生成一组稀疏的关键密钥帧作为快照，然后将其用于指导仅在这些关键时刻的预训练视频生成器的稀疏推理时间调整。我们的方法是调整效率，引入了最小的开销，并避免了密集的监督。关于复杂的多步骤场景的广泛实验表明，VCHAIN显着提高了生成的视频的质量。</li>
</ul>

<h3>Title: From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Mingkang Zhu, Xi Chen, Bei Yu, Hengshuang Zhao, Jiaya Jia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05095">https://arxiv.org/abs/2510.05095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05095">https://arxiv.org/pdf/2510.05095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05095]] From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models(https://arxiv.org/abs/2510.05095)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Large reasoning models (LRMs) generate intermediate reasoning traces before producing final answers, yielding strong gains on multi-step and mathematical tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for model deployment, remains underexplored. The statistically correct objective for preference alignment requires marginalizing over reasoning traces, but this computation is intractable in practice. A common workaround optimizes a single sampled trajectory, which introduces substantial gradient variance from stochastic trace sampling. To address this challenge, we frame preference optimization for LRMs through the lens of the bias--variance trade-off and propose Bias--Variance Optimized Preference Optimization (BVPO), a simple, drop-in method that mixes two gradient estimators: a high-variance trace-based estimator and a low-variance empty-trace estimator obtained by disabling reasoning trace generation. Our theory shows that BVPO strictly reduces trace-induced variance for any nontrivial mixture, provides a closed-form choice of the mixing weight that minimizes mean-squared error relative to the true marginal gradient, and under standard smoothness and step-size conditions, tightens classical convergence bounds for stochastic gradient descent. Empirically, BVPO improves alignment over the best baseline by up to 7.8 points on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on general conversational data, BVPO also boosts reasoning performance for base models by up to 4.0 points on the average of six math reasoning benchmarks. These results identify variance from trace sampling as a key bottleneck and demonstrate that directly optimizing the bias--variance trade-off yields more stable training and stronger overall performance.</li>
<li><strong>摘要：</strong>大型推理模型（LRMS）在产生最终答案之前会生成中间推理痕迹，从而在多步骤和数学任务上产生巨大的收益。然而，将LRM与人类偏好保持一致，这是模型部署的关键先决条件，仍然没有被忽视。偏好对齐的统计正确目标需要在推理轨迹上边缘化，但是该计算在实践中是棘手的。常见的解决方法优化了单个采样轨迹，该轨迹引入了随机痕量采样的实质梯度方差。为了应对这一挑战，我们通过偏见的镜头对LRM进行了优化优化 - 差异和提出偏见 - 优化优化优先优化（BVPO），一种简单的，液位的方法，可以将两个梯度估计器混合在一起：高空痕量痕量估计器和低差异的空trace估算器通过淡淡的痕量痕量痕迹估算，从而降低了痕量。我们的理论表明，BVPO严格降低了任何非平凡混合物的痕量诱导的方差，提供了混合重量的封闭形式选择，相对于真实的边缘梯度，在标准平滑度和阶跃尺寸的条件下，将均值误差最小化，并在标准尺寸的条件下，紧缩了经典梯度的经典渐变界限。从经验上讲，BVPO在Alpacaeval 〜2和6.8分的竞技场上将最佳基线比对准高达7.8分。尽管仅接受了一般对话数据的培训，但BVPO还将基本模型的推理性能提高了4.0分，平均六个数学推理基准。这些结果将痕量采样的差异确定为关键瓶颈，并证明直接优化偏见 - 差异的权衡会产生更稳定的训练和更强的整体性能。</li>
</ul>

<h3>Title: Paper2Video: Automatic Video Generation from Scientific Papers</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Zhu, Kevin Qinghong Lin, Mike Zheng Shou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.MA, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05096">https://arxiv.org/abs/2510.05096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05096">https://arxiv.org/pdf/2510.05096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05096]] Paper2Video: Automatic Video Generation from Scientific Papers(https://arxiv.org/abs/2510.05096)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video. Unlike natural video, presentation video generation involves distinctive challenges: inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker. To address these challenges, we introduce PaperTalker, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics--Meta Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos convey the paper's information to the audience. Building on this foundation, we propose PaperTalker, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at this https URL.</li>
<li><strong>摘要：</strong>学术演示视频已成为研究沟通的重要媒介，但生产它们仍然是高度劳动力密集的，通常需要数小时的幻灯片设计，录制和编辑，并在短短的2到10分钟视频中进行。与自然视频不同，介绍视频的生成涉及独特的挑战：研究论文，密集的多模式信息（文本，图形，表格）以及需要协调多个校准渠道的需求，例如幻灯片，字幕，语音，语音和人类谈话者。为了应对这些挑战，我们介绍了PaperTalker，这是101篇研究论文的第一个基准，并配对由作者创建的演示视频，幻灯片和扬声器元数据。我们进一步设计了四个量身定制的评估指标 -  Meta相似性，Presentarena，Presentquiz和IP内存 - 衡量视频如何将论文的信息传达给受众。在这个基础的基础上，我们提出了PaperTalker，这是第一个用于学术演示视频生成的多代理框架。它通过新颖的有效的树搜索视觉选择，光标接地，字幕，语音合成和说话头渲染，将幻灯片的生成与有效的布局精致整合在一起，同时使幻灯片生成的生成以提高效率。 Paper2Video上的实验表明，与现有基线相比，我们方法制作的演示视频比现有基线更忠实，更有信息，这是朝着自动化和即用的学术视频生成迈出的实用一步。我们的数据集，代理和代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration</h3>
<ul>
<li><strong>Authors: </strong>Cheng Xin, Fan Xu, Xin Ding, Jie Gao, Jiaxin Ding</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CG, math.AT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05102">https://arxiv.org/abs/2510.05102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05102">https://arxiv.org/pdf/2510.05102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05102]] TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration(https://arxiv.org/abs/2510.05102)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have shown remarkable success across various scientific fields, yet their adoption in critical decision-making is often hindered by a lack of interpretability. Recently, intrinsically interpretable GNNs have been studied to provide insights into model predictions by identifying rationale substructures in graphs. However, existing methods face challenges when the underlying rationale subgraphs are complex and varied. In this work, we propose TopInG: Topologically Interpretable Graph Learning, a novel topological framework that leverages persistent homology to identify persistent rationale subgraphs. TopInG employs a rationale filtration learning approach to model an autoregressive generation process of rationale subgraphs, and introduces a self-adjusted topological constraint, termed topological discrepancy, to enforce a persistent topological distinction between rationale subgraphs and irrelevant counterparts. We provide theoretical guarantees that our loss function is uniquely optimized by the ground truth under specific conditions. Extensive experiments demonstrate TopInG's effectiveness in tackling key challenges, such as handling variform rationale subgraphs, balancing predictive performance with interpretability, and mitigating spurious correlations. Results show that our approach improves upon state-of-the-art methods on both predictive accuracy and interpretation quality.</li>
<li><strong>摘要：</strong>图形神经网络（GNN）在各个科学领域都取得了巨大的成功，但是缺乏解释性通常会阻碍它们在批判决策中的采用。最近，已经研究了可解释的GNN，以通过识别图中的基本原理子结构来提供对模型预测的见解。但是，当基本的基本原理子图复杂且多样化时，现有方法面临挑战。在这项工作中，我们提出了饰面：拓扑解释的图形学习，这是一种新型的拓扑框架，利用持续的同源性来识别持续的理由子图。 Toping采用理由过滤学习方法来建模基本子图的自回归产生过程，并引入了自我调整的拓扑约束，称为拓扑差异，以实施实现拓扑结构的拓扑结构，并具有理由分数和无关的对应物。我们提供了理论上的保证，即在特定条件下，地面真理可以唯一优化我们的损失函数。广泛的实验表明，Toping在应对关键挑战方面的有效性，例如处理变异的理由子图，平衡预测性能与可解释性以及减轻虚假相关性。结果表明，我们的方法对预测精度和解释质量的最先进方法有所改善。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
