<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-15</h1>
<h3>Title: A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Miao, Dingxin Lu, Zhuqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.09721">https://arxiv.org/abs/2509.09721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.09721">https://arxiv.org/pdf/2509.09721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.09721]] A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval(https://arxiv.org/abs/2509.09721)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generation</a></li>
<li><strong>Abstract: </strong>After natural disasters, accurate evaluations of damage to housing are important for insurance claims response and planning of resources. In this work, we introduce a novel multimodal retrieval-augmented generation (MM-RAG) framework. On top of classical RAG architecture, we further the framework to devise a two-branch multimodal encoder structure that the image branch employs a visual encoder composed of ResNet and Transformer to extract the characteristic of building damage after disaster, and the text branch harnesses a BERT retriever for the text vectorization of posts as well as insurance policies and for the construction of a retrievable restoration index. To impose cross-modal semantic alignment, the model integrates a cross-modal interaction module to bridge the semantic representation between image and text via multi-head attention. Meanwhile, in the generation module, the introduced modal attention gating mechanism dynamically controls the role of visual evidence and text prior information during generation. The entire framework takes end-to-end training, and combines the comparison loss, the retrieval loss and the generation loss to form multi-task optimization objectives, and achieves image understanding and policy matching in collaborative learning. The results demonstrate superior performance in retrieval accuracy and classification index on damage severity, where the Top-1 retrieval accuracy has been improved by 9.6%.</li>
<li><strong>摘要：</strong>自然灾害之后，对住房损害的准确评估对于保险索赔的响应和资源规划至关重要。在这项工作中，我们介绍了一种新型的多模式检索生成一代（MM-rag）框架。除了古典的抹布体系结构外，我们还为设计一个两条多模式编码结构的框架提供了一个框架，该结构采用了由重新NET和变压器组成的视觉编码器来提取灾难后建筑损失的特征，并且文本分支在灾难后构建了构建损失的特征。为了强加跨模式的语义对齐，该模型集成了一个跨模式相互作用模块，以通过多头注意力桥接图像和文本之间的语义表示。同时，在发电模块中，引入的模态注意门控机制动态控制了在发电过程中视觉证据和文本的作用。整个框架需要端到端的培训，并结合了比较损失，检索损失和一代损失，以形成多任务优化目标，并在协作学习中实现图像理解和策略匹配。结果表明，在损伤严重性方面的检索准确性和分类指数方面表现出了卓越的性能，在这种情况下，TOP-1检索精度已提高了9.6％。</li>
</ul>

<h3>Title: MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance</h3>
<ul>
<li><strong>Authors: </strong>Kaikai Zhao, Zhaoxiang Liu, Peng Wang, Xin Wang, Zhicheng Ma, Yajun Xu, Wenjing Zhang, Yibing Nan, Kai Wang, Shiguo Lian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.09730">https://arxiv.org/abs/2509.09730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.09730">https://arxiv.org/pdf/2509.09730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.09730]] MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance(https://arxiv.org/abs/2509.09730)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>General-domain large multimodal models (LMMs) have achieved significant advances in various image-text tasks. However, their performance in the Intelligent Traffic Surveillance (ITS) domain remains limited due to the absence of dedicated multimodal datasets. To address this gap, we introduce MITS (Multimodal Intelligent Traffic Surveillance), the first large-scale multimodal benchmark dataset specifically designed for ITS. MITS includes 170,400 independently collected real-world ITS images sourced from traffic surveillance cameras, annotated with eight main categories and 24 subcategories of ITS-specific objects and events under diverse environmental conditions. Additionally, through a systematic data generation pipeline, we generate high-quality image captions and 5 million instruction-following visual question-answer pairs, addressing five critical ITS tasks: object and event recognition, object counting, object localization, background analysis, and event reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream LMMs on this dataset, enabling the development of ITS-specific applications. Experimental results show that MITS significantly improves LMM performance in ITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905 (+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to 0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the dataset, code, and models as open-source, providing high-value resources to advance both ITS and LMM research.</li>
<li><strong>摘要：</strong>通用大型多模型模型（LMM）在各种图像文本任务中取得了重大进步。但是，由于缺乏专用的多模式数据集，它们在智能交通监视（ITS）域中的性能仍然有限。为了解决这一差距，我们介绍了MITS（多模式智能交通监视），这是第一个专门为其设计的大型多模式基准数据集。 MITS包括170,400个独立收集的现实世界，其图像来自交通监视摄像机，在不同的环境条件下注释了八个主要类别和24个特定物体和事件的子类别。此外，通过系统的数据生成管道，我们生成了高质量的图像标题和500万个跟随视觉问题的指令对 - 答案对，解决了五个关键的任务：对象和事件识别，对象计数，对象本地化，背景分析和事件推理。为了证明MIT的有效性，我们在此数据集中微调了主流LMM，从而可以开发其特定的应用程序。实验结果表明，MIT在其应用中显着提高了LMM的性能，从而将LLAVA-1.5的性能从0.494提高到0.905（+83.2％），LLAVA-1.6，从0.678提高到0.678（+35.8％），qwen2-vl，qwen2-vl的0.584降至0.584（+584）（+584％）（+58.6％），以及+58.6％，QWER和QWER，以及QWER，QWER和QWER和QWER，以及QWER和QWER和QWER。 0.930（+27.0％）。我们将数据集，代码和模型作为开源，提供高价值资源来推进其和LMM研究。</li>
</ul>

<h3>Title: Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Md Fazle Rasul, Alanood Alqobaisi, Bruhadeshwar Bezawada, Indrakshi Ray</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.09742">https://arxiv.org/abs/2509.09742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.09742">https://arxiv.org/pdf/2509.09742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.09742]] Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning(https://arxiv.org/abs/2509.09742)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) allows multiple entities to train a shared model collaboratively. Its core, privacy-preserving principle is that participants only exchange model updates, such as gradients, and never their raw, sensitive data. This approach is fundamental for applications in domains where privacy and confidentiality are important. However, the security of this very mechanism is threatened by gradient inversion attacks, which can reverse-engineer private training data directly from the shared gradients, defeating the purpose of FL. While the impact of these attacks is known for image, text, and tabular data, their effect on video data remains an unexamined area of research. This paper presents the first analysis of video data leakage in FL using gradient inversion attacks. We evaluate two common video classification approaches: one employing pre-trained feature extractors and another that processes raw video frames with simple transformations. Our initial results indicate that the use of feature extractors offers greater resilience against gradient inversion attacks. We also demonstrate that image super-resolution techniques can enhance the frames extracted through gradient inversion attacks, enabling attackers to reconstruct higher-quality videos. Our experiments validate this across scenarios where the attacker has access to zero, one, or more reference frames from the target environment. We find that although feature extractors make attacks more challenging, leakage is still possible if the classifier lacks sufficient complexity. We, therefore, conclude that video data leakage in FL is a viable threat, and the conditions under which it occurs warrant further investigation.</li>
<li><strong>摘要：</strong>联合学习（FL）允许多个实体协作培训共享模型。它的核心，保护隐私的原则是，参与者仅交换模型更新（例如梯度），而从未开发过其原始的，敏感的数据。这种方法对于隐私和机密性很重要的领域中的应用是基础。但是，这种机制的安全性受到梯度反演攻击的威胁，该梯度反转攻击可以直接从共享梯度中反向工程私人培训数据，从而击败了FL的目的。尽管这些攻击的影响是图像，文本和表格数据，但它们对视频数据的影响仍然是研究的研究领域。本文使用梯度反转攻击对FL中的视频数据泄漏进行了首次分析。我们评估了两种常见的视频分类方法：一种采用预训练的功能提取器，另一种使用简单转换处理原始视频帧。我们的初始结果表明，功能提取器的使用为梯度反演攻击提供了更大的弹性。我们还证明了图像超分辨率技术可以增强通过梯度反转攻击提取的帧，从而使攻击者能够重建高质量的视频。我们的实验在攻击者可以从目标环境中访问零，一个或更多参考帧的情况下验证了这一点。我们发现，尽管功能提取器使攻击更具挑战性，但如果分类器缺乏足够的复杂性，泄漏仍然可能。因此，我们得出的结论是，FL中的视频数据泄漏是一个可行的威胁，并且发生的条件需要进一步调查。</li>
</ul>

<h3>Title: LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation</h3>
<ul>
<li><strong>Authors: </strong>Yiqun Shen, Song Yuan, Zhengze Zhang, Xiaoliang Wang, Daxin Jiang, Nguyen Cam-Tu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.09754">https://arxiv.org/abs/2509.09754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.09754">https://arxiv.org/pdf/2509.09754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.09754]] LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation(https://arxiv.org/abs/2509.09754)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>KV Cache is commonly used to accelerate LLM inference with long contexts, yet its high memory demand drives the need for cache compression. Existing compression methods, however, are largely heuristic and lack dynamic budget allocation. To address this limitation, we introduce a unified framework for cache compression by minimizing information loss in Transformer residual streams. Building on it, we analyze the layer attention output loss and derive a new metric to compare cache entries across heads, enabling layer-wise compression with dynamic head budgets. Additionally, by contrasting cross-layer information, we also achieve dynamic layer budgets. LAVa is the first unified strategy for cache eviction and dynamic budget allocation that, unlike prior methods, does not rely on training or the combination of multiple strategies. Experiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and InfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a new insight: dynamic layer budgets are crucial for generation tasks (e.g., code completion), while dynamic head budgets play a key role in extraction tasks (e.g., extractive QA). As a fully dynamic compression method, LAVa consistently maintains top performance across task types. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>KV缓存通常用于加速LLM使用长上下文，但其高内存需求驱动了缓存压缩的需求。但是，现有的压缩方法在很大程度上是启发式的，并且缺乏动态的预算分配。为了解决此限制，我们通过最大程度地减少变压器残差流中的信息丢失来引入一个统一的缓存压缩框架。在此基础上，我们分析了层的注意力输出损失，并得出了一个新的指标，以比较跨头的缓存条目，从而可以通过动态头部预算进行层的压缩。此外，与跨层信息相比，我们还实现了动态层预算。熔岩是缓存驱逐和动态预算分配的第一个统一策略，与先前的方法不同，它不依赖培训或多种策略的组合。实验基准（Longbench，针中的针刺，标尺和Infinitebench）证明了其优越性。此外，我们的实验揭示了一个新的见解：动态层预算对于生成任务至关重要（例如，代码完成），而动态头预算在提取任务（例如，提取质量请访问）中起关键作用。作为一种完全动态的压缩方法，熔岩始终在任务类型中保持最佳性能。我们的代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection</h3>
<ul>
<li><strong>Authors: </strong>Roshini Pulishetty, Mani Kishan Ghantasala, Keerthy Kaushik Dasoju, Niti Mangwani, Vishal Garimella, Aditya Mate, Somya Chatterjee, Yue Kang, Ehi Nosakhare, Sadid Hasan, Soundar Srinivasan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.09782">https://arxiv.org/abs/2509.09782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.09782">https://arxiv.org/pdf/2509.09782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.09782]] One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection(https://arxiv.org/abs/2509.09782)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The proliferation of large language models (LLMs) with varying computational costs and performance profiles presents a critical challenge for scalable, cost-effective deployment in real-world applications. We introduce a unified routing framework that leverages a single-head cross-attention mechanism to jointly model query and model embeddings, enabling dynamic selection of the optimal LLM for each input query. Our approach is evaluated on RouterBench, a large-scale, publicly available benchmark encompassing diverse LLM pools and domains. By explicitly capturing fine-grained query-model interactions, our router predicts both response quality and generation cost, achieving up to 6.6% improvement in Average Improvement in Quality (AIQ) and 2.9% in maximum performance over existing routers. To robustly balance performance and cost, we propose an exponential reward function that enhances stability across user preferences. The resulting architecture is lightweight, generalizes effectively across domains, and demonstrates improved efficiency compared to prior methods, establishing a new standard for cost-aware LLM routing.</li>
<li><strong>摘要：</strong>具有不同计算成本和性能概况的大型语言模型（LLM）的扩散为现实世界应用中可扩展的，具有成本效益的部署带来了关键的挑战。我们引入了一个统一的路由框架，该统一的路由框架利用单头交叉注意机制共同模拟查询和模型嵌入，从而为每个输入查询提供了最佳LLM的动态选择。我们的方法在Routerbench上进行了评估，Routerbench是一种大规模公开可用的基准测试，其中包含不同的LLM池和域。通过明确捕获细粒的查询模型相互作用，我们的路由器可以预测响应质量和发电成本，在平均质量提高（AIQ）的平均提高（AIQ）和2.9％的最高性能比现有路由器的平均成本高达6.6％。为了稳健地平衡性能和成本，我们提出了指数奖励功能，可以增强用户偏好的稳定性。最终的体系结构是轻巧的，在跨域中有效地概括了，并且与先前的方法相比，效率提高，建立了成本吸引的LLM路由的新标准。</li>
</ul>

<h3>Title: Latency and Token-Aware Test-Time Compute</h3>
<ul>
<li><strong>Authors: </strong>Jenny Y. Huang, Mehul Damani, Yousef El-Kurdi, Ramon Astudillo, Wei Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.09864">https://arxiv.org/abs/2509.09864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.09864">https://arxiv.org/pdf/2509.09864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.09864]] Latency and Token-Aware Test-Time Compute(https://arxiv.org/abs/2509.09864)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Inference-time scaling has emerged as a powerful way to improve large language model (LLM) performance by generating multiple candidate responses and selecting among them. However, existing work on dynamic allocation for test-time compute typically considers only parallel generation methods such as best-of-N, overlooking incremental decoding methods like beam search, and has largely ignored latency, focusing only on token usage. We formulate inference-time scaling as a problem of dynamic compute allocation and method selection, where the system must decide which strategy to apply and how much compute to allocate on a per-query basis. Our framework explicitly incorporates both token cost and wall-clock latency, the latter being critical for user experience and particularly for agentic workflows where models must issue multiple queries efficiently. Experiments on reasoning benchmarks show that our approach consistently outperforms static strategies, achieving favorable accuracy-cost trade-offs while remaining practical for deployment.</li>
<li><strong>摘要：</strong>推理时间缩放已成为一种通过产生多个候选响应并在其中选择来改善大型语言模型（LLM）绩效的有力方法。但是，对于测试时间计算的动态分配的现有工作通常仅考虑平行生成方法，例如最佳-N，忽略了Beam Search等增量解码方法，并且在很大程度上忽略了延迟，仅专注于令牌用法。我们将推理时间缩放作为动态计算分配和方法选择的问题，系统必须决定应用哪种策略以及每次分配多少计算。我们的框架明确结合了令牌成本和墙壁锁定的延迟，后者对于用户体验，尤其是对于模型必须有效发出多个查询的代理工作流程至关重要的。关于推理基准测试的实验表明，我们的方法始终超过静态策略，实现了良好的准确性成本折衷，同时保持了部署的实用性。</li>
</ul>

<h3>Title: DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yifei Wang, Wenbin Wang, Yong Luo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.09940">https://arxiv.org/abs/2509.09940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.09940">https://arxiv.org/pdf/2509.09940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.09940]] DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition(https://arxiv.org/abs/2509.09940)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Though Multimodal Intent Recognition (MIR) proves effective by utilizing rich information from multiple sources (e.g., language, video, and audio), the potential for intent-irrelevant and conflicting information across modalities may hinder performance from being further improved. Most current models attempt to fuse modalities by applying mechanisms like multi-head attention to unimodal feature sequences and then adding the result back to the original representation. This process risks corrupting the primary linguistic features with noisy or irrelevant non-verbal signals, as it often fails to capture the fine-grained, token-level influence where non-verbal cues should modulate, not just augment, textual meaning. To address this, we introduce DyKen-Hyena, which reframes the problem from feature fusion to processing modulation. Our model translates audio-visual cues into dynamic, per-token convolutional kernels that directly modulate textual feature extraction. This fine-grained approach achieves state-of-the-art results on the MIntRec and MIntRec2.0 benchmarks. Notably, it yields a +10.46% F1-score improvement in out-of-scope detection, validating that our method creates a fundamentally more robust intent representation.</li>
<li><strong>摘要：</strong>尽管多模式意图识别（MIR）通过利用来自多个来源（例如语言，视频和音频）的丰富信息而被证明有效，但跨模式的意图和相互矛盾的信息的潜力可能会阻碍性能进一步提高。大多数当前模型试图通过将多头注意力等机制应用于单峰特征序列，然后将结果添加回原始表示形式来融合方式。这个过程有可能用嘈杂或无关的非语言信号来破坏主要的语言特征，因为它通常无法捕获非语言提示应该调节，而不仅仅是增加文本含义的细粒度，令牌级别的影响。为了解决这个问题，我们介绍了Dyken-Hyena，将问题从功能融合到处理调制。我们的模型将视听线索转换为动态的，to的卷积内核，这些内核直接调节文本特征提取。这种细粒度的方法可在MinTrec和MinTrec2.0基准上获得最先进的结果。值得注意的是，它在越界检测中产生了 +10.46％的F1得分提高，证明了我们的方法可以产生一种从根本上更健壮的意图表示。</li>
</ul>

<h3>Title: Limited Reference, Reliable Generation: A Two-Component Framework for Tabular Data Generation in Low-Data Regimes</h3>
<ul>
<li><strong>Authors: </strong>Mingxuan Jiang, Yongxin Wang, Ziyue Dai, Yicun Liu, Hongyi Nie, Sen Liu, Hongfeng Chai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.09960">https://arxiv.org/abs/2509.09960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.09960">https://arxiv.org/pdf/2509.09960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.09960]] Limited Reference, Reliable Generation: A Two-Component Framework for Tabular Data Generation in Low-Data Regimes(https://arxiv.org/abs/2509.09960)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Synthetic tabular data generation is increasingly essential in data management, supporting downstream applications when real-world and high-quality tabular data is insufficient. Existing tabular generation approaches, such as generative adversarial networks (GANs), diffusion models, and fine-tuned Large Language Models (LLMs), typically require sufficient reference data, limiting their effectiveness in domain-specific databases with scarce records. While prompt-based LLMs offer flexibility without parameter tuning, they often fail to capture dataset-specific feature-label dependencies and generate redundant data, leading to degradation in downstream task performance. To overcome these issues, we propose ReFine, a framework that (i) derives symbolic "if-then" rules from interpretable models and embeds them into prompts to explicitly guide generation toward domain-specific feature distribution, and (ii) applies a dual-granularity filtering strategy that suppresses over-sampling patterns and selectively refines rare but informative samples to reduce distributional imbalance. Extensive experiments on various regression and classification benchmarks demonstrate that ReFine consistently outperforms state-of-the-art methods, achieving up to 0.44 absolute improvement in R-squared for regression and 10.0 percent relative improvement in F1 score for classification tasks.</li>
<li><strong>摘要：</strong>合成表格数据生成在数据管理中越来越重要，当现实世界和高质量表格数据不足时，支持下游应用程序。现有的表格生成方法，例如生成对抗网络（GAN），扩散模型和微调的大语言模型（LLMS），通常需要足够的参考数据，从而限制了其在特定于域的数据库中的有效性。虽然基于及时的LLM提供灵活性而无需参数调整，但它们通常无法捕获数据集特定特征标签依赖项并生成冗余数据，从而导致下游任务性能下降。为了克服这些问题，我们提出了精炼，该框架（i）从可解释的模型中得出了象征性的“ if-then”规则，并将它们嵌入到提示中，以明确指导生成针对域特异性特征分布，并且（ii）应用双重粒度性过滤策略，以抑制过度绘制的模式和选择性地提高稀有样品，以减少稀有的样品，以减少稀有性的分配。关于各种回归和分类基准的广泛实验表明，精炼始终超过最先进的方法，可在R-Squared的回归中实现高达0.44的绝对改进，分类任务的F1分数相对提高10.0％。</li>
</ul>

<h3>Title: Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Aupendu Kar, Vishnu Raj, Guan-Ming Su</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.09971">https://arxiv.org/abs/2509.09971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.09971">https://arxiv.org/pdf/2509.09971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.09971]] Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey(https://arxiv.org/abs/2509.09971)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution</a></li>
<li><strong>Abstract: </strong>Event camera sensors are bio-inspired sensors which asynchronously capture per-pixel brightness changes and output a stream of events encoding the polarity, location and time of these changes. These systems are witnessing rapid advancements as an emerging field, driven by their low latency, reduced power consumption, and ultra-high capture rates. This survey explores the evolution of fusing event-stream captured with traditional frame-based capture, highlighting how this synergy significantly benefits various video restoration and 3D reconstruction tasks. The paper systematically reviews major deep learning contributions to image/video enhancement and restoration, focusing on two dimensions: temporal enhancement (such as frame interpolation and motion deblurring) and spatial enhancement (including super-resolution, low-light and HDR enhancement, and artifact reduction). This paper also explores how the 3D reconstruction domain evolves with the advancement of event driven fusion. Diverse topics are covered, with in-depth discussions on recent works for improving visual quality under challenging conditions. Additionally, the survey compiles a comprehensive list of openly available datasets, enabling reproducible research and benchmarking. By consolidating recent progress and insights, this survey aims to inspire further research into leveraging event camera systems, especially in combination with deep learning, for advanced visual media restoration and enhancement.</li>
<li><strong>摘要：</strong>事件摄像头传感器是由生物启发的传感器，它们异步捕获每个像素亮度会改变并输出编码这些变化的极性，位置和时间的事件流。这些系统正在目睹作为一个新兴领域的快速发展，这是由于其潜伏期低，功耗减少和超高捕获率所驱动。这项调查探讨了通过传统的基于框架的捕获捕获的融合事件流的演变，突出了该协同作用如何显着使各种视频恢复和3D重建任务受益。该论文系统地回顾了对图像/视频增强和恢复的主要深度学习贡献，重点是二维：时间增强（例如框架插值和运动去除）和空间增强功能（包括超级分辨率，低光和HDR和HDR增强，以及减少伪像减少）。本文还探讨了3D重建域如何随着事件驱动的融合的发展而发展。涵盖了各种主题，并深入讨论有关在具有挑战性的条件下改善视觉质量的最新作品。此外，该调查还汇总了公开可用数据集的全面列表，从而实现了可重复的研究和基准测试。通过巩固最近的进步和见解，该调查旨在激发人们对利用事件相机系统的进一步研究，尤其是与深度学习结合使用，以进行高级视觉媒体的恢复和增强。</li>
</ul>

<h3>Title: Few-Part-Shot Font Generation</h3>
<ul>
<li><strong>Authors: </strong>Masaki Akiba, Shumpei Takezaki, Daichi Haraguchi, Seiichi Uchida</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10006">https://arxiv.org/abs/2509.10006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10006">https://arxiv.org/pdf/2509.10006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10006]] Few-Part-Shot Font Generation(https://arxiv.org/abs/2509.10006)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>This paper proposes a novel model of few-part-shot font generation, which designs an entire font based on a set of partial design elements, i.e., partial shapes. Unlike conventional few-shot font generation, which requires entire character shapes for a couple of character classes, our approach only needs partial shapes as input. The proposed model not only improves the efficiency of font creation but also provides insights into how partial design details influence the entire structure of the individual characters.</li>
<li><strong>摘要：</strong>本文提出了一个新型的模型，该模型是少数部分的字体生成，该模型根据一组部分设计元素（即部分形状）设计了整个字体。与传统的几种字体生成不同，需要几个角色类别的整个角色形状，我们的方法只需要部分形状作为输入。提出的模型不仅提高了字体创建的效率，而且还提供了有关部分设计细节如何影响各个角色的整个结构的见解。</li>
</ul>

<h3>Title: LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA</h3>
<ul>
<li><strong>Authors: </strong>Jing Huang, Zhiya Tan, Shutao Gong, Fanwei Zeng, Jianshu Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10026">https://arxiv.org/abs/2509.10026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10026">https://arxiv.org/pdf/2509.10026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10026]] LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA(https://arxiv.org/abs/2509.10026)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As large vision language models (VLMs) advance, their capabilities in multilingual visual question answering (mVQA) have significantly improved. Chain-of-thought (CoT) reasoning has been proven to enhance interpretability and complex reasoning. However, most existing approaches rely primarily on textual CoT and provide limited support for multilingual multimodal reasoning, constraining their deployment in real-world applications. To address this gap, we introduce \textbf{LaV-CoT}, the first Language-aware Visual CoT framework with Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable multi-stage reasoning pipeline consisting of Text Summary with Bounding Box (BBox), Language Identification, Spatial Object-level Captioning, and Step-by-step Logical Reasoning. Following this reasoning pipeline, we design an automated data curation method that generates multilingual CoT annotations through iterative generation, correction, and refinement, enabling scalable and high-quality training data. To improve reasoning and generalization, LaV-CoT adopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT) with Language-aware Group Relative Policy Optimization (GRPO), guided by verifiable multi-aspect rewards including language consistency, structural accuracy, and semantic alignment. Extensive evaluations on public datasets including MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up to \(\sim\)9.5\% accuracy improvements over open-source baselines of similar size and even surpasses models with 2$\times$ larger scales by \(\sim\)2.6\%. Moreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513 and Gemini-2.5-flash. We further conducted an online A/B test to validate our method on real-world data, highlighting its effectiveness for industrial deployment. Our code is available at this link: \href{this https URL}</li>
<li><strong>摘要：</strong>随着大型视觉语言模型（VLM）的提高，它们在多语言视觉问题答案（MVQA）中的功能已大大提高。经过思考链（COT）推理已被证明可以增强可解释性和复杂的推理。但是，大多数现有的方法主要依赖于文本COT，并为多语言多模式推理提供了有限的支持，从而限制了其在现实世界应用程序中的部署。为了解决此差距，我们介绍了\ textbf {lav-cot}，这是具有多倍率奖励优化的第一语言感知的视觉COT框架。 LAV-COT结合了一个可解释的多阶段推理管道，该管道由文本摘要和边界框（Bbox），语言标识，空间对象级字幕和逐步逻辑推理组成。在此推理管道之后，我们设计了一种自动数据策划方法，该方法通过迭代生成，校正和改进来生成多语言的COT注释，从而启用可扩展和高质量的培训数据。为了改善推理和概括，Lav-Cot采用了两阶段的培训范式，将受监督的微调（SFT）与语言意识到的小组相对政策优化（GRPO）相结合，并以可验证的多种相关奖励为指导，包括语言一致性，结构性准确性和语义一致性。对包括MMMB，多语言MMBench和MTVQA在内的公共数据集进行了广泛的评估表明，LAV-COT最多可实现\（\ sim \）9.5 \％的准确性改进，而准确性改进了相似大小的开放源基线，甚至超过2 $ \ times $ \ times $ scales $ by（\ sim \ sim sim sim s \ sim \％）。此外，LAV-COT优于高级专有模型，例如GPT-4O-0513和Gemini-2.5-Flash。我们进一步进行了在线A/B测试，以验证我们对现实数据的方法，强调了其工业部署的有效性。我们的代码可在此链接上找到：\ href {this https url}</li>
</ul>

<h3>Title: Uncertainty-Aware Tabular Prediction: Evaluating VBLL-Enhanced TabPFN in Safety-Critical Medical Data</h3>
<ul>
<li><strong>Authors: </strong>Madhushan Ramalingam</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10048">https://arxiv.org/abs/2509.10048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10048">https://arxiv.org/pdf/2509.10048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10048]] Uncertainty-Aware Tabular Prediction: Evaluating VBLL-Enhanced TabPFN in Safety-Critical Medical Data(https://arxiv.org/abs/2509.10048)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Predictive models are being increasingly used across a wide range of domains, including safety-critical applications such as medical diagnosis and criminal justice. Reliable uncertainty estimation is a crucial task in such settings. Tabular Prior-data Fitted Network (TabPFN) is a recently proposed machine learning foundation model for tabular dataset, which uses a generative transformer architecture. Variational Bayesian Last Layers (VBLL) is a state-of-the-art lightweight variational formulation that effectively improves uncertainty estimation with minimal computational overhead. In this work we aim to evaluate the performance of VBLL integrated with the recently proposed TabPFN in uncertainty calibration. Our experiments, conducted on three benchmark medical tabular datasets, compare the performance of the original TabPFN and the VBLL-integrated version. Contrary to expectations, we observed that original TabPFN consistently outperforms VBLL integrated TabPFN in uncertainty calibration across all datasets.</li>
<li><strong>摘要：</strong>预测模型越来越多地在广泛的领域中使用，包括医学诊断和刑事司法等关键性应用。在这种情况下，可靠的不确定性估计是至关重要的任务。表格Prior-DATA拟合网络（TABPFN）是最近提出的Tagular DataSet的机器学习基础模型，该模型使用生成变压器体系结构。变分贝叶斯的最后一层（VBLL）是一种最先进的轻量级变异配方，可通过最小的计算开销有效地改善不确定性估计。在这项工作中，我们旨在评估在不确定性校准中与最近提出的TABPFN集成的VBLL的性能。我们的实验是在三个基准医疗表格数据集上进行的，比较了原始TABPFN和VBLL集成版本的性能。与期望相反，我们观察到原始TABPFN在所有数据集中的不确定性校准中始终优于VBLL Integrated TABPFN。</li>
</ul>

<h3>Title: Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation</h3>
<ul>
<li><strong>Authors: </strong>Sung-Lin Tsai, Bo-Lun Huang, Yu Ting Shen, Cheng Yu Yeo, Chiang Tseng, Bo-Kai Ruan, Wen-Sheng Lien, Hong-Han Shuai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10058">https://arxiv.org/abs/2509.10058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10058">https://arxiv.org/pdf/2509.10058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10058]] Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation(https://arxiv.org/abs/2509.10058)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Accurate color alignment in text-to-image (T2I) generation is critical for applications such as fashion, product visualization, and interior design, yet current diffusion models struggle with nuanced and compound color terms (e.g., Tiffany blue, lime green, hot pink), often producing images that are misaligned with human intent. Existing approaches rely on cross-attention manipulation, reference images, or fine-tuning but fail to systematically resolve ambiguous color descriptions. To precisely render colors under prompt ambiguity, we propose a training-free framework that enhances color fidelity by leveraging a large language model (LLM) to disambiguate color-related prompts and guiding color blending operations directly in the text embedding space. Our method first employs a large language model (LLM) to resolve ambiguous color terms in the text prompt, and then refines the text embeddings based on the spatial relationships of the resulting color terms in the CIELAB color space. Unlike prior methods, our approach improves color accuracy without requiring additional training or external reference images. Experimental results demonstrate that our framework improves color alignment without compromising image quality, bridging the gap between text semantics and visual generation.</li>
<li><strong>摘要：</strong>文本到图像（T2i）生成中的准确颜色对齐对于诸如时尚，产品可视化和室内设计等应用至关重要，但是当前的扩散模型与细微差别和复合的色彩术语（例如，蒂法尼蓝色，柠檬绿色，热粉红色），通常会使人类意图失误。现有方法依赖于交叉注意操作，参考图像或微调，但无法系统地解决含糊的颜色描述。为了在迅速歧义下精确地呈现颜色，我们提出了一个无训练的框架，该框架通过利用大型语言模型（LLM）来增强色彩保真度，以消除与颜色相关的提示，并在文本嵌入空间中直接指导颜色混合操作。我们的方法首先采用大型语言模型（LLM）来解决文本提示中的模棱两可的颜色项，然后根据CIELAB颜色空间中所得的颜色项的空间关系来完善文本嵌入。与先前的方法不同，我们的方法提高了颜色的准确性，而无需其他培训或外部参考图像。实验结果表明，我们的框架可以改善颜色对齐，而不会损害图像质量，从而弥合文本语义和视觉生成之间的差距。</li>
</ul>

<h3>Title: A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss</h3>
<ul>
<li><strong>Authors: </strong>MohammadAli Hamidi, Hadi Amirpour, Luigi Atzori, Christian Timmerer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10114">https://arxiv.org/abs/2509.10114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10114">https://arxiv.org/pdf/2509.10114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10114]] A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss(https://arxiv.org/abs/2509.10114)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Face image quality assessment (FIQA) plays a critical role in face recognition and verification systems, especially in uncontrolled, real-world environments. Although several methods have been proposed, general-purpose no-reference image quality assessment techniques often fail to capture face-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be computationally intensive, limiting their practical applicability. We propose a lightweight and efficient method for FIQA, designed for the perceptual evaluation of face images in the wild. Our approach integrates an ensemble of two compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2, with prediction-level fusion via simple averaging. To enhance alignment with human perceptual judgments, we employ a correlation-aware loss (MSECorrLoss), combining mean squared error (MSE) with a Pearson correlation regularizer. Our method achieves a strong balance between accuracy and computational cost, making it suitable for real-world deployment. Experiments on the VQualA FIQA benchmark demonstrate that our model achieves a Spearman rank correlation coefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient (PLCC) of 0.9894, remaining within competition efficiency constraints.</li>
<li><strong>摘要：</strong>面部图像质量评估（FIQA）在面部识别和验证系统中起着至关重要的作用，尤其是在不受控制的现实环境中。尽管已经提出了几种方法，但通用的无参考图像质量评估技术通常无法捕获特定面部的降解。同时，最新的FIQA模型往往是计算密集型的，从而限制了其实际适用性。我们为FIQA提出了一种轻巧有效的方法，该方法旨在对野外面部图像的感知评估。我们的方法集成了两个紧凑的卷积神经网络的合奏，即Mobilenetv3-Small和ShufflenetV2，并通过简单平均进行预测级融合。为了增强与人类感知判断的一致性，我们采用了相关性损失（MSECORRLOSS），将平均平方误差（MSE）与Pearson相关正常化程序相结合。我们的方法在准确性和计算成本之间取得了良好的平衡，使其适合于现实世界的部署。 VQUALA FIQA基准的实验表明，我们的模型达到了Spearman等级相关系数（SRCC）为0.9829，而Pearson线性相关系数（PLCC）为0.9894，在竞争效率限制内保留。</li>
</ul>

<h3>Title: Realism Control One-step Diffusion for Real-World Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Zongliang Wu, Siming Zheng, Peng-Tao Jiang, Xin Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10122">https://arxiv.org/abs/2509.10122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10122">https://arxiv.org/pdf/2509.10122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10122]] Realism Control One-step Diffusion for Real-World Image Super-Resolution(https://arxiv.org/abs/2509.10122)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, super-resolution</a></li>
<li><strong>Abstract: </strong>Pre-trained diffusion models have shown great potential in real-world image super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions. While one-step diffusion (OSD) methods significantly improve efficiency compared to traditional multi-step approaches, they still have limitations in balancing fidelity and realism across diverse scenarios. Since the OSDs for SR are usually trained or distilled by a single timestep, they lack flexible control mechanisms to adaptively prioritize these competing objectives, which are inherently manageable in multi-step methods through adjusting sampling steps. To address this challenge, we propose a Realism Controlled One-step Diffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping strategy that enables explicit control over fidelity-realism trade-offs during the noise prediction phase with minimal training paradigm modifications and original training data. A degradation-aware sampling strategy is also introduced to align distillation regularization with the grouping strategy and enhance the controlling of trade-offs. Moreover, a visual prompt injection module is used to replace conventional text prompts with degradation-aware visual tokens, enhancing both restoration accuracy and semantic consistency. Our method achieves superior fidelity and perceptual quality while maintaining computational efficiency. Extensive experiments demonstrate that RCOD outperforms state-of-the-art OSD methods in both quantitative metrics and visual qualities, with flexible realism control capabilities in the inference stage. The code will be released.</li>
<li><strong>摘要：</strong>预先训练的扩散模型通过启用高分辨率重建，在现实世界图像超分辨率（实际ISR）任务中显示出巨大的潜力。与传统的多步方法相比，一步扩散（OSD）方法可显着提高效率，但它们在平衡各种情况下的忠诚度和现实主义方面仍然存在局限性。由于SR的OSD通常是通过单个时间段训练或蒸馏的，因此它们缺乏灵活的控制机制来适应优先考虑这些竞争目标，这些目标可以通过调整采样步骤来固有地在多步法中进行管理。为了应对这一挑战，我们提出了一个现实主义控制的一步扩散（RCOD）框架。 RCOD提供了一种潜在的域分组策略，该策略可以通过最小的训练范式修改和原始培训数据来明确控制噪声预测阶段中对忠诚现实主义权衡的权衡。还引入了一种降解感知的抽样策略，以使蒸馏正规化与分组策略保持一致，并增强了权衡的控制。此外，使用视觉提示注射模块用于用降解感知的视觉令牌替换传统的文本提示，从而提高了恢复精度和语义一致性。我们的方法在保持计算效率的同时，达到了卓越的忠诚度和知觉质量。广泛的实验表明，在定量指标和视觉质量中，RCOD优于最先进的OSD方法，在推理阶段具有灵活的现实主义控制能力。代码将发布。</li>
</ul>

<h3>Title: Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization</h3>
<ul>
<li><strong>Authors: </strong>Yifan Chang, Jie Qin, Limeng Qiao, Xiaofeng Wang, Zheng Zhu, Lin Ma, Xingang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10140">https://arxiv.org/abs/2509.10140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10140">https://arxiv.org/pdf/2509.10140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10140]] Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization(https://arxiv.org/abs/2509.10140)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Vector quantization (VQ) is a key component in discrete tokenizers for image generation, but its training is often unstable due to straight-through estimation bias, one-step-behind updates, and sparse codebook gradients, which lead to suboptimal reconstruction performance and low codebook usage. In this work, we analyze these fundamental challenges and provide a simple yet effective solution. To maintain high codebook usage in VQ networks (VQN) during learning annealing and codebook size expansion, we propose VQBridge, a robust, scalable, and efficient projector based on the map function method. VQBridge optimizes code vectors through a compress-process-recover pipeline, enabling stable and effective codebook training. By combining VQBridge with learning annealing, our VQN achieves full (100%) codebook usage across diverse codebook configurations, which we refer to as FVQ (FullVQ). Through extensive experiments, we demonstrate that FVQ is effective, scalable, and generalizable: it attains 100% codebook usage even with a 262k-codebook, achieves state-of-the-art reconstruction performance, consistently improves with larger codebooks, higher vector channels, or longer training, and remains effective across different VQ variants. Moreover, when integrated with LlamaGen, FVQ significantly enhances image generation performance, surpassing visual autoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID, highlighting the importance of high-quality tokenizers for strong autoregressive image generation.</li>
<li><strong>摘要：</strong>向量量化（VQ）是离散图像生成的离散令牌中的关键组件，但是由于直通估计偏差，单步范围更新和稀疏代码簿梯度，其训练通常是不稳定的，这会导致次级重建性能性能和低代码书的使用。在这项工作中，我们分析了这些基本挑战，并提供了一个简单而有效的解决方案。为了在学习退火和代码簿扩展过程中维护VQ网络（VQN）中的高密码簿使用情况，我们建议使用MAP功能方法的VQBridge，这是一个可靠，可扩展且有效的投影仪。 VQBRIDGE通过压缩过程回收管道优化了代码向量，从而实现了稳定有效的密码培训。通过将VQBridge与学习退火相结合，我们的VQN在不同的代码簿配置中实现了完整的（100％）代码簿，我们称为FVQ（FULLVQ）。通过广泛的实验，我们证明了FVQ具有有效，可扩展性和可推广性：即使使用262k编码书，它也可以达到100％的代码书使用，可以实现最先进的重建性能，并通过较大的代码书，更高的矢量通道或更长的培训，并且在不同的VQ变种中保持有效。此外，当与Llamagen集成时，FVQ显着提高了图像产生性能，通过0.5超过了视觉自回归模型（VAR），而扩散模型（DIT）则通过0.2 RFID，突出了高质量的标记器对强产生良好的自动性图像产生的重要性。</li>
</ul>

<h3>Title: Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks</h3>
<ul>
<li><strong>Authors: </strong>Francisco Javier Esono Nkulu Andong, Qi Min</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10163">https://arxiv.org/abs/2509.10163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10163">https://arxiv.org/pdf/2509.10163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10163]] Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks(https://arxiv.org/abs/2509.10163)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>As sixth-generation (6G) networks move toward ultra-dense, intelligent edge environments, efficient resource management under stringent privacy, mobility, and energy constraints becomes critical. This paper introduces a novel Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that incorporates cross-layer orchestration of both the MAC layer and application layer for energy-efficient, privacy-preserving, and real-time resource management across heterogeneous edge devices. Each agent uses a Deep Recurrent Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum access, and CPU energy adaptation based on local observations (e.g., queue length, energy, CPU usage, and mobility). To protect privacy, we introduce a secure aggregation protocol based on elliptic curve Diffie Hellman key exchange, which ensures accurate model updates without exposing raw data to semi-honest adversaries. We formulate the resource management problem as a partially observable multi-agent Markov decision process (POMMDP) with a multi-objective reward function that jointly optimizes latency, energy efficiency, spectral efficiency, fairness, and reliability under 6G-specific service requirements such as URLLC, eMBB, and mMTC. Simulation results demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines in task success rate, latency, energy efficiency, and fairness, while ensuring robust privacy protection and scalability in dynamic, resource-constrained 6G edge networks.</li>
<li><strong>摘要：</strong>随着第六代（6G）网络朝着超密集的，智能的边缘环境，严格的隐私，移动性和能量限制下的有效资源管理变得至关重要。本文介绍了一个新颖的联邦多代理增强学习（FED-MARL）框架，该框架结合了MAC层和应用层的跨层编排，以跨异构边缘设备进行能效，隐私和实时资源管理。每个代理都使用深层Q-NETWORK（DRQN）来学习基于局部观测值（例如，排队长度，能量，CPU使用和移动性）的任务卸载，频谱访问和CPU能量适应的分散策略。为了保护隐私，我们基于椭圆曲线Diffie Hellman密钥交换引入了一个安全的聚合协议，该协议可确保准确的模型更新，而不会将原始数据暴露于半honest对手。我们将资源管理问题提出为一种可观察到的多代理马尔可夫决策过程（POMMDP），具有多目标奖励功能，该功能在6G特异性服务要求（例如URLLC，EMBB和MMTC）下共同优化了潜伏期，能效，光谱效率，公平性，公平性，公平性和可靠性。仿真结果表明，在任务成功率，延迟，能源效率和公平性方面，FED-MARL优于集中的MAL和启发式基线，同时确保了在动态，资源约束的6G边缘网络中的强大隐私保护和可扩展性。</li>
</ul>

<h3>Title: GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Haozhen Yan, Yan Hong, Suning Lang, Jiahui Zhan, Yikun Ji, Yujie Gao, Jun Lan, Huijia Zhu, Weiqiang Wang, Jianfu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10250">https://arxiv.org/abs/2509.10250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10250">https://arxiv.org/pdf/2509.10250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10250]] GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection(https://arxiv.org/abs/2509.10250)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>With generative models becoming increasingly sophisticated and diverse, detecting AI-generated images has become increasingly challenging. While existing AI-genereted Image detectors achieve promising performance on in-distribution generated images, their generalization to unseen generative models remains limited. This limitation is largely attributed to their reliance on generation-specific artifacts, such as stylistic priors and compression patterns. To address these limitations, we propose GAMMA, a novel training framework designed to reduce domain bias and enhance semantic alignment. GAMMA introduces diverse manipulation strategies, such as inpainting-based manipulation and semantics-preserving perturbations, to ensure consistency between manipulated and authentic content. We employ multi-task supervision with dual segmentation heads and a classification head, enabling pixel-level source attribution across diverse generative domains. In addition, a reverse cross-attention mechanism is introduced to allow the segmentation heads to guide and correct biased representations in the classification branch. Our method achieves state-of-the-art generalization performance on the GenImage benchmark, imporving accuracy by 5.8%, but also maintains strong robustness on newly released generative model such as GPT-4o.</li>
<li><strong>摘要：</strong>随着生成模型变得越来越复杂和多样化，检测AI生成的图像变得越来越具有挑战性。尽管现有的AI生成图像探测器在分布生成的图像上实现了有希望的性能，但它们对看不见的生成模型的概括仍然有限。这种限制在很大程度上归因于它们对特定于发电的文物的依赖，例如风格先验和压缩模式。为了解决这些局限性，我们提出了Gamma，这是一个新颖的培训框架，旨在减少域偏差并增强语义一致性。伽玛介绍了各种操纵策略，例如基于介入的操纵和语义保护扰动，以确保操纵和真实内容之间的一致性。我们采用多任务监督，具有双分割头和一个分类头，从而在不同的生成域中实现了像素级源归因。此外，引入了反向跨注意机制，以使分割头能够指导和纠正分类分支中的偏见表示。我们的方法在基因成基准上实现了最新的概括性能，使准确性提高了5.8％，但在新发布的生成模型（例如GPT-4O）上也保持了强大的鲁棒性。</li>
</ul>

<h3>Title: Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI</h3>
<ul>
<li><strong>Authors: </strong>Ema Masterl, Tina Vipotnik Vesnaver, Žiga Špiclin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10257">https://arxiv.org/abs/2509.10257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10257">https://arxiv.org/pdf/2509.10257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10257]] Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI(https://arxiv.org/abs/2509.10257)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce motion artifacts caused by fetal movement. However, these stacks are typically low resolution, may suffer from motion corruption, and do not adequately capture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to address these limitations by combining slice-to-volume registration and super-resolution techniques to generate high-resolution (HR) 3D volumes. While several SRR methods have been proposed, their comparative performance - particularly in pathological cases - and their influence on downstream volumetric analysis and diagnostic tasks remain underexplored. In this study, we applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to 140 fetal brain MRI scans, including both healthy controls (HC) and pathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was segmented using the BoUNTi algorithm to extract volumes of nine principal brain structures. We evaluated visual quality, SRR success rates, volumetric measurement agreement, and diagnostic classification performance. NeSVoR demonstrated the highest and most consistent reconstruction success rate (>90%) across both HC and PC groups. Although significant differences in volumetric estimates were observed between SRR methods, classification performance for VM was not affected by the choice of SRR method. These findings highlight NeSVoR's robustness and the resilience of diagnostic performance despite SRR-induced volumetric variability.</li>
<li><strong>摘要：</strong>胎儿大脑MRI依靠快速的多视图2D切片采集来减少由胎儿运动引起的运动伪像。但是，这些堆栈通常是低分辨率的，可能会遭受运动腐败，并且不能充分捕获3D解剖结构。超分辨率重建（SRR）方法旨在通过结合切片到体积的注册和超分辨率技术来解决这些局限性，以生成高分辨率（HR）3D体积。尽管已经提出了几种SRR方法，但它们的比较性能（尤其是在病理病例中）以及它们对下游体积分析和诊断任务的影响仍然没有得到反应。在这项研究中，我们将三种最先进的SRR方法（Niftymic，svrtk和nesvor）应用于140次胎儿脑MRI扫描，包括健康对照（HC）和病理病例（PC）（PC），该病例（PC）具有心室肿瘤（VM）。使用Bounti算法对每个HR重建进行分割，以提取9个主脑结构的体积。我们评估了视觉质量，SRR成功率，体积测量协议和诊断分类性能。 Nesvor在HC和PC组中表现出最高和最一致的重建成功率（> 90％）。尽管在SRR方法之间观察到体积估计值的显着差异，但VM的分类性能不受SRR方法的选择影响。这些发现突出了Nesvor的鲁棒性和诊断性能的弹性，尽管SRR引起的体积变化。</li>
</ul>

<h3>Title: MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Jia Wang, Jie Hu, Xiaoqi Ma, Hanghang Ma, Yanbing Zeng, Xiaoming Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10260">https://arxiv.org/abs/2509.10260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10260">https://arxiv.org/pdf/2509.10260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10260]] MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation(https://arxiv.org/abs/2509.10260)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) generation has achieved remarkable progress in instruction following and aesthetics. However, a persistent challenge is the prevalence of physical artifacts, such as anatomical and structural flaws, which severely degrade perceptual quality and limit application. Given the diversity and complexity of these artifacts, a systematic and fine-grained evaluation framework is required, which is lacking in current benchmarks. To fill this gap, we introduce MagicMirror, a comprehensive framework for artifacts assessment. We first establish a detailed taxonomy of generated image artifacts. Guided by this taxonomy, we manually annotate MagicData340K, the first human-annotated large-scale dataset of 340K generated images with fine-grained artifact labels. Building on this dataset, we train MagicAssessor, a Vision-Language Model (VLM) that provides detailed assessments and corresponding labels. To overcome challenges like class imbalance and reward hacking, we design a novel data sampling strategy and a multi-level reward system for Group Relative Policy Optimization (GRPO). Finally, we leverage MagicAssessor to construct MagicBench, an automated benchmark for evaluating the image artifacts of current T2I models. Our evaluation with MagicBench reveals that despite their widespread adoption, even top-tier models like GPT-image-1 are consistently plagued by significant artifacts, highlighting artifact reduction as a critical frontier for future T2I development. Project page: this https URL.</li>
<li><strong>摘要：</strong>文本对图像（T2i）的一代在以下教学和美学方面取得了显着进步。但是，持续的挑战是物理伪像的普遍性，例如解剖和结构缺陷，它们严重降低了感知质量并限制了应用。鉴于这些文物的多样性和复杂性，需要系统的，细粒度的评估框架，这在当前的基准中缺乏。为了填补这一空白，我们介绍了MagicMirror，这是一个用于文物评估的综合框架。我们首先建立了生成的图像文物的详细分类学。在这种分类法的指导下，我们手动注释MagicData340k，这是第一个由人类注销的大规模数据集的340K生成的带有细颗粒文物标签的图像。在此数据集的基础上，我们训练MagicAssessor，这是一种视觉语言模型（VLM），可提供详细的评估和相应的标签。为了克服诸如阶级失衡和奖励黑客攻击之类的挑战，我们设计了一种新颖的数据采样策略和组相对政策优化的多级奖励系统（GRPO）。最后，我们利用MagicAssessor来构建MagicBench，这是一种自动基准，用于评估当前T2I模型的图像伪像。我们对MagicBench的评估表明，尽管采用了广泛的采用，但即使是GPT-Image-1（例如GPT-Image-1）的顶级模型也始终受到重要的人工制品的困扰，从而突出了人工制品的减少，这是未来T2I开发的关键领域。项目页面：此HTTPS URL。</li>
</ul>

<h3>Title: Proof of AutoML: SDN based Secure Energy Trading with Blockchain in Disaster Case</h3>
<ul>
<li><strong>Authors: </strong>Salih Toprak, Muge Erel-Ozcevik</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10291">https://arxiv.org/abs/2509.10291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10291">https://arxiv.org/pdf/2509.10291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10291]] Proof of AutoML: SDN based Secure Energy Trading with Blockchain in Disaster Case(https://arxiv.org/abs/2509.10291)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>In disaster scenarios where conventional energy infrastructure is compromised, secure and traceable energy trading between solar-powered households and mobile charging units becomes a necessity. To ensure the integrity of such transactions over a blockchain network, robust and unpredictable nonce generation is vital. This study proposes an SDN-enabled architecture where machine learning regressors are leveraged not for their accuracy, but for their potential to generate randomized values suitable as nonce candidates. Therefore, it is newly called Proof of AutoML. Here, SDN allows flexible control over data flows and energy routing policies even in fragmented or degraded networks, ensuring adaptive response during emergencies. Using a 9000-sample dataset, we evaluate five AutoML-selected regression models - Gradient Boosting, LightGBM, Random Forest, Extra Trees, and K-Nearest Neighbors - not by their prediction accuracy, but by their ability to produce diverse and non-deterministic outputs across shuffled data inputs. Randomness analysis reveals that Random Forest and Extra Trees regressors exhibit complete dependency on randomness, whereas Gradient Boosting, K-Nearest Neighbors and LightGBM show strong but slightly lower randomness scores (97.6%, 98.8% and 99.9%, respectively). These findings highlight that certain machine learning models, particularly tree-based ensembles, may serve as effective and lightweight nonce generators within blockchain-secured, SDN-based energy trading infrastructures resilient to disaster conditions.</li>
<li><strong>摘要：</strong>在灾难场景中，传统能源基础设施受到妥协，安全和可追溯的能源交易之间的太阳能家庭和移动充电单元之间的能源交易成为必要。为了确保这种交易在区块链网络上的完整性，鲁棒和不可预测的非CE产生至关重要。这项研究提出了一个支持SDN的体系结构，在该体系结构中，机器学习回归器不是为了其准确性，而是为了产生适合Nonce候选者的随机值的潜力。因此，它新被称为Automl证明。在这里，SDN即使在零散或退化的网络中也可以灵活地控制数据流和能量路由策略，从而确保紧急情况下的适应性响应。我们使用9000个样本数据集，我们评估了五个自动选择的回归模型 - 梯度提升，LightGBM，随机森林，额外的树木和K-Nearest邻居 - 不是通过其预测准确性，而是通过它们在跨改组数据输入中产生多样化和非确定性输出的能力。随机性分析表明，随机森林和额外的树回收期对随机性表现出完全依赖性，而梯度提升，k-neartiment邻居和LightGBM表现出强大但略低的随机得分（分别为97.6％，98.8％和99.9％）。这些发现凸显了某些机器学习模型，尤其是基于树木的合奏，可以作为区块链安全的，基于SDN的能源交易基础设施中有效且轻巧的Nonce发电机。</li>
</ul>

<h3>Title: Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching</h3>
<ul>
<li><strong>Authors: </strong>Zhixin Zheng, Xinyu Wang, Chang Zou, Shaobo Wang, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10312">https://arxiv.org/abs/2509.10312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10312">https://arxiv.org/pdf/2509.10312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10312]] Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching(https://arxiv.org/abs/2509.10312)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Diffusion transformers have gained significant attention in recent years for their ability to generate high-quality images and videos, yet still suffer from a huge computational cost due to their iterative denoising process. Recently, feature caching has been introduced to accelerate diffusion transformers by caching the feature computation in previous timesteps and reusing it in the following timesteps, which leverage the temporal similarity of diffusion models while ignoring the similarity in the spatial dimension. In this paper, we introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and complementary perspective for previous feature caching. Specifically, ClusCa performs spatial clustering on tokens in each timestep, computes only one token in each cluster and propagates their information to all the other tokens, which is able to reduce the number of tokens by over 90%. Extensive experiments on DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image and text-to-video generation. Besides, it can be directly applied to any diffusion transformer without requirements for training. For instance, ClusCa achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing the original model by 0.51%. The code is available at this https URL.</li>
<li><strong>摘要：</strong>近年来，扩散变压器因其产生高质量的图像和视频的能力而引起了人们的关注，但由于其迭代剥落过程，仍遭受了巨大的计算成本。最近，引入了功能缓存以加速扩散变压器，通过缓存以前的时间步中的特征计算并在以下时间段中重复使用它，从以下时间段中，它利用了扩散模型的时间相似性，同时忽略了空间维度的相似性。在本文中，我们将集群驱动的功能缓存（CLUSCA）作为先前功能缓存的正交和互补透视图。具体而言，Clusca在每个时间步中的令牌上执行空间聚类，在每个群集中仅计算一个令牌，并将其信息传播到其他所有令牌，这能够将令牌数量降低超过90％。关于DIT，Flux和Hunyuanvideo的广泛实验证明了其在文本到图像和文本之间的有效性。此外，它可以直接应用于任何扩散变压器，而无需训练。例如，克鲁斯卡（Clusca）在通量上达到4.96倍的加速度，成像向99.49％达到了超过原始模型的0.51％。该代码可在此HTTPS URL上找到。</li>
</ul>

<h3>Title: GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT</h3>
<ul>
<li><strong>Authors: </strong>Botond Fazekas, Thomas Pinetz, Guilherme Aresta, Taha Emre, Hrvoje Bogunovic</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10341">https://arxiv.org/abs/2509.10341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10341">https://arxiv.org/pdf/2509.10341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10341]] GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT(https://arxiv.org/abs/2509.10341)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration</a></li>
<li><strong>Abstract: </strong>Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing and monitoring retinal diseases. However, OCT images are inherently degraded by speckle noise, which obscures fine details and hinders accurate interpretation. While numerous denoising methods exist, many struggle to balance noise reduction with the preservation of crucial anatomical structures. This paper introduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel deep learning approach for OCT image despeckling that leverages the strengths of diffusion probabilistic models. Unlike conventional diffusion models that assume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more accurately reflect the statistical properties of speckle. Furthermore, we introduce a Noise-Reduced Fidelity Term that utilizes a pre-processed, less-noisy image to guide the denoising process. This crucial addition prevents the reintroduction of high-frequency noise. We accelerate the inference process by adapting the Denoising Diffusion Implicit Model framework to our Gamma-based model. Experiments on a dataset with paired noisy and less-noisy OCT B-scans demonstrate that GARD significantly outperforms traditional denoising methods and state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE. Qualitative results confirm that GARD produces sharper edges and better preserves fine anatomical details.</li>
<li><strong>摘要：</strong>光学相干断层扫描（OCT）是用于诊断和监测视网膜疾病的重要成像方式。但是，OCT图像固有地被斑点噪声降低，这掩盖了细节并阻碍准确的解释。尽管存在许多降级方法，但许多人努力与至关重要的解剖结构保持平衡。本文介绍了GARD（基于伽玛的解剖恢复和Denoising），这是一种新型的深度学习方法，用于OCT幻想，利用扩散概率模型的优势。与采用高斯噪声的常规扩散模型不同，Gard采用脱氧扩散伽玛模型更准确地反映了Speckle的统计特性。此外，我们引入了一个降噪的保真度项，该项利用了预处理的，不足的图像来指导降落过程。这种关键的添加阻止了高频噪声的重新引入。我们通过将denoising扩散隐式模型框架调整为基于伽玛的模型来加速推理过程。与PSNR，SSIM和MSE一起，在具有配对嘈杂且较少的OCT B扫描的数据集中的实验表明，GARD显着优于传统的Dinoising方法和最先进的深度学习模型。定性结果证实，Gard会产生更清晰的边缘，并更好地保留精美的解剖细节。</li>
</ul>

<h3>Title: Towards Understanding Visual Grounding in Visual Language Models</h3>
<ul>
<li><strong>Authors: </strong>Georgios Pantazopoulos, Eda B. Özyiğit</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10345">https://arxiv.org/abs/2509.10345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10345">https://arxiv.org/pdf/2509.10345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10345]] Towards Understanding Visual Grounding in Visual Language Models(https://arxiv.org/abs/2509.10345)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Visual grounding refers to the ability of a model to identify a region within some visual input that matches a textual description. Consequently, a model equipped with visual grounding capabilities can target a wide range of applications in various domains, including referring expression comprehension, answering questions pertinent to fine-grained details in images or videos, caption visual context by explicitly referring to entities, as well as low and high-level control in simulated and real environments. In this survey paper, we review representative works across the key areas of research on modern general-purpose vision language models (VLMs). We first outline the importance of grounding in VLMs, then delineate the core components of the contemporary paradigm for developing grounded models, and examine their practical applications, including benchmarks and evaluation metrics for grounded multimodal generation. We also discuss the multifaceted interrelations among visual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally, we analyse the challenges inherent to visual grounding and suggest promising directions for future research.</li>
<li><strong>摘要：</strong>视觉接地是指模型在与文本描述匹配的某些视觉输入中识别区域的能力。因此，配备了视觉接地功能的模型可以针对各种领域的广泛应用，包括参考表达理解，回答与图像或视频中的细粒细节有关的问题，通过明确指向实体以及模拟和真实环境中的低水平和高级控制和高级控制和高级控制和高级控制和真实环境。在本调查文件中，我们审查了有关现代通用视觉语言模型（VLM）研究的主要研究领域的代表性作品。我们首先概述了在VLMS中进行基础的重要性，然后描述了开发扎根模型的当代范式的核心组成部分，并检查其实际应用，包括基准测量和评估指标，以实现扎根的多模态发电。我们还讨论了VLMS中视觉接地，多模式链和推理之间的多方面相互关系。最后，我们分析了视觉接地固有的挑战，并提出了未来研究的有希望的方向。</li>
</ul>

<h3>Title: Flow Straight and Fast in Hilbert Space: Functional Rectified Flow</h3>
<ul>
<li><strong>Authors: </strong>Jianxin Zhang, Clayton Scott</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10384">https://arxiv.org/abs/2509.10384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10384">https://arxiv.org/pdf/2509.10384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10384]] Flow Straight and Fast in Hilbert Space: Functional Rectified Flow(https://arxiv.org/abs/2509.10384)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Many generative models originally developed in finite-dimensional Euclidean space have functional generalizations in infinite-dimensional settings. However, the extension of rectified flow to infinite-dimensional spaces remains unexplored. In this work, we establish a rigorous functional formulation of rectified flow in an infinite-dimensional Hilbert space. Our approach builds upon the superposition principle for continuity equations in an infinite-dimensional space. We further show that this framework extends naturally to functional flow matching and functional probability flow ODEs, interpreting them as nonlinear generalizations of rectified flow. Notably, our extension to functional flow matching removes the restrictive measure-theoretic assumptions in the existing theory of \citet{kerrigan2024functional}. Furthermore, we demonstrate experimentally that our method achieves superior performance compared to existing functional generative models.</li>
<li><strong>摘要：</strong>许多最初在有限维欧几里得空间中开发的生成模型在无限维度中具有功能概括。然而，将整流流到无限维空间的扩展仍未开发。在这项工作中，我们建立了在无限二维的希尔伯特空间中对整流流的严格功能公式。我们的方法基于无限维空间中连续方程的叠加原理。我们进一步表明，该框架自然扩展到功能流匹配和功能概率流量，将其解释为整流流的非线性概括。值得注意的是，我们向功能流匹配的扩展删除了\ citet {kerrigan2024功能}的现有理论中的限制性度量理论假设。此外，我们通过实验证明，与现有的功能生成模型相比，我们的方法的性能优越。</li>
</ul>

<h3>Title: Inpainting-Guided Policy Optimization for Diffusion Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Siyan Zhao, Mengchen Liu, Jing Huang, Miao Liu, Chenyu Wang, Bo Liu, Yuandong Tian, Guan Pang, Sean Bell, Aditya Grover, Feiyu Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10396">https://arxiv.org/abs/2509.10396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10396">https://arxiv.org/pdf/2509.10396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10396]] Inpainting-Guided Policy Optimization for Diffusion Large Language Models(https://arxiv.org/abs/2509.10396)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Masked diffusion large language models (dLLMs) are emerging as promising alternatives to autoregressive LLMs, offering competitive performance while supporting unique generation capabilities such as inpainting. We explore how inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with reinforcement learning faces an exploration challenge: sparse reward signals and sample waste when models fail to discover correct solutions. While this inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided Policy Optimization), an RL framework that strategically inserts partial ground-truth reasoning traces during online sampling. Unlike providing full solutions, inpainting steers exploration toward promising trajectory spaces while preserving self-generated reasoning, bridging supervised fine-tuning and reinforcement learning. We apply IGPO to group-based optimization methods such as GRPO, where exploration failures cause zero advantages and gradients. IGPO restores meaningful gradients while improving sample efficiency. We also propose supervised fine-tuning on synthetically rewritten concise traces that better align with dLLM generation patterns. With additional techniques including entropy-based filtering, our training recipe yields substantial gains across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new state-of-the-art results for full-attention masked dLLMs.</li>
<li><strong>摘要：</strong>蒙版扩散大语言模型（DLLM）正在成为自回旋LLM的有前途的替代品，提供了竞争性能的同时支持独特的生成能力，例如inpaining inpainting。我们探索如何为DLLM的RL算法设计提供信息。将LLM与加固学习对准面临探索挑战：当模型未能发现正确的解决方案时，稀疏的奖励信号和样品浪费。虽然这种效率低下会影响LLM，但DLLM提供了独特的机会 - 他们的介绍能力可以指导探索。我们介绍了IGPO（介绍指导的政策优化），这是一个RL框架，可以在线抽样过程中策略性地插入部分地面真实性推理痕迹。与提供完整的解决方案不同，介绍了对有前途的轨迹空间的探索，同时保留了自我生成的推理，桥接了监督的微调和增强学习。我们将IGPO应用于基于组的优化方法，例如GRPO，探索失败会导致零优点和梯度。 IGPO恢复有意义的梯度，同时提高样品效率。我们还提出了对合成重写的简洁痕迹的监督微调，以更好地与DLLM生成模式保持一致。借助包括基于熵过滤的其他技术，我们的培训配方可在三个数学基准测试（GSM8K，MATH500和AMC）中获得可观的增长，从而为全注意到蒙面的DLLM提供了新的最先进结果。</li>
</ul>

<h3>Title: InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Tao Han, Wanghan Xu, Junchao Gong, Xiaoyu Yue, Song Guo, Luping Zhou, Lei Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10441">https://arxiv.org/abs/2509.10441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10441">https://arxiv.org/pdf/2509.10441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10441]] InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis(https://arxiv.org/abs/2509.10441)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Arbitrary resolution image generation provides a consistent visual experience across devices, having extensive applications for producers and consumers. Current diffusion models increase computational demand quadratically with resolution, causing 4K image generation delays over 100 seconds. To solve this, we explore the second generation upon the latent diffusion models, where the fixed latent generated by diffusion models is regarded as the content representation and we propose to decode arbitrary resolution images with a compact generated latent using a one-step generator. Thus, we present the \textbf{InfGen}, replacing the VAE decoder with the new generator, for generating images at any resolution from a fixed-size latent without retraining the diffusion models, which simplifies the process, reducing computational complexity and can be applied to any model using the same latent space. Experiments show InfGen is capable of improving many models into the arbitrary high-resolution era while cutting 4K image generation time to under 10 seconds.</li>
<li><strong>摘要：</strong>任意分辨率图像生成提供了跨设备的一致的视觉体验，并为生产者和消费者提供了广泛的应用。当前的扩散模型通过分辨率四次增加计算需求，从而导致4K图像生成延迟超过100秒。为了解决这个问题，我们探索了潜在扩散模型的第二代，其中扩散模型生成的固定潜在被视为内容表示形式，我们建议使用使用一步生成器使用紧凑的潜在生成的潜在生成的潜在分辨率图像。因此，我们提出\ textbf {infgen}，用新的发电机代替VAE解码器，以从固定尺寸的潜在分辨率生成图像，而无需重新训练扩散模型，从而简化了过程，可以降低计算复杂性，并可以使用相同的潜在空间将其应用于任何模型。实验表明，INFGEN能够将许多模型改进到任意高分辨率时代，同时将4K图像的生成时间降低到10秒以下。</li>
</ul>

<h3>Title: SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets</h3>
<ul>
<li><strong>Authors: </strong>Emily Kaczmarek, Justin Szeto, Brennan Nichyporuk, Tal Arbel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10453">https://arxiv.org/abs/2509.10453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10453">https://arxiv.org/pdf/2509.10453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10453]] SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets(https://arxiv.org/abs/2509.10453)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Alzheimer's disease is a progressive, neurodegenerative disorder that causes memory loss and cognitive decline. While there has been extensive research in applying deep learning models to Alzheimer's prediction tasks, these models remain limited by lack of available labeled data, poor generalization across datasets, and inflexibility to varying numbers of input scans and time intervals between scans. In this study, we adapt three state-of-the-art temporal self-supervised learning (SSL) approaches for 3D brain MRI analysis, and add novel extensions designed to handle variable-length inputs and learn robust spatial features. We aggregate four publicly available datasets comprising 3,161 patients for pre-training, and show the performance of our model across multiple Alzheimer's prediction tasks including diagnosis classification, conversion detection, and future conversion prediction. Importantly, our SSL model implemented with temporal order prediction and contrastive learning outperforms supervised learning on six out of seven downstream tasks. It demonstrates adaptability and generalizability across tasks and number of input images with varying time intervals, highlighting its capacity for robust performance across clinical applications. We release our code and model publicly at this https URL.</li>
<li><strong>摘要：</strong>阿尔茨海默氏病是一种进行性神经退行性疾病，会导致记忆力丧失和认知能力下降。尽管在将深度学习模型应用于阿尔茨海默氏症的预测任务方面已经进行了广泛的研究，但这些模型仍然受到缺乏可用标记的数据，跨数据集的不良概括的限制，以及对不同数量的输入扫描和扫描之间的时间间隔的不灵活性。在这项研究中，我们适应了三种最先进的时间自我监督学习（SSL）方法，用于3D脑MRI分析，并添加旨在处理可变长度输入并学习强大空间特征的新型扩展。我们汇总了四个公开可用的数据集，其中包括3,161名患者进行预培训，并在多个阿尔茨海默氏症的预测任务中显示了我们的模型的性能，包括诊断分类，转换检测和将来的转换预测。重要的是，我们以时间顺序预测和对比度学习实施的SSL模型在七个下游任务中的六个方面都优于监督学习。它证明了跨任务和具有不同时间间隔的输入图像的数量的适应性和可推广性，从而突出了其在临床应用中稳健性能的能力。我们在此HTTPS URL上公开发布代码和模型。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
