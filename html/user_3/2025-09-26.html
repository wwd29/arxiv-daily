<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-26</h1>
<h3>Title: A Theory of Multi-Agent Generative Flow Networks</h3>
<ul>
<li><strong>Authors: </strong>Leo Maxime Brunswic, Haozhi Wang, Shuang Luo, Jianye Hao, Amir Rasouli, Yinchuan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20408">https://arxiv.org/abs/2509.20408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20408">https://arxiv.org/pdf/2509.20408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20408]] A Theory of Multi-Agent Generative Flow Networks(https://arxiv.org/abs/2509.20408)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative flow networks utilize a flow-matching loss to learn a stochastic policy for generating objects from a sequence of actions, such that the probability of generating a pattern can be proportional to the corresponding given reward. However, a theoretical framework for multi-agent generative flow networks (MA-GFlowNets) has not yet been proposed. In this paper, we propose the theory framework of MA-GFlowNets, which can be applied to multiple agents to generate objects collaboratively through a series of joint actions. We further propose four algorithms: a centralized flow network for centralized training of MA-GFlowNets, an independent flow network for decentralized execution, a joint flow network for achieving centralized training with decentralized execution, and its updated conditional version. Joint Flow training is based on a local-global principle allowing to train a collection of (local) GFN as a unique (global) GFN. This principle provides a loss of reasonable complexity and allows to leverage usual results on GFN to provide theoretical guarantees that the independent policies generate samples with probability proportional to the reward function. Experimental results demonstrate the superiority of the proposed framework compared to reinforcement learning and MCMC-based methods.</li>
<li><strong>摘要：</strong>生成流动网络利用流量匹配损失来学习从一系列动作生成对象的随机策略，以便生成模式的概率可以与相应的给定奖励成正比。但是，尚未提出针对多代理生成流网络（MA-GFLOWNETS）的理论框架。在本文中，我们提出了MA-GFLOWNETS的理论框架，该框架可以应用于多个代理，以通过一系列联合动作协作生成对象。我们进一步提出了四种算法：用于MA-GFLOWNET的集中式培训的集中式流动网络，用于分散执行的独立流网络，用于通过分散执行实现集中式培训的联合流网络及其更新的条件版本。联合流训练基于本地全球原则，允许训练（本地）GFN作为独特（全球）GFN的集合。该原理提供了丧失合理的复杂性，并允许在GFN上的常规结果，以提供理论保证，即独立策略生成与奖励功能成正比的样本。实验结果表明，与增强学习和基于MCMC的方法相比，所提出的框架的优势。</li>
</ul>

<h3>Title: FastEagle: Cascaded Drafting for Accelerating Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Haiduo Huang, Jiangcheng Song, Wenzhe Zhao, Pengju Ren</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20416">https://arxiv.org/abs/2509.20416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20416">https://arxiv.org/pdf/2509.20416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20416]] FastEagle: Cascaded Drafting for Accelerating Speculative Decoding(https://arxiv.org/abs/2509.20416)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Speculative decoding accelerates generation by drafting candidates and verifying them in parallel, yet state-of-the-art drafters (e.g., EAGLE) still require N sequential passes to propose N tokens. We present FastEagle, a non-autoregressive cascaded drafter that emits an entire draft in a single forward pass. FastEagle replaces temporal steps with a lightweight layer cascade and trains with layer-wise supervision to mitigate error accumulation. Coupled with a constrained draft tree that preserves lossless verification cost, FastEagle delivers substantial wall-clock speedups over strong autoregressive drafters while maintaining competitive acceptance behavior. Across multiple LLMs (Vicuna-13B, LLaMA-Instruct 3.x, and DeepSeek-R1-Distill-LLaMA) and tasks (MT-Bench, HumanEval, GSM8K, CNN/DM, Alpaca), FastEagle consistently outperforms EAGLE-3 in speedup under both greedy and stochastic decoding, with comparable average acceptance lengths. These results indicate that removing sequential dependencies in drafting is a practical path toward lossless LLM inference acceleration.</li>
<li><strong>摘要：</strong>投机解码通过起草候选物并并行验证它们，但最先进的起草者（例如，鹰）仍然需要n顺序通过才能提出n个令牌，从而加速了生成。我们介绍了Fasteagle，这是一种非自动回忆的级联起草者，在一次前传球中排放了整个选秀。 FastEagle用轻质层级联反替换时间步骤，并通过层面监督进行训练，以减轻错误积累。加上一棵约束的草稿树，可保留无损验证的成本，Fasteagle在强大的自动回归起草者的同时，在保持竞争性接受行为的同时，将大量的墙壁锁定加速提供。在多个LLM（Vicuna-13b，Llama-Instruct 3.x和DeepSeek-R1-Distill-Lalama）和任务（MT-Bench，HumananeVal，GSM8K，CNN/DM，Alpaca），FastEAgle，FastEagle在贪婪和稳定的平均范围内都均超过了EAGLE-3的EAGLE-3始终超过3号。这些结果表明，在起草中取消顺序依赖性是通往无损LLM推理加速度的实际途径。</li>
</ul>

<h3>Title: Quasi-Synthetic Riemannian Data Generation for Writer-Independent Offline Signature Verification</h3>
<ul>
<li><strong>Authors: </strong>Elias N. Zois, Moises Diaz, Salem Said, Miguel A. Ferrer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20420">https://arxiv.org/abs/2509.20420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20420">https://arxiv.org/pdf/2509.20420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20420]] Quasi-Synthetic Riemannian Data Generation for Writer-Independent Offline Signature Verification(https://arxiv.org/abs/2509.20420)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Offline handwritten signature verification remains a challenging task, particularly in writer-independent settings where models must generalize across unseen individuals. Recent developments have highlighted the advantage of geometrically inspired representations, such as covariance descriptors on Riemannian manifolds. However, past or present, handcrafted or data-driven methods usually depend on real-world signature datasets for classifier training. We introduce a quasi-synthetic data generation framework leveraging the Riemannian geometry of Symmetric Positive Definite matrices (SPD). A small set of genuine samples in the SPD space is the seed to a Riemannian Gaussian Mixture which identifies Riemannian centers as synthetic writers and variances as their properties. Riemannian Gaussian sampling on each center generates positive as well as negative synthetic SPD populations. A metric learning framework utilizes pairs of similar and dissimilar SPD points, subsequently testing it over on real-world datasets. Experiments conducted on two popular signature datasets, encompassing Western and Asian writing styles, demonstrate the efficacy of the proposed approach under both intra- and cross- dataset evaluation protocols. The results indicate that our quasi-synthetic approach achieves low error rates, highlighting the potential of generating synthetic data in Riemannian spaces for writer-independent signature verification systems.</li>
<li><strong>摘要：</strong>离线手写签名验证仍然是一项具有挑战性的任务，尤其是在独立于作者的环境中，模型必须概括在看不见的个人中。最近的发展强调了几何启发的表述的优势，例如在Riemannian流形上的协方差描述。但是，过去或现在，手工制作或数据驱动的方法通常取决于用于分类器培训的实际签名数据集。我们引入了一个准合成数据生成框架，该框架利用了对称正定矩阵（SPD）的riemannian几何形状。 SPD空间中的一小部分真实样品是Riemannian高斯混合物的种子，将Riemannian中心鉴定为合成作家和方差为其特性。每个中心上的Riemannian高斯抽样产生正合成SPD种群。公制学习框架利用了相似和不同的SPD点对，随后在现实世界数据集上对其进行了测试。在两个流行的签名数据集上进行的实验包括西方和亚洲写作风格，证明了在内部和跨数据集评估协议下提出的方法的功效。结果表明，我们的准合成方法达到了较低的错误率，突出了在Riemannian空间中生成合成数据的潜力，用于与作者无关的签名验证系统。</li>
</ul>

<h3>Title: Seedream 4.0: Toward Next-generation Multimodal Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, Xiaowen Jian, Huafeng Kuang, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, Wei Liu, Yanzuo Lu, Zhengxiong Luo, Tongtong Ou, Guang Shi, Yichun Shi, Shiqi Sun, Yu Tian, Zhi Tian, Peng Wang, Rui Wang, Xun Wang, Ye Wang, Guofeng Wu, Jie Wu, Wenxu Wu, Yonghui Wu, Xin Xia, Xuefeng Xiao, Shuang Xu, Xin Yan, Ceyuan Yang, Jianchao Yang, Zhonghua Zhai, Chenlin Zhang, Heng Zhang, Qi Zhang, Xinyu Zhang, Yuwei Zhang, Shijia Zhao, Wenliang Zhao, Wenjia Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20427">https://arxiv.org/abs/2509.20427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20427">https://arxiv.org/pdf/2509.20427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20427]] Seedream 4.0: Toward Next-generation Multimodal Image Generation(https://arxiv.org/abs/2509.20427)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We introduce Seedream 4.0, an efficient and high-performance multimodal image generation system that unifies text-to-image (T2I) synthesis, image editing, and multi-image composition within a single framework. We develop a highly efficient diffusion transformer with a powerful VAE which also can reduce the number of image tokens considerably. This allows for efficient training of our model, and enables it to fast generate native high-resolution images (e.g., 1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning diverse taxonomies and knowledge-centric concepts. Comprehensive data collection across hundreds of vertical scenarios, coupled with optimized strategies, ensures stable and large-scale training, with strong generalization. By incorporating a carefully fine-tuned VLM model, we perform multi-modal post-training for training both T2I and image editing tasks jointly. For inference acceleration, we integrate adversarial distillation, distribution matching, and quantization, as well as speculative decoding. It achieves an inference time of up to 1.8 seconds for generating a 2K image (without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream 4.0 can achieve state-of-the-art results on both T2I and multimodal image editing. In particular, it demonstrates exceptional multimodal capabilities in complex tasks, including precise image editing and in-context reasoning, and also allows for multi-image reference, and can generate multiple output images. This extends traditional T2I systems into an more interactive and multidimensional creative tool, pushing the boundary of generative AI for both creativity and professional applications. Seedream 4.0 is now accessible on this https URL.</li>
<li><strong>摘要：</strong>我们介绍了SeedReam 4.0，这是一种高效且高性能的多模式生成系统，该系统在单个框架中统一文本对图像（T2I）合成，图像编辑和多图像组成。我们使用强大的VAE开发高效的扩散变压器，也可以大大减少图像令牌的数量。这允许对我们的模型进行有效的训练，并使其能够快速生成天然高分辨率图像（例如1K-4K）。 SeedReam 4.0在数十亿对跨越各种分类学和以知识为中心的概念的文本图像对上进行了审议。数百个垂直方案的全面数据收集，再加上优化的策略，可确保稳定且大规模的培训，并具有强烈​​的概括。通过合并精心调整的VLM模型，我们可以共同训练T2I和图像编辑任务进行多模式后训练。对于推理加速度，我们整合了对抗性蒸馏，分布匹配和量化以及投机解码。它可以在生成2K图像（没有LLM/VLM作为PE模型）的推理时间长达1.8秒。全面的评估表明，种子Ream 4.0可以在T2I和多模式图像编辑上获得最新的结果。特别是，它在复杂的任务（包括精确的图像编辑和中下文推理）中展示了出色的多模式功能，还允许进行多图像参考，并且可以生成多个输出图像。这将传统的T2I系统扩展到了一种更具交互性和多维创意工具中，从而将生成AI的边界推向了创造力和专业应用。现在可以在此HTTPS URL上访问SeedReam 4.0。</li>
</ul>

<h3>Title: A Recovery Theory for Diffusion Priors: Deterministic Analysis of the Implicit Prior Algorithm</h3>
<ul>
<li><strong>Authors: </strong>Oscar Leong, Yann Traonmilin</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20511">https://arxiv.org/abs/2509.20511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20511">https://arxiv.org/pdf/2509.20511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20511]] A Recovery Theory for Diffusion Priors: Deterministic Analysis of the Implicit Prior Algorithm(https://arxiv.org/abs/2509.20511)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recovering high-dimensional signals from corrupted measurements is a central challenge in inverse problems. Recent advances in generative diffusion models have shown remarkable empirical success in providing strong data-driven priors, but rigorous recovery guarantees remain limited. In this work, we develop a theoretical framework for analyzing deterministic diffusion-based algorithms for inverse problems, focusing on a deterministic version of the algorithm proposed by Kadkhodaie \& Simoncelli \cite{kadkhodaie2021stochastic}. First, we show that when the underlying data distribution concentrates on a low-dimensional model set, the associated noise-convolved scores can be interpreted as time-varying projections onto such a set. This leads to interpreting previous algorithms using diffusion priors for inverse problems as generalized projected gradient descent methods with varying projections. When the sensing matrix satisfies a restricted isometry property over the model set, we can derive quantitative convergence rates that depend explicitly on the noise schedule. We apply our framework to two instructive data distributions: uniform distributions over low-dimensional compact, convex sets and low-rank Gaussian mixture models. In the latter setting, we can establish global convergence guarantees despite the nonconvexity of the underlying model set.</li>
<li><strong>摘要：</strong>从损坏的测量中恢复高维信号是反问题的核心挑战。生成扩散模型的最新进展在提供强大的数据驱动先验方面表现出了显着的经验成功，但是严格的恢复保证仍然有限。在这项工作中，我们开发了一个理论框架，用于分析基于逆问题的确定性扩散算法，重点是Kadkhodaie \＆simoncelli \ cite {kadkhodaieie2021stochastic}提出的算法的确定性版本。首先，我们表明，当基础数据分布集中在低维模型集上时，相关的噪声卷积得分可以解释为在此组合上的时变投影。这导致使用扩散先验来解释先前的算法，以作为具有不同投影的普遍投影梯度下降方法。当传感矩阵满足模型集的限制等轴测特性时，我们可以得出明确取决于噪声时间表的定量收敛速率。我们将框架应用于两个指导性的数据分布：低维紧凑，凸组和低级高​​斯混合模型上的均匀分布。在后一种情况下，尽管基础模型设置非概念性，我们仍可以建立全球融合保证。</li>
</ul>

<h3>Title: InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On</h3>
<ul>
<li><strong>Authors: </strong>Julien Han, Shuwen Qiu, Qi Li, Xingzi Xu, Mehmet Saygin Seyfioglu, Kavosh Asadi, Karim Bouyarmane</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20524">https://arxiv.org/abs/2509.20524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20524">https://arxiv.org/pdf/2509.20524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20524]] InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On(https://arxiv.org/abs/2509.20524)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present InstructVTON, an instruction-following interactive virtual try-on system that allows fine-grained and complex styling control of the resulting generation, guided by natural language, on single or multiple garments. A computationally efficient and scalable formulation of virtual try-on formulates the problem as an image-guided or image-conditioned inpainting task. These inpainting-based virtual try-on models commonly use a binary mask to control the generation layout. Producing a mask that yields desirable result is difficult, requires background knowledge, might be model dependent, and in some cases impossible with the masking-based approach (e.g. trying on a long-sleeve shirt with "sleeves rolled up" styling on a person wearing long-sleeve shirt with sleeves down, where the mask will necessarily cover the entire sleeve). InstructVTON leverages Vision Language Models (VLMs) and image segmentation models for automated binary mask generation. These masks are generated based on user-provided images and free-text style instructions. InstructVTON simplifies the end-user experience by removing the necessity of a precisely drawn mask, and by automating execution of multiple rounds of image generation for try-on scenarios that cannot be achieved with masking-based virtual try-on models alone. We show that InstructVTON is interoperable with existing virtual try-on models to achieve state-of-the-art results with styling control.</li>
<li><strong>摘要：</strong>我们提出了ConstructVton，这是一种遵循指令的交互式虚拟试验系统，可在单一或多件服装上以自然语言为指导，对产生的生成精细且复杂的造型控制。虚拟试验的计算高效且可扩展的公式将问题提出为图像引导或图像条件的授课任务。这些基于Inpainting的虚拟试验模型通常使用二进制掩码来控制生成布局。产生产生可取结果的面膜很难，需要背景知识，可能是依赖模型的，并且在某些情况下，基于掩蔽的方法不可能（例如，尝试穿着带有“袖子卷起”款式的长袖衬衫，戴着袖子上的长袖衬衫，袖子上遮盖了整个袖子）。指令Vton利用视觉语言模型（VLM）和图像分割模型来生成自动化二进制掩码。这些面具是根据用户提供的图像和自由文本样式说明生成的。 Denderchvton通过删除精确绘制的面膜的必要性，并自动执行多个图像生成的多个回合，从而简化了最终用户体验，从而可以单独使用基于掩盖的虚拟尝试模型来实现多个图像生成。我们表明，ConstractVton与现有的虚拟试用模型可互操作，可以通过样式控制获得最新的结果。</li>
</ul>

<h3>Title: PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Mingze Yuan, Pengfei Jin, Na Li, Quanzheng Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20570">https://arxiv.org/abs/2509.20570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20570">https://arxiv.org/pdf/2509.20570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20570]] PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models(https://arxiv.org/abs/2509.20570)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated strong generative capabilities across scientific domains, but often produce outputs that violate physical laws. We propose a new perspective by framing physics-informed generation as a sparse reward optimization problem, where adherence to physical constraints is treated as a reward signal. This formulation unifies prior approaches under a reward-based paradigm and reveals a shared bottleneck: reliance on diffusion posterior sampling (DPS)-style value function approximations, which introduce non-negligible errors and lead to training instability and inference inefficiency. To overcome this, we introduce Physics-Informed Reward Fine-tuning (PIRF), a method that bypasses value approximation by computing trajectory-level rewards and backpropagating their gradients directly. However, a naive implementation suffers from low sample efficiency and compromised data fidelity. PIRF mitigates these issues through two key strategies: (1) a layer-wise truncated backpropagation method that leverages the spatiotemporally localized nature of physics-based rewards, and (2) a weight-based regularization scheme that improves efficiency over traditional distillation-based methods. Across five PDE benchmarks, PIRF consistently achieves superior physical enforcement under efficient sampling regimes, highlighting the potential of reward fine-tuning for advancing scientific generative modeling.</li>
<li><strong>摘要：</strong>扩散模型表明，科学领域具有强大的生成能力，但通常会产生违反物理定律的产量。我们提出了一种新的观点，即通过将物理形式的生成作为一个稀疏的奖励优化问题，在该问题中，遵守物理约束被视为奖励信号。该公式在基于奖励的范式下统一了先验方法，并揭示了共享的瓶颈：依赖扩散后验采样（DPS） - 式函数函数函数近似值，这引入了不可忽略的错误并导致训练不稳定性和推理效率。为了克服这一点，我们引入了物理信息奖励微调（PIRF），这种方法通过计算轨迹级别的奖励并直接将其梯度回到梯度来绕过价值近似。但是，幼稚的实施遭受了较低的样本效率和数据保真度损害。 PIRF通过两种关键策略来减轻这些问题：（1）一种层面截短的反向传播方法，该方法利用基于物理学的奖励的时空局部局部化性质，以及（2）基于权重的正规化方案，可提高基于传统蒸馏方法的效率。在五个PDE基准中，PIRF在有效的抽样制度下始终达到了卓越的身体执法，突出了奖励微调的潜力，以推进科学生成建模。</li>
</ul>

<h3>Title: Reflect3r: Single-View 3D Stereo Reconstruction Aided by Mirror Reflections</h3>
<ul>
<li><strong>Authors: </strong>Jing Wu, Zirui Wang, Iro Laina, Victor Adrian Prisacariu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20607">https://arxiv.org/abs/2509.20607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20607">https://arxiv.org/pdf/2509.20607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20607]] Reflect3r: Single-View 3D Stereo Reconstruction Aided by Mirror Reflections(https://arxiv.org/abs/2509.20607)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Mirror reflections are common in everyday environments and can provide stereo information within a single capture, as the real and reflected virtual views are visible simultaneously. We exploit this property by treating the reflection as an auxiliary view and designing a transformation that constructs a physically valid virtual camera, allowing direct pixel-domain generation of the virtual view while adhering to the real-world imaging process. This enables a multi-view stereo setup from a single image, simplifying the imaging process, making it compatible with powerful feed-forward reconstruction models for generalizable and robust 3D reconstruction. To further exploit the geometric symmetry introduced by mirrors, we propose a symmetric-aware loss to refine pose estimation. Our framework also naturally extends to dynamic scenes, where each frame contains a mirror reflection, enabling efficient per-frame geometry recovery. For quantitative evaluation, we provide a fully customizable synthetic dataset of 16 Blender scenes, each with ground-truth point clouds and camera poses. Extensive experiments on real-world data and synthetic data are conducted to illustrate the effectiveness of our method.</li>
<li><strong>摘要：</strong>镜像在日常环境中很常见，可以在单个捕获中提供立体声信息，因为同时可见的真实和反映的虚拟视图。我们通过将反射视为辅助视图来利用此属性，并设计一种构建物理上有效的虚拟摄像头的转换，从而在粘附在现实世界成像过程的同时，可以直接的像素域生成虚拟视图。这可以从单个图像中启用多视图立体声设置，从而简化了成像过程，使其与强大的馈送前向前重建模型兼容，可构成可推广且可靠的3D重建。为了进一步利用镜子引入的几何对称性，我们提出了对称性感知的损失来完善姿势估计。我们的框架也自然地扩展到动态场景，每个帧都包含镜像反射，从而实现了有效的人均几何恢复。为了进行定量评估，我们提供了16个搅拌机场景的完全可定制的合成数据集，每个数据集都带有地面点云和相机姿势。对现实数据和合成数据进行了广泛的实验，以说明我们方法的有效性。</li>
</ul>

<h3>Title: Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Wenkai Guo, Xuefeng Liu, Haolin Wang, Jianwei Niu, Shaojie Tang, Jing Yuan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20680">https://arxiv.org/abs/2509.20680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20680">https://arxiv.org/pdf/2509.20680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20680]] Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation(https://arxiv.org/abs/2509.20680)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) with local data is a widely adopted approach for organizations seeking to adapt LLMs to their specific domains. Given the shared characteristics in data across different organizations, the idea of collaboratively fine-tuning an LLM using data from multiple sources presents an appealing opportunity. However, organizations are often reluctant to share local data, making centralized fine-tuning impractical. Federated learning (FL), a privacy-preserving framework, enables clients to retain local data while sharing only model parameters for collaborative training, offering a potential solution. While fine-tuning LLMs on centralized datasets risks data leakage through next-token prediction, the iterative aggregation process in FL results in a global model that encapsulates generalized knowledge, which some believe protects client privacy. In this paper, however, we present contradictory findings through extensive experiments. We show that attackers can still extract training data from the global model, even using straightforward generation methods, with leakage increasing as the model size grows. Moreover, we introduce an enhanced attack strategy tailored to FL, which tracks global model updates during training to intensify privacy leakage. To mitigate these risks, we evaluate privacy-preserving techniques in FL, including differential privacy, regularization-constrained updates and adopting LLMs with safety alignment. Our results provide valuable insights and practical guidelines for reducing privacy risks when training LLMs with FL.</li>
<li><strong>摘要：</strong>使用本地数据的微调大语言模型（LLM）是寻求将LLMS适应其特定领域的组织广泛采用的方法。鉴于在不同组织的数据中具有共同的特征，使用来自多个来源的数据进行协作微调LLM的想法带来了一个吸引人的机会。但是，组织通常不愿共享本地数据，从而使集中的微调不切实际。联合学习（FL）是一种隐私保护框架，使客户能够保留本地数据，同时仅共享用于协作培训的模型参数，并提供潜在的解决方案。在集中式数据集中微调LLMS通过下一步的预测风险泄漏数据时，FL中的迭代聚合过程会导致一个全局模型，该模型封装了广义知识，有些人认为保护客户隐私。但是，在本文中，我们通过广泛的实验提出了矛盾的发现。我们表明，攻击者仍然可以从全局模型中提取训练数据，即使使用直接生成方法，随着模型大小的增长，泄漏的增加。此外，我们引入了针对FL的增强攻击策略，该策略跟踪培训期间的全球模型更新，以加强隐私泄漏。为了减轻这些风险，我们评估了佛罗里达州的隐私技术，包括差异隐私，正规化受限的更新和采用安全一致的LLM。我们的结果提供了有价值的见解和实用指南，以降低使用FL培训LLM时的隐私风险。</li>
</ul>

<h3>Title: Neptune-X: Active X-to-Maritime Generation for Universal Maritime Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Yu Guo, Shengfeng He, Yuxu Lu, Haonan An, Yihang Tao, Huilin Zhu, Jingxian Liu, Yuguang Fang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20745">https://arxiv.org/abs/2509.20745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20745">https://arxiv.org/pdf/2509.20745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20745]] Neptune-X: Active X-to-Maritime Generation for Universal Maritime Object Detection(https://arxiv.org/abs/2509.20745)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Maritime object detection is essential for navigation safety, surveillance, and autonomous operations, yet constrained by two key challenges: the scarcity of annotated maritime data and poor generalization across various maritime attributes (e.g., object category, viewpoint, location, and imaging environment). % In particular, models trained on existing datasets often underperform in underrepresented scenarios such as open-sea environments. To address these challenges, we propose Neptune-X, a data-centric generative-selection framework that enhances training effectiveness by leveraging synthetic data generation with task-aware sample selection. From the generation perspective, we develop X-to-Maritime, a multi-modality-conditioned generative model that synthesizes diverse and realistic maritime scenes. A key component is the Bidirectional Object-Water Attention module, which captures boundary interactions between objects and their aquatic surroundings to improve visual fidelity. To further improve downstream tasking performance, we propose Attribute-correlated Active Sampling, which dynamically selects synthetic samples based on their task relevance. To support robust benchmarking, we construct the Maritime Generation Dataset, the first dataset tailored for generative maritime learning, encompassing a wide range of semantic conditions. Extensive experiments demonstrate that our approach sets a new benchmark in maritime scene synthesis, significantly improving detection accuracy, particularly in challenging and previously underrepresented this http URL code is available at this https URL.</li>
<li><strong>摘要：</strong>海上对象检测对于导航安全性，监视和自动操作至关重要，但受到两个关键挑战的约束：带注释的海上数据的稀缺性和各种海上属性（例如对象类别，视图，视图，位置和成像环境）的概括不良。尤其是％，在现有数据集中培训的模型通常在代表性不足的场景（例如开放式环境）中表现不佳。为了应对这些挑战，我们提出了Neptune-X，这是一个以数据为中心的生成式选择框架，通过利用合成数据生成，通过使用任务吸引的样本选择来提高培训效果。从一代角度来看，我们开发了X-to-Maritime，这是一种多模式条件的生成模型，综合了多样化和现实的海上场景。一个关键组成部分是双向对象 - 水注意模块，该模块捕获对象与其水生环境之间的边界相互作用，以提高视觉效果。为了进一步提高下游任务性能，我们提出了与属性相关的活动采样，该采样基于其任务相关性动态选择合成样本。为了支持强大的基准测试，我们构建了海上生成数据集，这是第一个用于生成海洋学习的数据集，其中包括广泛的语义条件。广泛的实验表明，我们的方法在海上现场合成中树立了新的基准，可显着提高检测准确性，尤其是在挑战性和以前代表性不足的HTTP URL代码中，可在此HTTPS URL上获得。</li>
</ul>

<h3>Title: FreeInsert: Personalized Object Insertion with Geometric and Style Control</h3>
<ul>
<li><strong>Authors: </strong>Yuhong Zhang, Han Wang, Yiwen Wang, Rong Xie, Li Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20756">https://arxiv.org/abs/2509.20756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20756">https://arxiv.org/pdf/2509.20756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20756]] FreeInsert: Personalized Object Insertion with Geometric and Style Control(https://arxiv.org/abs/2509.20756)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have made significant progress in image generation, allowing for effortless customized generation. However, existing image editing methods still face certain limitations when dealing with personalized image composition tasks. First, there is the issue of lack of geometric control over the inserted objects. Current methods are confined to 2D space and typically rely on textual instructions, making it challenging to maintain precise geometric control over the objects. Second, there is the challenge of style consistency. Existing methods often overlook the style consistency between the inserted object and the background, resulting in a lack of realism. In addition, the challenge of inserting objects into images without extensive training remains significant. To address these issues, we propose \textit{FreeInsert}, a novel training-free framework that customizes object insertion into arbitrary scenes by leveraging 3D geometric information. Benefiting from the advances in existing 3D generation models, we first convert the 2D object into 3D, perform interactive editing at the 3D level, and then re-render it into a 2D image from a specified view. This process introduces geometric controls such as shape or view. The rendered image, serving as geometric control, is combined with style and content control achieved through diffusion adapters, ultimately producing geometrically controlled, style-consistent edited images via the diffusion model.</li>
<li><strong>摘要：</strong>文本到图像扩散模型在图像生成方面取得了重大进展，从而毫不费力地定制生成。但是，在处理个性化图像组成任务时，现有的图像编辑方法仍然面临某些限制。首先，存在缺乏对插入对象的几何控制的问题。当前的方法仅限于2D空间，通常依靠文本指令，使对对象保持精确的几何控制具有挑战性。其次，风格一致性面临的挑战。现有方法通常忽略插入的对象与背景之间的样式一致性，从而导致缺乏现实主义。此外，在没有大规模培训的情况下将物体插入图像中的挑战仍然很大。为了解决这些问题，我们建议\ textit {freeinsert}，这是一个新颖的无训练框架，通过利用3D几何信息来自定义对象插入任意场景。从现有的3D代模型中的进步中受益，我们首先将2D对象转换为3D，在3D级别执行交互式编辑，然后从指定的视图将其重新渲染为2D图像。此过程引入了几何控制，例如形状或视图。用作几何控制的渲染图像与通过扩散适配器实现的样式和内容控制结合在一起，最终通过扩散模型生成了几何控制的，样式符合样式的编辑图像。</li>
</ul>

<h3>Title: Measuring LLM Sensitivity in Transformer-based Tabular Data Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Maria F. Davila R, Azizjon Turaev, Wolfram Wingerath</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20768">https://arxiv.org/abs/2509.20768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20768">https://arxiv.org/pdf/2509.20768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20768]] Measuring LLM Sensitivity in Transformer-based Tabular Data Synthesis(https://arxiv.org/abs/2509.20768)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Synthetic tabular data is used for privacy-preserving data sharing and data-driven model development. Its effectiveness, however, depends heavily on the used Tabular Data Synthesis (TDS) tool. Recent studies have shown that Transformer-based models outperform other state-of-the-art models such as Generative Adversarial Networks (GANs) and Diffusion models in terms of data quality. However, Transformer-based models also come with high computational costs, making them sometimes unfeasible for end users with prosumer hardware. This study presents a sensitivity assessment on how the choice of hyperparameters, such as number of layers or hidden dimension affects the quality of the resultant synthetic data and the computational performance. It is performed across two tools, GReaT and REaLTabFormer, evaluating 10 model setups that vary in architecture type and depth. We assess the sensitivity on three dimensions: runtime, machine learning (ML) utility, and similarity to real data distributions. Experiments were conducted on four real-world datasets. Our findings reveal that runtime is proportional to the number of hyperparameters, with shallower configurations completing faster. GReaT consistently achieves lower runtimes than REaLTabFormer, and only on the largest dataset they have comparable runtime. For small datasets, both tools achieve synthetic data with high utility and optimal similarity, but on larger datasets only REaLTabFormer sustains strong utility and similarity. As a result, REaLTabFormer with lightweight LLMs provides the best balance, since it preserves data quality while reducing computational requirements. Nonetheless, its runtime remains higher than that of GReaT and other TDS tools, suggesting that efficiency gains are possible but only up to a certain level.</li>
<li><strong>摘要：</strong>合成表格数据用于隐私保护数据共享和数据驱动的模型开发。但是，其有效性在很大程度上取决于使用的表格数据合成（TDS）工具。最近的研究表明，基于变压器的模型在数据质量方面优于其他最先进的模型，例如生成对抗网络（GAN）和扩散模型。但是，基于变压器的模型还具有高计算成本，使其有时对于使用Prosumer硬件的最终用户不可行。这项研究对超参数的选择（例如层数或隐藏尺寸）如何影响所得合成数据和计算性能的质量如何进行敏感性评估。它是跨两个工具（伟大又是房地产经纪人的工具，评估了10个模型设置，这些模型设置在体系结构类型和深度方面有所不同。我们评估了三个维度的灵敏度：运行时，机器学习（ML）实用程序以及与实际数据分布的相似性。实验是在四个现实世界数据集上进行的。我们的发现表明，运行时与超参数的数量成正比，配置较浅的速度更快。始终如一的运行时间比RealtabFormer的运行时间较低，并且只有在最大的数据集上，它们具有可比的运行时。对于小型数据集，这两个工具都可以实现具有高实用性和最佳相似性的合成数据，但是在较大的数据集上，只有RealtaBformer可以维持强大的实用性和相似性。结果，带有轻量级LLM的房地产银行提供了最佳的平衡，因为它可以保留数据质量的同时减少计算要求。尽管如此，它的运行时间仍高于伟大和其他TDS工具的运行时间，这表明效率提高是可能的，但只能达到一定水平。</li>
</ul>

<h3>Title: CusEnhancer: A Zero-Shot Scene and Controllability Enhancement Method for Photo Customization via ResInversion</h3>
<ul>
<li><strong>Authors: </strong>Maoye Ren, Praneetha Vaddamanu, Jianjin Xu, Fernando De la Torre Frade</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20775">https://arxiv.org/abs/2509.20775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20775">https://arxiv.org/pdf/2509.20775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20775]] CusEnhancer: A Zero-Shot Scene and Controllability Enhancement Method for Photo Customization via ResInversion(https://arxiv.org/abs/2509.20775)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Recently remarkable progress has been made in synthesizing realistic human photos using text-to-image diffusion models. However, current approaches face degraded scenes, insufficient control, and suboptimal perceptual identity. We introduce CustomEnhancer, a novel framework to augment existing identity customization models. CustomEnhancer is a zero-shot enhancement pipeline that leverages face swapping techniques, pretrained diffusion model, to obtain additional representations in a zeroshot manner for encoding into personalized models. Through our proposed triple-flow fused PerGeneration approach, which identifies and combines two compatible counter-directional latent spaces to manipulate a pivotal space of personalized model, we unify the generation and reconstruction processes, realizing generation from three flows. Our pipeline also enables comprehensive training-free control over the generation process of personalized models, offering precise controlled personalization for them and eliminating the need for controller retraining for per-model. Besides, to address the high time complexity of null-text inversion (NTI), we introduce ResInversion, a novel inversion method that performs noise rectification via a pre-diffusion mechanism, reducing the inversion time by 129 times. Experiments demonstrate that CustomEnhancer reach SOTA results at scene diversity, identity fidelity, training-free controls, while also showing the efficiency of our ResInversion over NTI. The code will be made publicly available upon paper acceptance.</li>
<li><strong>摘要：</strong>最近，使用文本对图像扩散模型合成现实的人类照片，取得了显着的进步。但是，当前的方法面临降解的场景，控制不足和次优感知身份。我们介绍了Customenhancer，这是一个新颖的框架，以增强现有的身份自定义模型。 CustomEnhancer是一种零射击增强管道，利用面部交换技术（预处理的扩散模型）以Zeroshot的方式获得其他表示形式，以编码为个性化模型。通过我们提出的三流融合衰变方法，该方法识别并结合了两个兼容的反向潜在空间来操纵个性化模型的关键空间，我们统一了生成和重建过程，从而从三个流中实现了产生。我们的管道还可以对个性化模型的生成过程进行全面的无培训控制，从而为他们提供精确的控制个性化，并消除了对人均控制器的需求。此外，为了解决null-Text倒置（NTI）的高时间复杂性，我们引入了Resinversion，这是一种新型的反转方法，该方法通过扩散机制执行噪声矫正，将反转时间减少了129次。实验表明，Customenhancer在场景多样性，身份保真度，无训练控制的情况下达到SOTA结果，同时还显示了我们分解对NTI的效率。该代码将在接受纸情况下公开提供。</li>
</ul>

<h3>Title: Federated Domain Generalization with Domain-specific Soft Prompts Generation</h3>
<ul>
<li><strong>Authors: </strong>Jianhan Wu, Xiaoyang Qu, Zhangcheng Huang, Jianzong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20807">https://arxiv.org/abs/2509.20807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20807">https://arxiv.org/pdf/2509.20807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20807]] Federated Domain Generalization with Domain-specific Soft Prompts Generation(https://arxiv.org/abs/2509.20807)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Prompt learning has become an efficient paradigm for adapting CLIP to downstream tasks. Compared with traditional fine-tuning, prompt learning optimizes a few parameters yet yields highly competitive results, especially appealing in federated learning for computational efficiency. engendering domain shift among clients and posing a formidable challenge for downstream-task adaptation. Existing federated domain generalization (FDG) methods based on prompt learning typically learn soft prompts from training samples, replacing manually designed prompts to enhance the generalization ability of federated models. However, these learned prompts exhibit limited diversity and tend to ignore information from unknown domains. We propose a novel and effective method from a generative perspective for handling FDG tasks, namely federated domain generalization with domain-specific soft prompts generation (FedDSPG). Specifically, during training, we introduce domain-specific soft prompts (DSPs) for each domain and integrate content and domain knowledge into the generative model among clients. In the inference phase, the generator is utilized to obtain DSPs for unseen target domains, thus guiding downstream tasks in unknown domains. Comprehensive evaluations across several public datasets confirm that our method outperforms existing strong baselines in FDG, achieving state-of-the-art results.</li>
<li><strong>摘要：</strong>及时学习已成为将剪辑适应下游任务的有效范式。与传统的微调相比，及时学习优化了一些参数，但产生了高度竞争的结果，尤其是在联邦学习方面提高了计算效率。引发客户之间的域名转移，并对下游任务改编提出了巨大的挑战。基于及时学习的现有联合域概括（FDG）方法通常会从培训样本中学习软提示，从而更换手动设计的提示，以增强联合模型的概括能力。但是，这些学习的提示表现出有限的多样性，并且倾向于忽略来自未知领域的信息。从生成的角度来处理FDG任务，我们提出了一种新颖有效的方法，即使用特定于域的软提示生成（FIDDSPG）的联合域概括。具体来说，在培训期间，我们为每个域引入特定领域的软提示（DSP），并将内容和域知识整合到客户之间的生成模型中。在推理阶段，发电机可用于获得未见目标域的DSP，从而指导未知域中的下游任务。几个公共数据集的全面评估证实，我们的方法的表现优于FDG中现有的强基础，从而实现了最新的结果。</li>
</ul>

<h3>Title: T2I-Diff: fMRI Signal Generation via Time-Frequency Image Transform and Classifier-Free Denoising Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Hwa Hui Tew, Junn Yong Loo, Yee-Fan Tan, Xinyu Tang, Hernando Ombao, Fuad Noman, Raphael C.-W. Phan, Chee-Ming Ting</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20822">https://arxiv.org/abs/2509.20822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20822">https://arxiv.org/pdf/2509.20822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20822]] T2I-Diff: fMRI Signal Generation via Time-Frequency Image Transform and Classifier-Free Denoising Diffusion Models(https://arxiv.org/abs/2509.20822)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Functional Magnetic Resonance Imaging (fMRI) is an advanced neuroimaging method that enables in-depth analysis of brain activity by measuring dynamic changes in the blood oxygenation level-dependent (BOLD) signals. However, the resource-intensive nature of fMRI data acquisition limits the availability of high-fidelity samples required for data-driven brain analysis models. While modern generative models can synthesize fMRI data, they often underperform because they overlook the complex non-stationarity and nonlinear BOLD dynamics. To address these challenges, we introduce T2I-Diff, an fMRI generation framework that leverages time-frequency representation of BOLD signals and classifier-free denoising diffusion. Specifically, our framework first converts BOLD signals into windowed spectrograms via a time-dependent Fourier transform, capturing both the underlying temporal dynamics and spectral evolution. Subsequently, a classifier-free diffusion model is trained to generate class-conditioned frequency spectrograms, which are then reverted to BOLD signals via inverse Fourier transforms. Finally, we validate the efficacy of our approach by demonstrating improved accuracy and generalization in downstream fMRI-based brain network classification.</li>
<li><strong>摘要：</strong>功能磁共振成像（fMRI）是一种先进的神经影像学方法，可以通过测量血液氧合水平依赖性（BOLD）信号的动态变化来深入分析大脑活动。但是，fMRI数据采集的资源密集型性质限制了数据驱动的大脑分析模型所需的高保真样本的可用性。尽管现代生成模型可以合成fMRI数据，但它们通常表现不佳，因为它们忽略了复杂的非平稳性和非线性粗体动力学。为了应对这些挑战，我们介绍了T2i-Diff，这是一个fMRI生成框架，利用大胆信号和无分类器的denoising扩散的时频表示。具体而言，我们的框架首先通过时间依赖性的傅立叶变换将粗体信号转换为窗口频谱图，从而捕获了基本的时间动力学和光谱演化。随后，对无分类器扩散模型进行了训练以生成类调节的频谱图，然后通过逆傅立叶变换将其恢复为粗体信号。最后，我们通过证明基于fMRI的大脑网络分类的提高准确性和概括来验证方法的功效。</li>
</ul>

<h3>Title: Causal Time Series Generation via Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yutong Xia, Chang Xu, Yuxuan Liang, Qingsong Wen, Roger Zimmermann, Jiang Bian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20846">https://arxiv.org/abs/2509.20846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20846">https://arxiv.org/pdf/2509.20846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20846]] Causal Time Series Generation via Diffusion Models(https://arxiv.org/abs/2509.20846)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Time series generation (TSG) synthesizes realistic sequences and has achieved remarkable success. Among TSG, conditional models generate sequences given observed covariates, however, such models learn observational correlations without considering unobserved confounding. In this work, we propose a causal perspective on conditional TSG and introduce causal time series generation as a new TSG task family, formalized within Pearl's causal ladder, extending beyond observational generation to include interventional and counterfactual settings. To instantiate these tasks, we develop CaTSG, a unified diffusion-based framework with backdoor-adjusted guidance that causally steers sampling toward desired interventions and individual counterfactuals while preserving observational fidelity. Specifically, our method derives causal score functions via backdoor adjustment and the abduction-action-prediction procedure, thus enabling principled support for all three levels of TSG. Extensive experiments on both synthetic and real-world datasets show that CaTSG achieves superior fidelity and also supporting interventional and counterfactual generation that existing baselines cannot handle. Overall, we propose the causal TSG family and instantiate it with CaTSG, providing an initial proof-of-concept and opening a promising direction toward more reliable simulation under interventions and counterfactual generation.</li>
<li><strong>摘要：</strong>时间序列生成（TSG）综合了现实的序列，并取得了杰出的成功。在TSG中，条件模型生成了给定的观察到的协变量的序列，但是，这样的模型学习了观察性相关性，而无需考虑未观察到的混杂。在这项工作中，我们提出了有关有条件的TSG的因果观点，并将因果时间系列作为一个新的TSG任务系列，在Pearl的因果阶梯中正式形式化，扩展了超越凝聚力的生成，以包括介入和反事实环境。为了实例化这些任务，我们开发了CATSG，这是一个基于统一的扩散框架，并具有后门调整的指导，该指导将有因果推动采样到所需的干预措施和个人反事实，同时保持观察性保真度。具体而言，我们的方法通过后门调整和绑架行动预测程序来得出因果分数功能，从而为所有三个级别的TSG提供了原则支持。对合成和现实世界数据集的广泛实验表明，CATSG实现了卓越的忠诚度，并支持现有基线无法处理的介入和反事实生成。总体而言，我们提出了因果TSG家族并与CATSG实例化，提供了初步的概念验证，并在干预措施和反事实生成下打开了有希望的方向，以更可靠的模拟。</li>
</ul>

<h3>Title: SD-RetinaNet: Topologically Constrained Semi-Supervised Retinal Lesion and Layer Segmentation in OCT</h3>
<ul>
<li><strong>Authors: </strong>Botond Fazekas, Guilherme Aresta, Philipp Seeböck, Julia Mai, Ursula Schmidt-Erfurth, Hrvoje Bogunović</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20864">https://arxiv.org/abs/2509.20864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20864">https://arxiv.org/pdf/2509.20864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20864]] SD-RetinaNet: Topologically Constrained Semi-Supervised Retinal Lesion and Layer Segmentation in OCT(https://arxiv.org/abs/2509.20864)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Optical coherence tomography (OCT) is widely used for diagnosing and monitoring retinal diseases, such as age-related macular degeneration (AMD). The segmentation of biomarkers such as layers and lesions is essential for patient diagnosis and follow-up. Recently, semi-supervised learning has shown promise in improving retinal segmentation performance. However, existing methods often produce anatomically implausible segmentations, fail to effectively model layer-lesion interactions, and lack guarantees on topological correctness. To address these limitations, we propose a novel semi-supervised model that introduces a fully differentiable biomarker topology engine to enforce anatomically correct segmentation of lesions and layers. This enables joint learning with bidirectional influence between layers and lesions, leveraging unlabeled and diverse partially labeled datasets. Our model learns a disentangled representation, separating spatial and style factors. This approach enables more realistic layer segmentations and improves lesion segmentation, while strictly enforcing lesion location in their anatomically plausible positions relative to the segmented layers. We evaluate the proposed model on public and internal datasets of OCT scans and show that it outperforms the current state-of-the-art in both lesion and layer segmentation, while demonstrating the ability to generalize layer segmentation to pathological cases using partially annotated training data. Our results demonstrate the potential of using anatomical constraints in semi-supervised learning for accurate, robust, and trustworthy retinal biomarker segmentation.</li>
<li><strong>摘要：</strong>光学相干断层扫描（OCT）广泛用于诊断和监测视网膜疾病，例如与年龄相关的黄斑变性（AMD）。生物标志物（如层和病变）的分割对于患者诊断和随访至关重要。最近，半监督的学习表现出了改善视网膜分割性能的希望。但是，现有的方法通常会产生解剖学上令人难以置信的分割，无法有效地对层质量相互作用进行建模，并且缺乏对拓扑正确性的保证。为了解决这些局限性，我们提出了一种新型的半监督模型，该模型引入了完全可分离的生物标志物拓扑引擎，以实施对病变和层的解剖学分割。这使联合学习能够在层和病变之间具有双向影响，从而利用未标记和多样化的部分标记的数据集。我们的模型学习了一个分离的表示形式，将空间和样式因素分开。这种方法可以实现更逼真的层分割并改善病变分割，同时严格地在其解剖学上相对于分段层的解剖学上可行的位置强制执行病变位置。我们在OCT扫描的公共和内部数据集上评估了所提出的模型，并表明它的表现优于病变和层分段中的当前最新技术，同时证明了使用部分注释的培训数据证明将层分割为病理案例的能力。我们的结果表明，在半监督学习中使用解剖学约束以进行准确，健壮和值得信赖的视网膜生物标志物分割的潜力。</li>
</ul>

<h3>Title: The Unanticipated Asymmetry Between Perceptual Optimization and Assessment</h3>
<ul>
<li><strong>Authors: </strong>Jiabei Zhang, Qi Wang, Siyu Wu, Du Chen, Tianhe Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20878">https://arxiv.org/abs/2509.20878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20878">https://arxiv.org/pdf/2509.20878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20878]] The Unanticipated Asymmetry Between Perceptual Optimization and Assessment(https://arxiv.org/abs/2509.20878)</code><input type="text"></li>
<li><strong>Keywords: </strong>quality assessment</a></li>
<li><strong>Abstract: </strong>Perceptual optimization is primarily driven by the fidelity objective, which enforces both semantic consistency and overall visual realism, while the adversarial objective provides complementary refinement by enhancing perceptual sharpness and fine-grained detail. Despite their central role, the correlation between their effectiveness as optimization objectives and their capability as image quality assessment (IQA) metrics remains underexplored. In this work, we conduct a systematic analysis and reveal an unanticipated asymmetry between perceptual optimization and assessment: fidelity metrics that excel in IQA are not necessarily effective for perceptual optimization, with this misalignment emerging more distinctly under adversarial training. In addition, while discriminators effectively suppress artifacts during optimization, their learned representations offer only limited benefits when reused as backbone initializations for IQA models. Beyond this asymmetry, our findings further demonstrate that discriminator design plays a decisive role in shaping optimization, with patch-level and convolutional architectures providing more faithful detail reconstruction than vanilla or Transformer-based alternatives. These insights advance the understanding of loss function design and its connection to IQA transferability, paving the way for more principled approaches to perceptual optimization.</li>
<li><strong>摘要：</strong>感知优化主要是由富裕目标驱动的，该目标既可以实现语义一致性和整体视觉现实主义，而对抗性目标则通过增强知觉清晰度和细粒细节来提供互补的完善。尽管它们的核心作用，但它们作为优化目标的有效性与作为图像质量评估（IQA）指标的能力之间的相关性仍然没有得到充实。在这项工作中，我们进行了系统的分析，并揭示了感知优化和评估之间意外的不对称性：IQA中出色的忠诚度指标不一定对感知优化有效，在对抗性训练下，这种错误对象更加明显。此外，尽管判别者在优化过程中有效地抑制了伪像，但在将其作为IQA模型的骨干初始化时，其学识渊博的表示仅提供有限的好处。除了这种不对称之外，我们的发现进一步表明，歧视器设计在塑造优化方面起着决定性的作用，贴片级和卷积架构提供了比香草或基于变压器的替代方案更忠实的细节重建。这些见解推进了对损失函数设计及其与IQA可传递性的联系的理解，为更有原则的感知优化铺平了道路。</li>
</ul>

<h3>Title: Nuclear Diffusion Models for Low-Rank Background Suppression in Videos</h3>
<ul>
<li><strong>Authors: </strong>Tristan S.W. Stevens, Oisín Nolan, Jean-Luc Robert, Ruud J.G. van Sloun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20886">https://arxiv.org/abs/2509.20886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20886">https://arxiv.org/pdf/2509.20886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20886]] Nuclear Diffusion Models for Low-Rank Background Suppression in Videos(https://arxiv.org/abs/2509.20886)</code><input type="text"></li>
<li><strong>Keywords: </strong>restoration, generative</a></li>
<li><strong>Abstract: </strong>Video sequences often contain structured noise and background artifacts that obscure dynamic content, posing challenges for accurate analysis and restoration. Robust principal component methods address this by decomposing data into low-rank and sparse components. Still, the sparsity assumption often fails to capture the rich variability present in real video data. To overcome this limitation, a hybrid framework that integrates low-rank temporal modeling with diffusion posterior sampling is proposed. The proposed method, Nuclear Diffusion, is evaluated on a real-world medical imaging problem, namely cardiac ultrasound dehazing, and demonstrates improved dehazing performance compared to traditional RPCA concerning contrast enhancement (gCNR) and signal preservation (KS statistic). These results highlight the potential of combining model-based temporal models with deep generative priors for high-fidelity video restoration.</li>
<li><strong>摘要：</strong>视频序列通常包含结构化的噪声和背景伪像，这些噪声掩盖了动态内容，并为准确的分析和恢复带来了挑战。强大的主成分方法通过将数据分解为低级别和稀疏组件来解决此问题。尽管如此，稀疏性假设通常无法捕获真实视频数据中存在的丰富可变性。为了克服这一限制，提出了将低级时间建模与扩散后采样集成的混合框架。与传统的RPCA相比，有关对比度增强（GCNR）和信号保存（KS统计量），对现实世界的医学成像问题（即心脏超声超声）进行了评估，即核扩散的核扩散。这些结果突出了将基于模型的时间模型与深层生成先验相结合的潜在视频恢复的潜力。</li>
</ul>

<h3>Title: FerretNet: Efficient Synthetic Image Detection via Local Pixel Dependencies</h3>
<ul>
<li><strong>Authors: </strong>Shuqiao Liang, Jian Liu, Renzhang Chen, Quanlong Guan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20890">https://arxiv.org/abs/2509.20890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20890">https://arxiv.org/pdf/2509.20890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20890]] FerretNet: Efficient Synthetic Image Detection via Local Pixel Dependencies(https://arxiv.org/abs/2509.20890)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The increasing realism of synthetic images generated by advanced models such as VAEs, GANs, and LDMs poses significant challenges for synthetic image detection. To address this issue, we explore two artifact types introduced during the generation process: (1) latent distribution deviations and (2) decoding-induced smoothing effects, which manifest as inconsistencies in local textures, edges, and color transitions. Leveraging local pixel dependencies (LPD) properties rooted in Markov Random Fields, we reconstruct synthetic images using neighboring pixel information to expose disruptions in texture continuity and edge coherence. Building upon LPD, we propose FerretNet, a lightweight neural network with only 1.1M parameters that delivers efficient and robust synthetic image detection. Extensive experiments demonstrate that FerretNet, trained exclusively on the 4-class ProGAN dataset, achieves an average accuracy of 97.1% on an open-world benchmark comprising across 22 generative models, surpassing state-of-the-art methods by 10.6%.</li>
<li><strong>摘要：</strong>高级模型（例如VAE，GAN和LDMS）产生的合成图像的现实主义越来越现实给合成图像检测带来了重大挑战。为了解决这个问题，我们探讨了在生成过程中引入的两种伪影类型：（1）潜在的分布偏差和（2）解码引起的平滑效果，它们在局部纹理，边缘和颜色过渡中表现出不一致的情况。利用植根于马尔可夫随机字段的本地像素依赖性（LPD）属性，我们使用相邻的像素信息重建合成图像，以暴露纹理连续性和边缘连贯性中的破坏。在LPD的基础上，我们提出了Ferretnet，这是一种仅提供110万参数的轻型神经网络，可提供有效且稳健的合成图像检测。广泛的实验表明，仅在4级Progan数据集中受过训练的Ferretnet在包括22个生成模型的开放世界基准上的平均准确度达到97.1％，超过了最先进的方法，达到了10.6％。</li>
</ul>

<h3>Title: Deterministic Discrete Denoising</h3>
<ul>
<li><strong>Authors: </strong>Hideyuki Suzuki, Hiroshi Yamashita</a></li>
<li><strong>Subjects: </strong>cs.LG, nlin.CD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20896">https://arxiv.org/abs/2509.20896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20896">https://arxiv.org/pdf/2509.20896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20896]] Deterministic Discrete Denoising(https://arxiv.org/abs/2509.20896)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We propose a deterministic denoising algorithm for discrete-state diffusion models based on Markov chains. The generative reverse process is derandomized by introducing a variant of the herding algorithm with weakly chaotic dynamics, which induces deterministic discrete state transitions. Our approach is a direct replacement for the stochastic denoising process, requiring neither retraining nor continuous state embeddings. We demonstrate consistent improvements in both efficiency and sample quality on text and image generation tasks. Thus, this simple derandomization approach is expected to enhance the significance of discrete diffusion in generative modeling. Furthermore, our results reveal that deterministic reverse processes, well established in continuous diffusion, can also be effective in discrete state spaces.</li>
<li><strong>摘要：</strong>我们提出了一种基于马尔可夫链的离散状态扩散模型的确定性授权算法。生成反向过程通过引入具有弱混沌动力学的放牧算法的变体来扩展，从而诱导确定性离散状态过渡。我们的方法是直接替代随机降解过程，不需要再培训也不需要连续的状态嵌入。我们证明了文本和图像生成任务的效率和样本质量的一致性提高。因此，这种简单的衍生方法有望增强生成建模中离散扩散的重要性。此外，我们的结果表明，在连续扩散中良好确定的确定性反向过程也可以在离散状态空间中有效。</li>
</ul>

<h3>Title: SimDiff: Simulator-constrained Diffusion Model for Physically Plausible Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Akihisa Watanabe, Jiawei Ren, Li Siyao, Yichen Peng, Erwin Wu, Edgar Simo-Serra</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20927">https://arxiv.org/abs/2509.20927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20927">https://arxiv.org/pdf/2509.20927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20927]] SimDiff: Simulator-constrained Diffusion Model for Physically Plausible Motion Generation(https://arxiv.org/abs/2509.20927)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating physically plausible human motion is crucial for applications such as character animation and virtual reality. Existing approaches often incorporate a simulator-based motion projection layer to the diffusion process to enforce physical plausibility. However, such methods are computationally expensive due to the sequential nature of the simulator, which prevents parallelization. We show that simulator-based motion projection can be interpreted as a form of guidance, either classifier-based or classifier-free, within the diffusion process. Building on this insight, we propose SimDiff, a Simulator-constrained Diffusion Model that integrates environment parameters (e.g., gravity, wind) directly into the denoising process. By conditioning on these parameters, SimDiff generates physically plausible motions efficiently, without repeated simulator calls at inference, and also provides fine-grained control over different physical coefficients. Moreover, SimDiff successfully generalizes to unseen combinations of environmental parameters, demonstrating compositional generalization.</li>
<li><strong>摘要：</strong>产生物理上合理的人类运动对于诸如角色动画和虚拟现实等应用至关重要。现有方法通常将基于模拟器的运动投影层纳入扩散过程，以实现物理合理性。但是，由于模拟器的顺序性质，这种方法在计算上昂贵，从而防止并行化。我们表明，基于模拟器的运动投影可以解释为在扩散过程中基于分类器或无分类器的指导形式。在此洞察力的基础上，我们提出了Simdiff，这是一个模拟器约束的扩散模型，将环境参数（例如重力，风）直接集成到剥离过程中。通过对这些参数进行调节，Simdiff可以有效地生成物理上合理的运动，而无需在推理时重复呼叫，并且还提供了对不同物理系数的细粒度控制。此外，Simdiff成功地概括了环境参数的看不见组合，证明了组成概括。</li>
</ul>

<h3>Title: GenFacts-Generative Counterfactual Explanations for Multi-Variate Time Series</h3>
<ul>
<li><strong>Authors: </strong>Sarah Seifi, Anass Ibrahimi, Tobias Sukianto, Cecilia Carbonelli, Lorenzo Servadei, Robert Wille</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20936">https://arxiv.org/abs/2509.20936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20936">https://arxiv.org/pdf/2509.20936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20936]] GenFacts-Generative Counterfactual Explanations for Multi-Variate Time Series(https://arxiv.org/abs/2509.20936)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Counterfactual explanations aim to enhance model transparency by showing how inputs can be minimally altered to change predictions. For multivariate time series, existing methods often generate counterfactuals that are invalid, implausible, or unintuitive. We introduce GenFacts, a generative framework based on a class-discriminative variational autoencoder. It integrates contrastive and classification-consistency objectives, prototype-based initialization, and realism-constrained optimization. We evaluate GenFacts on radar gesture data as an industrial use case and handwritten letter trajectories as an intuitive benchmark. Across both datasets, GenFacts outperforms state-of-the-art baselines in plausibility (+18.7%) and achieves the highest interpretability scores in a human study. These results highlight that plausibility and user-centered interpretability, rather than sparsity alone, are key to actionable counterfactuals in time series data.</li>
<li><strong>摘要：</strong>反事实解释旨在通过显示如何最小化输入以更改预测来提高模型透明度。对于多元时间序列，现有方法通常会产生无效，难以置信或不直觉的反事实。我们介绍了Genfacts，这是一种基于类歧视性变异自动编码器的生成框架。它集成了对比度和分类矛盾目标，基于原型的初始化以及现实主义约束的优化。我们将雷达手势数据上的GENFACT评估为工业用例，并将手写字母轨迹视为直观的基准。在这两个数据集中，Genfacts在合理性（+18.7％）中的表现优于最先进的基线，并且在人类研究中达到了最高的可解释性得分。这些结果表明，合理性和以用户为中心的解释性而不是稀疏性是时间序列数据中可行的反事实的关键。</li>
</ul>

<h3>Title: Decoding the Surgical Scene: A Scoping Review of Scene Graphs in Surgery</h3>
<ul>
<li><strong>Authors: </strong>Angelo Henriques, Korab Hoxha, Daniel Zapp, Peter C. Issa, Nassir Navab, M. Ali Nasseri</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20941">https://arxiv.org/abs/2509.20941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20941">https://arxiv.org/pdf/2509.20941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20941]] Decoding the Surgical Scene: A Scoping Review of Scene Graphs in Surgery(https://arxiv.org/abs/2509.20941)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Scene graphs (SGs) provide structured relational representations crucial for decoding complex, dynamic surgical environments. This PRISMA-ScR-guided scoping review systematically maps the evolving landscape of SG research in surgery, charting its applications, methodological advancements, and future directions. Our analysis reveals rapid growth, yet uncovers a critical 'data divide': internal-view research (e.g., triplet recognition) almost exclusively uses real-world 2D video, while external-view 4D modeling relies heavily on simulated data, exposing a key translational research gap. Methodologically, the field has advanced from foundational graph neural networks to specialized foundation models that now significantly outperform generalist large vision-language models in surgical contexts. This progress has established SGs as a cornerstone technology for both analysis, such as workflow recognition and automated safety monitoring, and generative tasks like controllable surgical simulation. Although challenges in data annotation and real-time implementation persist, they are actively being addressed through emerging techniques. Surgical SGs are maturing into an essential semantic bridge, enabling a new generation of intelligent systems to improve surgical safety, efficiency, and training.</li>
<li><strong>摘要：</strong>场景图（SG）提供了结构化的关系表示，对于解码复杂的动态手术环境至关重要。这项Prisma-SCR指导的范围审查系统地绘制了SG研究中不断发展的局势，绘制了其应用，方法论的进步和未来的方向。我们的分析揭示了快速增长，但发现了关键的“数据鸿沟”：内部视图研究（例如，三胞胎识别）几乎完全使用了现实世界2D视频，而外部视图4D建模在很大程度上依赖于模拟数据，暴露了关键的翻译研究差距。从方法上讲，该领域已从基础图神经网络发展到专门的基础模型，这些模型现在在手术环境下显着超过了通用大型视力模型。这一进展已建立了SGS作为用于分析的基石技术，例如工作流识别和自动化的安全监控以及诸如可控手术模拟之类的生成任务。尽管数据注释和实时实施方面的挑战仍然存在，但通过新兴技术积极解决。手术SG正在成为基本的语义桥，从而使新一代的智能系统可以提高手术安全性，效率和培训。</li>
</ul>

<h3>Title: Why Attention Fails: The Degeneration of Transformers into MLPs in Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Zida Liang, Jiayi Zhu, Weiqiang Sun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20942">https://arxiv.org/abs/2509.20942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20942">https://arxiv.org/pdf/2509.20942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20942]] Why Attention Fails: The Degeneration of Transformers into MLPs in Time Series Forecasting(https://arxiv.org/abs/2509.20942)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Transformer-based architectures achieved high performance in natural language processing and computer vision, yet many studies have shown that they have not demonstrated a clear advantage in time series forecasting and even underperform simple linear baselines in some cases. However, most of these studies have not thoroughly explored the reasons behind the failure of transformers. To better understand time-series transformers(TST), we designed a series of experiments, progressively modifying transformers into MLPs to investigate the impact of the attention mechanism. Surprisingly, transformer blocks often degenerate into simple MLPs in existing time-series transformers. We designed a interpretable dataset to investigate the reasons behind the failure of the attention mechanism and revealed that the attention mechanism is not working in the expected way. We theoretically analyzed the reasons behind this phenomenon, demonstrating that the current embedding methods fail to allow transformers to function in a well-structured latent space, and further analyzed the deeper underlying causes of the failure of embedding.</li>
<li><strong>摘要：</strong>基于变压器的体系结构在自然语言处理和计算机视觉中实现了高性能，但许多研究表明，在某些情况下，它们尚未在时间序列预测甚至表现不佳的简单线性基线中表现出明显的优势。但是，这些研究中的大多数尚未彻底探讨变压器失败背后的原因。为了更好地理解时间序列变压器（TST），我们设计了一系列实验，将变压器逐渐将变压器修改为MLP，以研究注意机制的影响。令人惊讶的是，变压器块通常在现有时间序列变压器中退化为简单的MLP。我们设计了一个可解释的数据集，以研究注意机制失败背后的原因，并揭示了注意机制无法以预期的方式起作用。我们从理论上分析了这种现象背后的原因，表明当前的嵌入方法无法允许变形金刚在结构良好的潜在空间中运行，并进一步分析了嵌入失败的深层根本原因。</li>
</ul>

<h3>Title: A Real-Time On-Device Defect Detection Framework for Laser Power-Meter Sensors via Unsupervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Dongqi Zheng, Wenjin Fu, Guangzong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20946">https://arxiv.org/abs/2509.20946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20946">https://arxiv.org/pdf/2509.20946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20946]] A Real-Time On-Device Defect Detection Framework for Laser Power-Meter Sensors via Unsupervised Learning(https://arxiv.org/abs/2509.20946)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We present an automated vision-based system for defect detection and classification of laser power meter sensor coatings. Our approach addresses the critical challenge of identifying coating defects such as thermal damage and scratches that can compromise laser energy measurement accuracy in medical and industrial applications. The system employs an unsupervised anomaly detection framework that trains exclusively on ``good'' sensor images to learn normal coating distribution patterns, enabling detection of both known and novel defect types without requiring extensive labeled defect datasets. Our methodology consists of three key components: (1) a robust preprocessing pipeline using Laplacian edge detection and K-means clustering to segment the area of interest, (2) synthetic data augmentation via StyleGAN2, and (3) a UFlow-based neural network architecture for multi-scale feature extraction and anomaly map generation. Experimental evaluation on 366 real sensor images demonstrates $93.8\%$ accuracy on defective samples and $89.3\%$ accuracy on good samples, with image-level AUROC of 0.957 and pixel-level AUROC of 0.961. The system provides potential annual cost savings through automated quality control and processing times of 0.5 seconds per image in on-device implementation.</li>
<li><strong>摘要：</strong>我们提出了一个基于自动视力的系统，用于对激光电表传感器涂料的缺陷检测和分类。我们的方法解决了识别涂层缺陷的关键挑战，例如热损伤和可能损害医疗和工业应用中激光能量测量精度的划痕。该系统采用无监督的异常检测框架，该框架专门训练``好''传感器图像来学习正常的涂料分布模式，从而可以检测已知和新颖的缺陷类型，而无需广泛标记的缺陷数据集。我们的方法由三个关键组成部分组成：（1）使用拉普拉斯边缘检测和K-均值聚类的强大预处理管道，（2）通过stylegan2进行综合数据增强，以及（3）基于UFLOW的神经网络结构，用于多尺度的特征提取物和Anomy Map Generation。 366个真实传感器图像的实验评估显示出$ 93.8 \％$的有缺陷样品的精度，$ 89.3 \％$ $ $精度的精度为0.957，像素级AUROC为0.961。该系统通过在设备实施中通过自动质量控制和处理时间为0.5秒来节省潜在的成本。</li>
</ul>

<h3>Title: Flow Matching in the Low-Noise Regime: Pathologies and a Contrastive Remedy</h3>
<ul>
<li><strong>Authors: </strong>Weili Zeng, Yichao Yan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20952">https://arxiv.org/abs/2509.20952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20952">https://arxiv.org/pdf/2509.20952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20952]] Flow Matching in the Low-Noise Regime: Pathologies and a Contrastive Remedy(https://arxiv.org/abs/2509.20952)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Flow matching has recently emerged as a powerful alternative to diffusion models, providing a continuous-time formulation for generative modeling and representation learning. Yet, we show that this framework suffers from a fundamental instability in the low-noise regime. As noise levels approach zero, arbitrarily small perturbations in the input can induce large variations in the velocity target, causing the condition number of the learning problem to diverge. This ill-conditioning not only slows optimization but also forces the encoder to reallocate its limited Jacobian capacity toward noise directions, thereby degrading semantic representations. We provide the first theoretical analysis of this phenomenon, which we term the low-noise pathology, establishing its intrinsic link to the structure of the flow matching objective. Building on these insights, we propose Local Contrastive Flow (LCF), a hybrid training protocol that replaces direct velocity regression with contrastive feature alignment at small noise levels, while retaining standard flow matching at moderate and high noise. Empirically, LCF not only improves convergence speed but also stabilizes representation quality. Our findings highlight the critical importance of addressing low-noise pathologies to unlock the full potential of flow matching for both generation and representation learning.</li>
<li><strong>摘要：</strong>流量匹配最近已成为扩散模型的强大替代方法，为生成建模和表示学习提供了连续的时间公式。但是，我们表明该框架在低噪声政权中的根本不稳定遭受。随着噪声水平接近零，输入中任意小的扰动会引起速度目标的巨大变化，从而导致学习问题的条件数量分歧。这种不良的条件不仅放慢了优化，而且还迫使编码器将其有限的雅各布能力重新分配给噪声方向，从而降低语义表示。我们提供了对这种现象的第一个理论分析，我们将其称为低噪声病理学，并建立了其与流匹配目标结构的内在联系。在这些见解的基础上，我们提出了局部对比度流（LCF），这是一种混合训练协议，该协议以较小的噪声水平取代直接速度回归，同时将标准流量匹配在中等和高噪声下。从经验上讲，LCF不仅提高了收敛速度，还可以稳定表示质量。我们的发现突出了解决低噪声病理的至关重要的重要性，以释放产生和表示学习的全部流量匹配潜力。</li>
</ul>

<h3>Title: Lossless Compression: A New Benchmark for Time Series Model Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Meng Wan, Benxi Tian, Jue Wang, Cui Hui, Ningming Nie, Tiantian Liu, Zongguo Wang, Cao Rongqiang, Peng Shi, Yangang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21002">https://arxiv.org/abs/2509.21002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21002">https://arxiv.org/pdf/2509.21002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21002]] Lossless Compression: A New Benchmark for Time Series Model Evaluation(https://arxiv.org/abs/2509.21002)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The evaluation of time series models has traditionally focused on four canonical tasks: forecasting, imputation, anomaly detection, and classification. While these tasks have driven significant progress, they primarily assess task-specific performance and do not rigorously measure whether a model captures the full generative distribution of the data. We introduce lossless compression as a new paradigm for evaluating time series models, grounded in Shannon's source coding theorem. This perspective establishes a direct equivalence between optimal compression length and the negative log-likelihood, providing a strict and unified information-theoretic criterion for modeling capacity. Then We define a standardized evaluation protocol and metrics. We further propose and open-source a comprehensive evaluation framework TSCom-Bench, which enables the rapid adaptation of time series models as backbones for lossless compression. Experiments across diverse datasets on state-of-the-art models, including TimeXer, iTransformer, and PatchTST, demonstrate that compression reveals distributional weaknesses overlooked by classic benchmarks. These findings position lossless compression as a principled task that complements and extends existing evaluation for time series modeling.</li>
<li><strong>摘要：</strong>时间序列模型的评估传统上集中在四个规范任务上：预测，归档，异常检测和分类。尽管这些任务取得了重大进展，但它们主要评估特定于任务的性能，并且不严格衡量模型是否捕获了数据的完整生成分布。我们将无损压缩作为评估时间序列模型的新范式，该范式基于香农的源编码定理。该观点在最佳压缩长度和负模具性之间建立了直接等效性，为建模能力提供了严格而统一的信息理论标准。然后，我们定义标准化的评估协议和指标。我们进一步提出并开源一个全面的评估框架TSCOM基座，该框架可以快速适应时间序列模型作为无损压缩的骨架。在包括TimeXer，Itransformer和PatchTST在内的最先进模型的各种数据集的实验表明，压缩揭示了经典基准测试所忽略的分布弱点。这些发现位置无损压缩是一项有原则的任务，可以补充并扩展时间序列建模的现有评估。</li>
</ul>

<h3>Title: A Single Neuron Works: Precise Concept Erasure in Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Qinqin He, Jiaqi Weng, Jialing Tao, Hui Xue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21008">https://arxiv.org/abs/2509.21008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21008">https://arxiv.org/pdf/2509.21008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21008]] A Single Neuron Works: Precise Concept Erasure in Text-to-Image Diffusion Models(https://arxiv.org/abs/2509.21008)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-to-image models exhibit remarkable capabilities in image generation. However, they also pose safety risks of generating harmful content. A key challenge of existing concept erasure methods is the precise removal of target concepts while minimizing degradation of image quality. In this paper, we propose Single Neuron-based Concept Erasure (SNCE), a novel approach that can precisely prevent harmful content generation by manipulating only a single neuron. Specifically, we train a Sparse Autoencoder (SAE) to map text embeddings into a sparse, disentangled latent space, where individual neurons align tightly with atomic semantic concepts. To accurately locate neurons responsible for harmful concepts, we design a novel neuron identification method based on the modulated frequency scoring of activation patterns. By suppressing activations of the harmful concept-specific neuron, SNCE achieves surgical precision in concept erasure with minimal disruption to image quality. Experiments on various benchmarks demonstrate that SNCE achieves state-of-the-art results in target concept erasure, while preserving the model's generation capabilities for non-target concepts. Additionally, our method exhibits strong robustness against adversarial attacks, significantly outperforming existing methods.</li>
<li><strong>摘要：</strong>文本对图像模型在图像生成中具有显着的功能。但是，它们还构成了产生有害内容的安全风险。现有概念擦除方法的主要挑战是，精确地删除了目标概念，同时最大程度地减少了图像质量的退化。在本文中，我们提出了基于单个神经元的概念擦除（SNCE），这是一种新型方法，可以通过仅操纵单个神经元来精确防止有害内容产生。具体来说，我们训练一个稀疏的自动编码器（SAE）将嵌入文本嵌入到一个稀疏的，分离的潜在空间中，在该空间中，单个神经元与原子语义概念紧密地保持一致。为了准确定位负责有害概念的神经元，我们根据激活模式的调制频率评分设计了一种新型的神经元识别方法。通过抑制有害概念特异性神经元的激活，SNCE在概念擦除方面达到了手术精度，对图像质量的破坏极少。各种基准的实验表明，SNCE实现了最新的实现目标概念擦除，同时保留了模型的非目标概念的生成能力。此外，我们的方法对对抗性攻击具有强大的鲁棒性，极大地表现出了现有方法的表现。</li>
</ul>

<h3>Title: ExMolRL: Phenotype-Target Joint Generation of De Novo Molecules via Multi-Objective Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Haotian Guo, Hui Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21010">https://arxiv.org/abs/2509.21010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21010">https://arxiv.org/pdf/2509.21010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21010]] ExMolRL: Phenotype-Target Joint Generation of De Novo Molecules via Multi-Objective Reinforcement Learning(https://arxiv.org/abs/2509.21010)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>The generation of high-quality candidate molecules remains a central challenge in AI-driven drug design. Current phenotype-based and target-based strategies each suffer limitations, either incurring high experimental costs or overlook system-level cellular responses. To bridge this gap, we propose ExMoIRL, a novel generative framework that synergistically integrates phenotypic and target-specific cues for de novo molecular generation. The phenotype-guided generator is first pretrained on expansive drug-induced transcriptional profiles and subsequently fine-tuned via multi-objective reinforcement learning (RL). Crucially, the reward function fuses docking affinity and drug-likeness scores, augmented with ranking loss, prior-likelihood regularization, and entropy maximization. The multi-objective RL steers the model toward chemotypes that are simultaneously potent, diverse, and aligned with the specified phenotypic effects. Extensive experiments demonstrate ExMoIRL's superior performance over state-of-the-art phenotype-based and target-based models across multiple well-characterized targets. Our generated molecules exhibit favorable drug-like properties, high target affinity, and inhibitory potency (IC50) against cancer cells. This unified framework showcases the synergistic potential of combining phenotype-guided and target-aware strategies, offering a more effective solution for de novo drug discovery.</li>
<li><strong>摘要：</strong>在AI驱动的药物设计中，高质量候选分子的产生仍然是一个核心挑战。当前基于表型和基于目标的策略每种局限性都遭受高实验成本或忽略系统级的细胞反应的局限性。为了弥合这一差距，我们提出了Exmoirl，这是一种新型的生成框架，可以协同整合从头分子产生的表型和靶标特异性提示。首先在膨胀的药物诱导的转录轮廓上鉴定表型引导的发生器，然后通过多目标增强学习（RL）进行微调。至关重要的是，奖励函数融合了对接的亲和力和毒品的得分，并随着排名损失，先前的样子正则化和熵最大化的速度增强。多目标RL将模型引导到同时有效，多样化和与指定的表型效应的化学型。广泛的实验表明，在多个特征良好的目标中，Exmoirl优于基于最先进的表型和基于目标的模型。我们产生的分子表现出有利的药物样特性，高靶亲和力和针对癌细胞的抑制效力（IC50）。这个统一的框架展示了结合表型引导和目标感知策略的协同潜力，为从头探索提供了更有效的解决方案。</li>
</ul>

<h3>Title: Physics of Learning: A Lagrangian perspective to different learning paradigms</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Guo, Bernhard Schölkopf</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21049">https://arxiv.org/abs/2509.21049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21049">https://arxiv.org/pdf/2509.21049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21049]] Physics of Learning: A Lagrangian perspective to different learning paradigms(https://arxiv.org/abs/2509.21049)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We study the problem of building an efficient learning system. Efficient learning processes information in the least time, i.e., building a system that reaches a desired error threshold with the least number of observations. Building upon least action principles from physics, we derive classic learning algorithms, Bellman's optimality equation in reinforcement learning, and the Adam optimizer in generative models from first principles, i.e., the Learning $\textit{Lagrangian}$. We postulate that learning searches for stationary paths in the Lagrangian, and learning algorithms are derivable by seeking the stationary trajectories.</li>
<li><strong>摘要：</strong>我们研究建立有效学习系统的问题。有效的学习过程在最少的时间内，即建立一个达到所需的误差阈值的系统，观察值数量最少。我们基于物理学的最小动作原则，我们得出经典的学习算法，贝尔曼在增强学习中的最佳方程以及从第一原理中的生成模型中的Adam Optimizer，即学习$ \ textit {lagrangian} $。我们假设学习搜索拉格朗日语中的固定路径，并且通过寻找固定轨迹来衍生学习算法。</li>
</ul>

<h3>Title: Stratify or Die: Rethinking Data Splits in Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Naga Venkata Sai Jitin Jami, Thomas Altstidl, Jonas Mueller, Jindong Li, Dario Zanca, Bjoern Eskofier, Heike Leutheuser</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21056">https://arxiv.org/abs/2509.21056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21056">https://arxiv.org/pdf/2509.21056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21056]] Stratify or Die: Rethinking Data Splits in Image Segmentation(https://arxiv.org/abs/2509.21056)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Random splitting of datasets in image segmentation often leads to unrepresentative test sets, resulting in biased evaluations and poor model generalization. While stratified sampling has proven effective for addressing label distribution imbalance in classification tasks, extending these ideas to segmentation remains challenging due to the multi-label structure and class imbalance typically present in such data. Building on existing stratification concepts, we introduce Iterative Pixel Stratification (IPS), a straightforward, label-aware sampling method tailored for segmentation tasks. Additionally, we present Wasserstein-Driven Evolutionary Stratification (WDES), a novel genetic algorithm designed to minimize the Wasserstein distance, thereby optimizing the similarity of label distributions across dataset splits. We prove that WDES is globally optimal given enough generations. Using newly proposed statistical heterogeneity metrics, we evaluate both methods against random sampling and find that WDES consistently produces more representative splits. Applying WDES across diverse segmentation tasks, including street scenes, medical imaging, and satellite imagery, leads to lower performance variance and improved model evaluation. Our results also highlight the particular value of WDES in handling small, imbalanced, and low-diversity datasets, where conventional splitting strategies are most prone to bias.</li>
<li><strong>摘要：</strong>图像分割中数据集的随机分裂通常会导致未代表性的测试集，从而导致偏见的评估和模型概括差。尽管经过分层的采样可有效解决分类任务中的标签分布不平衡，但由于多标签结构和类别不平衡，将这些想法扩展到细分方面仍然具有挑战性。在现有分层概念的基础上，我们引入了迭代像素分层（IPS），这是一种针对分割任务量身定制的直接，标签感知的采样方法。此外，我们介绍了瓦斯坦驱动的进化分层（WDES），这是一种新型的遗传算法，旨在最大程度地减少Wasserstein距离，从而优化了跨数据集拆分的标签分布的相似性。我们证明，给定足够几代的WDE是全球最佳的。使用新提出的统计异质性指标，我们评估了这两种方法针对随机抽样，并发现WDE始终产生更多代表性的分裂。在包括街道场景，医学成像和卫星图像在内的各种细分任务中应用WDE会导致性能差异较低和改进的模型评估。我们的结果还突出了WDE在处理小型，不平衡和低多样性数据集中的特定价值，在这些数据集中，传统的分裂策略最容易偏见。</li>
</ul>

<h3>Title: SPREAD: Sampling-based Pareto front Refinement via Efficient Adaptive Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Sedjro Salomon Hotegni, Sebastian Peitz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21058">https://arxiv.org/abs/2509.21058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21058">https://arxiv.org/pdf/2509.21058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21058]] SPREAD: Sampling-based Pareto front Refinement via Efficient Adaptive Diffusion(https://arxiv.org/abs/2509.21058)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Developing efficient multi-objective optimization methods to compute the Pareto set of optimal compromises between conflicting objectives remains a key challenge, especially for large-scale and expensive problems. To bridge this gap, we introduce SPREAD, a generative framework based on Denoising Diffusion Probabilistic Models (DDPMs). SPREAD first learns a conditional diffusion process over points sampled from the decision space and then, at each reverse diffusion step, refines candidates via a sampling scheme that uses an adaptive multiple gradient descent-inspired update for fast convergence alongside a Gaussian RBF-based repulsion term for diversity. Empirical results on multi-objective optimization benchmarks, including offline and Bayesian surrogate-based settings, show that SPREAD matches or exceeds leading baselines in efficiency, scalability, and Pareto front coverage.</li>
<li><strong>摘要：</strong>开发有效的多目标优化方法来计算矛盾目标之间的最佳妥协集是一个关键挑战，尤其是对于大规模且昂贵的问题。为了弥合这一差距，我们引入了扩散，这是一个基于脱氧扩散概率模型（DDPM）的生成框架。 vrave首先学习一个条件扩散过程，超过了从决策空间采样的点，然后在每个反向扩散步骤中，通过采样方案来完善候选者，该采样方案使用自适应多个梯度下降启发的更新更新，以与基于高斯RBF的抑制项以及多样性相同。多目标优化基准（包括离线和基于贝叶斯替代物的设置）的经验结果表明，效率，可伸缩性和帕累托前覆盖范围的传播匹配或超过了领先的基线。</li>
</ul>

<h3>Title: UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Guojun Lei, Rong Zhang, Chi Wang, Tianhang Liu, Hong Li, Zhiyuan Ma, Weiwei Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21086">https://arxiv.org/abs/2509.21086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21086">https://arxiv.org/pdf/2509.21086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21086]] UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition(https://arxiv.org/abs/2509.21086)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>We propose a novel architecture UniTransfer, which introduces both spatial and diffusion timestep decomposition in a progressive paradigm, achieving precise and controllable video concept transfer. Specifically, in terms of spatial decomposition, we decouple videos into three key components: the foreground subject, the background, and the motion flow. Building upon this decomposed formulation, we further introduce a dual-to-single-stream DiT-based architecture for supporting fine-grained control over different components in the videos. We also introduce a self-supervised pretraining strategy based on random masking to enhance the decomposed representation learning from large-scale unlabeled video data. Inspired by the Chain-of-Thought reasoning paradigm, we further revisit the denoising diffusion process and propose a Chain-of-Prompt (CoP) mechanism to achieve the timestep decomposition. We decompose the denoising process into three stages of different granularity and leverage large language models (LLMs) for stage-specific instructions to guide the generation progressively. We also curate an animal-centric video dataset called OpenAnimal to facilitate the advancement and benchmarking of research in video concept transfer. Extensive experiments demonstrate that our method achieves high-quality and controllable video concept transfer across diverse reference images and scenes, surpassing existing baselines in both visual fidelity and editability. Web Page: this https URL</li>
<li><strong>摘要：</strong>我们提出了一种新型的体系结构单位转移，该建筑转移引入了渐进式范式中的空间和扩散时间段分解，从而实现了精确且可控制的视频概念转移。具体而言，就空间分解而言，我们将视频分解为三个关键组成部分：前景主体，背景和运动流。在这种经过分解的配方的基础上，我们进一步引入了基于单一的dit dit架构，以支持对视频中不同组件的细粒度控制。我们还基于随机掩盖引入了一种自我监督的预处理策略，以增强大规模未标记视频数据的分解表示学习。受到思想链推理范式的启发，我们进一步重新审视了降级的扩散过程，并提出了实现时间段分解的预告链（COP）机制。我们将denoising过程分解为不同粒度的三个阶段，并利用大型语言模型（LLM）进行特定于阶段的指令，以逐步指导这一代。我们还策划了一个以动物为中心的视频数据集，称为OpenAnimal，以促进视频概念转移中研究的进步和基准测试。广泛的实验表明，我们的方法在各种参考图像和场景上实现了高质量和可控的视频概念转移，从而超过了视觉效果和编辑性的现有基准。网页：此HTTPS URL</li>
</ul>

<h3>Title: GraphUniverse: Enabling Systematic Evaluation of Inductive Generalization</h3>
<ul>
<li><strong>Authors: </strong>Louis Van Langendonck, Guillermo Bernárdez, Nina Miolane, Pere Barlet-Ros</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21097">https://arxiv.org/abs/2509.21097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21097">https://arxiv.org/pdf/2509.21097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21097]] GraphUniverse: Enabling Systematic Evaluation of Inductive Generalization(https://arxiv.org/abs/2509.21097)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>A fundamental challenge in graph learning is understanding how models generalize to new, unseen graphs. While synthetic benchmarks offer controlled settings for analysis, existing approaches are confined to single-graph, transductive settings where models train and test on the same graph structure. Addressing this gap, we introduce GraphUniverse, a framework for generating entire families of graphs to enable the first systematic evaluation of inductive generalization at scale. Our core innovation is the generation of graphs with persistent semantic communities, ensuring conceptual consistency while allowing fine-grained control over structural properties like homophily and degree distributions. This enables crucial but underexplored robustness tests, such as performance under controlled distribution shifts. Benchmarking a wide range of architectures -- from GNNs to graph transformers and topological architectures -- reveals that strong transductive performance is a poor predictor of inductive generalization. Furthermore, we find that robustness to distribution shift is highly sensitive not only to model architecture choice but also to the initial graph regime (e.g., high vs. low homophily). Beyond benchmarking, GraphUniverse's flexibility and scalability can facilitate the development of robust and truly generalizable architectures -- including next-generation graph foundation models. An interactive demo is available at this https URL.</li>
<li><strong>摘要：</strong>图形学习的一个基本挑战是了解模型如何推广到新的，看不见的图。虽然合成基准测试提供了受控设置进行分析，但现有方法仅限于单图设置，其中模型在同一图结构上训练和测试。在解决这一差距时，我们介绍了Graphuniverse，这是一个框架，用于生成整个图形家庭，以便在大规模上对归纳概括进行首次系统评估。我们的核心创新是具有持久语义群落的图形的产生，确保了概念的一致性，同时允许对诸如同性和程度分布之类的结构属性进行细粒度的控制。这可以实现至关重要但不充分的鲁棒性测试，例如在受控的分布变化下的性能。从GNN到图形变压器和拓扑结构进行基准测试广泛的体系结构 - 揭示了强大的转导性能是归纳概括的差预测指标。此外，我们发现分配转移的鲁棒性不仅对模型结构选择高度敏感，而且对初始图形制度（例如，高且低同质性）。除了基准测试之外，Fapheruniverse的灵活性和可扩展性还可以促进健壮且真正可概括的体系结构的发展 - 包括下一代图基础模型。此HTTPS URL可用交互式演示。</li>
</ul>

<h3>Title: MotionFlow:Learning Implicit Motion Flow for Complex Camera Trajectory Control in Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Guojun Lei, Chi Wang, Yikai Wang, Hong Li, Ying Song, Weiwei Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21119">https://arxiv.org/abs/2509.21119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21119">https://arxiv.org/pdf/2509.21119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21119]] MotionFlow:Learning Implicit Motion Flow for Complex Camera Trajectory Control in Video Generation(https://arxiv.org/abs/2509.21119)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Generating videos guided by camera trajectories poses significant challenges in achieving consistency and generalizability, particularly when both camera and object motions are present. Existing approaches often attempt to learn these motions separately, which may lead to confusion regarding the relative motion between the camera and the objects. To address this challenge, we propose a novel approach that integrates both camera and object motions by converting them into the motion of corresponding pixels. Utilizing a stable diffusion network, we effectively learn reference motion maps in relation to the specified camera trajectory. These maps, along with an extracted semantic object prior, are then fed into an image-to-video network to generate the desired video that can accurately follow the designated camera trajectory while maintaining consistent object motions. Extensive experiments verify that our model outperforms SOTA methods by a large margin.</li>
<li><strong>摘要：</strong>以相机轨迹为指导的产生视频在达到一致性和概括性方面构成了重大挑战，尤其是在存在相机和对象运动时。现有的方法通常试图单独学习这些动作，这可能会导致对摄像机和对象之间的相对运动的混乱。为了应对这一挑战，我们提出了一种新颖的方法，该方法通过将它们转换为相应像素的运动来整合相机和对象运动。利用稳定的扩散网络，我们有效地学习了与指定的摄像头轨迹相关的参考运动图。然后将这些地图以及提取的语义对象先验加入图像到视频网络，以生成所需的视频，该视频可以准确遵循指定的摄像头轨迹，同时保持一致的对象运动。广泛的实验证明，我们的模型的表现要优于SOTA方法。</li>
</ul>

<h3>Title: EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense</h3>
<ul>
<li><strong>Authors: </strong>Wei Huang, De-Tian Chu, Lin-Yuan Bai, Wei Kang, Hai-Tao Zhang, Bo Li, Zhi-Mo Han, Jing Ge, Hai-Feng Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21129">https://arxiv.org/abs/2509.21129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21129">https://arxiv.org/pdf/2509.21129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21129]] EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense(https://arxiv.org/abs/2509.21129)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Modern email spam and phishing attacks have evolved far beyond keyword blacklists or simple heuristics. Adversaries now craft multi-modal campaigns that combine natural-language text with obfuscated URLs, forged headers, and malicious attachments, adapting their strategies within days to bypass filters. Traditional spam detection systems, which rely on static rules or single-modality models, struggle to integrate heterogeneous signals or to continuously adapt, leading to rapid performance degradation. We propose EvoMail, a self-evolving cognitive agent framework for robust detection of spam and phishing. EvoMail first constructs a unified heterogeneous email graph that fuses textual content, metadata (headers, senders, domains), and embedded resources (URLs, attachments). A Cognitive Graph Neural Network enhanced by a Large Language Model (LLM) performs context-aware reasoning across these sources to identify coordinated spam campaigns. Most critically, EvoMail engages in an adversarial self-evolution loop: a ''red-team'' agent generates novel evasion tactics -- such as character obfuscation or AI-generated phishing text -- while the ''blue-team'' detector learns from failures, compresses experiences into a memory module, and reuses them for future reasoning. Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam, SpamAssassin, and TREC) and synthetic adversarial variants demonstrate that EvoMail consistently outperforms state-of-the-art baselines in detection accuracy, adaptability to evolving spam tactics, and interpretability of reasoning traces. These results highlight EvoMail's potential as a resilient and explainable defense framework against next-generation spam and phishing threats.</li>
<li><strong>摘要：</strong>现代的电子邮件垃圾邮件和网络钓鱼攻击远远超出了关键字黑名单或简单的启发式方法。现在的对手现在制作了多模式运动，将自然语言文本与混淆的URL，锻造的头条和恶意附件相结合，在几天之内调整策略以绕过过滤器。依靠静态规则或单模型模型的传统垃圾邮件检测系统难以整合异质信号或不断适应，从而导致迅速的性能下降。我们提出了Evomail，这是一种自我发展的认知剂框架，用于可靠地检测垃圾邮件和网络钓鱼。 Evomail首先构建了一个统一的异构电子邮件图，该电子邮件图融合了文本内容，元数据（标题，发件人，域）和嵌入式资源（URL，附件）。通过大型语言模型（LLM）增强的认知图神经网络在这些来源跨这些来源执行上下文感知的推理，以识别协调的垃圾邮件广告系列。最批判性的是，Evomail会参与对抗性的自我进化循环：A“红色团队”代理会产生新颖的逃避策略，例如角色混淆或AI生成的网络钓鱼文本，而“蓝色团队”探测器从失败中学习，将经验从内存模块中压缩为记忆模块，并将其重新提出以后的理性。对现实世界数据集（Enron-Spam，Ling-Spam，spamassassin和Trec）和合成对抗变体的广泛实验表明，在检测准确性中，Evomail始终优于先进基准，适应性的基准，适应性的垃圾邮件策略，以及对理由痕迹的解释能力。这些结果凸显了Evomail的潜力是针对下一代垃圾邮件和网络钓鱼威胁的弹性和可解释的防御框架。</li>
</ul>

<h3>Title: The Unwinnable Arms Race of AI Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Till Aczel, Lorenzo Vettor, Andreas Plesner, Roger Wattenhofer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21135">https://arxiv.org/abs/2509.21135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21135">https://arxiv.org/pdf/2509.21135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21135]] The Unwinnable Arms Race of AI Image Detection(https://arxiv.org/abs/2509.21135)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid progress of image generative AI has blurred the boundary between synthetic and real images, fueling an arms race between generators and discriminators. This paper investigates the conditions under which discriminators are most disadvantaged in this competition. We analyze two key factors: data dimensionality and data complexity. While increased dimensionality often strengthens the discriminators ability to detect subtle inconsistencies, complexity introduces a more nuanced effect. Using Kolmogorov complexity as a measure of intrinsic dataset structure, we show that both very simple and highly complex datasets reduce the detectability of synthetic images; generators can learn simple datasets almost perfectly, whereas extreme diversity masks imperfections. In contrast, intermediate-complexity datasets create the most favorable conditions for detection, as generators fail to fully capture the distribution and their errors remain visible.</li>
<li><strong>摘要：</strong>图像生成型AI的快速进步模糊了合成图像和真实图像之间的边界，从而加剧了发电机和歧视器之间的武器竞争。本文调查了歧视者在本次比赛中最不利的条件。我们分析了两个关键因素：数据维度和数据复杂性。尽管增加的维度通常会增强歧视者检测细微矛盾的能力，但复杂性引入了更细微的效果。使用Kolmogorov复杂性作为固有数据集结构的度量，我们表明非常简单且高度复杂的数据集降低了合成图像的可检测性。发电机几乎可以完美地学习简单的数据集，而极端多样性掩盖了瑕疵。相反，由于发电机无法完全捕获分布，并且其错误仍然可见，因此中间复杂数据集创造了最有利的检测条件。</li>
</ul>

<h3>Title: CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Ruiyu Wang, Shizhao Sun, Weijian Ma, Jiang Bian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21150">https://arxiv.org/abs/2509.21150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21150">https://arxiv.org/pdf/2509.21150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21150]] CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific Tokenization(https://arxiv.org/abs/2509.21150)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Computer-Aided Design (CAD) is a foundational component of industrial prototyping, where models are defined not by raw coordinates but by construction sequences such as sketches and extrusions. This sequential structure enables both efficient prototype initialization and subsequent editing. Text-guided CAD prototyping, which unifies Text-to-CAD generation and CAD editing, has the potential to streamline the entire design pipeline. However, prior work has not explored this setting, largely because standard large language model (LLM) tokenizers decompose CAD sequences into natural-language word pieces, failing to capture primitive-level CAD semantics and hindering attention modules from modeling geometric structure. We conjecture that a multimodal tokenization strategy, aligned with CAD's primitive and structural nature, can provide more effective representations. To this end, we propose CAD-Tokenizer, a framework that represents CAD data with modality-specific tokens using a sequence-based VQ-VAE with primitive-level pooling and constrained decoding. This design produces compact, primitive-aware representations that align with CAD's structural nature. Applied to unified text-guided CAD prototyping, CAD-Tokenizer significantly improves instruction following and generation quality, achieving better quantitative and qualitative performance over both general-purpose LLMs and task-specific baselines.</li>
<li><strong>摘要：</strong>计算机辅助设计（CAD）是工业原型制作的基础组成部分，其中模型不是由原始坐标来定义的，而是由构造序列（例如草图和挤出）定义。该顺序结构可以实现有效的原型初始化和随后的编辑。统一文本到基金生成和CAD编辑的文本指导的CAD原型制作有可能简化整个设计管道。但是，先前的工作尚未探索这种设置，这主要是因为标准的大语言模型（LLM）标记器将CAD序列分解为自然语言片段，因此无法捕获原始级别的CAD语义，并从建模几何结构中阻碍了注意力模块。我们推测，与CAD的原始和结构性质保持一致的多模式令牌化策略可以提供更有效的表示。为此，我们提出了CAD-Tokenizer，该框架使用基于序列的VQ-VAE具有具有原始级别池和约束解码的基于序列的VQ-VAE的CAD数据。该设计产生了与CAD的结构性质保持一致的紧凑，原始感知的表示。 CAD-Tokenizer应用于统一的文本引导的CAD原型制作，可显着提高跟随和发电质量的指导，从而比通用LLM和特定于任务的基线获得更好的定量和定性性能。</li>
</ul>

<h3>Title: A Unified Framework for Diffusion Model Unlearning with f-Divergence</h3>
<ul>
<li><strong>Authors: </strong>Nicola Novello, Federico Fontana, Luigi Cinque, Deniz Gunduz, Andrea M. Tonello</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21167">https://arxiv.org/abs/2509.21167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21167">https://arxiv.org/pdf/2509.21167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21167]] A Unified Framework for Diffusion Model Unlearning with f-Divergence(https://arxiv.org/abs/2509.21167)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Machine unlearning aims to remove specific knowledge from a trained model. While diffusion models (DMs) have shown remarkable generative capabilities, existing unlearning methods for text-to-image (T2I) models often rely on minimizing the mean squared error (MSE) between the output distribution of a target and an anchor concept. We show that this MSE-based approach is a special case of a unified $f$-divergence-based framework, in which any $f$-divergence can be utilized. We analyze the benefits of using different $f$-divergences, that mainly impact the convergence properties of the algorithm and the quality of unlearning. The proposed unified framework offers a flexible paradigm that allows to select the optimal divergence for a specific application, balancing different trade-offs between aggressive unlearning and concept preservation.</li>
<li><strong>摘要：</strong>Machine Unerning旨在从训练有素的模型中删除特定的知识。尽管扩散模型（DMS）表现出显着的生成能力，但现有的文本对图像（T2I）模型的未学习方法通​​常依赖于目标和锚概念的输出分布之间的平均平方误差（MSE）。我们表明，这种基于MSE的方法是统一的基于$ f $ divergence的框架的特殊情况，其中可以利用任何$ f $ divergence。我们分析了使用不同$ f $ divergences的好处，这些差异主要影响算法的收敛属性和未学习的质量。拟议的统一框架提供了一个灵活的范式，可以为特定应用程序选择最佳差异，从而平衡积极的学习和概念保存之间的不同权衡。</li>
</ul>

<h3>Title: Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Seyed Amir Kasaei, Ali Aghayari, Arash Marioriyad, Niki Sepasian, MohammadAmin Fazli, Mahdieh Soleymani Baghshah, Mohammad Hossein Rohban</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21227">https://arxiv.org/abs/2509.21227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21227">https://arxiv.org/pdf/2509.21227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21227]] Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation(https://arxiv.org/abs/2509.21227)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Text-image generation has advanced rapidly, but assessing whether outputs truly capture the objects, attributes, and relations described in prompts remains a central challenge. Evaluation in this space relies heavily on automated metrics, yet these are often adopted by convention or popularity rather than validated against human judgment. Because evaluation and reported progress in the field depend directly on these metrics, it is critical to understand how well they reflect human preferences. To address this, we present a broad study of widely used metrics for compositional text-image evaluation. Our analysis goes beyond simple correlation, examining their behavior across diverse compositional challenges and comparing how different metric families align with human judgments. The results show that no single metric performs consistently across tasks: performance varies with the type of compositional problem. Notably, VQA-based metrics, though popular, are not uniformly superior, while certain embedding-based metrics prove stronger in specific cases. Image-only metrics, as expected, contribute little to compositional evaluation, as they are designed for perceptual quality rather than alignment. These findings underscore the importance of careful and transparent metric selection, both for trustworthy evaluation and for their use as reward models in generation. Project page is available at \href{this https URL}{this URL}.</li>
<li><strong>摘要：</strong>文本图像的生成迅速发展，但是评估输出是否真正捕获提示中描述的对象，属性和关系仍然是一个核心挑战。在这个空间中的评估在很大程度上取决于自动指标，但是这些度量通常是通过惯例或受欢迎程度采用的，而不是根据人类判断的验证。由于该领域的评估和报告的进展直接取决于这些指标，因此了解它们反映人类偏好的程度至关重要。为了解决这个问题，我们介绍了广泛使用的指标，用于组成文本图像评估。我们的分析超越了简单的相关性，研究了它们在各种组成挑战中的行为，并比较了不同的度量家族与人类判断的一致性。结果表明，没有单个指标在任务之间持续执行：性能随组成问题的类型而变化。值得注意的是，基于VQA的指标虽然很受欢迎，但并不是统一的优越，而某些基于嵌入的指标在特定情况下则更强。正如预期的那样，仅图像指标对组成评估几乎没有贡献，因为它们是为了感知质量而不是对齐的。这些发现强调了谨慎和透明的度量选择的重要性，包括值得信赖的评估以及它们作为奖励模型的使用。项目页面可在\ href {此https url} {this url}上获得。</li>
</ul>

<h3>Title: Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets</h3>
<ul>
<li><strong>Authors: </strong>Team Hunyuan3D: Bowen Zhang, Chunchao Guo, Haolin Liu, Hongyu Yan, Huiwen Shi, Jingwei Huang, Junlin Yu, Kunhong Li, Linus, Penghao Wang, Qingxiang Lin, Sicong Liu, Xianghui Yang, Yixuan Tang, Yunfei Zhao, Zeqiang Lai, Zhihao Liang, Zibo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21245">https://arxiv.org/abs/2509.21245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21245">https://arxiv.org/pdf/2509.21245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21245]] Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets(https://arxiv.org/abs/2509.21245)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in 3D-native generative models have accelerated asset creation for games, film, and design. However, most methods still rely primarily on image or text conditioning and lack fine-grained, cross-modal controls, which limits controllability and practical adoption. To address this gap, we present Hunyuan3D-Omni, a unified framework for fine-grained, controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images, Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose priors as conditioning signals, enabling precise control over geometry, topology, and pose. Instead of separate heads for each modality, our model unifies all signals in a single cross-modal architecture. We train with a progressive, difficulty-aware sampling strategy that selects one control modality per example and biases sampling toward harder signals (e.g., skeletal pose) while downweighting easier ones (e.g., point clouds), encouraging robust multi-modal fusion and graceful handling of missing inputs. Experiments show that these additional controls improve generation accuracy, enable geometry-aware transformations, and increase robustness for production workflows.</li>
<li><strong>摘要：</strong>在3D-NENATIC生成模型中的最新进展已加速了游戏，电影和设计的资产创造。但是，大多数方法仍然主要依赖图像或文本调理，并且缺乏细粒度的跨模式控制，从而限制了可控性和实际采用。为了解决这一差距，我们提出了Hunyuan3d-omni，这是一个基于Hunyuan3d 2.1的细粒度，可控制的3D资产产生的统一框架。除了图像外，Hunyuan3D-OMNI还接受点云，体素，边界框和骨骼姿势先验作为条件信号，从而可以精确控制几何，拓扑和姿势。我们的模型不是单独的单独的头部，而是在单个跨模式体系结构中统一所有信号。我们采用渐进式，困难的抽样策略进行训练，该策略以示例选择一种控制方式，并偏向更难的信号（例如，骨骼姿势），同时减轻轻度更轻松的信号（例如，点云），鼓励强大的多模式融合和丢失的输入的优雅处理。实验表明，这些额外的控件提高了发电精度，实现了几何感知的转换，并提高了生产工作流的鲁棒性。</li>
</ul>

<h3>Title: Federated Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Zifan Wang, Anqi Dong, Mahmoud Selim, Michael M. Zavlanos, Karl H. Johansson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21250">https://arxiv.org/abs/2509.21250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21250">https://arxiv.org/pdf/2509.21250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21250]] Federated Flow Matching(https://arxiv.org/abs/2509.21250)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Data today is decentralized, generated and stored across devices and institutions where privacy, ownership, and regulation prevent centralization. This motivates the need to train generative models directly from distributed data locally without central aggregation. In this paper, we introduce Federated Flow Matching (FFM), a framework for training flow matching models under privacy constraints. Specifically, we first examine FFM-vanilla, where each client trains locally with independent source and target couplings, preserving privacy but yielding curved flows that slow inference. We then develop FFM-LOT, which employs local optimal transport couplings to improve straightness within each client but lacks global consistency under heterogeneous data. Finally, we propose FFM-GOT, a federated strategy based on the semi-dual formulation of optimal transport, where a shared global potential function coordinates couplings across clients. Experiments on synthetic and image datasets show that FFM enables privacy-preserving training while enhancing both the flow straightness and sample quality in federated settings, with performance comparable to the centralized baseline.</li>
<li><strong>摘要：</strong>当今的数据是分散，生成和存储在跨设备和机构中的，在这些设备和机构中，隐私，所有权和法规阻止集中化。这激发了需要直接从本地分布式数据训练生成模型的情况，而无需中央聚集。在本文中，我们介绍了联合流量匹配（FFM），这是一个在隐私限制下训练流匹配模型的框架。具体来说，我们首先检查了FFM-Vanilla，每个客户都会在本地使用独立的源和目标耦合进行训练，从而保留隐私，但产生了缓慢推理的弯曲流量。然后，我们开发FFM-LOT，该FFM-LOT采用本地最佳传输耦合来提高每个客户的直率，但在异质数据下缺乏全球一致性。最后，我们提出了FFM-GOT，这是一种基于最佳运输的半偶二偶配方的联合策略，在该策略中，共享的全球潜在函数可以在客户之间协调耦合。合成和图像数据集的实验表明，FFM可以实现隐私训练，同时增强了联合设置中的流动直率和样品质量，性能与集中式基线相当。</li>
</ul>

<h3>Title: Hallucination as an Upper Bound: A New Perspective on Text-to-Image Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Seyed Amir Kasaei, Mohammad Hossein Rohban</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21257">https://arxiv.org/abs/2509.21257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21257">https://arxiv.org/pdf/2509.21257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21257]] Hallucination as an Upper Bound: A New Perspective on Text-to-Image Evaluation(https://arxiv.org/abs/2509.21257)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In language and vision-language models, hallucination is broadly understood as content generated from a model's prior knowledge or biases rather than from the given input. While this phenomenon has been studied in those domains, it has not been clearly framed for text-to-image (T2I) generative models. Existing evaluations mainly focus on alignment, checking whether prompt-specified elements appear, but overlook what the model generates beyond the prompt. We argue for defining hallucination in T2I as bias-driven deviations and propose a taxonomy with three categories: attribute, relation, and object hallucinations. This framing introduces an upper bound for evaluation and surfaces hidden biases, providing a foundation for richer assessment of T2I models.</li>
<li><strong>摘要：</strong>在语言和视觉模型中，幻觉广泛理解为模型的先验知识或偏见而不是从给定输入中产生的内容。尽管这种现象已经在这些领域中进行了研究，但尚未明确构架文本对图像（T2i）生成模型。现有的评估主要集中于对齐，检查是否出现及时指定的元素，但忽略了模型超出提示的内容。我们认为将T2I中的幻觉定义为偏见驱动的偏差，并提出了一个分类法，其中有三类：属性，关系和对象幻觉。该框架引入了用于评估的上限，并且表面隐藏了偏见，为T2I模型的评估提供了基础。</li>
</ul>

<h3>Title: MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Liu, Guolei Sun, Cheng Wang, Yixuan Yuan, Ender Konukoglu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21265">https://arxiv.org/abs/2509.21265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21265">https://arxiv.org/pdf/2509.21265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21265]] MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation(https://arxiv.org/abs/2509.21265)</code><input type="text"></li>
<li><strong>Keywords: </strong>super-resolution</a></li>
<li><strong>Abstract: </strong>High-resolution (HR) medical videos are vital for accurate diagnosis, yet are hard to acquire due to hardware limitations and physiological constraints. Clinically, the collected low-resolution (LR) medical videos present unique challenges for video super-resolution (VSR) models, including camera shake, noise, and abrupt frame transitions, which result in significant optical flow errors and alignment difficulties. Additionally, tissues and organs exhibit continuous and nuanced structures, but current VSR models are prone to introducing artifacts and distorted features that can mislead doctors. To this end, we propose MedVSR, a tailored framework for medical VSR. It first employs Cross State-Space Propagation (CSSP) to address the imprecise alignment by projecting distant frames as control matrices within state-space models, enabling the selective propagation of consistent and informative features to neighboring frames for effective alignment. Moreover, we design an Inner State-Space Reconstruction (ISSR) module that enhances tissue structures and reduces artifacts with joint long-range spatial feature learning and large-kernel short-range information aggregation. Experiments across four datasets in diverse medical scenarios, including endoscopy and cataract surgeries, show that MedVSR significantly outperforms existing VSR models in reconstruction performance and efficiency. Code released at this https URL.</li>
<li><strong>摘要：</strong>高分辨率（HR）医疗视频对于准确的诊断至关重要，但是由于硬件限制和生理限制，很难获得。从临床上讲，收集的低分辨率（LR）医疗视频对视频超分辨率（VSR）模型提出了独特的挑战，包括摄像机震动，噪声和突然的框架过渡，这会导致严重的光流误差和对齐困难。此外，组织和器官表现出连续且细微的结构，但是当前的VSR模型容易引入可能误导医生的人工制品和扭曲的特征。为此，我们提出了MEDVSR，这是医学VSR的量身定制框架。它首先采用跨州空间传播（CSSP）来解决不精确的对准，通过将远处的框架投射为州空间模型中的控制矩阵，从而可以选择性地传播一致且内容丰富的相邻特征，以实现有效对齐。此外，我们设计了一个内部状态空间重建（ISSR）模块，该模块可以增强组织结构并通过关节远程空间特征学习和大型内核短距离信息聚合来减少伪影。在包括内窥镜检查和白内障手术在内的各种医疗方案中的四个数据集的实验表明，MEDVSR在重建性能和效率方面显着优于现有的VSR模型。在此HTTPS URL上发布的代码。</li>
</ul>

<h3>Title: SuperOffload: Unleashing the Power of Large-Scale LLM Training on Superchips</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Lian, Masahiro Tanaka, Olatunji Ruwase, Minjia Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21271">https://arxiv.org/abs/2509.21271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21271">https://arxiv.org/pdf/2509.21271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21271]] SuperOffload: Unleashing the Power of Large-Scale LLM Training on Superchips(https://arxiv.org/abs/2509.21271)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>The emergence of Superchips represents a significant advancement in next-generation AI hardware. These Superchips employ a tightly coupled heterogeneous architecture that integrates GPU and CPU on the same package, which offers unprecedented computational power. However, there has been scant research investigating how LLM training benefits from this new architecture. In this work, for the first time, we study LLM training solutions based on offloading for Superchips. We observe important differences between Superchips and traditional loosely-coupled GPU-CPU architecture, which necessitate revisiting prevailing assumptions about offloading. Based on that, we present SuperOffload, a Superchip-centric offloading system that simultaneously uses Hopper GPU, Grace CPU, and NVLink-C2C interconnect more efficiently. SuperOffload accomplishes this via a combination of techniques, such as adaptive weight offloading, bucketization repartitioning, Superchip-aware casting, speculative execution, and a highly optimized Adam optimizer for Grace CPUs. Our evaluation of SuperOffload on NVIDIA GH200 demonstrates up to 2.5x throughput improvement compared to state-of-the-art offloading-based systems, enabling training of up to 25B model on a single Superchip while achieving high training throughput. We also extend SuperOffload with ZeRO-style data parallelism and DeepSpeed-Ulysses sequence parallelism, enabling training of 13B model with sequence lengths up to 1 million tokens on 8 GH200 while achieving 55% MFU.</li>
<li><strong>摘要：</strong>SuperChips的出现代表了下一代AI硬件的重大进步。这些超级芯片采用紧密耦合的异质体系结构，将GPU和CPU集成在同一软件包上，该软件包提供了前所未有的计算能力。但是，研究了LLM培训如何从这种新建筑中受益的研究很少。在这项工作中，我们首次研究了基于SuperChips卸载的LLM培训解决方案。我们观察到超芯片与传统的松散耦合的GPU-CPU架构之间的重要差异，这需要重新审视有关卸载的主要假设。基于此，我们提出了SuperOffload，这是一个以超芯片为中心的卸载系统，同时使用Hopper GPU，GRACE CPU和NVLINK-C2C互连。 SuperOffload通过多种技术来实现这一目标，例如自适应重量卸载，桶装重新分配，超芯片感知铸造，投机执行以及对GRACE CPU的高度优化的ADAM Optimizer。与最新的基于卸载的系统相比，我们对NVIDIA GH200上的SuperOff载荷的评估表明，最高2.5倍的吞吐量改进，可以在单个超芯片上训练多达25B型号，同时实现高训练吞吐量。我们还使用零风格的数据并行性和DeepSpeed-ulysses序列并行性扩展了超卸载，从而可以对13B模型进行训练，其序列长度在8 GH200上高达100万个令牌，同时实现55％的MFU。</li>
</ul>

<h3>Title: A Sentinel-3 foundation model for ocean colour</h3>
<ul>
<li><strong>Authors: </strong>Geoffrey Dawson, Remy Vandaele, Andrew Taylor, David Moffat, Helen Tamura-Wicks, Sarah Jackson, Rosie Lickorish, Paolo Fraccaro, Hywel Williams, Chunbo Luo, Anne Jones</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21273">https://arxiv.org/abs/2509.21273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21273">https://arxiv.org/pdf/2509.21273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21273]] A Sentinel-3 foundation model for ocean colour(https://arxiv.org/abs/2509.21273)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence (AI) Foundation models (FMs), pre-trained on massive unlabelled datasets, have the potential to drastically change AI applications in ocean science, where labelled data are often sparse and expensive to collect. In this work, we describe a new foundation model using the Prithvi-EO Vision Transformer architecture which has been pre-trained to reconstruct data from the Sentinel-3 Ocean and Land Colour Instrument (OLCI). We evaluate the model by fine-tuning on two downstream marine earth observation tasks. We first assess model performance compared to current baseline models used to quantify chlorophyll concentration. We then evaluate the FMs ability to refine remote sensing-based estimates of ocean primary production. Our results demonstrate the utility of self-trained FMs for marine monitoring, in particular for making use of small amounts of high quality labelled data and in capturing detailed spatial patterns of ocean colour whilst matching point observations. We conclude that this new generation of geospatial AI models has the potential to provide more robust, data-driven insights into ocean ecosystems and their role in global climate processes.</li>
<li><strong>摘要：</strong>在大规模未标记数据集上进行培训的人工智能（AI）基础模型（FMS）有可能在海洋科学中大幅度更改AI应用程序，在海洋科学中，标有标记的数据通常很少且收集昂贵。在这项工作中，我们使用Prithvi-Eo Vision Transformer体系结构描述了一种新的基础模型，该模型已预先训练以重建Sentinel-3 Ocean and Land Color Instrument（OLCI）的数据。我们通过对两个下游海地观察任务进行微调来评估模型。我们首先评估模型性能与用于量化叶绿素浓度的当前基线模型相比。然后，我们评估FMS优化基于海洋初级生产的基于遥感的估计值的能力。我们的结果表明，自训练的FMS用于海洋监测的实用性，特别是用于利用少量高质量标记的数据，并捕获海洋颜色的详细空间模式，同时匹配点观测。我们得出的结论是，这种新一代的地理空间AI模型有可能为海洋生态系统及其在全球气候过程中的作用提供更强大的，数据驱动的见解。</li>
</ul>

<h3>Title: NewtonGen: Physics-Consistent and Controllable Text-to-Video Generation via Neural Newtonian Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Yu Yuan, Xijun Wang, Tharindu Wickremasinghe, Zeeshan Nadir, Bole Ma, Stanley H. Chan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21309">https://arxiv.org/abs/2509.21309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21309">https://arxiv.org/pdf/2509.21309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21309]] NewtonGen: Physics-Consistent and Controllable Text-to-Video Generation via Neural Newtonian Dynamics(https://arxiv.org/abs/2509.21309)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation</a></li>
<li><strong>Abstract: </strong>A primary bottleneck in large-scale text-to-video generation today is physical consistency and controllability. Despite recent advances, state-of-the-art models often produce unrealistic motions, such as objects falling upward, or abrupt changes in velocity and direction. Moreover, these models lack precise parameter control, struggling to generate physically consistent dynamics under different initial conditions. We argue that this fundamental limitation stems from current models learning motion distributions solely from appearance, while lacking an understanding of the underlying dynamics. In this work, we propose NewtonGen, a framework that integrates data-driven synthesis with learnable physical principles. At its core lies trainable Neural Newtonian Dynamics (NND), which can model and predict a variety of Newtonian motions, thereby injecting latent dynamical constraints into the video generation process. By jointly leveraging data priors and dynamical guidance, NewtonGen enables physically consistent video synthesis with precise parameter control.</li>
<li><strong>摘要：</strong>当今大规模文本到视频的大规模瓶颈是身体的一致性和可控性。尽管有最近的进步，但最先进的模型通常会产生不切实际的动作，例如对象向上掉落，或速度和方向的突然变化。此外，这些模型缺乏精确的参数控制，在不同的初始条件下努力生成身体一致的动态。我们认为，这种基本限制源于当前模型学习运动分布仅来自外观，同时缺乏对基本动力学的理解。在这项工作中，我们提出了Newtongen，该框架将数据驱动的合成与可学习的物理原理集成在一起。以可训练的神经牛顿动力学（NND）为核心，可以对牛顿动作进行建模和预测，从而将潜在的动力约束注入视频生成过程中。通过共同利用数据先验和动态指导，纽腾根可以通过精确的参数控制实现身体一致的视频综合。</li>
</ul>

<h3>Title: SD3.5-Flash: Distribution-Guided Distillation of Generative Flows</h3>
<ul>
<li><strong>Authors: </strong>Hmrishav Bandyopadhyay, Rahim Entezari, Jim Scott, Reshinth Adithyan, Yi-Zhe Song, Varun Jampani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21318">https://arxiv.org/abs/2509.21318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21318">https://arxiv.org/pdf/2509.21318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21318]] SD3.5-Flash: Distribution-Guided Distillation of Generative Flows(https://arxiv.org/abs/2509.21318)</code><input type="text"></li>
<li><strong>Keywords: </strong>generation, generative</a></li>
<li><strong>Abstract: </strong>We present SD3.5-Flash, an efficient few-step distillation framework that brings high-quality image generation to accessible consumer devices. Our approach distills computationally prohibitive rectified flow models through a reformulated distribution matching objective tailored specifically for few-step generation. We introduce two key innovations: "timestep sharing" to reduce gradient noise and "split-timestep fine-tuning" to improve prompt alignment. Combined with comprehensive pipeline optimizations like text encoder restructuring and specialized quantization, our system enables both rapid generation and memory-efficient deployment across different hardware configurations. This democratizes access across the full spectrum of devices, from mobile phones to desktop computers. Through extensive evaluation including large-scale user studies, we demonstrate that SD3.5-Flash consistently outperforms existing few-step methods, making advanced generative AI truly accessible for practical deployment.</li>
<li><strong>摘要：</strong>我们提出了SD3.5-Flash，这是一个有效的几步蒸馏框架，它为可访问的消费者设备带来了高质量的图像生成。我们的方法通过专门针对几步生成量身定制的重新计算分布匹配的物镜来提炼计算过度的整流流模型。我们介绍了两个关键的创新：“ TimeStep共享”，以减少梯度噪声和“分裂式微调”，以提高及时的对齐。结合全面的管道优化，例如文本编码器重组和专门量化，我们的系统可以在不同的硬件配置上快速生成和内存有效的部署。这使从手机到台式计算机的各种设备的访问使访问民主化。通过包括大规模用户研究在内的广泛评估，我们证明了SD3.5-Flash始终优于现有的几步方法，这使得先进的生成型AI真正可以用于实际部署。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
