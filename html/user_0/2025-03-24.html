<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-03-24</h1>
<h3>Title: Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science</h3>
<ul>
<li><strong>Authors: </strong>Lachlan McGinness, Peter Baumgartner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16515">https://arxiv.org/abs/2503.16515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16515">https://arxiv.org/pdf/2503.16515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16515]] Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science(https://arxiv.org/abs/2503.16515)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) were used to assist four Commonwealth Scientific and Industrial Research Organisation (CSIRO) researchers to perform systematic literature reviews (SLR). We evaluate the performance of LLMs for SLR tasks in these case studies. In each, we explore the impact of changing parameters on the accuracy of LLM responses. The LLM was tasked with extracting evidence from chosen academic papers to answer specific research questions. We evaluate the models' performance in faithfully reproducing quotes from the literature and subject experts were asked to assess the model performance in answering the research questions. We developed a semantic text highlighting tool to facilitate expert review of LLM responses. We found that state of the art LLMs were able to reproduce quotes from texts with greater than 95% accuracy and answer research questions with an accuracy of approximately 83%. We use two methods to determine the correctness of LLM responses; expert review and the cosine similarity of transformer embeddings of LLM and expert answers. The correlation between these methods ranged from 0.48 to 0.77, providing evidence that the latter is a valid metric for measuring semantic similarity.</li>
</ul>

<h3>Title: Using LLMs for Automated Privacy Policy Analysis: Prompt Engineering, Fine-Tuning and Explainability</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Chen, Peng Tang, Weidong Qiu, Shujun Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16516">https://arxiv.org/abs/2503.16516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16516">https://arxiv.org/pdf/2503.16516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16516]] Using LLMs for Automated Privacy Policy Analysis: Prompt Engineering, Fine-Tuning and Explainability(https://arxiv.org/abs/2503.16516)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, explainability, large language model</a></li>
<li><strong>Abstract: </strong>Privacy policies are widely used by digital services and often required for legal purposes. Many machine learning based classifiers have been developed to automate detection of different concepts in a given privacy policy, which can help facilitate other automated tasks such as producing a more reader-friendly summary and detecting legal compliance issues. Despite the successful applications of large language models (LLMs) to many NLP tasks in various domains, there is very little work studying the use of LLMs for automated privacy policy analysis, therefore, if and how LLMs can help automate privacy policy analysis remains under-explored. To fill this research gap, we conducted a comprehensive evaluation of LLM-based privacy policy concept classifiers, employing both prompt engineering and LoRA (low-rank adaptation) fine-tuning, on four state-of-the-art (SOTA) privacy policy corpora and taxonomies. Our experimental results demonstrated that combining prompt engineering and fine-tuning can make LLM-based classifiers outperform other SOTA methods, \emph{significantly} and \emph{consistently} across privacy policy corpora/taxonomies and concepts. Furthermore, we evaluated the explainability of the LLM-based classifiers using three metrics: completeness, logicality, and comprehensibility. For all three metrics, a score exceeding 91.1\% was observed in our evaluation, indicating that LLMs are not only useful to improve the classification performance, but also to enhance the explainability of detection results.</li>
</ul>

<h3>Title: Mind2: Mind-to-Mind Emotional Support System with Bidirectional Cognitive Discourse Analysis</h3>
<ul>
<li><strong>Authors: </strong>Shi Yin Hong, Uttamasha Oyshi, Quan Mai, Gibson Nkhata, Susan Gauch</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16523">https://arxiv.org/abs/2503.16523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16523">https://arxiv.org/pdf/2503.16523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16523]] Mind2: Mind-to-Mind Emotional Support System with Bidirectional Cognitive Discourse Analysis(https://arxiv.org/abs/2503.16523)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Emotional support (ES) systems alleviate users' mental distress by generating strategic supportive dialogues based on diverse user situations. However, ES systems are limited in their ability to generate effective ES dialogues that include timely context and interpretability, hindering them from earning public trust. Driven by cognitive models, we propose Mind-to-Mind (Mind2), an ES framework that approaches interpretable ES context modeling for the ES dialogue generation task from a discourse analysis perspective. Specifically, we perform cognitive discourse analysis on ES dialogues according to our dynamic discourse context propagation window, which accommodates evolving context as the conversation between the ES system and user progresses. To enhance interpretability, Mind2 prioritizes details that reflect each speaker's belief about the other speaker with bidirectionality, integrating Theory-of-Mind, physiological expected utility, and cognitive rationality to extract cognitive knowledge from ES conversations. Experimental results support that Mind2 achieves competitive performance versus state-of-the-art ES systems while trained with only 10\% of the available training data.</li>
</ul>

<h3>Title: KVShare: Semantic-Aware Key-Value Cache Sharing for Efficient Large Language Model Inference</h3>
<ul>
<li><strong>Authors: </strong>Huan Yang, Renji Zhang, Deyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16525">https://arxiv.org/abs/2503.16525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16525">https://arxiv.org/pdf/2503.16525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16525]] KVShare: Semantic-Aware Key-Value Cache Sharing for Efficient Large Language Model Inference(https://arxiv.org/abs/2503.16525)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents KVShare, a multi-user Key-Value (KV) Cache sharing technology based on semantic similarity, designed to enhance the inference efficiency of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Addressing the limitations of existing prefix caching (strict text prefix matching) and semantic caching (loss of response diversity), KVShare achieves fine-grained KV cache reuse through semantic alignment algorithms and differential editing operations. Experiments on real-world user conversation datasets demonstrate that KVShare improves KV cache hit rates by over 60%, while maintaining output quality comparable to full computation (no significant degradation in BLEU and Rouge-L metrics). This approach effectively reduces GPU resource consumption and is applicable to scenarios with repetitive queries, such as healthcare and education.</li>
</ul>

<h3>Title: LLM Generated Persona is a Promise with a Catch</h3>
<ul>
<li><strong>Authors: </strong>Ang Li, Haozhe Chen, Hongseok Namkoong, Tianyi Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16527">https://arxiv.org/abs/2503.16527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16527">https://arxiv.org/pdf/2503.16527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16527]] LLM Generated Persona is a Promise with a Catch(https://arxiv.org/abs/2503.16527)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>The use of large language models (LLMs) to simulate human behavior has gained significant attention, particularly through personas that approximate individual characteristics. Persona-based simulations hold promise for transforming disciplines that rely on population-level feedback, including social science, economic analysis, marketing research, and business operations. Traditional methods to collect realistic persona data face significant challenges. They are prohibitively expensive and logistically challenging due to privacy constraints, and often fail to capture multi-dimensional attributes, particularly subjective qualities. Consequently, synthetic persona generation with LLMs offers a scalable, cost-effective alternative. However, current approaches rely on ad hoc and heuristic generation techniques that do not guarantee methodological rigor or simulation precision, resulting in systematic biases in downstream tasks. Through extensive large-scale experiments including presidential election forecasts and general opinion surveys of the U.S. population, we reveal that these biases can lead to significant deviations from real-world outcomes. Our findings underscore the need to develop a rigorous science of persona generation and outline the methodological innovations, organizational and institutional support, and empirical foundations required to enhance the reliability and scalability of LLM-driven persona simulations. To support further research and development in this area, we have open-sourced approximately one million generated personas, available for public access and analysis at this https URL.</li>
</ul>

<h3>Title: HDLCoRe: A Training-Free Framework for Mitigating Hallucinations in LLM-Generated HDL</h3>
<ul>
<li><strong>Authors: </strong>Heng Ping, Shixuan Li, Peiyu Zhang, Anzhe Cheng, Shukai Duan, Nikos Kanakaris, Xiongye Xiao, Wei Yang, Shahin Nazarian, Andrei Irimia, Paul Bogdan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16528">https://arxiv.org/abs/2503.16528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16528">https://arxiv.org/pdf/2503.16528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16528]] HDLCoRe: A Training-Free Framework for Mitigating Hallucinations in LLM-Generated HDL(https://arxiv.org/abs/2503.16528)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, when applied to hardware description languages (HDL), these models exhibit significant limitations due to data scarcity, resulting in hallucinations and incorrect code generation. To address these challenges, we propose HDLCoRe, a training-free framework that enhances LLMs' HDL generation capabilities through prompt engineering techniques and retrieval-augmented generation (RAG). Our approach consists of two main components: (1) an HDL-aware Chain-of-Thought (CoT) prompting technique with self-verification that classifies tasks by complexity and type, incorporates domain-specific knowledge, and guides LLMs through step-by-step self-simulation for error correction; and (2) a two-stage heterogeneous RAG system that addresses formatting inconsistencies through key component extraction and efficiently retrieves relevant HDL examples through sequential filtering and re-ranking. HDLCoRe eliminates the need for model fine-tuning while substantially improving LLMs' HDL generation capabilities. Experimental results demonstrate that our framework achieves superior performance on the RTLLM2.0 benchmark, significantly reducing hallucinations and improving both syntactic and functional correctness.</li>
</ul>

<h3>Title: Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts</h3>
<ul>
<li><strong>Authors: </strong>Wenjing Zhang, Xuejiao Lei, Zhaoxiang Liu, Limin Han, Jiaojiao Zhao, Beibei Huang, Zhenhong Long, Junting Guo, Meijuan An, Rongjia Du, Ning Wang, Kai Wang, Shiguo Lian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16529">https://arxiv.org/abs/2503.16529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16529">https://arxiv.org/pdf/2503.16529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16529]] Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts(https://arxiv.org/abs/2503.16529)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>DeepSeek-R1, renowned for its exceptional reasoning capabilities and open-source strategy, is significantly influencing the global artificial intelligence landscape. However, it exhibits notable safety shortcomings. Recent research conducted by Robust Intelligence, a subsidiary of Cisco, in collaboration with the University of Pennsylvania, revealed that DeepSeek-R1 achieves a 100\% attack success rate when processing harmful prompts. Furthermore, multiple security firms and research institutions have identified critical security vulnerabilities within the model. Although China Unicom has uncovered safety vulnerabilities of R1 in Chinese contexts, the safety capabilities of the remaining distilled models in the R1 series have not yet been comprehensively evaluated. To address this gap, this study utilizes the comprehensive Chinese safety benchmark CHiSafetyBench to conduct an in-depth safety evaluation of the DeepSeek-R1 series distilled models. The objective is to assess the safety capabilities of these models in Chinese contexts both before and after distillation, and to further elucidate the adverse effects of distillation on model safety. Building on these findings, we implement targeted safety enhancements for six distilled models. Evaluation results indicate that the enhanced models achieve significant improvements in safety while maintaining reasoning capabilities without notable degradation. We open-source the safety-enhanced models at this https URL to serve as a valuable resource for future research and optimization of DeepSeek models.</li>
</ul>

<h3>Title: Enhancing LLM Generation with Knowledge Hypergraph for Evidence-Based Medicine</h3>
<ul>
<li><strong>Authors: </strong>Chengfeng Dou, Ying Zhang, Zhi Jin, Wenpin Jiao, Haiyan Zhao, Yongqiang Zhao, Zhengwei Tao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16530">https://arxiv.org/abs/2503.16530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16530">https://arxiv.org/pdf/2503.16530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16530]] Enhancing LLM Generation with Knowledge Hypergraph for Evidence-Based Medicine(https://arxiv.org/abs/2503.16530)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Evidence-based medicine (EBM) plays a crucial role in the application of large language models (LLMs) in healthcare, as it provides reliable support for medical decision-making processes. Although it benefits from current retrieval-augmented generation~(RAG) technologies, it still faces two significant challenges: the collection of dispersed evidence and the efficient organization of this evidence to support the complex queries necessary for EBM. To tackle these issues, we propose using LLMs to gather scattered evidence from multiple sources and present a knowledge hypergraph-based evidence management model to integrate these evidence while capturing intricate relationships. Furthermore, to better support complex queries, we have developed an Importance-Driven Evidence Prioritization (IDEP) algorithm that utilizes the LLM to generate multiple evidence features, each with an associated importance score, which are then used to rank the evidence and produce the final retrieval results. Experimental results from six datasets demonstrate that our approach outperforms existing RAG techniques in application domains of interest to EBM, such as medical quizzing, hallucination detection, and decision support. Testsets and the constructed knowledge graph can be accessed at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: From Patient Consultations to Graphs: Leveraging LLMs for Patient Journey Knowledge Graph Construction</h3>
<ul>
<li><strong>Authors: </strong>Hassan S. Al Khatib, Sudip Mittal, Shahram Rahimi, Nina Marhamati, Sean Bozorgzad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16533">https://arxiv.org/abs/2503.16533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16533">https://arxiv.org/pdf/2503.16533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16533]] From Patient Consultations to Graphs: Leveraging LLMs for Patient Journey Knowledge Graph Construction(https://arxiv.org/abs/2503.16533)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The transition towards patient-centric healthcare necessitates a comprehensive understanding of patient journeys, which encompass all healthcare experiences and interactions across the care spectrum. Existing healthcare data systems are often fragmented and lack a holistic representation of patient trajectories, creating challenges for coordinated care and personalized interventions. Patient Journey Knowledge Graphs (PJKGs) represent a novel approach to addressing the challenge of fragmented healthcare data by integrating diverse patient information into a unified, structured representation. This paper presents a methodology for constructing PJKGs using Large Language Models (LLMs) to process and structure both formal clinical documentation and unstructured patient-provider conversations. These graphs encapsulate temporal and causal relationships among clinical encounters, diagnoses, treatments, and outcomes, enabling advanced temporal reasoning and personalized care insights. The research evaluates four different LLMs, such as Claude 3.5, Mistral, Llama 3.1, and Chatgpt4o, in their ability to generate accurate and computationally efficient knowledge graphs. Results demonstrate that while all models achieved perfect structural compliance, they exhibited variations in medical entity processing and computational efficiency. The paper concludes by identifying key challenges and future research directions. This work contributes to advancing patient-centric healthcare through the development of comprehensive, actionable knowledge graphs that support improved care coordination and outcome prediction.</li>
</ul>

<h3>Title: Gender and content bias in Large Language Models: a case study on Google Gemini 2.0 Flash Experimental</h3>
<ul>
<li><strong>Authors: </strong>Roberto Balestri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16534">https://arxiv.org/abs/2503.16534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16534">https://arxiv.org/pdf/2503.16534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16534]] Gender and content bias in Large Language Models: a case study on Google Gemini 2.0 Flash Experimental(https://arxiv.org/abs/2503.16534)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>This study evaluates the biases in Gemini 2.0 Flash Experimental, a state-of-the-art large language model (LLM) developed by Google, focusing on content moderation and gender disparities. By comparing its performance to ChatGPT-4o, examined in a previous work of the author, the analysis highlights some differences in ethical moderation practices. Gemini 2.0 demonstrates reduced gender bias, notably with female-specific prompts achieving a substantial rise in acceptance rates compared to results obtained by ChatGPT-4o. It adopts a more permissive stance toward sexual content and maintains relatively high acceptance rates for violent prompts, including gender-specific cases. Despite these changes, whether they constitute an improvement is debatable. While gender bias has been reduced, this reduction comes at the cost of permitting more violent content toward both males and females, potentially normalizing violence rather than mitigating harm. Male-specific prompts still generally receive higher acceptance rates than female-specific ones. These findings underscore the complexities of aligning AI systems with ethical standards, highlighting progress in reducing certain biases while raising concerns about the broader implications of the model's permissiveness. Ongoing refinements are essential to achieve moderation practices that ensure transparency, fairness, and inclusivity without amplifying harmful content.</li>
</ul>

<h3>Title: Word2Minecraft: Generating 3D Game Levels through Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shuo Huang, Muhammad Umair Nasir, Steven James, Julian Togelius</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16536">https://arxiv.org/abs/2503.16536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16536">https://arxiv.org/pdf/2503.16536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16536]] Word2Minecraft: Generating 3D Game Levels through Large Language Models(https://arxiv.org/abs/2503.16536)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present Word2Minecraft, a system that leverages large language models to generate playable game levels in Minecraft based on structured stories. The system transforms narrative elements-such as protagonist goals, antagonist challenges, and environmental settings-into game levels with both spatial and gameplay constraints. We introduce a flexible framework that allows for the customization of story complexity, enabling dynamic level generation. The system employs a scaling algorithm to maintain spatial consistency while adapting key game elements. We evaluate Word2Minecraft using both metric-based and human-based methods. Our results show that GPT-4-Turbo outperforms GPT-4o-Mini in most areas, including story coherence and objective enjoyment, while the latter excels in aesthetic appeal. We also demonstrate the system' s ability to generate levels with high map enjoyment, offering a promising step forward in the intersection of story generation and game design. We open-source the code at this https URL</li>
</ul>

<h3>Title: Do Multimodal Large Language Models Understand Welding?</h3>
<ul>
<li><strong>Authors: </strong>Grigorii Khvatskii, Yong Suk Lee, Corey Angst, Maria Gibbs, Robert Landers, Nitesh V. Chawla</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16537">https://arxiv.org/abs/2503.16537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16537">https://arxiv.org/pdf/2503.16537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16537]] Do Multimodal Large Language Models Understand Welding?(https://arxiv.org/abs/2503.16537)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper examines the performance of Multimodal LLMs (MLLMs) in skilled production work, with a focus on welding. Using a novel data set of real-world and online weld images, annotated by a domain expert, we evaluate the performance of two state-of-the-art MLLMs in assessing weld acceptability across three contexts: RV \& Marine, Aeronautical, and Farming. While both models perform better on online images, likely due to prior exposure or memorization, they also perform relatively well on unseen, real-world weld images. Additionally, we introduce WeldPrompt, a prompting strategy that combines Chain-of-Thought generation with in-context learning to mitigate hallucinations and improve reasoning. WeldPrompt improves model recall in certain contexts but exhibits inconsistent performance across others. These results underscore the limitations and potentials of MLLMs in high-stakes technical domains and highlight the importance of fine-tuning, domain-specific data, and more sophisticated prompting strategies to improve model reliability. The study opens avenues for further research into multimodal learning in industry applications.</li>
</ul>

<h3>Title: Leveraging Vision-Language Models for Open-Vocabulary Instance Segmentation and Tracking</h3>
<ul>
<li><strong>Authors: </strong>Bastian PÃ¤tzold, Jan Nogga, Sven Behnke</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16538">https://arxiv.org/abs/2503.16538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16538">https://arxiv.org/pdf/2503.16538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16538]] Leveraging Vision-Language Models for Open-Vocabulary Instance Segmentation and Tracking(https://arxiv.org/abs/2503.16538)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel approach that leverages the capabilities of vision-language models (VLMs) by integrating them with established approaches for open-vocabulary detection (OVD), instance segmentation, and tracking. We utilize VLM-generated structured descriptions to identify visible object instances, collect application-relevant attributes, and inform an open-vocabulary detector to extract corresponding bounding boxes that are passed to a video segmentation model providing precise segmentation masks and tracking capabilities. Once initialized, this model can then directly extract segmentation masks, allowing processing of image streams in real time with minimal computational overhead. Tracks can be updated online as needed by generating new structured descriptions and corresponding open-vocabulary detections. This combines the descriptive power of VLMs with the grounding capability of OVD and the pixel-level understanding and speed of video segmentation. Our evaluation across datasets and robotics platforms demonstrates the broad applicability of this approach, showcasing its ability to extract task-specific attributes from non-standard objects in dynamic environments.</li>
</ul>

<h3>Title: Poly-FEVER: A Multilingual Fact Verification Benchmark for Hallucination Detection in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hanzhi Zhang, Sumera Anjum, Heng Fan, Weijian Zheng, Yan Huang, Yunhe Feng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16541">https://arxiv.org/abs/2503.16541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16541">https://arxiv.org/pdf/2503.16541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16541]] Poly-FEVER: A Multilingual Fact Verification Benchmark for Hallucination Detection in Large Language Models(https://arxiv.org/abs/2503.16541)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Hallucinations in generative AI, particularly in Large Language Models (LLMs), pose a significant challenge to the reliability of multilingual applications. Existing benchmarks for hallucination detection focus primarily on English and a few widely spoken languages, lacking the breadth to assess inconsistencies in model performance across diverse linguistic contexts. To address this gap, we introduce Poly-FEVER, a large-scale multilingual fact verification benchmark specifically designed for evaluating hallucination detection in LLMs. Poly-FEVER comprises 77,973 labeled factual claims spanning 11 languages, sourced from FEVER, Climate-FEVER, and SciFact. It provides the first large-scale dataset tailored for analyzing hallucination patterns across languages, enabling systematic evaluation of LLMs such as ChatGPT and the LLaMA series. Our analysis reveals how topic distribution and web resource availability influence hallucination frequency, uncovering language-specific biases that impact model accuracy. By offering a multilingual benchmark for fact verification, Poly-FEVER facilitates cross-linguistic comparisons of hallucination detection and contributes to the development of more reliable, language-inclusive AI systems. The dataset is publicly available to advance research in responsible AI, fact-checking methodologies, and multilingual NLP, promoting greater transparency and robustness in LLM performance. The proposed Poly-FEVER is available at: this https URL.</li>
</ul>

<h3>Title: Defending Against Gradient Inversion Attacks for Biomedical Images via Learnable Data Perturbation</h3>
<ul>
<li><strong>Authors: </strong>Shiyi Jiang, Farshad Firouzi, Krishnendu Chakrabarty</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16542">https://arxiv.org/abs/2503.16542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16542">https://arxiv.org/pdf/2503.16542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16542]] Defending Against Gradient Inversion Attacks for Biomedical Images via Learnable Data Perturbation(https://arxiv.org/abs/2503.16542)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense, attack, federate</a></li>
<li><strong>Abstract: </strong>The increasing need for sharing healthcare data and collaborating on clinical research has raised privacy concerns. Health information leakage due to malicious attacks can lead to serious problems such as misdiagnoses and patient identification issues. Privacy-preserving machine learning (PPML) and privacy-enhancing technologies, particularly federated learning (FL), have emerged in recent years as innovative solutions to balance privacy protection with data utility; however, they also suffer from inherent privacy vulnerabilities. Gradient inversion attacks constitute major threats to data sharing in federated learning. Researchers have proposed many defenses against gradient inversion attacks. However, current defense methods for healthcare data lack generalizability, i.e., existing solutions may not be applicable to data from a broader range of populations. In addition, most existing defense methods are tested using non-healthcare data, which raises concerns about their applicability to real-world healthcare systems. In this study, we present a defense against gradient inversion attacks in federated learning. We achieve this using latent data perturbation and minimax optimization, utilizing both general and medical image datasets. Our method is compared to two baselines, and the results show that our approach can outperform the baselines with a reduction of 12.5% in the attacker's accuracy in classifying reconstructed images. The proposed method also yields an increase of over 12.4% in Mean Squared Error (MSE) between the original and reconstructed images at the same level of model utility of around 90% client classification accuracy. The results suggest the potential of a generalizable defense for healthcare data.</li>
</ul>

<h3>Title: Causal Discovery and Counterfactual Reasoning to Optimize Persuasive Dialogue Policies</h3>
<ul>
<li><strong>Authors: </strong>Donghuo Zeng, Roberto Legaspi, Yuewen Sun, Xinshuai Dong, Kazushi Ikeda, Peter Spirtes, Kun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16544">https://arxiv.org/abs/2503.16544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16544">https://arxiv.org/pdf/2503.16544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16544]] Causal Discovery and Counterfactual Reasoning to Optimize Persuasive Dialogue Policies(https://arxiv.org/abs/2503.16544)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Tailoring persuasive conversations to users leads to more effective persuasion. However, existing dialogue systems often struggle to adapt to dynamically evolving user states. This paper presents a novel method that leverages causal discovery and counterfactual reasoning for optimizing system persuasion capability and outcomes. We employ the Greedy Relaxation of the Sparsest Permutation (GRaSP) algorithm to identify causal relationships between user and system utterance strategies, treating user strategies as states and system strategies as actions. GRaSP identifies user strategies as causal factors influencing system responses, which inform Bidirectional Conditional Generative Adversarial Networks (BiCoGAN) in generating counterfactual utterances for the system. Subsequently, we use the Dueling Double Deep Q-Network (D3QN) model to utilize counterfactual data to determine the best policy for selecting system utterances. Our experiments with the PersuasionForGood dataset show measurable improvements in persuasion outcomes using our approach over baseline methods. The observed increase in cumulative rewards and Q-values highlights the effectiveness of causal discovery in enhancing counterfactual reasoning and optimizing reinforcement learning policies for online dialogue systems.</li>
</ul>

<h3>Title: A Comprehensive Survey on Architectural Advances in Deep CNNs: Challenges, Applications, and Emerging Research Directions</h3>
<ul>
<li><strong>Authors: </strong>Saddam Hussain Khan, Rashid Iqbal (Artificial Intelligence Lab, Department of Computer Systems Engineering, University of Engineering and Applied Sciences (UEAS), Swat, Pakistan)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16546">https://arxiv.org/abs/2503.16546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16546">https://arxiv.org/pdf/2503.16546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16546]] A Comprehensive Survey on Architectural Advances in Deep CNNs: Challenges, Applications, and Emerging Research Directions(https://arxiv.org/abs/2503.16546)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, federate, transformer, generative</a></li>
<li><strong>Abstract: </strong>Deep Convolutional Neural Networks (CNNs) have significantly advanced deep learning, driving breakthroughs in computer vision, natural language processing, medical diagnosis, object detection, and speech recognition. Architectural innovations including 1D, 2D, and 3D convolutional models, dilated and grouped convolutions, depthwise separable convolutions, and attention mechanisms address domain-specific challenges and enhance feature representation and computational efficiency. Structural refinements such as spatial-channel exploitation, multi-path design, and feature-map enhancement contribute to robust hierarchical feature extraction and improved generalization, particularly through transfer learning. Efficient preprocessing strategies, including Fourier transforms, structured transforms, low-precision computation, and weight compression, optimize inference speed and facilitate deployment in resource-constrained environments. This survey presents a unified taxonomy that classifies CNN architectures based on spatial exploitation, multi-path structures, depth, width, dimensionality expansion, channel boosting, and attention mechanisms. It systematically reviews CNN applications in face recognition, pose estimation, action recognition, text classification, statistical language modeling, disease diagnosis, radiological analysis, cryptocurrency sentiment prediction, 1D data processing, video analysis, and speech recognition. In addition to consolidating architectural advancements, the review highlights emerging learning paradigms such as few-shot, zero-shot, weakly supervised, federated learning frameworks and future research directions include hybrid CNN-transformer models, vision-language integration, generative learning, etc. This review provides a comprehensive perspective on CNN's evolution from 2015 to 2025, outlining key innovations, challenges, and opportunities.</li>
</ul>

<h3>Title: MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical Problems</h3>
<ul>
<li><strong>Authors: </strong>Felix Chen, Hangjie Yuan, Yunqiu Xu, Tao Feng, Jun Cen, Pengwei Liu, Zeying Huang, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16549">https://arxiv.org/abs/2503.16549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16549">https://arxiv.org/pdf/2503.16549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16549]] MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical Problems(https://arxiv.org/abs/2503.16549)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite impressive performance across diverse tasks, Multimodal Large Language Models (MLLMs) have yet to fully demonstrate their potential in visual mathematical problem-solving, particularly in accurately perceiving and interpreting diagrams. Inspired by typical processes of humans, we hypothesize that the perception capabilities to extract meaningful information from diagrams is crucial, as it directly impacts subsequent inference processes. To validate this hypothesis, we developed FlowVerse, a comprehensive benchmark that categorizes all information used during problem-solving into four components, which are then combined into six problem versions for evaluation. Our preliminary results on FlowVerse reveal that existing MLLMs exhibit substantial limitations when extracting essential information and reasoned property from diagrams and performing complex reasoning based on these visual inputs. In response, we introduce MathFlow, a modular problem-solving pipeline that decouples perception and inference into distinct stages, thereby optimizing each independently. Given the perceptual limitations observed in current MLLMs, we trained MathFlow-P-7B as a dedicated perception model. Experimental results indicate that MathFlow-P-7B yields substantial performance gains when integrated with various closed-source and open-source inference models. This demonstrates the effectiveness of the MathFlow pipeline and its compatibility to diverse inference frameworks. The FlowVerse benchmark and code are available at this https URL.</li>
</ul>

<h3>Title: Unified Enhancement of the Generalization and Robustness of Language Models via Bi-Stage Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yudao Sun, Juan Yin, Juan Zhao, Fan Zhang, Yongheng Liu, Hongji Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16550">https://arxiv.org/abs/2503.16550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16550">https://arxiv.org/pdf/2503.16550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16550]] Unified Enhancement of the Generalization and Robustness of Language Models via Bi-Stage Optimization(https://arxiv.org/abs/2503.16550)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural network language models (LMs) are confronted with significant challenges in generalization and robustness. Currently, many studies focus on improving either generalization or robustness in isolation, without methods addressing both aspects simultaneously, which presents a significant challenge in developing LMs that are both robust and generalized. In this paper, we propose a bi-stage optimization framework to uniformly enhance both the generalization and robustness of LMs, termed UEGR. Specifically, during the forward propagation stage, we enrich the output probability distributions of adversarial samples by adaptive dropout to generate diverse sub models, and incorporate JS divergence and adversarial losses of these output distributions to reinforce output stability. During backward propagation stage, we compute parameter saliency scores and selectively update only the most critical parameters to minimize unnecessary deviations and consolidate the model's resilience. Theoretical analysis shows that our framework includes gradient regularization to limit the model's sensitivity to input perturbations and selective parameter updates to flatten the loss landscape, thus improving both generalization and robustness. The experimental results show that our method significantly improves the generalization and robustness of LMs compared to other existing methods across 13 publicly available language datasets, achieving state-of-the-art (SOTA) performance.</li>
</ul>

<h3>Title: A Foundational individual Mobility Prediction Model based on Open-Source Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhenlin Qin, Leizhen Wang, Francisco Camara Pereira, Zhenlinag Ma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16553">https://arxiv.org/abs/2503.16553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16553">https://arxiv.org/pdf/2503.16553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16553]] A Foundational individual Mobility Prediction Model based on Open-Source Large Language Models(https://arxiv.org/abs/2503.16553)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are widely applied to domain-specific tasks due to their massive general knowledge and remarkable inference capacities. Current studies on LLMs have shown immense potential in applying LLMs to model individual mobility prediction problems. However, most LLM-based mobility prediction models only train on specific datasets or use single well-designed prompts, leading to difficulty in adapting to different cities and users with diverse contexts. To fill these gaps, this paper proposes a unified fine-tuning framework to train a foundational open source LLM-based mobility prediction model. We conducted extensive experiments on six real-world mobility datasets to validate the proposed model. The results showed that the proposed model achieved the best performance in prediction accuracy and transferability over state-of-the-art models based on deep learning and LLMs.</li>
</ul>

<h3>Title: Explainable AI Components for Narrative Map Extraction</h3>
<ul>
<li><strong>Authors: </strong>Brian Keith, Fausto German, Eric Krokos, Sarah Joseph, Chris North</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16554">https://arxiv.org/abs/2503.16554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16554">https://arxiv.org/pdf/2503.16554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16554]] Explainable AI Components for Narrative Map Extraction(https://arxiv.org/abs/2503.16554)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>As narrative extraction systems grow in complexity, establishing user trust through interpretable and explainable outputs becomes increasingly critical. This paper presents an evaluation of an Explainable Artificial Intelligence (XAI) system for narrative map extraction that provides meaningful explanations across multiple levels of abstraction. Our system integrates explanations based on topical clusters for low-level document relationships, connection explanations for event relationships, and high-level structure explanations for overall narrative patterns. In particular, we evaluate the XAI system through a user study involving 10 participants that examined narratives from the 2021 Cuban protests. The analysis of results demonstrates that participants using the explanations made the users trust in the system's decisions, with connection explanations and important event detection proving particularly effective at building user confidence. Survey responses indicate that the multi-level explanation approach helped users develop appropriate trust in the system's narrative extraction capabilities. This work advances the state-of-the-art in explainable narrative extraction while providing practical insights for developing reliable narrative extraction systems that support effective human-AI collaboration.</li>
</ul>

<h3>Title: FutureGen: LLM-RAG Approach to Generate the Future Work of Scientific Article</h3>
<ul>
<li><strong>Authors: </strong>Ibrahim Al Azher, Miftahul Jannat Mokarrama, Zhishuai Guo, Sagnik Ray Choudhury, Hamed Alhoori</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16561">https://arxiv.org/abs/2503.16561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16561">https://arxiv.org/pdf/2503.16561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16561]] FutureGen: LLM-RAG Approach to Generate the Future Work of Scientific Article(https://arxiv.org/abs/2503.16561)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The future work section of a scientific article outlines potential research directions by identifying gaps and limitations of a current study. This section serves as a valuable resource for early-career researchers seeking unexplored areas and experienced researchers looking for new projects or collaborations. In this study, we generate future work suggestions from key sections of a scientific article alongside related papers and analyze how the trends have evolved. We experimented with various Large Language Models (LLMs) and integrated Retrieval-Augmented Generation (RAG) to enhance the generation process. We incorporate a LLM feedback mechanism to improve the quality of the generated content and propose an LLM-as-a-judge approach for evaluation. Our results demonstrated that the RAG-based approach with LLM feedback outperforms other methods evaluated through qualitative and quantitative metrics. Moreover, we conduct a human evaluation to assess the LLM as an extractor and judge. The code and dataset for this project are here, code: HuggingFace</li>
</ul>

<h3>Title: Bezier Distillation</h3>
<ul>
<li><strong>Authors: </strong>Ling Feng, SK Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16562">https://arxiv.org/abs/2503.16562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16562">https://arxiv.org/pdf/2503.16562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16562]] Bezier Distillation(https://arxiv.org/abs/2503.16562)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In Rectified Flow, by obtaining the rectified flow several times, the mapping relationship between distributions can be distilled into a neural network, and the target distribution can be directly predicted by the straight lines of the flow. However, during the pairing process of the mapping relationship, a large amount of error accumulation will occur, resulting in a decrease in performance after multiple rectifications. In the field of flow models, knowledge distillation of multi - teacher diffusion models is also a problem worthy of discussion in accelerating sampling. I intend to combine multi - teacher knowledge distillation with Bezier curves to solve the problem of error accumulation. Currently, the related paper is being written by myself.</li>
</ul>

<h3>Title: Chem42: a Family of chemical Language Models for Target-aware Ligand Generation</h3>
<ul>
<li><strong>Authors: </strong>Aahan Singh, Engin Tekin, Maryam Nadeem, Nancy A. ElNaker, Mohammad Amaan Sayeed, Natalia Vassilieva, Boulbaba Ben Amor</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16563">https://arxiv.org/abs/2503.16563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16563">https://arxiv.org/pdf/2503.16563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16563]] Chem42: a Family of chemical Language Models for Target-aware Ligand Generation(https://arxiv.org/abs/2503.16563)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Revolutionizing drug discovery demands more than just understanding molecular interactions - it requires generative models that can design novel ligands tailored to specific biological targets. While chemical Language Models (cLMs) have made strides in learning molecular properties, most fail to incorporate target-specific insights, restricting their ability to drive de-novo ligand generation. Chem42, a cutting-edge family of generative chemical Language Models, is designed to bridge this gap. By integrating atomic-level interactions with multimodal inputs from Prot42, a complementary protein Language Model, Chem42 achieves a sophisticated cross-modal representation of molecular structures, interactions, and binding patterns. This innovative framework enables the creation of structurally valid, synthetically accessible ligands with enhanced target specificity. Evaluations across diverse protein targets confirm that Chem42 surpasses existing approaches in chemical validity, target-aware design, and predicted binding affinity. By reducing the search space of viable drug candidates, Chem42 could accelerate the drug discovery pipeline, offering a powerful generative AI tool for precision medicine. Our Chem42 models set a new benchmark in molecule property prediction, conditional molecule generation, and target-aware ligand design. The models are publicly available at this http URL.</li>
</ul>

<h3>Title: REVAL: A Comprehension Evaluation on Reliability and Values of Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jie Zhang, Zheng Yuan, Zhongqi Wang, Bei Yan, Sibo Wang, Xiangkui Cao, Zonghui Guo, Shiguang Shan, Xilin Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16566">https://arxiv.org/abs/2503.16566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16566">https://arxiv.org/pdf/2503.16566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16566]] REVAL: A Comprehension Evaluation on Reliability and Values of Large Vision-Language Models(https://arxiv.org/abs/2503.16566)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, attack, robust</a></li>
<li><strong>Abstract: </strong>The rapid evolution of Large Vision-Language Models (LVLMs) has highlighted the necessity for comprehensive evaluation frameworks that assess these models across diverse dimensions. While existing benchmarks focus on specific aspects such as perceptual abilities, cognitive capabilities, and safety against adversarial attacks, they often lack the breadth and depth required to provide a holistic understanding of LVLMs' strengths and limitations. To address this gap, we introduce REVAL, a comprehensive benchmark designed to evaluate the \textbf{RE}liability and \textbf{VAL}ue of LVLMs. REVAL encompasses over 144K image-text Visual Question Answering (VQA) samples, structured into two primary sections: Reliability, which assesses truthfulness (\eg, perceptual accuracy and hallucination tendencies) and robustness (\eg, resilience to adversarial attacks, typographic attacks, and image corruption), and Values, which evaluates ethical concerns (\eg, bias and moral understanding), safety issues (\eg, toxicity and jailbreak vulnerabilities), and privacy problems (\eg, privacy awareness and privacy leakage). We evaluate 26 models, including mainstream open-source LVLMs and prominent closed-source models like GPT-4o and Gemini-1.5-Pro. Our findings reveal that while current LVLMs excel in perceptual tasks and toxicity avoidance, they exhibit significant vulnerabilities in adversarial scenarios, privacy preservation, and ethical reasoning. These insights underscore critical areas for future improvements, guiding the development of more secure, reliable, and ethically aligned LVLMs. REVAL provides a robust framework for researchers to systematically assess and compare LVLMs, fostering advancements in the field.</li>
</ul>

<h3>Title: Extract, Match, and Score: An Evaluation Paradigm for Long Question-context-answer Triplets in Financial Analysis</h3>
<ul>
<li><strong>Authors: </strong>Bo Hu, Han Yuan, Vlad Pandelea, Wuqiong Luo, Yingzhu Zhao, Zheng Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16575">https://arxiv.org/abs/2503.16575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16575">https://arxiv.org/pdf/2503.16575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16575]] Extract, Match, and Score: An Evaluation Paradigm for Long Question-context-answer Triplets in Financial Analysis(https://arxiv.org/abs/2503.16575)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has sparked widespread adoption across diverse applications, making robust evaluation frameworks crucial for assessing their performance. While conventional evaluation metrics remain applicable for shorter texts, their efficacy diminishes when evaluating the quality of long-form answers. This limitation is particularly critical in real-world scenarios involving extended questions, extensive context, and long-form answers, such as financial analysis or regulatory compliance. In this paper, we use a practical financial use case to illustrate applications that handle "long question-context-answer triplets". We construct a real-world financial dataset comprising long triplets and demonstrate the inadequacies of traditional metrics. To address this, we propose an effective Extract, Match, and Score (EMS) evaluation approach tailored to the complexities of long-form LLMs' outputs, providing practitioners with a reliable methodology for assessing LLMs' performance in complex real-world scenarios.</li>
</ul>

<h3>Title: World Knowledge from AI Image Generation for Robot Control</h3>
<ul>
<li><strong>Authors: </strong>Jonas Krumme, Christoph Zetzsche</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16579">https://arxiv.org/abs/2503.16579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16579">https://arxiv.org/pdf/2503.16579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16579]] World Knowledge from AI Image Generation for Robot Control(https://arxiv.org/abs/2503.16579)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>When interacting with the world robots face a number of difficult questions, having to make decisions when given under-specified tasks where they need to make choices, often without clearly defined right and wrong answers. Humans, on the other hand, can often rely on their knowledge and experience to fill in the gaps. For example, the simple task of organizing newly bought produce into the fridge involves deciding where to put each thing individually, how to arrange them together meaningfully, e.g. putting related things together, all while there is no clear right and wrong way to accomplish this task. We could encode all this information on how to do such things explicitly into the robots' knowledge base, but this can quickly become overwhelming, considering the number of potential tasks and circumstances the robot could encounter. However, images of the real world often implicitly encode answers to such questions and can show which configurations of objects are meaningful or are usually used by humans. An image of a full fridge can give a lot of information about how things are usually arranged in relation to each other and the full fridge at large. Modern generative systems are capable of generating plausible images of the real world and can be conditioned on the environment in which the robot operates. Here we investigate the idea of using the implicit knowledge about the world of modern generative AI systems given by their ability to generate convincing images of the real world to solve under-specified tasks.</li>
</ul>

<h3>Title: Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zahra Khalila, Arbi Haza Nasution, Winda Monika, Aytug Onan, Yohei Murakami, Yasir Bin Ismail Radi, Noor Mohammad Osmani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16581">https://arxiv.org/abs/2503.16581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16581">https://arxiv.org/pdf/2503.16581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16581]] Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models(https://arxiv.org/abs/2503.16581)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Accurate and contextually faithful responses are critical when applying large language models (LLMs) to sensitive and domain-specific tasks, such as answering queries related to quranic studies. General-purpose LLMs often struggle with hallucinations, where generated responses deviate from authoritative sources, raising concerns about their reliability in religious contexts. This challenge highlights the need for systems that can integrate domain-specific knowledge while maintaining response accuracy, relevance, and faithfulness. In this study, we investigate 13 open-source LLMs categorized into large (e.g., Llama3:70b, Gemma2:27b, QwQ:32b), medium (e.g., Gemma2:9b, Llama3:8b), and small (e.g., Llama3.2:3b, Phi3:3.8b). A Retrieval-Augmented Generation (RAG) is used to make up for the problems that come with using separate models. This research utilizes a descriptive dataset of Quranic surahs including the meanings, historical context, and qualities of the 114 surahs, allowing the model to gather relevant knowledge before responding. The models are evaluated using three key metrics set by human evaluators: context relevance, answer faithfulness, and answer relevance. The findings reveal that large models consistently outperform smaller models in capturing query semantics and producing accurate, contextually grounded responses. The Llama3.2:3b model, even though it is considered small, does very well on faithfulness (4.619) and relevance (4.857), showing the promise of smaller architectures that have been well optimized. This article examines the trade-offs between model size, computational efficiency, and response quality while using LLMs in domain-specific applications.</li>
</ul>

<h3>Title: Distributed LLMs and Multimodal Large Language Models: A Survey on Advances, Challenges, and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Hadi Amini, Md Jueal Mia, Yasaman Saadati, Ahmed Imteaj, Seyedsina Nabavirazavi, Urmish Thakker, Md Zarif Hossain, Awal Ahmed Fime, S.S. Iyengar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.DC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16585">https://arxiv.org/abs/2503.16585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16585">https://arxiv.org/pdf/2503.16585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16585]] Distributed LLMs and Multimodal Large Language Models: A Survey on Advances, Challenges, and Future Directions(https://arxiv.org/abs/2503.16585)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, large language model</a></li>
<li><strong>Abstract: </strong>Language models (LMs) are machine learning models designed to predict linguistic patterns by estimating the probability of word sequences based on large-scale datasets, such as text. LMs have a wide range of applications in natural language processing (NLP) tasks, including autocomplete and machine translation. Although larger datasets typically enhance LM performance, scalability remains a challenge due to constraints in computational power and resources. Distributed computing strategies offer essential solutions for improving scalability and managing the growing computational demand. Further, the use of sensitive datasets in training and deployment raises significant privacy concerns. Recent research has focused on developing decentralized techniques to enable distributed training and inference while utilizing diverse computational resources and enabling edge AI. This paper presents a survey on distributed solutions for various LMs, including large language models (LLMs), vision language models (VLMs), multimodal LLMs (MLLMs), and small language models (SLMs). While LLMs focus on processing and generating text, MLLMs are designed to handle multiple modalities of data (e.g., text, images, and audio) and to integrate them for broader applications. To this end, this paper reviews key advancements across the MLLM pipeline, including distributed training, inference, fine-tuning, and deployment, while also identifying the contributions, limitations, and future areas of improvement. Further, it categorizes the literature based on six primary focus areas of decentralization. Our analysis describes gaps in current methodologies for enabling distributed solutions for LMs and outline future research directions, emphasizing the need for novel solutions to enhance the robustness and applicability of distributed LMs.</li>
</ul>

<h3>Title: A Recipe for Generating 3D Worlds From a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Katja Schwarz, Denys Rozumnyi, Samuel Rota BulÃ², Lorenzo Porzi, Peter Kontschieder</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16611">https://arxiv.org/abs/2503.16611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16611">https://arxiv.org/pdf/2503.16611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16611]] A Recipe for Generating 3D Worlds From a Single Image(https://arxiv.org/abs/2503.16611)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce a recipe for generating immersive 3D worlds from a single image by framing the task as an in-context learning problem for 2D inpainting models. This approach requires minimal training and uses existing generative models. Our process involves two steps: generating coherent panoramas using a pre-trained diffusion model and lifting these into 3D with a metric depth estimator. We then fill unobserved regions by conditioning the inpainting model on rendered point clouds, requiring minimal fine-tuning. Tested on both synthetic and real images, our method produces high-quality 3D environments suitable for VR display. By explicitly modeling the 3D structure of the generated environment from the start, our approach consistently outperforms state-of-the-art, video synthesis-based methods along multiple quantitative image quality metrics. Project Page: this https URL</li>
</ul>

<h3>Title: SoK: Trusted Execution in SoC-FPGAs</h3>
<ul>
<li><strong>Authors: </strong>Garrett Perkins, Benjamin Macht, Lucas Ritzdorf, Tristan Running Crane, Brock LaMeres, Clemente Izurieta, Ann Marie Reinhold</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16612">https://arxiv.org/abs/2503.16612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16612">https://arxiv.org/pdf/2503.16612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16612]] SoK: Trusted Execution in SoC-FPGAs(https://arxiv.org/abs/2503.16612)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Trusted Execution Environments (TEEs) have emerged at the forefront of edge computing to combat the lack of trust between system components. Field Programmable Gate Arrays (FPGAs) are commonly used as edge computers but were not created with security as a primary consideration. Thus, FPGA-based edge computers are increasingly the target of cyberattacks. We analyze the existing literature to systematize the applications and features of FPGA-based TEEs. We identified 27 primary studies related to different types of System-on-Chip FPGA-based TEEs. Across a wide range of applications and features, the availability of extensible solutions is limited. Most solutions focus on specific features and applications, whereas few solutions focus on feature-rich, comprehensive TEEs that can be utilized across computer systems. Whether TEEs are specific or extensible, the paucity of published studies provides evidence of research gaps. This SoK delineates these gaps revealing opportunities for researchers and developers.</li>
</ul>

<h3>Title: Progressive Test Time Energy Adaptation for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xiaoran Zhang, Byung-Woo Hong, Hyoungseob Park, Daniel H. Pak, Anne-Marie Rickmann, Lawrence H. Staib, James S. Duncan, Alex Wong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16616">https://arxiv.org/abs/2503.16616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16616">https://arxiv.org/pdf/2503.16616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16616]] Progressive Test Time Energy Adaptation for Medical Image Segmentation(https://arxiv.org/abs/2503.16616)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We propose a model-agnostic, progressive test-time energy adaptation approach for medical image segmentation. Maintaining model performance across diverse medical datasets is challenging, as distribution shifts arise from inconsistent imaging protocols and patient variations. Unlike domain adaptation methods that require multiple passes through target data - impractical in clinical settings - our approach adapts pretrained models progressively as they process test data. Our method leverages a shape energy model trained on source data, which assigns an energy score at the patch level to segmentation maps: low energy represents in-distribution (accurate) shapes, while high energy signals out-of-distribution (erroneous) predictions. By minimizing this energy score at test time, we refine the segmentation model to align with the target distribution. To validate the effectiveness and adaptability, we evaluated our framework on eight public MRI (bSSFP, T1- and T2-weighted) and X-ray datasets spanning cardiac, spinal cord, and lung segmentation. We consistently outperform baselines both quantitatively and qualitatively.</li>
</ul>

<h3>Title: Leveraging Large Language Models for Explainable Activity Recognition in Smart Homes: A Critical Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Michele Fiori, Gabriele Civitarese, Priyankar Choudhary, Claudio Bettini</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16622">https://arxiv.org/abs/2503.16622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16622">https://arxiv.org/pdf/2503.16622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16622]] Leveraging Large Language Models for Explainable Activity Recognition in Smart Homes: A Critical Evaluation(https://arxiv.org/abs/2503.16622)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Explainable Artificial Intelligence (XAI) aims to uncover the inner reasoning of machine learning models. In IoT systems, XAI improves the transparency of models processing sensor data from multiple heterogeneous devices, ensuring end-users understand and trust their outputs. Among the many applications, XAI has also been applied to sensor-based Activities of Daily Living (ADLs) recognition in smart homes. Existing approaches highlight which sensor events are most important for each predicted activity, using simple rules to convert these events into natural language explanations for non-expert users. However, these methods produce rigid explanations lacking natural language flexibility and are not scalable. With the recent rise of Large Language Models (LLMs), it is worth exploring whether they can enhance explanation generation, considering their proven knowledge of human activities. This paper investigates potential approaches to combine XAI and LLMs for sensor-based ADL recognition. We evaluate if LLMs can be used: a) as explainable zero-shot ADL recognition models, avoiding costly labeled data collection, and b) to automate the generation of explanations for existing data-driven XAI approaches when training data is available and the goal is higher recognition rates. Our critical evaluation provides insights into the benefits and challenges of using LLMs for explainable ADL recognition.</li>
</ul>

<h3>Title: MobilePlantViT: A Mobile-friendly Hybrid ViT for Generalized Plant Disease Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Moshiur Rahman Tonmoy, Md. Mithun Hossain, Nilanjan Dey, M. F. Mridha</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16628">https://arxiv.org/abs/2503.16628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16628">https://arxiv.org/pdf/2503.16628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16628]] MobilePlantViT: A Mobile-friendly Hybrid ViT for Generalized Plant Disease Image Classification(https://arxiv.org/abs/2503.16628)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, transformer</a></li>
<li><strong>Abstract: </strong>Plant diseases significantly threaten global food security by reducing crop yields and undermining agricultural sustainability. AI-driven automated classification has emerged as a promising solution, with deep learning models demonstrating impressive performance in plant disease identification. However, deploying these models on mobile and edge devices remains challenging due to high computational demands and resource constraints, highlighting the need for lightweight, accurate solutions for accessible smart agriculture systems. To address this, we propose MobilePlantViT, a novel hybrid Vision Transformer (ViT) architecture designed for generalized plant disease classification, which optimizes resource efficiency while maintaining high performance. Extensive experiments across diverse plant disease datasets of varying scales show our model's effectiveness and strong generalizability, achieving test accuracies ranging from 80% to over 99%. Notably, with only 0.69 million parameters, our architecture outperforms the smallest versions of MobileViTv1 and MobileViTv2, despite their higher parameter counts. These results underscore the potential of our approach for real-world, AI-powered automated plant disease classification in sustainable and resource-efficient smart agriculture systems. All codes will be available in the GitHub repository: this https URL</li>
</ul>

<h3>Title: Visualizing Privacy-Relevant Data Flows in Android Applications</h3>
<ul>
<li><strong>Authors: </strong>Mugdha Khedkar, Michael Schlichtig, Santhosh Mohan, Eric Bodden</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16640">https://arxiv.org/abs/2503.16640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16640">https://arxiv.org/pdf/2503.16640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16640]] Visualizing Privacy-Relevant Data Flows in Android Applications(https://arxiv.org/abs/2503.16640)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Android applications collecting data from users must protect it according to the current legal frameworks. Such data protection has become even more important since in 2018 the European Union rolled out the General Data Protection Regulation (GDPR). Since app developers are not legal experts, they find it difficult to integrate privacy-aware practices into source code development. Despite these legal obligations, developers have limited tool support to reason about data protection throughout their app development process. This paper explores the use of static program slicing and software visualization to analyze privacy-relevant data flows in Android apps. We introduce SliceViz, a web tool that analyzes an Android app by slicing all privacy-relevant data sources detected in the source code on the back-end. It then helps developers by visualizing these privacy-relevant program slices. We conducted a user study with 12 participants demonstrating that SliceViz effectively aids developers in identifying privacy-relevant properties in Android apps. Our findings indicate that program slicing can be employed to identify and reason about privacy-relevant data flows in Android applications. With further usability improvements, developers can be better equipped to handle privacy-sensitive information.</li>
</ul>

<h3>Title: iFlame: Interleaving Full and Linear Attention for Efficient Mesh Generation</h3>
<ul>
<li><strong>Authors: </strong>Hanxiao Wang, Biao Zhang, Weize Quan, Dong-Ming Yan, Peter Wonka</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16653">https://arxiv.org/abs/2503.16653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16653">https://arxiv.org/pdf/2503.16653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16653]] iFlame: Interleaving Full and Linear Attention for Efficient Mesh Generation(https://arxiv.org/abs/2503.16653)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>This paper propose iFlame, a novel transformer-based network architecture for mesh generation. While attention-based models have demonstrated remarkable performance in mesh generation, their quadratic computational complexity limits scalability, particularly for high-resolution 3D data. Conversely, linear attention mechanisms offer lower computational costs but often struggle to capture long-range dependencies, resulting in suboptimal outcomes. To address this trade-off, we propose an interleaving autoregressive mesh generation framework that combines the efficiency of linear attention with the expressive power of full attention mechanisms. To further enhance efficiency and leverage the inherent structure of mesh representations, we integrate this interleaving approach into an hourglass architecture, which significantly boosts efficiency. Our approach reduces training time while achieving performance comparable to pure attention-based models. To improve inference efficiency, we implemented a caching algorithm that almost doubles the speed and reduces the KV cache size by seven-eighths compared to the original Transformer. We evaluate our framework on ShapeNet and Objaverse, demonstrating its ability to generate high-quality 3D meshes efficiently. Our results indicate that the proposed interleaving framework effectively balances computational efficiency and generative performance, making it a practical solution for mesh generation. The training takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces on Objaverse.</li>
</ul>

<h3>Title: Accelerating Antibiotic Discovery with Large Language Models and Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Maxime Delmas, Magdalena Wysocka, Danilo Gusicuma, AndrÃ© Freitas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16655">https://arxiv.org/abs/2503.16655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16655">https://arxiv.org/pdf/2503.16655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16655]] Accelerating Antibiotic Discovery with Large Language Models and Knowledge Graphs(https://arxiv.org/abs/2503.16655)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The discovery of novel antibiotics is critical to address the growing antimicrobial resistance (AMR). However, pharmaceutical industries face high costs (over $1 billion), long timelines, and a high failure rate, worsened by the rediscovery of known compounds. We propose an LLM-based pipeline that acts as an alarm system, detecting prior evidence of antibiotic activity to prevent costly rediscoveries. The system integrates organism and chemical literature into a Knowledge Graph (KG), ensuring taxonomic resolution, synonym handling, and multi-level evidence classification. We tested the pipeline on a private list of 73 potential antibiotic-producing organisms, disclosing 12 negative hits for evaluation. The results highlight the effectiveness of the pipeline for evidence reviewing, reducing false negatives, and accelerating decision-making. The KG for negative hits and the user interface for interactive exploration will be made publicly available.</li>
</ul>

<h3>Title: Advances in Protein Representation Learning: Methods, Applications, and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Viet Thanh Duy Nguyen, Truong-Son Hy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16659">https://arxiv.org/abs/2503.16659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16659">https://arxiv.org/pdf/2503.16659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16659]] Advances in Protein Representation Learning: Methods, Applications, and Future Directions(https://arxiv.org/abs/2503.16659)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Proteins are complex biomolecules that play a central role in various biological processes, making them critical targets for breakthroughs in molecular biology, medical research, and drug discovery. Deciphering their intricate, hierarchical structures, and diverse functions is essential for advancing our understanding of life at the molecular level. Protein Representation Learning (PRL) has emerged as a transformative approach, enabling the extraction of meaningful computational representations from protein data to address these challenges. In this paper, we provide a comprehensive review of PRL research, categorizing methodologies into five key areas: feature-based, sequence-based, structure-based, multimodal, and complex-based approaches. To support researchers in this rapidly evolving field, we introduce widely used databases for protein sequences, structures, and functions, which serve as essential resources for model development and evaluation. We also explore the diverse applications of these approaches in multiple domains, demonstrating their broad impact. Finally, we discuss pressing technical challenges and outline future directions to advance PRL, offering insights to inspire continued innovation in this foundational field.</li>
</ul>

<h3>Title: TextBite: A Historical Czech Document Dataset for Logical Page Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Martin KostelnÃ­k, Karel BeneÅ¡, Michal HradiÅ¡</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16664">https://arxiv.org/abs/2503.16664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16664">https://arxiv.org/pdf/2503.16664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16664]] TextBite: A Historical Czech Document Dataset for Logical Page Segmentation(https://arxiv.org/abs/2503.16664)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Logical page segmentation is an important step in document analysis, enabling better semantic representations, information retrieval, and text understanding. Previous approaches define logical segmentation either through text or geometric objects, relying on OCR or precise geometry. To avoid the need for OCR, we define the task purely as segmentation in the image domain. Furthermore, to ensure the evaluation remains unaffected by geometrical variations that do not impact text segmentation, we propose to use only foreground text pixels in the evaluation metric and disregard all background pixels. To support research in logical document segmentation, we introduce TextBite, a dataset of historical Czech documents spanning the 18th to 20th centuries, featuring diverse layouts from newspapers, dictionaries, and handwritten records. The dataset comprises 8,449 page images with 78,863 annotated segments of logically and thematically coherent text. We propose a set of baseline methods combining text region detection and relation prediction. The dataset, baselines and evaluation framework can be accessed at this https URL.</li>
</ul>

<h3>Title: Accelerating Transformer Inference and Training with 2:4 Activation Sparsity</h3>
<ul>
<li><strong>Authors: </strong>Daniel Haziza, Timothy Chou, Dhruv Choudhary, Luca Wehrstedt, Francisco Massa, Jiecao Yu, Geonhwa Jeong, Supriya Rao, Patrick Labatut, Jesse Cai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16672">https://arxiv.org/abs/2503.16672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16672">https://arxiv.org/pdf/2503.16672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16672]] Accelerating Transformer Inference and Training with 2:4 Activation Sparsity(https://arxiv.org/abs/2503.16672)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we demonstrate how to leverage 2:4 sparsity, a popular hardware-accelerated GPU sparsity pattern, to activations to accelerate large language model training and inference. Crucially we exploit the intrinsic sparsity found in Squared-ReLU activations to provide this acceleration with no accuracy loss. Our approach achieves up to 1.3x faster Feed Forward Network (FFNs) in both the forwards and backwards pass. This work highlights the potential for sparsity to play a key role in accelerating large language model training and inference.</li>
</ul>

<h3>Title: ATOM: A Framework of Detecting Query-Based Model Extraction Attacks for Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Zhan Cheng, Bolin Shen, Tianming Sha, Yuan Gao, Shibo Li, Yushun Dong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16693">https://arxiv.org/abs/2503.16693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16693">https://arxiv.org/pdf/2503.16693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16693]] ATOM: A Framework of Detecting Query-Based Model Extraction Attacks for Graph Neural Networks(https://arxiv.org/abs/2503.16693)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, extraction, watermark</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have gained traction in Graph-based Machine Learning as a Service (GMLaaS) platforms, yet they remain vulnerable to graph-based model extraction attacks (MEAs), where adversaries reconstruct surrogate models by querying the victim model. Existing defense mechanisms, such as watermarking and fingerprinting, suffer from poor real-time performance, susceptibility to evasion, or reliance on post-attack verification, making them inadequate for handling the dynamic characteristics of graph-based MEA variants. To address these limitations, we propose ATOM, a novel real-time MEA detection framework tailored for GNNs. ATOM integrates sequential modeling and reinforcement learning to dynamically detect evolving attack patterns, while leveraging $k$-core embedding to capture the structural properties, enhancing detection precision. Furthermore, we provide theoretical analysis to characterize query behaviors and optimize detection strategies. Extensive experiments on multiple real-world datasets demonstrate that ATOM outperforms existing approaches in detection performance, maintaining stable across different time steps, thereby offering a more effective defense mechanism for GMLaaS environments.</li>
</ul>

<h3>Title: Cross-Modal and Uncertainty-Aware Agglomeration for Open-Vocabulary 3D Scene Understanding</h3>
<ul>
<li><strong>Authors: </strong>Jinlong Li, Cristiano Saltori, Fabio Poiesi, Nicu Sebe</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16707">https://arxiv.org/abs/2503.16707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16707">https://arxiv.org/pdf/2503.16707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16707]] Cross-Modal and Uncertainty-Aware Agglomeration for Open-Vocabulary 3D Scene Understanding(https://arxiv.org/abs/2503.16707)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>The lack of a large-scale 3D-text corpus has led recent works to distill open-vocabulary knowledge from vision-language models (VLMs). owever, these methods typically rely on a single VLM to align the feature spaces of 3D models within a common language space, which limits the potential of 3D models to leverage the diverse spatial and semantic capabilities encapsulated in various foundation models. In this paper, we propose Cross-modal and Uncertainty-aware Agglomeration for Open-vocabulary 3D Scene Understanding dubbed CUA-O3D, the first model to integrate multiple foundation models-such as CLIP, DINOv2, and Stable Diffusion-into 3D scene understanding. We further introduce a deterministic uncertainty estimation to adaptively distill and harmonize the heterogeneous 2D feature embeddings from these models. Our method addresses two key challenges: (1) incorporating semantic priors from VLMs alongside the geometric knowledge of spatially-aware vision foundation models, and (2) using a novel deterministic uncertainty estimation to capture model-specific uncertainties across diverse semantic and geometric sensitivities, helping to reconcile heterogeneous representations during training. Extensive experiments on ScanNetV2 and Matterport3D demonstrate that our method not only advances open-vocabulary segmentation but also achieves robust cross-domain alignment and competitive spatial perception capabilities. The code will be available at \href{this https URL}{CUA_O3D}.</li>
</ul>

<h3>Title: NeuroSep-CP-LCB: A Deep Learning-based Contextual Multi-armed Bandit Algorithm with Uncertainty Quantification for Early Sepsis Prediction</h3>
<ul>
<li><strong>Authors: </strong>Anni Zhou, Raheem Beyah, Rishikesan Kamaleswaran</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16708">https://arxiv.org/abs/2503.16708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16708">https://arxiv.org/pdf/2503.16708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16708]] NeuroSep-CP-LCB: A Deep Learning-based Contextual Multi-armed Bandit Algorithm with Uncertainty Quantification for Early Sepsis Prediction(https://arxiv.org/abs/2503.16708)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In critical care settings, timely and accurate predictions can significantly impact patient outcomes, especially for conditions like sepsis, where early intervention is crucial. We aim to model patient-specific reward functions in a contextual multi-armed bandit setting. The goal is to leverage patient-specific clinical features to optimize decision-making under uncertainty. This paper proposes NeuroSep-CP-LCB, a novel integration of neural networks with contextual bandits and conformal prediction tailored for early sepsis detection. Unlike the algorithm pool selection problem in the previous paper, where the primary focus was identifying the most suitable pre-trained model for prediction tasks, this work directly models the reward function using a neural network, allowing for personalized and adaptive decision-making. Combining the representational power of neural networks with the robustness of conformal prediction intervals, this framework explicitly accounts for uncertainty in offline data distributions and provides actionable confidence bounds on predictions.</li>
</ul>

<h3>Title: 4D Gaussian Splatting SLAM</h3>
<ul>
<li><strong>Authors: </strong>Yanyan Li, Youxu Fang, Zunjie Zhu, Kunyi Li, Yong Ding, Federico Tombari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16710">https://arxiv.org/abs/2503.16710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16710">https://arxiv.org/pdf/2503.16710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16710]] 4D Gaussian Splatting SLAM(https://arxiv.org/abs/2503.16710)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Simultaneously localizing camera poses and constructing Gaussian radiance fields in dynamic scenes establish a crucial bridge between 2D images and the 4D real world. Instead of removing dynamic objects as distractors and reconstructing only static environments, this paper proposes an efficient architecture that incrementally tracks camera poses and establishes the 4D Gaussian radiance fields in unknown scenarios by using a sequence of RGB-D images. First, by generating motion masks, we obtain static and dynamic priors for each pixel. To eliminate the influence of static scenes and improve the efficiency on learning the motion of dynamic objects, we classify the Gaussian primitives into static and dynamic Gaussian sets, while the sparse control points along with an MLP is utilized to model the transformation fields of the dynamic Gaussians. To more accurately learn the motion of dynamic Gaussians, a novel 2D optical flow map reconstruction algorithm is designed to render optical flows of dynamic objects between neighbor images, which are further used to supervise the 4D Gaussian radiance fields along with traditional photometric and geometric constraints. In experiments, qualitative and quantitative evaluation results show that the proposed method achieves robust tracking and high-quality view synthesis performance in real-world environments.</li>
</ul>

<h3>Title: Practical Acoustic Eavesdropping On Typed Passphrases</h3>
<ul>
<li><strong>Authors: </strong>Darren FÃ¼rst, Andreas AÃmuth</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16719">https://arxiv.org/abs/2503.16719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16719">https://arxiv.org/pdf/2503.16719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16719]] Practical Acoustic Eavesdropping On Typed Passphrases(https://arxiv.org/abs/2503.16719)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack</a></li>
<li><strong>Abstract: </strong>Cloud services have become an essential infrastructure for enterprises and individuals. Access to these cloud services is typically governed by Identity and Access Management systems, where user authentication often relies on passwords. While best practices dictate the implementation of multi-factor authentication, it's a reality that many such users remain solely protected by passwords. This reliance on passwords creates a significant vulnerability, as these credentials can be compromised through various means, including side-channel attacks. This paper exploits keyboard acoustic emanations to infer typed natural language passphrases via unsupervised learning, necessitating no previous training data. Whilst this work focuses on short passphrases, it is also applicable to longer messages, such as confidential emails, where the margin for error is much greater, than with passphrases, making the attack even more effective in such a setting. Unlike traditional attacks that require physical access to the target device, acoustic side-channel attacks can be executed within the vicinity, without the user's knowledge, offering a worthwhile avenue for malicious actors. Our findings replicate and extend previous work, confirming that cross-correlation audio preprocessing outperforms methods like mel-frequency-cepstral coefficients and fast-fourier transforms in keystroke clustering. Moreover, we show that partial passphrase recovery through clustering and a dictionary attack can enable faster than brute-force attacks, further emphasizing the risks posed by this attack vector.</li>
</ul>

<h3>Title: EDiT: Efficient Diffusion Transformers with Linear Compressed Attention</h3>
<ul>
<li><strong>Authors: </strong>Philipp Becker, Abhinav Mehrotra, Ruchika Chavhan, Malcolm Chadwick, Luca Morreale, Mehdi Noroozi, Alberto Gil Ramos, Sourav Bhattacharya</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16726">https://arxiv.org/abs/2503.16726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16726">https://arxiv.org/pdf/2503.16726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16726]] EDiT: Efficient Diffusion Transformers with Linear Compressed Attention(https://arxiv.org/abs/2503.16726)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiTs) have emerged as a leading architecture for text-to-image synthesis, producing high-quality and photorealistic images. However, the quadratic scaling properties of the attention in DiTs hinder image generation with higher resolution or on devices with limited resources. This work introduces an efficient diffusion transformer (EDiT) to alleviate these efficiency bottlenecks in conventional DiTs and Multimodal DiTs (MM-DiTs). First, we present a novel linear compressed attention method that uses a multi-layer convolutional network to modulate queries with local information while keys and values are spatially aggregated. Second, we formulate a hybrid attention scheme for multi-modal inputs that combines linear attention for image-to-image interactions and standard scaled dot-product attention for interactions involving prompts. Merging these two approaches leads to an expressive, linear-time Multimodal Efficient Diffusion Transformer (MM-EDiT). We demonstrate the effectiveness of the EDiT and MM-EDiT architectures by integrating them into PixArt-Sigma(conventional DiT) and Stable Diffusion 3.5-Medium (MM-DiT), achieving up to 2.2x speedup with comparable image quality after distillation.</li>
</ul>

<h3>Title: Natural Language Generation</h3>
<ul>
<li><strong>Authors: </strong>Emiel van Miltenburg, Chenghua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16728">https://arxiv.org/abs/2503.16728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16728">https://arxiv.org/pdf/2503.16728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16728]] Natural Language Generation(https://arxiv.org/abs/2503.16728)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This article provides a brief overview of the field of Natural Language Generation. The term Natural Language Generation (NLG), in its broadest definition, refers to the study of systems that verbalize some form of information through natural language. That information could be stored in a large database or knowledge graph (in data-to-text applications), but NLG researchers may also study summarisation (text-to-text) or image captioning (image-to-text), for example. As a subfield of Natural Language Processing, NLG is closely related to other sub-disciplines such as Machine Translation (MT) and Dialog Systems. Some NLG researchers exclude MT from their definition of the field, since there is no content selection involved where the system has to determine what to say. Conversely, dialog systems do not typically fall under the header of Natural Language Generation since NLG is just one component of dialog systems (the others being Natural Language Understanding and Dialog Management). However, with the rise of Large Language Models (LLMs), different subfields of Natural Language Processing have converged on similar methodologies for the production of natural language and the evaluation of automatically generated text.</li>
</ul>

<h3>Title: Rethinking the Role of Spatial Mixing</h3>
<ul>
<li><strong>Authors: </strong>George Cazenavette, Joel Julin, Simon Lucey</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16760">https://arxiv.org/abs/2503.16760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16760">https://arxiv.org/pdf/2503.16760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16760]] Rethinking the Role of Spatial Mixing(https://arxiv.org/abs/2503.16760)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Until quite recently, the backbone of nearly every state-of-the-art computer vision model has been the 2D convolution. At its core, a 2D convolution simultaneously mixes information across both the spatial and channel dimensions of a representation. Many recent computer vision architectures consist of sequences of isotropic blocks that disentangle the spatial and channel-mixing components. This separation of the operations allows us to more closely juxtapose the effects of spatial and channel mixing in deep learning. In this paper, we take an initial step towards garnering a deeper understanding of the roles of these mixing operations. Through our experiments and analysis, we discover that on both classical (ResNet) and cutting-edge (ConvMixer) models, we can reach nearly the same level of classification performance by and leaving the spatial mixers at their random initializations. Furthermore, we show that models with random, fixed spatial mixing are naturally more robust to adversarial perturbations. Lastly, we show that this phenomenon extends past the classification regime, as such models can also decode pixel-shuffled images.</li>
</ul>

<h3>Title: Dynamic Attention Mechanism in Spatiotemporal Memory Networks for Object Tracking</h3>
<ul>
<li><strong>Authors: </strong>Meng Zhou, Jiadong Xie, Mingsheng Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16768">https://arxiv.org/abs/2503.16768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16768">https://arxiv.org/pdf/2503.16768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16768]] Dynamic Attention Mechanism in Spatiotemporal Memory Networks for Object Tracking(https://arxiv.org/abs/2503.16768)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Mainstream visual object tracking frameworks predominantly rely on template matching paradigms. Their performance heavily depends on the quality of template features, which becomes increasingly challenging to maintain in complex scenarios involving target deformation, occlusion, and background clutter. While existing spatiotemporal memory-based trackers emphasize memory capacity expansion, they lack effective mechanisms for dynamic feature selection and adaptive fusion. To address this gap, we propose a Dynamic Attention Mechanism in Spatiotemporal Memory Network (DASTM) with two key innovations: 1) A differentiable dynamic attention mechanism that adaptively adjusts channel-spatial attention weights by analyzing spatiotemporal correlations between the templates and memory features; 2) A lightweight gating network that autonomously allocates computational resources based on target motion states, prioritizing high-discriminability features in challenging scenarios. Extensive evaluations on OTB-2015, VOT 2018, LaSOT, and GOT-10K benchmarks demonstrate our DASTM's superiority, achieving state-of-the-art performance in success rate, robustness, and real-time efficiency, thereby offering a novel solution for real-time tracking in complex environments.</li>
</ul>

<h3>Title: OpenCity3D: What do Vision-Language Models know about Urban Environments?</h3>
<ul>
<li><strong>Authors: </strong>Valentin Bieri, Marco Zamboni, Nicolas S. Blumer, Qingxuan Chen, Francis Engelmann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16776">https://arxiv.org/abs/2503.16776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16776">https://arxiv.org/pdf/2503.16776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16776]] OpenCity3D: What do Vision-Language Models know about Urban Environments?(https://arxiv.org/abs/2503.16776)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Vision-language models (VLMs) show great promise for 3D scene understanding but are mainly applied to indoor spaces or autonomous driving, focusing on low-level tasks like segmentation. This work expands their use to urban-scale environments by leveraging 3D reconstructions from multi-view aerial imagery. We propose OpenCity3D, an approach that addresses high-level tasks, such as population density estimation, building age classification, property price prediction, crime rate assessment, and noise pollution evaluation. Our findings highlight OpenCity3D's impressive zero-shot and few-shot capabilities, showcasing adaptability to new contexts. This research establishes a new paradigm for language-driven urban analytics, enabling applications in planning, policy, and environmental monitoring. See our project page: this http URL</li>
</ul>

<h3>Title: Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of Frozen Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mengsong Wu, Tong Zhu, Han Han, Xiang Zhang, Wenbiao Shao, Wenliang Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16779">https://arxiv.org/abs/2503.16779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16779">https://arxiv.org/pdf/2503.16779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16779]] Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of Frozen Language Models(https://arxiv.org/abs/2503.16779)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Tool learning can further broaden the usage scenarios of large language models (LLMs). However most of the existing methods either need to finetune that the model can only use tools seen in the training data, or add tool demonstrations into the prompt with lower efficiency. In this paper, we present a new Tool Learning method Chain-of-Tools. It makes full use of the powerful semantic representation capability of frozen LLMs to finish tool calling in CoT reasoning with a huge and flexible tool pool which may contain unseen tools. Especially, to validate the effectiveness of our approach in the massive unseen tool scenario, we construct a new dataset SimpleToolQuestions. We conduct experiments on two numerical reasoning benchmarks (GSM8K-XL and FuncQA) and two knowledge-based question answering benchmarks (KAMEL and SimpleToolQuestions). Experimental results show that our approach performs better than the baseline. We also identify dimensions of the model output that are critical in tool selection, enhancing the model interpretability. Our code and data are available at: this https URL .</li>
</ul>

<h3>Title: Learning Part Knowledge to Facilitate Category Understanding for Fine-Grained Generalized Category Discovery</h3>
<ul>
<li><strong>Authors: </strong>Enguang Wang, Zhimao Peng, Zhengyuan Xie, Haori Lu, Fei Yang, Xialei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16782">https://arxiv.org/abs/2503.16782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16782">https://arxiv.org/pdf/2503.16782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16782]] Learning Part Knowledge to Facilitate Category Understanding for Fine-Grained Generalized Category Discovery(https://arxiv.org/abs/2503.16782)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Generalized Category Discovery (GCD) aims to classify unlabeled data containing both seen and novel categories. Although existing methods perform well on generic datasets, they struggle in fine-grained scenarios. We attribute this difficulty to their reliance on contrastive learning over global image features to automatically capture discriminative cues, which fails to capture the subtle local differences essential for distinguishing fine-grained categories. Therefore, in this paper, we propose incorporating part knowledge to address fine-grained GCD, which introduces two key challenges: the absence of annotations for novel classes complicates the extraction of the part features, and global contrastive learning prioritizes holistic feature invariance, inadvertently suppressing discriminative local part patterns. To address these challenges, we propose PartGCD, including 1) Adaptive Part Decomposition, which automatically extracts class-specific semantic parts via Gaussian Mixture Models, and 2) Part Discrepancy Regularization, enforcing explicit separation between part features to amplify fine-grained local part distinctions. Experiments demonstrate state-of-the-art performance across multiple fine-grained benchmarks while maintaining competitiveness on generic datasets, validating the effectiveness and robustness of our approach.</li>
</ul>

<h3>Title: CoBRA: A Universal Strategyproof Confirmation Protocol for Quorum-based Proof-of-Stake Blockchains</h3>
<ul>
<li><strong>Authors: </strong>Zeta Avarikioti, Eleftherios Kokoris Kogias, Ray Neiheiser, Christos Stefo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16783">https://arxiv.org/abs/2503.16783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16783">https://arxiv.org/pdf/2503.16783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16783]] CoBRA: A Universal Strategyproof Confirmation Protocol for Quorum-based Proof-of-Stake Blockchains(https://arxiv.org/abs/2503.16783)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>We present a formal analysis of quorum-based State Machine Replication (SMR) protocols in Proof-of-Stake (PoS) systems under a hybrid threat model comprising honest, Byzantine, and rational validators. Our analysis of traditional quorum-based protocols establishes two fundamental impossibility results: (1) in partially synchronous networks, no quorum-based protocol can achieve SMR when rational and Byzantine validators comprise more than $1/3$ of participants, and (2) in synchronous networks, SMR remains impossible when rational and Byzantine validators comprise $2/3$ or more of participants. To overcome these limitations, we propose two complementary solutions in our hybrid model. First, we introduce a protocol that enforces a bound on the volume of the total transacted amount that is finalized within any time window $\Delta$ and prove that this bound is necessary for secure SMR protocols in our model. Second, we present the \emph{strongest chain rule}, which enables efficient finalization of transactions when the majority of honest participants provably support the SMR execution. Through empirical analysis of Ethereum and Cosmos networks, we demonstrate that validator participation consistently exceeds the required ${5}/{6}$ threshold, establishing the practical feasibility of our solution in production PoS systems.</li>
</ul>

<h3>Title: DCEdit: Dual-Level Controlled Image Editing via Precisely Localized Semantics</h3>
<ul>
<li><strong>Authors: </strong>Yihan Hu, Jianing Peng, Yiheng Lin, Ting Liu, Xiaochao Qu, Luoqi Liu, Yao Zhao, Yunchao Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16795">https://arxiv.org/abs/2503.16795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16795">https://arxiv.org/pdf/2503.16795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16795]] DCEdit: Dual-Level Controlled Image Editing via Precisely Localized Semantics(https://arxiv.org/abs/2503.16795)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper presents a novel approach to improving text-guided image editing using diffusion-based models. Text-guided image editing task poses key challenge of precisly locate and edit the target semantic, and previous methods fall shorts in this aspect. Our method introduces a Precise Semantic Localization strategy that leverages visual and textual self-attention to enhance the cross-attention map, which can serve as a regional cues to improve editing performance. Then we propose a Dual-Level Control mechanism for incorporating regional cues at both feature and latent levels, offering fine-grained control for more precise edits. To fully compare our methods with other DiT-based approaches, we construct the RW-800 benchmark, featuring high resolution images, long descriptive texts, real-world images, and a new text editing task. Experimental results on the popular PIE-Bench and RW-800 benchmarks demonstrate the superior performance of our approach in preserving background and providing accurate edits.</li>
</ul>

<h3>Title: Seg2Box: 3D Object Detection by Point-Wise Semantics Supervision</h3>
<ul>
<li><strong>Authors: </strong>Maoji Zheng, Ziyu Xu, Qiming Xia, Hai Wu, Chenglu Wen, Cheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16811">https://arxiv.org/abs/2503.16811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16811">https://arxiv.org/pdf/2503.16811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16811]] Seg2Box: 3D Object Detection by Point-Wise Semantics Supervision(https://arxiv.org/abs/2503.16811)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>LiDAR-based 3D object detection and semantic segmentation are critical tasks in 3D scene understanding. Traditional detection and segmentation methods supervise their models through bounding box labels and semantic mask labels. However, these two independent labels inherently contain significant redundancy. This paper aims to eliminate the redundancy by supervising 3D object detection using only semantic labels. However, the challenge arises due to the incomplete geometry structure and boundary ambiguity of point-cloud instances, leading to inaccurate pseudo labels and poor detection results. To address these challenges, we propose a novel method, named Seg2Box. We first introduce a Multi-Frame Multi-Scale Clustering (MFMS-C) module, which leverages the spatio-temporal consistency of point clouds to generate accurate box-level pseudo-labels. Additionally, the Semantic?Guiding Iterative-Mining Self-Training (SGIM-ST) module is proposed to enhance the performance by progressively refining the pseudo-labels and mining the instances without generating pseudo-labels. Experiments on the Waymo Open Dataset and nuScenes Dataset show that our method significantly outperforms other competitive methods by 23.7\% and 10.3\% in mAP, respectively. The results demonstrate the great label-efficient potential and advancement of our method.</li>
</ul>

<h3>Title: When Debate Fails: Bias Reinforcement in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jihwan Oh, Minchan Jeong, Jongwoo Ko, Se-Young Yun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16814">https://arxiv.org/abs/2503.16814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16814">https://arxiv.org/pdf/2503.16814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16814]] When Debate Fails: Bias Reinforcement in Large Language Models(https://arxiv.org/abs/2503.16814)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models $($LLMs$)$ solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self-consistency and self-refinement aim to improve reliability, they often reinforce biases due to the lack of effective feedback mechanisms. Multi-Agent Debate $($MAD$)$ has emerged as an alternative, but we identify two key limitations: bias reinforcement, where debate amplifies model biases instead of correcting them, and lack of perspective diversity, as all agents share the same model and reasoning patterns, limiting true debate effectiveness. To systematically evaluate these issues, we introduce $\textit{MetaNIM Arena}$, a benchmark designed to assess LLMs in adversarial strategic decision-making, where dynamic interactions influence optimal decisions. To overcome MAD's limitations, we propose $\textbf{DReaMAD}$ $($$\textbf{D}$iverse $\textbf{Rea}$soning via $\textbf{M}$ulti-$\textbf{A}$gent $\textbf{D}$ebate with Refined Prompt$)$, a novel framework that $(1)$ refines LLM's strategic prior knowledge to improve reasoning quality and $(2)$ promotes diverse viewpoints within a single model by systematically modifying prompts, reducing bias. Empirical results show that $\textbf{DReaMAD}$ significantly improves decision accuracy, reasoning diversity, and bias mitigation across multiple strategic tasks, establishing it as a more effective approach for LLM-based decision-making.</li>
</ul>

<h3>Title: ST-Prompt Guided Histological Hypergraph Learning for Spatial Gene Expression Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yi Niu, Jiashuai Liu, Yingkang Zhan, Jiangbo Shi, Di Zhang, Ines Machado, Mireia Crispin-Ortuzar, Chen Li, Zeyu Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16816">https://arxiv.org/abs/2503.16816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16816">https://arxiv.org/pdf/2503.16816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16816]] ST-Prompt Guided Histological Hypergraph Learning for Spatial Gene Expression Prediction(https://arxiv.org/abs/2503.16816)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Spatial Transcriptomics (ST) reveals the spatial distribution of gene expression in tissues, offering critical insights into biological processes and disease mechanisms. However, predicting ST from H\&E-stained histology images is challenging due to the heterogeneous relationship between histomorphology and gene expression, which arises from substantial variability across different patients and tissue sections. A more practical and valuable approach is to utilize ST data from a few local regions to predict the spatial transcriptomic landscape across the remaining regions in H&E slides. In response, we propose PHG2ST, an ST-prompt guided histological hypergraph learning framework, which leverages sparse ST signals as prompts to guide histological hypergraph learning for global spatial gene expression prediction. Our framework fuses histological hypergraph representations at multiple scales through a masked ST-prompt encoding mechanism, improving robustness and generalizability. Benchmark evaluations on two public ST datasets demonstrate that PHG2ST outperforms the existing state-of-the-art methods and closely aligns with the ground truth. These results underscore the potential of leveraging sparse local ST data for scalable and cost-effective spatial gene expression mapping in real-world biomedical applications.</li>
</ul>

<h3>Title: When Tom Eats Kimchi: Evaluating Cultural Bias of Multimodal Large Language Models in Cultural Mixture Contexts</h3>
<ul>
<li><strong>Authors: </strong>Jun Seong Kim, Kyaw Ye Thu, Javad Ismayilzada, Junyeong Park, Eunsu Kim, Huzama Ahmad, Na Min An, James Thorne, Alice Oh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16826">https://arxiv.org/abs/2503.16826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16826">https://arxiv.org/pdf/2503.16826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16826]] When Tom Eats Kimchi: Evaluating Cultural Bias of Multimodal Large Language Models in Cultural Mixture Contexts(https://arxiv.org/abs/2503.16826)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In a highly globalized world, it is important for multi-modal large language models (MLLMs) to recognize and respond correctly to mixed-cultural inputs. For example, a model should correctly identify kimchi (Korean food) in an image both when an Asian woman is eating it, as well as an African man is eating it. However, current MLLMs show an over-reliance on the visual features of the person, leading to misclassification of the entities. To examine the robustness of MLLMs to different ethnicity, we introduce MixCuBe, a cross-cultural bias benchmark, and study elements from five countries and four ethnicities. Our findings reveal that MLLMs achieve both higher accuracy and lower sensitivity to such perturbation for high-resource cultures, but not for low-resource cultures. GPT-4o, the best-performing model overall, shows up to 58% difference in accuracy between the original and perturbed cultural settings in low-resource cultures. Our dataset is publicly available at: this https URL.</li>
</ul>

<h3>Title: Efficient and Expressive Public Key Authenticated Encryption with Keyword Search in Multi-user Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Jiayin Cai, Xingwen Zhao, Dexin Li, Hui Li, Kai Fan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16828">https://arxiv.org/abs/2503.16828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16828">https://arxiv.org/pdf/2503.16828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16828]] Efficient and Expressive Public Key Authenticated Encryption with Keyword Search in Multi-user Scenarios(https://arxiv.org/abs/2503.16828)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>Public key authenticated encryption with keyword search (PAEKS) represents a significant advancement of secure and searchable data sharing in public network systems, such as medical systems. It can effectively mitigate the risk of keyword guessing attacks (KGA), which is a critical issue in public key encryption with keyword search (PEKS). However, in scenarios with a large number of users, the enforced point-to-point access control necessitates that the data sender encrypt the same keyword using the public keys of multiple receivers to create indexes, while the data receiver also must generate trapdoors of size linear to senders in the system. The burden on users aiming for efficient data sharing is considerable, as the overheads increase linearly with the number of users. Furthermore, the majority of current PAEKS schemes lack expressive search functions, including conjunctions, disjunctions, or any monotone boolean formulas, which are prevalent in practical applications. To tackle the abovementioned challenges, we propose an efficient and expressive PAEKS scheme. In efficiency, one auxiliary server is integrated to assist users in generating indexes and trapdoors. Users encrypt with their respective private keys along with the public keys of the servers, facilitating secure and searchable data sharing while significantly minimizing overhead. Additionally, the LSSS is employed to implement expressive search, including monotone boolean queries. We also obfuscate the mapping relationship associated with the LSSS matrix to the keywords, thereby enhancing the privacy protection. Security analysis alongside theoretical and experimental evaluations of our scheme illustrates its practicality and efficiency in multi-user data sharing scenarios.</li>
</ul>

<h3>Title: Joint Self-Supervised Video Alignment and Action Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ali Shah Ali, Syed Ahmed Mahmood, Mubin Saeed, Andrey Konin, M. Zeeshan Zia, Quoc-Huy Tran</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16832">https://arxiv.org/abs/2503.16832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16832">https://arxiv.org/pdf/2503.16832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16832]] Joint Self-Supervised Video Alignment and Action Segmentation(https://arxiv.org/abs/2503.16832)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We introduce a novel approach for simultaneous self-supervised video alignment and action segmentation based on a unified optimal transport framework. In particular, we first tackle self-supervised video alignment by developing a fused Gromov-Wasserstein optimal transport formulation with a structural prior, which trains efficiently on GPUs and needs only a few iterations for solving the optimal transport problem. Our single-task method achieves the state-of-the-art performance on multiple video alignment benchmarks and outperforms VAVA, which relies on a traditional Kantorovich optimal transport formulation with an optimality prior. Furthermore, we extend our approach by proposing a unified optimal transport framework for joint self-supervised video alignment and action segmentation, which requires training and storing a single model and saves both time and memory consumption as compared to two different single-task models. Extensive evaluations on several video alignment and action segmentation datasets demonstrate that our multi-task method achieves comparable video alignment yet superior action segmentation results over previous methods in video alignment and action segmentation respectively. Finally, to the best of our knowledge, this is the first work to unify video alignment and action segmentation into a single model.</li>
</ul>

<h3>Title: Safe and Reliable Diffusion Models via Subspace Projection</h3>
<ul>
<li><strong>Authors: </strong>Huiqiang Chen, Tianqing Zhu, Linlin Wang, Xin Yu, Longxiang Gao, Wanlei Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16835">https://arxiv.org/abs/2503.16835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16835">https://arxiv.org/pdf/2503.16835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16835]] Safe and Reliable Diffusion Models via Subspace Projection(https://arxiv.org/abs/2503.16835)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Large-scale text-to-image (T2I) diffusion models have revolutionized image generation, enabling the synthesis of highly detailed visuals from textual descriptions. However, these models may inadvertently generate inappropriate content, such as copyrighted works or offensive images. While existing methods attempt to eliminate specific unwanted concepts, they often fail to ensure complete removal, allowing the concept to reappear in subtle forms. For instance, a model may successfully avoid generating images in Van Gogh's style when explicitly prompted with 'Van Gogh', yet still reproduce his signature artwork when given the prompt 'Starry Night'. In this paper, we propose SAFER, a novel and efficient approach for thoroughly removing target concepts from diffusion models. At a high level, SAFER is inspired by the observed low-dimensional structure of the text embedding space. The method first identifies a concept-specific subspace $S_c$ associated with the target concept c. It then projects the prompt embeddings onto the complementary subspace of $S_c$, effectively erasing the concept from the generated images. Since concepts can be abstract and difficult to fully capture using natural language alone, we employ textual inversion to learn an optimized embedding of the target concept from a reference image. This enables more precise subspace estimation and enhances removal performance. Furthermore, we introduce a subspace expansion strategy to ensure comprehensive and robust concept erasure. Extensive experiments demonstrate that SAFER consistently and effectively erases unwanted concepts from diffusion models while preserving generation quality.</li>
</ul>

<h3>Title: A Flexible Fairness Framework with Surrogate Loss Reweighting for Addressing Sociodemographic Disparities</h3>
<ul>
<li><strong>Authors: </strong>Wen Xu, Elham Dolatabadi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16836">https://arxiv.org/abs/2503.16836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16836">https://arxiv.org/pdf/2503.16836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16836]] A Flexible Fairness Framework with Surrogate Loss Reweighting for Addressing Sociodemographic Disparities(https://arxiv.org/abs/2503.16836)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>This paper presents a new algorithmic fairness framework called $\boldsymbol{\alpha}$-$\boldsymbol{\beta}$ Fair Machine Learning ($\boldsymbol{\alpha}$-$\boldsymbol{\beta}$ FML), designed to optimize fairness levels across sociodemographic attributes. Our framework employs a new family of surrogate loss functions, paired with loss reweighting techniques, allowing precise control over fairness-accuracy trade-offs through tunable hyperparameters $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$. To efficiently solve the learning objective, we propose Parallel Stochastic Gradient Descent with Surrogate Loss (P-SGD-S) and establish convergence guarantees for both convex and nonconvex loss functions. Experimental results demonstrate that our framework improves overall accuracy while reducing fairness violations, offering a smooth trade-off between standard empirical risk minimization and strict minimax fairness. Results across multiple datasets confirm its adaptability, ensuring fairness improvements without excessive performance degradation.</li>
</ul>

<h3>Title: LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jian Liang, Wenke Huang, Guancheng Wan, Qu Yang, Mang Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16843">https://arxiv.org/abs/2503.16843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16843">https://arxiv.org/pdf/2503.16843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16843]] LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models(https://arxiv.org/abs/2503.16843)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Multimodal Large Language Models (MLLMs) excel at generalizing across modalities and tasks, effectively adapting them to specific downstream tasks while simultaneously retaining both general and specialized knowledge remains challenging. Although Low-Rank Adaptation (LoRA) is widely used to efficiently acquire specialized knowledge in MLLMs, it introduces substantial harmful redundancy during visual instruction tuning, which exacerbates the forgetting of general knowledge and degrades downstream task performance. To address this issue, we propose LoRASculpt to eliminate harmful redundant parameters, thereby harmonizing general and specialized knowledge. Specifically, under theoretical guarantees, we introduce sparse updates into LoRA to discard redundant parameters effectively. Furthermore, we propose a Conflict Mitigation Regularizer to refine the update trajectory of LoRA, mitigating knowledge conflicts with the pretrained weights. Extensive experimental results demonstrate that even at very high degree of sparsity ($\le$ 5%), our method simultaneously enhances generalization and downstream task performance. This confirms that our approach effectively mitigates the catastrophic forgetting issue and further promotes knowledge harmonization in MLLMs.</li>
</ul>

<h3>Title: Early-MFC: Enhanced Flow Correlation Attacks on Tor via Multi-view Triplet Networks with Early Network Traffic</h3>
<ul>
<li><strong>Authors: </strong>Yali Yuan, Qianqi Niu, Yachao Yuan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16847">https://arxiv.org/abs/2503.16847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16847">https://arxiv.org/pdf/2503.16847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16847]] Early-MFC: Enhanced Flow Correlation Attacks on Tor via Multi-view Triplet Networks with Early Network Traffic(https://arxiv.org/abs/2503.16847)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Flow correlation attacks is an efficient network attacks, aiming to expose those who use anonymous network services, such as Tor. Conducting such attacks during the early stages of network communication is particularly critical for scenarios demanding rapid decision-making, such as cybercrime detection or financial fraud prevention. Although recent studies have made progress in flow correlation attacks techniques, research specifically addressing flow correlation with early network traffic flow remains limited. Moreover, due to factors such as model complexity, training costs, and real-time requirements, existing technologies cannot be directly applied to flow correlation with early network traffic flow. In this paper, we propose flow correlation attack with early network traffic, named Early-MFC, based on multi-view triplet networks. The proposed approach extracts multi-view traffic features from the payload at the transport layer and the Inter-Packet Delay. It then integrates multi-view flow information, converting the extracted features into shared embeddings. By leveraging techniques such as metric learning and contrastive learning, the method optimizes the embeddings space by ensuring that similar flows are mapped closer together while dissimilar flows are positioned farther apart. Finally, Bayesian decision theory is applied to determine flow correlation, enabling high-accuracy flow correlation with early network traffic flow. Furthermore, we investigate flow correlation attacks under extra-early network traffic flow conditions. To address this challenge, we propose Early-MFC+, which utilizes payload data to construct embedded feature representations, ensuring robust performance even with minimal packet availability.</li>
</ul>

<h3>Title: Physics-Informed Neural Network Surrogate Models for River Stage Prediction</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Zoch, Edward Holmberg, Pujan Pokhrel, Ken Pathak, Steven Sloan, Kendall Niles, Jay Ratcliff, Maik Flanagin, Elias Ioup, Christian Guetl, Mahdi Abdelguerfi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16850">https://arxiv.org/abs/2503.16850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16850">https://arxiv.org/pdf/2503.16850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16850]] Physics-Informed Neural Network Surrogate Models for River Stage Prediction(https://arxiv.org/abs/2503.16850)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This work investigates the feasibility of using Physics-Informed Neural Networks (PINNs) as surrogate models for river stage prediction, aiming to reduce computational cost while maintaining predictive accuracy. Our primary contribution demonstrates that PINNs can successfully approximate HEC-RAS numerical solutions when trained on a single river, achieving strong predictive accuracy with generally low relative errors, though some river segments exhibit higher deviations. By integrating the governing Saint-Venant equations into the learning process, the proposed PINN-based surrogate model enforces physical consistency and significantly improves computational efficiency compared to HEC-RAS. We evaluate the model's performance in terms of accuracy and computational speed, demonstrating that it closely approximates HEC-RAS predictions while enabling real-time inference. These results highlight the potential of PINNs as effective surrogate models for single-river hydrodynamics, offering a promising alternative for computationally efficient river stage forecasting. Future work will explore techniques to enhance PINN training stability and robustness across a more generalized multi-river model.</li>
</ul>

<h3>Title: Towards LLM Guardrails via Sparse Representation Steering</h3>
<ul>
<li><strong>Authors: </strong>Zeqing He, Zhibo Wang, Huiyu Xu, Kui Ren</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16851">https://arxiv.org/abs/2503.16851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16851">https://arxiv.org/pdf/2503.16851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16851]] Towards LLM Guardrails via Sparse Representation Steering(https://arxiv.org/abs/2503.16851)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable performance in natural language generation tasks, yet their uncontrolled outputs pose significant ethical and safety risks. Recently, representation engineering methods have shown promising results in steering model behavior by modifying the rich semantic information encoded in activation vectors. However, due to the difficulty of precisely disentangling semantic directions within high-dimensional representation space, existing approaches suffer from three major limitations: lack of fine-grained control, quality degradation of generated content, and poor interpretability. To address these challenges, we propose a sparse encoding-based representation engineering method, named SRE, which decomposes polysemantic activations into a structured, monosemantic feature space. By leveraging sparse autoencoding, our approach isolates and adjusts only task-specific sparse feature dimensions, enabling precise and interpretable steering of model behavior while preserving content quality. We validate our method on three critical domains, i.e., safety, fairness, and truthfulness using the open-source LLM Gemma-2-2B-it. Experimental results show that SRE achieves superior controllability while maintaining the overall quality of generated content (i.e., controllability and quality), demonstrating its effectiveness as a fine-grained and interpretable activation steering framework.</li>
</ul>

<h3>Title: Casual Inference via Style Bias Deconfounding for Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Jiaxi Li, Di Lin, Hao Chen, Hongying Liu, Liang Wan, Wei Feng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16852">https://arxiv.org/abs/2503.16852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16852">https://arxiv.org/pdf/2503.16852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16852]] Casual Inference via Style Bias Deconfounding for Domain Generalization(https://arxiv.org/abs/2503.16852)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, fair</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) often struggle with out-of-distribution data, limiting their reliability in diverse realworld applications. To address this issue, domain generalization methods have been developed to learn domain-invariant features from single or multiple training domains, enabling generalization to unseen testing domains. However, existing approaches usually overlook the impact of style frequency within the training set. This oversight predisposes models to capture spurious visual correlations caused by style confounding factors, rather than learning truly causal representations, thereby undermining inference reliability. In this work, we introduce Style Deconfounding Causal Learning (SDCL), a novel causal inference-based framework designed to explicitly address style as a confounding factor. Our approaches begins with constructing a structural causal model (SCM) tailored to the domain generalization problem and applies a backdoor adjustment strategy to account for style influence. Building on this foundation, we design a style-guided expert module (SGEM) to adaptively clusters style distributions during training, capturing the global confounding style. Additionally, a back-door causal learning module (BDCL) performs causal interventions during feature extraction, ensuring fair integration of global confounding styles into sample predictions, effectively reducing style bias. The SDCL framework is highly versatile and can be seamlessly integrated with state-of-the-art data augmentation techniques. Extensive experiments across diverse natural and medical image recognition tasks validate its efficacy, demonstrating superior performance in both multi-domain and the more challenging single-domain generalization scenarios.</li>
</ul>

<h3>Title: Imagine to Hear: Auditory Knowledge Generation can be an Effective Assistant for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Suho Yoo, Hyunjong Ok, Jaeho Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16853">https://arxiv.org/abs/2503.16853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16853">https://arxiv.org/pdf/2503.16853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16853]] Imagine to Hear: Auditory Knowledge Generation can be an Effective Assistant for Language Models(https://arxiv.org/abs/2503.16853)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Language models pretrained on text-only corpora often struggle with tasks that require auditory commonsense knowledge. Previous work addresses this problem by augmenting the language model to retrieve knowledge from external audio databases. This approach has several limitations, such as the potential lack of relevant audio in databases and the high costs associated with constructing and querying the databases. To address these issues, we propose Imagine to Hear, a novel approach that dynamically generates auditory knowledge using generative models. Our framework detects multiple audio-related textual spans from the given prompt and generates corresponding auditory knowledge. We develop several mechanisms to efficiently process multiple auditory knowledge, including a CLAP-based rejection sampler and a language-audio fusion module. Our experiments show that our method achieves state-of-the-art performance on AuditoryBench without relying on external databases, highlighting the effectiveness of our generation-based approach.</li>
</ul>

<h3>Title: Generative Compositor for Few-Shot Visual Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Zhibo Yang, Wei Hua, Sibo Song, Cong Yao, Yingying Zhu, Wenqing Cheng, Xiang Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16854">https://arxiv.org/abs/2503.16854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16854">https://arxiv.org/pdf/2503.16854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16854]] Generative Compositor for Few-Shot Visual Information Extraction(https://arxiv.org/abs/2503.16854)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>Visual Information Extraction (VIE), aiming at extracting structured information from visually rich document images, plays a pivotal role in document processing. Considering various layouts, semantic scopes, and languages, VIE encompasses an extensive range of types, potentially numbering in the thousands. However, many of these types suffer from a lack of training data, which poses significant challenges. In this paper, we propose a novel generative model, named Generative Compositor, to address the challenge of few-shot VIE. The Generative Compositor is a hybrid pointer-generator network that emulates the operations of a compositor by retrieving words from the source text and assembling them based on the provided prompts. Furthermore, three pre-training strategies are employed to enhance the model's perception of spatial context information. Besides, a prompt-aware resampler is specially designed to enable efficient matching by leveraging the entity-semantic prior contained in prompts. The introduction of the prompt-based retrieval mechanism and the pre-training strategies enable the model to acquire more effective spatial and semantic clues with limited training samples. Experiments demonstrate that the proposed method achieves highly competitive results in the full-sample training, while notably outperforms the baseline in the 1-shot, 5-shot, and 10-shot settings.</li>
</ul>

<h3>Title: Stack Transformer Based Spatial-Temporal Attention Model for Dynamic Multi-Culture Sign Language Recognition</h3>
<ul>
<li><strong>Authors: </strong>Koki Hirooka, Abu Saleh Musa Miah, Tatsuya Murakami, Yuto Akiba, Yong Seok Hwang, Jungpil Shin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16855">https://arxiv.org/abs/2503.16855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16855">https://arxiv.org/pdf/2503.16855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16855]] Stack Transformer Based Spatial-Temporal Attention Model for Dynamic Multi-Culture Sign Language Recognition(https://arxiv.org/abs/2503.16855)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Hand gesture-based Sign Language Recognition (SLR) serves as a crucial communication bridge between deaf and non-deaf individuals. Existing SLR systems perform well for their cultural SL but may struggle with multi-cultural sign languages (McSL). To address these challenges, this paper proposes a Stack Spatial-Temporal Transformer Network that leverages multi-head attention mechanisms to capture both spatial and temporal dependencies with hierarchical features using the Stack Transfer concept. In the proceed, firstly, we applied a fully connected layer to make a embedding vector which has high expressive power from the original dataset, then fed them a stack newly proposed transformer to achieve hierarchical features with short-range and long-range dependency. The network architecture is composed of several stages that process spatial and temporal relationships sequentially, ensuring effective feature extraction. After making the fully connected layer, the embedding vector is processed by the Spatial Multi-Head Attention Transformer, which captures spatial dependencies between joints. In the next stage, the Temporal Multi-Head Attention Transformer captures long-range temporal dependencies, and again, the features are concatenated with the output using another skip connection. The processed features are then passed to the Feed-Forward Network (FFN), which refines the feature representations further. After the FFN, additional skip connections are applied to combine the output with earlier layers, followed by a final normalization layer to produce the final output feature tensor. This process is repeated for 10 transformer blocks. The extensive experiment shows that the JSL, KSL and ASL datasets achieved good performance accuracy. Our approach demonstrates improved performance in McSL, and it will be consider as a novel work in this domain.</li>
</ul>

<h3>Title: MTBench: A Multimodal Time Series Benchmark for Temporal Reasoning and Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Jialin Chen, Aosong Feng, Ziyu Zhao, Juan Garza, Gaukhar Nurbek, Cheng Qin, Ali Maatouk, Leandros Tassiulas, Yifeng Gao, Rex Ying</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16858">https://arxiv.org/abs/2503.16858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16858">https://arxiv.org/pdf/2503.16858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16858]] MTBench: A Multimodal Time Series Benchmark for Temporal Reasoning and Question Answering(https://arxiv.org/abs/2503.16858)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding the relationship between textual news and time-series evolution is a critical yet under-explored challenge in applied data science. While multimodal learning has gained traction, existing multimodal time-series datasets fall short in evaluating cross-modal reasoning and complex question answering, which are essential for capturing complex interactions between narrative information and temporal patterns. To bridge this gap, we introduce Multimodal Time Series Benchmark (MTBench), a large-scale benchmark designed to evaluate large language models (LLMs) on time series and text understanding across financial and weather domains. MTbench comprises paired time series and textual data, including financial news with corresponding stock price movements and weather reports aligned with historical temperature records. Unlike existing benchmarks that focus on isolated modalities, MTbench provides a comprehensive testbed for models to jointly reason over structured numerical trends and unstructured textual narratives. The richness of MTbench enables formulation of diverse tasks that require a deep understanding of both text and time-series data, including time-series forecasting, semantic and technical trend analysis, and news-driven question answering (QA). These tasks target the model's ability to capture temporal dependencies, extract key insights from textual context, and integrate cross-modal information. We evaluate state-of-the-art LLMs on MTbench, analyzing their effectiveness in modeling the complex relationships between news narratives and temporal patterns. Our findings reveal significant challenges in current models, including difficulties in capturing long-term dependencies, interpreting causality in financial and weather trends, and effectively fusing multimodal information.</li>
</ul>

<h3>Title: Nonparametric Factor Analysis and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Yujia Zheng, Yang Liu, Jiaxiong Yao, Yingyao Hu, Kun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16865">https://arxiv.org/abs/2503.16865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16865">https://arxiv.org/pdf/2503.16865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16865]] Nonparametric Factor Analysis and Beyond(https://arxiv.org/abs/2503.16865)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Nearly all identifiability results in unsupervised representation learning inspired by, e.g., independent component analysis, factor analysis, and causal representation learning, rely on assumptions of additive independent noise or noiseless regimes. In contrast, we study the more general case where noise can take arbitrary forms, depend on latent variables, and be non-invertibly entangled within a nonlinear function. We propose a general framework for identifying latent variables in the nonparametric noisy settings. We first show that, under suitable conditions, the generative model is identifiable up to certain submanifold indeterminacies even in the presence of non-negligible noise. Furthermore, under the structural or distributional variability conditions, we prove that latent variables of the general nonlinear models are identifiable up to trivial indeterminacies. Based on the proposed theoretical framework, we have also developed corresponding estimation methods and validated them in various synthetic and real-world settings. Interestingly, our estimate of the true GDP growth from alternative measurements suggests more insightful information on the economies than official reports. We expect our framework to provide new insight into how both researchers and practitioners deal with latent variables in real-world scenarios.</li>
</ul>

<h3>Title: Joint Extraction Matters: Prompt-Based Visual Question Answering for Multi-Field Document Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Mengsay Loem, Taiju Hosaka</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16868">https://arxiv.org/abs/2503.16868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16868">https://arxiv.org/pdf/2503.16868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16868]] Joint Extraction Matters: Prompt-Based Visual Question Answering for Multi-Field Document Information Extraction(https://arxiv.org/abs/2503.16868)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Visual question answering (VQA) has emerged as a flexible approach for extracting specific pieces of information from document images. However, existing work typically queries each field in isolation, overlooking potential dependencies across multiple items. This paper investigates the merits of extracting multiple fields jointly versus separately. Through experiments on multiple large vision language models and datasets, we show that jointly extracting fields often improves accuracy, especially when the fields share strong numeric or contextual dependencies. We further analyze how performance scales with the number of requested items and use a regression based metric to quantify inter field relationships. Our results suggest that multi field prompts can mitigate confusion arising from similar surface forms and related numeric values, providing practical methods for designing robust VQA systems in document information extraction tasks.</li>
</ul>

<h3>Title: Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Anshumann, Mohd Abbas Zaidi, Akhil Kedia, Jinwoo Ahn, Taehwak Kwon, Kangwook Lee, Haejun Lee, Joohyung Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16870">https://arxiv.org/abs/2503.16870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16870">https://arxiv.org/pdf/2503.16870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16870]] Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs(https://arxiv.org/abs/2503.16870)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge distillation can be a cost-effective technique to distill knowledge in Large Language Models, if the teacher output logits can be pre-computed and cached. However, successfully applying this to pre-training remains largely unexplored. In this work, we prove that naive approaches for sparse knowledge distillation such as caching Top-K probabilities, while intuitive, provide biased estimates of teacher probability distribution to the student, resulting in suboptimal performance and calibration. We propose an importance-sampling-based method `Random Sampling Knowledge Distillation', which provides unbiased estimates, preserves the gradient in expectation, and requires storing significantly sparser logits. Our method enables faster training of student models with marginal overhead (<10%) compared to cross-entropy based training, while maintaining competitive performance compared to full distillation, across a range of model sizes from 300M to 3B.</li>
</ul>

<h3>Title: Lie Detector: Unified Backdoor Detection via Cross-Examination Framework</h3>
<ul>
<li><strong>Authors: </strong>Xuan Wang, Siyuan Liang, Dongping Liao, Han Fang, Aishan Liu, Xiaochun Cao, Yu-liang Lu, Ee-Chien Chang, Xitong Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16872">https://arxiv.org/abs/2503.16872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16872">https://arxiv.org/pdf/2503.16872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16872]] Lie Detector: Unified Backdoor Detection via Cross-Examination Framework(https://arxiv.org/abs/2503.16872)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust, large language model</a></li>
<li><strong>Abstract: </strong>Institutions with limited data and computing resources often outsource model training to third-party providers in a semi-honest setting, assuming adherence to prescribed training protocols with pre-defined learning paradigm (e.g., supervised or semi-supervised learning). However, this practice can introduce severe security risks, as adversaries may poison the training data to embed backdoors into the resulting model. Existing detection approaches predominantly rely on statistical analyses, which often fail to maintain universally accurate detection accuracy across different learning paradigms. To address this challenge, we propose a unified backdoor detection framework in the semi-honest setting that exploits cross-examination of model inconsistencies between two independent service providers. Specifically, we integrate central kernel alignment to enable robust feature similarity measurements across different model architectures and learning paradigms, thereby facilitating precise recovery and identification of backdoor triggers. We further introduce backdoor fine-tuning sensitivity analysis to distinguish backdoor triggers from adversarial perturbations, substantially reducing false positives. Extensive experiments demonstrate that our method achieves superior detection performance, improving accuracy by 5.4%, 1.6%, and 11.9% over SoTA baselines across supervised, semi-supervised, and autoregressive learning tasks, respectively. Notably, it is the first to effectively detect backdoors in multimodal large language models, further highlighting its broad applicability and advancing secure deep learning.</li>
</ul>

<h3>Title: Malliavin-Bismut Score-based Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ehsan Mirafzali, Utkarsh Gupta, Patrick Wyrod, Frank Proske, Daniele Venturi, Razvan Marinescu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16917">https://arxiv.org/abs/2503.16917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16917">https://arxiv.org/pdf/2503.16917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16917]] Malliavin-Bismut Score-based Diffusion Models(https://arxiv.org/abs/2503.16917)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce a new framework that employs Malliavin calculus to derive explicit expressions for the score function -- i.e., the gradient of the log-density -- associated with solutions to stochastic differential equations (SDEs). Our approach integrates classical integration-by-parts techniques with modern tools, such as Bismut's formula and Malliavin calculus, to address linear and nonlinear SDEs. In doing so, we establish a rigorous connection between the Malliavin derivative, its adjoint (the Malliavin divergence or the Skorokhod integral), Bismut's formula, and diffusion generative models, thus providing a systematic method for computing $\nabla \log p_t(x)$. For the linear case, we present a detailed study proving that our formula is equivalent to the actual score function derived from the solution of the Fokker--Planck equation for linear SDEs. Additionally, we derive a closed-form expression for $\nabla \log p_t(x)$ for nonlinear SDEs with state-independent diffusion coefficients. These advancements provide fresh theoretical insights into the smoothness and structure of probability densities and practical implications for score-based generative modelling, including the design and analysis of new diffusion models. Moreover, our findings promote the adoption of the robust Malliavin calculus framework in machine learning research. These results directly apply to various pure and applied mathematics fields, such as generative modelling, the study of SDEs driven by fractional Brownian motion, and the Fokker--Planck equations associated with nonlinear SDEs.</li>
</ul>

<h3>Title: When Preferences Diverge: Aligning Diffusion Models with Minority-Aware Adaptive DPO</h3>
<ul>
<li><strong>Authors: </strong>Lingfan Zhang, Chen Liu, Chengming Xu, Kai Hu, Donghao Luo, Chengjie Wang, Yanwei Fu, Yuan Yao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16921">https://arxiv.org/abs/2503.16921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16921">https://arxiv.org/pdf/2503.16921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16921]] When Preferences Diverge: Aligning Diffusion Models with Minority-Aware Adaptive DPO(https://arxiv.org/abs/2503.16921)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In recent years, the field of image generation has witnessed significant advancements, particularly in fine-tuning methods that align models with universal human preferences. This paper explores the critical role of preference data in the training process of diffusion models, particularly in the context of Diffusion-DPO and its subsequent adaptations. We investigate the complexities surrounding universal human preferences in image generation, highlighting the subjective nature of these preferences and the challenges posed by minority samples in preference datasets. Through pilot experiments, we demonstrate the existence of minority samples and their detrimental effects on model performance. We propose Adaptive-DPO -- a novel approach that incorporates a minority-instance-aware metric into the DPO objective. This metric, which includes intra-annotator confidence and inter-annotator stability, distinguishes between majority and minority samples. We introduce an Adaptive-DPO loss function which improves the DPO loss in two ways: enhancing the model's learning of majority labels while mitigating the negative impact of minority samples. Our experiments demonstrate that this method effectively handles both synthetic minority data and real-world preference data, paving the way for more effective training methodologies in image generation tasks.</li>
</ul>

<h3>Title: MerGen: Micro-electrode recording synthesis using a generative data-driven approach</h3>
<ul>
<li><strong>Authors: </strong>Thibault Martin, Paul Sauleau, Claire Haegelen, Pierre Jannin, John S. H. Baxter</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16928">https://arxiv.org/abs/2503.16928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16928">https://arxiv.org/pdf/2503.16928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16928]] MerGen: Micro-electrode recording synthesis using a generative data-driven approach(https://arxiv.org/abs/2503.16928)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The analysis of electrophysiological data is crucial for certain surgical procedures such as deep brain stimulation, which has been adopted for the treatment of a variety of neurological disorders. During the procedure, auditory analysis of these signals helps the clinical team to infer the neuroanatomical location of the stimulation electrode and thus optimize clinical outcomes. This task is complex, and requires an expert who in turn requires significant training. In this paper, we propose a generative neural network, called MerGen, capable of simulating de novo electrophysiological recordings, with a view to providing a realistic learning tool for clinicians trainees for identifying these signals. We demonstrate that the generated signals are perceptually indistinguishable from real signals by experts in the field, and that it is even possible to condition the generation efficiently to provide a didactic simulator adapted to a particular surgical scenario. The efficacy of this conditioning is demonstrated, comparing it to intra-observer and inter-observer variability amongst experts. We also demonstrate the use of this network for data augmentation for automatic signal classification which can play a role in decision-making support in the operating theatre.</li>
</ul>

<h3>Title: TEMPO: Temporal Preference Optimization of Video LLMs via Difficulty Scheduling and Pre-SFT Alignment</h3>
<ul>
<li><strong>Authors: </strong>Shicheng Li, Lei Li, Kun Ouyang, Shuhuai Ren, Yuanxin Liu, Yuanxing Zhang, Fuzheng Zhang, Lingpeng Kong, Qi Liu, Xu Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16929">https://arxiv.org/abs/2503.16929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16929">https://arxiv.org/pdf/2503.16929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16929]] TEMPO: Temporal Preference Optimization of Video LLMs via Difficulty Scheduling and Pre-SFT Alignment(https://arxiv.org/abs/2503.16929)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Video Large Language Models (Video LLMs) have achieved significant success by leveraging a two-stage paradigm: pretraining on large-scale video-text data for vision-language alignment, followed by supervised fine-tuning (SFT) for task-specific capabilities. However, existing approaches struggle with temporal reasoning due to weak temporal correspondence in the data and reliance on the next-token prediction paradigm during training. To address these limitations, we propose TEMPO (TEMporal Preference Optimization), a systematic framework that enhances Video LLMs' temporal reasoning capabilities through Direct Preference Optimization (DPO). To facilitate this, we introduce an automated preference data generation pipeline that systematically constructs preference pairs by selecting videos that are rich in temporal information, designing video-specific perturbation strategies, and finally evaluating model responses on clean and perturbed video inputs. Our temporal alignment features two key innovations: curriculum learning which that progressively increases perturbation difficulty to improve model robustness and adaptability; and ``Pre-SFT Alignment'', applying preference optimization before instruction tuning to prioritize fine-grained temporal comprehension. Extensive experiments demonstrate that our approach consistently improves Video LLM performance across multiple benchmarks with a relatively small set of self-generated DPO data. We further analyze the transferability of DPO data across architectures and the role of difficulty scheduling in optimization. Our findings highlight our TEMPO as a scalable and efficient complement to SFT-based methods, paving the way for developing reliable Video LLMs.</li>
</ul>

<h3>Title: Vision-Language Gradient Descent-driven All-in-One Deep Unfolding Networks</h3>
<ul>
<li><strong>Authors: </strong>Haijin Zeng, Xiangming Wang, Yongyong Chen, Jingyong Su, Jie Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16930">https://arxiv.org/abs/2503.16930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16930">https://arxiv.org/pdf/2503.16930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16930]] Vision-Language Gradient Descent-driven All-in-One Deep Unfolding Networks(https://arxiv.org/abs/2503.16930)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Dynamic image degradations, including noise, blur and lighting inconsistencies, pose significant challenges in image restoration, often due to sensor limitations or adverse environmental conditions. Existing Deep Unfolding Networks (DUNs) offer stable restoration performance but require manual selection of degradation matrices for each degradation type, limiting their adaptability across diverse scenarios. To address this issue, we propose the Vision-Language-guided Unfolding Network (VLU-Net), a unified DUN framework for handling multiple degradation types simultaneously. VLU-Net leverages a Vision-Language Model (VLM) refined on degraded image-text pairs to align image features with degradation descriptions, selecting the appropriate transform for target degradation. By integrating an automatic VLM-based gradient estimation strategy into the Proximal Gradient Descent (PGD) algorithm, VLU-Net effectively tackles complex multi-degradation restoration tasks while maintaining interpretability. Furthermore, we design a hierarchical feature unfolding structure to enhance VLU-Net framework, efficiently synthesizing degradation patterns across various levels. VLU-Net is the first all-in-one DUN framework and outperforms current leading one-by-one and all-in-one end-to-end methods by 3.74 dB on the SOTS dehazing dataset and 1.70 dB on the Rain100L deraining dataset.</li>
</ul>

<h3>Title: Re-HOLD: Video Hand Object Interaction Reenactment via adaptive Layout-instructed Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Yingying Fan, Quanwei Yang, Kaisiyuan Wang, Hang Zhou, Yingying Li, Haocheng Feng, Yu Wu, Jingdong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16942">https://arxiv.org/abs/2503.16942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16942">https://arxiv.org/pdf/2503.16942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16942]] Re-HOLD: Video Hand Object Interaction Reenactment via adaptive Layout-instructed Diffusion Model(https://arxiv.org/abs/2503.16942)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Current digital human studies focusing on lip-syncing and body movement are no longer sufficient to meet the growing industrial demand, while human video generation techniques that support interacting with real-world environments (e.g., objects) have not been well investigated. Despite human hand synthesis already being an intricate problem, generating objects in contact with hands and their interactions presents an even more challenging task, especially when the objects exhibit obvious variations in size and shape. To cope with these issues, we present a novel video Reenactment framework focusing on Human-Object Interaction (HOI) via an adaptive Layout-instructed Diffusion model (Re-HOLD). Our key insight is to employ specialized layout representation for hands and objects, respectively. Such representations enable effective disentanglement of hand modeling and object adaptation to diverse motion sequences. To further improve the generation quality of HOI, we have designed an interactive textural enhancement module for both hands and objects by introducing two independent memory banks. We also propose a layout-adjusting strategy for the cross-object reenactment scenario to adaptively adjust unreasonable layouts caused by diverse object sizes during inference. Comprehensive qualitative and quantitative evaluations demonstrate that our proposed framework significantly outperforms existing methods. Project page: this https URL.</li>
</ul>

<h3>Title: MagicColor: Multi-Instance Sketch Colorization</h3>
<ul>
<li><strong>Authors: </strong>Yinhan Zhang, Yue Ma, Bingyuan Wang, Qifeng Chen, Zeyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16948">https://arxiv.org/abs/2503.16948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16948">https://arxiv.org/pdf/2503.16948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16948]] MagicColor: Multi-Instance Sketch Colorization(https://arxiv.org/abs/2503.16948)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present \textit{MagicColor}, a diffusion-based framework for multi-instance sketch colorization. The production of multi-instance 2D line art colorization adheres to an industry-standard workflow, which consists of three crucial stages: the design of line art characters, the coloring of individual objects, and the refinement process. The artists are required to repeat the process of coloring each instance one by one, which is inaccurate and inefficient. Meanwhile, current generative methods fail to solve this task due to the challenge of multi-instance pair data collection. To tackle these challenges, we incorporate three technical designs to ensure precise character detail transcription and achieve multi-instance sketch colorization in a single forward. Specifically, we first propose the self-play training strategy to solve the lack of training data. Then we introduce an instance guider to feed the color of the instance. To achieve accurate color matching, we present fine-grained color matching with edge loss to enhance visual quality. Equipped with the proposed modules, MagicColor enables automatically transforming sketches into vividly-colored images with accurate consistency and multi-instance control. Experiments on our collected datasets show that our model outperforms existing methods regarding chromatic precision. Specifically, our model critically automates the colorization process with zero manual adjustments, so novice users can produce stylistically consistent artwork by providing reference instances and the original line art. Our code and additional details are available at this https URL</li>
</ul>

<h3>Title: CleanStack: A New Dual-Stack for Defending Against Stack-Based Memory Corruption Attacks</h3>
<ul>
<li><strong>Authors: </strong>Lei Chong</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16950">https://arxiv.org/abs/2503.16950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16950">https://arxiv.org/pdf/2503.16950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16950]] CleanStack: A New Dual-Stack for Defending Against Stack-Based Memory Corruption Attacks(https://arxiv.org/abs/2503.16950)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack</a></li>
<li><strong>Abstract: </strong>Stack-based memory corruption vulnerabilities have long been exploited by attackers to execute arbitrary code or perform unauthorized memory operations. Various defense mechanisms have been introduced to mitigate stack memory errors, but they typically focus on specific attack types, incur substantial performance overhead, or suffer from compatibility this http URL this paper, we present CleanStack, an efficient, highly compatible, and comprehensive stack protection mech anism. CleanStack isolates stack objects influenced by external input from other safe stack objects, thereby preventing attackers from modifying return addresses via controlled stack objects. Additionally, by randomizing the placement of tainted stack objects within the Unclean Stack, CleanStack mitigates non control data attacks by preventing attackers from predicting the stack layout.A key component of CleanStack is the identifica tion of tainted stack objects. We analyze both static program analysis and heuristic methods for this purpose. To maximize compatibility, we adopt a heuristic approach and implement CleanStack within the LLVM compiler framework, applying it to SPEC CPU2017 benchmarks and a real-world this http URL security evaluation demonstrates that CleanStack significantly reduces the exploitability of stack-based memory errors by providing a dual-stack system with isolation and randomization. Performance evaluation results indicate that CleanStack incurs an execution overhead of only 1.73% on the SPEC CPU2017 benchmark while introducing a minimal memory overhead of just 0.04%. Compared to existing stack protection techniques, CleanStack achieves an optimal balance between protection coverage, runtime overhead, and compatibility, making it one of the most comprehensive and efficient stack security solutions to date.</li>
</ul>

<h3>Title: Center-guided Classifier for Semantic Segmentation of Remote Sensing Images</h3>
<ul>
<li><strong>Authors: </strong>Wei Zhang, Mengting Ma, Yizhen Jiang, Rongrong Lian, Zhenkai Wu, Kangning Cui, Xiaowen Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16963">https://arxiv.org/abs/2503.16963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16963">https://arxiv.org/pdf/2503.16963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16963]] Center-guided Classifier for Semantic Segmentation of Remote Sensing Images(https://arxiv.org/abs/2503.16963)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>Compared with natural images, remote sensing images (RSIs) have the unique characteristic. i.e., larger intraclass variance, which makes semantic segmentation for remote sensing images more challenging. Moreover, existing semantic segmentation models for remote sensing images usually employ a vanilla softmax classifier, which has three drawbacks: (1) non-direct supervision for the pixel representations during training; (2) inadequate modeling ability of parametric softmax classifiers under large intraclass variance; and (3) opaque process of classification decision. In this paper, we propose a novel classifier (called CenterSeg) customized for RSI semantic segmentation, which solves the abovementioned problems with multiple prototypes, direct supervision under Grassmann manifold, and interpretability strategy. Specifically, for each class, our CenterSeg obtains local class centers by aggregating corresponding pixel features based on ground-truth masks, and generates multiple prototypes through hard attention assignment and momentum updating. In addition, we introduce the Grassmann manifold and constrain the joint embedding space of pixel features and prototypes based on two additional regularization terms. Especially, during the inference, CenterSeg can further provide interpretability to the model by restricting the prototype as a sample of the training set. Experimental results on three remote sensing segmentation datasets validate the effectiveness of the model. Besides the superior performance, CenterSeg has the advantages of simplicity, lightweight, compatibility, and interpretability. Code is available at this https URL.</li>
</ul>

<h3>Title: DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery</h3>
<ul>
<li><strong>Authors: </strong>Jiadong Tang, Yu Gao, Dianyi Yang, Liqi Yan, Yufeng Yue, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16964">https://arxiv.org/abs/2503.16964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16964">https://arxiv.org/pdf/2503.16964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16964]] DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery(https://arxiv.org/abs/2503.16964)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Drones have become essential tools for reconstructing wild scenes due to their outstanding maneuverability. Recent advances in radiance field methods have achieved remarkable rendering quality, providing a new avenue for 3D reconstruction from drone imagery. However, dynamic distractors in wild environments challenge the static scene assumption in radiance fields, while limited view constraints hinder the accurate capture of underlying scene geometry. To address these challenges, we introduce DroneSplat, a novel framework designed for robust 3D reconstruction from in-the-wild drone imagery. Our method adaptively adjusts masking thresholds by integrating local-global segmentation heuristics with statistical approaches, enabling precise identification and elimination of dynamic distractors in static scenes. We enhance 3D Gaussian Splatting with multi-view stereo predictions and a voxel-guided optimization strategy, supporting high-quality rendering under limited view constraints. For comprehensive evaluation, we provide a drone-captured 3D reconstruction dataset encompassing both dynamic and static scenes. Extensive experiments demonstrate that DroneSplat outperforms both 3DGS and NeRF baselines in handling in-the-wild drone imagery.</li>
</ul>

<h3>Title: ARFlow: Human Action-Reaction Flow Matching with Physical Guidance</h3>
<ul>
<li><strong>Authors: </strong>Wentao Jiang, Jingya Wang, Haotao Lu, Kaiyang Ji, Baoxiong Jia, Siyuan Huang, Ye Shi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16973">https://arxiv.org/abs/2503.16973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16973">https://arxiv.org/pdf/2503.16973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16973]] ARFlow: Human Action-Reaction Flow Matching with Physical Guidance(https://arxiv.org/abs/2503.16973)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Human action-reaction synthesis, a fundamental challenge in modeling causal human interactions, plays a critical role in applications ranging from virtual reality to social robotics. While diffusion-based models have demonstrated promising performance, they exhibit two key limitations for interaction synthesis: reliance on complex noise-to-reaction generators with intricate conditional mechanisms, and frequent physical violations in generated motions. To address these issues, we propose Action-Reaction Flow Matching (ARFlow), a novel framework that establishes direct action-to-reaction mappings, eliminating the need for complex conditional mechanisms. Our approach introduces two key innovations: an x1-prediction method that directly outputs human motions instead of velocity fields, enabling explicit constraint enforcement; and a training-free, gradient-based physical guidance mechanism that effectively prevents body penetration artifacts during sampling. Extensive experiments on NTU120 and Chi3D datasets demonstrate that ARFlow not only outperforms existing methods in terms of FrÃ©chet Inception Distance and motion diversity but also significantly reduces body collisions, as measured by our new Intersection Volume and Intersection Frequency metrics.</li>
</ul>

<h3>Title: EasyRobust: A Comprehensive and Easy-to-use Toolkit for Robust and Generalized Vision</h3>
<ul>
<li><strong>Authors: </strong>Xiaofeng Mao, Yuefeng Chen, Rong Zhang, Hui Xue, Zhao Li, Hang Su</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16975">https://arxiv.org/abs/2503.16975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16975">https://arxiv.org/pdf/2503.16975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16975]] EasyRobust: A Comprehensive and Easy-to-use Toolkit for Robust and Generalized Vision(https://arxiv.org/abs/2503.16975)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) has shown great promise in computer vision tasks. However, machine vision achieved by DNNs cannot be as robust as human perception. Adversarial attacks and data distribution shifts have been known as two major scenarios which degrade machine performance and obstacle the wide deployment of machines "in the wild". In order to break these obstructions and facilitate the research of model robustness, we develop EasyRobust, a comprehensive and easy-to-use toolkit for training, evaluation and analysis of robust vision models. EasyRobust targets at two types of robustness: 1) Adversarial robustness enables the model to defense against malicious inputs crafted by worst-case perturbations, also known as adversarial examples; 2) Non-adversarial robustness enhances the model performance on natural test images with corruptions or distribution shifts. Thorough benchmarks on image classification enable EasyRobust to provide an accurate robustness evaluation on vision models. We wish our EasyRobust can help for training practically-robust models and promote academic and industrial progress in closing the gap between human and machine vision. Codes and models of EasyRobust have been open-sourced in this https URL.</li>
</ul>

<h3>Title: GeoT: Geometry-guided Instance-dependent Transition Matrix for Semi-supervised Tooth Point Cloud Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Weihao Yu, Xiaoqing Guo, Chenxin Li, Yifan Liu, Yixuan Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16976">https://arxiv.org/abs/2503.16976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16976">https://arxiv.org/pdf/2503.16976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16976]] GeoT: Geometry-guided Instance-dependent Transition Matrix for Semi-supervised Tooth Point Cloud Segmentation(https://arxiv.org/abs/2503.16976)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Achieving meticulous segmentation of tooth point clouds from intra-oral scans stands as an indispensable prerequisite for various orthodontic applications. Given the labor-intensive nature of dental annotation, a significant amount of data remains unlabeled, driving increasing interest in semi-supervised approaches. One primary challenge of existing semi-supervised medical segmentation methods lies in noisy pseudo labels generated for unlabeled data. To address this challenge, we propose GeoT, the first framework that employs instance-dependent transition matrix (IDTM) to explicitly model noise in pseudo labels for semi-supervised dental segmentation. Specifically, to handle the extensive solution space of IDTM arising from tens of thousands of dental points, we introduce tooth geometric priors through two key components: point-level geometric regularization (PLGR) to enhance consistency between point adjacency relationships in 3D and IDTM spaces, and class-level geometric smoothing (CLGS) to leverage the fixed spatial distribution of tooth categories for optimal IDTM estimation. Extensive experiments performed on the public Teeth3DS dataset and private dataset demonstrate that our method can make full utilization of unlabeled data to facilitate segmentation, achieving performance comparable to fully supervised methods with only $20\%$ of the labeled data.</li>
</ul>

<h3>Title: Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haichao Zhang, Zhuowei Li, Dimitris Metaxas, Yun Fu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16980">https://arxiv.org/abs/2503.16980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16980">https://arxiv.org/pdf/2503.16980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16980]] Token Dynamics: Towards Efficient and Dynamic Video Token Representation for Video Large Language Models(https://arxiv.org/abs/2503.16980)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Token-based video representation has emerged as a promising approach for enabling large language models to interpret video content. However, existing token reduction techniques, such as token pruning and token merging, often disrupt essential spatial-temporal positional embeddings, failing to adequately balance computational efficiency with fewer tokens. Consequently, these methods result in relatively lengthy token sequences, limiting their applicability in scenarios requiring extreme token compression, such as video large language models. In this paper, we introduce the novel task of extreme short token reduction, aiming to represent extensive video sequences with a minimal number of tokens. To address this challenge, we propose Token Dynamics, a new video representation framework that dynamically reduces token count while preserving spatial-temporal coherence. Specifically, we disentangle video representations by separating visual embeddings from grid-level motion information, structuring them into: 1. a concise token base, created by clustering tokens that describe object-level content; 2. a token dynamics map, capturing detailed spatial-temporal motion patterns across grids. Furthermore, we introduce a cross-dynamics attention mechanism that integrates motion features into the token base without increasing token length, thereby maintaining compactness and spatial-temporal integrity. The experiments demonstrate a reduction of token count to merely 0.07% of the original tokens, with only a minor performance drop of 1.13%. Additionally, we propose two novel subtasks within extreme token reduction (fixed-length and adaptive-length compression), both effectively representing long token sequences for video-language tasks. Our method offers significantly lower theoretical complexity, fewer tokens, and enhanced throughput, thus providing an efficient solution for video LLMs.</li>
</ul>

<h3>Title: Enabling Versatile Controls for Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xu Zhang, Hao Zhou, Haoming Qin, Xiaobin Lu, Jiaxing Yan, Guanzhong Wang, Zeyu Chen, Yi Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16983">https://arxiv.org/abs/2503.16983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16983">https://arxiv.org/pdf/2503.16983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16983]] Enabling Versatile Controls for Video Diffusion Models(https://arxiv.org/abs/2503.16983)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Despite substantial progress in text-to-video generation, achieving precise and flexible control over fine-grained spatiotemporal attributes remains a significant unresolved challenge in video generation research. To address these limitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework designed to enable fine-grained control over pre-trained video diffusion models in a unified manner. VCtrl integrates diverse user-specified control signals-such as Canny edges, segmentation masks, and human keypoints-into pretrained video diffusion models via a generalizable conditional module capable of uniformly encoding multiple types of auxiliary signals without modifying the underlying generator. Additionally, we design a unified control signal encoding pipeline and a sparse residual connection mechanism to efficiently incorporate control representations. Comprehensive experiments and human evaluations demonstrate that VCtrl effectively enhances controllability and generation quality. The source code and pre-trained models are publicly available and implemented using the PaddlePaddle framework at this http URL.</li>
</ul>

<h3>Title: EVSOAR: Security Orchestration, Automation and Response via EV Charging Stations</h3>
<ul>
<li><strong>Authors: </strong>Tadeu Freitas, Erick Silva, Rehana Yasmin, Ali Shoker, Manuel E. Correia, Rolando Martins, Paulo Esteves-Verissimo</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16984">https://arxiv.org/abs/2503.16984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16984">https://arxiv.org/pdf/2503.16984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16984]] EVSOAR: Security Orchestration, Automation and Response via EV Charging Stations(https://arxiv.org/abs/2503.16984)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Vehicle cybersecurity has emerged as a critical concern, driven by the innovation in the automotive industry, e.g., automomous, electric, or connnected vehicles. Current efforts to address these challenges are constrained by the limited computational resources of vehicles and the reliance on connected infrastructures. This motivated the foundation of Vehicle Security Operations Centers (VSOCs) that extend IT-based Security Operations Centers (SOCs) to cover the entire automotive ecosystem, both the in-vehicle and off-vehicle scopes. Security Orchestration, Automation, and Response (SOAR) tools are considered key for impelementing an effective cybersecurity solution. However, existing state-of-the-art solutions depend on infrastructure networks such as 4G, 5G, and WiFi, which often face scalability and congestion issues. To address these limitations, we propose a novel SOAR architecture EVSOAR that leverages the EV charging stations for connectivity and computing to enhance vehicle cybersecurity. Our EV-specific SOAR architecture enables real-time analysis and automated responses to cybersecurity threats closer to the EV, reducing the cellular latency, bandwidth, and interference limitations. Our experimental results demonstrate a significant improvement in latency, stability, and scalability through the infrastructure and the capacity to deploy computationally intensive applications, that are otherwise infeasible within the resource constraints of individual vehicles.</li>
</ul>

<h3>Title: Steady Progress Beats Stagnation: Mutual Aid of Foundation and Conventional Models in Mixed Domain Semi-Supervised Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Qinghe Ma, Jian Zhang, Zekun Li, Lei Qi, Qian Yu, Yinghuan Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.16997">https://arxiv.org/abs/2503.16997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.16997">https://arxiv.org/pdf/2503.16997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.16997]] Steady Progress Beats Stagnation: Mutual Aid of Foundation and Conventional Models in Mixed Domain Semi-Supervised Medical Image Segmentation(https://arxiv.org/abs/2503.16997)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Large pretrained visual foundation models exhibit impressive general capabilities. However, the extensive prior knowledge inherent in these models can sometimes be a double-edged sword when adapting them to downstream tasks in specific domains. In the context of semi-supervised medical image segmentation with domain shift, foundation models like MedSAM tend to make overconfident predictions, some of which are incorrect. The error accumulation hinders the effective utilization of unlabeled data and limits further improvements. In this paper, we introduce a Synergistic training framework for Foundation and Conventional models (SynFoC) to address the issue. We observe that a conventional model trained from scratch has the ability to correct the high-confidence mispredictions of the foundation model, while the foundation model can supervise it with high-quality pseudo-labels in the early training stages. Furthermore, to enhance the collaborative training effectiveness of both models and promote reliable convergence towards optimization, the consensus-divergence consistency regularization is proposed. We demonstrate the superiority of our method across four public multi-domain datasets. In particular, our method improves the Dice score by 10.31\% on the Prostate dataset. Our code is available at this https URL .</li>
</ul>

<h3>Title: A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications</h3>
<ul>
<li><strong>Authors: </strong>Jian Guan, Junfei Wu, Jia-Nan Li, Chuanqi Cheng, Wei Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17003">https://arxiv.org/abs/2503.17003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17003">https://arxiv.org/pdf/2503.17003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17003]] A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications(https://arxiv.org/abs/2503.17003)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their transition to real-world applications reveals a critical limitation: the inability to adapt to individual preferences while maintaining alignment with universal human values. Current alignment techniques adopt a one-size-fits-all approach that fails to accommodate users' diverse backgrounds and needs. This paper presents the first comprehensive survey of personalized alignment-a paradigm that enables LLMs to adapt their behavior within ethical boundaries based on individual preferences. We propose a unified framework comprising preference memory management, personalized generation, and feedback-based alignment, systematically analyzing implementation approaches and evaluating their effectiveness across various scenarios. By examining current techniques, potential risks, and future challenges, this survey provides a structured foundation for developing more adaptable and ethically-aligned LLMs.</li>
</ul>

<h3>Title: Privacy Enhanced QKD Networks: Zero Trust Relay Architecture based on Homomorphic Encryption</h3>
<ul>
<li><strong>Authors: </strong>Aitor Brazaola-Vicario, Oscar Lage, Julen BernabÃ©-RodrÃ­guez, Eduardo Jacob, Jasone Astorga</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17011">https://arxiv.org/abs/2503.17011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17011">https://arxiv.org/pdf/2503.17011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17011]] Privacy Enhanced QKD Networks: Zero Trust Relay Architecture based on Homomorphic Encryption(https://arxiv.org/abs/2503.17011)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy</a></li>
<li><strong>Abstract: </strong>Quantum key distribution (QKD) enables unconditionally secure symmetric key exchange between parties. However, terrestrial fibre-optic links face inherent distance constraints due to quantum signal degradation. Traditional solutions to overcome these limits rely on trusted relay nodes, which perform intermediate re-encryption of keys using one-time pad (OTP) encryption. This approach, however, exposes keys as plaintext at each relay, requiring significant trust and stringent security controls at every intermediate node. These "trusted" relays become a security liability if compromised. To address this issue, we propose a zero-trust relay design that applies fully homomorphic encryption (FHE) to perform intermediate OTP re-encryption without exposing plaintext keys, effectively mitigating the risks associated with potentially compromised or malicious relay nodes. Additionally, the architecture enhances crypto-agility by incorporating external quantum random number generators, thus decoupling key generation from specific QKD hardware and reducing vulnerabilities tied to embedded key-generation modules. The solution is designed with the existing European Telecommunication Standards Institute (ETSI) QKD standards in mind, enabling straightforward integration into current infrastructures. Its feasibility has been successfully demonstrated through a hybrid network setup combining simulated and commercially available QKD equipment. The proposed zero-trust architecture thus significantly advances the scalability and practical security of large-scale QKD networks, greatly reducing reliance on fully trusted infrastructure.</li>
</ul>

<h3>Title: Do regularization methods for shortcut mitigation work as intended?</h3>
<ul>
<li><strong>Authors: </strong>Haoyang Hong, Ioanna Papanikolaou, Sonali Parbhoo</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17015">https://arxiv.org/abs/2503.17015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17015">https://arxiv.org/pdf/2503.17015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17015]] Do regularization methods for shortcut mitigation work as intended?(https://arxiv.org/abs/2503.17015)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Mitigating shortcuts, where models exploit spurious correlations in training data, remains a significant challenge for improving generalization. Regularization methods have been proposed to address this issue by enhancing model generalizability. However, we demonstrate that these methods can sometimes overregularize, inadvertently suppressing causal features along with spurious ones. In this work, we analyze the theoretical mechanisms by which regularization mitigates shortcuts and explore the limits of its effectiveness. Additionally, we identify the conditions under which regularization can successfully eliminate shortcuts without compromising causal features. Through experiments on synthetic and real-world datasets, our comprehensive analysis provides valuable insights into the strengths and limitations of regularization techniques for addressing shortcuts, offering guidance for developing more robust models.</li>
</ul>

<h3>Title: RAW-Adapter: Adapting Pre-trained Visual Model to Camera RAW Images and A Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Ziteng Cui, Jianfei Yang, Tatsuya Harada</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17027">https://arxiv.org/abs/2503.17027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17027">https://arxiv.org/pdf/2503.17027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17027]] RAW-Adapter: Adapting Pre-trained Visual Model to Camera RAW Images and A Benchmark(https://arxiv.org/abs/2503.17027)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In the computer vision community, the preference for pre-training visual models has largely shifted toward sRGB images due to their ease of acquisition and compact storage. However, camera RAW images preserve abundant physical details across diverse real-world scenarios. Despite this, most existing visual perception methods that utilize RAW data directly integrate image signal processing (ISP) stages with subsequent network modules, often overlooking potential synergies at the model level. Building on recent advances in adapter-based methodologies in both NLP and computer vision, we propose RAW-Adapter, a novel framework that incorporates learnable ISP modules as input-level adapters to adjust RAW inputs. At the same time, it employs model-level adapters to seamlessly bridge ISP processing with high-level downstream architectures. Moreover, RAW-Adapter serves as a general framework applicable to various computer vision frameworks. Furthermore, we introduce RAW-Bench, which incorporates 17 types of RAW-based common corruptions, including lightness degradations, weather effects, blurriness, camera imaging degradations, and variations in camera color response. Using this benchmark, we systematically compare the performance of RAW-Adapter with state-of-the-art (SOTA) ISP methods and other RAW-based high-level vision algorithms. Additionally, we propose a RAW-based data augmentation strategy to further enhance RAW-Adapter's performance and improve its out-of-domain (OOD) generalization ability. Extensive experiments substantiate the effectiveness and efficiency of RAW-Adapter, highlighting its robust performance across diverse scenarios.</li>
</ul>

<h3>Title: An Attentive Representative Sample Selection Strategy Combined with Balanced Batch Training for Skin Lesion Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Stephen Lloyd-Brown, Susan Francis, Caroline Hoad, Penny Gowland, Karen Mullinger, Andrew French, Xin Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17034">https://arxiv.org/abs/2503.17034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17034">https://arxiv.org/pdf/2503.17034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17034]] An Attentive Representative Sample Selection Strategy Combined with Balanced Batch Training for Skin Lesion Segmentation(https://arxiv.org/abs/2503.17034)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>An often overlooked problem in medical image segmentation research is the effective selection of training subsets to annotate from a complete set of unlabelled data. Many studies select their training sets at random, which may lead to suboptimal model performance, especially in the minimal supervision setting where each training image has a profound effect on performance outcomes. This work aims to address this issue. We use prototypical contrasting learning and clustering to extract representative and diverse samples for annotation. We improve upon prior works with a bespoke cluster-based image selection process. Additionally, we introduce the concept of unsupervised balanced batch dataloading to medical image segmentation, which aims to improve model learning with minimally annotated data. We evaluated our method on a public skin lesion dataset (ISIC 2018) and compared it to another state-of-the-art data sampling method. Our method achieved superior performance in a low annotation budget scenario.</li>
</ul>

<h3>Title: Scoring, Remember, and Reference: Catching Camouflaged Objects in Videos</h3>
<ul>
<li><strong>Authors: </strong>Yuang Feng, Shuyong Gao, Fuzhen Yan, Yicheng Song, Lingyi Hong, Junjie Hu, Wenqiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17050">https://arxiv.org/abs/2503.17050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17050">https://arxiv.org/pdf/2503.17050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17050]] Scoring, Remember, and Reference: Catching Camouflaged Objects in Videos(https://arxiv.org/abs/2503.17050)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Video Camouflaged Object Detection (VCOD) aims to segment objects whose appearances closely resemble their surroundings, posing a challenging and emerging task. Existing vision models often struggle in such scenarios due to the indistinguishable appearance of camouflaged objects and the insufficient exploitation of dynamic information in videos. To address these challenges, we propose an end-to-end VCOD framework inspired by human memory-recognition, which leverages historical video information by integrating memory reference frames for camouflaged sequence processing. Specifically, we design a dual-purpose decoder that simultaneously generates predicted masks and scores, enabling reference frame selection based on scores while introducing auxiliary supervision to enhance feature this http URL, this study introduces a novel reference-guided multilevel asymmetric attention mechanism, effectively integrating long-term reference information with short-term motion cues for comprehensive feature extraction. By combining these modules, we develop the Scoring, Remember, and Reference (SRR) framework, which efficiently extracts information to locate targets and employs memory guidance to improve subsequent processing. With its optimized module design and effective utilization of video data, our model achieves significant performance improvements, surpassing existing approaches by 10% on benchmark datasets while requiring fewer parameters (54M) and only a single pass through the video. The code will be made publicly available.</li>
</ul>

<h3>Title: ATHENA: An In-vehicle CAN Intrusion Detection Framework Based on Physical Characteristics of Vehicle Systems</h3>
<ul>
<li><strong>Authors: </strong>Kai Wang, Zhen Sun, Bailing Wang, Qilin Fan, Ming Li, Hongke Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17067">https://arxiv.org/abs/2503.17067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17067">https://arxiv.org/pdf/2503.17067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17067]] ATHENA: An In-vehicle CAN Intrusion Detection Framework Based on Physical Characteristics of Vehicle Systems(https://arxiv.org/abs/2503.17067)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal</a></li>
<li><strong>Abstract: </strong>With the growing interconnection between In-Vehicle Networks (IVNs) and external environments, intelligent vehicles are increasingly vulnerable to sophisticated external network attacks. This paper proposes ATHENA, the first IVN intrusion detection framework that adopts a vehicle-cloud integrated architecture to achieve better security performance for the resource-constrained vehicular environment. Specifically, in the cloud with sufficient resources, ATHENA uses the clustering method of multi-distribution mixture model combined with deep data mining technology to generate the raw Payload Rule Bank of IVN CAN messages, and then improves the rule quality with the help of exploitation on the first-principled physical knowledge of the vehicle system, after which the payload rules are periodically sent to the vehicle terminal. At the vehicle terminal, a simple LSTM component is used to generate the Time Rule Bank representing the long-term time series dependencies and the periodic characteristics of CAN messages, but not for any detection tasks as in traditional usage scenarios, where only the generated time rules are the candidates for further IVN intrusion detection tasks. Based on both the payload and time rules generated from cloud and vehicle terminal, ATHENA can achieve efficient intrusion detection capability by simple rule-base matching operations, rather than using complex black-box reasoning of resource-intensive neural network models, which is in fact only used for rule logic generation phase instead of the actual intrusion detection phase in our framework. Comparative experimental results on the ROAD dataset, which is current the most outstanding real-world in-vehicle CAN dataset covering new instances of sophisticated and stealthy masquerade attacks, demonstrate ATHENA significantly outperforms the state-of-the-art IVN intrusion detection methods in detecting complex attacks.</li>
</ul>

<h3>Title: PVChat: Personalized Video Chat with One-Shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Yufei Shi, Weilong Yan, Gang Xu, Yumeng Li, Yuchen Li, Zhenxi Li, Fei Richard Yu, Ming Li, Si Yong Yeo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17069">https://arxiv.org/abs/2503.17069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17069">https://arxiv.org/pdf/2503.17069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17069]] PVChat: Personalized Video Chat with One-Shot Learning(https://arxiv.org/abs/2503.17069)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video large language models (ViLLMs) excel in general video understanding, e.g., recognizing activities like talking and eating, but struggle with identity-aware comprehension, such as "Wilson is receiving chemotherapy" or "Tom is discussing with Sarah", limiting their applicability in smart healthcare and smart home environments. To address this limitation, we propose a one-shot learning framework PVChat, the first personalized ViLLM that enables subject-aware question answering (QA) from a single video for each subject. Our approach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically augmented video-QA dataset, leveraging a progressive image-to-video learning strategy. Specifically, we introduce an automated augmentation pipeline that synthesizes identity-preserving positive samples and retrieves hard negatives from existing video corpora, generating a diverse training dataset with four QA types: existence, appearance, action, and location inquiries. To enhance subject-specific learning, we propose a ReLU Routing MoH attention mechanism, alongside two novel objectives: (1) Smooth Proximity Regularization for progressive learning through exponential distance scaling and (2) Head Activation Enhancement for balanced attention routing. Finally, we adopt a two-stage training strategy, transitioning from image pre-training to video fine-tuning, enabling a gradual learning process from static attributes to dynamic representations. We evaluate PVChat on diverse datasets covering medical scenarios, TV series, anime, and real-world footage, demonstrating its superiority in personalized feature understanding after learning from a single video, compared to state-of-the-art ViLLMs.</li>
</ul>

<h3>Title: A Thorough Assessment of the Non-IID Data Impact in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Daniel M. Jimenez-Gutierrez, Mehrdad Hassanzadeh, Aris Anagnostopoulos, Ioannis Chatzigiannakis, Andrea Vitaletti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17070">https://arxiv.org/abs/2503.17070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17070">https://arxiv.org/pdf/2503.17070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17070]] A Thorough Assessment of the Non-IID Data Impact in Federated Learning(https://arxiv.org/abs/2503.17070)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) allows collaborative machine learning (ML) model training among decentralized clients' information, ensuring data privacy. The decentralized nature of FL deals with non-independent and identically distributed (non-IID) data. This open problem has notable consequences, such as decreased model performance and more significant convergence times. Despite its importance, experimental studies systematically addressing all types of data heterogeneity (a.k.a. non-IIDness) remain scarce. We aim to fill this gap by assessing and quantifying the non-IID effect through a thorough empirical analysis. We use the Hellinger Distance (HD) to measure differences in distribution among clients. Our study benchmarks four state-of-the-art strategies for handling non-IID data, including label, feature, quantity, and spatiotemporal skewness, under realistic and controlled conditions. This is the first comprehensive analysis of the spatiotemporal skew effect in FL. Our findings highlight the significant impact of label and spatiotemporal skew non-IID types on FL model performance, with notable performance drops occurring at specific HD thresholds. Additionally, the FL performance is heavily affected mainly when the non-IIDness is extreme. Thus, we provide recommendations for FL research to tackle data heterogeneity effectively. Our work represents the most extensive examination of non-IIDness in FL, offering a robust foundation for future research.</li>
</ul>

<h3>Title: Superpowering Open-Vocabulary Object Detectors for X-ray Vision</h3>
<ul>
<li><strong>Authors: </strong>Pablo Garcia-Fernandez, Lorenzo Vaquero, Mingxuan Liu, Feng Xue, Daniel Cores, Nicu Sebe, Manuel Mucientes, Elisa Ricci</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17071">https://arxiv.org/abs/2503.17071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17071">https://arxiv.org/pdf/2503.17071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17071]] Superpowering Open-Vocabulary Object Detectors for X-ray Vision(https://arxiv.org/abs/2503.17071)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Open-vocabulary object detection (OvOD) is set to revolutionize security screening by enabling systems to recognize any item in X-ray scans. However, developing effective OvOD models for X-ray imaging presents unique challenges due to data scarcity and the modality gap that prevents direct adoption of RGB-based solutions. To overcome these limitations, we propose RAXO, a training-free framework that repurposes off-the-shelf RGB OvOD detectors for robust X-ray detection. RAXO builds high-quality X-ray class descriptors using a dual-source retrieval strategy. It gathers relevant RGB images from the web and enriches them via a novel X-ray material transfer mechanism, eliminating the need for labeled databases. These visual descriptors replace text-based classification in OvOD, leveraging intra-modal feature distances for robust detection. Extensive experiments demonstrate that RAXO consistently improves OvOD performance, providing an average mAP increase of up to 17.0 points over base detectors. To further support research in this emerging field, we also introduce DET-COMPASS, a new benchmark featuring bounding box annotations for over 300 object categories, enabling large-scale evaluation of OvOD in X-ray. Code and dataset available at: this https URL.</li>
</ul>

<h3>Title: A Study into Investigating Temporal Robustness of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jonas Wallat, Abdelrahman Abdallah, Adam Jatowt, Avishek Anand</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17073">https://arxiv.org/abs/2503.17073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17073">https://arxiv.org/pdf/2503.17073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17073]] A Study into Investigating Temporal Robustness of LLMs(https://arxiv.org/abs/2503.17073)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) encapsulate a surprising amount of factual world knowledge. However, their performance on temporal questions and historical knowledge is limited because they often cannot understand temporal scope and orientation or neglect the temporal aspect altogether. In this study, we aim to measure precisely how robust LLMs are for question answering based on their ability to process temporal information and perform tasks requiring temporal reasoning and temporal factual knowledge. Specifically, we design eight time-sensitive robustness tests for factual information to check the sensitivity of six popular LLMs in the zero-shot setting. Overall, we find LLMs lacking temporal robustness, especially to temporal reformulations and the use of different granularities of temporal references. We show how a selection of these eight tests can be used automatically to judge a model's temporal robustness for user questions on the fly. Finally, we apply the findings of this study to improve the temporal QA performance by up to 55 percent.</li>
</ul>

<h3>Title: Zero-Shot Styled Text Image Generation, but Make It Autoregressive</h3>
<ul>
<li><strong>Authors: </strong>Vittorio Pippi, Fabio Quattrini, Silvia Cascianelli, Alessio Tonioni, Rita Cucchiara</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17074">https://arxiv.org/abs/2503.17074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17074">https://arxiv.org/pdf/2503.17074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17074]] Zero-Shot Styled Text Image Generation, but Make It Autoregressive(https://arxiv.org/abs/2503.17074)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Styled Handwritten Text Generation (HTG) has recently received attention from the computer vision and document analysis communities, which have developed several solutions, either GAN- or diffusion-based, that achieved promising results. Nonetheless, these strategies fail to generalize to novel styles and have technical constraints, particularly in terms of maximum output length and training efficiency. To overcome these limitations, in this work, we propose a novel framework for text image generation, dubbed Emuru. Our approach leverages a powerful text image representation model (a variational autoencoder) combined with an autoregressive Transformer. Our approach enables the generation of styled text images conditioned on textual content and style examples, such as specific fonts or handwriting styles. We train our model solely on a diverse, synthetic dataset of English text rendered in over 100,000 typewritten and calligraphy fonts, which gives it the capability to reproduce unseen styles (both fonts and users' handwriting) in zero-shot. To the best of our knowledge, Emuru is the first autoregressive model for HTG, and the first designed specifically for generalization to novel styles. Moreover, our model generates images without background artifacts, which are easier to use for downstream applications. Extensive evaluation on both typewritten and handwritten, any-length text image generation scenarios demonstrates the effectiveness of our approach.</li>
</ul>

<h3>Title: Halton Scheduler For Masked Generative Image Transformer</h3>
<ul>
<li><strong>Authors: </strong>Victor Besnier, Mickael Chen, David Hurych, Eduardo Valle, Matthieu Cord</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17076">https://arxiv.org/abs/2503.17076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17076">https://arxiv.org/pdf/2503.17076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17076]] Halton Scheduler For Masked Generative Image Transformer(https://arxiv.org/abs/2503.17076)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Masked Generative Image Transformers (MaskGIT) have emerged as a scalable and efficient image generation framework, able to deliver high-quality visuals with low inference costs. However, MaskGIT's token unmasking scheduler, an essential component of the framework, has not received the attention it deserves. We analyze the sampling objective in MaskGIT, based on the mutual information between tokens, and elucidate its shortcomings. We then propose a new sampling strategy based on our Halton scheduler instead of the original Confidence scheduler. More precisely, our method selects the token's position according to a quasi-random, low-discrepancy Halton sequence. Intuitively, that method spreads the tokens spatially, progressively covering the image uniformly at each step. Our analysis shows that it allows reducing non-recoverable sampling errors, leading to simpler hyper-parameters tuning and better quality images. Our scheduler does not require retraining or noise injection and may serve as a simple drop-in replacement for the original sampling strategy. Evaluation of both class-to-image synthesis on ImageNet and text-to-image generation on the COCO dataset demonstrates that the Halton scheduler outperforms the Confidence scheduler quantitatively by reducing the FID and qualitatively by generating more diverse and more detailed images. Our code is at this https URL.</li>
</ul>

<h3>Title: Seeing What Matters: Empowering CLIP with Patch Generation-to-Selection</h3>
<ul>
<li><strong>Authors: </strong>Gensheng Pei, Tao Chen, Yujia Wang, Xinhao Cai, Xiangbo Shu, Tianfei Zhou, Yazhou Yao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17080">https://arxiv.org/abs/2503.17080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17080">https://arxiv.org/pdf/2503.17080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17080]] Seeing What Matters: Empowering CLIP with Patch Generation-to-Selection(https://arxiv.org/abs/2503.17080)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The CLIP model has demonstrated significant advancements in aligning visual and language modalities through large-scale pre-training on image-text pairs, enabling strong zero-shot classification and retrieval capabilities on various domains. However, CLIP's training remains computationally intensive, with high demands on both data processing and memory. To address these challenges, recent masking strategies have emerged, focusing on the selective removal of image patches to improve training efficiency. Although effective, these methods often compromise key semantic information, resulting in suboptimal alignment between visual features and text descriptions. In this work, we present a concise yet effective approach called Patch Generation-to-Selection to enhance CLIP's training efficiency while preserving critical semantic content. Our method introduces a gradual masking process in which a small set of candidate patches is first pre-selected as potential mask regions. Then, we apply Sobel edge detection across the entire image to generate an edge mask that prioritizes the retention of the primary object areas. Finally, similarity scores between the candidate mask patches and their neighboring patches are computed, with optimal transport normalization refining the selection process to ensure a balanced similarity matrix. Our approach, CLIP-PGS, sets new state-of-the-art results in zero-shot classification and retrieval tasks, achieving superior performance in robustness evaluation and language compositionality benchmarks.</li>
</ul>

<h3>Title: Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics</h3>
<ul>
<li><strong>Authors: </strong>J. M. Diederik Kruijssen, Nicholas Emmons (Allora Foundation)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17085">https://arxiv.org/abs/2503.17085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17085">https://arxiv.org/pdf/2503.17085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17085]] Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics(https://arxiv.org/abs/2503.17085)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) systems powered by large language models have become increasingly prevalent in modern society, enabling a wide range of applications through natural language interaction. As AI agents proliferate in our daily lives, their generic and uniform expressiveness presents a significant limitation to their appeal and adoption. Personality expression represents a key prerequisite for creating more human-like and distinctive AI systems. We show that AI models can express deterministic and consistent personalities when instructed using established psychological frameworks, with varying degrees of accuracy depending on model capabilities. We find that more advanced models like GPT-4o and o1 demonstrate the highest accuracy in expressing specified personalities across both Big Five and Myers-Briggs assessments, and further analysis suggests that personality expression emerges from a combination of intelligence and reasoning capabilities. Our results reveal that personality expression operates through holistic reasoning rather than question-by-question optimization, with response-scale metrics showing higher variance than test-scale metrics. Furthermore, we find that model fine-tuning affects communication style independently of personality expression accuracy. These findings establish a foundation for creating AI agents with diverse and consistent personalities, which could significantly enhance human-AI interaction across applications from education to healthcare, while additionally enabling a broader range of more unique AI agents. The ability to quantitatively assess and implement personality expression in AI systems opens new avenues for research into more relatable, trustworthy, and ethically designed AI.</li>
</ul>

<h3>Title: Multi-modal Multi-platform Person Re-Identification: Benchmark and Method</h3>
<ul>
<li><strong>Authors: </strong>Ruiyang Ha, Songyi Jiang, Bin Li, Bikang Pan, Yihang Zhu, Junjie Zhang, Xiatian Zhu, Shaogang Gong, Jingya Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17096">https://arxiv.org/abs/2503.17096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17096">https://arxiv.org/pdf/2503.17096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17096]] Multi-modal Multi-platform Person Re-Identification: Benchmark and Method(https://arxiv.org/abs/2503.17096)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Conventional person re-identification (ReID) research is often limited to single-modality sensor data from static cameras, which fails to address the complexities of real-world scenarios where multi-modal signals are increasingly prevalent. For instance, consider an urban ReID system integrating stationary RGB cameras, nighttime infrared sensors, and UAVs equipped with dynamic tracking capabilities. Such systems face significant challenges due to variations in camera perspectives, lighting conditions, and sensor modalities, hindering effective person ReID. To address these challenges, we introduce the MP-ReID benchmark, a novel dataset designed specifically for multi-modality and multi-platform ReID. This benchmark uniquely compiles data from 1,930 identities across diverse modalities, including RGB, infrared, and thermal imaging, captured by both UAVs and ground-based cameras in indoor and outdoor environments. Building on this benchmark, we introduce Uni-Prompt ReID, a framework with specific-designed prompts, tailored for cross-modality and cross-platform scenarios. Our method consistently outperforms state-of-the-art approaches, establishing a robust foundation for future research in complex and dynamic ReID environments. Our dataset are available at:this https URL.</li>
</ul>

<h3>Title: R2LDM: An Efficient 4D Radar Super-Resolution Framework Leveraging Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Boyuan Zheng, Shouyi Lu, Renbo Huang, Minqing Huang, Fan Lu, Wei Tian, Guirong Zhuo, Lu Xiong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17097">https://arxiv.org/abs/2503.17097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17097">https://arxiv.org/pdf/2503.17097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17097]] R2LDM: An Efficient 4D Radar Super-Resolution Framework Leveraging Diffusion Model(https://arxiv.org/abs/2503.17097)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce R2LDM, an innovative approach for generating dense and accurate 4D radar point clouds, guided by corresponding LiDAR point clouds. Instead of utilizing range images or bird's eye view (BEV) images, we represent both LiDAR and 4D radar point clouds using voxel features, which more effectively capture 3D shape information. Subsequently, we propose the Latent Voxel Diffusion Model (LVDM), which performs the diffusion process in the latent space. Additionally, a novel Latent Point Cloud Reconstruction (LPCR) module is utilized to reconstruct point clouds from high-dimensional latent voxel features. As a result, R2LDM effectively generates LiDAR-like point clouds from paired raw radar data. We evaluate our approach on two different datasets, and the experimental results demonstrate that our model achieves 6- to 10-fold densification of radar point clouds, outperforming state-of-the-art baselines in 4D radar point cloud super-resolution. Furthermore, the enhanced radar point clouds generated by our method significantly improve downstream tasks, achieving up to 31.7% improvement in point cloud registration recall rate and 24.9% improvement in object detection accuracy.</li>
</ul>

<h3>Title: Large Language Model Compression via the Nested Activation-Aware Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Jun Lu, Tianyi Xu, Bill Ding, David Li, Yu Kang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17101">https://arxiv.org/abs/2503.17101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17101">https://arxiv.org/pdf/2503.17101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17101]] Large Language Model Compression via the Nested Activation-Aware Decomposition(https://arxiv.org/abs/2503.17101)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we tackle the critical challenge of compressing large language models (LLMs) to facilitate their practical deployment and broader adoption. We introduce a novel post-training compression paradigm that focuses on low-rank decomposition of LLM weights. Our analysis identifies two main challenges in this task: the variability in LLM activation distributions and handling unseen activations from different datasets and models. To address these challenges, we propose a nested activation-aware framework (NSVD) for LLMs, a training-free approach designed to enhance the accuracy of low-rank decompositions by managing activation outliers through transforming the weight matrix based on activation distribution and the original weight matrix. This method allows for the absorption of outliers into the transformed weight matrix, improving decomposition accuracy. Our comprehensive evaluation across eight datasets and six models from three distinct LLM families demonstrates the superiority of NSVD over current state-of-the-art methods, especially at medium to large compression ratios or in multilingual and multitask settings.</li>
</ul>

<h3>Title: Beyond Accuracy: What Matters in Designing Well-Behaved Models?</h3>
<ul>
<li><strong>Authors: </strong>Robin Hesse, DoÄukan BaÄcÄ±, Bernt Schiele, Simone Schaub-Meyer, Stefan Roth</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17110">https://arxiv.org/abs/2503.17110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17110">https://arxiv.org/pdf/2503.17110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17110]] Beyond Accuracy: What Matters in Designing Well-Behaved Models?(https://arxiv.org/abs/2503.17110)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Deep learning has become an essential part of computer vision, with deep neural networks (DNNs) excelling in predictive performance. However, they often fall short in other critical quality dimensions, such as robustness, calibration, or fairness. While existing studies have focused on a subset of these quality dimensions, none have explored a more general form of "well-behavedness" of DNNs. With this work, we address this gap by simultaneously studying nine different quality dimensions for image classification. Through a large-scale study, we provide a bird's-eye view by analyzing 326 backbone models and how different training paradigms and model architectures affect the quality dimensions. We reveal various new insights such that (i) vision-language models exhibit high fairness on ImageNet-1k classification and strong robustness against domain changes; (ii) self-supervised learning is an effective training paradigm to improve almost all considered quality dimensions; and (iii) the training dataset size is a major driver for most of the quality dimensions. We conclude our study by introducing the QUBA score (Quality Understanding Beyond Accuracy), a novel metric that ranks models across multiple dimensions of quality, enabling tailored recommendations based on specific user needs.</li>
</ul>

<h3>Title: Modifying Large Language Model Post-Training for Diverse Creative Writing</h3>
<ul>
<li><strong>Authors: </strong>John Joon Young Chung, Vishakh Padmakumar, Melissa Roemmele, Yuqian Sun, Max Kreminski</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17126">https://arxiv.org/abs/2503.17126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17126">https://arxiv.org/pdf/2503.17126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17126]] Modifying Large Language Model Post-Training for Diverse Creative Writing(https://arxiv.org/abs/2503.17126)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As creative writing tasks do not have singular correct answers, large language models (LLMs) trained to perform these tasks should be able to generate diverse valid outputs. However, LLM post-training often focuses on improving generation quality but neglects to facilitate output diversity. Hence, in creative writing generation, we investigate post-training approaches to promote both output diversity and quality. Our core idea is to include deviation -- the degree of difference between a training sample and all other samples with the same prompt -- in the training objective to facilitate learning from rare high-quality instances. By adopting our approach to direct preference optimization (DPO) and odds ratio preference optimization (ORPO), we demonstrate that we can promote the output diversity of trained models while minimally decreasing quality. Our best model with 8B parameters could achieve on-par diversity as a human-created dataset while having output quality similar to the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We further validate our approaches with a human evaluation, an ablation, and a comparison to an existing diversification approach, DivPO.</li>
</ul>

<h3>Title: Temporal-Guided Spiking Neural Networks for Event-Based Human Action Recognition</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Yang, Shilin Lu, Shizheng Wang, Meng Hwa Er, Zengwei Zheng, Alex C. Kot</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17132">https://arxiv.org/abs/2503.17132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17132">https://arxiv.org/pdf/2503.17132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17132]] Temporal-Guided Spiking Neural Networks for Event-Based Human Action Recognition(https://arxiv.org/abs/2503.17132)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>This paper explores the promising interplay between spiking neural networks (SNNs) and event-based cameras for privacy-preserving human action recognition (HAR). The unique feature of event cameras in capturing only the outlines of motion, combined with SNNs' proficiency in processing spatiotemporal data through spikes, establishes a highly synergistic compatibility for event-based HAR. Previous studies, however, have been limited by SNNs' ability to process long-term temporal information, essential for precise HAR. In this paper, we introduce two novel frameworks to address this: temporal segment-based SNN (\textit{TS-SNN}) and 3D convolutional SNN (\textit{3D-SNN}). The \textit{TS-SNN} extracts long-term temporal information by dividing actions into shorter segments, while the \textit{3D-SNN} replaces 2D spatial elements with 3D components to facilitate the transmission of temporal information. To promote further research in event-based HAR, we create a dataset, \textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V event camera $(1280 \times 800)$, comprising 7 distinct actions. Extensive experimental results show that our proposed frameworks surpass state-of-the-art SNN methods on our newly collected dataset and three other neuromorphic datasets, showcasing their effectiveness in handling long-range temporal information for event-based HAR.</li>
</ul>

<h3>Title: Semigroup-homomorphic Signature</h3>
<ul>
<li><strong>Authors: </strong>Heng Guo, Kun Tian, Fengxia Liu, Zhiyong Zheng</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17137">https://arxiv.org/abs/2503.17137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17137">https://arxiv.org/pdf/2503.17137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17137]] Semigroup-homomorphic Signature(https://arxiv.org/abs/2503.17137)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy</a></li>
<li><strong>Abstract: </strong>In 2002, Johnson et al. posed an open problem at the Cryptographers' Track of the RSA Conference: how to construct a secure homomorphic signature on a semigroup, rather than on a group. In this paper, we introduce, for the first time, a semigroup-homomorphic signature scheme. Under certain conditions, we prove that the security of this scheme is based on the hardness of the Short Integer Solution (SIS) problem and is tightly secure. Furthermore, we extend it to a linear semigroup-homomorphic signature scheme over lattices, and this scheme can also ensure privacy.</li>
</ul>

<h3>Title: Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Davide Berasi, Matteo Farina, Massimiliano Mancini, Elisa Ricci, Nicola Strisciuglio</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17142">https://arxiv.org/abs/2503.17142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17142">https://arxiv.org/pdf/2503.17142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17142]] Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models(https://arxiv.org/abs/2503.17142)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) learn a shared feature space for text and images, enabling the comparison of inputs of different modalities. While prior works demonstrated that VLMs organize natural language representations into regular structures encoding composite meanings, it remains unclear if compositional patterns also emerge in the visual embedding space. In this work, we investigate compositionality in the image domain, where the analysis of compositional properties is challenged by noise and sparsity of visual data. We address these problems and propose a framework, called Geodesically Decomposable Embeddings (GDE), that approximates image representations with geometry-aware compositional structures in the latent space. We demonstrate that visual embeddings of pre-trained VLMs exhibit a compositional arrangement, and evaluate the effectiveness of this property in the tasks of compositional classification and group robustness. GDE achieves stronger performance in compositional classification compared to its counterpart method that assumes linear geometry of the latent space. Notably, it is particularly effective for group robustness, where we achieve higher results than task-specific solutions. Our results indicate that VLMs can automatically develop a human-like form of compositional reasoning in the visual domain, making their underlying processes more interpretable. Code is available at this https URL.</li>
</ul>

<h3>Title: D2C: Unlocking the Potential of Continuous Autoregressive Image Generation with Discrete Tokens</h3>
<ul>
<li><strong>Authors: </strong>Panpan Wang, Liqiang Niu, Fandong Meng, Jinan Xu, Yufeng Chen, Jie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17155">https://arxiv.org/abs/2503.17155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17155">https://arxiv.org/pdf/2503.17155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17155]] D2C: Unlocking the Potential of Continuous Autoregressive Image Generation with Discrete Tokens(https://arxiv.org/abs/2503.17155)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In the domain of image generation, latent-based generative models occupy a dominant status; however, these models rely heavily on image tokenizer. To meet modeling requirements, autoregressive models possessing the characteristics of scalability and flexibility embrace a discrete-valued tokenizer, but face the challenge of poor image generation quality. In contrast, diffusion models take advantage of the continuous-valued tokenizer to achieve better generation quality but are subject to low efficiency and complexity. The existing hybrid models are mainly to compensate for information loss and simplify the diffusion learning process. The potential of merging discrete-valued and continuous-valued tokens in the field of image generation has not yet been explored. In this paper, we propose D2C, a novel two-stage method to enhance model generation capacity. In the first stage, the discrete-valued tokens representing coarse-grained image features are sampled by employing a small discrete-valued generator. Then in the second stage, the continuous-valued tokens representing fine-grained image features are learned conditioned on the discrete token sequence. In addition, we design two kinds of fusion modules for seamless interaction. On the ImageNet-256 benchmark, extensive experiment results validate that our model achieves superior performance compared with several continuous-valued and discrete-valued generative models on the class-conditional image generation tasks.</li>
</ul>

<h3>Title: DiTEC-WDN: A Large-Scale Dataset of Water Distribution Network Scenarios under Diverse Hydraulic Conditions</h3>
<ul>
<li><strong>Authors: </strong>Huy Truong, AndrÃ©s Tello, Alexander Lazovik, Victoria Degeler</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17167">https://arxiv.org/abs/2503.17167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17167">https://arxiv.org/pdf/2503.17167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17167]] DiTEC-WDN: A Large-Scale Dataset of Water Distribution Network Scenarios under Diverse Hydraulic Conditions(https://arxiv.org/abs/2503.17167)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Privacy restrictions hinder the sharing of real-world Water Distribution Network (WDN) models, limiting the application of emerging data-driven machine learning, which typically requires extensive observations. To address this challenge, we propose the dataset DiTEC-WDN that comprises 36,000 unique scenarios simulated over either short-term (24 hours) or long-term (1 year) periods. We constructed this dataset using an automated pipeline that optimizes crucial parameters (e.g., pressure, flow rate, and demand patterns), facilitates large-scale simulations, and records discrete, synthetic but hydraulically realistic states under standard conditions via rule validation and post-hoc analysis. With a total of 228 million generated graph-based states, DiTEC-WDN can support a variety of machine-learning tasks, including graph-level, node-level, and link-level regression, as well as time-series forecasting. This contribution, released under a public license, encourages open scientific research in the critical water sector, eliminates the risk of exposing sensitive data, and fulfills the need for a large-scale water distribution network benchmark for study comparisons and scenario analysis.</li>
</ul>

<h3>Title: Hi-ALPS -- An Experimental Robustness Quantification of Six LiDAR-based Object Detection Systems for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Alexandra Arzberger, Ramin Tavakoli Kolagari</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17168">https://arxiv.org/abs/2503.17168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17168">https://arxiv.org/pdf/2503.17168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17168]] Hi-ALPS -- An Experimental Robustness Quantification of Six LiDAR-based Object Detection Systems for Autonomous Driving(https://arxiv.org/abs/2503.17168)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Light Detection and Ranging (LiDAR) is an essential sensor technology for autonomous driving as it can capture high-resolution 3D data. As 3D object detection systems (OD) can interpret such point cloud data, they play a key role in the driving decisions of autonomous vehicles. Consequently, such 3D OD must be robust against all types of perturbations and must therefore be extensively tested. One approach is the use of adversarial examples, which are small, sometimes sophisticated perturbations in the input data that change, i.e., falsify, the prediction of the OD. These perturbations are carefully designed based on the weaknesses of the OD. The robustness of the OD cannot be quantified with adversarial examples in general, because if the OD is vulnerable to a given attack, it is unclear whether this is due to the robustness of the OD or whether the attack algorithm produces particularly strong adversarial examples. The contribution of this work is Hi-ALPS -- Hierarchical Adversarial-example-based LiDAR Perturbation Level System, where higher robustness of the OD is required to withstand the perturbations as the perturbation levels increase. In doing so, the Hi-ALPS levels successively implement a heuristic followed by established adversarial example approaches. In a series of comprehensive experiments using Hi-ALPS, we quantify the robustness of six state-of-the-art 3D OD under different types of perturbations. The results of the experiments show that none of the OD is robust against all Hi-ALPS levels; an important factor for the ranking is that human observers can still correctly recognize the perturbed objects, as the respective perturbations are small. To increase the robustness of the OD, we discuss the applicability of state-of-the-art countermeasures. In addition, we derive further suggestions for countermeasures based on our experimental results.</li>
</ul>

<h3>Title: Principal Eigenvalue Regularization for Improved Worst-Class Certified Robustness of Smoothed Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Gaojie Jin, Tianjin Huang, Ronghui Mu, Xiaowei Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17172">https://arxiv.org/abs/2503.17172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17172">https://arxiv.org/pdf/2503.17172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17172]] Principal Eigenvalue Regularization for Improved Worst-Class Certified Robustness of Smoothed Classifiers(https://arxiv.org/abs/2503.17172)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Recent studies have identified a critical challenge in deep neural networks (DNNs) known as ``robust fairness", where models exhibit significant disparities in robust accuracy across different classes. While prior work has attempted to address this issue in adversarial robustness, the study of worst-class certified robustness for smoothed classifiers remains unexplored. Our work bridges this gap by developing a PAC-Bayesian bound for the worst-class error of smoothed classifiers. Through theoretical analysis, we demonstrate that the largest eigenvalue of the smoothed confusion matrix fundamentally influences the worst-class error of smoothed classifiers. Based on this insight, we introduce a regularization method that optimizes the largest eigenvalue of smoothed confusion matrix to enhance worst-class accuracy of the smoothed classifier and further improve its worst-class certified robustness. We provide extensive experimental validation across multiple datasets and model architectures to demonstrate the effectiveness of our approach.</li>
</ul>

<h3>Title: Robustness of deep learning classification to adversarial input on GPUs: asynchronous parallel accumulation is a source of vulnerability</h3>
<ul>
<li><strong>Authors: </strong>Sanjif Shanmugavelu, Mathieu Taillefumier, Christopher Culver, Vijay Ganesh, Oscar Hernandez, Ada Sedova</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17173">https://arxiv.org/abs/2503.17173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17173">https://arxiv.org/pdf/2503.17173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17173]] Robustness of deep learning classification to adversarial input on GPUs: asynchronous parallel accumulation is a source of vulnerability(https://arxiv.org/abs/2503.17173)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>The ability of machine learning (ML) classification models to resist small, targeted input perturbations - known as adversarial attacks - is a key measure of their safety and reliability. We show that floating-point non associativity (FPNA) coupled with asynchronous parallel programming on GPUs is sufficient to result in misclassification, without any perturbation to the input. Additionally, we show this misclassification is particularly significant for inputs close to the decision boundary and that standard adversarial robustness results may be overestimated up to 4.6% when not considering machine-level details. We first study a linear classifier, before focusing on standard Graph Neural Network (GNN) architectures and datasets. We present a novel black-box attack using Bayesian optimization to determine external workloads that bias the output of reductions on GPUs and reliably lead to misclassification. Motivated by these results, we present a new learnable permutation (LP) gradient-based approach, to learn floating point operation orderings that lead to misclassifications, making the assumption that any reduction or permutation ordering is possible. This LP approach provides a worst-case estimate in a computationally efficient manner, avoiding the need to run identical experiments tens of thousands of times over a potentially large set of possible GPU states or architectures. Finally, we investigate parallel reduction ordering across different GPU architectures for a reduction under three conditions: (1) executing external background workloads, (2) utilizing multi-GPU virtualization, and (3) applying power capping. Our results demonstrate that parallel reduction ordering varies significantly across architectures under the first two conditions. The results and methods developed here can help to include machine-level considerations into adversarial robustness assessments.</li>
</ul>

<h3>Title: Which2comm: An Efficient Collaborative Perception Framework for 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Duanrui Yu, Jing You, Xin Pei, Anqi Qu, Dingyu Wang, Shaocheng Jia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17175">https://arxiv.org/abs/2503.17175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17175">https://arxiv.org/pdf/2503.17175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17175]] Which2comm: An Efficient Collaborative Perception Framework for 3D Object Detection(https://arxiv.org/abs/2503.17175)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Collaborative perception allows real-time inter-agent information exchange and thus offers invaluable opportunities to enhance the perception capabilities of individual agents. However, limited communication bandwidth in practical scenarios restricts the inter-agent data transmission volume, consequently resulting in performance declines in collaborative perception systems. This implies a trade-off between perception performance and communication cost. To address this issue, we propose Which2comm, a novel multi-agent 3D object detection framework leveraging object-level sparse features. By integrating semantic information of objects into 3D object detection boxes, we introduce semantic detection boxes (SemDBs). Innovatively transmitting these information-rich object-level sparse features among agents not only significantly reduces the demanding communication volume, but also improves 3D object detection performance. Specifically, a fully sparse network is constructed to extract SemDBs from individual agents; a temporal fusion approach with a relative temporal encoding mechanism is utilized to obtain the comprehensive spatiotemporal features. Extensive experiments on the V2XSet and OPV2V datasets demonstrate that Which2comm consistently outperforms other state-of-the-art methods on both perception performance and communication cost, exhibiting better robustness to real-world latency. These results present that for multi-agent collaborative 3D object detection, transmitting only object-level sparse features is sufficient to achieve high-precision and robust performance.</li>
</ul>

<h3>Title: D2Fusion: Dual-domain Fusion with Feature Superposition for Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Xueqi Qiu, Xingyu Miao, Fan Wan, Haoran Duan, Tejal Shah, Varun Ojhab, Yang Longa, Rajiv Ranjan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17184">https://arxiv.org/abs/2503.17184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17184">https://arxiv.org/pdf/2503.17184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17184]] D2Fusion: Dual-domain Fusion with Feature Superposition for Deepfake Detection(https://arxiv.org/abs/2503.17184)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Deepfake detection is crucial for curbing the harm it causes to society. However, current Deepfake detection methods fail to thoroughly explore artifact information across different domains due to insufficient intrinsic interactions. These interactions refer to the fusion and coordination after feature extraction processes across different domains, which are crucial for recognizing complex forgery clues. Focusing on more generalized Deepfake detection, in this work, we introduce a novel bi-directional attention module to capture the local positional information of artifact clues from the spatial domain. This enables accurate artifact localization, thus addressing the coarse processing with artifact features. To further address the limitation that the proposed bi-directional attention module may not well capture global subtle forgery information in the artifact feature (e.g., textures or edges), we employ a fine-grained frequency attention module in the frequency domain. By doing so, we can obtain high-frequency information in the fine-grained features, which contains the global and subtle forgery information. Although these features from the diverse domains can be effectively and independently improved, fusing them directly does not effectively improve the detection performance. Therefore, we propose a feature superposition strategy that complements information from spatial and frequency domains. This strategy turns the feature components into the form of wave-like tokens, which are updated based on their phase, such that the distinctions between authentic and artifact features can be amplified. Our method demonstrates significant improvements over state-of-the-art (SOTA) methods on five public Deepfake datasets in capturing abnormalities across different manipulated operations and real-life.</li>
</ul>

<h3>Title: MSCA-Net:Multi-Scale Context Aggregation Network for Infrared Small Target Detection</h3>
<ul>
<li><strong>Authors: </strong>Xiaojin Lu, Taoran yue, Jiaxi cai, Shibing Chu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17193">https://arxiv.org/abs/2503.17193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17193">https://arxiv.org/pdf/2503.17193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17193]] MSCA-Net:Multi-Scale Context Aggregation Network for Infrared Small Target Detection(https://arxiv.org/abs/2503.17193)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Detecting infrared small targets in complex backgrounds remains a challenging task because of the low contrast and high noise levels inherent in infrared images. These factors often lead to the loss of crucial details during feature extraction. Moreover, existing detection methods have limitations in adequately integrating global and local information, which constrains the efficiency and accuracy of infrared small target detection. To address these challenges, this paper proposes a novel network architecture named MSCA-Net, which integrates three key components: Multi-Scale Enhanced Detection Attention mechanism(MSEDA), Positional Convolutional Block Attention Module (PCBAM), and Channel Aggregation Block (CAB). Specifically, MSEDA employs a multi-scale feature fusion attention mechanism to adaptively aggregate information across different scales, enriching feature representation. PCBAM captures the correlation between global and local features through a correlation matrix-based strategy, enabling deep feature interaction. Moreover, CAB redistributes input feature channels, facilitating the efficient transmission of beneficial features and further enhancing the model detection capability in complex backgrounds. The experimental results demonstrate that MSCA-Net achieves outstanding small target detection performance in complex backgrounds. Specifically, it attains mIoU scores of 78.43\%, 94.56\%, and 67.08\% on the NUAA-SIRST, NUDT-SIRST, and IRTSD-1K datasets, respectively, underscoring its effectiveness and strong potential for real-world applications.</li>
</ul>

<h3>Title: TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided Subspace Partitioning</h3>
<ul>
<li><strong>Authors: </strong>Sheng Wang, Pengan Chen, Jingqi Zhou, Qintong Li, Jingwei Dong, Jiahui Gao, Boyang Xue, Jiyue Jiang, Lingpeng Kong, Chuan Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17195">https://arxiv.org/abs/2503.17195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17195">https://arxiv.org/pdf/2503.17195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17195]] TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided Subspace Partitioning(https://arxiv.org/abs/2503.17195)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Model customization requires high-quality and diverse datasets, but acquiring such data remains challenging and costly. Although large language models (LLMs) can synthesize training data, current approaches are constrained by limited seed data, model bias and insufficient control over the generation process, resulting in limited diversity and biased distribution with the increase of data scales. To tackle this challenge, we present TreeSynth, a tree-guided subspace-based data synthesis framework that recursively partitions the entire data space into hierar-chical subspaces, enabling comprehensive and diverse scaling of data synthesis. Briefly, given a task-specific description, we construct a data space partitioning tree by iteratively executing criteria determination and subspace coverage steps. This hierarchically divides the whole space (i.e., root node) into mutually exclusive and complementary atomic subspaces (i.e., leaf nodes). By collecting synthesized data according to the attributes of each leaf node, we obtain a diverse dataset that fully covers the data space. Empirically, our extensive experiments demonstrate that TreeSynth surpasses both human-designed datasets and the state-of-the-art data synthesis baselines, achieving maximum improvements of 45.2% in data diversity and 17.6% in downstream task performance across various models and tasks. Hopefully, TreeSynth provides a scalable solution to synthesize diverse and comprehensive datasets from scratch without human intervention.</li>
</ul>

<h3>Title: FreeUV: Ground-Truth-Free Realistic Facial UV Texture Recovery via Cross-Assembly Inference Strategy</h3>
<ul>
<li><strong>Authors: </strong>Xingchao Yang, Takafumi Taketomi, Yuki Endo, Yoshihiro Kanamori</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17197">https://arxiv.org/abs/2503.17197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17197">https://arxiv.org/pdf/2503.17197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17197]] FreeUV: Ground-Truth-Free Realistic Facial UV Texture Recovery via Cross-Assembly Inference Strategy(https://arxiv.org/abs/2503.17197)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Recovering high-quality 3D facial textures from single-view 2D images is a challenging task, especially under constraints of limited data and complex facial details such as makeup, wrinkles, and occlusions. In this paper, we introduce FreeUV, a novel ground-truth-free UV texture recovery framework that eliminates the need for annotated or synthetic UV data. FreeUV leverages pre-trained stable diffusion model alongside a Cross-Assembly inference strategy to fulfill this objective. In FreeUV, separate networks are trained independently to focus on realistic appearance and structural consistency, and these networks are combined during inference to generate coherent textures. Our approach accurately captures intricate facial features and demonstrates robust performance across diverse poses and occlusions. Extensive experiments validate FreeUV's effectiveness, with results surpassing state-of-the-art methods in both quantitative and qualitative metrics. Additionally, FreeUV enables new applications, including local editing, facial feature interpolation, and multi-view texture recovery. By reducing data requirements, FreeUV offers a scalable solution for generating high-fidelity 3D facial textures suitable for real-world scenarios.</li>
</ul>

<h3>Title: Jailbreaking the Non-Transferable Barrier via Test-Time Data Disguising</h3>
<ul>
<li><strong>Authors: </strong>Yongli Xiang, Ziming Hong, Lina Yao, Dadong Wang, Tongliang Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17198">https://arxiv.org/abs/2503.17198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17198">https://arxiv.org/pdf/2503.17198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17198]] Jailbreaking the Non-Transferable Barrier via Test-Time Data Disguising(https://arxiv.org/abs/2503.17198)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>Non-transferable learning (NTL) has been proposed to protect model intellectual property (IP) by creating a "non-transferable barrier" to restrict generalization from authorized to unauthorized domains. Recently, well-designed attack, which restores the unauthorized-domain performance by fine-tuning NTL models on few authorized samples, highlights the security risks of NTL-based applications. However, such attack requires modifying model weights, thus being invalid in the black-box scenario. This raises a critical question: can we trust the security of NTL models deployed as black-box systems? In this work, we reveal the first loophole of black-box NTL models by proposing a novel attack method (dubbed as JailNTL) to jailbreak the non-transferable barrier through test-time data disguising. The main idea of JailNTL is to disguise unauthorized data so it can be identified as authorized by the NTL model, thereby bypassing the non-transferable barrier without modifying the NTL model weights. Specifically, JailNTL encourages unauthorized-domain disguising in two levels, including: (i) data-intrinsic disguising (DID) for eliminating domain discrepancy and preserving class-related content at the input-level, and (ii) model-guided disguising (MGD) for mitigating output-level statistics difference of the NTL model. Empirically, when attacking state-of-the-art (SOTA) NTL models in the black-box scenario, JailNTL achieves an accuracy increase of up to 55.7% in the unauthorized domain by using only 1% authorized samples, largely exceeding existing SOTA white-box attacks.</li>
</ul>

<h3>Title: A Language Anchor-Guided Method for Robust Noisy Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Zilin Dai, Lehong Wang, Fangzhou Lin, Yidong Wang, Zhigang Li, Kazunori D Yamada, Ziming Zhang, Wang Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17211">https://arxiv.org/abs/2503.17211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17211">https://arxiv.org/pdf/2503.17211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17211]] A Language Anchor-Guided Method for Robust Noisy Domain Generalization(https://arxiv.org/abs/2503.17211)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Real-world machine learning applications often struggle with two major challenges: distribution shift and label noise. Models tend to overfit by focusing on redundant and uninformative features in the training data, which makes it hard for them to generalize to the target domain. Noisy data worsens this problem by causing further overfitting to the noise, meaning that existing methods often fail to tell the difference between true, invariant features and misleading, spurious ones. To tackle these issues, we introduce Anchor Alignment and Adaptive Weighting (A3W). This new algorithm uses sample reweighting guided by natural language processing (NLP) anchors to extract more representative features. In simple terms, A3W leverages semantic representations from natural language models as a source of domain-invariant prior knowledge. Additionally, it employs a weighted loss function that adjusts each sample's contribution based on its similarity to the corresponding NLP anchor. This adjustment makes the model more robust to noisy labels. Extensive experiments on standard benchmark datasets show that A3W consistently outperforms state-of-the-art domain generalization methods, offering significant improvements in both accuracy and robustness across different datasets and noise levels.</li>
</ul>

<h3>Title: PP-DocLayout: A Unified Document Layout Detection Model to Accelerate Large-Scale Data Construction</h3>
<ul>
<li><strong>Authors: </strong>Ting Sun, Cheng Cui, Yuning Du, Yi Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17213">https://arxiv.org/abs/2503.17213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17213">https://arxiv.org/pdf/2503.17213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17213]] PP-DocLayout: A Unified Document Layout Detection Model to Accelerate Large-Scale Data Construction(https://arxiv.org/abs/2503.17213)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Document layout analysis is a critical preprocessing step in document intelligence, enabling the detection and localization of structural elements such as titles, text blocks, tables, and formulas. Despite its importance, existing layout detection models face significant challenges in generalizing across diverse document types, handling complex layouts, and achieving real-time performance for large-scale data processing. To address these limitations, we present PP-DocLayout, which achieves high precision and efficiency in recognizing 23 types of layout regions across diverse document formats. To meet different needs, we offer three models of varying scales. PP-DocLayout-L is a high-precision model based on the RT-DETR-L detector, achieving 90.4% mAP@0.5 and an end-to-end inference time of 13.4 ms per page on a T4 GPU. PP-DocLayout-M is a balanced model, offering 75.2% mAP@0.5 with an inference time of 12.7 ms per page on a T4 GPU. PP-DocLayout-S is a high-efficiency model designed for resource-constrained environments and real-time applications, with an inference time of 8.1 ms per page on a T4 GPU and 14.5 ms on a CPU. This work not only advances the state of the art in document layout analysis but also provides a robust solution for constructing high-quality training data, enabling advancements in document intelligence and multimodal AI systems. Code and models are available at this https URL .</li>
</ul>

<h3>Title: Cyber Campaign Fractals -- Geometric Analysis of Hierarchical Cyber Attack Taxonomies</h3>
<ul>
<li><strong>Authors: </strong>Ronan Mouchoux, FranÃ§ois Moerman</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17219">https://arxiv.org/abs/2503.17219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17219">https://arxiv.org/pdf/2503.17219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17219]] Cyber Campaign Fractals -- Geometric Analysis of Hierarchical Cyber Attack Taxonomies(https://arxiv.org/abs/2503.17219)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel mathematical framework for analyzing cyber threat campaigns through fractal geometry. By conceptualizing hierarchical taxonomies (MITRE ATT&CK, DISARM) as snowflake-like structures with tactics, techniques, and sub-techniques forming concentric layers, we establish a rigorous method for campaign comparison using Hutchinson's Theorem and Hausdorff distance metrics. Evaluation results confirm that our fractal representation preserves hierarchical integrity while providing a dimensionality-based complexity assessment that correlates with campaign complexity. The proposed methodology bridges taxonomy-driven cyber threat analysis and computational geometry, providing analysts with both mathematical rigor and interpretable visualizations for addressing the growing complexity of adversarial operations across multiple threat domains.</li>
</ul>

<h3>Title: UniCon: Unidirectional Information Flow for Effective Control of Large-Scale Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Fanghua Yu, Jinjin Gu, Jinfan Hu, Zheyuan Li, Chao Dong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17221">https://arxiv.org/abs/2503.17221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17221">https://arxiv.org/pdf/2503.17221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17221]] UniCon: Unidirectional Information Flow for Effective Control of Large-Scale Diffusion Models(https://arxiv.org/abs/2503.17221)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce UniCon, a novel architecture designed to enhance control and efficiency in training adapters for large-scale diffusion models. Unlike existing methods that rely on bidirectional interaction between the diffusion model and control adapter, UniCon implements a unidirectional flow from the diffusion network to the adapter, allowing the adapter alone to generate the final output. UniCon reduces computational demands by eliminating the need for the diffusion model to compute and store gradients during adapter training. Our results indicate that UniCon reduces GPU memory usage by one-third and increases training speed by 2.3 times, while maintaining the same adapter parameter size. Additionally, without requiring extra computational resources, UniCon enables the training of adapters with double the parameter volume of existing ControlNets. In a series of image conditional generation tasks, UniCon has demonstrated precise responsiveness to control inputs and exceptional generation capabilities.</li>
</ul>

<h3>Title: Automating Adjudication of Cardiovascular Events Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sonish Sivarajkumar, Kimia Ameri, Chuqin Li, Yanshan Wang, Min Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17222">https://arxiv.org/abs/2503.17222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17222">https://arxiv.org/pdf/2503.17222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17222]] Automating Adjudication of Cardiovascular Events Using Large Language Models(https://arxiv.org/abs/2503.17222)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Cardiovascular events, such as heart attacks and strokes, remain a leading cause of mortality globally, necessitating meticulous monitoring and adjudication in clinical trials. This process, traditionally performed manually by clinical experts, is time-consuming, resource-intensive, and prone to inter-reviewer variability, potentially introducing bias and hindering trial progress. This study addresses these critical limitations by presenting a novel framework for automating the adjudication of cardiovascular events in clinical trials using Large Language Models (LLMs). We developed a two-stage approach: first, employing an LLM-based pipeline for event information extraction from unstructured clinical data and second, using an LLM-based adjudication process guided by a Tree of Thoughts approach and clinical endpoint committee (CEC) guidelines. Using cardiovascular event-specific clinical trial data, the framework achieved an F1-score of 0.82 for event extraction and an accuracy of 0.68 for adjudication. Furthermore, we introduce the CLEART score, a novel, automated metric specifically designed for evaluating the quality of AI-generated clinical reasoning in adjudicating cardiovascular events. This approach demonstrates significant potential for substantially reducing adjudication time and costs while maintaining high-quality, consistent, and auditable outcomes in clinical trials. The reduced variability and enhanced standardization also allow for faster identification and mitigation of risks associated with cardiovascular therapies.</li>
</ul>

<h3>Title: Neuro-Symbolic Scene Graph Conditioning for Synthetic Image Dataset Generation</h3>
<ul>
<li><strong>Authors: </strong>Giacomo Savazzi, Eugenio Lomurno, Cristian Sbrolli, Agnese Chiatti, Matteo Matteucci</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17224">https://arxiv.org/abs/2503.17224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17224">https://arxiv.org/pdf/2503.17224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17224]] Neuro-Symbolic Scene Graph Conditioning for Synthetic Image Dataset Generation(https://arxiv.org/abs/2503.17224)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative</a></li>
<li><strong>Abstract: </strong>As machine learning models increase in scale and complexity, obtaining sufficient training data has become a critical bottleneck due to acquisition costs, privacy constraints, and data scarcity in specialised domains. While synthetic data generation has emerged as a promising alternative, a notable performance gap remains compared to models trained on real data, particularly as task complexity grows. Concurrently, Neuro-Symbolic methods, which combine neural networks' learning strengths with symbolic reasoning's structured representations, have demonstrated significant potential across various cognitive tasks. This paper explores the utility of Neuro-Symbolic conditioning for synthetic image dataset generation, focusing specifically on improving the performance of Scene Graph Generation models. The research investigates whether structured symbolic representations in the form of scene graphs can enhance synthetic data quality through explicit encoding of relational constraints. The results demonstrate that Neuro-Symbolic conditioning yields significant improvements of up to +2.59% in standard Recall metrics and +2.83% in No Graph Constraint Recall metrics when used for dataset augmentation. These findings establish that merging Neuro-Symbolic and generative approaches produces synthetic data with complementary structural information that enhances model performance when combined with real data, providing a novel approach to overcome data scarcity limitations even for complex visual reasoning tasks.</li>
</ul>

<h3>Title: Leveraging Text-to-Image Generation for Handling Spurious Correlation</h3>
<ul>
<li><strong>Authors: </strong>Aryan Yazdan Parast, Basim Azam, Naveed Akhtar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17226">https://arxiv.org/abs/2503.17226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17226">https://arxiv.org/pdf/2503.17226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17226]] Leveraging Text-to-Image Generation for Handling Spurious Correlation(https://arxiv.org/abs/2503.17226)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Deep neural networks trained with Empirical Risk Minimization (ERM) perform well when both training and test data come from the same domain, but they often fail to generalize to out-of-distribution samples. In image classification, these models may rely on spurious correlations that often exist between labels and irrelevant features of images, making predictions unreliable when those features do not exist. We propose a technique to generate training samples with text-to-image (T2I) diffusion models for addressing the spurious correlation problem. First, we compute the best describing token for the visual features pertaining to the causal components of samples by a textual inversion mechanism. Then, leveraging a language segmentation method and a diffusion model, we generate new samples by combining the causal component with the elements from other classes. We also meticulously prune the generated samples based on the prediction probabilities and attribution scores of the ERM model to ensure their correct composition for our objective. Finally, we retrain the ERM model on our augmented dataset. This process reduces the model's reliance on spurious correlations by learning from carefully crafted samples for in which this correlation does not exist. Our experiments show that across different benchmarks, our technique achieves better worst-group accuracy than the existing state-of-the-art methods.</li>
</ul>

<h3>Title: FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Albert Sawczyn, Jakub Binkowski, Denis Janiak, Bogdan Gabrys, Tomasz Kajdanowicz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17229">https://arxiv.org/abs/2503.17229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17229">https://arxiv.org/pdf/2503.17229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17229]] FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs(https://arxiv.org/abs/2503.17229)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) frequently generate hallucinated content, posing significant challenges for applications where factuality is crucial. While existing hallucination detection methods typically operate at the sentence level or passage level, we propose FactSelfCheck, a novel black-box sampling-based method that enables fine-grained fact-level detection. Our approach represents text as knowledge graphs consisting of facts in the form of triples. Through analyzing factual consistency across multiple LLM responses, we compute fine-grained hallucination scores without requiring external resources or training data. Our evaluation demonstrates that FactSelfCheck performs competitively with leading sampling-based methods while providing more detailed insights. Most notably, our fact-level approach significantly improves hallucination correction, achieving a 35% increase in factual content compared to the baseline, while sentence-level SelfCheckGPT yields only an 8% improvement. The granular nature of our detection enables more precise identification and correction of hallucinated content.</li>
</ul>

<h3>Title: LoGoFair: Post-Processing for Local and Global Fairness in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Li Zhang, Chaochao Chen, Zhongxuan Han, Qiyong Zhong, Xiaolin Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17231">https://arxiv.org/abs/2503.17231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17231">https://arxiv.org/pdf/2503.17231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17231]] LoGoFair: Post-Processing for Local and Global Fairness in Federated Learning(https://arxiv.org/abs/2503.17231)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, fair</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) has garnered considerable interest for its capability to learn from decentralized data sources. Given the increasing application of FL in decision-making scenarios, addressing fairness issues across different sensitive groups (e.g., female, male) in FL is crucial. Current research often focuses on facilitating fairness at each client's data (local fairness) or within the entire dataset across all clients (global fairness). However, existing approaches that focus exclusively on either local or global fairness fail to address two key challenges: (\textbf{CH1}) Under statistical heterogeneity, global fairness does not imply local fairness, and vice versa. (\textbf{CH2}) Achieving fairness under model-agnostic setting. To tackle the aforementioned challenges, this paper proposes a novel post-processing framework for achieving both Local and Global Fairness in the FL context, namely LoGoFair. To address CH1, LoGoFair endeavors to seek the Bayes optimal classifier under local and global fairness constraints, which strikes the optimal accuracy-fairness balance in the probabilistic sense. To address CH2, LoGoFair employs a model-agnostic federated post-processing procedure that enables clients to collaboratively optimize global fairness while ensuring local fairness, thereby achieving the optimal fair classifier within FL. Experimental results on three real-world datasets further illustrate the effectiveness of the proposed LoGoFair framework.</li>
</ul>

<h3>Title: SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging</h3>
<ul>
<li><strong>Authors: </strong>Aladin Djuhera, Swanand Ravindra Kadhe, Farhan Ahmed, Syed Zawad, Holger Boche</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17239">https://arxiv.org/abs/2503.17239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17239">https://arxiv.org/pdf/2503.17239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17239]] SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging(https://arxiv.org/abs/2503.17239)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) on downstream tasks can inadvertently erode their safety alignment, even for benign fine-tuning datasets. We address this challenge by proposing SafeMERGE, a post-fine-tuning framework that preserves safety while maintaining task utility. It achieves this by selectively merging fine-tuned and safety-aligned model layers only when those deviate from safe behavior, measured by a cosine similarity criterion. We evaluate SafeMERGE against other fine-tuning- and post-fine-tuning-stage approaches for Llama-2-7B-Chat and Qwen-2-7B-Instruct models on GSM8K and PubMedQA tasks while exploring different merging strategies. We find that SafeMERGE consistently reduces harmful outputs compared to other baselines without significantly sacrificing performance, sometimes even enhancing it. The results suggest that our selective, subspace-guided, and per-layer merging method provides an effective safeguard against the inadvertent loss of safety in fine-tuned LLMs while outperforming simpler post-fine-tuning-stage defenses.</li>
</ul>

<h3>Title: Revisiting End To End Sparse Autoencoder Training -- A Short Finetune is All You Need</h3>
<ul>
<li><strong>Authors: </strong>Adam Karvonen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17272">https://arxiv.org/abs/2503.17272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17272">https://arxiv.org/pdf/2503.17272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17272]] Revisiting End To End Sparse Autoencoder Training -- A Short Finetune is All You Need(https://arxiv.org/abs/2503.17272)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Sparse autoencoders (SAEs) are widely used for interpreting language model activations. A key evaluation metric is the increase in cross-entropy loss when replacing model activations with SAE reconstructions. Typically, SAEs are trained solely on mean squared error (MSE) using precomputed, shuffled activations. Recent work introduced training SAEs directly with a combination of KL divergence and MSE ("end-to-end" SAEs), significantly improving reconstruction accuracy at the cost of substantially increased computation, which has limited their widespread adoption. We propose a brief KL+MSE fine-tuning step applied only to the final 25M training tokens (just a few percent of typical training budgets) that achieves comparable improvements, reducing the cross-entropy loss gap by 20-50%, while incurring minimal additional computational cost. We further find that multiple fine-tuning methods (KL fine-tuning, LoRA adapters, linear adapters) yield similar, non-additive cross-entropy improvements, suggesting a common, easily correctable error source in MSE-trained SAEs. We demonstrate a straightforward method for effectively transferring hyperparameters and sparsity penalties despite scale differences between KL and MSE losses. While both ReLU and TopK SAEs see significant cross-entropy loss improvements, evaluations on supervised SAEBench metrics yield mixed results, suggesting practical benefits depend on both SAE architecture and the specific downstream task. Nonetheless, our method offers meaningful improvements in interpretability applications such as circuit analysis with minor additional cost.</li>
</ul>

<h3>Title: CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement</h3>
<ul>
<li><strong>Authors: </strong>Gaifan Zhang, Yi Zhou, Danushka Bollegala</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17279">https://arxiv.org/abs/2503.17279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17279">https://arxiv.org/pdf/2503.17279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17279]] CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement(https://arxiv.org/abs/2503.17279)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The meaning conveyed by a sentence often depends on the context in which it appears. Despite the progress of sentence embedding methods, it remains unclear how to best modify a sentence embedding conditioned on its context. To address this problem, we propose Condition-Aware Sentence Embeddings (CASE), an efficient and accurate method to create an embedding for a sentence under a given condition. First, CASE creates an embedding for the condition using a Large Language Model (LLM), where the sentence influences the attention scores computed for the tokens in the condition during pooling. Next, a supervised nonlinear projection is learned to reduce the dimensionality of the LLM-based text embeddings. We show that CASE significantly outperforms previously proposed Conditional Semantic Textual Similarity (C-STS) methods on an existing standard benchmark dataset. We find that subtracting the condition embedding consistently improves the C-STS performance of LLM-based text embeddings. Moreover, we propose a supervised dimensionality reduction method that not only reduces the dimensionality of LLM-based embeddings but also significantly improves their performance.</li>
</ul>

<h3>Title: Offline Model-Based Optimization: Comprehensive Review</h3>
<ul>
<li><strong>Authors: </strong>Minsu Kim, Jiayao Gu, Ye Yuan, Taeyoung Yun, Zixuan Liu, Yoshua Bengio, Can Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17286">https://arxiv.org/abs/2503.17286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17286">https://arxiv.org/pdf/2503.17286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17286]] Offline Model-Based Optimization: Comprehensive Review(https://arxiv.org/abs/2503.17286)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Offline optimization is a fundamental challenge in science and engineering, where the goal is to optimize black-box functions using only offline datasets. This setting is particularly relevant when querying the objective function is prohibitively expensive or infeasible, with applications spanning protein engineering, material discovery, neural architecture search, and beyond. The main difficulty lies in accurately estimating the objective landscape beyond the available data, where extrapolations are fraught with significant epistemic uncertainty. This uncertainty can lead to objective hacking(reward hacking), exploiting model inaccuracies in unseen regions, or other spurious optimizations that yield misleadingly high performance estimates outside the training distribution. Recent advances in model-based optimization(MBO) have harnessed the generalization capabilities of deep neural networks to develop offline-specific surrogate and generative models. Trained with carefully designed strategies, these models are more robust against out-of-distribution issues, facilitating the discovery of improved designs. Despite its growing impact in accelerating scientific discovery, the field lacks a comprehensive review. To bridge this gap, we present the first thorough review of offline MBO. We begin by formalizing the problem for both single-objective and multi-objective settings and by reviewing recent benchmarks and evaluation metrics. We then categorize existing approaches into two key areas: surrogate modeling, which emphasizes accurate function approximation in out-of-distribution regions, and generative modeling, which explores high-dimensional design spaces to identify high-performing designs. Finally, we examine the key challenges and propose promising directions for advancement in this rapidly evolving field including safe control of superintelligent systems.</li>
</ul>

<h3>Title: FastCuRL: Curriculum Reinforcement Learning with Progressive Context Extension for Efficient Training R1-like Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Mingyang Song, Mao Zheng, Zheng Li, Wenjie Yang, Xuan Luo, Yue Pan, Feng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17287">https://arxiv.org/abs/2503.17287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17287">https://arxiv.org/pdf/2503.17287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17287]] FastCuRL: Curriculum Reinforcement Learning with Progressive Context Extension for Efficient Training R1-like Reasoning Models(https://arxiv.org/abs/2503.17287)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we propose \textbf{\textsc{FastCuRL}}, a simple yet efficient \textbf{Cu}rriculum \textbf{R}einforcement \textbf{L}earning approach with context window extending strategy to accelerate the reinforcement learning training efficiency for R1-like reasoning models while enhancing their performance in tackling complex reasoning tasks with long chain-of-thought rationales, particularly with a 1.5B parameter language model. \textbf{\textsc{FastCuRL}} consists of two main procedures: length-aware training data segmentation and context window extension training. Specifically, the former first splits the original training data into three different levels by the input prompt length, and then the latter leverages segmented training datasets with a progressively increasing context window length to train the reasoning model. Experimental results demonstrate that \textbf{\textsc{FastCuRL}}-1.5B-Preview surpasses DeepScaleR-1.5B-Preview across all five datasets (including MATH 500, AIME 2024, AMC 2023, Minerva Math, and OlympiadBench) while only utilizing 50\% of training steps. Furthermore, all training stages for FastCuRL-1.5B-Preview are completed using just a single node with 8 GPUs.</li>
</ul>

<h3>Title: UAV Resilience Against Stealthy Attacks</h3>
<ul>
<li><strong>Authors: </strong>Arthur Amorim, Max Taylor, Trevor Kann, Gary T. Leavens, William L. Harrison, Lance Joneckis</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17298">https://arxiv.org/abs/2503.17298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17298">https://arxiv.org/pdf/2503.17298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17298]] UAV Resilience Against Stealthy Attacks(https://arxiv.org/abs/2503.17298)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack, steal</a></li>
<li><strong>Abstract: </strong>Unmanned aerial vehicles (UAVs) depend on untrusted software components to automate dangerous or critical missions, making them a desirable target for attacks. Some work has been done to prevent an attacker who has either compromised a ground control station or parts of a UAV's software from sabotaging the vehicle, but not both. We present an architecture running a UAV software stack with runtime monitoring and seL4-based software isolation that prevents attackers from both exploiting software bugs and utilizing stealthy attacks. Our architecture retrofits legacy UAVs and secures the popular MAVLink protocol, making wide adoption possible.</li>
</ul>

<h3>Title: Preference-Guided Diffusion for Multi-Objective Offline Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yashas Annadani, Syrine Belakaria, Stefano Ermon, Stefan Bauer, Barbara E Engelhardt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17299">https://arxiv.org/abs/2503.17299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17299">https://arxiv.org/pdf/2503.17299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17299]] Preference-Guided Diffusion for Multi-Objective Offline Optimization(https://arxiv.org/abs/2503.17299)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Offline multi-objective optimization aims to identify Pareto-optimal solutions given a dataset of designs and their objective values. In this work, we propose a preference-guided diffusion model that generates Pareto-optimal designs by leveraging a classifier-based guidance mechanism. Our guidance classifier is a preference model trained to predict the probability that one design dominates another, directing the diffusion model toward optimal regions of the design space. Crucially, this preference model generalizes beyond the training distribution, enabling the discovery of Pareto-optimal solutions outside the observed dataset. We introduce a novel diversity-aware preference guidance, augmenting Pareto dominance preference with diversity criteria. This ensures that generated solutions are optimal and well-distributed across the objective space, a capability absent in prior generative methods for offline multi-objective optimization. We evaluate our approach on various continuous offline multi-objective optimization tasks and find that it consistently outperforms other inverse/generative approaches while remaining competitive with forward/surrogate-based optimization methods. Our results highlight the effectiveness of classifier-guided diffusion models in generating diverse and high-quality solutions that approximate the Pareto front well.</li>
</ul>

<h3>Title: Bugdar: AI-Augmented Secure Code Review for GitHub Pull Requests</h3>
<ul>
<li><strong>Authors: </strong>John Naulty, Eason Chen, Joy Wang, George Digkas, Kostas Chalkias</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.HC, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17302">https://arxiv.org/abs/2503.17302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17302">https://arxiv.org/pdf/2503.17302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17302]] Bugdar: AI-Augmented Secure Code Review for GitHub Pull Requests(https://arxiv.org/abs/2503.17302)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, large language model</a></li>
<li><strong>Abstract: </strong>As software systems grow increasingly complex, ensuring security during development poses significant challenges. Traditional manual code audits are often expensive, time-intensive, and ill-suited for fast-paced workflows, while automated tools frequently suffer from high false-positive rates, limiting their reliability. To address these issues, we introduce Bugdar, an AI-augmented code review system that integrates seamlessly into GitHub pull requests, providing near real-time, context-aware vulnerability analysis. Bugdar leverages fine-tunable Large Language Models (LLMs) and Retrieval Augmented Generation (RAGs) to deliver project-specific, actionable feedback that aligns with each codebase's unique requirements and developer practices. Supporting multiple programming languages, including Solidity, Move, Rust, and Python, Bugdar demonstrates exceptional efficiency, processing an average of 56.4 seconds per pull request or 30 lines of code per second. This is significantly faster than manual reviews, which could take hours per pull request. By facilitating a proactive approach to secure coding, Bugdar reduces the reliance on manual reviews, accelerates development cycles, and enhances the security posture of software systems without compromising productivity.</li>
</ul>

<h3>Title: Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors</h3>
<ul>
<li><strong>Authors: </strong>Wonbong Jang, Philippe Weinzaepfel, Vincent Leroy, Lourdes Agapito, Jerome Revaud</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17316">https://arxiv.org/abs/2503.17316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17316">https://arxiv.org/pdf/2503.17316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17316]] Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors(https://arxiv.org/abs/2503.17316)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present Pow3r, a novel large 3D vision regression model that is highly versatile in the input modalities it accepts. Unlike previous feed-forward models that lack any mechanism to exploit known camera or scene priors at test time, Pow3r incorporates any combination of auxiliary information such as intrinsics, relative pose, dense or sparse depth, alongside input images, within a single network. Building upon the recent DUSt3R paradigm, a transformer-based architecture that leverages powerful pre-training, our lightweight and versatile conditioning acts as additional guidance for the network to predict more accurate estimates when auxiliary information is available. During training we feed the model with random subsets of modalities at each iteration, which enables the model to operate under different levels of known priors at test time. This in turn opens up new capabilities, such as performing inference in native image resolution, or point-cloud completion. Our experiments on 3D reconstruction, depth completion, multi-view depth prediction, multi-view stereo, and multi-view pose estimation tasks yield state-of-the-art results and confirm the effectiveness of Pow3r at exploiting all available information. The project webpage is this https URL.</li>
</ul>

<h3>Title: CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Zhu, Antony Kellermann, Dylan Bowman, Philip Li, Akul Gupta, Adarsh Danda, Richard Fang, Conner Jensen, Eric Ihli, Jason Benn, Jet Geronimo, Avi Dhir, Sudhit Rao, Kaicheng Yu, Twm Stone, Daniel Kang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17332">https://arxiv.org/abs/2503.17332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17332">https://arxiv.org/pdf/2503.17332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17332]] CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities(https://arxiv.org/abs/2503.17332)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) agents are increasingly capable of autonomously conducting cyberattacks, posing significant threats to existing applications. This growing risk highlights the urgent need for a real-world benchmark to evaluate the ability of LLM agents to exploit web application vulnerabilities. However, existing benchmarks fall short as they are limited to abstracted Capture the Flag competitions or lack comprehensive coverage. Building a benchmark for real-world vulnerabilities involves both specialized expertise to reproduce exploits and a systematic approach to evaluating unpredictable threats. To address this challenge, we introduce CVE-Bench, a real-world cybersecurity benchmark based on critical-severity Common Vulnerabilities and Exposures. In CVE-Bench, we design a sandbox framework that enables LLM agents to exploit vulnerable web applications in scenarios that mimic real-world conditions, while also providing effective evaluation of their exploits. Our evaluation shows that the state-of-the-art agent framework can resolve up to 13% of vulnerabilities.</li>
</ul>

<h3>Title: Efficient Intent-Based Filtering for Multi-Party Conversations Using Knowledge Distillation from LLMs</h3>
<ul>
<li><strong>Authors: </strong>Reem Gody, Mohamed Abdelghaffar, Mohammed Jabreel, Ahmed Tawfik</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17336">https://arxiv.org/abs/2503.17336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17336">https://arxiv.org/pdf/2503.17336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17336]] Efficient Intent-Based Filtering for Multi-Party Conversations Using Knowledge Distillation from LLMs(https://arxiv.org/abs/2503.17336)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have showcased remarkable capabilities in conversational AI, enabling open-domain responses in chat-bots, as well as advanced processing of conversations like summarization, intent classification, and insights generation. However, these models are resource-intensive, demanding substantial memory and computational power. To address this, we propose a cost-effective solution that filters conversational snippets of interest for LLM processing, tailored to the target downstream application, rather than processing every snippet. In this work, we introduce an innovative approach that leverages knowledge distillation from LLMs to develop an intent-based filter for multi-party conversations, optimized for compute power constrained environments. Our method combines different strategies to create a diverse multi-party conversational dataset, that is annotated with the target intents and is then used to fine-tune the MobileBERT model for multi-label intent classification. This model achieves a balance between efficiency and performance, effectively filtering conversation snippets based on their intents. By passing only the relevant snippets to the LLM for further processing, our approach significantly reduces overall operational costs depending on the intents and the data distribution as demonstrated in our experiments.</li>
</ul>

<h3>Title: Dereflection Any Image with Diffusion Priors and Diversified Data</h3>
<ul>
<li><strong>Authors: </strong>Jichen Hu, Chen Yang, Zanwei Zhou, Jiemin Fang, Xiaokang Yang, Qi Tian, Wei Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17347">https://arxiv.org/abs/2503.17347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17347">https://arxiv.org/pdf/2503.17347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17347]] Dereflection Any Image with Diffusion Priors and Diversified Data(https://arxiv.org/abs/2503.17347)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Reflection removal of a single image remains a highly challenging task due to the complex entanglement between target scenes and unwanted reflections. Despite significant progress, existing methods are hindered by the scarcity of high-quality, diverse data and insufficient restoration priors, resulting in limited generalization across various real-world scenarios. In this paper, we propose Dereflection Any Image, a comprehensive solution with an efficient data preparation pipeline and a generalizable model for robust reflection removal. First, we introduce a dataset named Diverse Reflection Removal (DRR) created by randomly rotating reflective mediums in target scenes, enabling variation of reflection angles and intensities, and setting a new benchmark in scale, quality, and diversity. Second, we propose a diffusion-based framework with one-step diffusion for deterministic outputs and fast inference. To ensure stable learning, we design a three-stage progressive training strategy, including reflection-invariant finetuning to encourage consistent outputs across varying reflection patterns that characterize our dataset. Extensive experiments show that our method achieves SOTA performance on both common benchmarks and challenging in-the-wild images, showing superior generalization across diverse real-world scenes.</li>
</ul>

<h3>Title: Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jianing Qi, Jiawei Liu, Hao Tang, Zhigang Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17349">https://arxiv.org/abs/2503.17349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17349">https://arxiv.org/pdf/2503.17349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17349]] Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language Models(https://arxiv.org/abs/2503.17349)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) excel at identifying and describing objects but struggle with spatial reasoning such as accurately understanding the relative positions of objects. Inspired by the dual-pathway (ventral-dorsal) model of human vision, we investigate why VLMs fail spatial tasks despite strong object recognition capabilities. Our interpretability-driven analysis reveals a critical underlying cause: vision embeddings in VLMs are treated primarily as semantic ``bag-of-tokens," overshadowing subtle yet crucial positional cues due to their disproportionately large embedding norms. We validate this insight through extensive diagnostic experiments, demonstrating minimal performance impact when token orders or fine-grained spatial details are removed. Guided by these findings, we propose simple, interpretable interventions, including normalizing vision embedding norms and extracting mid-layer spatially rich features, to restore spatial awareness. Empirical results on both our synthetic data and standard benchmarks demonstrate improved spatial reasoning capabilities, highlighting the value of interpretability-informed design choices. Our study not only uncovers fundamental limitations in current VLM architectures but also provides actionable insights for enhancing structured perception of visual scenes.</li>
</ul>

<h3>Title: Decouple and Track: Benchmarking and Improving Video Diffusion Transformers for Motion Transfer</h3>
<ul>
<li><strong>Authors: </strong>Qingyu Shi, Jianzong Wu, Jinbin Bai, Jiangning Zhang, Lu Qi, Xiangtai Li, Yunhai Tong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17350">https://arxiv.org/abs/2503.17350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17350">https://arxiv.org/pdf/2503.17350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17350]] Decouple and Track: Benchmarking and Improving Video Diffusion Transformers for Motion Transfer(https://arxiv.org/abs/2503.17350)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>The motion transfer task involves transferring motion from a source video to newly generated videos, requiring the model to decouple motion from appearance. Previous diffusion-based methods primarily rely on separate spatial and temporal attention mechanisms within 3D U-Net. In contrast, state-of-the-art video Diffusion Transformers (DiT) models use 3D full attention, which does not explicitly separate temporal and spatial information. Thus, the interaction between spatial and temporal dimensions makes decoupling motion and appearance more challenging for DiT models. In this paper, we propose DeT, a method that adapts DiT models to improve motion transfer ability. Our approach introduces a simple yet effective temporal kernel to smooth DiT features along the temporal dimension, facilitating the decoupling of foreground motion from background appearance. Meanwhile, the temporal kernel effectively captures temporal variations in DiT features, which are closely related to motion. Moreover, we introduce explicit supervision along dense trajectories in the latent feature space to further enhance motion consistency. Additionally, we present MTBench, a general and challenging benchmark for motion transfer. We also introduce a hybrid motion fidelity metric that considers both the global and local motion similarity. Therefore, our work provides a more comprehensive evaluation than previous works. Extensive experiments on MTBench demonstrate that DeT achieves the best trade-off between motion fidelity and edit fidelity.</li>
</ul>

<h3>Title: Time-Series U-Net with Recurrence for Noise-Robust Imaging Photoplethysmography</h3>
<ul>
<li><strong>Authors: </strong>Vineet R. Shenoy, Shaoju Wu, Armand Comas, Tim K. Marks, Suhas Lohit, Hassan Mansour</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17351">https://arxiv.org/abs/2503.17351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17351">https://arxiv.org/pdf/2503.17351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17351]] Time-Series U-Net with Recurrence for Noise-Robust Imaging Photoplethysmography(https://arxiv.org/abs/2503.17351)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Remote estimation of vital signs enables health monitoring for situations in which contact-based devices are either not available, too intrusive, or too expensive. In this paper, we present a modular, interpretable pipeline for pulse signal estimation from video of the face that achieves state-of-the-art results on publicly available this http URL imaging photoplethysmography (iPPG) system consists of three modules: face and landmark detection, time-series extraction, and pulse signal/pulse rate estimation. Unlike many deep learning methods that make use of a single black-box model that maps directly from input video to output signal or heart rate, our modular approach enables each of the three parts of the pipeline to be interpreted individually. The pulse signal estimation module, which we call TURNIP (Time-Series U-Net with Recurrence for Noise-Robust Imaging Photoplethysmography), allows the system to faithfully reconstruct the underlying pulse signal waveform and uses it to measure heart rate and pulse rate variability metrics, even in the presence of motion. When parts of the face are occluded due to extreme head poses, our system explicitly detects such "self-occluded" regions and maintains estimation robustness despite the missing information. Our algorithm provides reliable heart rate estimates without the need for specialized sensors or contact with the skin, outperforming previous iPPG methods on both color (RGB) and near-infrared (NIR) datasets.</li>
</ul>

<h3>Title: OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement</h3>
<ul>
<li><strong>Authors: </strong>Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, Kai-Wei Chang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17352">https://arxiv.org/abs/2503.17352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17352">https://arxiv.org/pdf/2503.17352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17352]] OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement(https://arxiv.org/abs/2503.17352)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements demonstrated by DeepSeek-R1 have shown that complex reasoning abilities in large language models (LLMs), including sophisticated behaviors such as self-verification and self-correction, can be achieved by RL with verifiable rewards and significantly improves model performance on challenging tasks such as AIME. Motivated by these findings, our study investigates whether similar reasoning capabilities can be successfully integrated into large vision-language models (LVLMs) and assesses their impact on challenging multimodal reasoning tasks. We consider an approach that iteratively leverages supervised fine-tuning (SFT) on lightweight training data and Reinforcement Learning (RL) to further improve model generalization. Initially, reasoning capabilities were distilled from pure-text R1 models by generating reasoning steps using high-quality captions of the images sourced from diverse visual datasets. Subsequently, iterative RL training further enhance reasoning skills, with each iteration's RL-improved model generating refined SFT datasets for the next round. This iterative process yielded OpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on challenging benchmarks such as MathVista, MathVerse, and MathVision, demonstrating the potential of our strategy for robust vision-language reasoning. The code, model and data are held at this https URL.</li>
</ul>

<h3>Title: NdLinear Is All You Need for Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Alex Reneau, Jerry Yao-Chieh Hu, Zhongfang Zhuang, Ting-Chun Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17353">https://arxiv.org/abs/2503.17353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17353">https://arxiv.org/pdf/2503.17353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17353]] NdLinear Is All You Need for Representation Learning(https://arxiv.org/abs/2503.17353)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Many high-impact machine learning tasks involve multi-dimensional data (e.g., images, volumetric medical scans, multivariate time-series). Yet, most neural architectures flatten inputs, discarding critical cross-dimension information. We introduce NdLinear, a novel linear transformation that preserves these structures without extra overhead. By operating separately along each dimension, NdLinear captures dependencies that standard fully connected layers overlook. Extensive experiments across convolutional, recurrent, and transformer-based networks show significant improvements in representational power and parameter efficiency. Crucially, NdLinear serves as a foundational building block for large-scale foundation models by operating on any unimodal or multimodal data in its native form. This removes the need for flattening or modality-specific preprocessing. Ndlinear rethinks core architectural priorities beyond attention, enabling more expressive, context-aware models at scale. We propose NdLinear as a drop-in replacement for standard linear layers -- marking an important step toward next-generation neural architectures.</li>
</ul>

<h3>Title: Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image</h3>
<ul>
<li><strong>Authors: </strong>Jerred Chen, Ronald Clark</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17358">https://arxiv.org/abs/2503.17358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17358">https://arxiv.org/pdf/2503.17358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17358]] Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image(https://arxiv.org/abs/2503.17358)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In many robotics and VR/AR applications, fast camera motions cause a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting a dense motion flow field and a monocular depth map directly from a single motion-blurred image. We then recover the instantaneous camera velocity by solving a linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct a large-scale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP.</li>
</ul>

<h3>Title: Position: Interactive Generative Video as Next-Generation Game Engine</h3>
<ul>
<li><strong>Authors: </strong>Jiwen Yu, Yiran Qin, Haoxuan Che, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Xihui Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17359">https://arxiv.org/abs/2503.17359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17359">https://arxiv.org/pdf/2503.17359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17359]] Position: Interactive Generative Video as Next-Generation Game Engine(https://arxiv.org/abs/2503.17359)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modern game development faces significant challenges in creativity and cost due to predetermined content in traditional game engines. Recent breakthroughs in video generation models, capable of synthesizing realistic and interactive virtual environments, present an opportunity to revolutionize game creation. In this position paper, we propose Interactive Generative Video (IGV) as the foundation for Generative Game Engines (GGE), enabling unlimited novel content generation in next-generation gaming. GGE leverages IGV's unique strengths in unlimited high-quality content synthesis, physics-aware world modeling, user-controlled interactivity, long-term memory capabilities, and causal reasoning. We present a comprehensive framework detailing GGE's core modules and a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work charts a new course for game development in the AI era, envisioning a future where AI-powered generative systems fundamentally reshape how games are created and experienced.</li>
</ul>

<h3>Title: Gumbel-Softmax Flow Matching with Straight-Through Guidance for Controllable Biological Sequence Generation</h3>
<ul>
<li><strong>Authors: </strong>Sophia Tang, Yinuo Zhang, Alexander Tong, Pranam Chatterjee</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17361">https://arxiv.org/abs/2503.17361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17361">https://arxiv.org/pdf/2503.17361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17361]] Gumbel-Softmax Flow Matching with Straight-Through Guidance for Controllable Biological Sequence Generation(https://arxiv.org/abs/2503.17361)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Flow matching in the continuous simplex has emerged as a promising strategy for DNA sequence design, but struggles to scale to higher simplex dimensions required for peptide and protein generation. We introduce Gumbel-Softmax Flow and Score Matching, a generative framework on the simplex based on a novel Gumbel-Softmax interpolant with a time-dependent temperature. Using this interpolant, we introduce Gumbel-Softmax Flow Matching by deriving a parameterized velocity field that transports from smooth categorical distributions to distributions concentrated at a single vertex of the simplex. We alternatively present Gumbel-Softmax Score Matching which learns to regress the gradient of the probability density. Our framework enables high-quality, diverse generation and scales efficiently to higher-dimensional simplices. To enable training-free guidance, we propose Straight-Through Guided Flows (STGFlow), a classifier-based guidance method that leverages straight-through estimators to steer the unconditional velocity field toward optimal vertices of the simplex. STGFlow enables efficient inference-time guidance using classifiers pre-trained on clean sequences, and can be used with any discrete flow method. Together, these components form a robust framework for controllable de novo sequence generation. We demonstrate state-of-the-art performance in conditional DNA promoter design, sequence-only protein generation, and target-binding peptide design for rare disease treatment.</li>
</ul>

<h3>Title: Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural Language Self-Critique</h3>
<ul>
<li><strong>Authors: </strong>Yansi Li, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Qiuzhi Liu, Rui Wang, Zhuosheng Zhang, Zhaopeng Tu, Haitao Mi, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.17363">https://arxiv.org/abs/2503.17363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.17363">https://arxiv.org/pdf/2503.17363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.17363]] Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural Language Self-Critique(https://arxiv.org/abs/2503.17363)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Enhancing the reasoning capabilities of large language models (LLMs), particularly for complex tasks requiring multi-step logical deductions, remains a significant challenge. Traditional inference time scaling methods utilize scalar reward signals from process reward models to evaluate candidate reasoning steps, but these scalar rewards lack the nuanced qualitative information essential for understanding and justifying each step. In this paper, we propose a novel inference-time scaling approach -- stepwise natural language self-critique (PANEL), which employs self-generated natural language critiques as feedback to guide the step-level search process. By generating rich, human-readable critiques for each candidate reasoning step, PANEL retains essential qualitative information, facilitating better-informed decision-making during inference. This approach bypasses the need for task-specific verifiers and the associated training overhead, making it broadly applicable across diverse tasks. Experimental results on challenging reasoning benchmarks, including AIME and GPQA, demonstrate that PANEL significantly enhances reasoning performance, outperforming traditional scalar reward-based methods. Our code is available at this https URL to support and encourage future research in this promising field.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
