<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Blockchain based digital vaccine passport. (arXiv:2208.08760v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08760">http://arxiv.org/abs/2208.08760</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08760] Blockchain based digital vaccine passport](http://arxiv.org/abs/2208.08760)</code></li>
<li>Summary: <p>Travel has been challenging recently since different nations have implemented
varied immigration and travel policies. For the time being, immigration
officials want proof of each person's immunity to the virus. A vaccine passport
serves as evidence that a person has tested negative for or is immune to a
particular virus. In terms of COVID-19, those who hold a vaccine passport will
be permitted entry into other nations as long as they can provide proof that
they have COVID-19 antibodies from prior infections or from full COVID-19
immunizations. To reduce time and effort spent managing data, the vaccination
passport system has been digitalized. The process of contact tracing may be
facilitated by digitization. The "Blockchain technology" system, which is
currently in use, has demonstrated its security and privacy in systems for data
exchange among bitcoin users. The Digital Vaccination Passport scheme can use
Blockchain technology. The end result would be a decentralized, traceable,
transparent, reliable, auditable, secure, and trustworthy solution based on the
Ethereum block-chain that would allow tracking of vaccines given and the
history of diseases.
</p></li>
</ul>

<h3>Title: On the evolution of research in hypersonics: application of natural language processing and machine learning. (arXiv:2208.08507v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08507">http://arxiv.org/abs/2208.08507</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08507] On the evolution of research in hypersonics: application of natural language processing and machine learning](http://arxiv.org/abs/2208.08507)</code></li>
<li>Summary: <p>Research and development in hypersonics have progressed significantly in
recent years, with various military and commercial applications being
demonstrated increasingly. Public and private organizations in several
countries have been investing in hypersonics, with the aim to overtake their
competitors and secure/improve strategic advantage and deterrence. For these
organizations, being able to identify emerging technologies in a timely and
reliable manner is paramount. Recent advances in information technology have
made it possible to analyze large amounts of data, extract hidden patterns, and
provide decision-makers with new insights. In this study, we focus on
scientific publications about hypersonics within the period of 2000-2020, and
employ natural language processing and machine learning to characterize the
research landscape by identifying 12 key latent research themes and analyzing
their temporal evolution. Our publication similarity analysis revealed patterns
that are indicative of cycles during two decades of research. The study offers
a comprehensive analysis of the research field and the fact that the research
themes are algorithmically extracted removes subjectivity from the exercise and
enables consistent comparisons between topics and between time intervals.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: DF-Captcha: A Deepfake Captcha for Preventing Fake Calls. (arXiv:2208.08524v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08524">http://arxiv.org/abs/2208.08524</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08524] DF-Captcha: A Deepfake Captcha for Preventing Fake Calls](http://arxiv.org/abs/2208.08524)</code></li>
<li>Summary: <p>Social engineering (SE) is a form of deception that aims to trick people into
giving access to data, information, networks and even money. For decades SE has
been a key method for attackers to gain access to an organization, virtually
skipping all lines of defense. Attackers also regularly use SE to scam innocent
people by making threatening phone calls which impersonate an authority or by
sending infected emails which look like they have been sent from a loved one.
SE attacks will likely remain a top attack vector for criminals because humans
are the weakest link in cyber security.
</p></li>
</ul>

<p>Unfortunately, the threat will only get worse now that a new technology
called deepfakes as arrived. A deepfake is believable media (e.g., videos)
created by an AI. Although the technology has mostly been used to swap the
faces of celebrities, it can also be used to `puppet' different personas.
Recently, researchers have shown how this technology can be deployed in
real-time to clone someone's voice in a phone call or reenact a face in a video
call. Given that any novice user can download this technology to use it, it is
no surprise that criminals have already begun to monetize it to perpetrate
their SE attacks.
</p>
<p>In this paper, we propose a lightweight application which can protect
organizations and individuals from deepfake SE attacks. Through a challenge and
response approach, we leverage the technical and theoretical limitations of
deepfake technologies to expose the attacker. Existing defence solutions are
too heavy as an end-point solution and can be evaded by a dynamic attacker. In
contrast, our approach is lightweight and breaks the reactive arms race,
putting the attacker at a disadvantage.
</p>

<h3>Title: Lessons from a Space Lab -- An Image Acquisition Perspective. (arXiv:2208.08865v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08865">http://arxiv.org/abs/2208.08865</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08865] Lessons from a Space Lab -- An Image Acquisition Perspective](http://arxiv.org/abs/2208.08865)</code></li>
<li>Summary: <p>The use of Deep Learning (DL) algorithms has improved the performance of
vision-based space applications in recent years. However, generating large
amounts of annotated data for training these DL algorithms has proven
challenging. While synthetically generated images can be used, the DL models
trained on synthetic data are often susceptible to performance degradation,
when tested in real-world environments. In this context, the Interdisciplinary
Center of Security, Reliability and Trust (SnT) at the University of Luxembourg
has developed the 'SnT Zero-G Lab', for training and validating vision-based
space algorithms in conditions emulating real-world space environments. An
important aspect of the SnT Zero-G Lab development was the equipment selection.
From the lessons learned during the lab development, this article presents a
systematic approach combining market survey and experimental analyses for
equipment selection. In particular, the article focus on the image acquisition
equipment in a space lab: background materials, cameras and illumination lamps.
The results from the experiment analyses show that the market survey
complimented by experimental analyses is required for effective equipment
selection in a space lab development project.
</p></li>
</ul>

<h3>Title: Embracing Graph Neural Networks for Hardware Security (Invited Paper). (arXiv:2208.08554v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08554">http://arxiv.org/abs/2208.08554</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08554] Embracing Graph Neural Networks for Hardware Security (Invited Paper)](http://arxiv.org/abs/2208.08554)</code></li>
<li>Summary: <p>Graph neural networks (GNNs) have attracted increasing attention due to their
superior performance in deep learning on graph-structured data. GNNs have
succeeded across various domains such as social networks, chemistry, and
electronic design automation (EDA). Electronic circuits have a long history of
being represented as graphs, and to no surprise, GNNs have demonstrated
state-of-the-art performance in solving various EDA tasks. More importantly,
GNNs are now employed to address several hardware security problems, such as
detecting intellectual property (IP) piracy and hardware Trojans (HTs), to name
a few.
</p></li>
</ul>

<p>In this survey, we first provide a comprehensive overview of the usage of
GNNs in hardware security and propose the first taxonomy to divide the
state-of-the-art GNN-based hardware security systems into four categories: (i)
HT detection systems, (ii) IP piracy detection systems, (iii) reverse
engineering platforms, and (iv) attacks on logic locking. We summarize the
different architectures, graph types, node features, benchmark data sets, and
model evaluation of the employed GNNs. Finally, we elaborate on the lessons
learned and discuss future directions.
</p>

<h3>Title: ObfuNAS: A Neural Architecture Search-based DNN Obfuscation Approach. (arXiv:2208.08569v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08569">http://arxiv.org/abs/2208.08569</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08569] ObfuNAS: A Neural Architecture Search-based DNN Obfuscation Approach](http://arxiv.org/abs/2208.08569)</code></li>
<li>Summary: <p>Malicious architecture extraction has been emerging as a crucial concern for
deep neural network (DNN) security. As a defense, architecture obfuscation is
proposed to remap the victim DNN to a different architecture. Nonetheless, we
observe that, with only extracting an obfuscated DNN architecture, the
adversary can still retrain a substitute model with high performance (e.g.,
accuracy), rendering the obfuscation techniques ineffective. To mitigate this
under-explored vulnerability, we propose ObfuNAS, which converts the DNN
architecture obfuscation into a neural architecture search (NAS) problem. Using
a combination of function-preserving obfuscation strategies, ObfuNAS ensures
that the obfuscated DNN architecture can only achieve lower accuracy than the
victim. We validate the performance of ObfuNAS with open-source architecture
datasets like NAS-Bench-101 and NAS-Bench-301. The experimental results
demonstrate that ObfuNAS can successfully find the optimal mask for a victim
model within a given FLOPs constraint, leading up to 2.6% inference accuracy
degradation for attackers with only 0.14x FLOPs overhead. The code is available
at: https://github.com/Tongzhou0101/ObfuNAS.
</p></li>
</ul>

<h3>Title: Reverse Engineering of Integrated Circuits: Tools and Techniques. (arXiv:2208.08689v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08689">http://arxiv.org/abs/2208.08689</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08689] Reverse Engineering of Integrated Circuits: Tools and Techniques](http://arxiv.org/abs/2208.08689)</code></li>
<li>Summary: <p>Consumer and defense systems demanded design and manufacturing of electronics
with increased performance, compared to their predecessors. As such systems
became ubiquitous in a plethora of domains, their application surface
increased, thus making them a target for adversaries. Hence, with improved
performance the aspect of security demanded even more attention of the
designers. The research community is rife with extensive details of attacks
that target the confidential design details by exploiting vulnerabilities. The
adversary could target the physical design of a semiconductor chip or break a
cryptographic algorithm by extracting the secret keys, using attacks that will
be discussed in this thesis. This thesis focuses on presenting a brief overview
of IC reverse engineering attack and attacks targeting cryptographic systems.
Further, the thesis presents my contributions to the defenses for the discussed
attacks. The globalization of the Integrated Circuit (IC) supply chain has
rendered the advantage of low-cost and high-performance ICs in the market for
the end users. But this has also made the design vulnerable to over production,
IP Piracy, reverse engineering attacks and hardware malware during the
manufacturing and post manufacturing process. Logic locking schemes have been
proposed in the past to overcome the design trust issues but the new
state-of-the-art attacks such as SAT has proven a larger threat. This work
highlights the reverse engineering attack and a proposed hardened platform
along with its framework.
</p></li>
</ul>

<h3>Title: MPInspector: A Systematic and Automatic Approach for Evaluating the Security of IoT Messaging Protocols. (arXiv:2208.08751v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08751">http://arxiv.org/abs/2208.08751</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08751] MPInspector: A Systematic and Automatic Approach for Evaluating the Security of IoT Messaging Protocols](http://arxiv.org/abs/2208.08751)</code></li>
<li>Summary: <p>Facilitated by messaging protocols (MP), many home devices are connected to
the Internet, bringing convenience and accessibility to customers. However,
most deployed MPs on IoT platforms are fragmented and are not implemented
carefully to support secure communication. To the best of our knowledge, there
is no systematic solution to perform automatic security checks on MP
implementations yet.
</p></li>
</ul>

<p>To bridge the gap, we present MPInspector, the first automatic and systematic
solution for vetting the security of MP implementations. MPInspector combines
model learning with formal analysis and operates in three stages: (a) using
parameter semantics extraction and interaction logic extraction to
automatically infer the state machine of an MP implementation, (b) generating
security properties based on meta properties and the state machine, and (c)
applying automatic property based formal verification to identify property
violations. We evaluate MPInspector on three popular MPs, including MQTT, CoAP
and AMQP, implemented on nine leading IoT platforms. It identifies 252 property
violations, leveraging which we further identify eleven types of attacks under
two realistic attack scenarios. In addition, we demonstrate that MPInspector is
lightweight (the average overhead of end-to-end analysis is ~4.5 hours) and
effective with a precision of 100% in identifying property violations.
</p>

<h3>Title: Oh SSH-it, what's my fingerprint? A Large-Scale Analysis of SSH Host Key Fingerprint Verification Records in the DNS. (arXiv:2208.08846v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08846">http://arxiv.org/abs/2208.08846</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08846] Oh SSH-it, what's my fingerprint? A Large-Scale Analysis of SSH Host Key Fingerprint Verification Records in the DNS](http://arxiv.org/abs/2208.08846)</code></li>
<li>Summary: <p>The SSH protocol is commonly used to access remote systems on the Internet,
as it provides an encrypted and authenticated channel for communication. If
upon establishing a new connection, the presented server key is unknown to the
client, the user is asked to verify the key fingerprint manually, which is
prone to errors and often blindly trusted. The SSH standard describes an
alternative to such manual key verification: using the Domain Name System (DNS)
to publish the server key information in SSHFP records.
</p></li>
</ul>

<p>In this paper, we conduct a large-scale Internet study to measure the
prevalence of SSHFP records among DNS domain names. We scan the Tranco 1M list
and over 500 million names from the certificate transparency log over the
course of 26 days. The results show that in two studied populations, about 1 in
10,000 domains has SSHFP records, with more than half of them deployed without
using DNSSEC, drastically reducing security benefits.
</p>

<h3>Title: Aggregation and probabilistic verification for data authentication in VANETs. (arXiv:2208.08946v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08946">http://arxiv.org/abs/2208.08946</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08946] Aggregation and probabilistic verification for data authentication in VANETs](http://arxiv.org/abs/2208.08946)</code></li>
<li>Summary: <p>Vehicular ad-hoc networks, where traffic information is distributed from many
sources to many destinations, require data authentication mechanisms to detect
any malicious behavior of users, such as modification or replay attacks. In
this paper we propose a new data aggregation protocol that uses probabilistic
verification to detect such attack attempts a posteriori in an efficient way,
with minimal overhead and delay. The algorithm also contains an additional
security mechanism based on reactive groups created on demand, which ensure a
priori that vehicles generate trustworthy information. According to a
comprehensive analysis including both a small-scale real device implementation
and NS2 simulations, it is shown that the proposed protocol is robust.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Necessary Conditions in Multi-Server Differential Privacy. (arXiv:2208.08540v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08540">http://arxiv.org/abs/2208.08540</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08540] Necessary Conditions in Multi-Server Differential Privacy](http://arxiv.org/abs/2208.08540)</code></li>
<li>Summary: <p>We consider protocols where users communicate with multiple servers to
perform a computation on the users' data. An adversary exerts semi-honest
control over many of the parties but its view is differentially private with
respect to honest users. Prior work described protocols that required multiple
rounds of interaction or offered privacy against a computationally bounded
adversary. Our work presents limitations of non-interactive protocols that
offer privacy against unbounded adversaries. We show these protocols demand
exponentially more samples for some learning and estimation tasks than
centrally private counterparts. This means performing as well as the central
model requires interactivity or computational differential privacy, or both.
</p></li>
</ul>

<h3>Title: Private, Efficient, and Accurate: Protecting Models Trained by Multi-party Learning with Differential Privacy. (arXiv:2208.08662v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08662">http://arxiv.org/abs/2208.08662</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08662] Private, Efficient, and Accurate: Protecting Models Trained by Multi-party Learning with Differential Privacy](http://arxiv.org/abs/2208.08662)</code></li>
<li>Summary: <p>Secure multi-party computation-based machine learning, referred to as MPL,
has become an important technology to utilize data from multiple parties with
privacy preservation. While MPL provides rigorous security guarantees for the
computation process, the models trained by MPL are still vulnerable to attacks
that solely depend on access to the models. Differential privacy could help to
defend against such attacks. However, the accuracy loss brought by differential
privacy and the huge communication overhead of secure multi-party computation
protocols make it highly challenging to balance the 3-way trade-off between
privacy, efficiency, and accuracy.
</p></li>
</ul>

<p>In this paper, we are motivated to resolve the above issue by proposing a
solution, referred to as PEA (Private, Efficient, Accurate), which consists of
a secure DPSGD protocol and two optimization methods. First, we propose a
secure DPSGD protocol to enforce DPSGD in secret sharing-based MPL frameworks.
Second, to reduce the accuracy loss led by differential privacy noise and the
huge communication overhead of MPL, we propose two optimization methods for the
training process of MPL: (1) the data-independent feature extraction method,
which aims to simplify the trained model structure; (2) the local data-based
global model initialization method, which aims to speed up the convergence of
the model training. We implement PEA in two open-source MPL frameworks:
TF-Encrypted and Queqiao. The experimental results on various datasets
demonstrate the efficiency and effectiveness of PEA. E.g. when ${\epsilon}$ =
2, we can train a differentially private classification model with an accuracy
of 88% for CIFAR-10 within 7 minutes under the LAN setting. This result
significantly outperforms the one from CryptGPU, one SOTA MPL framework: it
costs more than 16 hours to train a non-private deep neural network model on
CIFAR-10 with the same accuracy.
</p>

<h2>protect</h2>
<h3>Title: Learning Spatial-Frequency Transformer for Visual Object Tracking. (arXiv:2208.08829v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08829">http://arxiv.org/abs/2208.08829</a></li>
<li>Code URL: <a href="https://github.com/tchuanm/sftranst">https://github.com/tchuanm/sftranst</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08829] Learning Spatial-Frequency Transformer for Visual Object Tracking](http://arxiv.org/abs/2208.08829)</code></li>
<li>Summary: <p>Recent trackers adopt the Transformer to combine or replace the widely used
ResNet as their new backbone network. Although their trackers work well in
regular scenarios, however, they simply flatten the 2D features into a sequence
to better match the Transformer. We believe these operations ignore the spatial
prior of the target object which may lead to sub-optimal results only. In
addition, many works demonstrate that self-attention is actually a low-pass
filter, which is independent of input features or key/queries. That is to say,
it may suppress the high-frequency component of the input features and preserve
or even amplify the low-frequency information. To handle these issues, in this
paper, we propose a unified Spatial-Frequency Transformer that models the
Gaussian spatial Prior and High-frequency emphasis Attention (GPHA)
simultaneously. To be specific, Gaussian spatial prior is generated using dual
Multi-Layer Perceptrons (MLPs) and injected into the similarity matrix produced
by multiplying Query and Key features in self-attention. The output will be fed
into a Softmax layer and then decomposed into two components, i.e., the direct
signal and high-frequency signal. The low- and high-pass branches are rescaled
and combined to achieve all-pass, therefore, the high-frequency features will
be protected well in stacked self-attention layers. We further integrate the
Spatial-Frequency Transformer into the Siamese tracking framework and propose a
novel tracking algorithm, termed SFTransT. The cross-scale fusion based
SwinTransformer is adopted as the backbone, and also a multi-head
cross-attention module is used to boost the interaction between search and
template features. The output will be fed into the tracking head for target
localization. Extensive experiments on both short-term and long-term tracking
benchmarks all demonstrate the effectiveness of our proposed framework.
</p></li>
</ul>

<h3>Title: Detecting Environmental Violations with Satellite Imagery in Near Real Time: Land Application under the Clean Water Act. (arXiv:2208.08919v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08919">http://arxiv.org/abs/2208.08919</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08919] Detecting Environmental Violations with Satellite Imagery in Near Real Time: Land Application under the Clean Water Act](http://arxiv.org/abs/2208.08919)</code></li>
<li>Summary: <p>This paper introduces a new, highly consequential setting for the use of
computer vision for environmental sustainability. Concentrated Animal Feeding
Operations (CAFOs) (aka intensive livestock farms or "factory farms") produce
significant manure and pollution. Dumping manure in the winter months poses
significant environmental risks and violates environmental law in many states.
Yet the federal Environmental Protection Agency (EPA) and state agencies have
relied primarily on self-reporting to monitor such instances of "land
application." Our paper makes four contributions. First, we introduce the
environmental, policy, and agricultural setting of CAFOs and land application.
Second, we provide a new dataset of high-cadence (daily to weekly) 3m/pixel
satellite imagery from 2018-20 for 330 CAFOs in Wisconsin with hand labeled
instances of land application (n=57,697). Third, we develop an object detection
model to predict land application and a system to perform inference in near
real-time. We show that this system effectively appears to detect land
application (PR AUC = 0.93) and we uncover several outlier facilities which
appear to apply regularly and excessively. Last, we estimate the population
prevalence of land application events in Winter 2021/22. We show that the
prevalence of land application is much higher than what is self-reported by
facilities. The system can be used by environmental regulators and interest
groups, one of which piloted field visits based on this system this past
winter. Overall, our application demonstrates the potential for AI-based
computer vision systems to solve major problems in environmental compliance
with near-daily imagery.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: LogKernel A Threat Hunting Approach Based on Behaviour Provenance Graph and Graph Kernel Clustering. (arXiv:2208.08820v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08820">http://arxiv.org/abs/2208.08820</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08820] LogKernel A Threat Hunting Approach Based on Behaviour Provenance Graph and Graph Kernel Clustering](http://arxiv.org/abs/2208.08820)</code></li>
<li>Summary: <p>Cyber threat hunting is a proactive search process for hidden threats in the
organization's information system. It is a crucial component of active defense
against advanced persistent threats (APTs). However, most of the current threat
hunting methods rely on Cyber Threat Intelligence(CTI), which can find known
attacks but cannot find unknown attacks that have not been disclosed by CTI. In
this paper, we propose LogKernel, a threat hunting method based on graph kernel
clustering which can effectively separates attack behaviour from benign
activities. LogKernel first abstracts system audit logs into Behaviour
Provenance Graphs (BPGs), and then clusters graphs by embedding them into a
continuous space using a graph kernel. In particular, we design a new graph
kernel clustering method based on the characteristics of BPGs, which can
capture structure information and rich label information of the BPGs. To reduce
false positives, LogKernel further quantifies the threat of abnormal behaviour.
We evaluate LogKernel on the malicious dataset which includes seven simulated
attack scenarios and the DAPRA CADETS dataset which includes four attack
scenarios. The result shows that LogKernel can hunt all attack scenarios among
them, and compared to the state-of-the-art methods, it can find unknown
attacks.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Enhancing Targeted Attack Transferability via Diversified Weight Pruning. (arXiv:2208.08677v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08677">http://arxiv.org/abs/2208.08677</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08677] Enhancing Targeted Attack Transferability via Diversified Weight Pruning](http://arxiv.org/abs/2208.08677)</code></li>
<li>Summary: <p>Malicious attackers can generate targeted adversarial examples by imposing
human-imperceptible noise on images, forcing neural network models to produce
specific incorrect outputs. With cross-model transferable adversarial examples,
the vulnerability of neural networks remains even if the model information is
kept secret from the attacker. Recent studies have shown the effectiveness of
ensemble-based methods in generating transferable adversarial examples.
However, existing methods fall short under the more challenging scenario of
creating targeted attacks transferable among distinct models. In this work, we
propose Diversified Weight Pruning (DWP) to further enhance the ensemble-based
methods by leveraging the weight pruning method commonly used in model
compression. Specifically, we obtain multiple diverse models by a random weight
pruning method. These models preserve similar accuracies and can serve as
additional models for ensemble-based methods, yielding stronger transferable
targeted attacks. Experiments on ImageNet-Compatible Dataset under the more
challenging scenarios are provided: transferring to distinct architectures and
to adversarially trained models. The results show that our proposed DWP
improves the targeted attack success rates with up to 4.1% and 8.0% on the
combination of state-of-the-art methods, respectively
</p></li>
</ul>

<h3>Title: Resisting Adversarial Attacks in Deep Neural Networks using Diverse Decision Boundaries. (arXiv:2208.08697v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08697">http://arxiv.org/abs/2208.08697</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08697] Resisting Adversarial Attacks in Deep Neural Networks using Diverse Decision Boundaries](http://arxiv.org/abs/2208.08697)</code></li>
<li>Summary: <p>The security of deep learning (DL) systems is an extremely important field of
study as they are being deployed in several applications due to their
ever-improving performance to solve challenging tasks. Despite overwhelming
promises, the deep learning systems are vulnerable to crafted adversarial
examples, which may be imperceptible to the human eye, but can lead the model
to misclassify. Protections against adversarial perturbations on ensemble-based
techniques have either been shown to be vulnerable to stronger adversaries or
shown to lack an end-to-end evaluation. In this paper, we attempt to develop a
new ensemble-based solution that constructs defender models with diverse
decision boundaries with respect to the original model. The ensemble of
classifiers constructed by (1) transformation of the input by a method called
Split-and-Shuffle, and (2) restricting the significant features by a method
called Contrast-Significant-Features are shown to result in diverse gradients
with respect to adversarial attacks, which reduces the chance of transferring
adversarial examples from the original to the defender model targeting the same
class. We present extensive experimentations using standard image
classification datasets, namely MNIST, CIFAR-10 and CIFAR-100 against
state-of-the-art adversarial attacks to demonstrate the robustness of the
proposed ensemble-based defense. We also evaluate the robustness in the
presence of a stronger adversary targeting all the models within the ensemble
simultaneously. Results for the overall false positives and false negatives
have been furnished to estimate the overall performance of the proposed
methodology.
</p></li>
</ul>

<h3>Title: Profiler: Profile-Based Model to Detect Phishing Emails. (arXiv:2208.08745v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08745">http://arxiv.org/abs/2208.08745</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08745] Profiler: Profile-Based Model to Detect Phishing Emails](http://arxiv.org/abs/2208.08745)</code></li>
<li>Summary: <p>Email phishing has become more prevalent and grows more sophisticated over
time. To combat this rise, many machine learning (ML) algorithms for detecting
phishing emails have been developed. However, due to the limited email data
sets on which these algorithms train, they are not adept at recognising varied
attacks and, thus, suffer from concept drift; attackers can introduce small
changes in the statistical characteristics of their emails or websites to
successfully bypass detection. Over time, a gap develops between the reported
accuracy from literature and the algorithm's actual effectiveness in the real
world. This realises itself in frequent false positive and false negative
classifications.
</p></li>
</ul>

<p>To this end, we propose a multidimensional risk assessment of emails to
reduce the feasibility of an attacker adapting their email and avoiding
detection. This horizontal approach to email phishing detection profiles an
incoming email on its main features. We develop a risk assessment framework
that includes three models which analyse an email's (1) threat level, (2)
cognitive manipulation, and (3) email type, which we combine to return the
final risk assessment score. The Profiler does not require large data sets to
train on to be effective and its analysis of varied email features reduces the
impact of concept drift. Our Profiler can be used in conjunction with ML
approaches, to reduce their misclassifications or as a labeller for large email
data sets in the training stage.
</p>
<p>We evaluate the efficacy of the Profiler against a machine learning ensemble
using state-of-the-art ML algorithms on a data set of 9000 legitimate and 900
phishing emails from a large Australian research organisation. Our results
indicate that the Profiler's mitigates the impact of concept drift, and
delivers 30% less false positive and 25% less false negative email
classifications over the ML ensemble's approach.
</p>

<h3>Title: Truth-Table Net: A New Convolutional Architecture Encodable By Design Into SAT Formulas. (arXiv:2208.08609v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08609">http://arxiv.org/abs/2208.08609</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08609] Truth-Table Net: A New Convolutional Architecture Encodable By Design Into SAT Formulas](http://arxiv.org/abs/2208.08609)</code></li>
<li>Summary: <p>With the expanding role of neural networks, the need for complete and sound
verification of their property has become critical. In the recent years, it was
established that Binary Neural Networks (BNNs) have an equivalent
representation in Boolean logic and can be formally analyzed using logical
reasoning tools such as SAT solvers. However, to date, only BNNs can be
transformed into a SAT formula. In this work, we introduce Truth Table Deep
Convolutional Neural Networks (TTnets), a new family of SAT-encodable models
featuring for the first time real-valued weights. Furthermore, it admits, by
construction, some valuable conversion features including post-tuning and
tractability in the robustness verification setting. The latter property leads
to a more compact SAT symbolic encoding than BNNs. This enables the use of
general SAT solvers, making property verification easier. We demonstrate the
value of TTnets regarding the formal robustness property: TTnets outperform the
verified accuracy of all BNNs with a comparable computation time. More
generally, they represent a relevant trade-off between all known complete
verification methods: TTnets achieve high verified accuracy with fast
verification time, being complete with no timeouts. We are exploring here a
proof of concept of TTnets for a very important application (complete
verification of robustness) and we believe this novel real-valued network
constitutes a practical response to the rising need for functional formal
verification. We postulate that TTnets can apply to various CNN-based
architectures and be extended to other properties such as fairness, fault
attack and exact rule extraction.
</p></li>
</ul>

<h3>Title: Complex-Value Spatio-temporal Graph Convolutional Neural Networks and its Applications to Electric Power Systems AI. (arXiv:2208.08485v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08485">http://arxiv.org/abs/2208.08485</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08485] Complex-Value Spatio-temporal Graph Convolutional Neural Networks and its Applications to Electric Power Systems AI](http://arxiv.org/abs/2208.08485)</code></li>
<li>Summary: <p>The effective representation, precessing, analysis, and visualization of
large-scale structured data over graphs are gaining a lot of attention. So far
most of the literature has focused on real-valued signals. However, signals are
often sparse in the Fourier domain, and more informative and compact
representations for them can be obtained using the complex envelope of their
spectral components, as opposed to the original real-valued signals. Motivated
by this fact, in this work we generalize graph convolutional neural networks
(GCN) to the complex domain, deriving the theory that allows to incorporate a
complex-valued graph shift operators (GSO) in the definition of graph filters
(GF) and process complex-valued graph signals (GS). The theory developed can
handle spatio-temporal complex network processes. We prove that complex-valued
GCNs are stable with respect to perturbations of the underlying graph support,
the bound of the transfer error and the bound of error propagation through
multiply layers. Then we apply complex GCN to power grid state forecasting,
power grid cyber-attack detection and localization.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Object Detection for Autonomous Dozers. (arXiv:2208.08570v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08570">http://arxiv.org/abs/2208.08570</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08570] Object Detection for Autonomous Dozers](http://arxiv.org/abs/2208.08570)</code></li>
<li>Summary: <p>We introduce a new type of autonomous vehicle - an autonomous dozer that is
expected to complete construction site tasks in an efficient, robust, and safe
manner. To better handle the path planning for the dozer and ensure
construction site safety, object detection plays one of the most critical
components among perception tasks. In this work, we first collect the
construction site data by driving around our dozers. Then we analyze the data
thoroughly to understand its distribution. Finally, two well-known object
detection models are trained, and their performances are benchmarked with a
wide range of training strategies and hyperparameters.
</p></li>
</ul>

<h3>Title: Spatial Temporal Graph Attention Network for Skeleton-Based Action Recognition. (arXiv:2208.08599v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08599">http://arxiv.org/abs/2208.08599</a></li>
<li>Code URL: <a href="https://github.com/hulianyuyy/STGAT">https://github.com/hulianyuyy/STGAT</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08599] Spatial Temporal Graph Attention Network for Skeleton-Based Action Recognition](http://arxiv.org/abs/2208.08599)</code></li>
<li>Summary: <p>It's common for current methods in skeleton-based action recognition to
mainly consider capturing long-term temporal dependencies as skeleton sequences
are typically long (>128 frames), which forms a challenging problem for
previous approaches. In such conditions, short-term dependencies are few
formally considered, which are critical for classifying similar actions. Most
current approaches are consisted of interleaving spatial-only modules and
temporal-only modules, where direct information flow among joints in adjacent
frames are hindered, thus inferior to capture short-term motion and distinguish
similar action pairs. To handle this limitation, we propose a general
framework, coined as STGAT, to model cross-spacetime information flow. It
equips the spatial-only modules with spatial-temporal modeling for regional
perception. While STGAT is theoretically effective for spatial-temporal
modeling, we propose three simple modules to reduce local spatial-temporal
feature redundancy and further release the potential of STGAT, which (1) narrow
the scope of self-attention mechanism, (2) dynamically weight joints along
temporal dimension, and (3) separate subtle motion from static features,
respectively. As a robust feature extractor, STGAT generalizes better upon
classifying similar actions than previous methods, witnessed by both
qualitative and quantitative results. STGAT achieves state-of-the-art
performance on three large-scale datasets: NTU RGB+D 60, NTU RGB+D 120, and
Kinetics Skeleton 400. Code is released.
</p></li>
</ul>

<h3>Title: Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance. (arXiv:2208.08664v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08664">http://arxiv.org/abs/2208.08664</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08664] Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance](http://arxiv.org/abs/2208.08664)</code></li>
<li>Summary: <p>Denoising diffusion probabilistic models (DDPMs) are a recent family of
generative models that achieve state-of-the-art results. In order to obtain
class-conditional generation, it was suggested to guide the diffusion process
by gradients from a time-dependent classifier. While the idea is theoretically
sound, deep learning-based classifiers are infamously susceptible to
gradient-based adversarial attacks. Therefore, while traditional classifiers
may achieve good accuracy scores, their gradients are possibly unreliable and
might hinder the improvement of the generation results. Recent work discovered
that adversarially robust classifiers exhibit gradients that are aligned with
human perception, and these could better guide a generative process towards
semantically meaningful images. We utilize this observation by defining and
training a time-dependent adversarially robust classifier and use it as
guidance for a generative diffusion model. In experiments on the highly
challenging and diverse ImageNet dataset, our scheme introduces significantly
more intelligible intermediate gradients, better alignment with theoretical
findings, as well as improved generation results under several evaluation
metrics. Furthermore, we conduct an opinion survey whose findings indicate that
human raters prefer our method's results.
</p></li>
</ul>

<h3>Title: SDA-SNE: Spatial Discontinuity-Aware Surface Normal Estimation via Multi-Directional Dynamic Programming. (arXiv:2208.08667v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08667">http://arxiv.org/abs/2208.08667</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08667] SDA-SNE: Spatial Discontinuity-Aware Surface Normal Estimation via Multi-Directional Dynamic Programming](http://arxiv.org/abs/2208.08667)</code></li>
<li>Summary: <p>The state-of-the-art (SoTA) surface normal estimators (SNEs) generally
translate depth images into surface normal maps in an end-to-end fashion.
Although such SNEs have greatly minimized the trade-off between efficiency and
accuracy, their performance on spatial discontinuities, e.g., edges and ridges,
is still unsatisfactory. To address this issue, this paper first introduces a
novel multi-directional dynamic programming strategy to adaptively determine
inliers (co-planar 3D points) by minimizing a (path) smoothness energy. The
depth gradients can then be refined iteratively using a novel recursive
polynomial interpolation algorithm, which helps yield more reasonable surface
normals. Our introduced spatial discontinuity-aware (SDA) depth gradient
refinement strategy is compatible with any depth-to-normal SNEs. Our proposed
SDA-SNE achieves much greater performance than all other SoTA approaches,
especially near/on spatial discontinuities. We further evaluate the performance
of SDA-SNE with respect to different iterations, and the results suggest that
it converges fast after only a few iterations. This ensures its high efficiency
in various robotics and computer vision applications requiring real-time
performance. Additional experiments on the datasets with different extents of
random noise further validate our SDA-SNE's robustness and environmental
adaptability. Our source code, demo video, and supplementary material are
publicly available at mias.group/SDA-SNE.
</p></li>
</ul>

<h3>Title: Evaluating Continual Test-Time Adaptation for Contextual and Semantic Domain Shifts. (arXiv:2208.08767v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08767">http://arxiv.org/abs/2208.08767</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08767] Evaluating Continual Test-Time Adaptation for Contextual and Semantic Domain Shifts](http://arxiv.org/abs/2208.08767)</code></li>
<li>Summary: <p>In this paper, our goal is to adapt a pre-trained Convolutional Neural
Network to domain shifts at test time. We do so continually with the incoming
stream of test batches, without labels. Existing literature mostly operates on
artificial shifts obtained via adversarial perturbations of a test image.
Motivated by this, we evaluate the state of the art on two realistic and
challenging sources of domain shifts, namely contextual and semantic shifts.
Contextual shifts correspond to the environment types, for example a model
pre-trained on indoor context has to adapt to the outdoor context on CORe-50
[7]. Semantic shifts correspond to the capture types, for example a model
pre-trained on natural images has to adapt to cliparts, sketches and paintings
on DomainNet [10]. We include in our analysis recent techniques such as
Prediction-Time Batch Normalization (BN) [8], Test Entropy Minimization (TENT)
[16] and Continual Test-Time Adaptation (CoTTA) [17]. Our findings are
three-fold: i) Test-time adaptation methods perform better and forget less on
contextual shifts compared to semantic shifts, ii) TENT outperforms other
methods on short-term adaptation, whereas CoTTA outpeforms other methods on
long-term adaptation, iii) BN is most reliable and robust.
</p></li>
</ul>

<h3>Title: Differentiable Architecture Search with Random Features. (arXiv:2208.08835v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08835">http://arxiv.org/abs/2208.08835</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08835] Differentiable Architecture Search with Random Features](http://arxiv.org/abs/2208.08835)</code></li>
<li>Summary: <p>Differentiable architecture search (DARTS) has significantly promoted the
development of NAS techniques because of its high search efficiency and
effectiveness but suffers from performance collapse. In this paper, we make
efforts to alleviate the performance collapse problem for DARTS from two
aspects. First, we investigate the expressive power of the supernet in DARTS
and then derive a new setup of DARTS paradigm with only training BatchNorm.
Second, we theoretically find that random features dilute the auxiliary
connection role of skip-connection in supernet optimization and enable search
algorithm focus on fairer operation selection, thereby solving the performance
collapse problem. We instantiate DARTS and PC-DARTS with random features to
build an improved version for each named RF-DARTS and RF-PCDARTS respectively.
Experimental results show that RF-DARTS obtains \textbf{94.36\%} test accuracy
on CIFAR-10 (which is the nearest optimal result in NAS-Bench-201), and
achieves the newest state-of-the-art top-1 test error of \textbf{24.0\%} on
ImageNet when transferring from CIFAR-10. Moreover, RF-DARTS performs robustly
across three datasets (CIFAR-10, CIFAR-100, and SVHN) and four search spaces
(S1-S4). Besides, RF-PCDARTS achieves even better results on ImageNet, that is,
\textbf{23.9\%} top-1 and \textbf{7.1\%} top-5 test error, surpassing
representative methods like single-path, training-free, and partial-channel
paradigms directly searched on ImageNet.
</p></li>
</ul>

<h3>Title: Analyzing Robustness of End-to-End Neural Models for Automatic Speech Recognition. (arXiv:2208.08509v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08509">http://arxiv.org/abs/2208.08509</a></li>
<li>Code URL: <a href="https://github.com/weizou52/robustness_analysis_asr">https://github.com/weizou52/robustness_analysis_asr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08509] Analyzing Robustness of End-to-End Neural Models for Automatic Speech Recognition](http://arxiv.org/abs/2208.08509)</code></li>
<li>Summary: <p>We investigate robustness properties of pre-trained neural models for
automatic speech recognition. Real life data in machine learning is usually
very noisy and almost never clean, which can be attributed to various factors
depending on the domain, e.g. outliers, random noise and adversarial noise.
Therefore, the models we develop for various tasks should be robust to such
kinds of noisy data, which led to the thriving field of robust machine
learning. We consider this important issue in the setting of automatic speech
recognition. With the increasing popularity of pre-trained models, it's an
important question to analyze and understand the robustness of such models to
noise. In this work, we perform a robustness analysis of the pre-trained neural
models wav2vec2, HuBERT and DistilHuBERT on the LibriSpeech and TIMIT datasets.
We use different kinds of noising mechanisms and measure the model performances
as quantified by the inference time and the standard Word Error Rate metric. We
also do an in-depth layer-wise analysis of the wav2vec2 model when injecting
noise in between layers, enabling us to predict at a high level what each layer
learns. Finally for this model, we visualize the propagation of errors across
the layers and compare how it behaves on clean versus noisy data. Our
experiments conform the predictions of Pasad et al. [2021] and also raise
interesting directions for future work.
</p></li>
</ul>

<h3>Title: Mere Contrastive Learning for Cross-Domain Sentiment Analysis. (arXiv:2208.08678v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08678">http://arxiv.org/abs/2208.08678</a></li>
<li>Code URL: <a href="https://github.com/luoxiaoheics/cobe">https://github.com/luoxiaoheics/cobe</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08678] Mere Contrastive Learning for Cross-Domain Sentiment Analysis](http://arxiv.org/abs/2208.08678)</code></li>
<li>Summary: <p>Cross-domain sentiment analysis aims to predict the sentiment of texts in the
target domain using the model trained on the source domain to cope with the
scarcity of labeled data. Previous studies are mostly cross-entropy-based
methods for the task, which suffer from instability and poor generalization. In
this paper, we explore contrastive learning on the cross-domain sentiment
analysis task. We propose a modified contrastive objective with in-batch
negative samples so that the sentence representations from the same class will
be pushed close while those from the different classes become further apart in
the latent space. Experiments on two widely used datasets show that our model
can achieve state-of-the-art performance in both cross-domain and multi-domain
sentiment analysis tasks. Meanwhile, visualizations demonstrate the
effectiveness of transferring knowledge learned in the source domain to the
target domain and the adversarial test verifies the robustness of our model.
</p></li>
</ul>

<h3>Title: Performance Evaluation of Selective Fixed-filter Active Noise Control based on Different Convolutional Neural Networks. (arXiv:2208.08440v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08440">http://arxiv.org/abs/2208.08440</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08440] Performance Evaluation of Selective Fixed-filter Active Noise Control based on Different Convolutional Neural Networks](http://arxiv.org/abs/2208.08440)</code></li>
<li>Summary: <p>Due to its rapid response time and a high degree of robustness, the selective
fixed-filter active noise control (SFANC) method appears to be a viable
candidate for widespread use in a variety of practical active noise control
(ANC) systems. In comparison to conventional fixed-filter ANC methods, SFANC
can select the pre-trained control filters for different types of noise. Deep
learning technologies, thus, can be used in SFANC methods to enable a more
flexible selection of the most appropriate control filters for attenuating
various noises. Furthermore, with the assistance of a deep neural network, the
selecting strategy can be learned automatically from noise data rather than
through trial and error, which significantly simplifies and improves the
practicability of ANC design. Therefore, this paper investigates the
performance of SFANC based on different one-dimensional and two-dimensional
convolutional neural networks. Additionally, we conducted comparative analyses
of several network training strategies and discovered that fine-tuning could
improve selection performance.
</p></li>
</ul>

<h3>Title: Robust Causal Graph Representation Learning against Confounding Effects. (arXiv:2208.08584v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08584">http://arxiv.org/abs/2208.08584</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08584] Robust Causal Graph Representation Learning against Confounding Effects](http://arxiv.org/abs/2208.08584)</code></li>
<li>Summary: <p>The prevailing graph neural network models have achieved significant progress
in graph representation learning. However, in this paper, we uncover an
ever-overlooked phenomenon: the pre-trained graph representation learning model
tested with full graphs underperforms the model tested with well-pruned graphs.
This observation reveals that there exist confounders in graphs, which may
interfere with the model learning semantic information, and current graph
representation learning methods have not eliminated their influence. To tackle
this issue, we propose Robust Causal Graph Representation Learning (RCGRL) to
learn robust graph representations against confounding effects. RCGRL
introduces an active approach to generate instrumental variables under
unconditional moment restrictions, which empowers the graph representation
learning model to eliminate confounders, thereby capturing discriminative
information that is causally related to downstream predictions. We offer
theorems and proofs to guarantee the theoretical effectiveness of the proposed
approach. Empirically, we conduct extensive experiments on a synthetic dataset
and multiple benchmark datasets. The results demonstrate that compared with
state-of-the-art methods, RCGRL achieves better prediction performance and
generalization ability.
</p></li>
</ul>

<h3>Title: Learning-based estimation of in-situ wind speed from underwater acoustics. (arXiv:2208.08912v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08912">http://arxiv.org/abs/2208.08912</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08912] Learning-based estimation of in-situ wind speed from underwater acoustics](http://arxiv.org/abs/2208.08912)</code></li>
<li>Summary: <p>Wind speed retrieval at sea surface is of primary importance for scientific
and operational applications. Besides weather models, in-situ measurements and
remote sensing technologies, especially satellite sensors, provide
complementary means to monitor wind speed. As sea surface winds produce sounds
that propagate underwater, underwater acoustics recordings can also deliver
fine-grained wind-related information. Whereas model-driven schemes, especially
data assimilation approaches, are the state-of-the-art schemes to address
inverse problems in geoscience, machine learning techniques become more and
more appealing to fully exploit the potential of observation datasets. Here, we
introduce a deep learning approach for the retrieval of wind speed time series
from underwater acoustics possibly complemented by other data sources such as
weather model reanalyses. Our approach bridges data assimilation and
learning-based frameworks to benefit both from prior physical knowledge and
computational efficiency. Numerical experiments on real data demonstrate that
we outperform the state-of-the-art data-driven methods with a relative gain up
to 16% in terms of RMSE. Interestingly, these results support the relevance of
the time dynamics of underwater acoustic data to better inform the time
evolution of wind speed. They also show that multimodal data, here underwater
acoustics data combined with ECMWF reanalysis data, may further improve the
reconstruction performance, including the robustness with respect to missing
underwater acoustics data.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Restoration of User Videos Shared on Social Media. (arXiv:2208.08597v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08597">http://arxiv.org/abs/2208.08597</a></li>
<li>Code URL: <a href="https://github.com/luohongming/votes">https://github.com/luohongming/votes</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08597] Restoration of User Videos Shared on Social Media](http://arxiv.org/abs/2208.08597)</code></li>
<li>Summary: <p>User videos shared on social media platforms usually suffer from degradations
caused by unknown proprietary processing procedures, which means that their
visual quality is poorer than that of the originals. This paper presents a new
general video restoration framework for the restoration of user videos shared
on social media platforms. In contrast to most deep learning-based video
restoration methods that perform end-to-end mapping, where feature extraction
is mostly treated as a black box, in the sense that what role a feature plays
is often unknown, our new method, termed Video restOration through adapTive
dEgradation Sensing (VOTES), introduces the concept of a degradation feature
map (DFM) to explicitly guide the video restoration process. Specifically, for
each video frame, we first adaptively estimate its DFM to extract features
representing the difficulty of restoring its different regions. We then feed
the DFM to a convolutional neural network (CNN) to compute hierarchical
degradation features to modulate an end-to-end video restoration backbone
network, such that more attention is paid explicitly to potentially more
difficult to restore areas, which in turn leads to enhanced restoration
performance. We will explain the design rationale of the VOTES framework and
present extensive experimental results to show that the new VOTES method
outperforms various state-of-the-art techniques both quantitatively and
qualitatively. In addition, we contribute a large scale real-world database of
user videos shared on different social media platforms. Codes and datasets are
available at https://github.com/luohongming/VOTES.git
</p></li>
</ul>

<h3>Title: A Two-Phase Paradigm for Joint Entity-Relation Extraction. (arXiv:2208.08659v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08659">http://arxiv.org/abs/2208.08659</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08659] A Two-Phase Paradigm for Joint Entity-Relation Extraction](http://arxiv.org/abs/2208.08659)</code></li>
<li>Summary: <p>An exhaustive study has been conducted to investigate span-based models for
the joint entity and relation extraction task. However, these models sample a
large number of negative entities and negative relations during the model
training, which are essential but result in grossly imbalanced data
distributions and in turn cause suboptimal model performance. In order to
address the above issues, we propose a two-phase paradigm for the span-based
joint entity and relation extraction, which involves classifying the entities
and relations in the first phase, and predicting the types of these entities
and relations in the second phase. The two-phase paradigm enables our model to
significantly reduce the data distribution gap, including the gap between
negative entities and other entities, as well as the gap between negative
relations and other relations. In addition, we make the first attempt at
combining entity type and entity distance as global features, which has proven
effective, especially for the relation extraction. Experimental results on
several datasets demonstrate that the spanbased joint extraction model
augmented with the two-phase paradigm and the global features consistently
outperforms previous state-of-the-art span-based models for the joint
extraction task, establishing a new standard benchmark. Qualitative and
quantitative analyses further validate the effectiveness the proposed paradigm
and the global features.
</p></li>
</ul>

<h3>Title: Open Information Extraction from 2007 to 2022 -- A Survey. (arXiv:2208.08690v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08690">http://arxiv.org/abs/2208.08690</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08690] Open Information Extraction from 2007 to 2022 -- A Survey](http://arxiv.org/abs/2208.08690)</code></li>
<li>Summary: <p>Open information extraction is an important NLP task that targets extracting
structured information from unstructured text without limitations on the
relation type or the domain of the text. This survey paper covers open
information extraction technologies from 2007 to 2022 with a focus on new
models not covered by previous surveys. We propose a new categorization method
from the source of information perspective to accommodate the development of
recent OIE technologies. In addition, we summarize three major approaches based
on task settings as well as current popular datasets and model evaluation
metrics. Given the comprehensive review, several future directions are shown
from datasets, source of information, output form, method, and evaluation
metric aspects.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: NET-FLEET: Achieving Linear Convergence Speedup for Fully Decentralized Federated Learning with Heterogeneous Data. (arXiv:2208.08490v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08490">http://arxiv.org/abs/2208.08490</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08490] NET-FLEET: Achieving Linear Convergence Speedup for Fully Decentralized Federated Learning with Heterogeneous Data](http://arxiv.org/abs/2208.08490)</code></li>
<li>Summary: <p>Federated learning (FL) has received a surge of interest in recent years
thanks to its benefits in data privacy protection, efficient communication, and
parallel data processing. Also, with appropriate algorithmic designs, one could
achieve the desirable linear speedup for convergence effect in FL. However,
most existing works on FL are limited to systems with i.i.d. data and
centralized parameter servers and results on decentralized FL with
heterogeneous datasets remains limited. Moreover, whether or not the linear
speedup for convergence is achievable under fully decentralized FL with data
heterogeneity remains an open question. In this paper, we address these
challenges by proposing a new algorithm, called NET-FLEET, for fully
decentralized FL systems with data heterogeneity. The key idea of our algorithm
is to enhance the local update scheme in FL (originally intended for
communication efficiency) by incorporating a recursive gradient correction
technique to handle heterogeneous datasets. We show that, under appropriate
parameter settings, the proposed NET-FLEET algorithm achieves a linear speedup
for convergence. We further conduct extensive numerical experiments to evaluate
the performance of the proposed NET-FLEET algorithm and verify our theoretical
findings.
</p></li>
</ul>

<h3>Title: A Hybrid Self-Supervised Learning Framework for Vertical Federated Learning. (arXiv:2208.08934v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08934">http://arxiv.org/abs/2208.08934</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08934] A Hybrid Self-Supervised Learning Framework for Vertical Federated Learning](http://arxiv.org/abs/2208.08934)</code></li>
<li>Summary: <p>Federated learning (FL) enables independent parties to collaboratively build
machine learning (ML) models while protecting data privacy. Vertical federated
learning (VFL), a variant of FL, has recently drawn increasing attention as the
VFL matches the enterprises' demands of leveraging more valuable features to
achieve better model performance without jeopardizing data privacy. However,
conventional VFL may run into data deficiency as it is only able to exploit
aligned samples (belonging to different parties) with labels, leaving often the
majority of unaligned and unlabeled samples unused. The data deficiency hampers
the effort of the federation. In this work, we propose a Federated Hybrid
Self-Supervised Learning framework, coined FedHSSL, to utilize all available
data (including unaligned and unlabeled samples) of participants to train the
joint VFL model. The core idea of FedHSSL is to utilize cross-party views
(i.e., dispersed features) of samples aligned among parties and local views
(i.e., augmentations) of samples within each party to improve the
representation learning capability of the joint VFL model through SSL (e.g.,
SimSiam). FedHSSL further exploits generic features shared among parties to
boost the performance of the joint model through partial model aggregation. We
empirically demonstrate that our FedHSSL achieves significant performance gains
compared with baseline methods, especially when the number of labeled samples
is small. We provide an in-depth analysis of FedHSSL regarding privacy leakage,
which is rarely discussed in existing self-supervised VFL works. We investigate
the protection mechanism for FedHSSL. The results show our protection can
thwart the state-of-the-art label inference attack.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Neural Payoff Machines: Predicting Fair and Stable Payoff Allocations Among Team Members. (arXiv:2208.08798v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08798">http://arxiv.org/abs/2208.08798</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08798] Neural Payoff Machines: Predicting Fair and Stable Payoff Allocations Among Team Members](http://arxiv.org/abs/2208.08798)</code></li>
<li>Summary: <p>In many multi-agent settings, participants can form teams to achieve
collective outcomes that may far surpass their individual capabilities.
Measuring the relative contributions of agents and allocating them shares of
the reward that promote long-lasting cooperation are difficult tasks.
Cooperative game theory offers solution concepts identifying distribution
schemes, such as the Shapley value, that fairly reflect the contribution of
individuals to the performance of the team or the Core, which reduces the
incentive of agents to abandon their team. Applications of such methods include
identifying influential features and sharing the costs of joint ventures or
team formation. Unfortunately, using these solutions requires tackling a
computational barrier as they are hard to compute, even in restricted settings.
In this work, we show how cooperative game-theoretic solutions can be distilled
into a learned model by training neural networks to propose fair and stable
payoff allocations. We show that our approach creates models that can
generalize to games far from the training distribution and can predict
solutions for more players than observed during training. An important
application of our framework is Explainable AI: our approach can be used to
speed-up Shapley value computations on many instances.
</p></li>
</ul>

<h3>Title: Long-term dynamics of fairness: understanding the impact of data-driven targeted help on job seekers. (arXiv:2208.08881v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2208.08881">http://arxiv.org/abs/2208.08881</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2208.08881] Long-term dynamics of fairness: understanding the impact of data-driven targeted help on job seekers](http://arxiv.org/abs/2208.08881)</code></li>
<li>Summary: <p>The use of data-driven decision support by public agencies is becoming more
widespread and already influences the allocation of public resources. This
raises ethical concerns, as it has adversely affected minorities and
historically discriminated groups. In this paper, we use an approach that
combines statistics and machine learning with dynamical modeling to assess
long-term fairness effects of labor market interventions. Specifically, we
develop and use a model to investigate the impact of decisions caused by a
public employment authority that selectively supports job-seekers through
targeted help. The selection of who receives what help is based on a
data-driven intervention model that estimates an individual's chances of
finding a job in a timely manner and is based on data that describes a
population in which skills relevant to the labor market are unevenly
distributed between two groups (e.g., males and females). The intervention
model has incomplete access to the individual's actual skills and can augment
this with knowledge of the individual's group affiliation, thus using a
protected attribute to increase predictive accuracy. We assess this
intervention model's dynamics -- especially fairness-related issues and
trade-offs between different fairness goals -- over time and compare it to an
intervention model that does not use group affiliation as a predictive feature.
We conclude that in order to quantify the trade-off correctly and to assess the
long-term fairness effects of such a system in the real-world, careful modeling
of the surrounding labor market is indispensable.
</p></li>
</ul>

<h2>interpretability</h2>
<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
