<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: Spatiotemporal and Semantic Zero-inflated Urban Anomaly Prediction. (arXiv:2304.01569v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01569">http://arxiv.org/abs/2304.01569</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01569] Spatiotemporal and Semantic Zero-inflated Urban Anomaly Prediction](http://arxiv.org/abs/2304.01569) #security</code></li>
<li>Summary: <p>Urban anomaly predictions, such as traffic accident prediction and crime
prediction, are of vital importance to smart city security and maintenance.
Existing methods typically use deep learning to capture the intra-dependencies
in spatial and temporal dimensions. However, numerous key challenges remain
unsolved, for instance, sparse zero-inflated data due to urban anomalies
occurring with low frequency (which can lead to poor performance on real-world
datasets), and both intra- and inter-dependencies of abnormal patterns across
spatial, temporal, and semantic dimensions. Moreover, a unified approach to
predict multiple kinds of anomaly is left to explore. In this paper, we propose
STS to jointly capture the intra- and inter-dependencies between the patterns
and the influential factors in three dimensions. Further, we use a multi-task
prediction module with a customized loss function to solve the zero-inflated
issue. To verify the effectiveness of the model, we apply it to two urban
anomaly prediction tasks, crime prediction and traffic accident risk
prediction, respectively. Experiments on two application scenarios with four
real-world datasets demonstrate the superiority of STS, which outperforms
state-of-the-art methods in the mean absolute error and the root mean square
error by 37.88% and 18.10% on zero-inflated datasets, and, 60.32% and 37.28% on
non-zero datasets, respectively.
</p></li>
</ul>

<h3>Title: CGDTest: A Constrained Gradient Descent Algorithm for Testing Neural Networks. (arXiv:2304.01826v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01826">http://arxiv.org/abs/2304.01826</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01826] CGDTest: A Constrained Gradient Descent Algorithm for Testing Neural Networks](http://arxiv.org/abs/2304.01826) #security</code></li>
<li>Summary: <p>In this paper, we propose a new Deep Neural Network (DNN) testing algorithm
called the Constrained Gradient Descent (CGD) method, and an implementation we
call CGDTest aimed at exposing security and robustness issues such as
adversarial robustness and bias in DNNs. Our CGD algorithm is a
gradient-descent (GD) method, with the twist that the user can also specify
logical properties that characterize the kinds of inputs that the user may
want. This functionality sets CGDTest apart from other similar DNN testing
tools since it allows users to specify logical constraints to test DNNs not
only for $\ell_p$ ball-based adversarial robustness but, more importantly,
includes richer properties such as disguised and flow adversarial constraints,
as well as adversarial robustness in the NLP domain. We showcase the utility
and power of CGDTest via extensive experimentation in the context of vision and
NLP domains, comparing against 32 state-of-the-art methods over these diverse
domains. Our results indicate that CGDTest outperforms state-of-the-art testing
tools for $\ell_p$ ball-based adversarial robustness, and is significantly
superior in testing for other adversarial robustness, with improvements in PAR2
scores of over 1500% in some cases over the next best tool. Our evaluation
shows that our CGD method outperforms competing methods we compared against in
terms of expressibility (i.e., a rich constraint language and concomitant tool
support to express a wide variety of properties), scalability (i.e., can be
applied to very large real-world models with up to 138 million parameters), and
generality (i.e., can be used to test a plethora of model architectures).
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Kernel Affine Hull Machines for Differentially Private Learning. (arXiv:2304.01300v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01300">http://arxiv.org/abs/2304.01300</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01300] Kernel Affine Hull Machines for Differentially Private Learning](http://arxiv.org/abs/2304.01300) #privacy</code></li>
<li>Summary: <p>This paper explores the use of affine hulls of points as a means of
representing data via learning in Reproducing Kernel Hilbert Spaces (RKHS),
with the goal of partitioning the data space into geometric bodies that conceal
privacy-sensitive information about individual data points, while preserving
the structure of the original learning problem. To this end, we introduce the
Kernel Affine Hull Machine (KAHM), which provides an effective way of computing
a distance measure from the resulting bounded geometric body. KAHM is a
critical building block in wide and deep autoencoders, which enable data
representation learning for classification applications. To ensure
privacy-preserving learning, we propose a novel method for generating
fabricated data, which involves smoothing differentially private data samples
through a transformation process. The resulting fabricated data guarantees not
only differential privacy but also ensures that the KAHM modeling error is not
larger than that of the original training data samples. We also address the
accuracy-loss issue that arises with differentially private classifiers by
using fabricated data. This approach results in a significant reduction in the
risk of membership inference attacks while incurring only a marginal loss of
accuracy. As an application, a KAHM based differentially private federated
learning scheme is introduced featuring that the evaluation of global
classifier requires only locally computed distance measures. Overall, our
findings demonstrate the potential of KAHM as effective tool for
privacy-preserving learning and classification.
</p></li>
</ul>

<h3>Title: A False Sense of Privacy: Towards a Reliable Evaluation Methodology for the Anonymization of Biometric Data. (arXiv:2304.01635v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01635">http://arxiv.org/abs/2304.01635</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01635] A False Sense of Privacy: Towards a Reliable Evaluation Methodology for the Anonymization of Biometric Data](http://arxiv.org/abs/2304.01635) #privacy</code></li>
<li>Summary: <p>Biometric data contains distinctive human traits such as facial features or
gait patterns. The use of biometric data permits an individuation so exact that
the data is utilized effectively in identification and authentication systems.
But for this same reason, privacy protections become indispensably necessary.
</p></li>
</ul>

<p>Privacy protection is extensively afforded by the technique of anonymization.
Anonymization techniques obfuscate or remove the sensitive personal data to
achieve high levels of anonymity. However, the effectiveness of anonymization
relies, in equal parts, on the effectiveness of the methods employed to
evaluate anonymization performance.
</p>
<p>In this paper, we assess the state-of-the-art methods used to evaluate the
performance of anonymization techniques for facial images and gait patterns. We
demonstrate that the state-of-the-art evaluation methods have serious and
frequent shortcomings. In particular, we find that the underlying assumptions
of the state-of-the-art are quite unwarranted. When a method evaluating the
performance of anonymization assumes a weak adversary or a weak recognition
scenario, then the resulting evaluation will very likely be a gross
overestimation of the anonymization performance. Therefore, we propose a
stronger adversary model which is alert to the recognition scenario as well as
to the anonymization scenario. Our adversary model implements an appropriate
measure of anonymization performance. We improve the selection process for the
evaluation dataset, and we reduce the numbers of identities contained in the
dataset while ensuring that these identities remain easily distinguishable from
one another. Our novel evaluation methodology surpasses the state-of-the-art
because we measure worst-case performance and so deliver a highly reliable
evaluation of biometric anonymization techniques.
</p>

<h3>Title: Privacy-Preserving Federated Discovery of DNA Motifs with Differential Privacy. (arXiv:2304.01689v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01689">http://arxiv.org/abs/2304.01689</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01689] Privacy-Preserving Federated Discovery of DNA Motifs with Differential Privacy](http://arxiv.org/abs/2304.01689) #privacy</code></li>
<li>Summary: <p>DNA motif discovery is an important issue in gene research, which aims to
identify transcription factor binding sites (i.e., motifs) in DNA sequences to
reveal the mechanisms that regulate gene expression. However, the phenomenon of
data silos and the problem of privacy leakage have seriously hindered the
development of DNA motif discovery. On the one hand, the phenomenon of data
silos makes data collection difficult. On the other hand, the collection and
use of DNA data become complicated and difficult because DNA is sensitive
private information. In this context, how discovering DNA motifs under the
premise of ensuring privacy and security and alleviating data silos has become
a very important issue. Therefore, this paper proposes a novel method, namely
DP-FLMD, to address this problem. Note that this is the first application of
federated learning to the field of genetics research. The federated learning
technique is used to solve the problem of data silos. It has the advantage of
enabling multiple participants to train models together and providing privacy
protection services. To address the challenges of federated learning in terms
of communication costs, this paper applies a sampling method and a strategy for
reducing communication costs to DP-FLMD. In addition, differential privacy, a
privacy protection technique with rigorous mathematical proof, is also applied
to DP-FLMD. Experiments on the DNA datasets show that DP-FLMD has high mining
accuracy and runtime efficiency, and the performance of the algorithm is
affected by some parameters.
</p></li>
</ul>

<h3>Title: SLPerf: a Unified Framework for Benchmarking Split Learning. (arXiv:2304.01502v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01502">http://arxiv.org/abs/2304.01502</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01502] SLPerf: a Unified Framework for Benchmarking Split Learning](http://arxiv.org/abs/2304.01502) #privacy</code></li>
<li>Summary: <p>Data privacy concerns has made centralized training of data, which is
scattered across silos, infeasible, leading to the need for collaborative
learning frameworks. To address that, two prominent frameworks emerged, i.e.,
federated learning (FL) and split learning (SL). While FL has established
various benchmark frameworks and research libraries, SL currently lacks a
unified library despite its diversity in terms of label sharing, model
aggregation, and cut layer choice. This lack of standardization makes comparing
SL paradigms difficult. To address this, we propose SLPerf, a unified research
framework and open research library for SL, and conduct extensive experiments
on four widely-used datasets under both IID and Non-IID data settings. Our
contributions include a comprehensive survey of recently proposed SL paradigms,
a detailed benchmark comparison of different SL paradigms in different
situations, and rich engineering take-away messages and research insights for
improving SL paradigms. SLPerf can facilitate SL algorithm development and fair
performance comparisons.
</p></li>
</ul>

<h3>Title: Selective Knowledge Sharing for Privacy-Preserving Federated Distillation without A Good Teacher. (arXiv:2304.01731v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01731">http://arxiv.org/abs/2304.01731</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01731] Selective Knowledge Sharing for Privacy-Preserving Federated Distillation without A Good Teacher](http://arxiv.org/abs/2304.01731) #privacy</code></li>
<li>Summary: <p>While federated learning is promising for privacy-preserving collaborative
learning without revealing local data, it remains vulnerable to white-box
attacks and struggles to adapt to heterogeneous clients. Federated distillation
(FD), built upon knowledge distillation--an effective technique for
transferring knowledge from a teacher model to student models--emerges as an
alternative paradigm, which provides enhanced privacy guarantees and addresses
model heterogeneity. Nevertheless, challenges arise due to variations in local
data distributions and the absence of a well-trained teacher model, which leads
to misleading and ambiguous knowledge sharing that significantly degrades model
performance. To address these issues, this paper proposes a selective knowledge
sharing mechanism for FD, termed Selective-FD. It includes client-side
selectors and a server-side selector to accurately and precisely identify
knowledge from local and ensemble predictions, respectively. Empirical studies,
backed by theoretical insights, demonstrate that our approach enhances the
generalization capabilities of the FD framework and consistently outperforms
baseline methods. This study presents a promising direction for effective
knowledge transfer in privacy-preserving collaborative learning.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: A Machine Learning Approach to Forecasting Honey Production with Tree-Based Methods. (arXiv:2304.01215v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01215">http://arxiv.org/abs/2304.01215</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01215] A Machine Learning Approach to Forecasting Honey Production with Tree-Based Methods](http://arxiv.org/abs/2304.01215) #protect</code></li>
<li>Summary: <p>The beekeeping sector has undergone considerable production variations over
the past years due to adverse weather conditions, occurring more frequently as
climate change progresses. These phenomena can be high-impact and cause the
environment to be unfavorable to the bees' activity. We disentangle the honey
production drivers with tree-based methods and predict honey production
variations for hives in Italy, one of the largest honey producers in Europe.
The database covers hundreds of beehive data from 2019-2022 gathered with
advanced precision beekeeping techniques. We train and interpret the machine
learning models making them prescriptive other than just predictive. Superior
predictive performances of tree-based methods compared to standard linear
techniques allow for better protection of bees' activity and assess potential
losses for beekeepers for risk management.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Defending Against Patch-based Backdoor Attacks on Self-Supervised Learning. (arXiv:2304.01482v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01482">http://arxiv.org/abs/2304.01482</a></li>
<li>Code URL: <a href="https://github.com/ucdvision/patchsearch">https://github.com/ucdvision/patchsearch</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01482] Defending Against Patch-based Backdoor Attacks on Self-Supervised Learning](http://arxiv.org/abs/2304.01482) #attack</code></li>
<li>Summary: <p>Recently, self-supervised learning (SSL) was shown to be vulnerable to
patch-based data poisoning backdoor attacks. It was shown that an adversary can
poison a small part of the unlabeled data so that when a victim trains an SSL
model on it, the final model will have a backdoor that the adversary can
exploit. This work aims to defend self-supervised learning against such
attacks. We use a three-step defense pipeline, where we first train a model on
the poisoned data. In the second step, our proposed defense algorithm
(PatchSearch) uses the trained model to search the training data for poisoned
samples and removes them from the training set. In the third step, a final
model is trained on the cleaned-up training set. Our results show that
PatchSearch is an effective defense. As an example, it improves a model's
accuracy on images containing the trigger from 38.2% to 63.7% which is very
close to the clean model's accuracy, 64.6%. Moreover, we show that PatchSearch
outperforms baselines and state-of-the-art defense approaches including those
using additional clean, trusted data. Our code is available at
https://github.com/UCDvision/PatchSearch
</p></li>
</ul>

<h3>Title: Untargeted Near-collision Attacks in Biometric Recognition. (arXiv:2304.01580v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01580">http://arxiv.org/abs/2304.01580</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01580] Untargeted Near-collision Attacks in Biometric Recognition](http://arxiv.org/abs/2304.01580) #attack</code></li>
<li>Summary: <p>A biometric recognition system can operate in two distinct modes,
identification or verification. In the first mode, the system recognizes an
individual by searching the enrolled templates of all the users for a match. In
the second mode, the system validates a claimed identity by comparing the fresh
template with the enrolled template for this identity. Both the experimentally
determined false match rate and false non-match rate through recognition
threshold adjustment define the recognition accuracy, and hence the security of
the system. The biometric transformation schemes usually produce binary
templates that are better handled by cryptographic schemes. One of the
requirements for these transformation schemes is their irreversibility. In this
work, we rely on probabilistic modelling to quantify the security strength of
binary templates. We investigate the influence of template size, database size
and threshold on the probability of having a near-collision, and we highlight
two attacks on biometric systems. We discuss the choice of parameters through
the generic presented attacks.
</p></li>
</ul>

<h3>Title: Re-thinking Model Inversion Attacks Against Deep Neural Networks. (arXiv:2304.01669v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01669">http://arxiv.org/abs/2304.01669</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01669] Re-thinking Model Inversion Attacks Against Deep Neural Networks](http://arxiv.org/abs/2304.01669) #attack</code></li>
<li>Summary: <p>Model inversion (MI) attacks aim to infer and reconstruct private training
data by abusing access to a model. MI attacks have raised concerns about the
leaking of sensitive information (e.g. private face images used in training a
face recognition system). Recently, several algorithms for MI have been
proposed to improve the attack performance. In this work, we revisit MI, study
two fundamental issues pertaining to all state-of-the-art (SOTA) MI algorithms,
and propose solutions to these issues which lead to a significant boost in
attack performance for all SOTA MI. In particular, our contributions are
two-fold: 1) We analyze the optimization objective of SOTA MI algorithms, argue
that the objective is sub-optimal for achieving MI, and propose an improved
optimization objective that boosts attack performance significantly. 2) We
analyze "MI overfitting", show that it would prevent reconstructed images from
learning semantics of training data, and propose a novel "model augmentation"
idea to overcome this issue. Our proposed solutions are simple and improve all
SOTA MI attack accuracy significantly. E.g., in the standard CelebA benchmark,
our solutions improve accuracy by 11.8% and achieve for the first time over 90%
attack accuracy. Our findings demonstrate that there is a clear risk of leaking
sensitive information from deep learning models. We urge serious consideration
to be given to the privacy implications. Our code, demo, and models are
available at
https://ngoc-nguyen-0.github.io/re-thinking_model_inversion_attacks/
</p></li>
</ul>

<h3>Title: Cross-Class Feature Augmentation for Class Incremental Learning. (arXiv:2304.01899v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01899">http://arxiv.org/abs/2304.01899</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01899] Cross-Class Feature Augmentation for Class Incremental Learning](http://arxiv.org/abs/2304.01899) #attack</code></li>
<li>Summary: <p>We propose a novel class incremental learning approach by incorporating a
feature augmentation technique motivated by adversarial attacks. We employ a
classifier learned in the past to complement training examples rather than
simply play a role as a teacher for knowledge distillation towards subsequent
models. The proposed approach has a unique perspective to utilize the previous
knowledge in class incremental learning since it augments features of arbitrary
target classes using examples in other classes via adversarial attacks on a
previously learned classifier. By allowing the cross-class feature
augmentations, each class in the old tasks conveniently populates samples in
the feature space, which alleviates the collapse of the decision boundaries
caused by sample deficiency for the previous tasks, especially when the number
of stored exemplars is small. This idea can be easily incorporated into
existing class incremental learning algorithms without any architecture
modification. Extensive experiments on the standard benchmarks show that our
method consistently outperforms existing class incremental learning methods by
significant margins in various scenarios, especially under an environment with
an extremely limited memory budget.
</p></li>
</ul>

<h3>Title: EGC: Image Generation and Classification via a Single Energy-Based Model. (arXiv:2304.02012v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.02012">http://arxiv.org/abs/2304.02012</a></li>
<li>Code URL: <a href="https://github.com/guoqiushan/egc">https://github.com/guoqiushan/egc</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.02012] EGC: Image Generation and Classification via a Single Energy-Based Model](http://arxiv.org/abs/2304.02012) #attack</code></li>
<li>Summary: <p>Learning image classification and image generation using the same set of
network parameters is a challenging problem. Recent advanced approaches perform
well in one task often exhibit poor performance in the other. This work
introduces an energy-based classifier and generator, namely EGC, which can
achieve superior performance in both tasks using a single neural network.
Unlike a conventional classifier that outputs a label given an image (i.e., a
conditional distribution $p(y|\mathbf{x})$), the forward pass in EGC is a
classifier that outputs a joint distribution $p(\mathbf{x},y)$, enabling an
image generator in its backward pass by marginalizing out the label $y$. This
is done by estimating the energy and classification probability given a noisy
image in the forward pass, while denoising it using the score function
estimated in the backward pass. EGC achieves competitive generation results
compared with state-of-the-art approaches on ImageNet-1k, CelebA-HQ and LSUN
Church, while achieving superior classification accuracy and robustness against
adversarial attacks on CIFAR-10. This work represents the first successful
attempt to simultaneously excel in both tasks using a single set of network
parameters. We believe that EGC bridges the gap between discriminative and
generative learning.
</p></li>
</ul>

<h3>Title: A Deep Multi-Modal Cyber-Attack Detection in Industrial Control Systems. (arXiv:2304.01440v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01440">http://arxiv.org/abs/2304.01440</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01440] A Deep Multi-Modal Cyber-Attack Detection in Industrial Control Systems](http://arxiv.org/abs/2304.01440) #attack</code></li>
<li>Summary: <p>The growing number of cyber-attacks against Industrial Control Systems (ICS)
in recent years has elevated security concerns due to the potential
catastrophic impact. Considering the complex nature of ICS, detecting a
cyber-attack in them is extremely challenging and requires advanced methods
that can harness multiple data modalities. This research utilizes network and
sensor modality data from ICS processed with a deep multi-modal cyber-attack
detection model for ICS. Results using the Secure Water Treatment (SWaT) system
show that the proposed model can outperform existing single modality models and
recent works in the literature by achieving 0.99 precision, 0.98 recall, and
0.98 f-measure, which shows the effectiveness of using both modalities in a
combined model for detecting cyber-attacks.
</p></li>
</ul>

<h3>Title: Side Channel-Assisted Inference Leakage from Machine Learning-based ECG Classification. (arXiv:2304.01990v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01990">http://arxiv.org/abs/2304.01990</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01990] Side Channel-Assisted Inference Leakage from Machine Learning-based ECG Classification](http://arxiv.org/abs/2304.01990) #attack</code></li>
<li>Summary: <p>The Electrocardiogram (ECG) measures the electrical cardiac activity
generated by the heart to detect abnormal heartbeat and heart attack. However,
the irregular occurrence of the abnormalities demands continuous monitoring of
heartbeats. Machine learning techniques are leveraged to automate the task to
reduce labor work needed during monitoring. In recent years, many companies
have launched products with ECG monitoring and irregular heartbeat alert. Among
all classification algorithms, the time series-based algorithm dynamic time
warping (DTW) is widely adopted to undertake the ECG classification task.
Though progress has been achieved, the DTW-based ECG classification also brings
a new attacking vector of leaking the patients' diagnosis results. This paper
shows that the ECG input samples' labels can be stolen via a side-channel
attack, Flush+Reload. In particular, we first identify the vulnerability of DTW
for ECG classification, i.e., the correlation between warping path choice and
prediction results. Then we implement an attack that leverages Flush+Reload to
monitor the warping path selection with known ECG data and then build a
predictor for constructing the relation between warping path selection and
labels of input ECG samples. Based on experiments, we find that the
Flush+Reload-based inference leakage can achieve an 84.0\% attacking success
rate to identify the labels of the two samples in DTW.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Online Distillation with Continual Learning for Cyclic Domain Shifts. (arXiv:2304.01239v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01239">http://arxiv.org/abs/2304.01239</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01239] Online Distillation with Continual Learning for Cyclic Domain Shifts](http://arxiv.org/abs/2304.01239) #robust</code></li>
<li>Summary: <p>In recent years, online distillation has emerged as a powerful technique for
adapting real-time deep neural networks on the fly using a slow, but accurate
teacher model. However, a major challenge in online distillation is
catastrophic forgetting when the domain shifts, which occurs when the student
model is updated with data from the new domain and forgets previously learned
knowledge. In this paper, we propose a solution to this issue by leveraging the
power of continual learning methods to reduce the impact of domain shifts.
Specifically, we integrate several state-of-the-art continual learning methods
in the context of online distillation and demonstrate their effectiveness in
reducing catastrophic forgetting. Furthermore, we provide a detailed analysis
of our proposed solution in the case of cyclic domain shifts. Our experimental
results demonstrate the efficacy of our approach in improving the robustness
and accuracy of online distillation, with potential applications in domains
such as video surveillance or autonomous driving. Overall, our work represents
an important step forward in the field of online distillation and continual
learning, with the potential to significantly impact real-world applications.
</p></li>
</ul>

<h3>Title: Accelerated parallel MRI using memory efficient and robust monotone operator learning (MOL). (arXiv:2304.01351v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01351">http://arxiv.org/abs/2304.01351</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01351] Accelerated parallel MRI using memory efficient and robust monotone operator learning (MOL)](http://arxiv.org/abs/2304.01351) #robust</code></li>
<li>Summary: <p>Model-based deep learning methods that combine imaging physics with learned
regularization priors have been emerging as powerful tools for parallel MRI
acceleration. The main focus of this paper is to determine the utility of the
monotone operator learning (MOL) framework in the parallel MRI setting. The MOL
algorithm alternates between a gradient descent step using a monotone
convolutional neural network (CNN) and a conjugate gradient algorithm to
encourage data consistency. The benefits of this approach include similar
guarantees as compressive sensing algorithms including uniqueness, convergence,
and stability, while being significantly more memory efficient than unrolled
methods. We validate the proposed scheme by comparing it with different
unrolled algorithms in the context of accelerated parallel MRI for static and
dynamic settings.
</p></li>
</ul>

<h3>Title: Functional Knowledge Transfer with Self-supervised Representation Learning. (arXiv:2304.01354v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01354">http://arxiv.org/abs/2304.01354</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01354] Functional Knowledge Transfer with Self-supervised Representation Learning](http://arxiv.org/abs/2304.01354) #robust</code></li>
<li>Summary: <p>This work investigates the unexplored usability of self-supervised
representation learning in the direction of functional knowledge transfer. In
this work, functional knowledge transfer is achieved by joint optimization of
self-supervised learning pseudo task and supervised learning task, improving
supervised learning task performance. Recent progress in self-supervised
learning uses a large volume of data, which becomes a constraint for its
applications on small-scale datasets. This work shares a simple yet effective
joint training framework that reinforces human-supervised task learning by
learning self-supervised representations just-in-time and vice versa.
Experiments on three public datasets from different visual domains, Intel
Image, CIFAR, and APTOS, reveal a consistent track of performance improvements
on classification tasks during joint optimization. Qualitative analysis also
supports the robustness of learnt representations. Source code and trained
models are available on GitHub.
</p></li>
</ul>

<h3>Title: Robust Outlier Rejection for 3D Registration with Variational Bayes. (arXiv:2304.01514v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01514">http://arxiv.org/abs/2304.01514</a></li>
<li>Code URL: <a href="https://github.com/jiang-hb/vbreg">https://github.com/jiang-hb/vbreg</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01514] Robust Outlier Rejection for 3D Registration with Variational Bayes](http://arxiv.org/abs/2304.01514) #robust</code></li>
<li>Summary: <p>Learning-based outlier (mismatched correspondence) rejection for robust 3D
registration generally formulates the outlier removal as an inlier/outlier
classification problem. The core for this to be successful is to learn the
discriminative inlier/outlier feature representations. In this paper, we
develop a novel variational non-local network-based outlier rejection framework
for robust alignment. By reformulating the non-local feature learning with
variational Bayesian inference, the Bayesian-driven long-range dependencies can
be modeled to aggregate discriminative geometric context information for
inlier/outlier distinction. Specifically, to achieve such Bayesian-driven
contextual dependencies, each query/key/value component in our non-local
network predicts a prior feature distribution and a posterior one. Embedded
with the inlier/outlier label, the posterior feature distribution is
label-dependent and discriminative. Thus, pushing the prior to be close to the
discriminative posterior in the training step enables the features sampled from
this prior at test time to model high-quality long-range dependencies. Notably,
to achieve effective posterior feature guidance, a specific probabilistic
graphical model is designed over our non-local model, which lets us derive a
variational low bound as our optimization objective for model training.
Finally, we propose a voting-based inlier searching strategy to cluster the
high-quality hypothetical inliers for transformation estimation. Extensive
experiments on 3DMatch, 3DLoMatch, and KITTI datasets verify the effectiveness
of our method.
</p></li>
</ul>

<h3>Title: A real-time algorithm for human action recognition in RGB and thermal video. (arXiv:2304.01567v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01567">http://arxiv.org/abs/2304.01567</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01567] A real-time algorithm for human action recognition in RGB and thermal video](http://arxiv.org/abs/2304.01567) #robust</code></li>
<li>Summary: <p>Monitoring the movement and actions of humans in video in real-time is an
important task. We present a deep learning based algorithm for human action
recognition for both RGB and thermal cameras. It is able to detect and track
humans and recognize four basic actions (standing, walking, running, lying) in
real-time on a notebook with a NVIDIA GPU. For this, it combines state of the
art components for object detection (Scaled YoloV4), optical flow (RAFT) and
pose estimation (EvoSkeleton). Qualitative experiments on a set of tunnel
videos show that the proposed algorithm works robustly for both RGB and thermal
video.
</p></li>
</ul>

<h3>Title: SC-ML: Self-supervised Counterfactual Metric Learning for Debiased Visual Question Answering. (arXiv:2304.01647v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01647">http://arxiv.org/abs/2304.01647</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01647] SC-ML: Self-supervised Counterfactual Metric Learning for Debiased Visual Question Answering](http://arxiv.org/abs/2304.01647) #robust</code></li>
<li>Summary: <p>Visual question answering (VQA) is a critical multimodal task in which an
agent must answer questions according to the visual cue. Unfortunately,
language bias is a common problem in VQA, which refers to the model generating
answers only by associating with the questions while ignoring the visual
content, resulting in biased results. We tackle the language bias problem by
proposing a self-supervised counterfactual metric learning (SC-ML) method to
focus the image features better. SC-ML can adaptively select the
question-relevant visual features to answer the question, reducing the negative
influence of question-irrelevant visual features on inferring answers. In
addition, question-irrelevant visual features can be seamlessly incorporated
into counterfactual training schemes to further boost robustness. Extensive
experiments have proved the effectiveness of our method with improved results
on the VQA-CP dataset. Our code will be made publicly available.
</p></li>
</ul>

<h3>Title: Fully Convolutional Networks for Dense Water Flow Intensity Prediction in Swedish Catchment Areas. (arXiv:2304.01658v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01658">http://arxiv.org/abs/2304.01658</a></li>
<li>Code URL: <a href="https://github.com/aleksispi/fcn-water-flow">https://github.com/aleksispi/fcn-water-flow</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01658] Fully Convolutional Networks for Dense Water Flow Intensity Prediction in Swedish Catchment Areas](http://arxiv.org/abs/2304.01658) #robust</code></li>
<li>Summary: <p>Intensifying climate change will lead to more extreme weather events,
including heavy rainfall and drought. Accurate stream flow prediction models
which are adaptable and robust to new circumstances in a changing climate will
be an important source of information for decisions on climate adaptation
efforts, especially regarding mitigation of the risks of and damages associated
with flooding. In this work we propose a machine learning-based approach for
predicting water flow intensities in inland watercourses based on the physical
characteristics of the catchment areas, obtained from geospatial data
(including elevation and soil maps, as well as satellite imagery), in addition
to temporal information about past rainfall quantities and temperature
variations. We target the one-day-ahead regime, where a fully convolutional
neural network model receives spatio-temporal inputs and predicts the water
flow intensity in every coordinate of the spatial input for the subsequent day.
To the best of our knowledge, we are the first to tackle the task of dense
water flow intensity prediction; earlier works have considered predicting flow
intensities at a sparse set of locations at a time. An extensive set of model
evaluations and ablations are performed, which empirically justify our various
design choices. Code and preprocessed data have been made publicly available at
https://github.com/aleksispi/fcn-water-flow.
</p></li>
</ul>

<h3>Title: Learning Invariant Representation via Contrastive Feature Alignment for Clutter Robust SAR Target Recognition. (arXiv:2304.01747v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01747">http://arxiv.org/abs/2304.01747</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01747] Learning Invariant Representation via Contrastive Feature Alignment for Clutter Robust SAR Target Recognition](http://arxiv.org/abs/2304.01747) #robust</code></li>
<li>Summary: <p>The deep neural networks (DNNs) have freed the synthetic aperture radar
automatic target recognition (SAR ATR) from expertise-based feature designing
and demonstrated superiority over conventional solutions. There has been shown
the unique deficiency of ground vehicle benchmarks in shapes of strong
background correlation results in DNNs overfitting the clutter and being
non-robust to unfamiliar surroundings. However, the gap between fixed
background model training and varying background application remains
underexplored. Inspired by contrastive learning, this letter proposes a
solution called Contrastive Feature Alignment (CFA) aiming to learn invariant
representation for robust recognition. The proposed method contributes a mixed
clutter variants generation strategy and a new inference branch equipped with
channel-weighted mean square error (CWMSE) loss for invariant representation
learning. In specific, the generation strategy is delicately designed to better
attract clutter-sensitive deviation in feature space. The CWMSE loss is further
devised to better contrast this deviation and align the deep features activated
by the original images and corresponding clutter variants. The proposed CFA
combines both classification and CWMSE losses to train the model jointly, which
allows for the progressive learning of invariant target representation.
Extensive evaluations on the MSTAR dataset and six DNN models prove the
effectiveness of our proposal. The results demonstrated that the CFA-trained
models are capable of recognizing targets among unfamiliar surroundings that
are not included in the dataset, and are robust to varying signal-to-clutter
ratios.
</p></li>
</ul>

<h3>Title: Randomized Adversarial Style Perturbations for Domain Generalization. (arXiv:2304.01959v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01959">http://arxiv.org/abs/2304.01959</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01959] Randomized Adversarial Style Perturbations for Domain Generalization](http://arxiv.org/abs/2304.01959) #robust</code></li>
<li>Summary: <p>We propose a novel domain generalization technique, referred to as Randomized
Adversarial Style Perturbation (RASP), which is motivated by the observation
that the characteristics of each domain are captured by the feature statistics
corresponding to style. The proposed algorithm perturbs the style of a feature
in an adversarial direction towards a randomly selected class, and makes the
model learn against being misled by the unexpected styles observed in unseen
target domains. While RASP is effective to handle domain shifts, its naive
integration into the training procedure might degrade the capability of
learning knowledge from source domains because it has no restriction on the
perturbations of representations. This challenge is alleviated by Normalized
Feature Mixup (NFM), which facilitates the learning of the original features
while achieving robustness to perturbed representations via their mixup during
training. We evaluate the proposed algorithm via extensive experiments on
various benchmarks and show that our approach improves domain generalization
performance, especially in large-scale benchmarks.
</p></li>
</ul>

<h3>Title: Revisiting the Evaluation of Image Synthesis with GANs. (arXiv:2304.01999v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01999">http://arxiv.org/abs/2304.01999</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01999] Revisiting the Evaluation of Image Synthesis with GANs](http://arxiv.org/abs/2304.01999) #robust</code></li>
<li>Summary: <p>A good metric, which promises a reliable comparison between solutions, is
essential to a well-defined task. Unlike most vision tasks that have per-sample
ground-truth, image synthesis targets generating \emph{unseen} data and hence
is usually evaluated with a distributional distance between one set of real
samples and another set of generated samples. This work provides an empirical
study on the evaluation of synthesis performance by taking the popular
generative adversarial networks (GANs) as a representative of generative
models. In particular, we make in-depth analyses on how to represent a data
point in the feature space, how to calculate a fair distance using selected
samples, and how many instances to use from each set. Experiments on multiple
datasets and settings suggest that (1) a group of models including both
CNN-based and ViT-based architectures serve as reliable and robust feature
extractors, (2) Centered Kernel Alignment (CKA) enables better comparison
across various extractors and hierarchical layers in one model, and (3) CKA
shows satisfactory sample efficiency and complements existing metrics
(\textit{e.g.}, FID) in characterizing the similarity between two internal data
correlations. These findings help us design a new measurement system, based on
which we re-evaluate the state-of-the-art generative models in a consistent and
reliable way.
</p></li>
</ul>

<h3>Title: MonoHuman: Animatable Human Neural Field from Monocular Video. (arXiv:2304.02001v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.02001">http://arxiv.org/abs/2304.02001</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.02001] MonoHuman: Animatable Human Neural Field from Monocular Video](http://arxiv.org/abs/2304.02001) #robust</code></li>
<li>Summary: <p>Animating virtual avatars with free-view control is crucial for various
applications like virtual reality and digital entertainment. Previous studies
have attempted to utilize the representation power of the neural radiance field
(NeRF) to reconstruct the human body from monocular videos. Recent works
propose to graft a deformation network into the NeRF to further model the
dynamics of the human neural field for animating vivid human motions. However,
such pipelines either rely on pose-dependent representations or fall short of
motion coherency due to frame-independent optimization, making it difficult to
generalize to unseen pose sequences realistically. In this paper, we propose a
novel framework MonoHuman, which robustly renders view-consistent and
high-fidelity avatars under arbitrary novel poses. Our key insight is to model
the deformation field with bi-directional constraints and explicitly leverage
the off-the-peg keyframe information to reason the feature correlations for
coherent results. Specifically, we first propose a Shared Bidirectional
Deformation module, which creates a pose-independent generalizable deformation
field by disentangling backward and forward deformation correspondences into
shared skeletal motion weight and separate non-rigid motions. Then, we devise a
Forward Correspondence Search module, which queries the correspondence feature
of keyframes to guide the rendering network. The rendered results are thus
multi-view consistent with high fidelity, even under challenging novel pose
settings. Extensive experiments demonstrate the superiority of our proposed
MonoHuman over state-of-the-art methods.
</p></li>
</ul>

<h3>Title: GlueStick: Robust Image Matching by Sticking Points and Lines Together. (arXiv:2304.02008v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.02008">http://arxiv.org/abs/2304.02008</a></li>
<li>Code URL: <a href="https://github.com/cvg/gluestick">https://github.com/cvg/gluestick</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.02008] GlueStick: Robust Image Matching by Sticking Points and Lines Together](http://arxiv.org/abs/2304.02008) #robust</code></li>
<li>Summary: <p>Line segments are powerful features complementary to points. They offer
structural cues, robust to drastic viewpoint and illumination changes, and can
be present even in texture-less areas. However, describing and matching them is
more challenging compared to points due to partial occlusions, lack of texture,
or repetitiveness. This paper introduces a new matching paradigm, where points,
lines, and their descriptors are unified into a single wireframe structure. We
propose GlueStick, a deep matching Graph Neural Network (GNN) that takes two
wireframes from different images and leverages the connectivity information
between nodes to better glue them together. In addition to the increased
efficiency brought by the joint matching, we also demonstrate a large boost of
performance when leveraging the complementary nature of these two features in a
single architecture. We show that our matching strategy outperforms the
state-of-the-art approaches independently matching line segments and points for
a wide variety of datasets and tasks. The code is available at
https://github.com/cvg/GlueStick.
</p></li>
</ul>

<h3>Title: Attribute-Consistent Knowledge Graph Representation Learning for Multi-Modal Entity Alignment. (arXiv:2304.01563v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01563">http://arxiv.org/abs/2304.01563</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01563] Attribute-Consistent Knowledge Graph Representation Learning for Multi-Modal Entity Alignment](http://arxiv.org/abs/2304.01563) #robust</code></li>
<li>Summary: <p>The multi-modal entity alignment (MMEA) aims to find all equivalent entity
pairs between multi-modal knowledge graphs (MMKGs). Rich attributes and
neighboring entities are valuable for the alignment task, but existing works
ignore contextual gap problems that the aligned entities have different numbers
of attributes on specific modality when learning entity representations. In
this paper, we propose a novel attribute-consistent knowledge graph
representation learning framework for MMEA (ACK-MMEA) to compensate the
contextual gaps through incorporating consistent alignment knowledge.
Attribute-consistent KGs (ACKGs) are first constructed via multi-modal
attribute uniformization with merge and generate operators so that each entity
has one and only one uniform feature in each modality. The ACKGs are then fed
into a relation-aware graph neural network with random dropouts, to obtain
aggregated relation representations and robust entity representations. In order
to evaluate the ACK-MMEA facilitated for entity alignment, we specially design
a joint alignment loss for both entity and attribute evaluation. Extensive
experiments conducted on two benchmark datasets show that our approach achieves
excellent performance compared to its competitors.
</p></li>
</ul>

<h3>Title: A Survey on Contextualised Semantic Shift Detection. (arXiv:2304.01666v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01666">http://arxiv.org/abs/2304.01666</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01666] A Survey on Contextualised Semantic Shift Detection](http://arxiv.org/abs/2304.01666) #robust</code></li>
<li>Summary: <p>Semantic Shift Detection (SSD) is the task of identifying, interpreting, and
assessing the possible change over time in the meanings of a target word.
Traditionally, SSD has been addressed by linguists and social scientists
through manual and time-consuming activities. In the recent years,
computational approaches based on Natural Language Processing and word
embeddings gained increasing attention to automate SSD as much as possible. In
particular, over the past three years, significant advancements have been made
almost exclusively based on word contextualised embedding models, which can
handle the multiple usages/meanings of the words and better capture the related
semantic shifts. In this paper, we survey the approaches based on
contextualised embeddings for SSD (i.e., CSSDetection) and we propose a
classification framework characterised by meaning representation,
time-awareness, and learning modality dimensions. The framework is exploited i)
to review the measures for shift assessment, ii) to compare the approaches on
performance, and iii) to discuss the current issues in terms of scalability,
interpretability, and robustness. Open challenges and future research
directions about CSSDetection are finally outlined.
</p></li>
</ul>

<h3>Title: San-BERT: Extractive Summarization for Sanskrit Documents using BERT and it's variants. (arXiv:2304.01894v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01894">http://arxiv.org/abs/2304.01894</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01894] San-BERT: Extractive Summarization for Sanskrit Documents using BERT and it's variants](http://arxiv.org/abs/2304.01894) #robust</code></li>
<li>Summary: <p>In this work, we develop language models for the Sanskrit language, namely
Bidirectional Encoder Representations from Transformers (BERT) and its
variants: A Lite BERT (ALBERT), and Robustly Optimized BERT (RoBERTa) using
Devanagari Sanskrit text corpus. Then we extracted the features for the given
text from these models. We applied the dimensional reduction and clustering
techniques on the features to generate an extractive summary for a given
Sanskrit document. Along with the extractive text summarization techniques, we
have also created and released a Sanskrit Devanagari text corpus publicly.
</p></li>
</ul>

<h3>Title: A Guide for Practical Use of ADMG Causal Data Augmentation. (arXiv:2304.01237v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01237">http://arxiv.org/abs/2304.01237</a></li>
<li>Code URL: <a href="https://github.com/audreypoinsot/admg_data_augmentation">https://github.com/audreypoinsot/admg_data_augmentation</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01237] A Guide for Practical Use of ADMG Causal Data Augmentation](http://arxiv.org/abs/2304.01237) #robust</code></li>
<li>Summary: <p>Data augmentation is essential when applying Machine Learning in small-data
regimes. It generates new samples following the observed data distribution
while increasing their diversity and variability to help researchers and
practitioners improve their models' robustness and, thus, deploy them in the
real world. Nevertheless, its usage in tabular data still needs to be improved,
as prior knowledge about the underlying data mechanism is seldom considered,
limiting the fidelity and diversity of the generated data. Causal data
augmentation strategies have been pointed out as a solution to handle these
challenges by relying on conditional independence encoded in a causal graph. In
this context, this paper experimentally analyzed the ADMG causal augmentation
method considering different settings to support researchers and practitioners
in understanding under which conditions prior knowledge helps generate new data
points and, consequently, enhances the robustness of their models. The results
highlighted that the studied method (a) is independent of the underlying model
mechanism, (b) requires a minimal number of observations that may be
challenging in a small-data regime to improve an ML model's accuracy, (c)
propagates outliers to the augmented set degrading the performance of the
model, and (d) is sensitive to its hyperparameter's value.
</p></li>
</ul>

<h3>Title: RARE: Robust Masked Graph Autoencoder. (arXiv:2304.01507v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01507">http://arxiv.org/abs/2304.01507</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01507] RARE: Robust Masked Graph Autoencoder](http://arxiv.org/abs/2304.01507) #robust</code></li>
<li>Summary: <p>Masked graph autoencoder (MGAE) has emerged as a promising self-supervised
graph pre-training (SGP) paradigm due to its simplicity and effectiveness.
However, existing efforts perform the mask-then-reconstruct operation in the
raw data space as is done in computer vision (CV) and natural language
processing (NLP) areas, while neglecting the important non-Euclidean property
of graph data. As a result, the highly unstable local connection structures
largely increase the uncertainty in inferring masked data and decrease the
reliability of the exploited self-supervision signals, leading to inferior
representations for downstream evaluations. To address this issue, we propose a
novel SGP method termed Robust mAsked gRaph autoEncoder (RARE) to improve the
certainty in inferring masked data and the reliability of the self-supervision
mechanism by further masking and reconstructing node samples in the high-order
latent feature space. Through both theoretical and empirical analyses, we have
discovered that performing a joint mask-then-reconstruct strategy in both
latent feature and raw data spaces could yield improved stability and
performance. To this end, we elaborately design a masked latent feature
completion scheme, which predicts latent features of masked nodes under the
guidance of high-order sample correlations that are hard to be observed from
the raw data perspective. Specifically, we first adopt a latent feature
predictor to predict the masked latent features from the visible ones. Next, we
encode the raw data of masked samples with a momentum graph encoder and
subsequently employ the resulting representations to improve predicted results
through latent feature matching. Extensive experiments on seventeen datasets
have demonstrated the effectiveness and robustness of RARE against
state-of-the-art (SOTA) competitors across three downstream tasks.
</p></li>
</ul>

<h3>Title: Multimodal Neural Processes for Uncertainty Estimation. (arXiv:2304.01518v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01518">http://arxiv.org/abs/2304.01518</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01518] Multimodal Neural Processes for Uncertainty Estimation](http://arxiv.org/abs/2304.01518) #robust</code></li>
<li>Summary: <p>Neural processes (NPs) have brought the representation power of parametric
deep neural networks and the reliable uncertainty estimation of non-parametric
Gaussian processes together. Although recent development of NPs has shown
success in both regression and classification, how to adapt NPs to multimodal
data has not be carefully studied. For the first time, we propose a new model
of NP family for multimodal uncertainty estimation, namely Multimodal Neural
Processes. In a holistic and principled way, we develop a dynamic context
memory updated by the classification error, a multimodal Bayesian aggregation
mechanism to aggregate multimodal representations, and a new attention
mechanism for calibrated predictions. In extensive empirical evaluation, our
method achieves the state-of-the-art multimodal uncertainty estimation
performance, showing its appealing ability of being robust against noisy
samples and reliable in out-of-domain detection.
</p></li>
</ul>

<h3>Title: Incremental Verification of Neural Networks. (arXiv:2304.01874v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01874">http://arxiv.org/abs/2304.01874</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01874] Incremental Verification of Neural Networks](http://arxiv.org/abs/2304.01874) #robust</code></li>
<li>Summary: <p>Complete verification of deep neural networks (DNNs) can exactly determine
whether the DNN satisfies a desired trustworthy property (e.g., robustness,
fairness) on an infinite set of inputs or not. Despite the tremendous progress
to improve the scalability of complete verifiers over the years on individual
DNNs, they are inherently inefficient when a deployed DNN is updated to improve
its inference speed or accuracy. The inefficiency is because the expensive
verifier needs to be run from scratch on the updated DNN. To improve
efficiency, we propose a new, general framework for incremental and complete
DNN verification based on the design of novel theory, data structure, and
algorithms. Our contributions implemented in a tool named IVAN yield an overall
geometric mean speedup of 2.4x for verifying challenging MNIST and CIFAR10
classifiers and a geometric mean speedup of 3.8x for the ACAS-XU classifiers
over the state-of-the-art baselines.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Multi-Channel Time-Series Person and Soft-Biometric Identification. (arXiv:2304.01585v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01585">http://arxiv.org/abs/2304.01585</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01585] Multi-Channel Time-Series Person and Soft-Biometric Identification](http://arxiv.org/abs/2304.01585) #biometric</code></li>
<li>Summary: <p>Multi-channel time-series datasets are popular in the context of human
activity recognition (HAR). On-body device (OBD) recordings of human movements
are often preferred for HAR applications not only for their reliability but as
an approach for identity protection, e.g., in industrial settings.
Contradictory, the gait activity is a biometric, as the cyclic movement is
distinctive and collectable. In addition, the gait cycle has proven to contain
soft-biometric information of human groups, such as age and height. Though
general human movements have not been considered a biometric, they might
contain identity information. This work investigates person and soft-biometrics
identification from OBD recordings of humans performing different activities
using deep architectures. Furthermore, we propose the use of attribute
representation for soft-biometric identification. We evaluate the method on
four datasets of multi-channel time-series HAR, measuring the performance of a
person and soft-biometrics identification and its relation concerning performed
activities. We find that person identification is not limited to gait activity.
The impact of activities on the identification performance was found to be
training and dataset specific. Soft-biometric based attribute representation
shows promising results and emphasis the necessity of larger datasets.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Parkinsons Disease Detection via Resting-State Electroencephalography Using Signal Processing and Machine Learning Techniques. (arXiv:2304.01214v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01214">http://arxiv.org/abs/2304.01214</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01214] Parkinsons Disease Detection via Resting-State Electroencephalography Using Signal Processing and Machine Learning Techniques](http://arxiv.org/abs/2304.01214) #extraction</code></li>
<li>Summary: <p>Parkinsons Disease (PD) is a neurodegenerative disorder resulting in motor
deficits due to advancing degeneration of dopaminergic neurons. PD patients
report experiencing tremor, rigidity, visual impairment, bradykinesia, and
several cognitive deficits. Although Electroencephalography (EEG) indicates
abnormalities in PD patients, one major challenge is the lack of a consistent,
accurate, and systemic biomarker for PD in order to closely monitor the disease
with therapeutic treatments and medication. In this study, we collected
Electroencephalographic data from 15 PD patients and 16 Healthy Controls (HC).
We first preprocessed every EEG signal using several techniques and extracted
relevant features using many feature extraction algorithms. Afterwards, we
applied several machine learning algorithms to classify PD versus HC. We found
the most significant metrics to be achieved by the Random Forest ensemble
learning approach, with an accuracy, precision, recall, F1 score, and AUC of
97.5%, 100%, 95%, 0.967, and 0.975, respectively. The results of this study
show promise for exposing PD abnormalities using EEG during clinical diagnosis,
and automating this process using signal processing techniques and ML
algorithms to evaluate the difference between healthy individuals and PD
patients.
</p></li>
</ul>

<h3>Title: Self-Supervised Image Denoising for Real-World Images with Context-aware Transformer. (arXiv:2304.01627v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01627">http://arxiv.org/abs/2304.01627</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01627] Self-Supervised Image Denoising for Real-World Images with Context-aware Transformer](http://arxiv.org/abs/2304.01627) #extraction</code></li>
<li>Summary: <p>In recent years, the development of deep learning has been pushing image
denoising to a new level. Among them, self-supervised denoising is increasingly
popular because it does not require any prior knowledge. Most of the existing
self-supervised methods are based on convolutional neural networks (CNN), which
are restricted by the locality of the receptive field and would cause color
shifts or textures loss. In this paper, we propose a novel Denoise Transformer
for real-world image denoising, which is mainly constructed with Context-aware
Denoise Transformer (CADT) units and Secondary Noise Extractor (SNE) block.
CADT is designed as a dual-branch structure, where the global branch uses a
window-based Transformer encoder to extract the global information, while the
local branch focuses on the extraction of local features with small receptive
field. By incorporating CADT as basic components, we build a hierarchical
network to directly learn the noise distribution information through residual
learning and obtain the first stage denoised output. Then, we design SNE in low
computation for secondary global noise extraction. Finally the blind spots are
collected from the Denoise Transformer output and reconstructed, forming the
final denoised image. Extensive experiments on the real-world SIDD benchmark
achieve 50.62/0.990 for PSNR/SSIM, which is competitive with the current
state-of-the-art method and only 0.17/0.001 lower. Visual comparisons on public
sRGB, Raw-RGB and greyscale datasets prove that our proposed Denoise
Transformer has a competitive performance, especially on blurred textures and
low-light images, without using additional knowledge, e.g., noise level or
noise type, regarding the underlying unknown noise.
</p></li>
</ul>

<h3>Title: DWA: Differential Wavelet Amplifier for Image Super-Resolution. (arXiv:2304.01994v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01994">http://arxiv.org/abs/2304.01994</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01994] DWA: Differential Wavelet Amplifier for Image Super-Resolution](http://arxiv.org/abs/2304.01994) #extraction</code></li>
<li>Summary: <p>This work introduces Differential Wavelet Amplifier (DWA), a drop-in module
for wavelet-based image Super-Resolution (SR). DWA invigorates an approach
recently receiving less attention, namely Discrete Wavelet Transformation
(DWT). DWT enables an efficient image representation for SR and reduces the
spatial area of its input by a factor of 4, the overall model size, and
computation cost, framing it as an attractive approach for sustainable ML. Our
proposed DWA model improves wavelet-based SR models by leveraging the
difference between two convolutional filters to refine relevant feature
extraction in the wavelet domain, emphasizing local contrasts and suppressing
common noise in the input signals. We show its effectiveness by integrating it
into existing SR models, e.g., DWSR and MWCNN, and demonstrate a clear
improvement in classical SR tasks. Moreover, DWA enables a direct application
of DWSR and MWCNN to input image space, reducing the DWT representation
channel-wise since it omits traditional DWT.
</p></li>
</ul>

<h3>Title: NPC: Neural Point Characters from Video. (arXiv:2304.02013v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.02013">http://arxiv.org/abs/2304.02013</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.02013] NPC: Neural Point Characters from Video](http://arxiv.org/abs/2304.02013) #extraction</code></li>
<li>Summary: <p>High-fidelity human 3D models can now be learned directly from videos,
typically by combining a template-based surface model with neural
representations. However, obtaining a template surface requires expensive
multi-view capture systems, laser scans, or strictly controlled conditions.
Previous methods avoid using a template but rely on a costly or ill-posed
mapping from observation to canonical space. We propose a hybrid point-based
representation for reconstructing animatable characters that does not require
an explicit surface model, while being generalizable to novel poses. For a
given video, our method automatically produces an explicit set of 3D points
representing approximate canonical geometry, and learns an articulated
deformation model that produces pose-dependent point transformations. The
points serve both as a scaffold for high-frequency neural features and an
anchor for efficiently mapping between observation and canonical space. We
demonstrate on established benchmarks that our representation overcomes
limitations of prior work operating in either canonical or in observation
space. Moreover, our automatic point extraction approach enables learning
models of human and animal characters alike, matching the performance of the
methods using rigged surface templates despite being more general. Project
website: https://lemonatsu.github.io/npc/
</p></li>
</ul>

<h3>Title: PromptORE -- A Novel Approach Towards Fully Unsupervised Relation Extraction. (arXiv:2304.01209v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01209">http://arxiv.org/abs/2304.01209</a></li>
<li>Code URL: <a href="https://github.com/alteca/promptore">https://github.com/alteca/promptore</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01209] PromptORE -- A Novel Approach Towards Fully Unsupervised Relation Extraction](http://arxiv.org/abs/2304.01209) #extraction</code></li>
<li>Summary: <p>Unsupervised Relation Extraction (RE) aims to identify relations between
entities in text, without having access to labeled data during training. This
setting is particularly relevant for domain specific RE where no annotated
dataset is available and for open-domain RE where the types of relations are a
priori unknown. Although recent approaches achieve promising results, they
heavily depend on hyperparameters whose tuning would most often require labeled
data. To mitigate the reliance on hyperparameters, we propose PromptORE, a
''Prompt-based Open Relation Extraction'' model. We adapt the novel
prompt-tuning paradigm to work in an unsupervised setting, and use it to embed
sentences expressing a relation. We then cluster these embeddings to discover
candidate relations, and we experiment different strategies to automatically
estimate an adequate number of clusters. To the best of our knowledge,
PromptORE is the first unsupervised RE model that does not need hyperparameter
tuning. Results on three general and specific domain datasets show that
PromptORE consistently outperforms state-of-the-art models with a relative gain
of more than 40% in B 3 , V-measure and ARI. Qualitative analysis also
indicates PromptORE's ability to identify semantically coherent clusters that
are very close to true relations.
</p></li>
</ul>

<h3>Title: Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach. (arXiv:2304.01240v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01240">http://arxiv.org/abs/2304.01240</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01240] Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach](http://arxiv.org/abs/2304.01240) #extraction</code></li>
<li>Summary: <p>Pain is a common reason for accessing healthcare resources and is a growing
area of research, especially in its overlap with mental health. Mental health
electronic health records are a good data source to study this overlap.
However, much information on pain is held in the free text of these records,
where mentions of pain present a unique natural language processing problem due
to its ambiguous nature. This project uses data from an anonymised mental
health electronic health records database. The data are used to train a machine
learning based classification algorithm to classify sentences as discussing
patient pain or not. This will facilitate the extraction of relevant pain
information from large databases, and the use of such outputs for further
studies on pain and mental health. 1,985 documents were manually
triple-annotated for creation of gold standard training data, which was used to
train three commonly used classification algorithms. The best performing model
achieved an F1-score of 0.98 (95% CI 0.98-0.99).
</p></li>
</ul>

<h3>Title: End-to-End Models for Chemical-Protein Interaction Extraction: Better Tokenization and Span-Based Pipeline Strategies. (arXiv:2304.01344v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01344">http://arxiv.org/abs/2304.01344</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01344] End-to-End Models for Chemical-Protein Interaction Extraction: Better Tokenization and Span-Based Pipeline Strategies](http://arxiv.org/abs/2304.01344) #extraction</code></li>
<li>Summary: <p>End-to-end relation extraction (E2ERE) is an important task in information
extraction, more so for biomedicine as scientific literature continues to grow
exponentially. E2ERE typically involves identifying entities (or named entity
recognition (NER)) and associated relations, while most RE tasks simply assume
that the entities are provided upfront and end up performing relation
classification. E2ERE is inherently more difficult than RE alone given the
potential snowball effect of errors from NER leading to more errors in RE. A
complex dataset in biomedical E2ERE is the ChemProt dataset (BioCreative VI,
2017) that identifies relations between chemical compounds and genes/proteins
in scientific literature. ChemProt is included in all recent biomedical natural
language processing benchmarks including BLUE, BLURB, and BigBio. However, its
treatment in these benchmarks and in other separate efforts is typically not
end-to-end, with few exceptions. In this effort, we employ a span-based
pipeline approach to produce a new state-of-the-art E2ERE performance on the
ChemProt dataset, resulting in $> 4\%$ improvement in F1-score over the prior
best effort. Our results indicate that a straightforward fine-grained
tokenization scheme helps span-based approaches excel in E2ERE, especially with
regards to handling complex named entities. Our error analysis also identifies
a few key failure modes in E2ERE for ChemProt.
</p></li>
</ul>

<h3>Title: Thematic context vector association based on event uncertainty for Twitter. (arXiv:2304.01423v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01423">http://arxiv.org/abs/2304.01423</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01423] Thematic context vector association based on event uncertainty for Twitter](http://arxiv.org/abs/2304.01423) #extraction</code></li>
<li>Summary: <p>Keyword extraction is a crucial process in text mining. The extraction of
keywords with respective contextual events in Twitter data is a big challenge.
The challenging issues are mainly because of the informality in the language
used. The use of misspelled words, acronyms, and ambiguous terms causes
informality. The extraction of keywords with informal language in current
systems is pattern based or event based. In this paper, contextual keywords are
extracted using thematic events with the help of data association. The thematic
context for events is identified using the uncertainty principle in the
proposed system. The thematic contexts are weighed with the help of vectors
called thematic context vectors which signifies the event as certain or
uncertain. The system is tested on the Twitter COVID-19 dataset and proves to
be effective. The system extracts event-specific thematic context vectors from
the test dataset and ranks them. The extracted thematic context vectors are
used for the clustering of contextual thematic vectors which improves the
silhouette coefficient by 0.5% than state of art methods namely TF and TF-IDF.
The thematic context vector can be used in other applications like
Cyberbullying, sarcasm detection, figurative language detection, etc.
</p></li>
</ul>

<h3>Title: EDeR: A Dataset for Exploring Dependency Relations Between Events. (arXiv:2304.01612v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01612">http://arxiv.org/abs/2304.01612</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01612] EDeR: A Dataset for Exploring Dependency Relations Between Events](http://arxiv.org/abs/2304.01612) #extraction</code></li>
<li>Summary: <p>Relation extraction is a central task in natural language processing (NLP)
and information retrieval (IR) research. We argue that an important type of
relation not explored in NLP or IR research to date is that of an event being
an argument - required or optional - of another event. We introduce the
human-annotated Event Dependency Relation dataset (EDeR) which provides this
dependency relation. The annotation is done on a sample of documents from the
OntoNotes dataset, which has the added benefit that it integrates with
existing, orthogonal, annotations of this dataset. We investigate baseline
approaches for predicting the event dependency relation, the best of which
achieves an accuracy of 82.61 for binary argument/non-argument classification.
We show that recognizing this relation leads to more accurate event extraction
(semantic role labelling) and can improve downstream tasks that depend on this,
such as co-reference resolution. Furthermore, we demonstrate that predicting
the three-way classification into the required argument, optional argument or
non-argument is a more challenging task.
</p></li>
</ul>

<h3>Title: Time-space-frequency feature Fusion for 3-channel motor imagery classification. (arXiv:2304.01461v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01461">http://arxiv.org/abs/2304.01461</a></li>
<li>Code URL: <a href="https://github.com/miaozhengqing/tsff">https://github.com/miaozhengqing/tsff</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01461] Time-space-frequency feature Fusion for 3-channel motor imagery classification](http://arxiv.org/abs/2304.01461) #extraction</code></li>
<li>Summary: <p>Low-channel EEG devices are crucial for portable and entertainment
applications. However, the low spatial resolution of EEG presents challenges in
decoding low-channel motor imagery. This study introduces TSFF-Net, a novel
network architecture that integrates time-space-frequency features, effectively
compensating for the limitations of single-mode feature extraction networks
based on time-series or time-frequency modalities. TSFF-Net comprises four main
components: time-frequency representation, time-frequency feature extraction,
time-space feature extraction, and feature fusion and classification.
Time-frequency representation and feature extraction transform raw EEG signals
into time-frequency spectrograms and extract relevant features. The time-space
network processes time-series EEG trials as input and extracts temporal-spatial
features. Feature fusion employs MMD loss to constrain the distribution of
time-frequency and time-space features in the Reproducing Kernel Hilbert Space,
subsequently combining these features using a weighted fusion approach to
obtain effective time-space-frequency features. Moreover, few studies have
explored the decoding of three-channel motor imagery based on time-frequency
spectrograms. This study proposes a shallow, lightweight decoding architecture
(TSFF-img) based on time-frequency spectrograms and compares its classification
performance in low-channel motor imagery with other methods using two publicly
available datasets. Experimental results demonstrate that TSFF-Net not only
compensates for the shortcomings of single-mode feature extraction networks in
EEG decoding, but also outperforms other state-of-the-art methods. Overall,
TSFF-Net offers considerable advantages in decoding low-channel motor imagery
and provides valuable insights for algorithmically enhancing low-channel EEG
decoding.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: FedBEVT: Federated Learning Bird's Eye View Perception Transformer in Road Traffic Systems. (arXiv:2304.01534v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01534">http://arxiv.org/abs/2304.01534</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01534] FedBEVT: Federated Learning Bird's Eye View Perception Transformer in Road Traffic Systems](http://arxiv.org/abs/2304.01534) #federate</code></li>
<li>Summary: <p>Bird's eye view (BEV) perception is becoming increasingly important in the
field of autonomous driving. It uses multi-view camera data to learn a
transformer model that directly projects the perception of the road environment
onto the BEV perspective. However, training a transformer model often requires
a large amount of data, and as camera data for road traffic is often private,
it is typically not shared. Federated learning offers a solution that enables
clients to collaborate and train models without exchanging data. In this paper,
we propose FedBEVT, a federated transformer learning approach for BEV
perception. We address two common data heterogeneity issues in FedBEVT: (i)
diverse sensor poses and (ii) varying sensor numbers in perception systems. We
present federated learning with camera-attentive personalization~(FedCaP) and
adaptive multi-camera masking~(AMCM) to enhance the performance in real-world
scenarios. To evaluate our method in real-world settings, we create a dataset
consisting of four typical federated use cases. Our findings suggest that
FedBEVT outperforms the baseline approaches in all four use cases,
demonstrating the potential of our approach for improving BEV perception in
autonomous driving. We will make all codes and data publicly available.
</p></li>
</ul>

<h3>Title: Personalized Federated Learning with Local Attention. (arXiv:2304.01783v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01783">http://arxiv.org/abs/2304.01783</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01783] Personalized Federated Learning with Local Attention](http://arxiv.org/abs/2304.01783) #federate</code></li>
<li>Summary: <p>Federated Learning (FL) aims to learn a single global model that enables the
central server to help the model training in local clients without accessing
their local data. The key challenge of FL is the heterogeneity of local data in
different clients, such as heterogeneous label distribution and feature shift,
which could lead to significant performance degradation of the learned models.
Although many studies have been proposed to address the heterogeneous label
distribution problem, few studies attempt to explore the feature shift issue.
To address this issue, we propose a simple yet effective algorithm, namely
\textbf{p}ersonalized \textbf{Fed}erated learning with \textbf{L}ocal
\textbf{A}ttention (pFedLA), by incorporating the attention mechanism into
personalized models of clients while keeping the attention blocks
client-specific. Specifically, two modules are proposed in pFedLA, i.e., the
personalized single attention module and the personalized hybrid attention
module. In addition, the proposed pFedLA method is quite flexible and general
as it can be incorporated into any FL method to improve their performance
without introducing additional communication costs. Extensive experiments
demonstrate that the proposed pFedLA method can boost the performance of
state-of-the-art FL methods on different tasks such as image classification and
object detection tasks.
</p></li>
</ul>

<h3>Title: MP-FedCL: Multi-Prototype Federated Contrastive Learning for Edge Intelligence. (arXiv:2304.01950v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01950">http://arxiv.org/abs/2304.01950</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01950] MP-FedCL: Multi-Prototype Federated Contrastive Learning for Edge Intelligence](http://arxiv.org/abs/2304.01950) #federate</code></li>
<li>Summary: <p>Federated learning-assisted edge intelligence enables privacy protection in
modern intelligent services. However, not Independent and Identically
Distributed (non-IID) distribution among edge clients can impair the local
model performance. The existing single prototype-based strategy represents a
sample by using the mean of the feature space. However, feature spaces are
usually not clustered, and a single prototype may not represent a sample well.
Motivated by this, this paper proposes a multi-prototype federated contrastive
learning approach (MP-FedCL) which demonstrates the effectiveness of using a
multi-prototype strategy over a single-prototype under non-IID settings,
including both label and feature skewness. Specifically, a multi-prototype
computation strategy based on \textit{k-means} is first proposed to capture
different embedding representations for each class space, using multiple
prototypes ($k$ centroids) to represent a class in the embedding space. In each
global round, the computed multiple prototypes and their respective model
parameters are sent to the edge server for aggregation into a global prototype
pool, which is then sent back to all clients to guide their local training.
Finally, local training for each client minimizes their own supervised learning
tasks and learns from shared prototypes in the global prototype pool through
supervised contrastive learning, which encourages them to learn knowledge
related to their own class from others and reduces the absorption of unrelated
knowledge in each global iteration. Experimental results on MNIST, Digit-5,
Office-10, and DomainNet show that our method outperforms multiple baselines,
with an average test accuracy improvement of about 4.6\% and 10.4\% under
feature and label non-IID distributions, respectively.
</p></li>
</ul>

<h3>Title: A Survey on Vertical Federated Learning: From a Layered Perspective. (arXiv:2304.01829v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01829">http://arxiv.org/abs/2304.01829</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01829] A Survey on Vertical Federated Learning: From a Layered Perspective](http://arxiv.org/abs/2304.01829) #federate</code></li>
<li>Summary: <p>Vertical federated learning (VFL) is a promising category of federated
learning for the scenario where data is vertically partitioned and distributed
among parties. VFL enriches the description of samples using features from
different parties to improve model capacity. Compared with horizontal federated
learning, in most cases, VFL is applied in the commercial cooperation scenario
of companies. Therefore, VFL contains tremendous business values. In the past
few years, VFL has attracted more and more attention in both academia and
industry. In this paper, we systematically investigate the current work of VFL
from a layered perspective. From the hardware layer to the vertical federated
system layer, researchers contribute to various aspects of VFL. Moreover, the
application of VFL has covered a wide range of areas, e.g., finance,
healthcare, etc. At each layer, we categorize the existing work and explore the
challenges for the convenience of further research and development of VFL.
Especially, we design a novel MOSP tree taxonomy to analyze the core component
of VFL, i.e., secure vertical federated machine learning algorithm. Our
taxonomy considers four dimensions, i.e., machine learning model (M),
protection object (O), security model (S), and privacy-preserving protocol (P),
and provides a comprehensive investigation.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Exploration of Lightweight Single Image Denoising with Transformers and Truly Fair Training. (arXiv:2304.01805v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01805">http://arxiv.org/abs/2304.01805</a></li>
<li>Code URL: <a href="https://github.com/rami0205/lwdn">https://github.com/rami0205/lwdn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01805] Exploration of Lightweight Single Image Denoising with Transformers and Truly Fair Training](http://arxiv.org/abs/2304.01805) #fair</code></li>
<li>Summary: <p>As multimedia content often contains noise from intrinsic defects of digital
devices, image denoising is an important step for high-level vision recognition
tasks. Although several studies have developed the denoising field employing
advanced Transformers, these networks are too momory-intensive for real-world
applications. Additionally, there is a lack of research on lightweight denosing
(LWDN) with Transformers. To handle this, this work provides seven comparative
baseline Transformers for LWDN, serving as a foundation for future research. We
also demonstrate the parts of randomly cropped patches significantly affect the
denoising performances during training. While previous studies have overlooked
this aspect, we aim to train our baseline Transformers in a truly fair manner.
Furthermore, we conduct empirical analyses of various components to determine
the key considerations for constructing LWDN Transformers. Codes are available
at https://github.com/rami0205/LWDN.
</p></li>
</ul>

<h3>Title: Fair Evaluation of Graph Markov Neural Networks. (arXiv:2304.01235v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01235">http://arxiv.org/abs/2304.01235</a></li>
<li>Code URL: <a href="https://github.com/toinesayan/fair-evaluation-of-gmnns">https://github.com/toinesayan/fair-evaluation-of-gmnns</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01235] Fair Evaluation of Graph Markov Neural Networks](http://arxiv.org/abs/2304.01235) #fair</code></li>
<li>Summary: <p>Graph Markov Neural Networks (GMNN) have recently been proposed to improve
regular graph neural networks (GNN) by including label dependencies into the
semi-supervised node classification task. GMNNs do this in a theoretically
principled way and use three kinds of information to predict labels. Just like
ordinary GNNs, they use the node features and the graph structure but they
moreover leverage information from the labels of neighboring nodes to improve
the accuracy of their predictions. In this paper, we introduce a new dataset
named WikiVitals which contains a graph of 48k mutually referred Wikipedia
articles classified into 32 categories and connected by 2.3M edges. Our aim is
to rigorously evaluate the contributions of three distinct sources of
information to the prediction accuracy of GMNN for this dataset: the content of
the articles, their connections with each other and the correlations among
their labels. For this purpose we adapt a method which was recently proposed
for performing fair comparisons of GNN performance using an appropriate
randomization over partitions and a clear separation of model selection and
model assessment.
</p></li>
</ul>

<h3>Title: Counterfactual Learning on Graphs: A Survey. (arXiv:2304.01391v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01391">http://arxiv.org/abs/2304.01391</a></li>
<li>Code URL: <a href="https://github.com/timelovercc/awesome-graph-causal-learning">https://github.com/timelovercc/awesome-graph-causal-learning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01391] Counterfactual Learning on Graphs: A Survey](http://arxiv.org/abs/2304.01391) #fair</code></li>
<li>Summary: <p>Graph-structured data are pervasive in the real-world such as social
networks, molecular graphs and transaction networks. Graph neural networks
(GNNs) have achieved great success in representation learning on graphs,
facilitating various downstream tasks. However, GNNs have several drawbacks
such as lacking interpretability, can easily inherit the bias of the training
data and cannot model the casual relations. Recently, counterfactual learning
on graphs has shown promising results in alleviating these drawbacks. Various
graph counterfactual learning approaches have been proposed for counterfactual
fairness, explainability, link prediction and other applications on graphs. To
facilitate the development of this promising direction, in this survey, we
categorize and comprehensively review papers on graph counterfactual learning.
We divide existing methods into four categories based on research problems
studied. For each category, we provide background and motivating examples, a
general framework summarizing existing works and a detailed review of these
works. We point out promising future research directions at the intersection of
graph-structured data, counterfactual learning, and real-world applications. To
offer a comprehensive view of resources for future studies, we compile a
collection of open-source implementations, public datasets, and commonly-used
evaluation metrics. This survey aims to serve as a ``one-stop-shop'' for
building a unified understanding of graph counterfactual learning categories
and current resources. We also maintain a repository for papers and resources
and will keep updating the repository
https://github.com/TimeLovercc/Awesome-Graph-Causal-Learning.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: An interpretability framework for Similar case matching. (arXiv:2304.01622v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01622">http://arxiv.org/abs/2304.01622</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01622] An interpretability framework for Similar case matching](http://arxiv.org/abs/2304.01622) #interpretability</code></li>
<li>Summary: <p>Similar Case Matching (SCM) is designed to determine whether two cases are
similar. The task has an essential role in the legal system, helping legal
professionals to find relevant cases quickly and thus deal with them more
efficiently. Existing research has focused on improving the model's performance
but not on its interpretability. Therefore, this paper proposes a pipeline
framework for interpretable SCM, which consists of four modules: a judicial
feature sentence identification module, a case matching module, a feature
sentence alignment module, and a conflict disambiguation module. Unlike
existing SCM methods, our framework will identify feature sentences in a case
that contain essential information, perform similar case matching based on the
extracted feature sentence results, and align the feature sentences in the two
cases to provide evidence for the similarity of the cases. SCM results may
conflict with feature sentence alignment results, and our framework further
disambiguates against this inconsistency. The experimental results show the
effectiveness of our framework, and our work provides a new benchmark for
interpretable SCM.
</p></li>
</ul>

<h3>Title: Multidimensional Perceptron for Efficient and Explainable Long Text Classification. (arXiv:2304.01638v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01638">http://arxiv.org/abs/2304.01638</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01638] Multidimensional Perceptron for Efficient and Explainable Long Text Classification](http://arxiv.org/abs/2304.01638) #interpretability</code></li>
<li>Summary: <p>Because of the inevitable cost and complexity of transformer and pre-trained
models, efficiency concerns are raised for long text classification. Meanwhile,
in the highly sensitive domains, e.g., healthcare and legal long-text mining,
potential model distrust, yet underrated and underexplored, may hatch vital
apprehension. Existing methods generally segment the long text, encode each
piece with the pre-trained model, and use attention or RNNs to obtain long text
representation for classification. In this work, we propose a simple but
effective model, Segment-aWare multIdimensional PErceptron (SWIPE), to replace
attention/RNNs in the above framework. Unlike prior efforts, SWIPE can
effectively learn the label of the entire text with supervised training, while
perceive the labels of the segments and estimate their contributions to the
long-text labeling in an unsupervised manner. As a general classifier, SWIPE
can endorse different encoders, and it outperforms SOTA models in terms of
classification accuracy and model efficiency. It is noteworthy that SWIPE
achieves superior interpretability to transparentize long text classification
results.
</p></li>
</ul>

<h3>Title: Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01890">http://arxiv.org/abs/2304.01890</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01890] Sociocultural knowledge is needed for selection of shots in hate speech detection tasks](http://arxiv.org/abs/2304.01890) #interpretability</code></li>
<li>Summary: <p>We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for
the countries of Brazil, Germany, India and Kenya, to aid training and
interpretability of models. We demonstrate how our lexicon can be used to
interpret model predictions, showing that models developed to classify extreme
speech rely heavily on target words when making predictions. Further, we
propose a method to aid shot selection for training in low-resource settings
via HATELEXICON. In few-shot learning, the selection of shots is of paramount
importance to model performance. In our work, we simulate a few-shot setting
for German and Hindi, using HASOC data for training and the Multilingual
HateCheck (MHC) as a benchmark. We show that selecting shots based on our
lexicon leads to models performing better on MHC than models trained on shots
sampled randomly. Thus, when given only a few training examples, using our
lexicon to select shots containing more sociocultural information leads to
better few-shot performance.
</p></li>
</ul>

<h3>Title: On the Prime Number Divisibility by Deep Learning. (arXiv:2304.01333v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01333">http://arxiv.org/abs/2304.01333</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01333] On the Prime Number Divisibility by Deep Learning](http://arxiv.org/abs/2304.01333) #interpretability</code></li>
<li>Summary: <p>Certain tasks such as determining whether a given integer can be divided by
2, 3, or other prime numbers may be trivial for human beings, but can be less
straightforward for computers in the absence of pre-specified algorithms. In
this paper, we tested multiple deep learning architectures and feature
engineering approaches, and evaluated the scenario of determining divisibility
of large finite integers (up to $2^{32}$) by small prime numbers. It turns out
that, regardless of the network frameworks or the complexity of the network
structures (CNN, RNN, Transformer, etc.), the ability to predict the prime
number divisibility critically depends on the feature space fed into the deep
learning models. We also evaluated commercially available Automated Machine
Learning (AutoML) pipelines from Amazon, Google and Microsoft, and demonstrated
that they failed to address this issue unless appropriately engineered features
were provided. We further proposed a closed form solution to the problem using
the ordinary linear regression on Fourier series basis vectors, and showed its
success. Finally, we evaluated prompt-based learning using ChatGPT and
demonstrated its success on small primes and apparent failures on larger
primes. We conclude that feature engineering remains an important task to
improve the performance, increase the interpretability, and reduce the
complexity of machine learning/deep learning models, even in the era of AutoML
and large-language models (LLMs).
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Generative Diffusion Prior for Unified Image Restoration and Enhancement. (arXiv:2304.01247v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01247">http://arxiv.org/abs/2304.01247</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01247] Generative Diffusion Prior for Unified Image Restoration and Enhancement](http://arxiv.org/abs/2304.01247) #diffusion</code></li>
<li>Summary: <p>Existing image restoration methods mostly leverage the posterior distribution
of natural images. However, they often assume known degradation and also
require supervised training, which restricts their adaptation to complex real
applications. In this work, we propose the Generative Diffusion Prior (GDP) to
effectively model the posterior distributions in an unsupervised sampling
manner. GDP utilizes a pre-train denoising diffusion generative model (DDPM)
for solving linear inverse, non-linear, or blind problems. Specifically, GDP
systematically explores a protocol of conditional guidance, which is verified
more practical than the commonly used guidance way. Furthermore, GDP is
strength at optimizing the parameters of degradation model during the denoising
process, achieving blind image restoration. Besides, we devise hierarchical
guidance and patch-based methods, enabling the GDP to generate images of
arbitrary resolutions. Experimentally, we demonstrate GDP's versatility on
several image datasets for linear problems, such as super-resolution,
deblurring, inpainting, and colorization, as well as non-linear and blind
issues, such as low-light enhancement and HDR image recovery. GDP outperforms
the current leading unsupervised methods on the diverse benchmarks in
reconstruction quality and perceptual quality. Moreover, GDP also generalizes
well for natural images or synthesized images with arbitrary sizes from various
tasks out of the distribution of the ImageNet training set.
</p></li>
</ul>

<h3>Title: Text-Conditioned Sampling Framework for Text-to-Image Generation with Masked Generative Models. (arXiv:2304.01515v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01515">http://arxiv.org/abs/2304.01515</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01515] Text-Conditioned Sampling Framework for Text-to-Image Generation with Masked Generative Models](http://arxiv.org/abs/2304.01515) #diffusion</code></li>
<li>Summary: <p>Token-based masked generative models are gaining popularity for their fast
inference time with parallel decoding. While recent token-based approaches
achieve competitive performance to diffusion-based models, their generation
performance is still suboptimal as they sample multiple tokens simultaneously
without considering the dependence among them. We empirically investigate this
problem and propose a learnable sampling model, Text-Conditioned Token
Selection (TCTS), to select optimal tokens via localized supervision with text
information. TCTS improves not only the image quality but also the semantic
alignment of the generated images with the given texts. To further improve the
image quality, we introduce a cohesive sampling strategy, Frequency Adaptive
Sampling (FAS), to each group of tokens divided according to the self-attention
maps. We validate the efficacy of TCTS combined with FAS with various
generative tasks, demonstrating that it significantly outperforms the baselines
in image-text alignment and image quality. Our text-conditioned sampling
framework further reduces the original inference time by more than 50% without
modifying the original generative model.
</p></li>
</ul>

<h3>Title: A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material. (arXiv:2304.01565v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01565">http://arxiv.org/abs/2304.01565</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01565] A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material](http://arxiv.org/abs/2304.01565) #diffusion</code></li>
<li>Summary: <p>Diffusion models have become a new SOTA generative modeling method in various
fields, for which there are multiple survey works that provide an overall
survey. With the number of articles on diffusion models increasing
exponentially in the past few years, there is an increasing need for surveys of
diffusion models on specific fields. In this work, we are committed to
conducting a survey on the graph diffusion models. Even though our focus is to
cover the progress of diffusion models in graphs, we first briefly summarize
how other generative modeling methods are used for graphs. After that, we
introduce the mechanism of diffusion models in various forms, which facilitates
the discussion on the graph diffusion models. The applications of graph
diffusion models mainly fall into the category of AI-generated content (AIGC)
in science, for which we mainly focus on how graph diffusion models are
utilized for generating molecules and proteins but also cover other cases,
including materials design. Moreover, we discuss the issue of evaluating
diffusion models in the graph domain and the existing challenges.
</p></li>
</ul>

<h3>Title: Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion. (arXiv:2304.01893v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01893">http://arxiv.org/abs/2304.01893</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01893] Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion](http://arxiv.org/abs/2304.01893) #diffusion</code></li>
<li>Summary: <p>We introduce a method for generating realistic pedestrian trajectories and
full-body animations that can be controlled to meet user-defined goals. We draw
on recent advances in guided diffusion modeling to achieve test-time
controllability of trajectories, which is normally only associated with
rule-based systems. Our guided diffusion model allows users to constrain
trajectories through target waypoints, speed, and specified social groups while
accounting for the surrounding environment context. This trajectory diffusion
model is integrated with a novel physics-based humanoid controller to form a
closed-loop, full-body pedestrian animation system capable of placing large
crowds in a simulated environment with varying terrains. We further propose
utilizing the value function learned during RL training of the animation
controller to guide diffusion to produce trajectories better suited for
particular scenarios such as collision avoidance and traversing uneven terrain.
Video results are available on the project page at
https://nv-tlabs.github.io/trace-pace .
</p></li>
</ul>

<h3>Title: PODIA-3D: Domain Adaptation of 3D Generative Model Across Large Domain Gap Using Pose-Preserved Text-to-Image Diffusion. (arXiv:2304.01900v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.01900">http://arxiv.org/abs/2304.01900</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.01900] PODIA-3D: Domain Adaptation of 3D Generative Model Across Large Domain Gap Using Pose-Preserved Text-to-Image Diffusion](http://arxiv.org/abs/2304.01900) #diffusion</code></li>
<li>Summary: <p>Recently, significant advancements have been made in 3D generative models,
however training these models across diverse domains is challenging and
requires an huge amount of training data and knowledge of pose distribution.
Text-guided domain adaptation methods have allowed the generator to be adapted
to the target domains using text prompts, thereby obviating the need for
assembling numerous data. Recently, DATID-3D presents impressive quality of
samples in text-guided domain, preserving diversity in text by leveraging
text-to-image diffusion. However, adapting 3D generators to domains with
significant domain gaps from the source domain still remains challenging due to
issues in current text-to-image diffusion models as following: 1) shape-pose
trade-off in diffusion-based translation, 2) pose bias, and 3) instance bias in
the target domain, resulting in inferior 3D shapes, low text-image
correspondence, and low intra-domain diversity in the generated samples. To
address these issues, we propose a novel pipeline called PODIA-3D, which uses
pose-preserved text-to-image diffusion-based domain adaptation for 3D
generative models. We construct a pose-preserved text-to-image diffusion model
that allows the use of extremely high-level noise for significant domain
changes. We also propose specialized-to-general sampling strategies to improve
the details of the generated samples. Moreover, to overcome the instance bias,
we introduce a text-guided debiasing method that improves intra-domain
diversity. Consequently, our method successfully adapts 3D generators across
significant domain gaps. Our qualitative results and user study demonstrates
that our approach outperforms existing 3D text-guided domain adaptation methods
in terms of text-image correspondence, realism, diversity of rendered images,
and sense of depth of 3D shapes in the generated samples
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
