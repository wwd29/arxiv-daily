<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-04-28</h1>
<h3>Title: Spectral Dictionary Learning for Generative Image Modeling</h3>
<ul>
<li><strong>Authors: </strong>Andrew Kiruluta</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17804">https://arxiv.org/abs/2504.17804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17804">https://arxiv.org/pdf/2504.17804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17804]] Spectral Dictionary Learning for Generative Image Modeling(https://arxiv.org/abs/2504.17804)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, generative</a></li>
<li><strong>Abstract: </strong>We propose a novel spectral generative model for image synthesis that departs radically from the common variational, adversarial, and diffusion paradigms. In our approach, images, after being flattened into one-dimensional signals, are reconstructed as linear combinations of a set of learned spectral basis functions, where each basis is explicitly parameterized in terms of frequency, phase, and amplitude. The model jointly learns a global spectral dictionary with time-varying modulations and per-image mixing coefficients that quantify the contributions of each spectral component. Subsequently, a simple probabilistic model is fitted to these mixing coefficients, enabling the deterministic generation of new images by sampling from the latent space. This framework leverages deterministic dictionary learning, offering a highly interpretable and physically meaningful representation compared to methods relying on stochastic inference or adversarial training. Moreover, the incorporation of frequency-domain loss functions, computed via the short-time Fourier transform (STFT), ensures that the synthesized images capture both global structure and fine-grained spectral details, such as texture and edge information. Experimental evaluations on the CIFAR-10 benchmark demonstrate that our approach not only achieves competitive performance in terms of reconstruction quality and perceptual fidelity but also offers improved training stability and computational efficiency. This new type of generative model opens up promising avenues for controlled synthesis, as the learned spectral dictionary affords a direct handle on the intrinsic frequency content of the images, thus providing enhanced interpretability and potential for novel applications in image manipulation and analysis.</li>
</ul>

<h3>Title: SmallGS: Gaussian Splatting-based Camera Pose Estimation for Small-Baseline Videos</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Yao, Yan Zhang, Zhening Huang, Joan Lasenby</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17810">https://arxiv.org/abs/2504.17810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17810">https://arxiv.org/pdf/2504.17810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17810]] SmallGS: Gaussian Splatting-based Camera Pose Estimation for Small-Baseline Videos(https://arxiv.org/abs/2504.17810)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Dynamic videos with small baseline motions are ubiquitous in daily life, especially on social media. However, these videos present a challenge to existing pose estimation frameworks due to ambiguous features, drift accumulation, and insufficient triangulation constraints. Gaussian splatting, which maintains an explicit representation for scenes, provides a reliable novel view rasterization when the viewpoint change is small. Inspired by this, we propose SmallGS, a camera pose estimation framework that is specifically designed for small-baseline videos. SmallGS optimizes sequential camera poses using Gaussian splatting, which reconstructs the scene from the first frame in each video segment to provide a stable reference for the rest. The temporal consistency of Gaussian splatting within limited viewpoint differences reduced the requirement of sufficient depth variations in traditional camera pose estimation. We further incorporate pretrained robust visual features, e.g. DINOv2, into Gaussian splatting, where high-dimensional feature map rendering enhances the robustness of camera pose estimation. By freezing the Gaussian splatting and optimizing camera viewpoints based on rasterized features, SmallGS effectively learns camera poses without requiring explicit feature correspondences or strong parallax motion. We verify the effectiveness of SmallGS in small-baseline videos in TUM-Dynamics sequences, which achieves impressive accuracy in camera pose estimation compared to MonST3R and DORID-SLAM for small-baseline videos in dynamic scenes. Our project page is at: this https URL</li>
</ul>

<h3>Title: Object Learning and Robust 3D Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Sara Sabour</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17812">https://arxiv.org/abs/2504.17812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17812">https://arxiv.org/pdf/2504.17812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17812]] Object Learning and Robust 3D Reconstruction(https://arxiv.org/abs/2504.17812)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In this thesis we discuss architectural designs and training methods for a neural network to have the ability of dissecting an image into objects of interest without supervision. The main challenge in 2D unsupervised object segmentation is distinguishing between foreground objects of interest and background. FlowCapsules uses motion as a cue for the objects of interest in 2D scenarios. The last part of this thesis focuses on 3D applications where the goal is detecting and removal of the object of interest from the input images. In these tasks, we leverage the geometric consistency of scenes in 3D to detect the inconsistent dynamic objects. Our transient object masks are then used for designing robust optimization kernels to improve 3D modelling in a casual capture setup. One of our goals in this thesis is to show the merits of unsupervised object based approaches in computer vision. Furthermore, we suggest possible directions for defining objects of interest or foreground objects without requiring supervision. Our hope is to motivate and excite the community into further exploring explicit object representations in image understanding tasks.</li>
</ul>

<h3>Title: CLOC: Contrastive Learning for Ordinal Classification with Multi-Margin N-pair Loss</h3>
<ul>
<li><strong>Authors: </strong>Dileepa Pitawela, Gustavo Carneiro, Hsiang-Ting Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17813">https://arxiv.org/abs/2504.17813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17813">https://arxiv.org/pdf/2504.17813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17813]] CLOC: Contrastive Learning for Ordinal Classification with Multi-Margin N-pair Loss(https://arxiv.org/abs/2504.17813)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>In ordinal classification, misclassifying neighboring ranks is common, yet the consequences of these errors are not the same. For example, misclassifying benign tumor categories is less consequential, compared to an error at the pre-cancerous to cancerous threshold, which could profoundly influence treatment choices. Despite this, existing ordinal classification methods do not account for the varying importance of these margins, treating all neighboring classes as equally significant. To address this limitation, we propose CLOC, a new margin-based contrastive learning method for ordinal classification that learns an ordered representation based on the optimization of multiple margins with a novel multi-margin n-pair loss (MMNP). CLOC enables flexible decision boundaries across key adjacent categories, facilitating smooth transitions between classes and reducing the risk of overfitting to biases present in the training data. We provide empirical discussion regarding the properties of MMNP and show experimental results on five real-world image datasets (Adience, Historical Colour Image Dating, Knee Osteoarthritis, Indian Diabetic Retinopathy Image, and Breast Carcinoma Subtyping) and one synthetic dataset simulating clinical decision bias. Our results demonstrate that CLOC outperforms existing ordinal classification methods and show the interpretability and controllability of CLOC in learning meaningful, ordered representations that align with clinical and practical needs.</li>
</ul>

<h3>Title: Visibility-Uncertainty-guided 3D Gaussian Inpainting via Scene Conceptional Learning</h3>
<ul>
<li><strong>Authors: </strong>Mingxuan Cui, Qing Guo, Yuyi Wang, Hongkai Yu, Di Lin, Qin Zou, Ming-Ming Cheng, Xi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17815">https://arxiv.org/abs/2504.17815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17815">https://arxiv.org/pdf/2504.17815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17815]] Visibility-Uncertainty-guided 3D Gaussian Inpainting via Scene Conceptional Learning(https://arxiv.org/abs/2504.17815)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting (3DGS) has emerged as a powerful and efficient 3D representation for novel view synthesis. This paper extends 3DGS capabilities to inpainting, where masked objects in a scene are replaced with new contents that blend seamlessly with the surroundings. Unlike 2D image inpainting, 3D Gaussian inpainting (3DGI) is challenging in effectively leveraging complementary visual and semantic cues from multiple input views, as occluded areas in one view may be visible in others. To address this, we propose a method that measures the visibility uncertainties of 3D points across different input views and uses them to guide 3DGI in utilizing complementary visual cues. We also employ uncertainties to learn a semantic concept of scene without the masked object and use a diffusion model to fill masked objects in input images based on the learned concept. Finally, we build a novel 3DGI framework, VISTA, by integrating VISibility-uncerTainty-guided 3DGI with scene conceptuAl learning. VISTA generates high-quality 3DGS models capable of synthesizing artifact-free and naturally inpainted novel views. Furthermore, our approach extends to handling dynamic distractors arising from temporal object changes, enhancing its versatility in diverse scene reconstruction scenarios. We demonstrate the superior performance of our method over state-of-the-art techniques using two challenging datasets: the SPIn-NeRF dataset, featuring 10 diverse static 3D inpainting scenes, and an underwater 3D inpainting dataset derived from UTB180, including fast-moving fish as inpainting targets.</li>
</ul>

<h3>Title: A multi-scale vision transformer-based multimodal GeoAI model for mapping Arctic permafrost thaw</h3>
<ul>
<li><strong>Authors: </strong>Wenwen Li, Chia-Yu Hsu, Sizhe Wang, Zhining Gu, Yili Yang, Brendan M. Rogers, Anna Liljedahl</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17822">https://arxiv.org/abs/2504.17822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17822">https://arxiv.org/pdf/2504.17822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17822]] A multi-scale vision transformer-based multimodal GeoAI model for mapping Arctic permafrost thaw(https://arxiv.org/abs/2504.17822)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Retrogressive Thaw Slumps (RTS) in Arctic regions are distinct permafrost landforms with significant environmental impacts. Mapping these RTS is crucial because their appearance serves as a clear indication of permafrost thaw. However, their small scale compared to other landform features, vague boundaries, and spatiotemporal variation pose significant challenges for accurate detection. In this paper, we employed a state-of-the-art deep learning model, the Cascade Mask R-CNN with a multi-scale vision transformer-based backbone, to delineate RTS features across the Arctic. Two new strategies were introduced to optimize multimodal learning and enhance the model's predictive performance: (1) a feature-level, residual cross-modality attention fusion strategy, which effectively integrates feature maps from multiple modalities to capture complementary information and improve the model's ability to understand complex patterns and relationships within the data; (2) pre-trained unimodal learning followed by multimodal fine-tuning to alleviate high computing demand while achieving strong model performance. Experimental results demonstrated that our approach outperformed existing models adopting data-level fusion, feature-level convolutional fusion, and various attention fusion strategies, providing valuable insights into the efficient utilization of multimodal data for RTS mapping. This research contributes to our understanding of permafrost landforms and their environmental implications.</li>
</ul>

<h3>Title: Dual Prompting Image Restoration with Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Dehong Kong, Fan Li, Zhixin Wang, Jiaqi Xu, Renjing Pei, Wenbo Li, WenQi Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17825">https://arxiv.org/abs/2504.17825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17825">https://arxiv.org/pdf/2504.17825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17825]] Dual Prompting Image Restoration with Diffusion Transformers(https://arxiv.org/abs/2504.17825)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Recent state-of-the-art image restoration methods mostly adopt latent diffusion models with U-Net backbones, yet still facing challenges in achieving high-quality restoration due to their limited capabilities. Diffusion transformers (DiTs), like SD3, are emerging as a promising alternative because of their better quality with scalability. In this paper, we introduce DPIR (Dual Prompting Image Restoration), a novel image restoration method that effectivly extracts conditional information of low-quality images from multiple perspectives. Specifically, DPIR consits of two branches: a low-quality image conditioning branch and a dual prompting control branch. The first branch utilizes a lightweight module to incorporate image priors into the DiT with high efficiency. More importantly, we believe that in image restoration, textual description alone cannot fully capture its rich visual characteristics. Therefore, a dual prompting module is designed to provide DiT with additional visual cues, capturing both global context and local appearance. The extracted global-local visual prompts as extra conditional control, alongside textual prompts to form dual prompts, greatly enhance the quality of the restoration. Extensive experimental results demonstrate that DPIR delivers superior image restoration performance.</li>
</ul>

<h3>Title: VEU-Bench: Towards Comprehensive Understanding of Video Editing</h3>
<ul>
<li><strong>Authors: </strong>Bozheng Li, Yongliang Wu, Yi Lu, Jiashuo Yu, Licheng Tang, Jiawang Cao, Wenqing Zhu, Yuyang Sun, Jay Wu, Wenbo Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17828">https://arxiv.org/abs/2504.17828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17828">https://arxiv.org/pdf/2504.17828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17828]] VEU-Bench: Towards Comprehensive Understanding of Video Editing(https://arxiv.org/abs/2504.17828)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Widely shared videos on the internet are often edited. Recently, although Video Large Language Models (Vid-LLMs) have made great progress in general video understanding tasks, their capabilities in video editing understanding (VEU) tasks remain unexplored. To address this gap, in this paper, we introduce VEU-Bench (Video Editing Understanding Benchmark), a comprehensive benchmark that categorizes video editing components across various dimensions, from intra-frame features like shot size to inter-shot attributes such as cut types and transitions. Unlike previous video editing understanding benchmarks that focus mainly on editing element classification, VEU-Bench encompasses 19 fine-grained tasks across three stages: recognition, reasoning, and judging. To enhance the annotation of VEU automatically, we built an annotation pipeline integrated with an ontology-based knowledge base. Through extensive experiments with 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs face significant challenges in VEU tasks, with some performing worse than random choice. To alleviate this issue, we develop Oscars, a VEU expert model fine-tuned on the curated VEU-Bench dataset. It outperforms existing open-source Vid-LLMs on VEU-Bench by over 28.3% in accuracy and achieves performance comparable to commercial models like GPT-4o. We also demonstrate that incorporating VEU data significantly enhances the performance of Vid-LLMs on general video understanding benchmarks, with an average improvement of 8.3% across nine reasoning tasks.</li>
</ul>

<h3>Title: Fine-Tuning Adversarially-Robust Transformers for Single-Image Dehazing</h3>
<ul>
<li><strong>Authors: </strong>Vlad Vasilescu, Ana Neacsu, Daniela Faur</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17829">https://arxiv.org/abs/2504.17829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17829">https://arxiv.org/pdf/2504.17829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17829]] Fine-Tuning Adversarially-Robust Transformers for Single-Image Dehazing(https://arxiv.org/abs/2504.17829)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>Single-image dehazing is an important topic in remote sensing applications, enhancing the quality of acquired images and increasing object detection precision. However, the reliability of such structures has not been sufficiently analyzed, which poses them to the risk of imperceptible perturbations that can significantly hinder their performance. In this work, we show that state-of-the-art image-to-image dehazing transformers are susceptible to adversarial noise, with even 1 pixel change being able to decrease the PSNR by as much as 2.8 dB. Next, we propose two lightweight fine-tuning strategies aimed at increasing the robustness of pre-trained transformers. Our methods results in comparable clean performance, while significantly increasing the protection against adversarial data. We further present their applicability in two remote sensing scenarios, showcasing their robust behavior for out-of-distribution data. The source code for adversarial fine-tuning and attack algorithms can be found at this http URL.</li>
</ul>

<h3>Title: High-Performance Reinforcement Learning on Spot: Optimizing Simulation Parameters with Distributional Measures</h3>
<ul>
<li><strong>Authors: </strong>A. J Miller, Fangzhou Yu, Michael Brauckmann, Farbod Farshidian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17857">https://arxiv.org/abs/2504.17857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17857">https://arxiv.org/pdf/2504.17857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17857]] High-Performance Reinforcement Learning on Spot: Optimizing Simulation Parameters with Distributional Measures(https://arxiv.org/abs/2504.17857)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This work presents an overview of the technical details behind a high performance reinforcement learning policy deployment with the Spot RL Researcher Development Kit for low level motor access on Boston Dynamics Spot. This represents the first public demonstration of an end to end end reinforcement learning policy deployed on Spot hardware with training code publicly available through Nvidia IsaacLab and deployment code available through Boston Dynamics. We utilize Wasserstein Distance and Maximum Mean Discrepancy to quantify the distributional dissimilarity of data collected on hardware and in simulation to measure our sim2real gap. We use these measures as a scoring function for the Covariance Matrix Adaptation Evolution Strategy to optimize simulated parameters that are unknown or difficult to measure from Spot. Our procedure for modeling and training produces high quality reinforcement learning policies capable of multiple gaits, including a flight phase. We deploy policies capable of over 5.2ms locomotion, more than triple Spots default controller maximum speed, robustness to slippery surfaces, disturbance rejection, and overall agility previously unseen on Spot. We detail our method and release our code to support future work on Spot with the low level API.</li>
</ul>

<h3>Title: Crypto-ncRNA: Non-coding RNA (ncRNA) Based Encryption Algorithm</h3>
<ul>
<li><strong>Authors: </strong>Xu Wang, Yiquan Wang, Tin-yeh Huang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17878">https://arxiv.org/abs/2504.17878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17878">https://arxiv.org/pdf/2504.17878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17878]] Crypto-ncRNA: Non-coding RNA (ncRNA) Based Encryption Algorithm(https://arxiv.org/abs/2504.17878)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>In the looming post-quantum era, traditional cryptographic systems are increasingly vulnerable to quantum computing attacks that can compromise their mathematical foundations. To address this critical challenge, we propose crypto-ncRNA-a bio-convergent cryptographic framework that leverages the dynamic folding properties of non-coding RNA (ncRNA) to generate high-entropy, quantum-resistant keys and produce unpredictable ciphertexts. The framework employs a novel, multi-stage process: encoding plaintext into RNA sequences, predicting and manipulating RNA secondary structures using advanced algorithms, and deriving cryptographic keys through the intrinsic physical unclonability of RNA molecules. Experimental evaluations indicate that, although crypto-ncRNA's encryption speed is marginally lower than that of AES, it significantly outperforms RSA in terms of efficiency and scalability while achieving a 100% pass rate on the NIST SP 800-22 randomness tests. These results demonstrate that crypto-ncRNA offers a promising and robust approach for securing digital infrastructures against the evolving threats posed by quantum computing.</li>
</ul>

<h3>Title: Do We Need Transformers to Play FPS Video Games?</h3>
<ul>
<li><strong>Authors: </strong>Karmanbir Batth, Krish Sethi, Aly Shariff, Leo Shi, Hetul Patel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17891">https://arxiv.org/abs/2504.17891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17891">https://arxiv.org/pdf/2504.17891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17891]] Do We Need Transformers to Play FPS Video Games?(https://arxiv.org/abs/2504.17891)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we explore the Transformer based architectures for reinforcement learning in both online and offline settings within the Doom game environment. Our investigation focuses on two primary approaches: Deep Transformer Q- learning Networks (DTQN) for online learning and Decision Transformers (DT) for offline reinforcement learning. DTQN leverages the sequential modelling capabilities of Transformers to enhance Q-learning in partially observable environments,while Decision Transformers repurpose sequence modelling techniques to enable offline agents to learn from past trajectories without direct interaction with the environment. We conclude that while Transformers might have performed well in Atari games, more traditional methods perform better than Transformer based method in both the settings in the VizDoom environment.</li>
</ul>

<h3>Title: DCT-Shield: A Robust Frequency Domain Defense against Malicious Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Aniruddha Bala, Rohit Chowdhury, Rohan Jaiswal, Siddharth Roheda</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17894">https://arxiv.org/abs/2504.17894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17894">https://arxiv.org/pdf/2504.17894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17894]] DCT-Shield: A Robust Frequency Domain Defense against Malicious Image Editing(https://arxiv.org/abs/2504.17894)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Advancements in diffusion models have enabled effortless image editing via text prompts, raising concerns about image security. Attackers with access to user images can exploit these tools for malicious edits. Recent defenses attempt to protect images by adding a limited noise in the pixel space to disrupt the functioning of diffusion-based editing models. However, the adversarial noise added by previous methods is easily noticeable to the human eye. Moreover, most of these methods are not robust to purification techniques like JPEG compression under a feasible pixel budget. We propose a novel optimization approach that introduces adversarial perturbations directly in the frequency domain by modifying the Discrete Cosine Transform (DCT) coefficients of the input image. By leveraging the JPEG pipeline, our method generates adversarial images that effectively prevent malicious image editing. Extensive experiments across a variety of tasks and datasets demonstrate that our approach introduces fewer visual artifacts while maintaining similar levels of edit protection and robustness to noise purification techniques.</li>
</ul>

<h3>Title: CAMU: Context Augmentation for Meme Understanding</h3>
<ul>
<li><strong>Authors: </strong>Girish A. Koushik, Diptesh Kanojia, Helen Treharne, Aditya Joshi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17902">https://arxiv.org/abs/2504.17902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17902">https://arxiv.org/pdf/2504.17902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17902]] CAMU: Context Augmentation for Meme Understanding(https://arxiv.org/abs/2504.17902)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Social media memes are a challenging domain for hate detection because they intertwine visual and textual cues into culturally nuanced messages. We introduce a novel framework, CAMU, which leverages large vision-language models to generate more descriptive captions, a caption-scoring neural network to emphasise hate-relevant content, and parameter-efficient fine-tuning of CLIP's text encoder for an improved multimodal understanding of memes. Experiments on publicly available hateful meme datasets show that simple projection layer fine-tuning yields modest gains, whereas selectively tuning deeper text encoder layers significantly boosts performance on all evaluation metrics. Moreover, our approach attains high accuracy (0.807) and F1-score (0.806) on the Hateful Memes dataset, at par with the existing SoTA framework while being much more efficient, offering practical advantages in real-world scenarios that rely on fixed decision thresholds. CAMU also achieves the best F1-score of 0.673 on the MultiOFF dataset for offensive meme identification, demonstrating its generalisability. Additional analyses on benign confounders reveal that robust visual grounding and nuanced text representations are crucial for reliable hate and offence detection. We will publicly release CAMU along with the resultant models for further research. Disclaimer: This paper includes references to potentially disturbing, hateful, or offensive content due to the nature of the task.</li>
</ul>

<h3>Title: Biting the CHERI bullet: Blockers, Enablers and Security Implications of CHERI in Defence</h3>
<ul>
<li><strong>Authors: </strong>Shamal Faily</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR, cs.OS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17904">https://arxiv.org/abs/2504.17904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17904">https://arxiv.org/pdf/2504.17904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17904]] Biting the CHERI bullet: Blockers, Enablers and Security Implications of CHERI in Defence(https://arxiv.org/abs/2504.17904)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>There is growing interest in securing the hardware foundations software stacks build upon. However, before making any investment decision, software and hardware supply chain stakeholders require evidence from realistic, multiple long-term studies of adoption. We present results from a 12 month evaluation of one such secure hardware solution, CHERI, where 15 teams from industry and academia ported software relevant to Defence to Arm's experimental Morello board. We identified six types of blocker inhibiting adoption: dependencies, a knowledge premium, missing utilities, performance, platform instability, and technical debt. We also identified three types of enabler: tool assistance, improved quality, and trivial code porting. Finally, we identified five types of potential vulnerability that CHERI could, if not appropriately configured, expand a system's attack surface: state leaks, memory leaks, use after free vulnerabilities, unsafe defaults, and tool chain instability. Future work should remove potentially insecure defaults from CHERI tooling, and develop a CHERI body of knowledge to further adoption.</li>
</ul>

<h3>Title: The use of Multi-domain Electroencephalogram Representations in the building of Models based on Convolutional and Recurrent Neural Networks for Epilepsy Detection</h3>
<ul>
<li><strong>Authors: </strong>Luiz Antonio Nicolau Anghinoni, Gustavo Weber Denardin, Jadson Castro Gertrudes, Dalcimar Casanova, Jefferson Tales Oliva</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17908">https://arxiv.org/abs/2504.17908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17908">https://arxiv.org/pdf/2504.17908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17908]] The use of Multi-domain Electroencephalogram Representations in the building of Models based on Convolutional and Recurrent Neural Networks for Epilepsy Detection(https://arxiv.org/abs/2504.17908)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Epilepsy, affecting approximately 50 million people globally, is characterized by abnormal brain activity and remains challenging to treat. The diagnosis of epilepsy relies heavily on electroencephalogram (EEG) data, where specialists manually analyze epileptiform patterns across pre-ictal, ictal, post-ictal, and interictal periods. However, the manual analysis of EEG signals is prone to variability between experts, emphasizing the need for automated solutions. Although previous studies have explored preprocessing techniques and machine learning approaches for seizure detection, there is a gap in understanding how the representation of EEG data (time, frequency, or time-frequency domains) impacts the predictive performance of deep learning models. This work addresses this gap by systematically comparing deep neural networks trained on EEG data in these three domains. Through the use of statistical tests, we identify the optimal data representation and model architecture for epileptic seizure detection. The results demonstrate that frequency-domain data achieves detection metrics exceeding 97\%, providing a robust foundation for more accurate and reliable seizure detection systems.</li>
</ul>

<h3>Title: Secured Encryption scheme based on the Ree groups</h3>
<ul>
<li><strong>Authors: </strong>Gennady Khalimov, Yevgen Kotukh</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17919">https://arxiv.org/abs/2504.17919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17919">https://arxiv.org/pdf/2504.17919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17919]] Secured Encryption scheme based on the Ree groups(https://arxiv.org/abs/2504.17919)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>An improved design of a cryptosystem based on small Ree groups is proposed. We have changed the encryption algorithm and propose to use a logarithmic signature for the entire Ree group. This approach improves security against sequential key recovery attacks. Hence, the complexity of the key recovery attack will be defined by a brute-force attack over the entire group. In this paper, we have proved that to construct secure cryptosystems with group computations over a small finite field, it is needed to use a 3-parametric small Ree group.</li>
</ul>

<h3>Title: Optimized Approaches to Malware Detection: A Study of Machine Learning and Deep Learning Techniques</h3>
<ul>
<li><strong>Authors: </strong>Abrar Fahim, Shamik Dey, Md. Nurul Absur, Md Kamrul Siam, Md. Tahmidul Huque, Jafreen Jafor Godhuli</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17930">https://arxiv.org/abs/2504.17930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17930">https://arxiv.org/pdf/2504.17930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17930]] Optimized Approaches to Malware Detection: A Study of Machine Learning and Deep Learning Techniques(https://arxiv.org/abs/2504.17930)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, robust</a></li>
<li><strong>Abstract: </strong>Digital systems find it challenging to keep up with cybersecurity threats. The daily emergence of more than 560,000 new malware strains poses significant hazards to the digital ecosystem. The traditional malware detection methods fail to operate properly and yield high false positive rates with low accuracy of the protection system. This study explores the ways in which malware can be detected using these machine learning (ML) and deep learning (DL) approaches to address those shortcomings. This study also includes a systematic comparison of the performance of some of the widely used ML models, such as random forest, multi-layer perceptron (MLP), and deep neural network (DNN), for determining the effectiveness of the domain of modern malware threat systems. We use a considerable-sized database from Kaggle, which has undergone optimized feature selection and preprocessing to improve model performance. Our finding suggests that the DNN model outperformed the other traditional models with the highest training accuracy of 99.92% and an almost perfect AUC score. Furthermore, the feature selection and preprocessing can help improve the capabilities of detection. This research makes an important contribution by analyzing the performance of the model on the performance metrics and providing insight into the effectiveness of the advanced detection techniques to build more robust and more reliable cybersecurity solutions against the growing malware threats.</li>
</ul>

<h3>Title: Masked strategies for images with small objects</h3>
<ul>
<li><strong>Authors: </strong>H. Martin Gillis, Ming Hill, Paul Hollensen, Alan Fine, Thomas Trappenberg</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17935">https://arxiv.org/abs/2504.17935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17935">https://arxiv.org/pdf/2504.17935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17935]] Masked strategies for images with small objects(https://arxiv.org/abs/2504.17935)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The hematology analytics used for detection and classification of small blood components is a significant challenge. In particular, when objects exists as small pixel-sized entities in a large context of similar objects. Deep learning approaches using supervised models with pre-trained weights, such as residual networks and vision transformers have demonstrated success for many applications. Unfortunately, when applied to images outside the domain of learned representations, these methods often result with less than acceptable performance. A strategy to overcome this can be achieved by using self-supervised models, where representations are learned and weights are then applied for downstream applications. Recently, masked autoencoders have proven to be effective to obtain representations that captures global context information. By masking regions of an image and having the model learn to reconstruct both the masked and non-masked regions, weights can be used for various applications. However, if the sizes of the objects in images are less than the size of the mask, the global context information is lost, making it almost impossible to reconstruct the image. In this study, we investigated the effect of mask ratios and patch sizes for blood components using a MAE to obtain learned ViT encoder representations. We then applied the encoder weights to train a U-Net Transformer for semantic segmentation to obtain both local and global contextual information. Our experimental results demonstrates that both smaller mask ratios and patch sizes improve the reconstruction of images using a MAE. We also show the results of semantic segmentation with and without pre-trained weights, where smaller-sized blood components benefited with pre-training. Overall, our proposed method offers an efficient and effective strategy for the segmentation and classification of small objects.</li>
</ul>

<h3>Title: Causality-Driven Neural Network Repair: Challenges and Opportunities</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Vares, Brittany Johnson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17946">https://arxiv.org/abs/2504.17946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17946">https://arxiv.org/pdf/2504.17946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17946]] Causality-Driven Neural Network Repair: Challenges and Opportunities(https://arxiv.org/abs/2504.17946)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, interpretability</a></li>
<li><strong>Abstract: </strong>Deep Neural Networks (DNNs) often rely on statistical correlations rather than causal reasoning, limiting their robustness and interpretability. While testing methods can identify failures, effective debugging and repair remain challenging. This paper explores causal inference as an approach primarily for DNN repair, leveraging causal debugging, counterfactual analysis, and structural causal models (SCMs) to identify and correct failures. We discuss in what ways these techniques support fairness, adversarial robustness, and backdoor mitigation by providing targeted interventions. Finally, we discuss key challenges, including scalability, generalization, and computational efficiency, and outline future directions for integrating causality-driven interventions to enhance DNN reliability.</li>
</ul>

<h3>Title: Fishing for Phishers: Learning-Based Phishing Detection in Ethereum Transactions</h3>
<ul>
<li><strong>Authors: </strong>Ahod Alghuried, Abdulaziz Alghamdi, Ali Alkinoon, Soohyeon Choi, Manar Mohaisen, David Mohaisen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17953">https://arxiv.org/abs/2504.17953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17953">https://arxiv.org/pdf/2504.17953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17953]] Fishing for Phishers: Learning-Based Phishing Detection in Ethereum Transactions(https://arxiv.org/abs/2504.17953)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Phishing detection on Ethereum has increasingly leveraged advanced machine learning techniques to identify fraudulent transactions. However, limited attention has been given to understanding the effectiveness of feature selection strategies and the role of graph-based models in enhancing detection accuracy. In this paper, we systematically examine these issues by analyzing and contrasting explicit transactional features and implicit graph-based features, both experimentally and analytically. We explore how different feature sets impact the performance of phishing detection models, particularly in the context of Ethereum's transactional network. Additionally, we address key challenges such as class imbalance and dataset composition and their influence on the robustness and precision of detection methods. Our findings demonstrate the advantages and limitations of each feature type, while also providing a clearer understanding of how feature affect model resilience and generalization in adversarial environments.</li>
</ul>

<h3>Title: Cluster-Aware Attacks on Graph Watermarks</h3>
<ul>
<li><strong>Authors: </strong>Alexander Nemecek, Emre Yilmaz, Erman Ayday</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17971">https://arxiv.org/abs/2504.17971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17971">https://arxiv.org/pdf/2504.17971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17971]] Cluster-Aware Attacks on Graph Watermarks(https://arxiv.org/abs/2504.17971)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, watermark</a></li>
<li><strong>Abstract: </strong>Data from domains such as social networks, healthcare, finance, and cybersecurity can be represented as graph-structured information. Given the sensitive nature of this data and their frequent distribution among collaborators, ensuring secure and attributable sharing is essential. Graph watermarking enables attribution by embedding user-specific signatures into graph-structured data. While prior work has addressed random perturbation attacks, the threat posed by adversaries leveraging structural properties through community detection remains unexplored. In this work, we introduce a cluster-aware threat model in which adversaries apply community-guided modifications to evade detection. We propose two novel attack strategies and evaluate them on real-world social network graphs. Our results show that cluster-aware attacks can reduce attribution accuracy by up to 80% more than random baselines under equivalent perturbation budgets on sparse graphs. To mitigate this threat, we propose a lightweight embedding enhancement that distributes watermark nodes across graph communities. This approach improves attribution accuracy by up to 60% under attack on dense graphs, without increasing runtime or structural distortion. Our findings underscore the importance of cluster-topological awareness in both watermarking design and adversarial modeling.</li>
</ul>

<h3>Title: Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English</h3>
<ul>
<li><strong>Authors: </strong>Sabur Butt, Fazlourrahman Balouchzahi, Ahmad Imam Amjad, Maaz Amjad, Hector G. Ceballos, Salud Maria Jimenez-Zafra</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17974">https://arxiv.org/abs/2504.17974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17974">https://arxiv.org/pdf/2504.17974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17974]] Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English(https://arxiv.org/abs/2504.17974)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Hope is a complex and underexplored emotional state that plays a significant role in education, mental health, and social interaction. Unlike basic emotions, hope manifests in nuanced forms ranging from grounded optimism to exaggerated wishfulness or sarcasm, making it difficult for Natural Language Processing systems to detect accurately. This study introduces PolyHope V2, a multilingual, fine-grained hope speech dataset comprising over 30,000 annotated tweets in English and Spanish. This resource distinguishes between four hope subtypes Generalized, Realistic, Unrealistic, and Sarcastic and enhances existing datasets by explicitly labeling sarcastic instances. We benchmark multiple pretrained transformer models and compare them with large language models (LLMs) such as GPT 4 and Llama 3 under zero-shot and few-shot regimes. Our findings show that fine-tuned transformers outperform prompt-based LLMs, especially in distinguishing nuanced hope categories and sarcasm. Through qualitative analysis and confusion matrices, we highlight systematic challenges in separating closely related hope subtypes. The dataset and results provide a robust foundation for future emotion recognition tasks that demand greater semantic and contextual sensitivity across languages.</li>
</ul>

<h3>Title: Back to Fundamentals: Low-Level Visual Features Guided Progressive Token Pruning</h3>
<ul>
<li><strong>Authors: </strong>Yuanbing Ouyang, Yizhuo Liang, Qingpeng Li, Xinfei Guo, Yiming Luo, Di Wu, Hao Wang, Yushan Pan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.17996">https://arxiv.org/abs/2504.17996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.17996">https://arxiv.org/pdf/2504.17996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.17996]] Back to Fundamentals: Low-Level Visual Features Guided Progressive Token Pruning(https://arxiv.org/abs/2504.17996)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViTs) excel in semantic segmentation but demand significant computation, posing challenges for deployment on resource-constrained devices. Existing token pruning methods often overlook fundamental visual data characteristics. This study introduces 'LVTP', a progressive token pruning framework guided by multi-scale Tsallis entropy and low-level visual features with twice clustering. It integrates high-level semantics and basic visual attributes for precise segmentation. A novel dynamic scoring mechanism using multi-scale Tsallis entropy weighting overcomes limitations of traditional single-parameter entropy. The framework also incorporates low-level feature analysis to preserve critical edge information while optimizing computational cost. As a plug-and-play module, it requires no architectural changes or additional training. Evaluations across multiple datasets show 20%-45% computational reductions with negligible performance loss, outperforming existing methods in balancing cost and accuracy, especially in complex edge regions.</li>
</ul>

<h3>Title: Self-Balancing, Memory Efficient, Dynamic Metric Space Data Maintenance, for Rapid Multi-Kernel Estimation</h3>
<ul>
<li><strong>Authors: </strong>Aditya S Ellendula, Chandrajit Bajaj</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18003">https://arxiv.org/abs/2504.18003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18003">https://arxiv.org/pdf/2504.18003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18003]] Self-Balancing, Memory Efficient, Dynamic Metric Space Data Maintenance, for Rapid Multi-Kernel Estimation(https://arxiv.org/abs/2504.18003)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a dynamic self-balancing octree data structure that enables efficient neighborhood maintenance in evolving metric spaces, a key challenge in modern machine learning systems. Many learning and generative models operate as dynamical systems whose representations evolve during training, requiring fast, adaptive spatial organization. Our two-parameter octree supports logarithmic-time updates and queries, eliminating the need for costly full rebuilds as data distributions shift. We demonstrate its effectiveness in four areas: (1) accelerating Stein variational gradient descent by supporting more particles with lower overhead; (2) enabling real-time, incremental KNN classification with logarithmic complexity; (3) facilitating efficient, dynamic indexing and retrieval for retrieval-augmented generation; and (4) improving sample efficiency by jointly optimizing input and latent spaces. Across all applications, our approach yields exponential speedups while preserving accuracy, particularly in high-dimensional spaces where maintaining adaptive spatial structure is critical.</li>
</ul>

<h3>Title: TGDT: A Temporal Graph-based Digital Twin for Urban Traffic Corridors</h3>
<ul>
<li><strong>Authors: </strong>Nooshin Yousefzadeh, Rahul Sengupta, Sanjay Ranka</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18008">https://arxiv.org/abs/2504.18008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18008">https://arxiv.org/pdf/2504.18008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18008]] TGDT: A Temporal Graph-based Digital Twin for Urban Traffic Corridors(https://arxiv.org/abs/2504.18008)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Urban congestion at signalized intersections leads to significant delays, economic losses, and increased emissions. Existing deep learning models often lack spatial generalizability, rely on complex architectures, and struggle with real-time deployment. To address these limitations, we propose the Temporal Graph-based Digital Twin (TGDT), a scalable framework that integrates Temporal Convolutional Networks and Attentional Graph Neural Networks for dynamic, direction-aware traffic modeling and assessment at urban corridors. TGDT estimates key Measures of Effectiveness (MOEs) for traffic flow optimization at both the intersection level (e.g., queue length, waiting time) and the corridor level (e.g., traffic volume, travel time). Its modular architecture and sequential optimization scheme enable easy extension to any number of intersections and MOEs. The model outperforms state-of-the-art baselines by accurately producing high-dimensional, concurrent multi-output estimates. It also demonstrates high robustness and accuracy across diverse traffic conditions, including extreme scenarios, while relying on only a minimal set of traffic features. Fully parallelized, TGDT can simulate over a thousand scenarios within a matter of seconds, offering a cost-effective, interpretable, and real-time solution for traffic signal optimization.</li>
</ul>

<h3>Title: Diffusion-Driven Universal Model Inversion Attack for Face Recognition</h3>
<ul>
<li><strong>Authors: </strong>Hanrui Wang, Shuo Wang, Chun-Shien Lu, Isao Echizen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18015">https://arxiv.org/abs/2504.18015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18015">https://arxiv.org/pdf/2504.18015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18015]] Diffusion-Driven Universal Model Inversion Attack for Face Recognition(https://arxiv.org/abs/2504.18015)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, biometric, diffusion</a></li>
<li><strong>Abstract: </strong>Facial recognition technology poses significant privacy risks, as it relies on biometric data that is inherently sensitive and immutable if compromised. To mitigate these concerns, face recognition systems convert raw images into embeddings, traditionally considered privacy-preserving. However, model inversion attacks pose a significant privacy threat by reconstructing these private facial images, making them a crucial tool for evaluating the privacy risks of face recognition systems. Existing methods usually require training individual generators for each target model, a computationally expensive process. In this paper, we propose DiffUMI, a training-free diffusion-driven universal model inversion attack for face recognition systems. DiffUMI is the first approach to apply a diffusion model for unconditional image generation in model inversion. Unlike other methods, DiffUMI is universal, eliminating the need for training target-specific generators. It operates within a fixed framework and pretrained diffusion model while seamlessly adapting to diverse target identities and models. DiffUMI breaches privacy-preserving face recognition systems with state-of-the-art success, demonstrating that an unconditional diffusion model, coupled with optimized adversarial search, enables efficient and high-fidelity facial reconstruction. Additionally, we introduce a novel application of out-of-domain detection (OODD), marking the first use of model inversion to distinguish non-face inputs from face inputs based solely on embeddings.</li>
</ul>

<h3>Title: Federated Client-tailored Adapter for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Guyue Hu, Siyuan Song, Yukun Kang, Zhu Yin, Gangming Zhao, Chenglong Li, Jin Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18020">https://arxiv.org/abs/2504.18020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18020">https://arxiv.org/pdf/2504.18020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18020]] Federated Client-tailored Adapter for Medical Image Segmentation(https://arxiv.org/abs/2504.18020)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation in X-ray images is beneficial for computer-aided diagnosis and lesion localization. Existing methods mainly fall into a centralized learning paradigm, which is inapplicable in the practical medical scenario that only has access to distributed data islands. Federated Learning has the potential to offer a distributed solution but struggles with heavy training instability due to client-wise domain heterogeneity (including distribution diversity and class imbalance). In this paper, we propose a novel Federated Client-tailored Adapter (FCA) framework for medical image segmentation, which achieves stable and client-tailored adaptive segmentation without sharing sensitive local data. Specifically, the federated adapter stirs universal knowledge in off-the-shelf medical foundation models to stabilize the federated training process. In addition, we develop two client-tailored federated updating strategies that adaptively decompose the adapter into common and individual components, then globally and independently update the parameter groups associated with common client-invariant and individual client-specific units, respectively. They further stabilize the heterogeneous federated learning process and realize optimal client-tailored instead of sub-optimal global-compromised segmentation models. Extensive experiments on three large-scale datasets demonstrate the effectiveness and superiority of the proposed FCA framework for federated medical segmentation.</li>
</ul>

<h3>Title: A Large Vision-Language Model based Environment Perception System for Visually Impaired People</h3>
<ul>
<li><strong>Authors: </strong>Zezhou Chen, Zhaoxiang Liu, Kai Wang, Kohou Wang, Shiguo Lian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18027">https://arxiv.org/abs/2504.18027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18027">https://arxiv.org/pdf/2504.18027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18027]] A Large Vision-Language Model based Environment Perception System for Visually Impaired People(https://arxiv.org/abs/2504.18027)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>It is a challenging task for visually impaired people to perceive their surrounding environment due to the complexity of the natural scenes. Their personal and social activities are thus highly limited. This paper introduces a Large Vision-Language Model(LVLM) based environment perception system which helps them to better understand the surrounding environment, by capturing the current scene they face with a wearable device, and then letting them retrieve the analysis results through the device. The visually impaired people could acquire a global description of the scene by long pressing the screen to activate the LVLM output, retrieve the categories of the objects in the scene resulting from a segmentation model by tapping or swiping the screen, and get a detailed description of the objects they are interested in by double-tapping the screen. To help visually impaired people more accurately perceive the world, this paper proposes incorporating the segmentation result of the RGB image as external knowledge into the input of LVLM to reduce the LVLM's hallucination. Technical experiments on POPE, MME and LLaVA-QA90 show that the system could provide a more accurate description of the scene compared to Qwen-VL-Chat, exploratory experiments show that the system helps visually impaired people to perceive the surrounding environment effectively.</li>
</ul>

<h3>Title: Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Chen Chen, Daochang Liu, Mubarak Shah, Chang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18032">https://arxiv.org/abs/2504.18032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18032">https://arxiv.org/pdf/2504.18032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18032]] Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models(https://arxiv.org/abs/2504.18032)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have demonstrated remarkable capabilities in creating images highly aligned with user prompts, yet their proclivity for memorizing training set images has sparked concerns about the originality of the generated images and privacy issues, potentially leading to legal complications for both model owners and users, particularly when the memorized images contain proprietary content. Although methods to mitigate these issues have been suggested, enhancing privacy often results in a significant decrease in the utility of the outputs, as indicated by text-alignment scores. To bridge the research gap, we introduce a novel method, PRSS, which refines the classifier-free guidance approach in diffusion models by integrating prompt re-anchoring (PR) to improve privacy and incorporating semantic prompt search (SS) to enhance utility. Extensive experiments across various privacy levels demonstrate that our approach consistently improves the privacy-utility trade-off, establishing a new state-of-the-art.</li>
</ul>

<h3>Title: Cabbage: A Differential Growth Framework for Open Surfaces</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyi Liu, Hao Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18040">https://arxiv.org/abs/2504.18040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18040">https://arxiv.org/pdf/2504.18040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18040]] Cabbage: A Differential Growth Framework for Open Surfaces(https://arxiv.org/abs/2504.18040)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose Cabbage, a differential growth framework to model buckling behavior in 3D open surfaces found in nature-like the curling of flower petals. Cabbage creates high-quality triangular meshes free of self-intersection. Cabbage-Shell is driven by edge subdivision which differentially increases discretization resolution. Shell forces expands the surface, generating buckling over time. Feature-aware smoothing and remeshing ensures mesh quality. Corrective collision effectively prevents self-collision even in tight spaces. We additionally provide Cabbage-Collision, and approximate alternative, followed by CAD-ready surface generation. Cabbage is the first open-source effort with this calibre and robustness, outperforming SOTA methods in its morphological expressiveness, mesh quality, and stably generates large, complex patterns over hundreds of simulation steps. It is a source not only of computational modeling, digital fabrication, education, but also high-quality, annotated data for geometry processing and shape analysis.</li>
</ul>

<h3>Title: RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bang An, Shiyue Zhang, Mark Dredze</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18041">https://arxiv.org/abs/2504.18041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18041">https://arxiv.org/pdf/2504.18041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18041]] RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models(https://arxiv.org/abs/2504.18041)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Efforts to ensure the safety of large language models (LLMs) include safety fine-tuning, evaluation, and red teaming. However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on standard LLMs, which means we know little about how RAG use cases change a model's safety profile. We conduct a detailed comparative analysis of RAG and non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe and change their safety profile. We explore the causes of this change and find that even combinations of safe models with safe documents can cause unsafe generations. In addition, we evaluate some existing red teaming methods for RAG settings and show that they are less effective than when used for non-RAG settings. Our work highlights the need for safety research and red-teaming methods specifically tailored for RAG LLMs.</li>
</ul>

<h3>Title: Modes of Sequence Models and Learning Coefficients</h3>
<ul>
<li><strong>Authors: </strong>Zhongtian Chen, Daniel Murfet</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18048">https://arxiv.org/abs/2504.18048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18048">https://arxiv.org/pdf/2504.18048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18048]] Modes of Sequence Models and Learning Coefficients(https://arxiv.org/abs/2504.18048)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We develop a geometric account of sequence modelling that links patterns in the data to measurable properties of the loss landscape in transformer networks. First, we cast conditional sequence distributions into a Hilbert-space framework and apply tensor decompositions to identify their principal modes. Truncating the small-amplitude modes yields an effective data distribution that preserves dominant structure while discarding statistical detail. Second, we show theoretically that Local Learning Coefficient (LLC) estimates are insensitive to modes below a data-dependent threshold. Consequently, the LLC calculated in practice characterises the geometry of the effective rather than the true distribution. This insight clarifies why reliable LLC estimates can be obtained even when a network parameter is not a strict minimiser of the population loss, and it highlights how the inverse temperature in SGLD acts as a resolution dial on the landscape structure.</li>
</ul>

<h3>Title: A BERT-Style Self-Supervised Learning CNN for Disease Identification from Retinal Images</h3>
<ul>
<li><strong>Authors: </strong>Xin Li, Wenhui Zhu, Peijie Qiu, Oana M. Dumitrascu, Amal Youssef, Yalin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18049">https://arxiv.org/abs/2504.18049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18049">https://arxiv.org/pdf/2504.18049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18049]] A BERT-Style Self-Supervised Learning CNN for Disease Identification from Retinal Images(https://arxiv.org/abs/2504.18049)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In the field of medical imaging, the advent of deep learning, especially the application of convolutional neural networks (CNNs) has revolutionized the analysis and interpretation of medical images. Nevertheless, deep learning methods usually rely on large amounts of labeled data. In medical imaging research, the acquisition of high-quality labels is both expensive and difficult. The introduction of Vision Transformers (ViT) and self-supervised learning provides a pre-training strategy that utilizes abundant unlabeled data, effectively alleviating the label acquisition challenge while broadening the breadth of data utilization. However, ViT's high computational density and substantial demand for computing power, coupled with the lack of localization characteristics of its operations on image patches, limit its efficiency and applicability in many application scenarios. In this study, we employ nn-MobileNet, a lightweight CNN framework, to implement a BERT-style self-supervised learning approach. We pre-train the network on the unlabeled retinal fundus images from the UK Biobank to improve downstream application performance. We validate the results of the pre-trained model on Alzheimer's disease (AD), Parkinson's disease (PD), and various retinal diseases identification. The results show that our approach can significantly improve performance in the downstream tasks. In summary, this study combines the benefits of CNNs with the capabilities of advanced self-supervised learning in handling large-scale unlabeled data, demonstrating the potential of CNNs in the presence of label scarcity.</li>
</ul>

<h3>Title: DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jianyu Liu, Hangyu Guo, Ranjie Duan, Xingyuan Bu, Yancheng He, Shilong Li, Hui Huang, Jiaheng Liu, Yucheng Wang, Chenchen Jing, Xingwei Qu, Xiao Zhang, Yingshui Tan, Yanan Wu, Jihao Gu, Yangguang Li, Jianke Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18053">https://arxiv.org/abs/2504.18053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18053">https://arxiv.org/pdf/2504.18053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18053]] DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models(https://arxiv.org/abs/2504.18053)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) pose unique safety challenges due to their integration of visual and textual data, thereby introducing new dimensions of potential attacks and complex risk combinations. In this paper, we begin with a detailed analysis aimed at disentangling risks through step-by-step reasoning within multimodal inputs. We find that systematic multimodal risk disentanglement substantially enhances the risk awareness of MLLMs. Via leveraging the strong discriminative abilities of multimodal risk disentanglement, we further introduce \textbf{DREAM} (\textit{\textbf{D}isentangling \textbf{R}isks to \textbf{E}nhance Safety \textbf{A}lignment in \textbf{M}LLMs}), a novel approach that enhances safety alignment in MLLMs through supervised fine-tuning and iterative Reinforcement Learning from AI Feedback (RLAIF). Experimental results show that DREAM significantly boosts safety during both inference and training phases without compromising performance on normal tasks (namely oversafety), achieving a 16.17\% improvement in the SIUO safe\&effective score compared to GPT-4V. The data and code are available at this https URL.</li>
</ul>

<h3>Title: POET: Prompt Offset Tuning for Continual Human Action Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Prachi Garg, Joseph K J, Vineeth N Balasubramanian, Necati Cihan Camgoz, Chengde Wan, Kenrick Kin, Weiguang Si, Shugao Ma, Fernando De La Torre</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18059">https://arxiv.org/abs/2504.18059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18059">https://arxiv.org/pdf/2504.18059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18059]] POET: Prompt Offset Tuning for Continual Human Action Adaptation(https://arxiv.org/abs/2504.18059)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, transformer</a></li>
<li><strong>Abstract: </strong>As extended reality (XR) is redefining how users interact with computing devices, research in human action recognition is gaining prominence. Typically, models deployed on immersive computing devices are static and limited to their default set of classes. The goal of our research is to provide users and developers with the capability to personalize their experience by adding new action classes to their device models continually. Importantly, a user should be able to add new classes in a low-shot and efficient manner, while this process should not require storing or replaying any of user's sensitive training data. We formalize this problem as privacy-aware few-shot continual action recognition. Towards this end, we propose POET: Prompt-Offset Tuning. While existing prompt tuning approaches have shown great promise for continual learning of image, text, and video modalities; they demand access to extensively pretrained transformers. Breaking away from this assumption, POET demonstrates the efficacy of prompt tuning a significantly lightweight backbone, pretrained exclusively on the base class data. We propose a novel spatio-temporal learnable prompt offset tuning approach, and are the first to apply such prompt tuning to Graph Neural Networks. We contribute two new benchmarks for our new problem setting in human action recognition: (i) NTU RGB+D dataset for activity recognition, and (ii) SHREC-2017 dataset for hand gesture recognition. We find that POET consistently outperforms comprehensive benchmarks. Source code at this https URL.</li>
</ul>

<h3>Title: S3MOT: Monocular 3D Object Tracking with Selective State Space Model</h3>
<ul>
<li><strong>Authors: </strong>Zhuohao Yan, Shaoquan Feng, Xingxing Li, Yuxuan Zhou, Chunxi Xia, Shengyu Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18068">https://arxiv.org/abs/2504.18068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18068">https://arxiv.org/pdf/2504.18068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18068]] S3MOT: Monocular 3D Object Tracking with Selective State Space Model(https://arxiv.org/abs/2504.18068)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate and reliable multi-object tracking (MOT) in 3D space is essential for advancing robotics and computer vision applications. However, it remains a significant challenge in monocular setups due to the difficulty of mining 3D spatiotemporal associations from 2D video streams. In this work, we present three innovative techniques to enhance the fusion and exploitation of heterogeneous cues for monocular 3D MOT: (1) we introduce the Hungarian State Space Model (HSSM), a novel data association mechanism that compresses contextual tracking cues across multiple paths, enabling efficient and comprehensive assignment decisions with linear complexity. HSSM features a global receptive field and dynamic weights, in contrast to traditional linear assignment algorithms that rely on hand-crafted association costs. (2) We propose Fully Convolutional One-stage Embedding (FCOE), which eliminates ROI pooling by directly using dense feature maps for contrastive learning, thus improving object re-identification accuracy under challenging conditions such as varying viewpoints and lighting. (3) We enhance 6-DoF pose estimation through VeloSSM, an encoder-decoder architecture that models temporal dependencies in velocity to capture motion dynamics, overcoming the limitations of frame-based 3D inference. Experiments on the KITTI public test benchmark demonstrate the effectiveness of our method, achieving a new state-of-the-art performance of 76.86~HOTA at 31~FPS. Our approach outperforms the previous best by significant margins of +2.63~HOTA and +3.62~AssA, showcasing its robustness and efficiency for monocular 3D MOT tasks. The code and models are available at this https URL.</li>
</ul>

<h3>Title: PropRAG: Guiding Retrieval with Beam Search over Proposition Paths</h3>
<ul>
<li><strong>Authors: </strong>Jingjin Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18070">https://arxiv.org/abs/2504.18070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18070">https://arxiv.org/pdf/2504.18070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18070]] PropRAG: Guiding Retrieval with Beam Search over Proposition Paths(https://arxiv.org/abs/2504.18070)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval Augmented Generation (RAG) has become the standard non-parametric approach for equipping Large Language Models (LLMs) with up-to-date knowledge and mitigating catastrophic forgetting common in continual learning. However, standard RAG, relying on independent passage retrieval, fails to capture the interconnected nature of human memory crucial for complex reasoning (associativity) and contextual understanding (sense-making). While structured RAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples, the inherent context loss limits fidelity. We introduce PropRAG, a framework leveraging contextually rich propositions and a novel beam search algorithm over proposition paths to explicitly discover multi-step reasoning chains. Crucially, PropRAG's online retrieval process operates entirely without invoking generative LLMs, relying instead on efficient graph traversal and pre-computed embeddings. This avoids online LLM inference costs and potential inconsistencies during evidence gathering. LLMs are used effectively offline for high-quality proposition extraction and post-retrieval for answer generation. PropRAG achieves state-of-the-art zero-shot Recall@5 results on PopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside top F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through richer representation and explicit, LLM-free online path finding, PropRAG advances non-parametric continual learning.</li>
</ul>

<h3>Title: Privacy-Preserving Personalized Federated Learning for Distributed Photovoltaic Disaggregation under Statistical Heterogeneity</h3>
<ul>
<li><strong>Authors: </strong>Xiaolu Chen, Chenghao Huang, Yanru Zhang, Hao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18078">https://arxiv.org/abs/2504.18078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18078">https://arxiv.org/pdf/2504.18078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18078]] Privacy-Preserving Personalized Federated Learning for Distributed Photovoltaic Disaggregation under Statistical Heterogeneity(https://arxiv.org/abs/2504.18078)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate, transformer</a></li>
<li><strong>Abstract: </strong>The rapid expansion of distributed photovoltaic (PV) installations worldwide, many being behind-the-meter systems, has significantly challenged energy management and grid operations, as unobservable PV generation further complicates the supply-demand balance. Therefore, estimating this generation from net load, known as PV disaggregation, is critical. Given privacy concerns and the need for large training datasets, federated learning becomes a promising approach, but statistical heterogeneity, arising from geographical and behavioral variations among prosumers, poses new challenges to PV disaggregation. To overcome these challenges, a privacy-preserving distributed PV disaggregation framework is proposed using Personalized Federated Learning (PFL). The proposed method employs a two-level framework that combines local and global modeling. At the local level, a transformer-based PV disaggregation model is designed to generate solar irradiance embeddings for representing local PV conditions. A novel adaptive local aggregation mechanism is adopted to mitigate the impact of statistical heterogeneity on the local model, extracting a portion of global information that benefits the local model. At the global level, a central server aggregates information uploaded from multiple data centers, preserving privacy while enabling cross-center knowledge sharing. Experiments on real-world data demonstrate the effectiveness of this proposed framework, showing improved accuracy and robustness compared to benchmark methods.</li>
</ul>

<h3>Title: Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Wataru Kawakami, Keita Suzuki, Junichiro Iwasawa</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18080">https://arxiv.org/abs/2504.18080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18080">https://arxiv.org/pdf/2504.18080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18080]] Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning Preference Optimization(https://arxiv.org/abs/2504.18080)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) show potential in medicine, yet clinical adoption is hindered by concerns over factual accuracy, language-specific limitations (e.g., Japanese), and critically, their reliability when required to generate reasoning explanations -- a prerequisite for trust. This paper introduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the Japanese medical domain to achieve both high accuracy and stable reasoning. We employ a two-stage fine-tuning process on the Qwen2.5-72B base model: first, Continued Pretraining (CPT) on a comprehensive Japanese medical corpus instills deep domain knowledge. Second, Reasoning Preference Optimization (RPO), a preference-based method, enhances the generation of reliable reasoning pathways while preserving high answer accuracy. Evaluations on the Japanese Medical Licensing Exam benchmark (IgakuQA) show Preferred-MedLLM-Qwen-72B achieves state-of-the-art performance (0.868 accuracy), surpassing strong proprietary models like GPT-4o (0.866). Crucially, unlike baseline or CPT-only models which exhibit significant accuracy degradation (up to 11.5\% and 3.8\% respectively on IgakuQA) when prompted for explanations, our model maintains its high accuracy (0.868) under such conditions. This highlights RPO's effectiveness in stabilizing reasoning generation. This work underscores the importance of optimizing for reliable explanations alongside accuracy. We release the Preferred-MedLLM-Qwen-72B model weights to foster research into trustworthy LLMs for specialized, high-stakes applications.</li>
</ul>

<h3>Title: Automating Function-Level TARA for Automotive Full-Lifecycle Security</h3>
<ul>
<li><strong>Authors: </strong>Yuqiao Yang, Yongzhao Zhang, Wenhao Liu, Jun Li, Pengtao Shi, DingYu Zhong, Jie Yang, Ting Chen, Sheng Cao, Yuntao Ren, Yongyue Wu, Xiaosong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18083">https://arxiv.org/abs/2504.18083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18083">https://arxiv.org/pdf/2504.18083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18083]] Automating Function-Level TARA for Automotive Full-Lifecycle Security(https://arxiv.org/abs/2504.18083)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>As modern vehicles evolve into intelligent and connected systems, their growing complexity introduces significant cybersecurity risks. Threat Analysis and Risk Assessment (TARA) has therefore become essential for managing these risks under mandatory regulations. However, existing TARA automation methods rely on static threat libraries, limiting their utility in the detailed, function-level analyses demanded by industry. This paper introduces DefenseWeaver, the first system that automates function-level TARA using component-specific details and large language models (LLMs). DefenseWeaver dynamically generates attack trees and risk evaluations from system configurations described in an extended OpenXSAM++ format, then employs a multi-agent framework to coordinate specialized LLM roles for more robust analysis. To further adapt to evolving threats and diverse standards, DefenseWeaver incorporates Low-Rank Adaptation (LoRA) fine-tuning and Retrieval-Augmented Generation (RAG) with expert-curated TARA reports. We validated DefenseWeaver through deployment in four automotive security projects, where it identified 11 critical attack paths, verified through penetration testing, and subsequently reported and remediated by the relevant automakers and suppliers. Additionally, DefenseWeaver demonstrated cross-domain adaptability, successfully applying to unmanned aerial vehicles (UAVs) and marine navigation systems. In comparison to human experts, DefenseWeaver outperformed manual attack tree generation across six assessment scenarios. Integrated into commercial cybersecurity platforms such as UAES and Xiaomi, DefenseWeaver has generated over 8,200 attack trees. These results highlight its ability to significantly reduce processing time, and its scalability and transformative impact on cybersecurity across industries.</li>
</ul>

<h3>Title: Random-Set Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Mubashar, Shireen Kudukkil Manchingal, Fabio Cuzzolin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18085">https://arxiv.org/abs/2504.18085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18085">https://arxiv.org/pdf/2504.18085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18085]] Random-Set Large Language Models(https://arxiv.org/abs/2504.18085)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are known to produce very high-quality tests and responses to our queries. But how much can we trust this generated text? In this paper, we study the problem of uncertainty quantification in LLMs. We propose a novel Random-Set Large Language Model (RSLLM) approach which predicts finite random sets (belief functions) over the token space, rather than probability vectors as in classical LLMs. In order to allow so efficiently, we also present a methodology based on hierarchical clustering to extract and use a budget of "focal" subsets of tokens upon which the belief prediction is defined, rather than using all possible collections of tokens, making the method scalable yet effective. RS-LLMs encode the epistemic uncertainty induced in their generation process by the size and diversity of its training set via the size of the credal sets associated with the predicted belief functions. The proposed approach is evaluated on CoQA and OBQA datasets using Llama2-7b, Mistral-7b and Phi-2 models and is shown to outperform the standard model in both datasets in terms of correctness of answer while also showing potential in estimating the second level uncertainty in its predictions and providing the capability to detect when its hallucinating.</li>
</ul>

<h3>Title: Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation</h3>
<ul>
<li><strong>Authors: </strong>Weipeng Tan, Chuming Lin, Chengming Xu, FeiFan Xu, Xiaobin Hu, Xiaozhong Ji, Junwei Zhu, Chengjie Wang, Yanwei Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18087">https://arxiv.org/abs/2504.18087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18087">https://arxiv.org/pdf/2504.18087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18087]] Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation(https://arxiv.org/abs/2504.18087)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in Talking Head Generation (THG) have achieved impressive lip synchronization and visual quality through diffusion models; yet existing methods struggle to generate emotionally expressive portraits while preserving speaker identity. We identify three critical limitations in current emotional talking head generation: insufficient utilization of audio's inherent emotional cues, identity leakage in emotion representations, and isolated learning of emotion correlations. To address these challenges, we propose a novel framework dubbed as DICE-Talk, following the idea of disentangling identity with emotion, and then cooperating emotions with similar characteristics. First, we develop a disentangled emotion embedder that jointly models audio-visual emotional cues through cross-modal attention, representing emotions as identity-agnostic Gaussian distributions. Second, we introduce a correlation-enhanced emotion conditioning module with learnable Emotion Banks that explicitly capture inter-emotion relationships through vector quantization and attention-based feature aggregation. Third, we design an emotion discrimination objective that enforces affective consistency during the diffusion process through latent-space classification. Extensive experiments on MEAD and HDTF datasets demonstrate our method's superiority, outperforming state-of-the-art approaches in emotion accuracy while maintaining competitive lip-sync performance. Qualitative results and user studies further confirm our method's ability to generate identity-preserving portraits with rich, correlated emotional expressions that naturally adapt to unseen identities.</li>
</ul>

<h3>Title: Subject-independent Classification of Meditative State from the Resting State using EEG</h3>
<ul>
<li><strong>Authors: </strong>Jerrin Thomas Panachakel, Pradeep Kumar G., Suryaa Seran, Kanishka Sharma, Ramakrishnan Angarai Ganesan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18095">https://arxiv.org/abs/2504.18095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18095">https://arxiv.org/pdf/2504.18095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18095]] Subject-independent Classification of Meditative State from the Resting State using EEG(https://arxiv.org/abs/2504.18095)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>While it is beneficial to objectively determine whether a subject is meditating, most research in the literature reports good results only in a subject-dependent manner. This study aims to distinguish the modified state of consciousness experienced during Rajyoga meditation from the resting state of the brain in a subject-independent manner using EEG data. Three architectures have been proposed and evaluated: The CSP-LDA Architecture utilizes common spatial pattern (CSP) for feature extraction and linear discriminant analysis (LDA) for classification. The CSP-LDA-LSTM Architecture employs CSP for feature extraction, LDA for dimensionality reduction, and long short-term memory (LSTM) networks for classification, modeling the binary classification problem as a sequence learning problem. The SVD-NN Architecture uses singular value decomposition (SVD) to select the most relevant components of the EEG signals and a shallow neural network (NN) for classification. The CSP-LDA-LSTM architecture gives the best performance with 98.2% accuracy for intra-subject classification. The SVD-NN architecture provides significant performance with 96.4\% accuracy for inter-subject classification. This is comparable to the best-reported accuracies in the literature for intra-subject classification. Both architectures are capable of capturing subject-invariant EEG features for effectively classifying the meditative state from the resting state. The high intra-subject and inter-subject classification accuracies indicate these systems' robustness and their ability to generalize across different subjects.</li>
</ul>

<h3>Title: Application and Optimization of Large Models Based on Prompt Tuning for Fact-Check-Worthiness Estimation</h3>
<ul>
<li><strong>Authors: </strong>Yinglong Yu, Hao Shen, Zhengyi Lyu, Qi He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18104">https://arxiv.org/abs/2504.18104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18104">https://arxiv.org/pdf/2504.18104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18104]] Application and Optimization of Large Models Based on Prompt Tuning for Fact-Check-Worthiness Estimation(https://arxiv.org/abs/2504.18104)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In response to the growing problem of misinformation in the context of globalization and informatization, this paper proposes a classification method for fact-check-worthiness estimation based on prompt tuning. We construct a model for fact-check-worthiness estimation at the methodological level using prompt tuning. By applying designed prompt templates to large language models, we establish in-context learning and leverage prompt tuning technology to improve the accuracy of determining whether claims have fact-check-worthiness, particularly when dealing with limited or unlabeled data. Through extensive experiments on public datasets, we demonstrate that the proposed method surpasses or matches multiple baseline methods in the classification task of fact-check-worthiness estimation assessment, including classical pre-trained models such as BERT, as well as recent popular large models like GPT-3.5 and GPT-4. Experiments show that the prompt tuning-based method proposed in this study exhibits certain advantages in evaluation metrics such as F1 score and accuracy, thereby effectively validating its effectiveness and advancement in the task of fact-check-worthiness estimation.</li>
</ul>

<h3>Title: Temperature Estimation in Induction Motors using Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Dinan Li, Panagiotis Kakosimos</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18105">https://arxiv.org/abs/2504.18105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18105">https://arxiv.org/pdf/2504.18105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18105]] Temperature Estimation in Induction Motors using Machine Learning(https://arxiv.org/abs/2504.18105)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>The number of electrified powertrains is ever increasing today towards a more sustainable future; thus, it is essential that unwanted failures are prevented, and a reliable operation is secured. Monitoring the internal temperatures of motors and keeping them under their thresholds is an important first step. Conventional modeling methods require expert knowledge and complicated mathematical approaches. With all the data a modern electric drive collects nowadays during the system operation, it is feasible to apply data-driven approaches for estimating thermal behaviors. In this paper, multiple machine-learning methods are investigated on their capability to approximate the temperatures of the stator winding and bearing in induction motors. The explored algorithms vary from linear to neural networks. For this reason, experimental lab data have been captured from a powertrain under predetermined operating conditions. For each approach, a hyperparameter search is then performed to find the optimal configuration. All the models are evaluated by various metrics, and it has been found that neural networks perform satisfactorily even under transient conditions.</li>
</ul>

<h3>Title: Comparative Study on the Discourse Meaning of Chinese and English Media in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt Engineering</h3>
<ul>
<li><strong>Authors: </strong>Yinglong Yu, Zhaopu Yao, Fang Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18106">https://arxiv.org/abs/2504.18106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18106">https://arxiv.org/pdf/2504.18106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18106]] Comparative Study on the Discourse Meaning of Chinese and English Media in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt Engineering(https://arxiv.org/abs/2504.18106)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study analyzes Chinese and English media reports on the Paris Olympics using topic modeling, Large Language Model (LLM) prompt engineering, and corpus phraseology methods to explore similarities and differences in discourse construction and attitudinal meanings. Common topics include the opening ceremony, athlete performance, and sponsorship brands. Chinese media focus on specific sports, sports spirit, doping controversies, and new technologies, while English media focus on female athletes, medal wins, and eligibility controversies. Chinese reports show more frequent prepositional co-occurrences and positive semantic prosody in describing the opening ceremony and sports spirit. English reports exhibit positive semantic prosody when covering female athletes but negative prosody in predicting opening ceremony reactions and discussing women's boxing controversies.</li>
</ul>

<h3>Title: Study on Real-Time Road Surface Reconstruction Using Stereo Vision</h3>
<ul>
<li><strong>Authors: </strong>Deepak Ghimire, Byoungjun Kim, Donghoon Kim, SungHwan Jeong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18112">https://arxiv.org/abs/2504.18112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18112">https://arxiv.org/pdf/2504.18112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18112]] Study on Real-Time Road Surface Reconstruction Using Stereo Vision(https://arxiv.org/abs/2504.18112)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Road surface reconstruction plays a crucial role in autonomous driving, providing essential information for safe and smooth navigation. This paper enhances the RoadBEV [1] framework for real-time inference on edge devices by optimizing both efficiency and accuracy. To achieve this, we proposed to apply Isomorphic Global Structured Pruning to the stereo feature extraction backbone, reducing network complexity while maintaining performance. Additionally, the head network is redesigned with an optimized hourglass structure, dynamic attention heads, reduced feature channels, mixed precision inference, and efficient probability volume computation. Our approach improves inference speed while achieving lower reconstruction error, making it well-suited for real-time road surface reconstruction in autonomous driving.</li>
</ul>

<h3>Title: Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection</h3>
<ul>
<li><strong>Authors: </strong>Atharva Kulkarni, Yuan Zhang, Joel Ruben Antony Moniz, Xiou Ge, Bo-Hsiang Tseng, Dhivya Piraviperumal, Swabha Swayamdipta, Hong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18114">https://arxiv.org/abs/2504.18114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18114">https://arxiv.org/pdf/2504.18114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18114]] Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection(https://arxiv.org/abs/2504.18114)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Hallucinations pose a significant obstacle to the reliability and widespread adoption of language models, yet their accurate measurement remains a persistent challenge. While many task- and domain-specific metrics have been proposed to assess faithfulness and factuality concerns, the robustness and generalization of these metrics are still untested. In this paper, we conduct a large-scale empirical evaluation of 6 diverse sets of hallucination detection metrics across 4 datasets, 37 language models from 5 families, and 5 decoding methods. Our extensive investigation reveals concerning gaps in current hallucination evaluation: metrics often fail to align with human judgments, take an overtly myopic view of the problem, and show inconsistent gains with parameter scaling. Encouragingly, LLM-based evaluation, particularly with GPT-4, yields the best overall results, and mode-seeking decoding methods seem to reduce hallucinations, especially in knowledge-grounded settings. These findings underscore the need for more robust metrics to understand and quantify hallucinations, and better strategies to mitigate them.</li>
</ul>

<h3>Title: Think, Prune, Train, Improve: Scaling Reasoning without Scaling Models</h3>
<ul>
<li><strong>Authors: </strong>Caia Costello, Simon Guo, Anna Goldie, Azalia Mirhoseini</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18116">https://arxiv.org/abs/2504.18116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18116">https://arxiv.org/pdf/2504.18116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18116]] Think, Prune, Train, Improve: Scaling Reasoning without Scaling Models(https://arxiv.org/abs/2504.18116)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated strong capabilities in programming and mathematical reasoning tasks, but are constrained by limited high-quality training data. Synthetic data can be leveraged to enhance fine-tuning outcomes, but several factors influence this process, including model size, synthetic data volume, pruning strategy, and number of fine-tuning rounds. We explore these axes and investigate which conditions enable model self-improvement. We introduce the Think, Prune, Train process, a scalable framework that iteratively fine-tunes models on their own reasoning traces, using ground-truth pruning to ensure high-quality training data. This approach yields improved performance: on GSM8K, Gemma2-2B achieves a Pass@1 of 57.6% (from 41.9%), Gemma2-9B reaches 82%, matching LLaMA-3.1-70B, and LLaMA-3.1-70B attains 91%, even surpassing GPT-4o, demonstrating the effectiveness of self-generated reasoning and systematic data selection for improving LLM capabilities.</li>
</ul>

<h3>Title: Score-Based Deterministic Density Sampling</h3>
<ul>
<li><strong>Authors: </strong>Vasily Ilin, Bamdad Hosseini, Jingwei Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.PR, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18130">https://arxiv.org/abs/2504.18130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18130">https://arxiv.org/pdf/2504.18130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18130]] Score-Based Deterministic Density Sampling(https://arxiv.org/abs/2504.18130)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We propose and analyze a deterministic sampling framework using Score-Based Transport Modeling (SBTM) for sampling an unnormalized target density $\pi$. While diffusion generative modeling relies on pre-training the score function $\nabla \log f_t$ using samples from $\pi$, SBTM addresses the more general and challenging setting where only $\nabla \log\pi$ is known. SBTM approximates the Wasserstein gradient flow on KL$(f_t\|\pi)$ by learning the time-varying score $\nabla \log f_t$ on the fly using score matching. The learned score gives immediate access to relative Fisher information, providing a built-in convergence diagnostic. The deterministic trajectories are smooth, interpretable, and free of Brownian-motion noise, while having the same distribution as ULA. We prove that SBTM dissipates relative entropy at the same rate as the exact gradient flow, provided sufficient training. We further extend our framework to annealed dynamics, to handle non log-concave targets. Numerical experiments validate our theoretical findings: SBTM converges at the optimal rate, has smooth trajectories, and is easily integrated with annealed dynamics. We compare to the baselines of ULA and annealed ULA.</li>
</ul>

<h3>Title: Tree Boosting Methods for Balanced andImbalanced Classification and their Robustness Over Time in Risk Assessment</h3>
<ul>
<li><strong>Authors: </strong>Gissel Velarde, Michael Weichert, Anuj Deshmunkh, Sanjay Deshmane, Anindya Sudhir, Khushboo Sharma, Vaibhav Joshi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18133">https://arxiv.org/abs/2504.18133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18133">https://arxiv.org/pdf/2504.18133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18133]] Tree Boosting Methods for Balanced andImbalanced Classification and their Robustness Over Time in Risk Assessment(https://arxiv.org/abs/2504.18133)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Most real-world classification problems deal with imbalanced datasets, posing a challenge for Artificial Intelligence (AI), i.e., machine learning algorithms, because the minority class, which is of extreme interest, often proves difficult to be detected. This paper empirically evaluates tree boosting methods' performance given different dataset sizes and class distributions, from perfectly balanced to highly imbalanced. For tabular data, tree-based methods such as XGBoost, stand out in several benchmarks due to detection performance and speed. Therefore, XGBoost and Imbalance-XGBoost are evaluated. After introducing the motivation to address risk assessment with machine learning, the paper reviews evaluation metrics for detection systems or binary classifiers. It proposes a method for data preparation followed by tree boosting methods including hyper-parameter optimization. The method is evaluated on private datasets of 1 thousand (K), 10K and 100K samples on distributions with 50, 45, 25, and 5 percent positive samples. As expected, the developed method increases its recognition performance as more data is given for training and the F1 score decreases as the data distribution becomes more imbalanced, but it is still significantly superior to the baseline of precision-recall determined by the ratio of positives divided by positives and negatives. Sampling to balance the training set does not provide consistent improvement and deteriorates detection. In contrast, classifier hyper-parameter optimization improves recognition, but should be applied carefully depending on data volume and distribution. Finally, the developed method is robust to data variation over time up to some point. Retraining can be used when performance starts deteriorating.</li>
</ul>

<h3>Title: NoEsis: Differentially Private Knowledge Transfer in Modular LLM Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Rob Romijnders, Stefanos Laskaridis, Ali Shahin Shamsabadi, Hamed Haddadi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18147">https://arxiv.org/abs/2504.18147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18147">https://arxiv.org/pdf/2504.18147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18147]] NoEsis: Differentially Private Knowledge Transfer in Modular LLM Adaptation(https://arxiv.org/abs/2504.18147)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLM) are typically trained on vast amounts of data from various sources. Even when designed modularly (e.g., Mixture-of-Experts), LLMs can leak privacy on their sources. Conversely, training such models in isolation arguably prohibits generalization. To this end, we propose a framework, NoEsis, which builds upon the desired properties of modularity, privacy, and knowledge transfer. NoEsis integrates differential privacy with a hybrid two-staged parameter-efficient fine-tuning that combines domain-specific low-rank adapters, acting as experts, with common prompt tokens, acting as a knowledge-sharing backbone. Results from our evaluation on CodeXGLUE showcase that NoEsis can achieve provable privacy guarantees with tangible knowledge transfer across domains, and empirically show protection against Membership Inference Attacks. Finally, on code completion tasks, NoEsis bridges at least 77% of the accuracy gap between the non-shared and the non-private baseline.</li>
</ul>

<h3>Title: A Generative Graph Contrastive Learning Model with Global Signal</h3>
<ul>
<li><strong>Authors: </strong>Xiaofan Wei, Binyan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18148">https://arxiv.org/abs/2504.18148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18148">https://arxiv.org/pdf/2504.18148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18148]] A Generative Graph Contrastive Learning Model with Global Signal(https://arxiv.org/abs/2504.18148)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Graph contrastive learning (GCL) has garnered significant attention recently since it learns complex structural information from graphs through self-supervised learning manner. However, prevalent GCL models may suffer from performance degradation due to inappropriate contrastive signals. Concretely, they commonly generate augmented views based on random perturbation, which leads to biased essential structures due to the introduction of noise. In addition, they assign equal weight to both hard and easy sample pairs, thereby ignoring the difference in importance of the sample pairs. To address these issues, this study proposes a novel Contrastive Signal Generative Framework for Accurate Graph Learning (CSG2L) with the following two-fold ideas: a) building a singular value decomposition (SVD)-directed augmented module (SVD-aug) to obtain the global interactions as well as avoiding the random noise perturbation; b) designing a local-global dependency learning module (LGDL) with an adaptive reweighting strategy which can differentiate the effects of hard and easy sample pairs. Extensive experiments on benchmark datasets demonstrate that the proposed CSG2L outperforms the state-of-art baselines. Moreover, CSG2L is compatible with a variety of GNNs.</li>
</ul>

<h3>Title: E-InMeMo: Enhanced Prompting for Visual In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Zhang, Bowen Wang, Hong Liu, Liangzhi Li, Yuta Nakashima, Hajime Nagahara</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18158">https://arxiv.org/abs/2504.18158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18158">https://arxiv.org/pdf/2504.18158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18158]] E-InMeMo: Enhanced Prompting for Visual In-Context Learning(https://arxiv.org/abs/2504.18158)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Large-scale models trained on extensive datasets have become the standard due to their strong generalizability across diverse tasks. In-context learning (ICL), widely used in natural language processing, leverages these models by providing task-specific prompts without modifying their parameters. This paradigm is increasingly being adapted for computer vision, where models receive an input-output image pair, known as an in-context pair, alongside a query image to illustrate the desired output. However, the success of visual ICL largely hinges on the quality of these prompts. To address this, we propose Enhanced Instruct Me More (E-InMeMo), a novel approach that incorporates learnable perturbations into in-context pairs to optimize prompting. Through extensive experiments on standard vision tasks, E-InMeMo demonstrates superior performance over existing state-of-the-art methods. Notably, it improves mIoU scores by 7.99 for foreground segmentation and by 17.04 for single object detection when compared to the baseline without learnable prompts. These results highlight E-InMeMo as a lightweight yet effective strategy for enhancing visual ICL. Code is publicly available at: this https URL</li>
</ul>

<h3>Title: PerfCam: Digital Twinning for Production Lines Using 3D Gaussian Splatting and Vision Models</h3>
<ul>
<li><strong>Authors: </strong>Michel Gokan Khan, Renan Guarese, Fabian Johnson, Xi Vincent Wang, Anders Bergman, Benjamin Edvinsson, Mario Romero, Jrmy Vachier, Jan Kronqvist</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18165">https://arxiv.org/abs/2504.18165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18165">https://arxiv.org/pdf/2504.18165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18165]] PerfCam: Digital Twinning for Production Lines Using 3D Gaussian Splatting and Vision Models(https://arxiv.org/abs/2504.18165)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning framework that combines camera and sensory data with 3D Gaussian Splatting and computer vision models for digital twinning, object tracking, and Key Performance Indicators (KPIs) extraction in industrial production lines. By utilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam offers a semi-automated approach to object tracking and spatial mapping, enabling digital twins that capture real-time KPIs such as availability, performance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts in the production line. We validate the effectiveness of PerfCam through a practical deployment within realistic test production lines in the pharmaceutical industry and contribute an openly published dataset to support further research and development in the field. The results demonstrate PerfCam's ability to deliver actionable insights through its precise digital twin capabilities, underscoring its value as an effective tool for developing usable digital twins in smart manufacturing environments and extracting operational analytics.</li>
</ul>

<h3>Title: Unveiling 3D Ocean Biogeochemical Provinces: A Machine Learning Approach for Systematic Clustering and Validation</h3>
<ul>
<li><strong>Authors: </strong>Yvonne Jenniges, Maike Sonnewald, Sebastian Maneth, Are Olsen, Boris P. Koch</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18181">https://arxiv.org/abs/2504.18181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18181">https://arxiv.org/pdf/2504.18181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18181]] Unveiling 3D Ocean Biogeochemical Provinces: A Machine Learning Approach for Systematic Clustering and Validation(https://arxiv.org/abs/2504.18181)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Defining ocean regions and water masses helps to understand marine processes and can serve downstream-tasks such as defining marine protected areas. However, such definitions are often a result of subjective decisions potentially producing misleading, unreproducible results. Here, the aim was to objectively define regions of the North Atlantic. For this, a data-driven, systematic machine learning approach was applied to generate and validate ocean clusters employing external, internal and relative validation techniques. About 300 million measured salinity, temperature, and oxygen, nitrate, phosphate and silicate concentration values served as input for various clustering methods (KMeans, agglomerative Ward, and Density-Based Spatial Clustering of Applications with Noise (DBSCAN)). Uniform Manifold Approximation and Projection (UMAP) emphasised (dis-)similarities in the data while reducing dimensionality. Based on a systematic validation of the considered clustering methods and their hyperparameters, the results showed that UMAP-DBSCAN best represented the data. To address stochastic variability, 100 UMAP-DBSCAN clustering runs were conducted and aggregated using Native Emergent Manifold Interrogation (NEMI), producing a final set of 321 clusters. Reproducibility was evaluated by calculating the ensemble overlap (88.81 +- 1.8%) and the mean grid cell-wise uncertainty estimated by NEMI (15.49 +- 20%). The presented clustering results agreed very well with common water mass definitions. This study revealed a more detailed regionalization compared to previous concepts such as the Longhurst provinces. The applied method is objective, efficient and reproducible and will support future research focusing on biogeochemical differences and changes in oceanic regions.</li>
</ul>

<h3>Title: What is the Added Value of UDA in the VFM Era?</h3>
<ul>
<li><strong>Authors: </strong>Brun B. Englert, Tommie Kerssies, Gijs Dubbelman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18190">https://arxiv.org/abs/2504.18190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18190">https://arxiv.org/pdf/2504.18190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18190]] What is the Added Value of UDA in the VFM Era?(https://arxiv.org/abs/2504.18190)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Unsupervised Domain Adaptation (UDA) can improve a perception model's generalization to an unlabeled target domain starting from a labeled source domain. UDA using Vision Foundation Models (VFMs) with synthetic source data can achieve generalization performance comparable to fully-supervised learning with real target data. However, because VFMs have strong generalization from their pre-training, more straightforward, source-only fine-tuning can also perform well on the target. As data scenarios used in academic research are not necessarily representative for real-world applications, it is currently unclear (a) how UDA behaves with more representative and diverse data and (b) if source-only fine-tuning of VFMs can perform equally well in these scenarios. Our research aims to close these gaps and, similar to previous studies, we focus on semantic segmentation as a representative perception task. We assess UDA for synth-to-real and real-to-real use cases with different source and target data combinations. We also investigate the effect of using a small amount of labeled target data in UDA. We clarify that while these scenarios are more realistic, they are not necessarily more challenging. Our results show that, when using stronger synthetic source data, UDA's improvement over source-only fine-tuning of VFMs reduces from +8 mIoU to +2 mIoU, and when using more diverse real source data, UDA has no added value. However, UDA generalization is always higher in all synthetic data scenarios than source-only fine-tuning and, when including only 1/16 of Cityscapes labels, synthetic UDA obtains the same state-of-the-art segmentation quality of 85 mIoU as a fully-supervised model using all labels. Considering the mixed results, we discuss how UDA can best support robust autonomous driving at scale.</li>
</ul>

<h3>Title: Multi-Grained Compositional Visual Clue Learning for Image Intent Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yin Tang, Jiankai Li, Hongyu Yang, Xuan Dong, Lifeng Fan, Weixin Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18201">https://arxiv.org/abs/2504.18201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18201">https://arxiv.org/pdf/2504.18201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18201]] Multi-Grained Compositional Visual Clue Learning for Image Intent Recognition(https://arxiv.org/abs/2504.18201)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>In an era where social media platforms abound, individuals frequently share images that offer insights into their intents and interests, impacting individual life quality and societal stability. Traditional computer vision tasks, such as object detection and semantic segmentation, focus on concrete visual representations, while intent recognition relies more on implicit visual clues. This poses challenges due to the wide variation and subjectivity of such clues, compounded by the problem of intra-class variety in conveying abstract concepts, e.g. "enjoy life". Existing methods seek to solve the problem by manually designing representative features or building prototypes for each class from global features. However, these methods still struggle to deal with the large visual diversity of each intent category. In this paper, we introduce a novel approach named Multi-grained Compositional visual Clue Learning (MCCL) to address these challenges for image intent recognition. Our method leverages the systematic compositionality of human cognition by breaking down intent recognition into visual clue composition and integrating multi-grained features. We adopt class-specific prototypes to alleviate data imbalance. We treat intent recognition as a multi-label classification problem, using a graph convolutional network to infuse prior knowledge through label embedding correlations. Demonstrated by a state-of-the-art performance on the Intentonomy and MDID datasets, our approach advances the accuracy of existing methods while also possessing good interpretability. Our work provides an attempt for future explorations in understanding complex and miscellaneous forms of human expression.</li>
</ul>

<h3>Title: LiDAR-Guided Monocular 3D Object Detection for Long-Range Railway Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Raul David Dominguez Sanchez, Xavier Diaz Ortiz, Xingcheng Zhou, Max Peter Ronecker, Michael Karner, Daniel Watzenig, Alois Knoll</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18203">https://arxiv.org/abs/2504.18203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18203">https://arxiv.org/pdf/2504.18203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18203]] LiDAR-Guided Monocular 3D Object Detection for Long-Range Railway Monitoring(https://arxiv.org/abs/2504.18203)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Railway systems, particularly in Germany, require high levels of automation to address legacy infrastructure challenges and increase train traffic safely. A key component of automation is robust long-range perception, essential for early hazard detection, such as obstacles at level crossings or pedestrians on tracks. Unlike automotive systems with braking distances of ~70 meters, trains require perception ranges exceeding 1 km. This paper presents an deep-learning-based approach for long-range 3D object detection tailored for autonomous trains. The method relies solely on monocular images, inspired by the Faraway-Frustum approach, and incorporates LiDAR data during training to improve depth estimation. The proposed pipeline consists of four key modules: (1) a modified YOLOv9 for 2.5D object detection, (2) a depth estimation network, and (3-4) dedicated short- and long-range 3D detection heads. Evaluations on the OSDaR23 dataset demonstrate the effectiveness of the approach in detecting objects up to 250 meters. Results highlight its potential for railway automation and outline areas for future improvement.</li>
</ul>

<h3>Title: Optimizing Multi-Round Enhanced Training in Diffusion Models for Improved Preference Understanding</h3>
<ul>
<li><strong>Authors: </strong>Kun Li, Jianhui Wang, Yangfan He, Xinyuan Song, Ruoyu Wang, Hongyang He, Wenxin Zhang, Jiaqi Chen, Keqin Li, Sida Li, Miao Zhang, Tianyu Shi, Xueqian Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18204">https://arxiv.org/abs/2504.18204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18204">https://arxiv.org/pdf/2504.18204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18204]] Optimizing Multi-Round Enhanced Training in Diffusion Models for Improved Preference Understanding(https://arxiv.org/abs/2504.18204)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative AI has significantly changed industries by enabling text-driven image generation, yet challenges remain in achieving high-resolution outputs that align with fine-grained user preferences. Consequently, multi-round interactions are necessary to ensure the generated images meet expectations. Previous methods enhanced prompts via reward feedback but did not optimize over a multi-round dialogue dataset. In this work, we present a Visual Co-Adaptation (VCA) framework incorporating human-in-the-loop feedback, leveraging a well-trained reward model aligned with human preferences. Using a diverse multi-turn dialogue dataset, our framework applies multiple reward functions, such as diversity, consistency, and preference feedback, while fine-tuning the diffusion model through LoRA, thus optimizing image generation based on user input. We also construct multi-round dialogue datasets of prompts and image pairs aligned with user intent. Experiments demonstrate that our method outperforms state-of-the-art baselines, significantly improving image consistency and alignment with user intent. Our approach consistently surpasses competing models in user satisfaction, especially in multi-turn dialogue scenarios.</li>
</ul>

<h3>Title: Ultra-fast feature learning for the training of two-layer neural networks in the two-timescale regime</h3>
<ul>
<li><strong>Authors: </strong>Raphal Barboni (NS-PSL), Gabriel Peyr (CNRS and NS-PSL), Franois-Xavier Vialard (LIGM)</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18208">https://arxiv.org/abs/2504.18208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18208">https://arxiv.org/pdf/2504.18208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18208]] Ultra-fast feature learning for the training of two-layer neural networks in the two-timescale regime(https://arxiv.org/abs/2504.18208)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We study the convergence of gradient methods for the training of mean-field single hidden layer neural networks with square loss. Observing this is a separable non-linear least-square problem which is linear w.r.t. the outer layer's weights, we consider a Variable Projection (VarPro) or two-timescale learning algorithm, thereby eliminating the linear variables and reducing the learning problem to the training of the feature distribution. Whereas most convergence rates or the training of neural networks rely on a neural tangent kernel analysis where features are fixed, we show such a strategy enables provable convergence rates for the sampling of a teacher feature distribution. Precisely, in the limit where the regularization strength vanishes, we show that the dynamic of the feature distribution corresponds to a weighted ultra-fast diffusion equation. Relying on recent results on the asymptotic behavior of such PDEs, we obtain guarantees for the convergence of the trained feature distribution towards the teacher feature distribution in a teacher-student setup.</li>
</ul>

<h3>Title: A Data-Centric Approach to 3D Semantic Segmentation of Railway Scenes</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Mnger, Max Peter Ronecker, Xavier Diaz, Michael Karner, Daniel Watzenig, Jan Skaloud</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18213">https://arxiv.org/abs/2504.18213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18213">https://arxiv.org/pdf/2504.18213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18213]] A Data-Centric Approach to 3D Semantic Segmentation of Railway Scenes(https://arxiv.org/abs/2504.18213)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>LiDAR-based semantic segmentation is critical for autonomous trains, requiring accurate predictions across varying distances. This paper introduces two targeted data augmentation methods designed to improve segmentation performance on the railway-specific OSDaR23 dataset. The person instance pasting method enhances segmentation of pedestrians at distant ranges by injecting realistic variations into the dataset. The track sparsification method redistributes point density in LiDAR scans, improving track segmentation at far distances with minimal impact on close-range accuracy. Both methods are evaluated using a state-of-the-art 3D semantic segmentation network, demonstrating significant improvements in distant-range performance while maintaining robustness in close-range predictions. We establish the first 3D semantic segmentation benchmark for OSDaR23, demonstrating the potential of data-centric approaches to address railway-specific challenges in autonomous train perception.</li>
</ul>

<h3>Title: Unify3D: An Augmented Holistic End-to-end Monocular 3D Human Reconstruction via Anatomy Shaping and Twins Negotiating</h3>
<ul>
<li><strong>Authors: </strong>Nanjie Yao, Gangjian Zhang, Wenhao Shen, Jian Shu, Hao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18215">https://arxiv.org/abs/2504.18215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18215">https://arxiv.org/pdf/2504.18215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18215]] Unify3D: An Augmented Holistic End-to-end Monocular 3D Human Reconstruction via Anatomy Shaping and Twins Negotiating(https://arxiv.org/abs/2504.18215)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Monocular 3D clothed human reconstruction aims to create a complete 3D avatar from a single image. To tackle the human geometry lacking in one RGB image, current methods typically resort to a preceding model for an explicit geometric representation. For the reconstruction itself, focus is on modeling both it and the input image. This routine is constrained by the preceding model, and overlooks the integrity of the reconstruction task. To address this, this paper introduces a novel paradigm that treats human reconstruction as a holistic process, utilizing an end-to-end network for direct prediction from 2D image to 3D avatar, eliminating any explicit intermediate geometry display. Based on this, we further propose a novel reconstruction framework consisting of two core components: the Anatomy Shaping Extraction module, which captures implicit shape features taking into account the specialty of human anatomy, and the Twins Negotiating Reconstruction U-Net, which enhances reconstruction through feature interaction between two U-Nets of different modalities. Moreover, we propose a Comic Data Augmentation strategy and construct 15k+ 3D human scans to bolster model performance in more complex case input. Extensive experiments on two test sets and many in-the-wild cases show the superiority of our method over SOTA methods. Our demos can be found in : this https URL.</li>
</ul>

<h3>Title: Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family</h3>
<ul>
<li><strong>Authors: </strong>Pierre-Carl Langlais, Pavel Chizhov, Mattia Nee, Carlos Rosas Hinostroza, Matthieu Delsart, Irne Girard, Othman Hicheur, Anastasia Stasenko, Ivan P. Yamshchikov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18225">https://arxiv.org/abs/2504.18225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18225">https://arxiv.org/pdf/2504.18225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18225]] Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family(https://arxiv.org/abs/2504.18225)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce a new generation of small reasoning models for RAG, search, and source summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a large synthetic dataset emulating the retrieval of a wide variety of multilingual open sources from the Common Corpus. They provide native support for citation and grounding with literal quotes and reintegrate multiple features associated with RAG workflows, such as query routing, query reformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B outperform SLMs below 4 billion parameters on standardized RAG benchmarks (HotPotQA, 2wiki) and are competitive with popular larger models, including Qwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date maintaining consistent RAG performance across leading European languages and ensuring systematic reference grounding for statements. Due to their size and ease of deployment on constrained infrastructure and higher factuality by design, the models unlock a range of new use cases for generative AI.</li>
</ul>

<h3>Title: DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Rong Cheng, Jinyi Liu, YAN ZHENG, Fei Ni, Jiazhen Du, Hangyu Mao, Fuzheng Zhang, Bo Wang, Jianye HAO</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18243">https://arxiv.org/abs/2504.18243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18243">https://arxiv.org/pdf/2504.18243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18243]] DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering(https://arxiv.org/abs/2504.18243)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-Hop Question Answering (MHQA) tasks permeate real-world applications, posing challenges in orchestrating multi-step reasoning across diverse knowledge domains. While existing approaches have been improved with iterative retrieval, they still struggle to identify and organize dynamic knowledge. To address this, we propose DualRAG, a synergistic dual-process framework that seamlessly integrates reasoning and retrieval. DualRAG operates through two tightly coupled processes: Reasoning-augmented Querying (RaQ) and progressive Knowledge Aggregation (pKA). They work in concert: as RaQ navigates the reasoning path and generates targeted queries, pKA ensures that newly acquired knowledge is systematically integrated to support coherent reasoning. This creates a virtuous cycle of knowledge enrichment and reasoning refinement. Through targeted fine-tuning, DualRAG preserves its sophisticated reasoning and retrieval capabilities even in smaller-scale models, demonstrating its versatility and core advantages across different scales. Extensive experiments demonstrate that this dual-process approach substantially improves answer accuracy and coherence, approaching, and in some cases surpassing, the performance achieved with oracle knowledge access. These results establish DualRAG as a robust and efficient solution for complex multi-hop reasoning tasks.</li>
</ul>

<h3>Title: Efficient Single-Pass Training for Multi-Turn Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Ritesh Goru, Shanay Mehta, Prateek Jain</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18246">https://arxiv.org/abs/2504.18246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18246">https://arxiv.org/pdf/2504.18246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18246]] Efficient Single-Pass Training for Multi-Turn Reasoning(https://arxiv.org/abs/2504.18246)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Training Large Language Models ( LLMs) to generate explicit reasoning before they produce an answer has been shown to improve their performance across various tasks such as mathematics and coding. However, fine-tuning LLMs on multi-turn reasoning datasets presents a unique challenge: LLMs must generate reasoning tokens that are excluded from subsequent inputs to the LLM. This discrepancy prevents us from processing an entire conversation in a single forward pass-an optimization readily available when we fine-tune on a multi-turn non-reasoning dataset. This paper proposes a novel approach that overcomes this limitation through response token duplication and a custom attention mask that enforces appropriate visibility constraints. Our approach significantly reduces the training time and allows efficient fine-tuning on multi-turn reasoning datasets.</li>
</ul>

<h3>Title: MAGI: Multi-Agent Guided Interview for Psychiatric Assessment</h3>
<ul>
<li><strong>Authors: </strong>Guanqun Bi, Zhuang Chen, Zhoufu Liu, Hongkai Wang, Xiyao Xiao, Yuqiang Xie, Wen Zhang, Yongkang Huang, Yuxuan Chen, Libiao Peng, Yi Feng, Minlie Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18260">https://arxiv.org/abs/2504.18260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18260">https://arxiv.org/pdf/2504.18260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18260]] MAGI: Multi-Agent Guided Interview for Psychiatric Assessment(https://arxiv.org/abs/2504.18260)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automating structured clinical interviews could revolutionize mental healthcare accessibility, yet existing large language models (LLMs) approaches fail to align with psychiatric diagnostic protocols. We present MAGI, the first framework that transforms the gold-standard Mini International Neuropsychiatric Interview (MINI) into automatic computational workflows through coordinated multi-agent collaboration. MAGI dynamically navigates clinical logic via four specialized agents: 1) an interview tree guided navigation agent adhering to the MINI's branching structure, 2) an adaptive question agent blending diagnostic probing, explaining, and empathy, 3) a judgment agent validating whether the response from participants meet the node, and 4) a diagnosis Agent generating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map symptoms to clinical criteria. Experimental results on 1,002 real-world participants covering depression, generalized anxiety, social anxiety and suicide shows that MAGI advances LLM- assisted mental health assessment by combining clinical rigor, conversational adaptability, and explainable reasoning.</li>
</ul>

<h3>Title: Local Statistical Parity for the Estimation of Fair Decision Trees</h3>
<ul>
<li><strong>Authors: </strong>Andrea Quintanilla, Johan Van Horebeek</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18262">https://arxiv.org/abs/2504.18262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18262">https://arxiv.org/pdf/2504.18262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18262]] Local Statistical Parity for the Estimation of Fair Decision Trees(https://arxiv.org/abs/2504.18262)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Given the high computational complexity of decision tree estimation, classical methods construct a tree by adding one node at a time in a recursive way. To facilitate promoting fairness, we propose a fairness criterion local to the tree nodes. We prove how it is related to the Statistical Parity criterion, popular in the Algorithmic Fairness literature, and show how to incorporate it into standard recursive tree estimation algorithms. We present a tree estimation algorithm called Constrained Logistic Regression Tree (C-LRT), which is a modification of the standard CART algorithm using locally linear classifiers and imposing restrictions as done in Constrained Logistic Regression. Finally, we evaluate the performance of trees estimated with C-LRT on datasets commonly used in the Algorithmic Fairness literature, using various classification and fairness metrics. The results confirm that C-LRT successfully allows to control and balance accuracy and fairness.</li>
</ul>

<h3>Title: Neural operators struggle to learn complex PDEs in pedestrian mobility: Hughes model case study</h3>
<ul>
<li><strong>Authors: </strong>Prajwal Chauhan, Salah Eddine Choutri, Mohamed Ghattassi, Nader Masmoudi, Saif Eddin Jabari</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18267">https://arxiv.org/abs/2504.18267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18267">https://arxiv.org/pdf/2504.18267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18267]] Neural operators struggle to learn complex PDEs in pedestrian mobility: Hughes model case study(https://arxiv.org/abs/2504.18267)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper investigates the limitations of neural operators in learning solutions for a Hughes model, a first-order hyperbolic conservation law system for crowd dynamics. The model couples a Fokker-Planck equation representing pedestrian density with a Hamilton-Jacobi-type (eikonal) equation. This Hughes model belongs to the class of nonlinear hyperbolic systems that often exhibit complex solution structures, including shocks and discontinuities. In this study, we assess the performance of three state-of-the-art neural operators (Fourier Neural Operator, Wavelet Neural Operator, and Multiwavelet Neural Operator) in various challenging scenarios. Specifically, we consider (1) discontinuous and Gaussian initial conditions and (2) diverse boundary conditions, while also examining the impact of different numerical schemes. Our results show that these neural operators perform well in easy scenarios with fewer discontinuities in the initial condition, yet they struggle in complex scenarios with multiple initial discontinuities and dynamic boundary conditions, even when trained specifically on such complex samples. The predicted solutions often appear smoother, resulting in a reduction in total variation and a loss of important physical features. This smoothing behavior is similar to issues discussed by Daganzo (1995), where models that introduce artificial diffusion were shown to miss essential features such as shock waves in hyperbolic systems. These results suggest that current neural operator architectures may introduce unintended regularization effects that limit their ability to capture transport dynamics governed by discontinuities. They also raise concerns about generalizing these methods to traffic applications where shock preservation is essential.</li>
</ul>

<h3>Title: TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Shintaro Ozaki, Kazuki Hayashi, Yusuke Sakai, Jingun Kwon, Hidetaka Kamigaito, Katsuhiko Hayashi, Manabu Okumura, Taro Watanabe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18269">https://arxiv.org/abs/2504.18269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18269">https://arxiv.org/pdf/2504.18269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18269]] TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation(https://arxiv.org/abs/2504.18269)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Generating images from prompts containing specific entities requires models to retain as much entity-specific knowledge as possible. However, fully memorizing such knowledge is impractical due to the vast number of entities and their continuous emergence. To address this, we propose Text-based Intelligent Generation with Entity prompt Refinement (TextTIGER), which augments knowledge on entities included in the prompts and then summarizes the augmented descriptions using Large Language Models (LLMs) to mitigate performance degradation from longer inputs. To evaluate our method, we introduce WiT-Cub (WiT with Captions and Uncomplicated Background-explanations), a dataset comprising captions, images, and an entity list. Experiments on four image generation models and five LLMs show that TextTIGER improves image generation performance in standard metrics (IS, FID, and CLIPScore) compared to caption-only prompts. Additionally, multiple annotators' evaluation confirms that the summarized descriptions are more informative, validating LLMs' ability to generate concise yet rich descriptions. These findings demonstrate that refining prompts with augmented and summarized entity-related descriptions enhances image generation capabilities. The code and dataset will be available upon acceptance.</li>
</ul>

<h3>Title: Studying Small Language Models with Susceptibilities</h3>
<ul>
<li><strong>Authors: </strong>Garrett Baker, George Wang, Jesse Hoogland, Daniel Murfet</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18274">https://arxiv.org/abs/2504.18274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18274">https://arxiv.org/pdf/2504.18274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18274]] Studying Small Language Models with Susceptibilities(https://arxiv.org/abs/2504.18274)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>We develop a linear response framework for interpretability that treats a neural network as a Bayesian statistical mechanical system. A small, controlled perturbation of the data distribution, for example shifting the Pile toward GitHub or legal text, induces a first-order change in the posterior expectation of an observable localized on a chosen component of the network. The resulting susceptibility can be estimated efficiently with local SGLD samples and factorizes into signed, per-token contributions that serve as attribution scores. Building a set of perturbations (probes) yields a response matrix whose low-rank structure separates functional modules such as multigram and induction heads in a 3M-parameter transformer. Susceptibilities link local learning coefficients from singular learning theory with linear-response theory, and quantify how local loss landscape geometry deforms under shifts in the data distribution.</li>
</ul>

<h3>Title: Seeing Soundscapes: Audio-Visual Generation and Separation from Soundscapes Using Audio-Visual Separator</h3>
<ul>
<li><strong>Authors: </strong>Minjae Kang, Martim Brando</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18283">https://arxiv.org/abs/2504.18283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18283">https://arxiv.org/pdf/2504.18283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18283]] Seeing Soundscapes: Audio-Visual Generation and Separation from Soundscapes Using Audio-Visual Separator(https://arxiv.org/abs/2504.18283)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent audio-visual generative models have made substantial progress in generating images from audio. However, existing approaches focus on generating images from single-class audio and fail to generate images from mixed audio. To address this, we propose an Audio-Visual Generation and Separation model (AV-GAS) for generating images from soundscapes (mixed audio containing multiple classes). Our contribution is threefold: First, we propose a new challenge in the audio-visual generation task, which is to generate an image given a multi-class audio input, and we propose a method that solves this task using an audio-visual separator. Second, we introduce a new audio-visual separation task, which involves generating separate images for each class present in a mixed audio input. Lastly, we propose new evaluation metrics for the audio-visual generation task: Class Representation Score (CRS) and a modified R@K. Our model is trained and evaluated on the VGGSound dataset. We show that our method outperforms the state-of-the-art, achieving 7% higher CRS and 4% higher R@2* in generating plausible images with mixed audio.</li>
</ul>

<h3>Title: Enhancing Long-Term Re-Identification Robustness Using Synthetic Data: A Comparative Analysis</h3>
<ul>
<li><strong>Authors: </strong>Christian Pionzewski, Rebecca Rademacher, Jrme Rutinowski, Antonia Ponikarov, Stephan Matzke, Tim Chilla, Pia Schreynemackers, Alice Kirchheim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18286">https://arxiv.org/abs/2504.18286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18286">https://arxiv.org/pdf/2504.18286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18286]] Enhancing Long-Term Re-Identification Robustness Using Synthetic Data: A Comparative Analysis(https://arxiv.org/abs/2504.18286)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This contribution explores the impact of synthetic training data usage and the prediction of material wear and aging in the context of re-identification. Different experimental setups and gallery set expanding strategies are tested, analyzing their impact on performance over time for aging re-identification subjects. Using a continuously updating gallery, we were able to increase our mean Rank-1 accuracy by 24%, as material aging was taken into account step by step. In addition, using models trained with 10% artificial training data, Rank-1 accuracy could be increased by up to 13%, in comparison to a model trained on only real-world data, significantly boosting generalized performance on hold-out data. Finally, this work introduces a novel, open-source re-identification dataset, pallet-block-2696. This dataset contains 2,696 images of Euro pallets, taken over a period of 4 months. During this time, natural aging processes occurred and some of the pallets were damaged during their usage. These wear and tear processes significantly changed the appearance of the pallets, providing a dataset that can be used to generate synthetically aged pallets or other wooden materials.</li>
</ul>

<h3>Title: STP4D: Spatio-Temporal-Prompt Consistent Modeling for Text-to-4D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Yunze Deng, Haijun Xiong, Bin Feng, Xinggang Wang, Wenyu Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18318">https://arxiv.org/abs/2504.18318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18318">https://arxiv.org/pdf/2504.18318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18318]] STP4D: Spatio-Temporal-Prompt Consistent Modeling for Text-to-4D Gaussian Splatting(https://arxiv.org/abs/2504.18318)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-4D generation is rapidly developing and widely applied in various scenarios. However, existing methods often fail to incorporate adequate spatio-temporal modeling and prompt alignment within a unified framework, resulting in temporal inconsistencies, geometric distortions, or low-quality 4D content that deviates from the provided texts. Therefore, we propose STP4D, a novel approach that aims to integrate comprehensive spatio-temporal-prompt consistency modeling for high-quality text-to-4D generation. Specifically, STP4D employs three carefully designed modules: Time-varying Prompt Embedding, Geometric Information Enhancement, and Temporal Extension Deformation, which collaborate to accomplish this goal. Furthermore, STP4D is among the first methods to exploit the Diffusion model to generate 4D Gaussians, combining the fine-grained modeling capabilities and the real-time rendering process of 4DGS with the rapid inference speed of the Diffusion model. Extensive experiments demonstrate that STP4D excels in generating high-fidelity 4D content with exceptional efficiency (approximately 4.6s per asset), surpassing existing methods in both quality and speed.</li>
</ul>

<h3>Title: PHEATPRUNER: Interpretable Data-centric Feature Selection for Multivariate Time Series Classification through Persistent Homology</h3>
<ul>
<li><strong>Authors: </strong>Anh-Duy Pham, Olivier Basole Kashongwe, Martin Atzmueller, Tim Rmer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18329">https://arxiv.org/abs/2504.18329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18329">https://arxiv.org/pdf/2504.18329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18329]] PHEATPRUNER: Interpretable Data-centric Feature Selection for Multivariate Time Series Classification through Persistent Homology(https://arxiv.org/abs/2504.18329)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Balancing performance and interpretability in multivariate time series classification is a significant challenge due to data complexity and high dimensionality. This paper introduces PHeatPruner, a method integrating persistent homology and sheaf theory to address these challenges. Persistent homology facilitates the pruning of up to 45% of the applied variables while maintaining or enhancing the accuracy of models such as Random Forest, CatBoost, XGBoost, and LightGBM, all without depending on posterior probabilities or supervised optimization algorithms. Concurrently, sheaf theory contributes explanatory vectors that provide deeper insights into the data's structural nuances. The approach was validated using the UEA Archive and a mastitis detection dataset for dairy cows. The results demonstrate that PHeatPruner effectively preserves model accuracy. Furthermore, our results highlight PHeatPruner's key features, i.e. simplifying complex data and offering actionable insights without increasing processing time or complexity. This method bridges the gap between complexity reduction and interpretability, suggesting promising applications in various fields.</li>
</ul>

<h3>Title: SSD-Poser: Avatar Pose Estimation with State Space Duality from Sparse Observations</h3>
<ul>
<li><strong>Authors: </strong>Shuting Zhao, Linxin Bai, Liangjing Shao, Ye Zhang, Xinrong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18332">https://arxiv.org/abs/2504.18332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18332">https://arxiv.org/pdf/2504.18332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18332]] SSD-Poser: Avatar Pose Estimation with State Space Duality from Sparse Observations(https://arxiv.org/abs/2504.18332)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>The growing applications of AR/VR increase the demand for real-time full-body pose estimation from Head-Mounted Displays (HMDs). Although HMDs provide joint signals from the head and hands, reconstructing a full-body pose remains challenging due to the unconstrained lower body. Recent advancements often rely on conventional neural networks and generative models to improve performance in this task, such as Transformers and diffusion models. However, these approaches struggle to strike a balance between achieving precise pose reconstruction and maintaining fast inference speed. To overcome these challenges, a lightweight and efficient model, SSD-Poser, is designed for robust full-body motion estimation from sparse observations. SSD-Poser incorporates a well-designed hybrid encoder, State Space Attention Encoders, to adapt the state space duality to complex motion poses and enable real-time realistic pose reconstruction. Moreover, a Frequency-Aware Decoder is introduced to mitigate jitter caused by variable-frequency motion signals, remarkably enhancing the motion smoothness. Comprehensive experiments on the AMASS dataset demonstrate that SSD-Poser achieves exceptional accuracy and computational efficiency, showing outstanding inference efficiency compared to state-of-the-art methods.</li>
</ul>

<h3>Title: Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections</h3>
<ul>
<li><strong>Authors: </strong>Narek Maloyan, Dmitry Namiot</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18333">https://arxiv.org/abs/2504.18333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18333">https://arxiv.org/pdf/2504.18333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18333]] Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections(https://arxiv.org/abs/2504.18333)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>LLM as judge systems used to assess text quality code correctness and argument strength are vulnerable to prompt injection attacks. We introduce a framework that separates content author attacks from system prompt attacks and evaluate five models Gemma 3.27B Gemma 3.4B Llama 3.2 3B GPT 4 and Claude 3 Opus on four tasks with various defenses using fifty prompts per condition. Attacks achieved up to seventy three point eight percent success smaller models proved more vulnerable and transferability ranged from fifty point five to sixty two point six percent. Our results contrast with Universal Prompt Injection and AdvPrompter We recommend multi model committees and comparative scoring and release all code and datasets</li>
</ul>

<h3>Title: Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review</h3>
<ul>
<li><strong>Authors: </strong>Toghrul Abbasli, Kentaroh Toyoda, Yuan Wang, Leon Witt, Muhammad Asif Ali, Yukai Miao, Dan Li, Qingsong Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18346">https://arxiv.org/abs/2504.18346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18346">https://arxiv.org/pdf/2504.18346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18346]] Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review(https://arxiv.org/abs/2504.18346)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have been transformative across many domains. However, hallucination -- confidently outputting incorrect information -- remains one of the leading challenges for LLMs. This raises the question of how to accurately assess and quantify the uncertainty of LLMs. Extensive literature on traditional models has explored Uncertainty Quantification (UQ) to measure uncertainty and employed calibration techniques to address the misalignment between uncertainty and accuracy. While some of these methods have been adapted for LLMs, the literature lacks an in-depth analysis of their effectiveness and does not offer a comprehensive benchmark to enable insightful comparison among existing solutions. In this work, we fill this gap via a systematic survey of representative prior works on UQ and calibration for LLMs and introduce a rigorous benchmark. Using two widely used reliability datasets, we empirically evaluate six related methods, which justify the significant findings of our review. Finally, we provide outlooks for key future directions and outline open challenges. To the best of our knowledge, this survey is the first dedicated study to review the calibration methods and relevant metrics for LLMs.</li>
</ul>

<h3>Title: TSCL:Multi-party loss Balancing scheme for deep learning Image steganography based on Curriculum learning</h3>
<ul>
<li><strong>Authors: </strong>Fengchun Liu. Tong Zhang, Chunying Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18348">https://arxiv.org/abs/2504.18348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18348">https://arxiv.org/pdf/2504.18348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18348]] TSCL:Multi-party loss Balancing scheme for deep learning Image steganography based on Curriculum learning(https://arxiv.org/abs/2504.18348)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>For deep learning-based image steganography frameworks, in order to ensure the invisibility and recoverability of the information embedding, the loss function usually contains several losses such as embedding loss, recovery loss and steganalysis loss. In previous research works, fixed loss weights are usually chosen for training optimization, and this setting is not linked to the importance of the steganography task itself and the training process. In this paper, we propose a Two-stage Curriculum Learning loss scheduler (TSCL) for balancing multinomial losses in deep learning image steganography algorithms. TSCL consists of two phases: a priori curriculum control and loss dynamics control. The first phase firstly focuses the model on learning the information embedding of the original image by controlling the loss weights in the multi-party adversarial training; secondly, it makes the model shift its learning focus to improving the decoding accuracy; and finally, it makes the model learn to generate a steganographic image that is resistant to steganalysis. In the second stage, the learning speed of each training task is evaluated by calculating the loss drop of the before and after iteration rounds to balance the learning of each task. Experimental results on three large public datasets, ALASKA2, VOC2012 and ImageNet, show that the proposed TSCL strategy improves the quality of steganography, decoding accuracy and security.</li>
</ul>

<h3>Title: Revisiting Data Auditing in Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Zhu, Sichu Liang, Wenwen Wang, Boheng Li, Tongxin Yuan, Fangqi Li, ShiLin Wang, Zhuosheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18349">https://arxiv.org/abs/2504.18349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18349">https://arxiv.org/pdf/2504.18349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18349]] Revisiting Data Auditing in Large Vision-Language Models(https://arxiv.org/abs/2504.18349)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>With the surge of large language models (LLMs), Large Vision-Language Models (VLMs)--which integrate vision encoders with LLMs for accurate visual grounding--have shown great potential in tasks like generalist agents and robotic control. However, VLMs are typically trained on massive web-scraped images, raising concerns over copyright infringement and privacy violations, and making data auditing increasingly urgent. Membership inference (MI), which determines whether a sample was used in training, has emerged as a key auditing technique, with promising results on open-source VLMs like LLaVA (AUC > 80%). In this work, we revisit these advances and uncover a critical issue: current MI benchmarks suffer from distribution shifts between member and non-member images, introducing shortcut cues that inflate MI performance. We further analyze the nature of these shifts and propose a principled metric based on optimal transport to quantify the distribution discrepancy. To evaluate MI in realistic settings, we construct new benchmarks with i.i.d. member and non-member images. Existing MI methods fail under these unbiased conditions, performing only marginally better than chance. Further, we explore the theoretical upper bound of MI by probing the Bayes Optimality within the VLM's embedding space and find the irreducible error rate remains high. Despite this pessimistic outlook, we analyze why MI for VLMs is particularly challenging and identify three practical scenarios--fine-tuning, access to ground-truth texts, and set-based inference--where auditing becomes feasible. Our study presents a systematic view of the limits and opportunities of MI for VLMs, providing guidance for future efforts in trustworthy data auditing.</li>
</ul>

<h3>Title: Testing Individual Fairness in Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Roya Nasiri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18353">https://arxiv.org/abs/2504.18353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18353">https://arxiv.org/pdf/2504.18353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18353]] Testing Individual Fairness in Graph Neural Networks(https://arxiv.org/abs/2504.18353)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The biases in artificial intelligence (AI) models can lead to automated decision-making processes that discriminate against groups and/or individuals based on sensitive properties such as gender and race. While there are many studies on diagnosing and mitigating biases in various AI models, there is little research on individual fairness in Graph Neural Networks (GNNs). Unlike traditional models, which treat data features independently and overlook their inter-relationships, GNNs are designed to capture graph-based structure where nodes are interconnected. This relational approach enables GNNs to model complex dependencies, but it also means that biases can propagate through these connections, complicating the detection and mitigation of individual fairness violations. This PhD project aims to develop a testing framework to assess and ensure individual fairness in GNNs. It first systematically reviews the literature on individual fairness, categorizing existing approaches to define, measure, test, and mitigate model biases, creating a taxonomy of individual fairness. Next, the project will develop a framework for testing and ensuring fairness in GNNs by adapting and extending current fairness testing and mitigation techniques. The framework will be evaluated through industrial case studies, focusing on graph-based large language models.</li>
</ul>

<h3>Title: Interpretable Affordance Detection on 3D Point Clouds with Probabilistic Prototypes</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Xiling Li, Korbinian Rudolf, Nils Blank, Rudolf Lioutikov</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18355">https://arxiv.org/abs/2504.18355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18355">https://arxiv.org/pdf/2504.18355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18355]] Interpretable Affordance Detection on 3D Point Clouds with Probabilistic Prototypes(https://arxiv.org/abs/2504.18355)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Robotic agents need to understand how to interact with objects in their environment, both autonomously and during human-robot interactions. Affordance detection on 3D point clouds, which identifies object regions that allow specific interactions, has traditionally relied on deep learning models like PointNet++, DGCNN, or PointTransformerV3. However, these models operate as black boxes, offering no insight into their decision-making processes. Prototypical Learning methods, such as ProtoPNet, provide an interpretable alternative to black-box models by employing a "this looks like that" case-based reasoning approach. However, they have been primarily applied to image-based tasks. In this work, we apply prototypical learning to models for affordance detection on 3D point clouds. Experiments on the 3D-AffordanceNet benchmark dataset show that prototypical models achieve competitive performance with state-of-the-art black-box models and offer inherent interpretability. This makes prototypical models a promising candidate for human-robot interaction scenarios that require increased trust and safety.</li>
</ul>

<h3>Title: COCO-Inpaint: A Benchmark for Image Inpainting Detection and Manipulation Localization</h3>
<ul>
<li><strong>Authors: </strong>Haozhen Yan, Yan Hong, Jiahui Zhan, Yikun Ji, Jun Lan, Huijia Zhu, Weiqiang Wang, Jianfu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18361">https://arxiv.org/abs/2504.18361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18361">https://arxiv.org/pdf/2504.18361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18361]] COCO-Inpaint: A Benchmark for Image Inpainting Detection and Manipulation Localization(https://arxiv.org/abs/2504.18361)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Recent advancements in image manipulation have achieved unprecedented progress in generating photorealistic content, but also simultaneously eliminating barriers to arbitrary manipulation and editing, raising concerns about multimedia authenticity and cybersecurity. However, existing Image Manipulation Detection and Localization (IMDL) methodologies predominantly focus on splicing or copy-move forgeries, lacking dedicated benchmarks for inpainting-based manipulations. To bridge this gap, we present COCOInpaint, a comprehensive benchmark specifically designed for inpainting detection, with three key contributions: 1) High-quality inpainting samples generated by six state-of-the-art inpainting models, 2) Diverse generation scenarios enabled by four mask generation strategies with optional text guidance, and 3) Large-scale coverage with 258,266 inpainted images with rich semantic diversity. Our benchmark is constructed to emphasize intrinsic inconsistencies between inpainted and authentic regions, rather than superficial semantic artifacts such as object shapes. We establish a rigorous evaluation protocol using three standard metrics to assess existing IMDL approaches. The dataset will be made publicly available to facilitate future research in this area.</li>
</ul>

<h3>Title: ThreMoLIA: Threat Modeling of Large Language Model-Integrated Applications</h3>
<ul>
<li><strong>Authors: </strong>Felix Viktor Jedrzejewski, Davide Fucci, Oleksandr Adamov</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18369">https://arxiv.org/abs/2504.18369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18369">https://arxiv.org/pdf/2504.18369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18369]] ThreMoLIA: Threat Modeling of Large Language Model-Integrated Applications(https://arxiv.org/abs/2504.18369)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are currently being integrated into industrial software applications to help users perform more complex tasks in less time. However, these LLM-Integrated Applications (LIA) expand the attack surface and introduce new kinds of threats. Threat modeling is commonly used to identify these threats and suggest mitigations. However, it is a time-consuming practice that requires the involvement of a security practitioner. Our goals are to 1) provide a method for performing threat modeling for LIAs early in their lifecycle, (2) develop a threat modeling tool that integrates existing threat models, and (3) ensure high-quality threat modeling. To achieve the goals, we work in collaboration with our industry partner. Our proposed way of performing threat modeling will benefit industry by requiring fewer security experts' participation and reducing the time spent on this activity. Our proposed tool combines LLMs and Retrieval Augmented Generation (RAG) and uses sources such as existing threat models and application architecture repositories to continuously create and update threat models. We propose to evaluate the tool offline -- i.e., using benchmarking -- and online with practitioners in the field. We conducted an early evaluation using ChatGPT on a simple LIA and obtained results that encouraged us to proceed with our research efforts.</li>
</ul>

<h3>Title: Explainable AI for UAV Mobility Management: A Deep Q-Network Approach for Handover Minimization</h3>
<ul>
<li><strong>Authors: </strong>Irshad A. Meer, Bruno Hrmann, Mustafa Ozger, Fabien Geyer, Alberto Viseras, Dominic Schupke, Cicek Cavdar</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18371">https://arxiv.org/abs/2504.18371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18371">https://arxiv.org/pdf/2504.18371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18371]] Explainable AI for UAV Mobility Management: A Deep Q-Network Approach for Handover Minimization(https://arxiv.org/abs/2504.18371)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The integration of unmanned aerial vehicles (UAVs) into cellular networks presents significant mobility management challenges, primarily due to frequent handovers caused by probabilistic line-of-sight conditions with multiple ground base stations (BSs). To tackle these challenges, reinforcement learning (RL)-based methods, particularly deep Q-networks (DQN), have been employed to optimize handover decisions dynamically. However, a major drawback of these learning-based approaches is their black-box nature, which limits interpretability in the decision-making process. This paper introduces an explainable AI (XAI) framework that incorporates Shapley Additive Explanations (SHAP) to provide deeper insights into how various state parameters influence handover decisions in a DQN-based mobility management system. By quantifying the impact of key features such as reference signal received power (RSRP), reference signal received quality (RSRQ), buffer status, and UAV position, our approach enhances the interpretability and reliability of RL-based handover solutions. To validate and compare our framework, we utilize real-world network performance data collected from UAV flight trials. Simulation results show that our method provides intuitive explanations for policy decisions, effectively bridging the gap between AI-driven models and human decision-makers.</li>
</ul>

<h3>Title: Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant</h3>
<ul>
<li><strong>Authors: </strong>Lei Shen, Xiaoyu Shen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18373">https://arxiv.org/abs/2504.18373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18373">https://arxiv.org/pdf/2504.18373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18373]] Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant(https://arxiv.org/abs/2504.18373)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, multi-agent frameworks powered by large language models (LLMs) have advanced rapidly. Despite this progress, there is still a notable absence of benchmark datasets specifically tailored to evaluate their performance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset aimed at evaluating LLM-based multi-agent frameworks in the context of intelligent personal assistants. Auto-SLURP extends the original SLURP dataset -- initially developed for natural language understanding tasks -- by relabeling the data and integrating simulated servers and external services. This enhancement enables a comprehensive end-to-end evaluation pipeline, covering language understanding, task execution, and response generation. Our experiments demonstrate that Auto-SLURP presents a significant challenge for current state-of-the-art frameworks, highlighting that truly reliable and intelligent multi-agent personal assistants remain a work in progress. The dataset and related code are available at this https URL.</li>
</ul>

<h3>Title: Bandit on the Hunt: Dynamic Crawling for Cyber Threat Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Philipp Kuehn, Dilara Nadermahmoodi, Markus Bayer, Christian Reuter</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18375">https://arxiv.org/abs/2504.18375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18375">https://arxiv.org/pdf/2504.18375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18375]] Bandit on the Hunt: Dynamic Crawling for Cyber Threat Intelligence(https://arxiv.org/abs/2504.18375)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Public information contains valuable Cyber Threat Intelligence (CTI) that is used to prevent future attacks. While standards exist for sharing this information, much appears in non-standardized news articles or blogs. Monitoring online sources for threats is time-consuming and source selection is uncertain. Current research focuses on extracting Indicators of Compromise from known sources, rarely addressing new source identification. This paper proposes a CTI-focused crawler using multi-armed bandit (MAB) and various crawling strategies. It employs SBERT to identify relevant documents while dynamically adapting its crawling path. Our system ThreatCrawl achieves a harvest rate exceeding 25% and expands its seed by over 300% while maintaining topical focus. Additionally, the crawler identifies previously unknown but highly relevant overview pages, datasets, and domains.</li>
</ul>

<h3>Title: Pushing the boundary on Natural Language Inference</h3>
<ul>
<li><strong>Authors: </strong>Pablo Miralles-Gonzlez, Javier Huertas-Tato, Alejandro Martn, David Camacho</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18376">https://arxiv.org/abs/2504.18376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18376">https://arxiv.org/pdf/2504.18376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18376]] Pushing the boundary on Natural Language Inference(https://arxiv.org/abs/2504.18376)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Natural Language Inference (NLI) is a central task in natural language understanding with applications in fact-checking, question answering, and information retrieval. Despite its importance, current NLI systems heavily rely on supervised learning with datasets that often contain annotation artifacts and biases, limiting generalization and real-world applicability. In this work, we apply a reinforcement learning-based approach using Group Relative Policy Optimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the need for labeled rationales and enabling this type of training on more challenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language models using parameter-efficient techniques (LoRA and QLoRA), demonstrating strong performance across standard and adversarial NLI benchmarks. Our 32B AWQ-quantized model surpasses state-of-the-art results on 7 out of 11 adversarial sets$\unicode{x2013}$or on all of them considering our replication$\unicode{x2013}$within a 22GB memory footprint, showing that robust reasoning can be retained under aggressive quantization. This work provides a scalable and practical framework for building robust NLI systems without sacrificing inference quality.</li>
</ul>

<h3>Title: Model Evaluation in the Dark: Robust Classifier Metrics with Missing Labels</h3>
<ul>
<li><strong>Authors: </strong>Danial Dervovic, Michael Cashmore</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18385">https://arxiv.org/abs/2504.18385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18385">https://arxiv.org/pdf/2504.18385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18385]] Model Evaluation in the Dark: Robust Classifier Metrics with Missing Labels(https://arxiv.org/abs/2504.18385)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Missing data in supervised learning is well-studied, but the specific issue of missing labels during model evaluation has been overlooked. Ignoring samples with missing values, a common solution, can introduce bias, especially when data is Missing Not At Random (MNAR). We propose a multiple imputation technique for evaluating classifiers using metrics such as precision, recall, and ROC-AUC. This method not only offers point estimates but also a predictive distribution for these quantities when labels are missing. We empirically show that the predictive distribution's location and shape are generally correct, even in the MNAR regime. Moreover, we establish that this distribution is approximately Gaussian and provide finite-sample convergence bounds. Additionally, a robustness proof is presented, confirming the validity of the approximation under a realistic error model.</li>
</ul>

<h3>Title: Fast Autoregressive Models for Continuous Latent Generation</h3>
<ul>
<li><strong>Authors: </strong>Tiankai Hang, Jianmin Bao, Fangyun Wei, Dong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18391">https://arxiv.org/abs/2504.18391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18391">https://arxiv.org/pdf/2504.18391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18391]] Fast Autoregressive Models for Continuous Latent Generation(https://arxiv.org/abs/2504.18391)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Autoregressive models have demonstrated remarkable success in sequential data generation, particularly in NLP, but their extension to continuous-domain image generation presents significant challenges. Recent work, the masked autoregressive model (MAR), bypasses quantization by modeling per-token distributions in continuous spaces using a diffusion head but suffers from slow inference due to the high computational cost of the iterative denoising process. To address this, we propose the Fast AutoRegressive model (FAR), a novel framework that replaces MAR's diffusion head with a lightweight shortcut head, enabling efficient few-step sampling while preserving autoregressive principles. Additionally, FAR seamlessly integrates with causal Transformers, extending them from discrete to continuous token generation without requiring architectural modifications. Experiments demonstrate that FAR achieves $2.3\times$ faster inference than MAR while maintaining competitive FID and IS scores. This work establishes the first efficient autoregressive paradigm for high-fidelity continuous-space image generation, bridging the critical gap between quality and scalability in visual autoregressive modeling.</li>
</ul>

<h3>Title: Three Types of Calibration with Properties and their Semantic and Formal Relationships</h3>
<ul>
<li><strong>Authors: </strong>Rabanus Derr, Jessie Finocchiaro, Robert C. Williamson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18395">https://arxiv.org/abs/2504.18395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18395">https://arxiv.org/pdf/2504.18395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18395]] Three Types of Calibration with Properties and their Semantic and Formal Relationships(https://arxiv.org/abs/2504.18395)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Fueled by discussions around "trustworthiness" and algorithmic fairness, calibration of predictive systems has regained scholars attention. The vanilla definition and understanding of calibration is, simply put, on all days on which the rain probability has been predicted to be p, the actual frequency of rain days was p. However, the increased attention has led to an immense variety of new notions of "calibration." Some of the notions are incomparable, serve different purposes, or imply each other. In this work, we provide two accounts which motivate calibration: self-realization of forecasted properties and precise estimation of incurred losses of the decision makers relying on forecasts. We substantiate the former via the reflection principle and the latter by actuarial fairness. For both accounts we formulate prototypical definitions via properties $\Gamma$ of outcome distributions, e.g., the mean or median. The prototypical definition for self-realization, which we call $\Gamma$-calibration, is equivalent to a certain type of swap regret under certain conditions. These implications are strongly connected to the omniprediction learning paradigm. The prototypical definition for precise loss estimation is a modification of decision calibration adopted from Zhao et al. [73]. For binary outcome sets both prototypical definitions coincide under appropriate choices of reference properties. For higher-dimensional outcome sets, both prototypical definitions can be subsumed by a natural extension of the binary definition, called distribution calibration with respect to a property. We conclude by commenting on the role of groupings in both accounts of calibration often used to obtain multicalibration. In sum, this work provides a semantic map of calibration in order to navigate a fragmented terrain of notions and definitions.</li>
</ul>

<h3>Title: Unsupervised Visual Chain-of-Thought Reasoning via Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Kesen Zhao, Beier Zhu, Qianru Sun, Hanwang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18397">https://arxiv.org/abs/2504.18397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18397">https://arxiv.org/pdf/2504.18397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18397]] Unsupervised Visual Chain-of-Thought Reasoning via Preference Optimization(https://arxiv.org/abs/2504.18397)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-thought (CoT) reasoning greatly improves the interpretability and problem-solving abilities of multimodal large language models (MLLMs). However, existing approaches are focused on text CoT, limiting their ability to leverage visual cues. Visual CoT remains underexplored, and the only work is based on supervised fine-tuning (SFT) that relies on extensive labeled bounding-box data and is hard to generalize to unseen cases. In this paper, we introduce Unsupervised Visual CoT (UV-CoT), a novel framework for image-level CoT reasoning via preference optimization. UV-CoT performs preference comparisons between model-generated bounding boxes (one is preferred and the other is dis-preferred), eliminating the need for bounding-box annotations. We get such preference data by introducing an automatic data generation pipeline. Given an image, our target MLLM (e.g., LLaVA-1.5-7B) generates seed bounding boxes using a template prompt and then answers the question using each bounded region as input. An evaluator MLLM (e.g., OmniLLM-12B) ranks the responses, and these rankings serve as supervision to train the target MLLM with UV-CoT by minimizing negative log-likelihood losses. By emulating human perception--identifying key regions and reasoning based on them--UV-CoT can improve visual comprehension, particularly in spatial reasoning tasks where textual descriptions alone fall short. Our experiments on six datasets demonstrate the superiority of UV-CoT, compared to the state-of-the-art textual and visual CoT methods. Our zero-shot testing on four unseen datasets shows the strong generalization of UV-CoT. The code is available in this https URL.</li>
</ul>

<h3>Title: HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?</h3>
<ul>
<li><strong>Authors: </strong>Yusen Zhang, Wenliang Zheng, Aashrith Madasu, Peng Shi, Ryo Kamoi, Hao Zhou, Zhuoyang Zou, Shu Zhao, Sarkar Snigdha Sarathi Das, Vipul Gupta, Xiaoxin Lu, Nan Zhang, Ranran Haoran Zhang, Avitej Iyer, Renze Lou, Wenpeng Yin, Rui Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18406">https://arxiv.org/abs/2504.18406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18406">https://arxiv.org/pdf/2504.18406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18406]] HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?(https://arxiv.org/abs/2504.18406)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>High-resolution image (HRI) understanding aims to process images with a large number of pixels, such as pathological images and agricultural aerial images, both of which can exceed 1 million pixels. Vision Large Language Models (VLMs) can allegedly handle HRIs, however, there is a lack of a comprehensive benchmark for VLMs to evaluate HRI understanding. To address this gap, we introduce HRScene, a novel unified benchmark for HRI understanding with rich scenes. HRScene incorporates 25 real-world datasets and 2 synthetic diagnostic datasets with resolutions ranging from 1,024 $\times$ 1,024 to 35,503 $\times$ 26,627. HRScene is collected and re-annotated by 10 graduate-level annotators, covering 25 scenarios, ranging from microscopic to radiology images, street views, long-range pictures, and telescope images. It includes HRIs of real-world objects, scanned documents, and composite multi-image. The two diagnostic evaluation datasets are synthesized by combining the target image with the gold answer and distracting images in different orders, assessing how well models utilize regions in HRI. We conduct extensive experiments involving 28 VLMs, including Gemini 2.0 Flash and GPT-4o. Experiments on HRScene show that current VLMs achieve an average accuracy of around 50% on real-world tasks, revealing significant gaps in HRI understanding. Results on synthetic datasets reveal that VLMs struggle to effectively utilize HRI regions, showing significant Regional Divergence and lost-in-middle, shedding light on future research.</li>
</ul>

<h3>Title: Heavy-Tailed Privacy: The Symmetric alpha-Stable Privacy Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Christopher C. Zawacki, Eyad H. Abed</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18411">https://arxiv.org/abs/2504.18411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18411">https://arxiv.org/pdf/2504.18411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18411]] Heavy-Tailed Privacy: The Symmetric alpha-Stable Privacy Mechanism(https://arxiv.org/abs/2504.18411)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>With the rapid growth of digital platforms, there is increasing apprehension about how personal data is collected, stored, and used by various entities. These concerns arise from the increasing frequency of data breaches, cyber-attacks, and misuse of personal information for targeted advertising and surveillance. To address these matters, Differential Privacy (DP) has emerged as a prominent tool for quantifying a digital system's level of protection. The Gaussian mechanism is commonly used because the Gaussian density is closed under convolution, and is a common method utilized when aggregating datasets. However, the Gaussian mechanism only satisfies an approximate form of Differential Privacy. In this work, we present and analyze of the Symmetric alpha-Stable (SaS) mechanism. We prove that the mechanism achieves pure differential privacy while remaining closed under convolution. Additionally, we study the nuanced relationship between the level of privacy achieved and the parameters of the density. Lastly, we compare the expected error introduced to dataset queries by the Gaussian and SaS mechanisms. From our analysis, we believe the SaS Mechanism is an appealing choice for privacy-focused applications.</li>
</ul>

<h3>Title: Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers</h3>
<ul>
<li><strong>Authors: </strong>Jared Moore, Declan Grabb, William Agnew, Kevin Klyman, Stevie Chancellor, Desmond C. Ong, Nick Haber</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18412">https://arxiv.org/abs/2504.18412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18412">https://arxiv.org/pdf/2504.18412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18412]] Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers(https://arxiv.org/abs/2504.18412)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Should a large language model (LLM) be used as a therapist? In this paper, we investigate the use of LLMs to *replace* mental health providers, a use case promoted in the tech startup and research space. We conduct a mapping review of therapy guides used by major medical institutions to identify crucial aspects of therapeutic relationships, such as the importance of a therapeutic alliance between therapist and client. We then assess the ability of LLMs to reproduce and adhere to these aspects of therapeutic relationships by conducting several experiments investigating the responses of current LLMs, such as `gpt-4o`. Contrary to best practices in the medical community, LLMs 1) express stigma toward those with mental health conditions and 2) respond inappropriately to certain common (and critical) conditions in naturalistic therapy settings -- e.g., LLMs encourage clients' delusional thinking, likely due to their sycophancy. This occurs even with larger and newer LLMs, indicating that current safety practices may not address these gaps. Furthermore, we note foundational and practical barriers to the adoption of LLMs as therapists, such as that a therapeutic alliance requires human characteristics (e.g., identity and stakes). For these reasons, we conclude that LLMs should not replace therapists, and we discuss alternative roles for LLMs in clinical therapy.</li>
</ul>

<h3>Title: BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Wang, Shuming Ma, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18415">https://arxiv.org/abs/2504.18415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18415">https://arxiv.org/pdf/2504.18415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18415]] BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs(https://arxiv.org/abs/2504.18415)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward network activations, we propose H-BitLinear, a module applying an online Hadamard transformation prior to activation quantization. This transformation smooths sharp activation distributions into more Gaussian-like forms, suitable for low-bit representation. Experiments show BitNet v2 trained from scratch with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2 achieves minimal performance degradation when trained with native 4-bit activations, significantly reducing memory footprint and computational cost for batched inference.</li>
</ul>

<h3>Title: LLMpatronous: Harnessing the Power of LLMs For Vulnerability Detection</h3>
<ul>
<li><strong>Authors: </strong>Rajesh Yarra</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18423">https://arxiv.org/abs/2504.18423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18423">https://arxiv.org/pdf/2504.18423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18423]] LLMpatronous: Harnessing the Power of LLMs For Vulnerability Detection(https://arxiv.org/abs/2504.18423)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Despite the transformative impact of Artificial Intelligence (AI) across various sectors, cyber security continues to rely on traditional static and dynamic analysis tools, hampered by high false positive rates and superficial code comprehension. While generative AI offers promising automation capabilities for software development, leveraging Large Language Models (LLMs) for vulnerability detection presents unique challenges. This paper explores the potential and limitations of LLMs in identifying vulnerabilities, acknowledging inherent weaknesses such as hallucinations, limited context length, and knowledge cut-offs. Previous attempts employing machine learning models for vulnerability detection have proven ineffective due to limited real-world applicability, feature engineering challenges, lack of contextual understanding, and the complexities of training models to keep pace with the evolving threat landscape. Therefore, we propose a robust AI-driven approach focused on mitigating these limitations and ensuring the quality and reliability of LLM based vulnerability detection. Through innovative methodologies combining Retrieval-Augmented Generation (RAG) and Mixtureof-Agents (MoA), this research seeks to leverage the strengths of LLMs while addressing their weaknesses, ultimately paving the way for dependable and efficient AI-powered solutions in securing the ever-evolving software landscape.</li>
</ul>

<h3>Title: LaRI: Layered Ray Intersections for Single-view 3D Geometric Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Rui Li, Biao Zhang, Zhenyu Li, Federico Tombari, Peter Wonka</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18424">https://arxiv.org/abs/2504.18424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18424">https://arxiv.org/pdf/2504.18424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18424]] LaRI: Layered Ray Intersections for Single-view 3D Geometric Reasoning(https://arxiv.org/abs/2504.18424)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present layered ray intersections (LaRI), a new method for unseen geometry reasoning from a single image. Unlike conventional depth estimation that is limited to the visible surface, LaRI models multiple surfaces intersected by the camera rays using layered point maps. Benefiting from the compact and layered representation, LaRI enables complete, efficient, and view-aligned geometric reasoning to unify object- and scene-level tasks. We further propose to predict the ray stopping index, which identifies valid intersecting pixels and layers from LaRI's output. We build a complete training data generation pipeline for synthetic and real-world data, including 3D objects and scenes, with necessary data cleaning steps and coordination between rendering engines. As a generic method, LaRI's performance is validated in two scenarios: It yields comparable object-level results to the recent large generative model using 4% of its training data and 17% of its parameters. Meanwhile, it achieves scene-level occluded geometry reasoning in only one feed-forward.</li>
</ul>

<h3>Title: Iterative Event-based Motion Segmentation by Variational Contrast Maximization</h3>
<ul>
<li><strong>Authors: </strong>Ryo Yamaki, Shintaro Shiba, Guillermo Gallego, Yoshimitsu Aoki</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18447">https://arxiv.org/abs/2504.18447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18447">https://arxiv.org/pdf/2504.18447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18447]] Iterative Event-based Motion Segmentation by Variational Contrast Maximization(https://arxiv.org/abs/2504.18447)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Event cameras provide rich signals that are suitable for motion estimation since they respond to changes in the scene. As any visual changes in the scene produce event data, it is paramount to classify the data into different motions (i.e., motion segmentation), which is useful for various tasks such as object detection and visual servoing. We propose an iterative motion segmentation method, by classifying events into background (e.g., dominant motion hypothesis) and foreground (independent motion residuals), thus extending the Contrast Maximization framework. Experimental results demonstrate that the proposed method successfully classifies event clusters both for public and self-recorded datasets, producing sharp, motion-compensated edge-like images. The proposed method achieves state-of-the-art accuracy on moving object detection benchmarks with an improvement of over 30%, and demonstrates its possibility of applying to more complex and noisy real-world scenes. We hope this work broadens the sensitivity of Contrast Maximization with respect to both motion parameters and input events, thus contributing to theoretical advancements in event-based motion segmentation estimation. this https URL</li>
</ul>

<h3>Title: Pseudo-Asynchronous Local SGD: Robust and Efficient Data-Parallel Training</h3>
<ul>
<li><strong>Authors: </strong>Hiroki Naganuma, Xinzhi Zhang, Man-Chung Yue, Ioannis Mitliagkas, Philipp A. Witte, Russell J. Hewett, Yin Tat Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18454">https://arxiv.org/abs/2504.18454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18454">https://arxiv.org/pdf/2504.18454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18454]] Pseudo-Asynchronous Local SGD: Robust and Efficient Data-Parallel Training(https://arxiv.org/abs/2504.18454)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Following AI scaling trends, frontier models continue to grow in size and continue to be trained on larger datasets. Training these models requires huge investments in exascale computational resources, which has in turn driven development of distributed deep learning methods. Data parallelism is an essential approach to speed up training, but it requires frequent global communication between workers, which can bottleneck training at the largest scales. In this work, we propose a method called Pseudo-Asynchronous Local SGD (PALSGD) to improve the efficiency of data-parallel training. PALSGD is an extension of Local SGD (Stich, 2018) and DiLoCo (Douillard et al., 2023), designed to further reduce communication frequency by introducing a pseudo-synchronization mechanism. PALSGD allows the use of longer synchronization intervals compared to standard Local SGD. Despite the reduced communication frequency, the pseudo-synchronization approach ensures that model consistency is maintained, leading to performance results comparable to those achieved with more frequent synchronization. Furthermore, we provide a theoretical analysis of PALSGD, establishing its convergence and deriving its convergence rate. This analysis offers insights into the algorithm's behavior and performance guarantees. We evaluated PALSGD on image classification and language modeling tasks. Our results show that PALSGD achieves better performance in less time compared to existing methods like Distributed Data Parallel (DDP), and DiLoCo. Notably, PALSGD trains 18.4% faster than DDP on ImageNet-1K with ResNet-50, 24.4% faster than DDP on TinyStories with GPT-Neo125M, and 21.1% faster than DDP on TinyStories with GPT-Neo-8M.</li>
</ul>

<h3>Title: Generative Induction of Dialogue Task Schemas with Streaming Refinement and Simulated Interactions</h3>
<ul>
<li><strong>Authors: </strong>James D. Finch, Yasasvi Josyula, Jinho D. Choi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18474">https://arxiv.org/abs/2504.18474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18474">https://arxiv.org/pdf/2504.18474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18474]] Generative Induction of Dialogue Task Schemas with Streaming Refinement and Simulated Interactions(https://arxiv.org/abs/2504.18474)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In task-oriented dialogue (TOD) systems, Slot Schema Induction (SSI) is essential for automatically identifying key information slots from dialogue data without manual intervention. This paper presents a novel state-of-the-art (SoTA) approach that formulates SSI as a text generation task, where a language model incrementally constructs and refines a slot schema over a stream of dialogue data. To develop this approach, we present a fully automatic LLM-based TOD simulation method that creates data with high-quality state labels for novel task domains. Furthermore, we identify issues in SSI evaluation due to data leakage and poor metric alignment with human judgment. We resolve these by creating new evaluation data using our simulation method with human guidance and correction, as well as designing improved evaluation metrics. These contributions establish a foundation for future SSI research and advance the SoTA in dialogue understanding and system development.</li>
</ul>

<h3>Title: Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Leandra Fichtel, Maximilian Spliethver, Eyke Hllermeier, Patricia Jimenez, Nils Klowait, Stefan Kopp, Axel-Cyrille Ngonga Ngomo, Amelie Robrecht, Ingrid Scharlau, Lutz Terfloth, Anna-Lisa Vollmer, Henning Wachsmuth</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18483">https://arxiv.org/abs/2504.18483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18483">https://arxiv.org/pdf/2504.18483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18483]] Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues(https://arxiv.org/abs/2504.18483)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The ability to generate explanations that are understood by explainees is the quintessence of explainable artificial intelligence. Since understanding depends on the explainee's background and needs, recent research has focused on co-constructive explanation dialogues, where the explainer continuously monitors the explainee's understanding and adapts explanations dynamically. We investigate the ability of large language models (LLMs) to engage as explainers in co-constructive explanation dialogues. In particular, we present a user study in which explainees interact with LLMs, of which some have been instructed to explain a predefined topic co-constructively. We evaluate the explainees' understanding before and after the dialogue, as well as their perception of the LLMs' co-constructive behavior. Our results indicate that current LLMs show some co-constructive behaviors, such as asking verification questions, that foster the explainees' engagement and can improve understanding of a topic. However, their ability to effectively monitor the current understanding and scaffold the explanations accordingly remains limited.</li>
</ul>

<h3>Title: An Improved ResNet50 Model for Predicting Pavement Condition Index (PCI) Directly from Pavement Images</h3>
<ul>
<li><strong>Authors: </strong>Andrews Danyo, Anthony Dontoh, Armstrong Aboah</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18490">https://arxiv.org/abs/2504.18490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18490">https://arxiv.org/pdf/2504.18490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18490]] An Improved ResNet50 Model for Predicting Pavement Condition Index (PCI) Directly from Pavement Images(https://arxiv.org/abs/2504.18490)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Accurately predicting the Pavement Condition Index (PCI), a measure of roadway conditions, from pavement images is crucial for infrastructure maintenance. This study proposes an enhanced version of the Residual Network (ResNet50) architecture, integrated with a Convolutional Block Attention Module (CBAM), to predict PCI directly from pavement images without additional annotations. By incorporating CBAM, the model autonomously prioritizes critical features within the images, improving prediction accuracy. Compared to the original baseline ResNet50 and DenseNet161 architectures, the enhanced ResNet50-CBAM model achieved a significantly lower mean absolute percentage error (MAPE) of 58.16%, compared to the baseline models that achieved 70.76% and 65.48% respectively. These results highlight the potential of using attention mechanisms to refine feature extraction, ultimately enabling more accurate and efficient assessments of pavement conditions. This study emphasizes the importance of targeted feature refinement in advancing automated pavement analysis through attention mechanisms.</li>
</ul>

<h3>Title: DeSIA: Attribute Inference Attacks Against Limited Fixed Aggregate Statistics</h3>
<ul>
<li><strong>Authors: </strong>Yifeng Mao, Bozhidar Stevanoski, Yves-Alexandre de Montjoye</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18497">https://arxiv.org/abs/2504.18497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18497">https://arxiv.org/pdf/2504.18497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18497]] DeSIA: Attribute Inference Attacks Against Limited Fixed Aggregate Statistics(https://arxiv.org/abs/2504.18497)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Empirical inference attacks are a popular approach for evaluating the privacy risk of data release mechanisms in practice. While an active attack literature exists to evaluate machine learning models or synthetic data release, we currently lack comparable methods for fixed aggregate statistics, in particular when only a limited number of statistics are released. We here propose an inference attack framework against fixed aggregate statistics and an attribute inference attack called DeSIA. We instantiate DeSIA against the U.S. Census PPMF dataset and show it to strongly outperform reconstruction-based attacks. In particular, we show DeSIA to be highly effective at identifying vulnerable users, achieving a true positive rate of 0.14 at a false positive rate of $10^{-3}$. We then show DeSIA to perform well against users whose attributes cannot be verified and when varying the number of aggregate statistics and level of noise addition. We also perform an extensive ablation study of DeSIA and show how DeSIA can be successfully adapted to the membership inference task. Overall, our results show that aggregation alone is not sufficient to protect privacy, even when a relatively small number of aggregates are being released, and emphasize the need for formal privacy mechanisms and testing before aggregate statistics are released.</li>
</ul>

<h3>Title: Action-Minimization Meets Generative Modeling: Efficient Transition Path Sampling with the Onsager-Machlup Functional</h3>
<ul>
<li><strong>Authors: </strong>Sanjeev Raja, Martin pka, Michael Psenka, Tobias Kreiman, Michal Pavelka, Aditi S. Krishnapriyan</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, physics.chem-ph, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18506">https://arxiv.org/abs/2504.18506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18506">https://arxiv.org/pdf/2504.18506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18506]] Action-Minimization Meets Generative Modeling: Efficient Transition Path Sampling with the Onsager-Machlup Functional(https://arxiv.org/abs/2504.18506)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, data-free, generative</a></li>
<li><strong>Abstract: </strong>Transition path sampling (TPS), which involves finding probable paths connecting two points on an energy landscape, remains a challenge due to the complexity of real-world atomistic systems. Current machine learning approaches use expensive, task-specific, and data-free training procedures, limiting their ability to benefit from recent advances in atomistic machine learning, such as high-quality datasets and large-scale pre-trained models. In this work, we address TPS by interpreting candidate paths as trajectories sampled from stochastic dynamics induced by the learned score function of pre-trained generative models, specifically denoising diffusion and flow matching. Under these dynamics, finding high-likelihood transition paths becomes equivalent to minimizing the Onsager-Machlup (OM) action functional. This enables us to repurpose pre-trained generative models for TPS in a zero-shot manner, in contrast with bespoke, task-specific TPS models trained in previous work. We demonstrate our approach on varied molecular systems, obtaining diverse, physically realistic transition pathways and generalizing beyond the pre-trained model's original training dataset. Our method can be easily incorporated into new generative models, making it practically relevant as models continue to scale and improve with increased data availability.</li>
</ul>

<h3>Title: Eval3D: Interpretable and Fine-grained Evaluation for 3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Shivam Duggal, Yushi Hu, Oscar Michel, Aniruddha Kembhavi, William T. Freeman, Noah A. Smith, Ranjay Krishna, Antonio Torralba, Ali Farhadi, Wei-Chiu Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18509">https://arxiv.org/abs/2504.18509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18509">https://arxiv.org/pdf/2504.18509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18509]] Eval3D: Interpretable and Fine-grained Evaluation for 3D Generation(https://arxiv.org/abs/2504.18509)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the unprecedented progress in the field of 3D generation, current systems still often fail to produce high-quality 3D assets that are visually appealing and geometrically and semantically consistent across multiple viewpoints. To effectively assess the quality of the generated 3D data, there is a need for a reliable 3D evaluation tool. Unfortunately, existing 3D evaluation metrics often overlook the geometric quality of generated assets or merely rely on black-box multimodal large language models for coarse assessment. In this paper, we introduce Eval3D, a fine-grained, interpretable evaluation tool that can faithfully evaluate the quality of generated 3D assets based on various distinct yet complementary criteria. Our key observation is that many desired properties of 3D generation, such as semantic and geometric consistency, can be effectively captured by measuring the consistency among various foundation models and tools. We thus leverage a diverse set of models and tools as probes to evaluate the inconsistency of generated 3D assets across different aspects. Compared to prior work, Eval3D provides pixel-wise measurement, enables accurate 3D spatial feedback, and aligns more closely with human judgments. We comprehensively evaluate existing 3D generation models using Eval3D and highlight the limitations and challenges of current models.</li>
</ul>

<h3>Title: Examining the Impact of Optical Aberrations to Image Classification and Object Detection Models</h3>
<ul>
<li><strong>Authors: </strong>Patrick Mller, Alexander Braun, Margret Keuper</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18510">https://arxiv.org/abs/2504.18510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18510">https://arxiv.org/pdf/2504.18510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18510]] Examining the Impact of Optical Aberrations to Image Classification and Object Detection Models(https://arxiv.org/abs/2504.18510)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) have proven to be successful in various computer vision applications such that models even infer in safety-critical situations. Therefore, vision models have to behave in a robust way to disturbances such as noise or blur. While seminal benchmarks exist to evaluate model robustness to diverse corruptions, blur is often approximated in an overly simplistic way to model defocus, while ignoring the different blur kernel shapes that result from optical systems. To study model robustness against realistic optical blur effects, this paper proposes two datasets of blur corruptions, which we denote OpticsBench and LensCorruptions. OpticsBench examines primary aberrations such as coma, defocus, and astigmatism, i.e. aberrations that can be represented by varying a single parameter of Zernike polynomials. To go beyond the principled but synthetic setting of primary aberrations, LensCorruptions samples linear combinations in the vector space spanned by Zernike polynomials, corresponding to 100 real lenses. Evaluations for image classification and object detection on ImageNet and MSCOCO show that for a variety of different pre-trained models, the performance on OpticsBench and LensCorruptions varies significantly, indicating the need to consider realistic image corruptions to evaluate a model's robustness against blur.</li>
</ul>

<h3>Title: Intelligent Attacks and Defense Methods in Federated Learning-enabled Energy-Efficient Wireless Networks</h3>
<ul>
<li><strong>Authors: </strong>Han Zhang, Hao Zhou, Medhat Elsayed, Majid Bavand, Raimundas Gaigalas, Yigit Ozcan, Melike Erol-Kantarci</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18519">https://arxiv.org/abs/2504.18519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18519">https://arxiv.org/pdf/2504.18519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18519]] Intelligent Attacks and Defense Methods in Federated Learning-enabled Energy-Efficient Wireless Networks(https://arxiv.org/abs/2504.18519)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, federate, generative</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a promising technique for learning-based functions in wireless networks, thanks to its distributed implementation capability. On the other hand, distributed learning may increase the risk of exposure to malicious attacks where attacks on a local model may spread to other models by parameter exchange. Meanwhile, such attacks can be hard to detect due to the dynamic wireless environment, especially considering local models can be heterogeneous with non-independent and identically distributed (non-IID) data. Therefore, it is critical to evaluate the effect of malicious attacks and develop advanced defense techniques for FL-enabled wireless networks. In this work, we introduce a federated deep reinforcement learning-based cell sleep control scenario that enhances the energy efficiency of the network. We propose multiple intelligent attacks targeting the learning-based approach and we propose defense methods to mitigate such attacks. In particular, we have designed two attack models, generative adversarial network (GAN)-enhanced model poisoning attack and regularization-based model poisoning attack. As a counteraction, we have proposed two defense schemes, autoencoder-based defense, and knowledge distillation (KD)-enabled defense. The autoencoder-based defense method leverages an autoencoder to identify the malicious participants and only aggregate the parameters of benign local models during the global aggregation, while KD-based defense protects the model from attacks by controlling the knowledge transferred between the global model and local models.</li>
</ul>

<h3>Title: TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation</h3>
<ul>
<li><strong>Authors: </strong>Gwen Yidou Weng, Benjie Wang, Guy Van den Broeck</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.18535">https://arxiv.org/abs/2504.18535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.18535">https://arxiv.org/pdf/2504.18535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.18535]] TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation(https://arxiv.org/abs/2504.18535)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LMs) advance, there is an increasing need to control their outputs to align with human values (e.g., detoxification) or desired attributes (e.g., personalization, topic). However, autoregressive models focus on next-token predictions and struggle with global properties that require looking ahead. Existing solutions either tune or post-train LMs for each new attribute - expensive and inflexible - or approximate the Expected Attribute Probability (EAP) of future sequences by sampling or training, which is slow and unreliable for rare attributes. We introduce TRACE (Tractable Probabilistic Reasoning for Adaptable Controllable gEneration), a novel framework that efficiently computes EAP and adapts to new attributes through tractable probabilistic reasoning and lightweight control. TRACE distills a Hidden Markov Model (HMM) from an LM and pairs it with a small classifier to estimate attribute probabilities, enabling exact EAP computation over the HMM's predicted futures. This EAP is then used to reweigh the LM's next-token probabilities for globally compliant continuations. Empirically, TRACE achieves state-of-the-art results in detoxification with only 10% decoding overhead, adapts to 76 low-resource personalized LLMs within seconds, and seamlessly extends to composite attributes.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
