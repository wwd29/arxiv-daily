<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-12-02</h1>
<h3>Title: MOTION: ML-Assisted On-Device Low-Latency Motion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Veeramani Pugazhenthi, Wei-Hsiang Chu, Junwei Lu, Jadyn N. Miyahira, Soheil Salehi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00008">https://arxiv.org/abs/2512.00008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00008">https://arxiv.org/pdf/2512.00008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00008]] MOTION: ML-Assisted On-Device Low-Latency Motion Recognition(https://arxiv.org/abs/2512.00008)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>The use of tiny devices capable of low-latency gesture recognition is gaining momentum in everyday human-computer interaction and especially in medical monitoring fields. Embedded solutions such as fall detection, rehabilitation tracking, and patient supervision require fast and efficient tracking of movements while avoiding unwanted false alarms. This study presents an efficient solution on how to build very efficient motion-based models only using triaxial accelerometer sensors. We explore the capability of the AutoML pipelines to extract the most important features from the data segments. This approach also involves training multiple lightweight machine learning algorithms using the extracted features. We use WeBe Band, a multi-sensor wearable device that is equipped with a powerful enough MCU to effectively perform gesture recognition entirely on the device. Of the models explored, we found that the neural network provided the best balance between accuracy, latency, and memory use. Our results also demonstrate that reliable real-time gesture recognition can be achieved in WeBe Band, with great potential for real-time medical monitoring solutions that require a secure and fast response time.</li>
</ul>

<h3>Title: Text Annotation via Inductive Coding: Comparing Human Experts to LLMs in Qualitative Data Analysis</h3>
<ul>
<li><strong>Authors: </strong>Angelina Parfenova, Andreas Marfurt, Alexander Denzler, Juergen Pfeffer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00046">https://arxiv.org/abs/2512.00046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00046">https://arxiv.org/pdf/2512.00046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00046]] Text Annotation via Inductive Coding: Comparing Human Experts to LLMs in Qualitative Data Analysis(https://arxiv.org/abs/2512.00046)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper investigates the automation of qualitative data analysis, focusing on inductive coding using large language models (LLMs). Unlike traditional approaches that rely on deductive methods with predefined labels, this research investigates the inductive process where labels emerge from the data. The study evaluates the performance of six open-source LLMs compared to human experts. As part of the evaluation, experts rated the perceived difficulty of the quotes they coded. The results reveal a peculiar dichotomy: human coders consistently perform well when labeling complex sentences but struggle with simpler ones, while LLMs exhibit the opposite trend. Additionally, the study explores systematic deviations in both human and LLM generated labels by comparing them to the golden standard from the test set. While human annotations may sometimes differ from the golden standard, they are often rated more favorably by other humans. In contrast, some LLMs demonstrate closer alignment with the true labels but receive lower evaluations from experts.</li>
</ul>

<h3>Title: Emergent Convergence in Multi-Agent LLM Annotation</h3>
<ul>
<li><strong>Authors: </strong>Angelina Parfenova, Alexander Denzler, Juergen Pfeffer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00047">https://arxiv.org/abs/2512.00047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00047">https://arxiv.org/pdf/2512.00047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00047]] Emergent Convergence in Multi-Agent LLM Annotation(https://arxiv.org/abs/2512.00047)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly deployed in collaborative settings, yet little is known about how they coordinate when treated as black-box agents. We simulate 7500 multi-agent, multi-round discussions in an inductive coding task, generating over 125000 utterances that capture both final annotations and their interactional histories. We introduce process-level metrics: code stability, semantic self-consistency, and lexical confidence alongside sentiment and convergence measures, to track coordination dynamics. To probe deeper alignment signals, we analyze the evolving geometry of output embeddings, showing that intrinsic dimensionality declines over rounds, suggesting semantic compression. The results reveal that LLM groups converge lexically and semantically, develop asymmetric influence patterns, and exhibit negotiation-like behaviors despite the absence of explicit role prompting. This work demonstrates how black-box interaction analysis can surface emergent coordination strategies, offering a scalable complement to internal probe-based interpretability methods.</li>
</ul>

<h3>Title: PEFT-DML: Parameter-Efficient Fine-Tuning Deep Metric Learning for Robust Multi-Modal 3D Object Detection in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Abdolazim Rezaei, Mehdi Sookhak</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00060">https://arxiv.org/abs/2512.00060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00060">https://arxiv.org/pdf/2512.00060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00060]] PEFT-DML: Parameter-Efficient Fine-Tuning Deep Metric Learning for Robust Multi-Modal 3D Object Detection in Autonomous Driving(https://arxiv.org/abs/2512.00060)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This study introduces PEFT-DML, a parameter-efficient deep metric learning framework for robust multi-modal 3D object detection in autonomous driving. Unlike conventional models that assume fixed sensor availability, PEFT-DML maps diverse modalities (LiDAR, radar, camera, IMU, GNSS) into a shared latent space, enabling reliable detection even under sensor dropout or unseen modality class combinations. By integrating Low-Rank Adaptation (LoRA) and adapter layers, PEFT-DML achieves significant training efficiency while enhancing robustness to fast motion, weather variability, and domain shifts. Experiments on benchmarks nuScenes demonstrate superior accuracy.</li>
</ul>

<h3>Title: Satellite to Street : Disaster Impact Estimator</h3>
<ul>
<li><strong>Authors: </strong>Sreesritha Sai, Sai Venkata Suma Sreeja, Deepthi, Nikhil</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00065">https://arxiv.org/abs/2512.00065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00065">https://arxiv.org/pdf/2512.00065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00065]] Satellite to Street : Disaster Impact Estimator(https://arxiv.org/abs/2512.00065)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Accurate post-disaster damage assessment is of high importance for prioritizing emergency response; however, manual interpretation of satellite imagery is slow, subjective, and hard to scale. While deep-learning models for image segmentation, such as U-Net-based baselines and change-detection models, are useful baselines, they often struggle with subtle structural variations and severe class imbalance, yielding poor detection of highly damaged regions. The present work proposes a deep-learning framework that jointly processes pre- and post-disaster satellite images to obtain fine-grained pixel-level damage maps: Satellite-to-Street: Disaster Impact Estimator. The model uses a modified dual-input U-Net architecture with enhanced feature fusion to capture both the local structural changes as well as the broader contextual cues. Class-aware weighted loss functions are integrated in order to handle the dominance of undamaged pixels in real disaster datasets, thus enhancing sensitivity toward major and destroyed categories. Experimentation on publicly available disaster datasets shows improved localization and classification of structural damage when compared to traditional segmentation and baseline change-detection models. The resulting damage maps provide a rapid and consistent assessment mechanism to support and not replace expert decision-making, thus allowing more efficient, data-driven disaster management.</li>
</ul>

<h3>Title: ProvRain: Rain-Adaptive Denoising and Vehicle Detection via MobileNet-UNet and Faster R-CNN</h3>
<ul>
<li><strong>Authors: </strong>Aswinkumar Varathakumaran, Nirmala Paramanandham</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00073">https://arxiv.org/abs/2512.00073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00073">https://arxiv.org/pdf/2512.00073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00073]] ProvRain: Rain-Adaptive Denoising and Vehicle Detection via MobileNet-UNet and Faster R-CNN(https://arxiv.org/abs/2512.00073)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Provident vehicle detection has a lot of scope in the detection of vehicle during night time. The extraction of features other than the headlamps of vehicles allows us to detect oncoming vehicles before they appear directly on the camera. However, it faces multiple issues especially in the field of night vision, where a lot of noise caused due to weather conditions such as rain or snow as well as camera conditions. This paper focuses on creating a pipeline aimed at dealing with such noise while at the same time maintaining the accuracy of provident vehicular detection. The pipeline in this paper, ProvRain, uses a lightweight MobileNet-U-Net architecture tuned to generalize to robust weather conditions by using the concept of curricula training. A mix of synthetic as well as available data from the PVDN dataset is used for this. This pipeline is compared to the base Faster RCNN architecture trained on the PVDN dataset to see how much the addition of a denoising architecture helps increase the detection model's performance in rainy conditions. The system boasts an 8.94\% increase in accuracy and a 10.25\% increase in recall in the detection of vehicles in rainy night time frames. Similarly, the custom MobileNet-U-Net architecture that was trained also shows a 10-15\% improvement in PSNR, a 5-6\% increase in SSIM, and upto a 67\% reduction in perceptual error (LPIPS) compared to other transformer approaches.</li>
</ul>

<h3>Title: Adapter Shield: A Unified Framework with Built-in Authentication for Preventing Unauthorized Zero-Shot Image-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Jun Jia, Hongyi Miao, Yingjie Zhou, Wangqiu Zhou, Jianbo Zhang, Linhan Cao, Dandan Zhu, Hua Yang, Xiongkuo Min, Wei Sun, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00075">https://arxiv.org/abs/2512.00075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00075">https://arxiv.org/pdf/2512.00075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00075]] Adapter Shield: A Unified Framework with Built-in Authentication for Preventing Unauthorized Zero-Shot Image-to-Image Generation(https://arxiv.org/abs/2512.00075)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, protect, defense, diffusion</a></li>
<li><strong>Abstract: </strong>With the rapid progress in diffusion models, image synthesis has advanced to the stage of zero-shot image-to-image generation, where high-fidelity replication of facial identities or artistic styles can be achieved using just one portrait or artwork, without modifying any model weights. Although these techniques significantly enhance creative possibilities, they also pose substantial risks related to intellectual property violations, including unauthorized identity cloning and stylistic imitation. To counter such threats, this work presents Adapter Shield, the first universal and authentication-integrated solution aimed at defending personal images from misuse in zero-shot generation scenarios. We first investigate how current zero-shot methods employ image encoders to extract embeddings from input images, which are subsequently fed into the UNet of diffusion models through cross-attention layers. Inspired by this mechanism, we construct a reversible encryption system that maps original embeddings into distinct encrypted representations according to different secret keys. The authorized users can restore the authentic embeddings via a decryption module and the correct key, enabling normal usage for authorized generation tasks. For protection purposes, we design a multi-target adversarial perturbation method that actively shifts the original embeddings toward designated encrypted patterns. Consequently, protected images are embedded with a defensive layer that ensures unauthorized users can only produce distorted or encrypted outputs. Extensive evaluations demonstrate that our method surpasses existing state-of-the-art defenses in blocking unauthorized zero-shot image synthesis, while supporting flexible and secure access control for verified users.</li>
</ul>

<h3>Title: Diffusion-Based Synthetic Brightfield Microscopy Images for Enhanced Single Cell Detection</h3>
<ul>
<li><strong>Authors: </strong>Mario de Jesus da Graca, Jörg Dahlkemper, Peer Stelldinger</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00078">https://arxiv.org/abs/2512.00078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00078">https://arxiv.org/pdf/2512.00078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00078]] Diffusion-Based Synthetic Brightfield Microscopy Images for Enhanced Single Cell Detection(https://arxiv.org/abs/2512.00078)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Accurate single cell detection in brightfield microscopy is crucial for biological research, yet data scarcity and annotation bottlenecks limit the progress of deep learning methods. We investigate the use of unconditional models to generate synthetic brightfield microscopy images and evaluate their impact on object detection performance. A U-Net based diffusion model was trained and used to create datasets with varying ratios of synthetic and real images. Experiments with YOLOv8, YOLOv9 and RT-DETR reveal that training with synthetic data can achieve improved detection accuracies (at minimal costs). A human expert survey demonstrates the high realism of generated images, with experts not capable to distinguish them from real microscopy images (accuracy 50%). Our findings suggest that diffusion-based synthetic data generation is a promising avenue for augmenting real datasets in microscopy image analysis, reducing the reliance on extensive manual annotation and potentially improving the robustness of cell detection models.</li>
</ul>

<h3>Title: Conceptual Evaluation of Deep Visual Stereo Odometry for the MARWIN Radiation Monitoring Robot in Accelerator Tunnels</h3>
<ul>
<li><strong>Authors: </strong>André Dehne, Juri Zach, Peer Stelldinger</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00080">https://arxiv.org/abs/2512.00080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00080">https://arxiv.org/pdf/2512.00080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00080]] Conceptual Evaluation of Deep Visual Stereo Odometry for the MARWIN Radiation Monitoring Robot in Accelerator Tunnels(https://arxiv.org/abs/2512.00080)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The MARWIN robot operates at the European XFEL to perform autonomous radiation monitoring in long, monotonous accelerator tunnels where conventional localization approaches struggle. Its current navigation concept combines lidar-based edge detection, wheel/lidar odometry with periodic QR-code referencing, and fuzzy control of wall distance, rotation, and longitudinal position. While robust in predefined sections, this design lacks flexibility for unknown geometries and obstacles. This paper explores deep visual stereo odometry (DVSO) with 3D-geometric constraints as a focused alternative. DVSO is purely vision-based, leveraging stereo disparity, optical flow, and self-supervised learning to jointly estimate depth and ego-motion without labeled data. For global consistency, DVSO can subsequently be fused with absolute references (e.g., landmarks) or other sensors. We provide a conceptual evaluation for accelerator tunnel environments, using the European XFEL as a case study. Expected benefits include reduced scale drift via stereo, low-cost sensing, and scalable data collection, while challenges remain in low-texture surfaces, lighting variability, computational load, and robustness under radiation. The paper defines a research agenda toward enabling MARWIN to navigate more autonomously in constrained, safety-critical infrastructures.</li>
</ul>

<h3>Title: Exploring Diagnostic Prompting Approach for Multimodal LLM-based Visual Complexity Assessment: A Case Study of Amazon Search Result Pages</h3>
<ul>
<li><strong>Authors: </strong>Divendar Murtadak, Yoon Kim, Trilokya Akula</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00082">https://arxiv.org/abs/2512.00082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00082">https://arxiv.org/pdf/2512.00082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00082]] Exploring Diagnostic Prompting Approach for Multimodal LLM-based Visual Complexity Assessment: A Case Study of Amazon Search Result Pages(https://arxiv.org/abs/2512.00082)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study investigates whether diagnostic prompting can improve Multimodal Large Language Model (MLLM) reliability for visual complexity assessment of Amazon Search Results Pages (SRP). We compare diagnostic prompting with standard gestalt principles-based prompting using 200 Amazon SRP pages and human expert annotations. Diagnostic prompting showed notable improvements in predicting human complexity judgments, with F1-score increasing from 0.031 to 0.297 (+858\% relative improvement), though absolute performance remains modest (Cohen's $\kappa$ = 0.071). The decision tree revealed that models prioritize visual design elements (badge clutter: 38.6\% importance) while humans emphasize content similarity, suggesting partial alignment in reasoning patterns. Failure case analysis reveals persistent challenges in MLLM visual perception, particularly for product similarity and color intensity assessment. Our findings indicate that diagnostic prompting represents a promising initial step toward human-aligned MLLM-based evaluation, though failure cases with consistent human-MLLM disagreement require continued research and refinement in prompting approaches with larger ground truth datasets for reliable practical deployment.</li>
</ul>

<h3>Title: A Fast and Efficient Modern BERT based Text-Conditioned Diffusion Model for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Venkata Siddharth Dhara, Pawan Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00084">https://arxiv.org/abs/2512.00084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00084">https://arxiv.org/pdf/2512.00084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00084]] A Fast and Efficient Modern BERT based Text-Conditioned Diffusion Model for Medical Image Segmentation(https://arxiv.org/abs/2512.00084)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>In recent times, denoising diffusion probabilistic models (DPMs) have proven effective for medical image generation and denoising, and as representation learners for downstream segmentation. However, segmentation performance is limited by the need for dense pixel-wise labels, which are expensive, time-consuming, and require expert knowledge. We propose FastTextDiff, a label-efficient diffusion-based segmentation model that integrates medical text annotations to enhance semantic representations. Our approach uses ModernBERT, a transformer capable of processing long clinical notes, to tightly link textual annotations with semantic content in medical images. Trained on MIMIC-III and MIMIC-IV, ModernBERT encodes clinical knowledge that guides cross-modal attention between visual and textual features. This study validates ModernBERT as a fast, scalable alternative to Clinical BioBERT in diffusion-based segmentation pipelines and highlights the promise of multi-modal techniques for medical image analysis. By replacing Clinical BioBERT with ModernBERT, FastTextDiff benefits from FlashAttention 2, an alternating attention mechanism, and a 2-trillion-token corpus, improving both segmentation accuracy and training efficiency over traditional diffusion-based models.</li>
</ul>

<h3>Title: Exploring Automated Recognition of Instructional Activity and Discourse from Multimodal Classroom Data</h3>
<ul>
<li><strong>Authors: </strong>Ivo Bueno, Ruikun Hou, Babette Bühler, Tim Fütterer, James Drimalla, Jonathan Kyle Foster, Peter Youngs, Peter Gerjets, Ulrich Trautwein, Enkelejda Kasneci</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00087">https://arxiv.org/abs/2512.00087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00087">https://arxiv.org/pdf/2512.00087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00087]] Exploring Automated Recognition of Instructional Activity and Discourse from Multimodal Classroom Data(https://arxiv.org/abs/2512.00087)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Observation of classroom interactions can provide concrete feedback to teachers, but current methods rely on manual annotation, which is resource-intensive and hard to scale. This work explores AI-driven analysis of classroom recordings, focusing on multimodal instructional activity and discourse recognition as a foundation for actionable feedback. Using a densely annotated dataset of 164 hours of video and 68 lesson transcripts, we design parallel, modality-specific pipelines. For video, we evaluate zero-shot multimodal LLMs, fine-tuned vision-language models, and self-supervised video transformers on 24 activity labels. For transcripts, we fine-tune a transformer-based classifier with contextualized inputs and compare it against prompting-based LLMs on 19 discourse labels. To handle class imbalance and multi-label complexity, we apply per-label thresholding, context windows, and imbalance-aware loss functions. The results show that fine-tuned models consistently outperform prompting-based approaches, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts. These results demonstrate the feasibility of automated classroom analysis and establish a foundation for scalable teacher feedback systems.</li>
</ul>

<h3>Title: SemImage: Semantic Image Representation for Text, a Novel Framework for Embedding Disentangled Linguistic Features</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Zare</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00088">https://arxiv.org/abs/2512.00088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00088">https://arxiv.org/pdf/2512.00088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00088]] SemImage: Semantic Image Representation for Text, a Novel Framework for Embedding Disentangled Linguistic Features(https://arxiv.org/abs/2512.00088)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We propose SemImage, a novel method for representing a text document as a two-dimensional semantic image to be processed by convolutional neural networks (CNNs). In a SemImage, each word is represented as a pixel in a 2D image: rows correspond to sentences and an additional boundary row is inserted between sentences to mark semantic transitions. Each pixel is not a typical RGB value but a vector in a disentangled HSV color space, encoding different linguistic features: the Hue with two components H_cos and H_sin to account for circularity encodes the topic, Saturation encodes the sentiment, and Value encodes intensity or certainty. We enforce this disentanglement via a multi-task learning framework: a ColorMapper network maps each word embedding to the HSV space, and auxiliary supervision is applied to the Hue and Saturation channels to predict topic and sentiment labels, alongside the main task objective. The insertion of dynamically computed boundary rows between sentences yields sharp visual boundaries in the image when consecutive sentences are semantically dissimilar, effectively making paragraph breaks salient. We integrate SemImage with standard 2D CNNs (e.g., ResNet) for document classification. Experiments on multi-label datasets (with both topic and sentiment annotations) and single-label benchmarks demonstrate that SemImage can achieve competitive or better accuracy than strong text classification baselines (including BERT and hierarchical attention networks) while offering enhanced interpretability. An ablation study confirms the importance of the multi-channel HSV representation and the dynamic boundary rows. Finally, we present visualizations of SemImage that qualitatively reveal clear patterns corresponding to topic shifts and sentiment changes in the generated image, suggesting that our representation makes these linguistic features visible to both humans and machines.</li>
</ul>

<h3>Title: TeleViT1.0: Teleconnection-aware Vision Transformers for Subseasonal to Seasonal Wildfire Pattern Forecasts</h3>
<ul>
<li><strong>Authors: </strong>Ioannis Prapas, Nikolaos Papadopoulos, Nikolaos-Ioannis Bountos, Dimitrios Michail, Gustau Camps-Valls, Ioannis Papoutsis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00089">https://arxiv.org/abs/2512.00089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00089">https://arxiv.org/pdf/2512.00089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00089]] TeleViT1.0: Teleconnection-aware Vision Transformers for Subseasonal to Seasonal Wildfire Pattern Forecasts(https://arxiv.org/abs/2512.00089)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Forecasting wildfires weeks to months in advance is difficult, yet crucial for planning fuel treatments and allocating resources. While short-term predictions typically rely on local weather conditions, long-term forecasting requires accounting for the Earth's interconnectedness, including global patterns and teleconnections. We introduce TeleViT, a Teleconnection-aware Vision Transformer that integrates (i) fine-scale local fire drivers, (ii) coarsened global fields, and (iii) teleconnection indices. This multi-scale fusion is achieved through an asymmetric tokenization strategy that produces heterogeneous tokens processed jointly by a transformer encoder, followed by a decoder that preserves spatial structure by mapping local tokens to their corresponding prediction patches. Using the global SeasFire dataset (2001-2021, 8-day resolution), TeleViT improves AUPRC performance over U-Net++, ViT, and climatology across all lead times, including horizons up to four months. At zero lead, TeleViT with indices and global inputs reaches AUPRC 0.630 (ViT 0.617, U-Net 0.620), at 16x8day lead (around 4 months), TeleViT variants using global input maintain 0.601-0.603 (ViT 0.582, U-Net 0.578), while surpassing the climatology (0.572) at all lead times. Regional results show the highest skill in seasonally consistent fire regimes, such as African savannas, and lower skill in boreal and arid regions. Attention and attribution analyses indicate that predictions rely mainly on local tokens, with global fields and indices contributing coarse contextual information. These findings suggest that architectures explicitly encoding large-scale Earth-system context can extend wildfire predictability on subseasonal-to-seasonal timescales.</li>
</ul>

<h3>Title: Deep Filament Extraction for 3D Concrete Printing</h3>
<ul>
<li><strong>Authors: </strong>Karam Mawas, Mehdi Maboudi, Pedro Achanccaray, Markus Gerke</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00091">https://arxiv.org/abs/2512.00091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00091">https://arxiv.org/pdf/2512.00091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00091]] Deep Filament Extraction for 3D Concrete Printing(https://arxiv.org/abs/2512.00091)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The architecture, engineering and construction (AEC) industry is constantly evolving to meet the demand for sustainable and effective design and construction of the built environment. In the literature, two primary deposition techniques for large-scale 3D concrete printing (3DCP) have been described, namely extrusion-based (Contour Crafting-CC) and shotcrete 3D printing (SC3DP) methods. The deposition methods use a digitally controlled nozzle to print material layer by layer. The continuous flow of concrete material used to create the printed structure is called a filament or layer. As these filaments are the essential structure defining the printed object, the filaments' geometry quality control is crucial. This paper presents an automated procedure for quality control (QC) of filaments in extrusion-based and SC3DP printing methods. The paper also describes a workflow that is independent of the sensor used for data acquisition, such as a camera, a structured light system (SLS) or a terrestrial laser scanner (TLS). This method can be used with materials in either the fresh or cured state. Thus, it can be used for online and post-printing QC.</li>
</ul>

<h3>Title: HMARK: Radioactive Multi-Bit Semantic-Latent Watermarking for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kexin Li, Guozhen Ding, Ilya Grishchenko, David Lie</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00094">https://arxiv.org/abs/2512.00094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00094">https://arxiv.org/pdf/2512.00094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00094]] HMARK: Radioactive Multi-Bit Semantic-Latent Watermarking for Diffusion Models(https://arxiv.org/abs/2512.00094)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, interpretability, watermark, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Modern generative diffusion models rely on vast training datasets, often including images with uncertain ownership or usage rights. Radioactive watermarks -- marks that transfer to a model's outputs -- can help detect when such unauthorized data has been used for training. Moreover, aside from being radioactive, an effective watermark for protecting images from unauthorized training also needs to meet other existing requirements, such as imperceptibility, robustness, and multi-bit capacity. To overcome these challenges, we propose HMARK, a novel multi-bit watermarking scheme, which encodes ownership information as secret bits in the semantic-latent space (h-space) for image diffusion models. By leveraging the interpretability and semantic significance of h-space, ensuring that watermark signals correspond to meaningful semantic attributes, the watermarks embedded by HMARK exhibit radioactivity, robustness to distortions, and minimal impact on perceptual quality. Experimental results demonstrate that HMARK achieves 98.57% watermark detection accuracy, 95.07% bit-level recovery accuracy, 100% recall rate, and 1.0 AUC on images produced by the downstream adversarial model finetuned with LoRA on watermarked data across various types of distortions.</li>
</ul>

<h3>Title: Guarding Against Malicious Biased Threats (GAMBiT): Experimental Design of Cognitive Sensors and Triggers with Behavioral Impact Analysis</h3>
<ul>
<li><strong>Authors: </strong>Brandon Beltz, Po-Yu Chen, James Doty, Yvonne Fonken, Nikolos Gurney, Hsiang-Wen Hsing, Sofia Hirschmann, Brett Israelsen, Nathan Lau, Mengyun Li, Stacy Marsella, Michael Murray, Jinwoo Oh, Amy Sliva, Kunal Srivastava, Stoney Trent, Peggy Wu, Ya-Ting Yang, Quanyan Zhu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00098">https://arxiv.org/abs/2512.00098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00098">https://arxiv.org/pdf/2512.00098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00098]] Guarding Against Malicious Biased Threats (GAMBiT): Experimental Design of Cognitive Sensors and Triggers with Behavioral Impact Analysis(https://arxiv.org/abs/2512.00098)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>This paper introduces GAMBiT (Guarding Against Malicious Biased Threats), a cognitive-informed cyber defense framework that leverages deviations from human rationality as a new defensive surface. Conventional cyber defenses assume rational, utility-maximizing attackers, yet real-world adversaries exhibit cognitive constraints and biases that shape their interactions with complex digital systems. GAMBiT embeds insights from cognitive science into cyber environments through cognitive triggers, which activate biases such as loss aversion, base-rate neglect, and sunk-cost fallacy, and through newly developed cognitive sensors that infer attackers' cognitive states from behavioral and network data. Three rounds of human-subject experiments (total n=61) in a simulated small business network demonstrate that these manipulations significantly disrupt attacker performance, reducing mission progress, diverting actions off the true attack path, and increasing detectability. These results demonstrate that cognitive biases can be systematically triggered to degrade the attacker's efficiency and enhance the defender's advantage. GAMBiT establishes a new paradigm in which the attacker's mind becomes part of the battlefield and cognitive manipulation becomes a proactive vector for cyber defense.</li>
</ul>

<h3>Title: Comparative Analysis of Vision Transformer, Convolutional, and Hybrid Architectures for Mental Health Classification Using Actigraphy-Derived Images</h3>
<ul>
<li><strong>Authors: </strong>Ifeanyi Okala</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00103">https://arxiv.org/abs/2512.00103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00103">https://arxiv.org/pdf/2512.00103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00103]] Comparative Analysis of Vision Transformer, Convolutional, and Hybrid Architectures for Mental Health Classification Using Actigraphy-Derived Images(https://arxiv.org/abs/2512.00103)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This work examines how three different image-based methods, VGG16, ViT-B/16, and CoAtNet-Tiny, perform in identifying depression, schizophrenia, and healthy controls using daily actigraphy records. Wrist-worn activity signals from the Psykose and Depresjon datasets were converted into 30 by 48 images and evaluated through a three-fold subject-wise split. Although all methods fitted the training data well, their behaviour on unseen data differed. VGG16 improved steadily but often settled at lower accuracy. ViT-B/16 reached strong results in some runs, but its performance shifted noticeably from fold to fold. CoAtNet-Tiny stood out as the most reliable, recording the highest average accuracy and the most stable curves across folds. It also produced the strongest precision, recall, and F1-scores, particularly for the underrepresented depression and schizophrenia classes. Overall, the findings indicate that CoAtNet-Tiny performed most consistently on the actigraphy images, while VGG16 and ViT-B/16 yielded mixed results. These observations suggest that certain hybrid designs may be especially suited for mental-health work that relies on actigraphy-derived images.</li>
</ul>

<h3>Title: Quantum-Adversary-Resilient Evidence Structures and Migration Strategies for Regulated AI Audit Trails</h3>
<ul>
<li><strong>Authors: </strong>Leo Kao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00110">https://arxiv.org/abs/2512.00110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00110">https://arxiv.org/pdf/2512.00110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00110]] Quantum-Adversary-Resilient Evidence Structures and Migration Strategies for Regulated AI Audit Trails(https://arxiv.org/abs/2512.00110)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Constant-size cryptographic evidence records are increasingly used to build audit trails for regulated AI workloads in clinical, pharmaceutical, and financial settings, where each execution is summarized by a compact, verifiable record of code identity, model version, data digests, and platform measurements. Existing instantiations, however, typically rely on classical signature schemes whose long-term security is threatened by quantum-capable adversaries. In this paper we formalize security notions for evidence structures in the presence of quantum adversaries and study post-quantum (PQ) instantiations and migration strategies for deployed audit logs. We recall an abstraction of constant-size evidence structures and introduce game-based definitions of Q-Audit Integrity, Q-Non-Equivocation, and Q-Binding, capturing the inability of a quantum adversary to forge, equivocate, or rebind evidence items. We then analyze a hash-and-sign instantiation in the quantum random-oracle model (QROM), assuming an existentially unforgeable PQ signature scheme against quantum adversaries, and show that the resulting evidence structure satisfies these notions under standard assumptions. Building on this, we present three migration patterns for existing evidence logs: hybrid signatures, re-signing of legacy evidence, and Merkle-root anchoring, and analyze their security, storage, and computational trade-offs. A case study based on an industrial constant-size evidence platform for regulated AI at Codebat Technologies Inc. suggests that quantum-safe audit trails are achievable with moderate overhead and that systematic migration can significantly extend the evidentiary lifetime of existing deployments.</li>
</ul>

<h3>Title: TinyViT: Field Deployable Transformer Pipeline for Solar Panel Surface Fault and Severity Screening</h3>
<ul>
<li><strong>Authors: </strong>Ishwaryah Pandiarajan, Mohamed Mansoor Roomi Sindha, Uma Maheswari Pandyan, Sharafia N</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00117">https://arxiv.org/abs/2512.00117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00117">https://arxiv.org/pdf/2512.00117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00117]] TinyViT: Field Deployable Transformer Pipeline for Solar Panel Surface Fault and Severity Screening(https://arxiv.org/abs/2512.00117)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Sustained operation of solar photovoltaic assets hinges on accurate detection and prioritization of surface faults across vast, geographically distributed modules. While multi modal imaging strategies are popular, they introduce logistical and economic barriers for routine farm level deployment. This work demonstrates that deep learning and classical machine learning may be judiciously combined to achieve robust surface anomaly categorization and severity estimation from planar visible band imagery alone. We introduce TinyViT which is a compact pipeline integrating Transformer based segmentation, spectral-spatial feature engineering, and ensemble regression. The system ingests consumer grade color camera mosaics of PV panels, classifies seven nuanced surface faults, and generates actionable severity grades for maintenance triage. By eliminating reliance on electroluminescence or IR sensors, our method enables affordable, scalable upkeep for resource limited installations, and advances the state of solar health monitoring toward universal field accessibility. Experiments on real public world datasets validate both classification and regression sub modules, achieving accuracy and interpretability competitive with specialized approaches.</li>
</ul>

<h3>Title: NetDeTox: Adversarial and Efficient Evasion of Hardware-Security GNNs via RL-LLM Orchestration</h3>
<ul>
<li><strong>Authors: </strong>Zeng Wang, Minghao Shao, Akashdeep Saha, Ramesh Karri, Johann Knechtel, Muhammad Shafique, Ozgur Sinanoglu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00119">https://arxiv.org/abs/2512.00119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00119">https://arxiv.org/pdf/2512.00119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00119]] NetDeTox: Adversarial and Efficient Evasion of Hardware-Security GNNs via RL-LLM Orchestration(https://arxiv.org/abs/2512.00119)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Graph neural networks (GNNs) have shown promise in hardware security by learning structural motifs from netlist graphs. However, this reliance on motifs makes GNNs vulnerable to adversarial netlist rewrites; even small-scale edits can mislead GNN predictions. Existing adversarial approaches, ranging from synthesis-recipe perturbations to gate transformations, come with high design overheads. We present NetDeTox, an automated end-to-end framework that orchestrates large language models (LLMs) with reinforcement learning (RL) in a systematic manner, enabling focused local rewriting. The RL agent identifies netlist components critical for GNN-based reasoning, while the LLM devises rewriting plans to diversify motifs that preserve functionality. Iterative feedback between the RL and LLM stages refines adversarial rewritings to limit overheads. Compared to the SOTA work AttackGNN, NetDeTox successfully degrades the effectiveness of all security schemes with fewer rewrites and substantially lower area overheads (reductions of 54.50% for GNN-RE, 25.44% for GNN4IP, and 41.04% for OMLA, respectively). For GNN4IP, ours can even optimize/reduce the original benchmarks' area, in particular for larger circuits, demonstrating the practicality and scalability of NetDeTox.</li>
</ul>

<h3>Title: Hybrid Synthetic Data Generation with Domain Randomization Enables Zero-Shot Vision-Based Part Inspection Under Extreme Class Imbalance</h3>
<ul>
<li><strong>Authors: </strong>Ruo-Syuan Mei, Sixian Jia, Guangze Li, Soo Yeon Lee, Brian Musser, William Keller, Sreten Zakula, Jorge Arinez, Chenhui Shao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00125">https://arxiv.org/abs/2512.00125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00125">https://arxiv.org/pdf/2512.00125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00125]] Hybrid Synthetic Data Generation with Domain Randomization Enables Zero-Shot Vision-Based Part Inspection Under Extreme Class Imbalance(https://arxiv.org/abs/2512.00125)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Machine learning, particularly deep learning, is transforming industrial quality inspection. Yet, training robust machine learning models typically requires large volumes of high-quality labeled data, which are expensive, time-consuming, and labor-intensive to obtain in manufacturing. Moreover, defective samples are intrinsically rare, leading to severe class imbalance that degrades model performance. These data constraints hinder the widespread adoption of machine learning-based quality inspection methods in real production environments. Synthetic data generation (SDG) offers a promising solution by enabling the creation of large, balanced, and fully annotated datasets in an efficient, cost-effective, and scalable manner. This paper presents a hybrid SDG framework that integrates simulation-based rendering, domain randomization, and real background compositing to enable zero-shot learning for computer vision-based industrial part inspection without manual annotation. The SDG pipeline generates 12,960 labeled images in one hour by varying part geometry, lighting, and surface properties, and then compositing synthetic parts onto real image backgrounds. A two-stage architecture utilizing a YOLOv8n backbone for object detection and MobileNetV3-small for quality classification is trained exclusively on synthetic data and evaluated on 300 real industrial parts. The proposed approach achieves an mAP@0.5 of 0.995 for detection, 96% classification accuracy, and 90.1% balanced accuracy. Comparative evaluation against few-shot real-data baseline approaches demonstrates significant improvement. The proposed SDG-based approach achieves 90-91% balanced accuracy under severe class imbalance, while the baselines reach only 50% accuracy. These results demonstrate that the proposed method enables annotation-free, scalable, and robust quality inspection for real-world manufacturing applications.</li>
</ul>

<h3>Title: Analysis of Incursive Breast Cancer in Mammograms Using YOLO, Explainability, and Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Jayan Adhikari, Prativa Joshi, Susish Baral</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00129">https://arxiv.org/abs/2512.00129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00129">https://arxiv.org/pdf/2512.00129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00129]] Analysis of Incursive Breast Cancer in Mammograms Using YOLO, Explainability, and Domain Adaptation(https://arxiv.org/abs/2512.00129)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Deep learning models for breast cancer detection from mammographic images have significant reliability problems when presented with Out-of-Distribution (OOD) inputs such as other imaging modalities (CT, MRI, X-ray) or equipment variations, leading to unreliable detection and misdiagnosis. The current research mitigates the fundamental OOD issue through a comprehensive approach integrating ResNet50-based OOD filtering with YOLO architectures (YOLOv8, YOLOv11, YOLOv12) for accurate detection of breast cancer. Our strategy establishes an in-domain gallery via cosine similarity to rigidly reject non-mammographic inputs prior to processing, ensuring that only domain-associated images supply the detection pipeline. The OOD detection component achieves 99.77\% general accuracy with immaculate 100\% accuracy on OOD test sets, effectively eliminating irrelevant imaging modalities. ResNet50 was selected as the optimum backbone after 12 CNN architecture searches. The joint framework unites OOD robustness with high detection performance (mAP@0.5: 0.947) and enhanced interpretability through Grad-CAM visualizations. Experimental validation establishes that OOD filtering significantly improves system reliability by preventing false alarms on out-of-distribution inputs while maintaining higher detection accuracy on mammographic data. The present study offers a fundamental foundation for the deployment of reliable AI-based breast cancer detection systems in diverse clinical environments with inherent data heterogeneity.</li>
</ul>

<h3>Title: Local and Global Context-and-Object-part-Aware Superpixel-based Data Augmentation for Deep Visual Recognition</h3>
<ul>
<li><strong>Authors: </strong>Fadi Dornaika, Danyang Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00130">https://arxiv.org/abs/2512.00130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00130">https://arxiv.org/pdf/2512.00130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00130]] Local and Global Context-and-Object-part-Aware Superpixel-based Data Augmentation for Deep Visual Recognition(https://arxiv.org/abs/2512.00130)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Cutmix-based data augmentation, which uses a cut-and-paste strategy, has shown remarkable generalization capabilities in deep learning. However, existing methods primarily consider global semantics with image-level constraints, which excessively reduces attention to the discriminative local context of the class and leads to a performance improvement bottleneck. Moreover, existing methods for generating augmented samples usually involve cutting and pasting rectangular or square regions, resulting in a loss of object part information. To mitigate the problem of inconsistency between the augmented image and the generated mixed label, existing methods usually require double forward propagation or rely on an external pre-trained network for object centering, which is inefficient. To overcome the above limitations, we propose LGCOAMix, an efficient context-aware and object-part-aware superpixel-based grid blending method for data augmentation. To the best of our knowledge, this is the first time that a label mixing strategy using a superpixel attention approach has been proposed for cutmix-based data augmentation. It is the first instance of learning local features from discriminative superpixel-wise regions and cross-image superpixel contrasts. Extensive experiments on various benchmark datasets show that LGCOAMix outperforms state-of-the-art cutmix-based data augmentation methods on classification tasks, {and weakly supervised object location on CUB200-2011.} We have demonstrated the effectiveness of LGCOAMix not only for CNN networks, but also for Transformer networks. Source codes are available at this https URL.</li>
</ul>

<h3>Title: An Empirical Study on the Security Vulnerabilities of GPTs</h3>
<ul>
<li><strong>Authors: </strong>Tong Wu, Weibin Wu, Zibin Zheng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00136">https://arxiv.org/abs/2512.00136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00136">https://arxiv.org/pdf/2512.00136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00136]] An Empirical Study on the Security Vulnerabilities of GPTs(https://arxiv.org/abs/2512.00136)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Equipped with various tools and knowledge, GPTs, one kind of customized AI agents based on OpenAI's large language models, have illustrated great potential in many fields, such as writing, research, and programming. Today, the number of GPTs has reached three millions, with the range of specific expert domains becoming increasingly diverse. However, given the consistent framework shared among these LLM agent applications, systemic security vulnerabilities may exist and remain underexplored. To fill this gap, we present an empirical study on the security vulnerabilities of GPTs. Building upon prior research on LLM security, we first adopt a platform-user perspective to conduct a comprehensive attack surface analysis across different system components. Then, we design a systematic and multidimensional attack suite with the explicit objectives of information leakage and tool misuse based on the attack surface analysis, thereby concretely demonstrating the security vulnerabilities that various components of GPT-based systems face. Finally, we accordingly propose defense mechanisms to address the aforementioned security vulnerabilities. By increasing the awareness of these vulnerabilities and offering critical insights into their implications, this study seeks to facilitate the secure and responsible application of GPTs while contributing to developing robust defense mechanisms that protect users and systems against malicious attacks.</li>
</ul>

<h3>Title: DeFi TrustBoost: Blockchain and AI for Trustworthy Decentralized Financial Decisions</h3>
<ul>
<li><strong>Authors: </strong>Swati Sachan, Dale S. Fickett</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, q-fin.CP, q-fin.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00142">https://arxiv.org/abs/2512.00142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00142">https://arxiv.org/pdf/2512.00142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00142]] DeFi TrustBoost: Blockchain and AI for Trustworthy Decentralized Financial Decisions(https://arxiv.org/abs/2512.00142)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack</a></li>
<li><strong>Abstract: </strong>This research introduces the Decentralized Finance (DeFi) TrustBoost Framework, which combines blockchain technology and Explainable AI to address challenges faced by lenders underwriting small business loan applications from low-wealth households. The framework is designed with a strong emphasis on fulfilling four crucial requirements of blockchain and AI systems: confidentiality, compliance with data protection laws, resistance to adversarial attacks, and compliance with regulatory audits. It presents a technique for tamper-proof auditing of automated AI decisions and a strategy for on-chain (inside-blockchain) and off-chain data storage to facilitate collaboration within and across financial organizations.</li>
</ul>

<h3>Title: Measuring What LLMs Think They Do: SHAP Faithfulness and Deployability on Financial Tabular Classification</h3>
<ul>
<li><strong>Authors: </strong>Saeed AlMarri, Mathieu Ravaut, Kristof Juhasz, Gautier Marti, Hamdan Al Ahbabi, Ibrahim Elfadel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00163">https://arxiv.org/abs/2512.00163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00163">https://arxiv.org/pdf/2512.00163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00163]] Measuring What LLMs Think They Do: SHAP Faithfulness and Deployability on Financial Tabular Classification(https://arxiv.org/abs/2512.00163)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have attracted significant attention for classification tasks, offering a flexible alternative to trusted classical machine learning models like LightGBM through zero-shot prompting. However, their reliability for structured tabular data remains unclear, particularly in high stakes applications like financial risk assessment. Our study systematically evaluates LLMs and generates their SHAP values on financial classification tasks. Our analysis shows a divergence between LLMs self-explanation of feature impact and their SHAP values, as well as notable differences between LLMs and LightGBM SHAP values. These findings highlight the limitations of LLMs as standalone classifiers for structured financial modeling, but also instill optimism that improved explainability mechanisms coupled with few-shot prompting will make LLMs usable in risk-sensitive domains.</li>
</ul>

<h3>Title: Faster Verified Explanations for Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Alessandro De Palma, Greta Dolcetti, Caterina Urban</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00164">https://arxiv.org/abs/2512.00164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00164">https://arxiv.org/pdf/2512.00164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00164]] Faster Verified Explanations for Neural Networks(https://arxiv.org/abs/2512.00164)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Verified explanations are a theoretically-principled way to explain the decisions taken by neural networks, which are otherwise black-box in nature. However, these techniques face significant scalability challenges, as they require multiple calls to neural network verifiers, each of them with an exponential worst-case complexity. We present FaVeX, a novel algorithm to compute verified explanations. FaVeX accelerates the computation by dynamically combining batch and sequential processing of input features, and by reusing information from previous queries, both when proving invariances with respect to certain input features, and when searching for feature assignments altering the prediction. Furthermore, we present a novel and hierarchical definition of verified explanations, termed verifier-optimal robust explanations, that explicitly factors the incompleteness of network verifiers within the explanation. Our comprehensive experimental evaluation demonstrates the superior scalability of both FaVeX, and of verifier-optimal robust explanations, which together can produce meaningful formal explanation on networks with hundreds of thousands of non-linear activations.</li>
</ul>

<h3>Title: Orion-Bix: Bi-Axial Attention for Tabular In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Bouadi, Pratinav Seth, Aditya Tanna, Vinay Kumar Sankarapu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00181">https://arxiv.org/abs/2512.00181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00181">https://arxiv.org/pdf/2512.00181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00181]] Orion-Bix: Bi-Axial Attention for Tabular In-Context Learning(https://arxiv.org/abs/2512.00181)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Tabular data drive most real-world machine learning applications, yet building general-purpose models for them remains difficult. Mixed numeric and categorical fields, weak feature structure, and limited labeled data make scaling and generalization challenging. To this end, we introduce Orion-Bix, a tabular foundation model that combines biaxial attention with meta-learned in-context reasoning for few-shot tabular learning. Its encoder alternates standard, grouped, hierarchical, and relational attention, fusing their outputs through multi-CLS summarization to capture both local and global dependencies efficiently. A label-aware ICL head adapts on the fly and scales to large label spaces via hierarchical decision routing. Meta-trained on synthetically generated, structurally diverse tables with causal priors, Orion-Bix learns transferable inductive biases across heterogeneous data. Delivered as a scikit-learn compatible foundation model, it outperforms gradient-boosting baselines and remains competitive with state-of-the-art tabular foundation models on public benchmarks, showing that biaxial attention with episodic meta-training enables robust, few-shot-ready tabular learning. The model is publicly available at this https URL .</li>
</ul>

<h3>Title: Hybrid Context-Fusion Attention (CFA) U-Net and Clustering for Robust Seismic Horizon Interpretation</h3>
<ul>
<li><strong>Authors: </strong>Jose Luis Lima de Jesus Silva, Joao Pedro Gomes, Paulo Roberto de Melo Barros Junior, Vitor Hugo Serravalle Reis Rodrigues, Alexsandro Guerra Cerqueira</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00191">https://arxiv.org/abs/2512.00191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00191">https://arxiv.org/pdf/2512.00191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00191]] Hybrid Context-Fusion Attention (CFA) U-Net and Clustering for Robust Seismic Horizon Interpretation(https://arxiv.org/abs/2512.00191)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Interpreting seismic horizons is a critical task for characterizing subsurface structures in hydrocarbon exploration. Recent advances in deep learning, particularly U-Net-based architectures, have significantly improved automated horizon tracking. However, challenges remain in accurately segmenting complex geological features and interpolating horizons from sparse annotations. To address these issues, a hybrid framework is presented that integrates advanced U-Net variants with spatial clustering to enhance horizon continuity and geometric fidelity. The core contribution is the Context Fusion Attention (CFA) U-Net, a novel architecture that fuses spatial and Sobel-derived geometric features within attention gates to improve both precision and surface completeness. The performance of five architectures, the U-Net (Standard and compressed), U-Net++, Attention U-Net, and CFA U-Net, was systematically evaluated across various data sparsity regimes (10-, 20-, and 40-line spacing). This approach outperformed existing baselines, achieving state-of-the-art results on the Mexilhao field (Santos Basin, Brazil) dataset with a validation IoU of 0.881 and MAE of 2.49ms, and excellent surface coverage of 97.6% on the F3 Block of the North Sea dataset under sparse conditions. The framework further refines merged horizon predictions (inline and cross-line) using Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to produce geologically plausible surfaces. The results demonstrate the advantages of hybrid methodologies and attention-based architectures enhanced with geometric context, providing a robust and generalizable solution for seismic interpretation in structurally complex and data-scarce environments.</li>
</ul>

<h3>Title: AutocleanEEG ICVision: Automated ICA Artifact Classification Using Vision-Language AI</h3>
<ul>
<li><strong>Authors: </strong>Zag ElSayed, Grace Westerkamp, Gavin Gammoh, Yanchen Liu, Peyton Siekierski, Craig Erickson, Ernest Pedapati</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00194">https://arxiv.org/abs/2512.00194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00194">https://arxiv.org/pdf/2512.00194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00194]] AutocleanEEG ICVision: Automated ICA Artifact Classification Using Vision-Language AI(https://arxiv.org/abs/2512.00194)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce EEG Autoclean Vision Language AI (ICVision) a first-of-its-kind system that emulates expert-level EEG ICA component classification through AI-agent vision and natural language reasoning. Unlike conventional classifiers such as ICLabel, which rely on handcrafted features, ICVision directly interprets ICA dashboard visualizations topography, time series, power spectra, and ERP plots, using a multimodal large language model (GPT-4 Vision). This allows the AI to see and explain EEG components the way trained neurologists do, making it the first scientific implementation of AI-agent visual cognition in neurophysiology. ICVision classifies each component into one of six canonical categories (brain, eye, heart, muscle, channel noise, and other noise), returning both a confidence score and a human-like explanation. Evaluated on 3,168 ICA components from 124 EEG datasets, ICVision achieved k = 0.677 agreement with expert consensus, surpassing MNE ICLabel, while also preserving clinically relevant brain signals in ambiguous cases. Over 97% of its outputs were rated as interpretable and actionable by expert reviewers. As a core module of the open-source EEG Autoclean platform, ICVision signals a paradigm shift in scientific AI, where models do not just classify, but see, reason, and communicate. It opens the door to globally scalable, explainable, and reproducible EEG workflows, marking the emergence of AI agents capable of expert-level visual decision-making in brain science and beyond.</li>
</ul>

<h3>Title: Mammo-FM: Breast-specific foundational model for Integrated Mammographic Diagnosis, Prognosis, and Reporting</h3>
<ul>
<li><strong>Authors: </strong>Shantanu Ghosh, Vedant Parthesh Joshi, Rayan Syed, Aya Kassem, Abhishek Varshney, Payel Basak, Weicheng Dai, Judy Wawira Gichoya, Hari M. Trivedi, Imon Banerjee, Shyam Visweswaran, Clare B. Poynton, Kayhan Batmanghelich</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00198">https://arxiv.org/abs/2512.00198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00198">https://arxiv.org/pdf/2512.00198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00198]] Mammo-FM: Breast-specific foundational model for Integrated Mammographic Diagnosis, Prognosis, and Reporting(https://arxiv.org/abs/2512.00198)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Breast cancer is one of the leading causes of death among women worldwide. We introduce Mammo-FM, the first foundation model specifically for mammography, pretrained on the largest and most diverse dataset to date - 140,677 patients (821,326 mammograms) across four U.S. institutions. Mammo-FM provides a unified foundation for core clinical tasks in breast imaging, including cancer diagnosis, pathology localization, structured report generation, and cancer risk prognosis within a single framework. Its alignment between images and text enables both visual and textual interpretability, improving transparency and clinical auditability, which are essential for real-world adoption. We rigorously evaluate Mammo-FM across diagnosis, prognosis, and report-generation tasks in in- and out-of-distribution datasets. Despite operating on native-resolution mammograms and using only one-third of the parameters of state-of-the-art generalist FMs, Mammo-FM consistently outperforms them across multiple public and private benchmarks. These results highlight the efficiency and value of domain-specific foundation models designed around the full spectrum of tasks within a clinical domain and emphasize the importance of rigorous, domain-aligned evaluation.</li>
</ul>

<h3>Title: Tree Matching Networks for Natural Language Inference: Parameter-Efficient Semantic Understanding via Dependency Parse Trees</h3>
<ul>
<li><strong>Authors: </strong>Jason Lunder</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00204">https://arxiv.org/abs/2512.00204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00204">https://arxiv.org/pdf/2512.00204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00204]] Tree Matching Networks for Natural Language Inference: Parameter-Efficient Semantic Understanding via Dependency Parse Trees(https://arxiv.org/abs/2512.00204)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In creating sentence embeddings for Natural Language Inference (NLI) tasks, using transformer-based models like BERT leads to high accuracy, but require hundreds of millions of parameters. These models take in sentences as a sequence of tokens, and learn to encode the meaning of the sequence into embeddings such that those embeddings can be used reliably for NLI tasks. Essentially, every word is considered against every other word in the sequence, and the transformer model is able to determine the relationships between them, entirely from scratch. However, a model that accepts explicit linguistic structures like dependency parse trees may be able to leverage prior encoded information about these relationships, without having to learn them from scratch, thus improving learning efficiency. To investigate this, we adapt Graph Matching Networks (GMN) to operate on dependency parse trees, creating Tree Matching Networks (TMN). We compare TMN to a BERT based model on the SNLI entailment task and on the SemEval similarity task. TMN is able to achieve significantly better results with a significantly reduced memory footprint and much less training time than the BERT based model on the SNLI task, while both models struggled to preform well on the SemEval. Explicit structural representations significantly outperform sequence-based models at comparable scales, but current aggregation methods limit scalability. We propose multi-headed attention aggregation to address this limitation.</li>
</ul>

<h3>Title: Constructing Efficient Fact-Storing MLPs for Transformers</h3>
<ul>
<li><strong>Authors: </strong>Owen Dugan, Roberto Garcia, Ronny Junkins, Jerry Liu, Dylan Zinsley, Sabri Eyuboglu, Atri Rudra, Chris Ré</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00207">https://arxiv.org/abs/2512.00207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00207">https://arxiv.org/pdf/2512.00207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00207]] Constructing Efficient Fact-Storing MLPs for Transformers(https://arxiv.org/abs/2512.00207)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The success of large language models (LLMs) can be attributed in part to their ability to efficiently store factual knowledge as key-value mappings within their MLP parameters. Recent work has proposed explicit weight constructions to build such fact-storing MLPs, providing an improved understanding of LLM fact storage mechanisms. In this paper, we introduce an MLP construction framework that improves over previous constructions in three areas: it 1) works for all but a measure-zero set of feasible input-output pairs, 2) achieves asymptotically optimal parameter efficiency matching information-theoretic bounds for some embeddings, and 3) maintains usability within Transformers for factual recall. Through our improvements, we 1) discover a metric on value embeddings that characterizes facts-per-parameter scaling for both constructed and gradient-descent-trained MLPs, 2) identify a simple encoder-decoder mechanism that empirically matches gradient-descent MLP facts-per-parameter asymptotics across all the inputs and outputs we test, and 3) uncover a fundamental tradeoff between an MLP's fact-storage capacity and its usability within Transformers. Finally, we demonstrate a proof-of-concept application of fact-storing MLPs: modular fact editing on one-layer Transformers by \textit{replacing entire MLPs at once}.</li>
</ul>

<h3>Title: Towards Corpus-Grounded Agentic LLMs for Multilingual Grammatical Analysis</h3>
<ul>
<li><strong>Authors: </strong>Matej Klemen, Tjaša Arčon, Luka Terčon, Marko Robnik-Šikonja, Kaja Dobrovoljc</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00214">https://arxiv.org/abs/2512.00214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00214">https://arxiv.org/pdf/2512.00214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00214]] Towards Corpus-Grounded Agentic LLMs for Multilingual Grammatical Analysis(https://arxiv.org/abs/2512.00214)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Empirical grammar research has become increasingly data-driven, but the systematic analysis of annotated corpora still requires substantial methodological and technical effort. We explore how agentic large language models (LLMs) can streamline this process by reasoning over annotated corpora and producing interpretable, data-grounded answers to linguistic questions. We introduce an agentic framework for corpus-grounded grammatical analysis that integrates concepts such as natural-language task interpretation, code generation, and data-driven reasoning. As a proof of concept, we apply it to Universal Dependencies (UD) corpora, testing it on multilingual grammatical tasks inspired by the World Atlas of Language Structures (WALS). The evaluation spans 13 word-order features and over 170 languages, assessing system performance across three complementary dimensions - dominant-order accuracy, order-coverage completeness, and distributional fidelity - which reflect how well the system generalizes, identifies, and quantifies word-order variations. The results demonstrate the feasibility of combining LLM reasoning with structured linguistic data, offering a first step toward interpretable, scalable automation of corpus-based grammatical inquiry.</li>
</ul>

<h3>Title: Minimal-Edit Instruction Tuning for Low-Resource Indic GEC</h3>
<ul>
<li><strong>Authors: </strong>Akhil Rajeev P</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00219">https://arxiv.org/abs/2512.00219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00219">https://arxiv.org/pdf/2512.00219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00219]] Minimal-Edit Instruction Tuning for Low-Resource Indic GEC(https://arxiv.org/abs/2512.00219)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Grammatical error correction for Indic languages faces limited supervision, diverse scripts, and rich morphology. We propose an augmentation-free setup that uses instruction-tuned large language models and conservative decoding. A 12B GEMMA 3 model is instruction-tuned in bnb 4-bit precision with parameter-efficient fine-tuning (PEFT) and Alpaca-style formatting. Decoding follows a deterministic, constraint-aware procedure with a lightweight normaliser that encourages minimal, meaning-preserving edits. We operationalise inference, subsequent to instruction fine-tuning (IFT), via a fixed, language-specific prompt directly synthesised from a deterministic error classifier's taxonomy, label distributions, and precedence ordering computed on the training corpus. Under the official untuned GLEU evaluation, the system scores 92.41 on Malayalam, sixth overall, and 81.44 on Hindi, third overall. These results indicate that classifier-informed prompt design, adapter-based instruction tuning, and deterministic decoding provide a reproducible and a computationally efficient alternative to augmentation-centred pipelines for Indic GEC. The approach also motivates future work on stronger morphosyntactic constraints and human-centred evaluation of conservative edits.</li>
</ul>

<h3>Title: DenseScan: Advancing 3D Scene Understanding with 2D Dense Annotation</h3>
<ul>
<li><strong>Authors: </strong>Zirui Wang, Tao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00226">https://arxiv.org/abs/2512.00226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00226">https://arxiv.org/pdf/2512.00226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00226]] DenseScan: Advancing 3D Scene Understanding with 2D Dense Annotation(https://arxiv.org/abs/2512.00226)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>3D understanding is a key capability for real-world AI assistance. High-quality data plays an important role in driving the development of the 3D understanding community. Current 3D scene understanding datasets often provide geometric and instance-level information, yet they lack the rich semantic annotations necessary for nuanced visual-language this http URL this work, we introduce DenseScan, a novel dataset with detailed multi-level descriptions generated by an automated pipeline leveraging multi-view 2D images and multimodal large language models (MLLMs). Our approach enables dense captioning of scene elements, ensuring comprehensive object-level descriptions that capture context-sensitive details. Furthermore, we extend these annotations through scenario-based question generation, producing high-level queries that integrate object properties, spatial relationships, and scene context. By coupling geometric detail with semantic richness, DenseScan broadens the range of downstream tasks, from detailed visual-language navigation to interactive question answering. Experimental results demonstrate that our method significantly enhances object-level understanding and question-answering performance in 3D environments compared to traditional annotation pipelines. We release both the annotated dataset and our annotation pipeline to facilitate future research and applications in robotics, augmented reality, and beyond. Through DenseScan, we aim to catalyze new avenues in 3D scene understanding, allowing researchers and practitioners to tackle the complexities of real-world environments with richer, more contextually aware annotations.</li>
</ul>

<h3>Title: TIE: A Training-Inversion-Exclusion Framework for Visually Interpretable and Uncertainty-Guided Out-of-Distribution Detection</h3>
<ul>
<li><strong>Authors: </strong>Pirzada Suhail, Rehna Afroz, Amit Sethi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, eess.IV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00229">https://arxiv.org/abs/2512.00229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00229">https://arxiv.org/pdf/2512.00229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00229]] TIE: A Training-Inversion-Exclusion Framework for Visually Interpretable and Uncertainty-Guided Out-of-Distribution Detection(https://arxiv.org/abs/2512.00229)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks often struggle to recognize when an input lies outside their training experience, leading to unreliable and overconfident predictions. Building dependable machine learning systems therefore requires methods that can both estimate predictive \textit{uncertainty} and detect \textit{out-of-distribution (OOD)} samples in a unified manner. In this paper, we propose \textbf{TIE: a Training--Inversion--Exclusion} framework for visually interpretable and uncertainty-guided anomaly detection that jointly addresses these challenges through iterative refinement. TIE extends a standard $n$-class classifier to an $(n+1)$-class model by introducing a garbage class initialized with Gaussian noise to represent outlier inputs. Within each epoch, TIE performs a closed-loop process of \textit{training, inversion, and exclusion}, where highly uncertain inverted samples reconstructed from the just-trained classifier are excluded into the garbage class. Over successive iterations, the inverted samples transition from noisy artifacts into visually coherent class prototypes, providing transparent insight into how the model organizes its learned manifolds. During inference, TIE rejects OOD inputs by either directly mapping them to the garbage class or producing low-confidence, uncertain misclassifications within the in-distribution classes that are easily separable, all without relying on external OOD datasets. A comprehensive threshold-based evaluation using multiple OOD metrics and performance measures such as \textit{AUROC}, \textit{AUPR}, and \textit{FPR@95\%TPR} demonstrates that TIE offers a unified and interpretable framework for robust anomaly detection and calibrated uncertainty estimation (UE) achieving near-perfect OOD detection with \textbf{\(\!\approx\!\) 0 FPR@95\%TPR} when trained on MNIST or FashionMNIST and tested against diverse unseen datasets.</li>
</ul>

<h3>Title: OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion</h3>
<ul>
<li><strong>Authors: </strong>Sai Koneru, Matthias Huck, Jan Niehues</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00234">https://arxiv.org/abs/2512.00234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00234">https://arxiv.org/pdf/2512.00234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00234]] OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion(https://arxiv.org/abs/2512.00234)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>There has been significant progress in open-source text-only translation large language models (LLMs) with better language coverage and quality. However, these models can be only used in cascaded pipelines for speech translation (ST), performing automatic speech recognition first followed by translation. This introduces additional latency, which is particularly critical in simultaneous ST (SimulST), and prevents the model from exploiting multimodal context, such as images, which can aid disambiguation. Pretrained multimodal foundation models (MMFMs) already possess strong perception and reasoning capabilities across multiple modalities, but generally lack the multilingual coverage and specialized translation performance of dedicated translation LLMs. To build an effective multimodal translation system, we propose an end-to-end approach that fuses MMFMs with translation LLMs. We introduce a novel fusion strategy that connects hidden states from multiple layers of a pretrained MMFM to a translation LLM, enabling joint end-to-end training. The resulting model, OmniFusion, built on Omni 2.5-7B as the MMFM and SeedX PPO-7B as the translation LLM, can perform speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation. Experiments demonstrate that OmniFusion effectively leverages both audio and visual inputs, achieves a 1-second latency reduction in SimulST compared to cascaded pipelines and also improves the overall translation quality\footnote{Code is available at this https URL}.</li>
</ul>

<h3>Title: Self-Supervised Dynamical System Representations for Physiological Time-Series</h3>
<ul>
<li><strong>Authors: </strong>Yenho Chen, Maxwell A. Xu, James M. Rehg, Christopher J. Rozell</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00239">https://arxiv.org/abs/2512.00239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00239">https://arxiv.org/pdf/2512.00239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00239]] Self-Supervised Dynamical System Representations for Physiological Time-Series(https://arxiv.org/abs/2512.00239)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The effectiveness of self-supervised learning (SSL) for physiological time series depends on the ability of a pretraining objective to preserve information about the underlying physiological state while filtering out unrelated noise. However, existing strategies are limited due to reliance on heuristic principles or poorly constrained generative tasks. To address this limitation, we propose a pretraining framework that exploits the information structure of a dynamical systems generative model across multiple time-series. This framework reveals our key insight that class identity can be efficiently captured by extracting information about the generative variables related to the system parameters shared across similar time series samples, while noise unique to individual samples should be discarded. Building on this insight, we propose PULSE, a cross-reconstruction-based pretraining objective for physiological time series datasets that explicitly extracts system information while discarding non-transferrable sample-specific ones. We establish theory that provides sufficient conditions for the system information to be recovered, and empirically validate it using a synthetic dynamical systems experiment. Furthermore, we apply our method to diverse real-world datasets, demonstrating that PULSE learns representations that can broadly distinguish semantic classes, increase label efficiency, and improve transfer learning.</li>
</ul>

<h3>Title: Polynomial Neural Sheaf Diffusion: A Spectral Filtering Approach on Cellular Sheaves</h3>
<ul>
<li><strong>Authors: </strong>Alessio Borgi, Fabrizio Silvestri, Pietro Liò</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.ET, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00242">https://arxiv.org/abs/2512.00242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00242">https://arxiv.org/pdf/2512.00242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00242]] Polynomial Neural Sheaf Diffusion: A Spectral Filtering Approach on Cellular Sheaves(https://arxiv.org/abs/2512.00242)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Sheaf Neural Networks equip graph structures with a cellular sheaf: a geometric structure which assigns local vector spaces (stalks) and a linear learnable restriction/transport maps to nodes and edges, yielding an edge-aware inductive bias that handles heterophily and limits oversmoothing. However, common Neural Sheaf Diffusion implementations rely on SVD-based sheaf normalization and dense per-edge restriction maps, which scale with stalk dimension, require frequent Laplacian rebuilds, and yield brittle gradients. To address these limitations, we introduce Polynomial Neural Sheaf Diffusion (PolyNSD), a new sheaf diffusion approach whose propagation operator is a degree-K polynomial in a normalised sheaf Laplacian, evaluated via a stable three-term recurrence on a spectrally rescaled operator. This provides an explicit K-hop receptive field in a single layer (independently of the stalk dimension), with a trainable spectral response obtained as a convex mixture of K+1 orthogonal polynomial basis responses. PolyNSD enforces stability via convex mixtures, spectral rescaling, and residual/gated paths, reaching new state-of-the-art results on both homophilic and heterophilic benchmarks, inverting the Neural Sheaf Diffusion trend by obtaining these results with just diagonal restriction maps, decoupling performance from large stalk dimension, while reducing runtime and memory requirements.</li>
</ul>

<h3>Title: A Hierarchical Hybrid AI Approach: Integrating Deep Reinforcement Learning and Scripted Agents in Combat Simulations</h3>
<ul>
<li><strong>Authors: </strong>Scotty Black, Christian Darken</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00249">https://arxiv.org/abs/2512.00249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00249">https://arxiv.org/pdf/2512.00249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00249]] A Hierarchical Hybrid AI Approach: Integrating Deep Reinforcement Learning and Scripted Agents in Combat Simulations(https://arxiv.org/abs/2512.00249)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In the domain of combat simulations in support of wargaming, the development of intelligent agents has predominantly been characterized by rule-based, scripted methodologies with deep reinforcement learning (RL) approaches only recently being introduced. While scripted agents offer predictability and consistency in controlled environments, they fall short in dynamic, complex scenarios due to their inherent inflexibility. Conversely, RL agents excel in adaptability and learning, offering potential improvements in handling unforeseen situations, but suffer from significant challenges such as black-box decision-making processes and scalability issues in larger simulation environments. This paper introduces a novel hierarchical hybrid artificial intelligence (AI) approach that synergizes the reliability and predictability of scripted agents with the dynamic, adaptive learning capabilities of RL. By structuring the AI system hierarchically, the proposed approach aims to utilize scripted agents for routine, tactical-level decisions and RL agents for higher-level, strategic decision-making, thus addressing the limitations of each method while leveraging their individual strengths. This integration is shown to significantly improve overall performance, providing a robust, adaptable, and effective solution for developing and training intelligent agents in complex simulation environments.</li>
</ul>

<h3>Title: SD-CGAN: Conditional Sinkhorn Divergence GAN for DDoS Anomaly Detection in IoT Networks</h3>
<ul>
<li><strong>Authors: </strong>Henry Onyeka, Emmanuel Samson, Liang Hong, Tariqul Islam, Imtiaz Ahmed, Kamrul Hasan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00251">https://arxiv.org/abs/2512.00251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00251">https://arxiv.org/pdf/2512.00251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00251]] SD-CGAN: Conditional Sinkhorn Divergence GAN for DDoS Anomaly Detection in IoT Networks(https://arxiv.org/abs/2512.00251)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, generative</a></li>
<li><strong>Abstract: </strong>The increasing complexity of IoT edge networks presents significant challenges for anomaly detection, particularly in identifying sophisticated Denial-of-Service (DoS) attacks and zero-day exploits under highly dynamic and imbalanced traffic conditions. This paper proposes SD-CGAN, a Conditional Generative Adversarial Network framework enhanced with Sinkhorn Divergence, tailored for robust anomaly detection in IoT edge environments. The framework incorporates CTGAN-based synthetic data augmentation to address class imbalance and leverages Sinkhorn Divergence as a geometry-aware loss function to improve training stability and reduce mode collapse. The model is evaluated on exploitative attack subsets from the CICDDoS2019 dataset and compared against baseline deep learning and GAN-based approaches. Results show that SD-CGAN achieves superior detection accuracy, precision, recall, and F1-score while maintaining computational efficiency suitable for deployment in edge-enabled IoT environments.</li>
</ul>

<h3>Title: Relightable Holoported Characters: Capturing and Relighting Dynamic Human Performance from Sparse Views</h3>
<ul>
<li><strong>Authors: </strong>Kunwar Maheep Singh, Jianchun Chen, Vladislav Golyanik, Stephan J. Garbin, Thabo Beeler, Rishabh Dabral, Marc Habermann, Christian Theobalt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00255">https://arxiv.org/abs/2512.00255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00255">https://arxiv.org/pdf/2512.00255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00255]] Relightable Holoported Characters: Capturing and Relighting Dynamic Human Performance from Sparse Views(https://arxiv.org/abs/2512.00255)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present Relightable Holoported Characters (RHC), a novel person-specific method for free-view rendering and relighting of full-body and highly dynamic humans solely observed from sparse-view RGB videos at inference. In contrast to classical one-light-at-a-time (OLAT)-based human relighting, our transformer-based RelightNet predicts relit appearance within a single network pass, avoiding costly OLAT-basis capture and generation. For training such a model, we introduce a new capture strategy and dataset recorded in a multi-view lightstage, where we alternate frames lit by random environment maps with uniformly lit tracking frames, simultaneously enabling accurate motion tracking and diverse illumination as well as dynamics coverage. Inspired by the rendering equation, we derive physics-informed features that encode geometry, albedo, shading, and the virtual camera view from a coarse human mesh proxy and the input views. Our RelightNet then takes these features as input and cross-attends them with a novel lighting condition, and regresses the relit appearance in the form of texel-aligned 3D Gaussian splats attached to the coarse mesh proxy. Consequently, our RelightNet implicitly learns to efficiently compute the rendering equation for novel lighting conditions within a single feed-forward pass. Experiments demonstrate our method's superior visual fidelity and lighting reproduction compared to state-of-the-art approaches. Project page: this https URL</li>
</ul>

<h3>Title: Scalable and Interpretable Scientific Discovery via Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)</h3>
<ul>
<li><strong>Authors: </strong>Y. Sungtaek Ju</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00260">https://arxiv.org/abs/2512.00260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00260">https://arxiv.org/pdf/2512.00260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00260]] Scalable and Interpretable Scientific Discovery via Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)(https://arxiv.org/abs/2512.00260)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Kolmogorov-Arnold Networks (KANs) offer a promising alternative to Multi-Layer Perceptron (MLP) by placing learnable univariate functions on network edges, enhancing interpretability. However, standard KANs lack probabilistic outputs, limiting their utility in applications requiring uncertainty quantification. While recent Gaussian Process (GP) extensions to KANs address this, they utilize exact inference methods that scale cubically with data size N, restricting their application to smaller datasets. We introduce the Sparse Variational GP-KAN (SVGP-KAN), an architecture that integrates sparse variational inference with the KAN topology. By employing $M$ inducing points and analytic moment matching, our method reduces computational complexity from $O(N^3)$ to $O(NM^2)$ or linear in sample size, enabling the application of probabilistic KANs to larger scientific datasets. Furthermore, we demonstrate that integrating a permutation-based importance analysis enables the network to function as a framework for structural identification, identifying relevant inputs and classifying functional relationships.</li>
</ul>

<h3>Title: UniDiff: Parameter-Efficient Adaptation of Diffusion Models for Land Cover Classification with Multi-Modal Remotely Sensed Imagery and Sparse Annotations</h3>
<ul>
<li><strong>Authors: </strong>Yuzhen Hu, Saurabh Prasad</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00261">https://arxiv.org/abs/2512.00261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00261">https://arxiv.org/pdf/2512.00261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00261]] UniDiff: Parameter-Efficient Adaptation of Diffusion Models for Land Cover Classification with Multi-Modal Remotely Sensed Imagery and Sparse Annotations(https://arxiv.org/abs/2512.00261)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion</a></li>
<li><strong>Abstract: </strong>Sparse annotations fundamentally constrain multimodal remote sensing: even recent state-of-the-art supervised methods such as MSFMamba are limited by the availability of labeled data, restricting their practical deployment despite architectural advances. ImageNet-pretrained models provide rich visual representations, but adapting them to heterogeneous modalities such as hyperspectral imaging (HSI) and synthetic aperture radar (SAR) without large labeled datasets remains challenging. We propose UniDiff, a parameter-efficient framework that adapts a single ImageNet-pretrained diffusion model to multiple sensing modalities using only target-domain data. UniDiff combines FiLM-based timestep-modality conditioning, parameter-efficient adaptation of approximately 5% of parameters, and pseudo-RGB anchoring to preserve pre-trained representations and prevent catastrophic forgetting. This design enables effective feature extraction from remote sensing data under sparse annotations. Our results with two established multi-modal benchmarking datasets demonstrate that unsupervised adaptation of a pre-trained diffusion model effectively mitigates annotation constraints and achieves effective fusion of multi-modal remotely sensed data.</li>
</ul>

<h3>Title: HeartFormer: Semantic-Aware Dual-Structure Transformers for 3D Four-Chamber Cardiac Point Cloud Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Zhengda Ma, Abhirup Banerjee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00264">https://arxiv.org/abs/2512.00264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00264">https://arxiv.org/pdf/2512.00264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00264]] HeartFormer: Semantic-Aware Dual-Structure Transformers for 3D Four-Chamber Cardiac Point Cloud Reconstruction(https://arxiv.org/abs/2512.00264)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>We present the first geometric deep learning framework based on point cloud representation for 3D four-chamber cardiac reconstruction from cine MRI data. This work addresses a long-standing limitation in conventional cine MRI, which typically provides only 2D slice images of the heart, thereby restricting a comprehensive understanding of cardiac morphology and physiological mechanisms in both healthy and pathological conditions. To overcome this, we propose \textbf{HeartFormer}, a novel point cloud completion network that extends traditional single-class point cloud completion to the multi-class. HeartFormer consists of two key components: a Semantic-Aware Dual-Structure Transformer Network (SA-DSTNet) and a Semantic-Aware Geometry Feature Refinement Transformer Network (SA-GFRTNet). SA-DSTNet generates an initial coarse point cloud with both global geometry features and substructure geometry features. Guided by these semantic-geometry representations, SA-GFRTNet progressively refines the coarse output, effectively leveraging both global and substructure geometric priors to produce high-fidelity and geometrically consistent reconstructions. We further construct \textbf{HeartCompv1}, the first publicly available large-scale dataset with 17,000 high-resolution 3D multi-class cardiac meshes and point-clouds, to establish a general benchmark for this emerging research direction. Extensive cross-domain experiments on HeartCompv1 and UK Biobank demonstrate that HeartFormer achieves robust, accurate, and generalizable performance, consistently surpassing state-of-the-art (SOTA) methods. Code and dataset will be released upon acceptance at: this https URL.</li>
</ul>

<h3>Title: USB: Unified Synthetic Brain Framework for Bidirectional Pathology-Healthy Generation and Editing</h3>
<ul>
<li><strong>Authors: </strong>Jun Wang, Peirong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00269">https://arxiv.org/abs/2512.00269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00269">https://arxiv.org/pdf/2512.00269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00269]] USB: Unified Synthetic Brain Framework for Bidirectional Pathology-Healthy Generation and Editing(https://arxiv.org/abs/2512.00269)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Understanding the relationship between pathological and healthy brain structures is fundamental to neuroimaging, connecting disease diagnosis and detection with modeling, prediction, and treatment planning. However, paired pathological-healthy data are extremely difficult to obtain, as they rely on pre- and post-treatment imaging, constrained by clinical outcomes and longitudinal data availability. Consequently, most existing brain image generation and editing methods focus on visual quality yet remain domain-specific, treating pathological and healthy image modeling independently. We introduce USB (Unified Synthetic Brain), the first end-to-end framework that unifies bidirectional generation and editing of pathological and healthy brain images. USB models the joint distribution of lesions and brain anatomy through a paired diffusion mechanism and achieves both pathological and healthy image generation. A consistency guidance algorithm further preserves anatomical consistency and lesion correspondence during bidirectional pathology-healthy editing. Extensive experiments on six public brain MRI datasets including healthy controls, stroke, and Alzheimer's patients, demonstrate USB's ability to produce diverse and realistic results. By establishing the first unified benchmark for brain image generation and editing, USB opens opportunities for scalable dataset creation and robust neuroimaging analysis. Code is available at this https URL.</li>
</ul>

<h3>Title: Teleportation-Based Defenses for Privacy in Approximate Machine Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Mohammad M Maheri, Xavier Cadet, Peter Chin, Hamed Haddadi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00272">https://arxiv.org/abs/2512.00272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00272">https://arxiv.org/pdf/2512.00272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00272]] Teleportation-Based Defenses for Privacy in Approximate Machine Unlearning(https://arxiv.org/abs/2512.00272)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Approximate machine unlearning aims to efficiently remove the influence of specific data points from a trained model, offering a practical alternative to full retraining. However, it introduces privacy risks: an adversary with access to pre- and post-unlearning models can exploit their differences for membership inference or data reconstruction. We show these vulnerabilities arise from two factors: large gradient norms of forget-set samples and the close proximity of unlearned parameters to the original model. To demonstrate their severity, we propose unlearning-specific membership inference and reconstruction attacks, showing that several state-of-the-art methods (e.g., NGP, SCRUB) remain vulnerable. To mitigate this leakage, we introduce WARP, a plug-and-play teleportation defense that leverages neural network symmetries to reduce forget-set gradient energy and increase parameter dispersion while preserving predictions. This reparameterization obfuscates the signal of forgotten data, making it harder for attackers to distinguish forgotten samples from non-members or recover them via reconstruction. Across six unlearning algorithms, our approach achieves consistent privacy gains, reducing adversarial advantage (AUC) by up to 64% in black-box and 92% in white-box settings, while maintaining accuracy on retained data. These results highlight teleportation as a general tool for reducing attack success in approximate unlearning.</li>
</ul>

<h3>Title: Lost without translation -- Can transformer (language models) understand mood states?</h3>
<ul>
<li><strong>Authors: </strong>Prakrithi Shivaprakash, Diptadhi Mukherjee, Lekhansh Shukla, Animesh Mukherjee, Prabhat Chand, Pratima Murthy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00274">https://arxiv.org/abs/2512.00274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00274">https://arxiv.org/pdf/2512.00274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00274]] Lost without translation -- Can transformer (language models) understand mood states?(https://arxiv.org/abs/2512.00274)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Background: Large Language Models show promise in psychiatry but are English-centric. Their ability to understand mood states in other languages is unclear, as different languages have their own idioms of distress. Aim: To quantify the ability of language models to faithfully represent phrases (idioms of distress) of four distinct mood states (depression, euthymia, euphoric mania, dysphoric mania) expressed in Indian languages. Methods: We collected 247 unique phrases for the four mood states across 11 Indic languages. We tested seven experimental conditions, comparing k-means clustering performance on: (a) direct embeddings of native and Romanised scripts (using multilingual and Indic-specific models) and (b) embeddings of phrases translated to English and Chinese. Performance was measured using a composite score based on Adjusted Rand Index, Normalised Mutual Information, Homogeneity and Completeness. Results: Direct embedding of Indic languages failed to cluster mood states (Composite Score = 0.002). All translation-based approaches showed significant improvement. High performance was achieved using Gemini-translated English (Composite=0.60) and human-translated English (Composite=0.61) embedded with gemini-001. Surprisingly, human-translated English, further translated into Chinese and embedded with a Chinese model, performed best (Composite = 0.67). Specialised Indic models (IndicBERT and Sarvam-M) performed poorly. Conclusion: Current models cannot meaningfully represent mood states directly from Indic languages, posing a fundamental barrier to their psychiatric application for diagnostic or therapeutic purposes in India. While high-quality translation bridges this gap, reliance on proprietary models or complex translation pipelines is unsustainable. Models must first be built to understand diverse local languages to be effective in global mental health.</li>
</ul>

<h3>Title: Rethinking Lung Cancer Screening: AI Nodule Detection and Diagnosis Outperforms Radiologists, Leading Models, and Standards Beyond Size and Growth</h3>
<ul>
<li><strong>Authors: </strong>Sylvain Bodard, Pierre Baudot, Benjamin Renoust, Charles Voyton, Gwendoline De Bie, Ezequiel Geremia, Van-Khoa Le, Danny Francis, Pierre-Henri Siot, Yousra Haddou, Vincent Bobin, Jean-Christophe Brisset, Carey C. Thomson, Valerie Bourdes, Benoit Huet</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00281">https://arxiv.org/abs/2512.00281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00281">https://arxiv.org/pdf/2512.00281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00281]] Rethinking Lung Cancer Screening: AI Nodule Detection and Diagnosis Outperforms Radiologists, Leading Models, and Standards Beyond Size and Growth(https://arxiv.org/abs/2512.00281)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Early detection of malignant lung nodules is critical, but its dependence on size and growth in screening inherently delays diagnosis. We present an AI system that redefines lung cancer screening by performing both detection and malignancy diagnosis directly at the nodule level on low-dose CT scans. To address limitations in dataset scale and explainability, we designed an ensemble of shallow deep learning and feature-based specialized models. Trained and evaluated on 25,709 scans with 69,449 annotated nodules, the system outperforms radiologists, Lung-RADS, and leading AI models (Sybil, Brock, Google, Kaggle). It achieves an area under the receiver operating characteristic curve (AUC) of 0.98 internally and 0.945 on an independent cohort. With 0.5 false positives per scan at 99.3\% sensitivity, it addresses key barriers to AI adoption. Critically, it outperforms radiologists across all nodule sizes and stages, excelling in stage 1 cancers, and all growth-based metrics, including the least accurate: Volume-Doubling Time. It also surpasses radiologists by up to one year in diagnosing indeterminate and slow-growing nodules.</li>
</ul>

<h3>Title: Data-Driven Modeling and Correction of Vehicle Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Nguyen Ly, Caroline Tatsuoka, Jai Nagaraj, Jacob Levy, Fernando Palafox, David Fridovich-Keil, Hannah Lu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00289">https://arxiv.org/abs/2512.00289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00289">https://arxiv.org/pdf/2512.00289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00289]] Data-Driven Modeling and Correction of Vehicle Dynamics(https://arxiv.org/abs/2512.00289)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We develop a data-driven framework for learning and correcting non-autonomous vehicle dynamics. Physics-based vehicle models are often simplified for tractability and therefore exhibit inherent model-form uncertainty, motivating the need for data-driven correction. Moreover, non-autonomous dynamics are governed by time-dependent control inputs, which pose challenges in learning predictive models directly from temporal snapshot data. To address these, we reformulate the vehicle dynamics via a local parameterization of the time-dependent inputs, yielding a modified system composed of a sequence of local parametric dynamical systems. We approximate these parametric systems using two complementary approaches. First, we employ the DRIPS (dimension reduction and interpolation in parameter space) methodology to construct efficient linear surrogate models, equipped with lifted observable spaces and manifold-based operator interpolation. This enables data-efficient learning of vehicle models whose dynamics admit accurate linear representations in the lifted spaces. Second, for more strongly nonlinear systems, we employ FML (Flow Map Learning), a deep neural network approach that approximates the parametric evolution map without requiring special treatment of nonlinearities. We further extend FML with a transfer-learning-based model correction procedure, enabling the correction of misspecified prior models using only a sparse set of high-fidelity or experimental measurements, without assuming a prescribed form for the correction term. Through a suite of numerical experiments on unicycle, simplified bicycle, and slip-based bicycle models, we demonstrate that DRIPS offers robust and highly data-efficient learning of non-autonomous vehicle dynamics, while FML provides expressive nonlinear modeling and effective correction of model-form errors under severe data scarcity.</li>
</ul>

<h3>Title: EduEval: A Hierarchical Cognitive Benchmark for Evaluating Large Language Models in Chinese Education</h3>
<ul>
<li><strong>Authors: </strong>Guoqing Ma, Jia Zhu, Hanghui Guo, Weijie Shi, Yue Cui, Jiawei Shen, Zilong Li, Yidan Liang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00290">https://arxiv.org/abs/2512.00290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00290">https://arxiv.org/pdf/2512.00290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00290]] EduEval: A Hierarchical Cognitive Benchmark for Evaluating Large Language Models in Chinese Education(https://arxiv.org/abs/2512.00290)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate significant potential for educational applications. However, their unscrutinized deployment poses risks to educational standards, underscoring the need for rigorous evaluation. We introduce EduEval, a comprehensive hierarchical benchmark for evaluating LLMs in Chinese K-12 education. This benchmark makes three key contributions: (1) Cognitive Framework: We propose the EduAbility Taxonomy, which unifies Bloom's Taxonomy and Webb's Depth of Knowledge to organize tasks across six cognitive dimensions including Memorization, Understanding, Application, Reasoning, Creativity, and Ethics. (2) Authenticity: Our benchmark integrates real exam questions, classroom conversation, student essays, and expert-designed prompts to reflect genuine educational challenges; (3) Scale: EduEval comprises 24 distinct task types with over 11,000 questions spanning primary to high school levels. We evaluate 14 leading LLMs under both zero-shot and few-shot settings, revealing that while models perform well on factual tasks, they struggle with classroom dialogue classification and exhibit inconsistent results in creative content generation. Interestingly, several open source models outperform proprietary systems on complex educational reasoning. Few-shot prompting shows varying effectiveness across cognitive dimensions, suggesting that different educational objectives require tailored approaches. These findings provide targeted benchmarking metrics for developing LLMs specifically optimized for diverse Chinese educational tasks.</li>
</ul>

<h3>Title: FiCoTS: Fine-to-Coarse LLM-Enhanced Hierarchical Cross-Modality Interaction for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Yafei Lyu, Hao Zhou, Lu Zhang, Xu Yang, Zhiyong Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00293">https://arxiv.org/abs/2512.00293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00293">https://arxiv.org/pdf/2512.00293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00293]] FiCoTS: Fine-to-Coarse LLM-Enhanced Hierarchical Cross-Modality Interaction for Time Series Forecasting(https://arxiv.org/abs/2512.00293)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Time series forecasting is central to data analysis and web technologies. The recent success of Large Language Models (LLMs) offers significant potential for this field, especially from the cross-modality aspect. Most methods adopt an LLM-as-Predictor paradigm, using LLM as the forecasting backbone and designing modality alignment mechanisms to enable LLM to understand time series data. However, the semantic information in the two modalities of time series and text differs significantly, making it challenging for LLM to fully understand time series data. To mitigate this challenge, our work follows an LLM-as-Enhancer paradigm to fully utilize the advantage of LLM in text understanding, where LLM is only used to encode text modality to complement time series modality. Based on this paradigm, we propose FiCoTS, an LLM-enhanced fine-to-coarse framework for multimodal time series forecasting. Specifically, the framework facilitates progressive cross-modality interaction by three levels in a fine-to-coarse scheme: First, in the token-level modality alignment module, a dynamic heterogeneous graph is constructed to filter noise and align time series patches with text tokens; Second, in the feature-level modality interaction module, a global cross-attention mechanism is introduced to enable each time series variable to connect with relevant textual contexts; Third, in the decision-level modality fusion module, we design a gated network to adaptively fuse the results of the two modalities for robust predictions. These three modules work synergistically to let the two modalities interact comprehensively across three semantic levels, enabling textual information to effectively support temporal prediction. Extensive experiments on seven real-world benchmarks demonstrate that our model achieves state-of-the-art performance. The codes will be released publicly.</li>
</ul>

<h3>Title: Words into World: A Task-Adaptive Agent for Language-Guided Spatial Retrieval in AR</h3>
<ul>
<li><strong>Authors: </strong>Lixing Guo, Tobias Höllerer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00294">https://arxiv.org/abs/2512.00294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00294">https://arxiv.org/pdf/2512.00294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00294]] Words into World: A Task-Adaptive Agent for Language-Guided Spatial Retrieval in AR(https://arxiv.org/abs/2512.00294)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Traditional augmented reality (AR) systems predominantly rely on fixed class detectors or fiducial markers, limiting their ability to interpret complex, open-vocabulary natural language queries. We present a modular AR agent system that integrates multimodal large language models (MLLMs) with grounded vision models to enable relational reasoning in space and language-conditioned spatial retrieval in physical environments. Our adaptive task agent coordinates MLLMs and coordinate-aware perception tools to address varying query complexities, ranging from simple object identification to multi-object relational reasoning, while returning meter-accurate 3D anchors. It constructs dynamic AR scene graphs encoding nine typed relations (spatial, structural-semantic, causal-functional), enabling MLLMs to understand not just what objects exist, but how they relate and interact in 3D space. Through task-adaptive region-of-interest highlighting and contextual spatial retrieval, the system guides human attention to information-dense areas while supporting human-in-the-loop refinement. The agent dynamically invokes coordinate-aware tools for complex queries-selection, measurement, comparison, and actuation-grounding language understanding in physical operations. The modular architecture supports plug-and-use vision-language models without retraining, establishing AR agents as intermediaries that augment MLLMs with real-world spatial intelligence for interactive scene understanding. We also introduce GroundedAR-Bench, an evaluation framework for language-driven real world localization and relation grounding across diverse environments.</li>
</ul>

<h3>Title: Challenges of Heterogeneity in Big Data: A Comparative Study of Classification in Large-Scale Structured and Unstructured Domains</h3>
<ul>
<li><strong>Authors: </strong>González Trigueros Jesús Eduardo, Alonso Sánchez Alejandro, Muñoz Rivera Emilio, Peñarán Prieto Mariana Jaqueline, Mendoza González Camila Natalia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00298">https://arxiv.org/abs/2512.00298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00298">https://arxiv.org/pdf/2512.00298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00298]] Challenges of Heterogeneity in Big Data: A Comparative Study of Classification in Large-Scale Structured and Unstructured Domains(https://arxiv.org/abs/2512.00298)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>This study analyzes the impact of heterogeneity ("Variety") in Big Data by comparing classification strategies across structured (Epsilon) and unstructured (Rest-Mex, IMDB) domains. A dual methodology was implemented: evolutionary and Bayesian hyperparameter optimization (Genetic Algorithms, Optuna) in Python for numerical data, and distributed processing in Apache Spark for massive textual corpora. The results reveal a "complexity paradox": in high-dimensional spaces, optimized linear models (SVM, Logistic Regression) outperformed deep architectures and Gradient Boosting. Conversely, in text-based domains, the constraints of distributed fine-tuning led to overfitting in complex models, whereas robust feature engineering -- specifically Transformer-based embeddings (ROBERTa) and Bayesian Target Encoding -- enabled simpler models to generalize effectively. This work provides a unified framework for algorithm selection based on data nature and infrastructure constraints.</li>
</ul>

<h3>Title: Gradient Inversion in Federated Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Shenghong He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00303">https://arxiv.org/abs/2512.00303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00303">https://arxiv.org/pdf/2512.00303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00303]] Gradient Inversion in Federated Reinforcement Learning(https://arxiv.org/abs/2512.00303)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, federate</a></li>
<li><strong>Abstract: </strong>Federated reinforcement learning (FRL) enables distributed learning of optimal policies while preserving local data privacy through gradient this http URL, FRL faces the risk of data privacy leaks, where attackers exploit shared gradients to reconstruct local training this http URL to traditional supervised federated learning, successful reconstruction in FRL requires the generated data not only to match the shared gradients but also to align with real transition dynamics of the environment (i.e., aligning with the real data transition distribution).To address this issue, we propose a novel attack method called Regularization Gradient Inversion Attack (RGIA), which enforces prior-knowledge-based regularization on states, rewards, and transition dynamics during the optimization process to ensure that the reconstructed data remain close to the true transition this http URL, we prove that the prior-knowledge-based regularization term narrows the solution space from a broad set containing spurious solutions to a constrained subset that satisfies both gradient matching and true transition this http URL experiments on control tasks and autonomous driving tasks demonstrate that RGIA can effectively constrain reconstructed data transition distributions and thus successfully reconstruct local private data.</li>
</ul>

<h3>Title: Adversarial Signed Graph Learning with Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Haobin Ke, Sen Zhang, Qingqing Ye, Xun Ran, Haibo Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00307">https://arxiv.org/abs/2512.00307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00307">https://arxiv.org/pdf/2512.00307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00307]] Adversarial Signed Graph Learning with Differential Privacy(https://arxiv.org/abs/2512.00307)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust</a></li>
<li><strong>Abstract: </strong>Signed graphs with positive and negative edges can model complex relationships in social networks. Leveraging on balance theory that deduces edge signs from multi-hop node pairs, signed graph learning can generate node embeddings that preserve both structural and sign information. However, training on sensitive signed graphs raises significant privacy concerns, as model parameters may leak private link information. Existing protection methods with differential privacy (DP) typically rely on edge or gradient perturbation for unsigned graph protection. Yet, they are not well-suited for signed graphs, mainly because edge perturbation tends to cascading errors in edge sign inference under balance theory, while gradient perturbation increases sensitivity due to node interdependence and gradient polarity change caused by sign flips, resulting in larger noise injection. In this paper, motivated by the robustness of adversarial learning to noisy interactions, we present ASGL, a privacy-preserving adversarial signed graph learning method that preserves high utility while achieving node-level DP. We first decompose signed graphs into positive and negative subgraphs based on edge signs, and then design a gradient-perturbed adversarial module to approximate the true signed connectivity distribution. In particular, the gradient perturbation helps mitigate cascading errors, while the subgraph separation facilitates sensitivity reduction. Further, we devise a constrained breadth-first search tree strategy that fuses with balance theory to identify the edge signs between generated node pairs. This strategy also enables gradient decoupling, thereby effectively lowering gradient sensitivity. Extensive experiments on real-world datasets show that ASGL achieves favorable privacy-utility trade-offs across multiple downstream tasks.</li>
</ul>

<h3>Title: Optimizing Distributional Geometry Alignment with Optimal Transport for Generative Dataset Distillation</h3>
<ul>
<li><strong>Authors: </strong>Xiao Cui, Yulei Qin, Wengang Zhou, Hongsheng Li, Houqiang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00308">https://arxiv.org/abs/2512.00308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00308">https://arxiv.org/pdf/2512.00308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00308]] Optimizing Distributional Geometry Alignment with Optimal Transport for Generative Dataset Distillation(https://arxiv.org/abs/2512.00308)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Dataset distillation seeks to synthesize a compact distilled dataset, enabling models trained on it to achieve performance comparable to models trained on the full dataset. Recent methods for large-scale datasets focus on matching global distributional statistics (e.g., mean and variance), but overlook critical instance-level characteristics and intraclass variations, leading to suboptimal generalization. We address this limitation by reformulating dataset distillation as an Optimal Transport (OT) distance minimization problem, enabling fine-grained alignment at both global and instance levels throughout the pipeline. OT offers a geometrically faithful framework for distribution matching. It effectively preserves local modes, intra-class patterns, and fine-grained variations that characterize the geometry of complex, high-dimensional distributions. Our method comprises three components tailored for preserving distributional geometry: (1) OT-guided diffusion sampling, which aligns latent distributions of real and distilled images; (2) label-image-aligned soft relabeling, which adapts label distributions based on the complexity of distilled image distributions; and (3) OT-based logit matching, which aligns the output of student models with soft-label distributions. Extensive experiments across diverse architectures and large-scale datasets demonstrate that our method consistently outperforms state-of-the-art approaches in an efficient manner, achieving at least 4% accuracy improvement under IPC=10 settings for each architecture on ImageNet-1K.</li>
</ul>

<h3>Title: ART-ASyn: Anatomy-aware Realistic Texture-based Anomaly Synthesis Framework for Chest X-Rays</h3>
<ul>
<li><strong>Authors: </strong>Qinyi Cao, Jianan Fan, Weidong Cai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00310">https://arxiv.org/abs/2512.00310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00310">https://arxiv.org/pdf/2512.00310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00310]] ART-ASyn: Anatomy-aware Realistic Texture-based Anomaly Synthesis Framework for Chest X-Rays(https://arxiv.org/abs/2512.00310)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Unsupervised anomaly detection aims to identify anomalies without pixel-level annotations. Synthetic anomaly-based methods exhibit a unique capacity to introduce controllable irregularities with known masks, enabling explicit supervision during training. However, existing methods often produce synthetic anomalies that are visually distinct from real pathological patterns and ignore anatomical structure. This paper presents a novel Anatomy-aware Realistic Texture-based Anomaly Synthesis framework (ART-ASyn) for chest X-rays that generates realistic and anatomically consistent lung opacity related anomalies using texture-based augmentation guided by our proposed Progressive Binary Thresholding Segmentation method (PBTSeg) for lung segmentation. The generated paired samples of synthetic anomalies and their corresponding precise pixel-level anomaly mask for each normal sample enable explicit segmentation supervision. In contrast to prior work limited to one-class classification, ART-ASyn is further evaluated for zero-shot anomaly segmentation, demonstrating generalizability on an unseen dataset without target-domain annotations. Code availability is available at this https URL.</li>
</ul>

<h3>Title: Tracing Mathematical Proficiency Through Problem-Solving Processes</h3>
<ul>
<li><strong>Authors: </strong>Jungyang Park, Suho Kang, Jaewoo Park, Jaehong Kim, Jaewoo Shin, Seonjoon Park, Youngjae Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00311">https://arxiv.org/abs/2512.00311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00311">https://arxiv.org/pdf/2512.00311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00311]] Tracing Mathematical Proficiency Through Problem-Solving Processes(https://arxiv.org/abs/2512.00311)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Knowledge Tracing (KT) aims to model student's knowledge state and predict future performance to enable personalized learning in Intelligent Tutoring Systems. However, traditional KT methods face fundamental limitations in explainability, as they rely solely on the response correctness, neglecting the rich information embedded in students' problem-solving processes. To address this gap, we propose Knowledge Tracing Leveraging Problem-Solving Process (KT-PSP), which incorporates students' problem-solving processes to capture the multidimensional aspects of mathematical proficiency. We also introduce KT-PSP-25, a new dataset specifically designed for the KT-PSP. Building on this, we present StatusKT, a KT framework that employs a teacher-student-teacher three-stage LLM pipeline to extract students' MP as intermediate signals. In this pipeline, the teacher LLM first extracts problem-specific proficiency indicators, then a student LLM generates responses based on the student's solution process, and a teacher LLM evaluates these responses to determine mastery of each indicator. The experimental results on KT-PSP-25 demonstrate that StatusKT improves the prediction performance of existing KT methods. Moreover, StatusKT provides interpretable explanations for its predictions by explicitly modeling students' mathematical proficiency.</li>
</ul>

<h3>Title: Odometry Without Correspondence from Inertially Constrained Ruled Surfaces</h3>
<ul>
<li><strong>Authors: </strong>Chenqi Zhu, Levi Burner, Yiannis Aloimonos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00327">https://arxiv.org/abs/2512.00327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00327">https://arxiv.org/pdf/2512.00327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00327]] Odometry Without Correspondence from Inertially Constrained Ruled Surfaces(https://arxiv.org/abs/2512.00327)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Visual odometry techniques typically rely on feature extraction from a sequence of images and subsequent computation of optical flow. This point-to-point correspondence between two consecutive frames can be costly to compute and suffers from varying accuracy, which affects the odometry estimate's quality. Attempts have been made to bypass the difficulties originating from the correspondence problem by adopting line features and fusing other sensors (event camera, IMU) to improve performance, many of which still heavily rely on correspondence. If the camera observes a straight line as it moves, the image of the line sweeps a smooth surface in image-space time. It is a ruled surface and analyzing its shape gives information about odometry. Further, its estimation requires only differentially computed updates from point-to-line associations. Inspired by event cameras' propensity for edge detection, this research presents a novel algorithm to reconstruct 3D scenes and visual odometry from these ruled surfaces. By constraining the surfaces with the inertia measurements from an onboard IMU sensor, the dimensionality of the solution space is greatly reduced.</li>
</ul>

<h3>Title: Assertion-Conditioned Compliance: A Provenance-Aware Vulnerability in Multi-Turn Tool-Calling Agents</h3>
<ul>
<li><strong>Authors: </strong>Daud Waqas, Aaryamaan Golthi, Erika Hayashida, Huanzhi Mao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00332">https://arxiv.org/abs/2512.00332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00332">https://arxiv.org/pdf/2512.00332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00332]] Assertion-Conditioned Compliance: A Provenance-Aware Vulnerability in Multi-Turn Tool-Calling Agents(https://arxiv.org/abs/2512.00332)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-turn tool-calling LLMs (models capable of invoking external APIs or tools across several user turns) have emerged as a key feature in modern AI assistants, enabling extended dialogues from benign tasks to critical business, medical, and financial operations. Yet implementing multi-turn pipelines remains difficult for many safety-critical industries due to ongoing concerns regarding model resilience. While standardized benchmarks such as the Berkeley Function-Calling Leaderboard (BFCL) have underpinned confidence concerning advanced function-calling models (like Salesforce's xLAM V2), there is still a lack of visibility into multi-turn conversation-level robustness, especially given their exposure to real-world systems. In this paper, we introduce Assertion-Conditioned Compliance (A-CC), a novel evaluation paradigm for multi-turn function-calling dialogues. A-CC provides holistic metrics that evaluate a model's behavior when confronted with misleading assertions originating from two distinct vectors: (1) user-sourced assertions (USAs), which measure sycophancy toward plausible but misinformed user beliefs, and (2) function-sourced assertions (FSAs), which measure compliance with plausible but contradictory system policies (e.g., stale hints from unmaintained tools). Our results show that models are highly vulnerable to both USA sycophancy and FSA policy conflicts, confirming A-CC as a critical, latent vulnerability in deployed agents.</li>
</ul>

<h3>Title: IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages</h3>
<ul>
<li><strong>Authors: </strong>Ayush Maheshwari, Kaushal Sharma, Vivek Patel, Aditya Maheshwari</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00333">https://arxiv.org/abs/2512.00333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00333">https://arxiv.org/pdf/2512.00333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00333]] IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages(https://arxiv.org/abs/2512.00333)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models excel on high-resource multilingual tasks, low- and extremely low-resource Indic languages remain severely under-evaluated. We present IndicParam, a human-curated benchmark of over 13,000 multiple-choice questions covering 11 such languages (Nepali, Gujarati, Marathi, Odia as low-resource; Dogri, Maithili, Rajasthani, Sanskrit, Bodo, Santali, Konkani as extremely low-resource) plus Sanskrit-English code-mixed set. We evaluated 19 LLMs, both proprietary and open-weights, which reveals that even the top-performing GPT-5 reaches only 45.0% average accuracy, followed by DeepSeek-3.2 (43.1) and Claude-4.5 (42.7). We additionally label each question as knowledge-oriented or purely linguistic to discriminate factual recall from grammatical proficiency. Further, we assess the ability of LLMs to handle diverse question formats-such as list-based matching, assertion-reason pairs, and sequence ordering-alongside conventional multiple-choice questions. IndicParam provides insights into limitations of cross-lingual transfer and establishes a challenging benchmark for Indic languages. The dataset is available at this https URL. Scripts to run benchmark are present at this https URL.</li>
</ul>

<h3>Title: MVAD : A Comprehensive Multimodal Video-Audio Dataset for AIGC Detection</h3>
<ul>
<li><strong>Authors: </strong>Mengxue Hu, Yunfeng Diao, Changtao Miao, Jianshu Li, Zhe Li, Joey Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00336">https://arxiv.org/abs/2512.00336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00336">https://arxiv.org/pdf/2512.00336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00336]] MVAD : A Comprehensive Multimodal Video-Audio Dataset for AIGC Detection(https://arxiv.org/abs/2512.00336)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of AI-generated multimodal video-audio content has raised significant concerns regarding information security and content authenticity. Existing synthetic video datasets predominantly focus on the visual modality alone, while the few incorporating audio are largely confined to facial deepfakes--a limitation that fails to address the expanding landscape of general multimodal AI-generated content and substantially impedes the development of trustworthy detection systems. To bridge this critical gap, we introduce the Multimodal Video-Audio Dataset (MVAD), the first comprehensive dataset specifically designed for detecting AI-generated multimodal video-audio content. Our dataset exhibits three key characteristics: (1) genuine multimodality with samples generated according to three realistic video-audio forgery patterns; (2) high perceptual quality achieved through diverse state-of-the-art generative models; and (3) comprehensive diversity spanning realistic and anime visual styles, four content categories (humans, animals, objects, and scenes), and four video-audio multimodal data types. Our dataset will be available at this https URL.</li>
</ul>

<h3>Title: Assimilation Matters: Model-level Backdoor Detection in Vision-Language Pretrained Models</h3>
<ul>
<li><strong>Authors: </strong>Zhongqi Wang, Jie Zhang, Shiguang Shan, Xilin Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00343">https://arxiv.org/abs/2512.00343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00343">https://arxiv.org/pdf/2512.00343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00343]] Assimilation Matters: Model-level Backdoor Detection in Vision-Language Pretrained Models(https://arxiv.org/abs/2512.00343)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Vision-language pretrained models (VLPs) such as CLIP have achieved remarkable success, but are also highly vulnerable to backdoor attacks. Given a model fine-tuned by an untrusted third party, determining whether the model has been injected with a backdoor is a critical and challenging problem. Existing detection methods usually rely on prior knowledge of training dataset, backdoor triggers and targets, or downstream classifiers, which may be impractical for real-world applications. To address this, To address this challenge, we introduce Assimilation Matters in DETection (AMDET), a novel model-level detection framework that operates without any such prior knowledge. Specifically, we first reveal the feature assimilation property in backdoored text encoders: the representations of all tokens within a backdoor sample exhibit a high similarity. Further analysis attributes this effect to the concentration of attention weights on the trigger token. Leveraging this insight, AMDET scans a model by performing gradient-based inversion on token embeddings to recover implicit features that capable of activating backdoor behaviors. Furthermore, we identify the natural backdoor feature in the OpenAI's official CLIP model, which are not intentionally injected but still exhibit backdoor-like behaviors. We then filter them out from real injected backdoor by analyzing their loss landscapes. Extensive experiments on 3,600 backdoored and benign-finetuned models with two attack paradigms and three VLP model structures show that AMDET detects backdoors with an F1 score of 89.90%. Besides, it achieves one complete detection in approximately 5 minutes on a RTX 4090 GPU and exhibits strong robustness against adaptive attacks. Code is available at: this https URL</li>
</ul>

<h3>Title: mmPred: Radar-based Human Motion Prediction in the Dark</h3>
<ul>
<li><strong>Authors: </strong>Junqiao Fan, Haocong Rao, Jiarui Zhang, Jianfei Yang, Lihua Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00345">https://arxiv.org/abs/2512.00345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00345">https://arxiv.org/pdf/2512.00345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00345]] mmPred: Radar-based Human Motion Prediction in the Dark(https://arxiv.org/abs/2512.00345)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Existing Human Motion Prediction (HMP) methods based on RGB-D cameras are sensitive to lighting conditions and raise privacy concerns, limiting their real-world applications such as firefighting and healthcare. Motivated by the robustness and privacy-preserving nature of millimeter-wave (mmWave) radar, this work introduces radar as a novel sensing modality for HMP, for the first time. Nevertheless, radar signals often suffer from specular reflections and multipath effects, resulting in noisy and temporally inconsistent measurements, such as body-part miss-detection. To address these radar-specific artifacts, we propose mmPred, the first diffusion-based framework tailored for radar-based HMP. mmPred introduces a dual-domain historical motion representation to guide the generation process, combining a Time-domain Pose Refinement (TPR) branch for learning fine-grained details and a Frequency-domain Dominant Motion (FDM) branch for capturing global motion trends and suppressing frame-level inconsistency. Furthermore, we design a Global Skeleton-relational Transformer (GST) as the diffusion backbone to model global inter-joint cooperation, enabling corrupted joints to dynamically aggregate information from others. Extensive experiments show that mmPred achieves state-of-the-art performance, outperforming existing methods by 8.6% on mmBody and 22% on mm-Fi.</li>
</ul>

<h3>Title: Sample-Efficient Tabular Self-Play for Offline Robust Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Na Li, Zewu Zheng, Wei Ni, Hangguan Shan, Wenjie Zhang, Xinyu Li</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00352">https://arxiv.org/abs/2512.00352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00352">https://arxiv.org/pdf/2512.00352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00352]] Sample-Efficient Tabular Self-Play for Offline Robust Reinforcement Learning(https://arxiv.org/abs/2512.00352)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-agent reinforcement learning (MARL), as a thriving field, explores how multiple agents independently make decisions in a shared dynamic environment. Due to environmental uncertainties, policies in MARL must remain robust to tackle the sim-to-real gap. We focus on robust two-player zero-sum Markov games (TZMGs) in offline settings, specifically on tabular robust TZMGs (RTZMGs). We propose a model-based algorithm (\textit{RTZ-VI-LCB}) for offline RTZMGs, which is optimistic robust value iteration combined with a data-driven Bernstein-style penalty term for robust value estimation. By accounting for distribution shifts in the historical dataset, the proposed algorithm establishes near-optimal sample complexity guarantees under partial coverage and environmental uncertainty. An information-theoretic lower bound is developed to confirm the tightness of our algorithm's sample complexity, which is optimal regarding both state and action spaces. To the best of our knowledge, RTZ-VI-LCB is the first to attain this optimality, sets a new benchmark for offline RTZMGs, and is validated experimentally.</li>
</ul>

<h3>Title: SMamDiff: Spatial Mamba for Stochastic Human Motion Prediction</h3>
<ul>
<li><strong>Authors: </strong>Junqiao Fan, Pengfei Liu, Haocong Rao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00355">https://arxiv.org/abs/2512.00355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00355">https://arxiv.org/pdf/2512.00355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00355]] SMamDiff: Spatial Mamba for Stochastic Human Motion Prediction(https://arxiv.org/abs/2512.00355)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With intelligent room-side sensing and service robots widely deployed, human motion prediction (HMP) is essential for safe, proactive assistance. However, many existing HMP methods either produce a single, deterministic forecast that ignores uncertainty or rely on probabilistic models that sacrifice kinematic plausibility. Diffusion models improve the accuracy-diversity trade-off but often depend on multi-stage pipelines that are costly for edge deployment. This work focuses on how to ensure spatial-temporal coherence within a single-stage diffusion model for HMP. We introduce SMamDiff, a Spatial Mamba-based Diffusion model with two novel designs: (i) a residual-DCT motion encoding that subtracts the last observed pose before a temporal DCT, reducing the first DC component ($f=0$) dominance and highlighting informative higher-frequency cues so the model learns how joints move rather than where they are; and (ii) a stickman-drawing spatial-mamba module that processes joints in an ordered, joint-by-joint manner, making later joints condition on earlier ones to induce long-range, cross-joint dependencies. On Human3.6M and HumanEva, these coherence mechanisms deliver state-of-the-art results among single-stage probabilistic HMP methods while using less latency and memory than multi-stage diffusion baselines.</li>
</ul>

<h3>Title: Learning Causal States Under Partial Observability and Perturbation</h3>
<ul>
<li><strong>Authors: </strong>Na Li, Hangguan Shan, Wei Ni, Wenjie Zhang, Xinyu Li, Yamin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00357">https://arxiv.org/abs/2512.00357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00357">https://arxiv.org/pdf/2512.00357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00357]] Learning Causal States Under Partial Observability and Perturbation(https://arxiv.org/abs/2512.00357)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>A critical challenge for reinforcement learning (RL) is making decisions based on incomplete and noisy observations, especially in perturbed and partially observable Markov decision processes (P$^2$OMDPs). Existing methods fail to mitigate perturbations while addressing partial observability. We propose \textit{Causal State Representation under Asynchronous Diffusion Model (CaDiff)}, a framework that enhances any RL algorithm by uncovering the underlying causal structure of P$^2$OMDPs. This is achieved by incorporating a novel asynchronous diffusion model (ADM) and a new bisimulation metric. ADM enables forward and reverse processes with different numbers of steps, thus interpreting the perturbation of P$^2$OMDP as part of the noise suppressed through diffusion. The bisimulation metric quantifies the similarity between partially observable environments and their causal counterparts. Moreover, we establish the theoretical guarantee of CaDiff by deriving an upper bound for the value function approximation errors between perturbed observations and denoised causal states, reflecting a principled trade-off between approximation errors of reward and transition-model. Experiments on Roboschool tasks show that CaDiff enhances returns by at least 14.18\% compared to baselines. CaDiff is the first framework that approximates causal states using diffusion models with both theoretical rigor and practicality.</li>
</ul>

<h3>Title: CourseTimeQA: A Lecture-Video Benchmark and a Latency-Constrained Cross-Modal Fusion Method for Timestamped QA</h3>
<ul>
<li><strong>Authors: </strong>Vsevolod Kovalev, Parteek Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00360">https://arxiv.org/abs/2512.00360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00360">https://arxiv.org/pdf/2512.00360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00360]] CourseTimeQA: A Lecture-Video Benchmark and a Latency-Constrained Cross-Modal Fusion Method for Timestamped QA(https://arxiv.org/abs/2512.00360)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We study timestamped question answering over educational lecture videos under a single-GPU latency/memory budget. Given a natural-language query, the system retrieves relevant timestamped segments and synthesizes a grounded answer. We present CourseTimeQA (52.3 h, 902 queries across six courses) and a lightweight, latency-constrained cross-modal retriever (CrossFusion-RAG) that combines frozen encoders, a learned 512->768 vision projection, shallow query-agnostic cross-attention over ASR and frames with a temporal-consistency regularizer, and a small cross-attentive reranker. On CourseTimeQA, CrossFusion-RAG improves nDCG@10 by 0.10 and MRR by 0.08 over a strong BLIP-2 retriever while achieving approximately 1.55 s median end-to-end latency on a single A100. Closest comparators (zero-shot CLIP multi-frame pooling; CLIP + cross-encoder reranker + MMR; learned late-fusion gating; text-only hybrid with cross-encoder reranking and its MMR variant; caption-augmented text retrieval; non-learned temporal smoothing) are evaluated under matched hardware and indexing. We report robustness across ASR noise (WER quartiles), diagnostics for temporal localization, and full training/tuning details to support reproducible comparison.</li>
</ul>

<h3>Title: MM-DETR: An Efficient Multimodal Detection Transformer with Mamba-Driven Dual-Granularity Fusion and Frequency-Aware Modality Adapters</h3>
<ul>
<li><strong>Authors: </strong>Jianhong Han, Yupei Wang, Yuan Zhang, Liang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00363">https://arxiv.org/abs/2512.00363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00363">https://arxiv.org/pdf/2512.00363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00363]] MM-DETR: An Efficient Multimodal Detection Transformer with Mamba-Driven Dual-Granularity Fusion and Frequency-Aware Modality Adapters(https://arxiv.org/abs/2512.00363)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Multimodal remote sensing object detection aims to achieve more accurate and robust perception under challenging conditions by fusing complementary information from different modalities. However, existing approaches that rely on attention-based or deformable convolution fusion blocks still struggle to balance performance and lightweight design. Beyond fusion complexity, extracting modality features with shared backbones yields suboptimal representations due to insufficient modality-specific modeling, whereas dual-stream architectures nearly double the parameter count, ultimately limiting practical deployment. To this end, we propose MM-DETR, a lightweight and efficient framework for multimodal object detection. Specifically, we propose a Mamba-based dual granularity fusion encoder that reformulates global interaction as channel-wise dynamic gating and leverages a 1D selective scan for efficient cross-modal modeling with linear complexity. Following this design, we further reinterpret multimodal fusion as a modality completion problem. A region-aware 2D selective scanning completion branch is introduced to recover modality-specific cues, supporting fine-grained fusion along a bidirectional pyramid pathway with minimal overhead. To further reduce parameter redundancy while retaining strong feature extraction capability, a lightweight frequency-aware modality adapter is inserted into the shared backbone. This adapter employs a spatial-frequency co-expert structure to capture modality-specific cues, while a pixel-wise router dynamically balances expert contributions for efficient spatial-frequency fusion. Extensive experiments conducted on four multimodal benchmark datasets demonstrate the effectiveness and generalization capability of the proposed method.</li>
</ul>

<h3>Title: Towards aligned body representations in vision models</h3>
<ul>
<li><strong>Authors: </strong>Andrey Gizdov, Andrea Procopio, Yichen Li, Daniel Harari, Tomer Ullman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00365">https://arxiv.org/abs/2512.00365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00365">https://arxiv.org/pdf/2512.00365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00365]] Towards aligned body representations in vision models(https://arxiv.org/abs/2512.00365)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Human physical reasoning relies on internal "body" representations - coarse, volumetric approximations that capture an object's extent and support intuitive predictions about motion and physics. While psychophysical evidence suggests humans use such coarse representations, their internal structure remains largely unknown. Here we test whether vision models trained for segmentation develop comparable representations. We adapt a psychophysical experiment conducted with 50 human participants to a semantic segmentation task and test a family of seven segmentation networks, varying in size. We find that smaller models naturally form human-like coarse body representations, whereas larger models tend toward overly detailed, fine-grain encodings. Our results demonstrate that coarse representations can emerge under limited computational resources, and that machine representations can provide a scalable path toward understanding the structure of physical reasoning in the brain.</li>
</ul>

<h3>Title: POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Wenshuo Chen, Haosen Li, Shaofeng Liang, Lei Wang, Haozhe Jia, Kaishen Yuan, Jieming Wu, Bowen Tian, Yutao Yue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00369">https://arxiv.org/abs/2512.00369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00369">https://arxiv.org/pdf/2512.00369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00369]] POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models(https://arxiv.org/abs/2512.00369)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>The Inversion-Denoising Paradigm, which is based on diffusion models, excels in diverse image editing and restoration tasks. We revisit its mechanism and reveal a critical, overlooked factor in reconstruction degradation: the approximate noise error. This error stems from approximating the noise at step t with the prediction at step t-1, resulting in severe error accumulation throughout the inversion process. We introduce Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS), which reformulates inversion from an error-compensation problem into an error-origin problem. Rather than optimizing embeddings or latent codes to offset accumulated drift, POLARIS treats the guidance scale {\omega} as a step-wise variable and derives a mathematically grounded formula to minimize inversion error at each step. Remarkably, POLARIS improves inversion latent quality with just one line of code. With negligible performance overhead, it substantially mitigates noise approximation errors and consistently improves the accuracy of downstream tasks.</li>
</ul>

<h3>Title: Measuring Memecoin Fragility</h3>
<ul>
<li><strong>Authors: </strong>Yuexin Xiang, SM Mahir Shazeed Rish, Qishuang Fu, Yuquan Li, Qin Wang, Tsz Hon Yuen, Jiangshan Yu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00377">https://arxiv.org/abs/2512.00377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00377">https://arxiv.org/pdf/2512.00377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00377]] Measuring Memecoin Fragility(https://arxiv.org/abs/2512.00377)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Memecoins, emerging from internet culture and community-driven narratives, have rapidly evolved into a unique class of crypto assets. Unlike technology-driven cryptocurrencies, their market dynamics are primarily shaped by viral social media diffusion, celebrity influence, and speculative capital inflows. To capture the distinctive vulnerabilities of these ecosystems, we present the first Memecoin Ecosystem Fragility Framework (ME2F). ME2F formalizes memecoin risks in three dimensions: i) Volatility Dynamics Score capturing persistent and extreme price swings together with spillover from base chains; ii) Whale Dominance Score quantifying ownership concentration among top holders; and iii) Sentiment Amplification Score measuring the impact of attention-driven shocks on market stability. We apply ME2F to representative tokens (over 65\% market share) and show that fragility is not evenly distributed across the ecosystem. Politically themed tokens such as TRUMP, MELANIA, and LIBRA concentrate the highest risks, combining volatility, ownership concentration, and sensitivity to sentiment shocks. Established memecoins such as DOGE, SHIB, and PEPE fall into an intermediate range. Benchmark tokens ETH and SOL remain consistently resilient due to deeper liquidity and institutional participation. Our findings provide the first ecosystem-level evidence of memecoin fragility and highlight governance implications for enhancing market resilience in the Web3 era.</li>
</ul>

<h3>Title: Efficient and Programmable Exploration of Synthesizable Chemical Space</h3>
<ul>
<li><strong>Authors: </strong>Shitong Luo, Connor W. Coley</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00384">https://arxiv.org/abs/2512.00384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00384">https://arxiv.org/pdf/2512.00384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00384]] Efficient and Programmable Exploration of Synthesizable Chemical Space(https://arxiv.org/abs/2512.00384)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The constrained nature of synthesizable chemical space poses a significant challenge for sampling molecules that are both synthetically accessible and possess desired properties. In this work, we present PrexSyn, an efficient and programmable model for molecular discovery within synthesizable chemical space. PrexSyn is based on a decoder-only transformer trained on a billion-scale datastream of synthesizable pathways paired with molecular properties, enabled by a real-time, high-throughput C++-based data generation engine. The large-scale training data allows PrexSyn to reconstruct the synthesizable chemical space nearly perfectly at a high inference speed and learn the association between properties and synthesizable molecules. Based on its learned property-pathway mappings, PrexSyn can generate synthesizable molecules that satisfy not only single-property conditions but also composite property queries joined by logical operators, thereby allowing users to ``program'' generation objectives. Moreover, by exploiting this property-based querying capability, PrexSyn can efficiently optimize molecules against black-box oracle functions via iterative query refinement, achieving higher sampling efficiency than even synthesis-agnostic baselines, making PrexSyn a powerful general-purpose molecular optimization tool. Overall, PrexSyn pushes the frontier of synthesizable molecular design by setting a new state of the art in synthesizable chemical space coverage, molecular sampling efficiency, and inference speed.</li>
</ul>

<h3>Title: EZ-SP: Fast and Lightweight Superpoint-Based 3D Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Louis Geist, Loic Landrieu, Damien Robert</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00385">https://arxiv.org/abs/2512.00385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00385">https://arxiv.org/pdf/2512.00385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00385]] EZ-SP: Fast and Lightweight Superpoint-Based 3D Segmentation(https://arxiv.org/abs/2512.00385)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Superpoint-based pipelines provide an efficient alternative to point- or voxel-based 3D semantic segmentation, but are often bottlenecked by their CPU-bound partition step. We propose a learnable, fully GPU partitioning algorithm that generates geometrically and semantically coherent superpoints 13$\times$ faster than prior methods. Our module is compact (under 60k parameters), trains in under 20 minutes with a differentiable surrogate loss, and requires no handcrafted features. Combine with a lightweight superpoint classifier, the full pipeline fits in $<$2 MB of VRAM, scales to multi-million-point scenes, and supports real-time inference. With 72$\times$ faster inference and 120$\times$ fewer parameters, EZ-SP matches the accuracy of point-based SOTA models across three domains: indoor scans (S3DIS), autonomous driving (KITTI-360), and aerial LiDAR (DALES). Code and pretrained models are accessible at this http URL.</li>
</ul>

<h3>Title: Solving Neural Min-Max Games: The Role of Architecture, Initialization & Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Deep Patel, Emmanouil-Vasileios Vlatakis-Gkaragkounis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00389">https://arxiv.org/abs/2512.00389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00389">https://arxiv.org/pdf/2512.00389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00389]] Solving Neural Min-Max Games: The Role of Architecture, Initialization & Dynamics(https://arxiv.org/abs/2512.00389)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Many emerging applications - such as adversarial training, AI alignment, and robust optimization - can be framed as zero-sum games between neural nets, with von Neumann-Nash equilibria (NE) capturing the desirable system behavior. While such games often involve non-convex non-concave objectives, empirical evidence shows that simple gradient methods frequently converge, suggesting a hidden geometric structure. In this paper, we provide a theoretical framework that explains this phenomenon through the lens of hidden convexity and overparameterization. We identify sufficient conditions - spanning initialization, training dynamics, and network width - that guarantee global convergence to a NE in a broad class of non-convex min-max games. To our knowledge, this is the first such result for games that involve two-layer neural networks. Technically, our approach is twofold: (a) we derive a novel path-length bound for the alternating gradient descent-ascent scheme in min-max games; and (b) we show that the reduction from a hidden convex-concave geometry to two-sided Polyak-Łojasiewicz (PŁ) min-max condition hold with high probability under overparameterization, using tools from random matrix theory.</li>
</ul>

<h3>Title: Mitigating the Threshold Priming Effect in Large Language Model-Based Relevance Judgments via Personality Infusing</h3>
<ul>
<li><strong>Authors: </strong>Nuo Chen, Hanpei Fang, Jiqun Liu, Wilson Wei, Tetsuya Sakai, Xiao-Ming Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00390">https://arxiv.org/abs/2512.00390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00390">https://arxiv.org/pdf/2512.00390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00390]] Mitigating the Threshold Priming Effect in Large Language Model-Based Relevance Judgments via Personality Infusing(https://arxiv.org/abs/2512.00390)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent research has explored LLMs as scalable tools for relevance labeling, but studies indicate they are susceptible to priming effects, where prior relevance judgments influence later ones. Although psychological theories link personality traits to such biases, it is unclear whether simulated personalities in LLMs exhibit similar effects. We investigate how Big Five personality profiles in LLMs influence priming in relevance labeling, using multiple LLMs on TREC 2021 and 2022 Deep Learning Track datasets. Our results show that certain profiles, such as High Openness and Low Neuroticism, consistently reduce priming susceptibility. Additionally, the most effective personality in mitigating priming may vary across models and task types. Based on these findings, we propose personality prompting as a method to mitigate threshold priming, connecting psychological evidence with LLM-based evaluation practices.</li>
</ul>

<h3>Title: A Taxonomy of Errors in English as she is spoke: Toward an AI-Based Method of Error Analysis for EFL Writing Instruction</h3>
<ul>
<li><strong>Authors: </strong>Damian Heywood, Joseph Andrew Carrier, Kyu-Hong Hwang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00392">https://arxiv.org/abs/2512.00392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00392">https://arxiv.org/pdf/2512.00392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00392]] A Taxonomy of Errors in English as she is spoke: Toward an AI-Based Method of Error Analysis for EFL Writing Instruction(https://arxiv.org/abs/2512.00392)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study describes the development of an AI-assisted error analysis system designed to identify, categorize, and correct writing errors in English. Utilizing Large Language Models (LLMs) like Claude 3.5 Sonnet and DeepSeek R1, the system employs a detailed taxonomy grounded in linguistic theories from Corder (1967), Richards (1971), and James (1998). Errors are classified at both word and sentence levels, covering spelling, grammar, and punctuation. Implemented through Python-coded API calls, the system provides granular feedback beyond traditional rubric-based assessments. Initial testing on isolated errors refined the taxonomy, addressing challenges like overlapping categories. Final testing used "English as she is spoke" by Jose da Fonseca (1855), a text rich with authentic linguistic errors, to evaluate the system's capacity for handling complex, multi-layered analysis. The AI successfully identified diverse error types but showed limitations in contextual understanding and occasionally generated new error categories when encountering uncoded errors. This research demonstrates AI's potential to transform EFL instruction by automating detailed error analysis and feedback. While promising, further development is needed to improve contextual accuracy and expand the taxonomy to stylistic and discourse-level errors.</li>
</ul>

<h3>Title: Better, Stronger, Faster: Tackling the Trilemma in MLLM-based Segmentation with Simultaneous Textual Mask Prediction</h3>
<ul>
<li><strong>Authors: </strong>Jiazhen Liu, Mingkuan Feng, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00395">https://arxiv.org/abs/2512.00395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00395">https://arxiv.org/pdf/2512.00395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00395]] Better, Stronger, Faster: Tackling the Trilemma in MLLM-based Segmentation with Simultaneous Textual Mask Prediction(https://arxiv.org/abs/2512.00395)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Integrating segmentation into Multimodal Large Language Models (MLLMs) presents a core trilemma: simultaneously preserving dialogue ability, achieving high segmentation performance, and ensuring fast inference. Prevailing paradigms are forced into a compromise. Embedding prediction methods introduce a conflicting pixel-level objective that degrades the MLLM's general dialogue abilities. The alternative, next-token prediction, reframes segmentation as an autoregressive task, which preserves dialogue but forces a trade-off between poor segmentation performance with sparse outputs or prohibitive inference speeds with rich ones. We resolve this trilemma with all-mask prediction, a novel paradigm that decouples autoregressive dialogue generation from non-autoregressive mask prediction. We present STAMP: Simultaneous Textual All-Mask Prediction, an MLLM that embodies this paradigm. After generating a textual response, STAMP predicts an entire segmentation mask in a single forward pass by treating it as a parallel "fill-in-the-blank" task over image patches. This design maintains the MLLM's dialogue ability by avoiding conflicting objectives, enables high segmentation performance by leveraging rich, bidirectional spatial context for all mask tokens, and achieves exceptional speed. Extensive experiments show that STAMP significantly outperforms state-of-the-art methods across multiple segmentation benchmarks, providing a solution that excels in dialogue, segmentation, and speed without compromise.</li>
</ul>

<h3>Title: Low-Bitrate Video Compression through Semantic-Conditioned Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Lingdong Wang, Guan-Ming Su, Divya Kothandaraman, Tsung-Wei Huang, Mohammad Hajiesmaili, Ramesh K. Sitaraman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00408">https://arxiv.org/abs/2512.00408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00408">https://arxiv.org/pdf/2512.00408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00408]] Low-Bitrate Video Compression through Semantic-Conditioned Diffusion(https://arxiv.org/abs/2512.00408)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Traditional video codecs optimized for pixel fidelity collapse at ultra-low bitrates and produce severe artifacts. This failure arises from a fundamental misalignment between pixel accuracy and human perception. We propose a semantic video compression framework named DiSCo that transmits only the most meaningful information while relying on generative priors for detail synthesis. The source video is decomposed into three compact modalities: a textual description, a spatiotemporally degraded video, and optional sketches or poses that respectively capture semantic, appearance, and motion cues. A conditional video diffusion model then reconstructs high-quality, temporally coherent videos from these compact representations. Temporal forward filling, token interleaving, and modality-specific codecs are proposed to improve multimodal generation and modality compactness. Experiments show that our method outperforms baseline semantic and traditional codecs by 2-10X on perceptual metrics at low bitrates.</li>
</ul>

<h3>Title: Red Teaming Large Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Chen, Yang Yang, Chao Yu, Yu Tian, Zhi Cao, Linghao Li, Hang Su, Zhaoxia Yin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00412">https://arxiv.org/abs/2512.00412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00412">https://arxiv.org/pdf/2512.00412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00412]] Red Teaming Large Reasoning Models(https://arxiv.org/abs/2512.00412)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Reasoning Models (LRMs) have emerged as a powerful advancement in multi-step reasoning tasks, offering enhanced transparency and logical consistency through explicit chains of thought (CoT). However, these models introduce novel safety and reliability risks, such as CoT-hijacking and prompt-induced inefficiencies, which are not fully captured by existing evaluation methods. To address this gap, we propose RT-LRM, a unified benchmark designed to assess the trustworthiness of LRMs. RT-LRM evaluates three core dimensions: truthfulness, safety and efficiency. Beyond metric-based evaluation, we further introduce the training paradigm as a key analytical perspective to investigate the systematic impact of different training strategies on model trustworthiness. We achieve this by designing a curated suite of 30 reasoning tasks from an observational standpoint. We conduct extensive experiments on 26 models and identify several valuable insights into the trustworthiness of LRMs. For example, LRMs generally face trustworthiness challenges and tend to be more fragile than Large Language Models (LLMs) when encountering reasoning-induced risks. These findings uncover previously underexplored vulnerabilities and highlight the need for more targeted evaluations. In addition, we release a scalable toolbox for standardized trustworthiness research to support future advancements in this important field. Our code and datasets will be open-sourced.</li>
</ul>

<h3>Title: SplatFont3D: Structure-Aware Text-to-3D Artistic Font Generation with Part-Level Style Control</h3>
<ul>
<li><strong>Authors: </strong>Ji Gan, Lingxu Chen, Jiaxu Leng, Xinbo Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00413">https://arxiv.org/abs/2512.00413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00413">https://arxiv.org/pdf/2512.00413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00413]] SplatFont3D: Structure-Aware Text-to-3D Artistic Font Generation with Part-Level Style Control(https://arxiv.org/abs/2512.00413)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Artistic font generation (AFG) can assist human designers in creating innovative artistic fonts. However, most previous studies primarily focus on 2D artistic fonts in flat design, leaving personalized 3D-AFG largely underexplored. 3D-AFG not only enables applications in immersive 3D environments such as video games and animations, but also may enhance 2D-AFG by rendering 2D fonts of novel views. Moreover, unlike general 3D objects, 3D fonts exhibit precise semantics with strong structural constraints and also demand fine-grained part-level style control. To address these challenges, we propose SplatFont3D, a novel structure-aware text-to-3D AFG framework with 3D Gaussian splatting, which enables the creation of 3D artistic fonts from diverse style text prompts with precise part-level style control. Specifically, we first introduce a Glyph2Cloud module, which progressively enhances both the shapes and styles of 2D glyphs (or components) and produces their corresponding 3D point clouds for Gaussian initialization. The initialized 3D Gaussians are further optimized through interaction with a pretrained 2D diffusion model using score distillation sampling. To enable part-level control, we present a dynamic component assignment strategy that exploits the geometric priors of 3D Gaussians to partition components, while alleviating drift-induced entanglement during 3D Gaussian optimization. Our SplatFont3D provides more explicit and effective part-level style control than NeRF, attaining faster rendering efficiency. Experiments show that our SplatFont3D outperforms existing 3D models for 3D-AFG in style-text consistency, visual quality, and rendering efficiency.</li>
</ul>

<h3>Title: BEACON: Automatic Container Policy Generation using Environment-aware Dynamic Analysis</h3>
<ul>
<li><strong>Authors: </strong>Haney Kang, Eduard Marin, Myoungsung You, Diego Perino, Seungwon Shin, Jinwoo Kim</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00414">https://arxiv.org/abs/2512.00414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00414">https://arxiv.org/pdf/2512.00414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00414]] BEACON: Automatic Container Policy Generation using Environment-aware Dynamic Analysis(https://arxiv.org/abs/2512.00414)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>This paper introduces BeaCon, a novel tool for the automated generation of adjustable container security policies. Unlike prior approaches, BeaCon leverages dynamic analysis to simulate realistic environments, uncovering container execution paths that may remain hidden during the profiling phase. To address the challenge of exploring vast profiling spaces, we employ efficient heuristics to reveal additional system events with minimal effort. In addition, BeaCon incorporates a security and functionality scoring mechanism to prioritize system calls and capabilities based on their impact on the host OS kernel's security and the functionality of containerized applications. By integrating these scores, BeaCon achieves a customized balance between security and functionality, enabling cloud providers to enforce security measures while maintaining tenant availability. We implemented a prototype of BeaCon using eBPF kernel technology and conducted extensive evaluations. Results from the top 15 containers, which revealed significant improvements, demonstrate that BeaCon identifies an average of 16.5% additional syscalls by applying diverse environments. Furthermore, we evaluated its effectiveness in mitigating risks associated with 45 known vulnerabilities (e.g., CVEs), showcasing its potential to significantly enhance container security. Additionally, we performed proof-of-concept demonstrations for two well-known security vulnerabilities, showing that BeaCon successfully reduces attack surface by blocking these exploits.</li>
</ul>

<h3>Title: CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Guo, Suozhi Huang, Zixin Yao, Yifan Zhang, Yifu Lu, Jiashuo Liu, Zihao Li, Yanyan Deng, Qixin Xiao, Jia Tian, Kanghong Zhan, Tianyi Li, Xiaochen Liu, Jason Ge, Chaoyang He, Kaixuan Huang, Lin Yang, Wenhao Huang, Mengdi Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00417">https://arxiv.org/abs/2512.00417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00417">https://arxiv.org/pdf/2512.00417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00417]] CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency(https://arxiv.org/abs/2512.00417)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \emph{extreme time-sensitivity}, \emph{a highly adversarial information environment}, and the critical need to synthesize data from \emph{diverse, specialized sources}, such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills. Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a \textit{retrieval-prediction imbalance}, where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.</li>
</ul>

<h3>Title: TrendGNN: Towards Understanding of Epidemics, Beliefs, and Behaviors</h3>
<ul>
<li><strong>Authors: </strong>Mulin Tian, Ajitesh Srivastava</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00421">https://arxiv.org/abs/2512.00421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00421">https://arxiv.org/pdf/2512.00421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00421]] TrendGNN: Towards Understanding of Epidemics, Beliefs, and Behaviors(https://arxiv.org/abs/2512.00421)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Epidemic outcomes have a complex interplay with human behavior and beliefs. Most of the forecasting literature has focused on the task of predicting epidemic signals using simple mechanistic models or black-box models, such as deep transformers, that ingest all available signals without offering interpretability. However, to better understand the mechanisms and predict the impact of interventions, we need the ability to forecast signals associated with beliefs and behaviors in an interpretable manner. In this work, we propose a graph-based forecasting framework that first constructs a graph of interrelated signals based on trend similarity, and then applies graph neural networks (GNNs) for prediction. This approach enables interpretable analysis by revealing which signals are more predictable and which relationships contribute most to forecasting accuracy. We believe our method provides early steps towards a framework for interpretable modeling in domains with multiple potentially interdependent signals, with implications for building future simulation models that integrate behavior, beliefs, and observations.</li>
</ul>

<h3>Title: PhysGen: Physically Grounded 3D Shape Generation for Industrial Design</h3>
<ul>
<li><strong>Authors: </strong>Yingxuan You, Chen Zhao, Hantao Zhang, Mingda Xu, Pascal Fua</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00422">https://arxiv.org/abs/2512.00422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00422">https://arxiv.org/pdf/2512.00422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00422]] PhysGen: Physically Grounded 3D Shape Generation for Industrial Design(https://arxiv.org/abs/2512.00422)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Existing generative models for 3D shapes can synthesize high-fidelity and visually plausible shapes. For certain classes of shapes that have undergone an engineering design process, the realism of the shape is tightly coupled with the underlying physical properties, e.g., aerodynamic efficiency for automobiles. Since existing methods lack knowledge of such physics, they are unable to use this knowledge to enhance the realism of shape generation. Motivated by this, we propose a unified physics-based 3D shape generation pipeline, with a focus on industrial design applications. Specifically, we introduce a new flow matching model with explicit physical guidance, consisting of an alternating update process. We iteratively perform a velocity-based update and a physics-based refinement, progressively adjusting the latent code to align with the desired 3D shapes and physical properties. We further strengthen physical validity by incorporating a physics-aware regularization term into the velocity-based update step. To support such physics-guided updates, we build a shape-and-physics variational autoencoder (SP-VAE) that jointly encodes shape and physics information into a unified latent space. The experiments on three benchmarks show that this synergistic formulation improves shape realism beyond mere visual plausibility.</li>
</ul>

<h3>Title: Recovering Origin Destination Flows from Bus CCTV: Early Results from Nairobi and Kigali</h3>
<ul>
<li><strong>Authors: </strong>Nthenya Kyatha, Jay Taneja</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00424">https://arxiv.org/abs/2512.00424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00424">https://arxiv.org/pdf/2512.00424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00424]] Recovering Origin Destination Flows from Bus CCTV: Early Results from Nairobi and Kigali(https://arxiv.org/abs/2512.00424)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Public transport in sub-Saharan Africa (SSA) often operates in overcrowded conditions where existing automated systems fail to capture reliable passenger flow data. Leveraging onboard CCTV already deployed for security, we present a baseline pipeline that combines YOLOv12 detection, BotSORT tracking, OSNet embeddings, OCR-based timestamping, and telematics-based stop classification to recover bus origin--destination (OD) flows. On annotated CCTV segments from Nairobi and Kigali buses, the system attains high counting accuracy under low-density, well-lit conditions (recall $\approx$95\%, precision $\approx$91\%, F1 $\approx$93\%). It produces OD matrices that closely match manual tallies. Under realistic stressors such as overcrowding, color-to-monochrome shifts, posture variation, and non-standard door use, performance degrades sharply (e.g., $\sim$40\% undercount in peak-hour boarding and a $\sim$17 percentage-point drop in recall for monochrome segments), revealing deployment-specific failure modes and motivating more robust, deployment-focused Re-ID methods for SSA transit.</li>
</ul>

<h3>Title: What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards</h3>
<ul>
<li><strong>Authors: </strong>Minh-Quan Le, Yuanzhi Zhu, Vicky Kalogeiton, Dimitris Samaras</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00425">https://arxiv.org/abs/2512.00425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00425">https://arxiv.org/pdf/2512.00425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00425]] What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards(https://arxiv.org/abs/2512.00425)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent video diffusion models can synthesize visually compelling clips, yet often violate basic physical laws-objects float, accelerations drift, and collisions behave inconsistently-revealing a persistent gap between visual realism and physical realism. We propose $\texttt{NewtonRewards}$, the first physics-grounded post-training framework for video generation based on $\textit{verifiable rewards}$. Instead of relying on human or VLM feedback, $\texttt{NewtonRewards}$ extracts $\textit{measurable proxies}$ from generated videos using frozen utility models: optical flow serves as a proxy for velocity, while high-level appearance features serve as a proxy for mass. These proxies enable explicit enforcement of Newtonian structure through two complementary rewards: a Newtonian kinematic constraint enforcing constant-acceleration dynamics, and a mass conservation reward preventing trivial, degenerate solutions. We evaluate $\texttt{NewtonRewards}$ on five Newtonian Motion Primitives (free fall, horizontal/parabolic throw, and ramp sliding down/up) using our newly constructed large-scale benchmark, $\texttt{NewtonBench-60K}$. Across all primitives in visual and physics metrics, $\texttt{NewtonRewards}$ consistently improves physical plausibility, motion smoothness, and temporal coherence over prior post-training methods. It further maintains strong performance under out-of-distribution shifts in height, speed, and friction. Our results show that physics-grounded verifiable rewards offer a scalable path toward physics-aware video generation.</li>
</ul>

<h3>Title: Privacy-Preserving Generative Modeling and Clinical Validation of Longitudinal Health Records for Chronic Disease</h3>
<ul>
<li><strong>Authors: </strong>Benjamin D. Ballyk, Ankit Gupta, Sujay Konda, Kavitha Subramanian, Chris Landon, Ahmed Ammar Naseer, Georg Maierhofer, Sumanth Swaminathan, Vasudevan Venkateshwaran</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00434">https://arxiv.org/abs/2512.00434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00434">https://arxiv.org/pdf/2512.00434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00434]] Privacy-Preserving Generative Modeling and Clinical Validation of Longitudinal Health Records for Chronic Disease(https://arxiv.org/abs/2512.00434)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>Data privacy is a critical challenge in modern medical workflows as the adoption of electronic patient records has grown rapidly. Stringent data protection regulations limit access to clinical records for training and integrating machine learning models that have shown promise in improving diagnostic accuracy and personalized care outcomes. Synthetic data offers a promising alternative; however, current generative models either struggle with time-series data or lack formal privacy guaranties. In this paper, we enhance a state-of-the-art time-series generative model to better handle longitudinal clinical data while incorporating quantifiable privacy safeguards. Using real data from chronic kidney disease and ICU patients, we evaluate our method through statistical tests, a Train-on-Synthetic-Test-on-Real (TSTR) setup, and expert clinical review. Our non-private model (Augmented TimeGAN) outperforms transformer- and flow-based models on statistical metrics in several datasets, while our private model (DP-TimeGAN) maintains a mean authenticity of 0.778 on the CKD dataset, outperforming existing state-of-the-art models on the privacy-utility frontier. Both models achieve performance comparable to real data in clinician evaluations, providing robust input data necessary for developing models for complex chronic conditions without compromising data privacy.</li>
</ul>

<h3>Title: RECTor: Robust and Efficient Correlation Attack on Tor</h3>
<ul>
<li><strong>Authors: </strong>Binghui Wu, Dinil Mon Divakaran, Levente Csikor, Mohan Gurusamy</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00436">https://arxiv.org/abs/2512.00436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00436">https://arxiv.org/pdf/2512.00436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00436]] RECTor: Robust and Efficient Correlation Attack on Tor(https://arxiv.org/abs/2512.00436)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Tor is a widely used anonymity network that conceals user identities by routing traffic through encrypted relays, yet it remains vulnerable to traffic correlation attacks that deanonymize users by matching patterns in ingress and egress traffic. However, existing correlation methods suffer from two major limitations: limited robustness to noise and partial observations, and poor scalability due to computationally expensive pairwise matching. To address these challenges, we propose RECTor, a machine learning-based framework for traffic correlation under realistic conditions. RECTor employs attention-based Multiple Instance Learning (MIL) and GRU-based temporal encoding to extract robust flow representations, even when traffic data is incomplete or obfuscated. These embeddings are mapped into a shared space via a Siamese network and efficiently matched using approximate nearest neighbor (aNN) search. Empirical evaluations show that RECTor outperforms state-of-the-art baselines such as DeepCorr, DeepCOFFEA, and FlowTracker, achieving up to 60% higher true positive rates under high-noise conditions and reducing training and inference time by over 50%. Moreover, RECTor demonstrates strong scalability: inference cost grows near-linearly as the number of flows increases. These findings reveal critical vulnerabilities in Tor's anonymity model and highlight the need for advanced model-aware defenses.</li>
</ul>

<h3>Title: SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling</h3>
<ul>
<li><strong>Authors: </strong>Yang Xiao, Chunpu Xu, Ruifeng Yuan, Jiashuo Wang, Wenjie Li, Pengfei Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00466">https://arxiv.org/abs/2512.00466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00466">https://arxiv.org/pdf/2512.00466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00466]] SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling(https://arxiv.org/abs/2512.00466)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Test-time compute scaling has emerged as a powerful paradigm for enhancing mathematical reasoning in large language models (LLMs) by allocating additional computational resources during inference. However, current methods employ uniform resource distribution across all reasoning sub-problems, creating fundamental bottlenecks where challenging sub-problems receive insufficient attention while routine operations consume disproportionate resources. This uniform allocation creates performance bottlenecks where additional computational resources yield diminishing returns. Inspired by dual-process theory, we propose \textbf{SCALE} (Selective Resource Allocation), a framework that selectively allocates computational resources based on sub-problem difficulty. SCALE operates through four stages: (1) problem decomposition into sequential reasoning sub-problems, (2) difficulty assessment of each sub-problem to distinguish between routine operations and computationally challenging sub-problems, (3) selective processing mode assignment between System 1 for simple sub-problems and System 2 for complex ones, and (4) sequential execution with context propagation. By concentrating resources on challenging sub-problems while processing routine operations efficiently, SCALE achieves substantial performance improvements with superior resource utilization. Extensive experiments demonstrate that SCALE significantly outperforms uniform scaling baselines, achieving accuracy improvements of up to 13.75 percentage points (57.50% to 71.25% on AIME25) while reducing computational costs by 33%-53%, representing a major advance in test-time scaling that addresses fundamental limitations of current approaches.</li>
</ul>

<h3>Title: FairMT: Fairness for Heterogeneous Multi-Task Learning</h3>
<ul>
<li><strong>Authors: </strong>Guanyu Hu, Tangzheng Lian, Na Yan, Dimitrios Kollias, Xinyu Yang, Oya Celiktutan, Siyang Song, Zeyu Fu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00469">https://arxiv.org/abs/2512.00469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00469">https://arxiv.org/pdf/2512.00469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00469]] FairMT: Fairness for Heterogeneous Multi-Task Learning(https://arxiv.org/abs/2512.00469)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Fairness in machine learning has been extensively studied in single-task settings, while fair multi-task learning (MTL), especially with heterogeneous tasks (classification, detection, regression) and partially missing labels, remains largely unexplored. Existing fairness methods are predominantly classification-oriented and fail to extend to continuous outputs, making a unified fairness objective difficult to formulate. Further, existing MTL optimization is structurally misaligned with fairness: constraining only the shared representation, allowing task heads to absorb bias and leading to uncontrolled task-specific disparities. Finally, most work treats fairness as a zero-sum trade-off with utility, enforcing symmetric constraints that achieve parity by degrading well-served groups. We introduce FairMT, a unified fairness-aware MTL framework that accommodates all three task types under incomplete supervision. At its core is an Asymmetric Heterogeneous Fairness Constraint Aggregation mechanism, which consolidates task-dependent asymmetric violations into a unified fairness constraint. Utility and fairness are jointly optimized via a primal--dual formulation, while a head-aware multi-objective optimization proxy provides a tractable descent geometry that explicitly accounts for head-induced anisotropy. Across three homogeneous and heterogeneous MTL benchmarks encompassing diverse modalities and supervision regimes, FairMT consistently achieves substantial fairness gains while maintaining superior task utility. Code will be released upon paper acceptance.</li>
</ul>

<h3>Title: RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards</h3>
<ul>
<li><strong>Authors: </strong>Junyan Ye, Leiqi Zhu, Yuncheng Guo, Dongzhi Jiang, Zilong Huang, Yifan Zhang, Zhiyuan Yan, Haohuan Fu, Conghui He, Weijia Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00473">https://arxiv.org/abs/2512.00473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00473">https://arxiv.org/pdf/2512.00473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00473]] RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards(https://arxiv.org/abs/2512.00473)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With the continuous advancement of image generation technology, advanced models such as GPT-Image-1 and Qwen-Image have achieved remarkable text-to-image consistency and world knowledge However, these models still fall short in photorealistic image generation. Even on simple T2I tasks, they tend to produce " fake" images with distinct AI artifacts, often characterized by "overly smooth skin" and "oily facial sheens". To recapture the original goal of "indistinguishable-from-reality" generation, we propose RealGen, a photorealistic text-to-image framework. RealGen integrates an LLM component for prompt optimization and a diffusion model for realistic image generation. Inspired by adversarial generation, RealGen introduces a "Detector Reward" mechanism, which quantifies artifacts and assesses realism using both semantic-level and feature-level synthetic image detectors. We leverage this reward signal with the GRPO algorithm to optimize the entire generation pipeline, significantly enhancing image realism and detail. Furthermore, we propose RealBench, an automated evaluation benchmark employing Detector-Scoring and Arena-Scoring. It enables human-free photorealism assessment, yielding results that are more accurate and aligned with real user experience. Experiments demonstrate that RealGen significantly outperforms general models like GPT-Image-1 and Qwen-Image, as well as specialized photorealistic models like FLUX-Krea, in terms of realism, detail, and aesthetics. The code is available at this https URL.</li>
</ul>

<h3>Title: Structured Context Learning for Generic Event Boundary Detection</h3>
<ul>
<li><strong>Authors: </strong>Xin Gu, Congcong Li, Xinyao Wang, Dexiang Hong, Libo Zhang, Tiejian Luo, Longyin Wen, Heng Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00475">https://arxiv.org/abs/2512.00475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00475">https://arxiv.org/pdf/2512.00475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00475]] Structured Context Learning for Generic Event Boundary Detection(https://arxiv.org/abs/2512.00475)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Generic Event Boundary Detection (GEBD) aims to identify moments in videos that humans perceive as event boundaries. This paper proposes a novel method for addressing this task, called Structured Context Learning, which introduces the Structured Partition of Sequence (SPoS) to provide a structured context for learning temporal information. Our approach is end-to-end trainable and flexible, not restricted to specific temporal models like GRU, LSTM, and Transformers. This flexibility enables our method to achieve a better speed-accuracy trade-off. Specifically, we apply SPoS to partition the input frame sequence and provide a structured context for the subsequent temporal model. Notably, SPoS's overall computational complexity is linear with respect to the video length. We next calculate group similarities to capture differences between frames, and a lightweight fully convolutional network is utilized to determine the event boundaries based on the grouped similarity maps. To remedy the ambiguities of boundary annotations, we adapt the Gaussian kernel to preprocess the ground-truth event boundaries. Our proposed method has been extensively evaluated on the challenging Kinetics-GEBD, TAPOS, and shot transition detection datasets, demonstrating its superiority over existing state-of-the-art methods.</li>
</ul>

<h3>Title: A Unified Framework for Constructing Information-Theoretic Private Information Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Liang Feng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00480">https://arxiv.org/abs/2512.00480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00480">https://arxiv.org/pdf/2512.00480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00480]] A Unified Framework for Constructing Information-Theoretic Private Information Retrieval(https://arxiv.org/abs/2512.00480)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>Retrieving up-to-date information from a publicly accessible database poses significant threats to the user's privacy. {\em Private information retrieval} (PIR) protocols allow a user to retrieve any entry from a database, without revealing the identity of the entry being retrieved to the server(s). Such protocols have found numerous applications in both theoretical studies and real-life scenarios. The existing PIR constructions mainly give multi-server {\em information-theoretic} PIR (IT-PIR) protocols or single-server computational PIR (CPIR) protocols. Compared with CPIR, IT-PIR protocols are computationally more efficient and secure in the presence of unbounded servers. The most classical and challenging problem in the realm of IT-PIR is constructing protocols with lower {\em communication complexity}. In this review, we introduce a new discrete structure called {\em families of orthogonal arrays with span capability} (FOASC) and propose a unified framework for constructing IT-PIR protocols. We show how the most influential IT-PIR protocols in the literature can be captured by the framework. We also put forward several interesting open problems concerning FOASC, whose solutions may result in innovative IT-PIR protocols.</li>
</ul>

<h3>Title: Learning What Helps: Task-Aligned Context Selection for Vision Tasks</h3>
<ul>
<li><strong>Authors: </strong>Jingyu Guo, Emir Konuk, Fredrik Strand, Christos Matsoukas, Kevin Smith</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00489">https://arxiv.org/abs/2512.00489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00489">https://arxiv.org/pdf/2512.00489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00489]] Learning What Helps: Task-Aligned Context Selection for Vision Tasks(https://arxiv.org/abs/2512.00489)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Humans often resolve visual uncertainty by comparing an image with relevant examples, but ViTs lack the ability to identify which examples would improve their predictions. We present Task-Aligned Context Selection (TACS), a framework that learns to select paired examples which truly improve task performance rather than those that merely appear similar. TACS jointly trains a selector network with the task model through a hybrid optimization scheme combining gradient-based supervision and reinforcement learning, making retrieval part of the learning objective. By aligning selection with task rewards, TACS enables discriminative models to discover which contextual examples genuinely help. Across 18 datasets covering fine-grained recognition, medical image classification, and medical image segmentation, TACS consistently outperforms similarity-based retrieval, particularly in challenging or data-limited settings.</li>
</ul>

<h3>Title: ESPO: Entropy Importance Sampling Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yuepeng Sheng, Yuwei Huang, Shuman Liu, Haibo Zhang, Anxiang Zeng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00499">https://arxiv.org/abs/2512.00499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00499">https://arxiv.org/pdf/2512.00499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00499]] ESPO: Entropy Importance Sampling Policy Optimization(https://arxiv.org/abs/2512.00499)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) reinforcement learning has increasingly relied on group-based policy optimization frameworks, such as GRPO and GSPO, to achieve stable fine-tuning at scale. However, a fundamental trade-off persists between optimization granularity and training stability. While GSPO improves robustness via sequence-level optimization, its monolithic treatment of sequences introduces severe inefficiencies: its conservative clipping mechanism indiscriminately discards valid training samples-a phenomenon we term gradient underutilization-and its uniform credit assignment fails to capture the heterogeneous contributions of critical reasoning steps. In this work, we propose Entropy Importance Sampling Policy Optimization (ESPO), a novel framework that reconciles fine-grained control with training stability. ESPO decomposes sequences into groups based on predictive entropy, enabling (1) Entropy-driven Importance Sampling to capture intra-sequence heterogeneity, and (2) Entropy-adaptive Clipping to dynamically allocate trust regions based on model uncertainty. Extensive experiments on mathematical reasoning benchmarks demonstrate that ESPO not only accelerates convergence but also achieves state-of-the-art performance, notably improving accuracy on the challenging HMMT benchmark from 4.4% to 13.13%.</li>
</ul>

<h3>Title: G-KV: Decoding-Time KV Cache Eviction with Global Attention</h3>
<ul>
<li><strong>Authors: </strong>Mengqi Liao, Lu Wang, Chaoyun Zhang, Zekai Shen, Xiaowei Mao, Si Qin, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Huaiyu Wan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00504">https://arxiv.org/abs/2512.00504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00504">https://arxiv.org/pdf/2512.00504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00504]] G-KV: Decoding-Time KV Cache Eviction with Global Attention(https://arxiv.org/abs/2512.00504)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent reasoning large language models (LLMs) excel in complex tasks but encounter significant computational and memory challenges due to long sequence lengths. KV cache compression has emerged as an effective approach to greatly enhance the efficiency of reasoning. However, existing methods often focus on prompt compression or token eviction with local attention score, overlooking the long-term importance of tokens. We propose G-KV, a KV cache eviction method that employs a global scoring mechanism, combining local and historical attention scores to more accurately assess token importance. Additionally, we introduce post-training techniques, including reinforcement learning and distillation, to optimize models for compressed KV cache settings. The code of this paper is available on: this https URL.</li>
</ul>

<h3>Title: Terrain Sensing with Smartphone Structured Light: 2D Dynamic Time Warping for Grid Pattern Matching</h3>
<ul>
<li><strong>Authors: </strong>Tanaka Nobuaki</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00514">https://arxiv.org/abs/2512.00514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00514">https://arxiv.org/pdf/2512.00514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00514]] Terrain Sensing with Smartphone Structured Light: 2D Dynamic Time Warping for Grid Pattern Matching(https://arxiv.org/abs/2512.00514)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Low-cost mobile rovers often operate on uneven terrain where small bumps or tilts are difficult to perceive visually but can significantly affect locomotion stability. To address this problem, we explore a smartphone-based structured-light system that projects a grid pattern onto the ground and reconstructs local terrain unevenness from a single handheld device. The system is inspired by face-recognition projectors, but adapted for ground sensing. A key technical challenge is robustly matching the projected grid with its deformed observation under perspective distortion and partial occlusion. Conventional one-dimensional dynamic time warping (1D-DTW) is not directly applicable to such two-dimensional grid patterns. We therefore propose a topology-constrained two-dimensional dynamic time warping (2D-DTW) algorithm that performs column-wise alignment under a global grid consistency constraint. The proposed method is designed to be simple enough to run on resource limited platforms while preserving the grid structure required for accurate triangulation. We demonstrate that our 2D-DTW formulation can be used not only for terrain sensing but also as a general tool for matching structured grid patterns in image processing scenarios. This paper describes the overall system design as well as the 2D-DTW extension that emerged from this application.</li>
</ul>

<h3>Title: Developing a Comprehensive Framework for Sentiment Analysis in Turkish</h3>
<ul>
<li><strong>Authors: </strong>Cem Rifki Aydin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00515">https://arxiv.org/abs/2512.00515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00515">https://arxiv.org/pdf/2512.00515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00515]] Developing a Comprehensive Framework for Sentiment Analysis in Turkish(https://arxiv.org/abs/2512.00515)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In this thesis, we developed a comprehensive framework for sentiment analysis that takes its many aspects into account mainly for Turkish. We have also proposed several approaches specific to sentiment analysis in English only. We have accordingly made five major and three minor contributions. We generated a novel and effective feature set by combining unsupervised, semi-supervised, and supervised metrics. We then fed them as input into classical machine learning methods, and outperformed neural network models for datasets of different genres in both Turkish and English. We created a polarity lexicon with a semi-supervised domain-specific method, which has been the first approach applied for corpora in Turkish. We performed a fine morphological analysis for the sentiment classification task in Turkish by determining the polarities of morphemes. This can be adapted to other morphologically-rich or agglutinative languages as well. We have built a novel neural network architecture, which combines recurrent and recursive neural network models for English. We built novel word embeddings that exploit sentiment, syntactic, semantic, and lexical characteristics for both Turkish and English. We also redefined context windows as subclauses in modelling word representations in English. This can also be applied to other linguistic fields and natural language processing tasks. We have achieved state-of-the-art and significant results for all these original approaches. Our minor contributions include methods related to aspect-based sentiment in Turkish, parameter redefinition in the semi-supervised approach, and aspect term extraction techniques for English. This thesis can be considered the most detailed and comprehensive study made on sentiment analysis in Turkish as of July, 2020. Our work has also contributed to the opinion classification problem in English.</li>
</ul>

<h3>Title: Pushing the Boundaries of Interpretability: Incremental Enhancements to the Explainable Boosting Machine</h3>
<ul>
<li><strong>Authors: </strong>Isara Liyanage, Uthayasanker Thayasivam</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00528">https://arxiv.org/abs/2512.00528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00528">https://arxiv.org/pdf/2512.00528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00528]] Pushing the Boundaries of Interpretability: Incremental Enhancements to the Explainable Boosting Machine(https://arxiv.org/abs/2512.00528)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, interpretability</a></li>
<li><strong>Abstract: </strong>The widespread adoption of complex machine learning models in high-stakes domains has brought the "black-box" problem to the forefront of responsible AI research. This paper aims at addressing this issue by improving the Explainable Boosting Machine (EBM), a state-of-the-art glassbox model that delivers both high accuracy and complete transparency. The paper outlines three distinct enhancement methodologies: targeted hyperparameter optimization with Bayesian methods, the implementation of a custom multi-objective function for fairness for hyperparameter optimization, and a novel self-supervised pre-training pipeline for cold-start scenarios. All three methodologies are evaluated across standard benchmark datasets, including the Adult Income, Credit Card Fraud Detection, and UCI Heart Disease datasets. The analysis indicates that while the tuning process yielded marginal improvements in the primary ROC AUC metric, it led to a subtle but important shift in the model's decision-making behavior, demonstrating the value of a multi-faceted evaluation beyond a single performance score. This work is positioned as a critical step toward developing machine learning systems that are not only accurate but also robust, equitable, and transparent, meeting the growing demands of regulatory and ethical compliance.</li>
</ul>

<h3>Title: Image Generation as a Visual Planner for Robotic Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Ye Pang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00532">https://arxiv.org/abs/2512.00532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00532">https://arxiv.org/pdf/2512.00532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00532]] Image Generation as a Visual Planner for Robotic Manipulation(https://arxiv.org/abs/2512.00532)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating realistic robotic manipulation videos is an important step toward unifying perception, planning, and action in embodied agents. While existing video diffusion models require large domain-specific datasets and struggle to generalize, recent image generation models trained on language-image corpora exhibit strong compositionality, including the ability to synthesize temporally coherent grid images. This suggests a latent capacity for video-like generation even without explicit temporal modeling. We explore whether such models can serve as visual planners for robots when lightly adapted using LoRA finetuning. We propose a two-part framework that includes: (1) text-conditioned generation, which uses a language instruction and the first frame, and (2) trajectory-conditioned generation, which uses a 2D trajectory overlay and the same initial frame. Experiments on the Jaco Play dataset, Bridge V2, and the RT1 dataset show that both modes produce smooth, coherent robot videos aligned with their respective conditions. Our findings indicate that pretrained image generators encode transferable temporal priors and can function as video-like robotic planners under minimal supervision. Code is released at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: SAIDO: Generalizable Detection of AI-Generated Images via Scene-Aware and Importance-Guided Dynamic Optimization in Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Yongkang Hu, Yu Cheng, Yushuo Zhang, Yuan Xie, Zhaoxia Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00539">https://arxiv.org/abs/2512.00539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00539">https://arxiv.org/pdf/2512.00539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00539]] SAIDO: Generalizable Detection of AI-Generated Images via Scene-Aware and Importance-Guided Dynamic Optimization in Continual Learning(https://arxiv.org/abs/2512.00539)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, generative</a></li>
<li><strong>Abstract: </strong>The widespread misuse of image generation technologies has raised security concerns, driving the development of AI-generated image detection methods. However, generalization has become a key challenge and open problem: existing approaches struggle to adapt to emerging generative methods and content types in real-world scenarios. To address this issue, we propose a Scene-Aware and Importance-Guided Dynamic Optimization detection framework with continual learning (SAIDO). Specifically, we design Scene-Awareness-Based Expert Module (SAEM) that dynamically identifies and incorporates new scenes using VLLMs. For each scene, independent expert modules are dynamically allocated, enabling the framework to capture scene-specific forgery features better and enhance cross-scene generalization. To mitigate catastrophic forgetting when learning from multiple image generative methods, we introduce Importance-Guided Dynamic Optimization Mechanism (IDOM), which optimizes each neuron through an importance-guided gradient projection strategy, thereby achieving an effective balance between model plasticity and stability. Extensive experiments on continual learning tasks demonstrate that our method outperforms the current SOTA method in both stability and plasticity, achieving 44.22\% and 40.57\% relative reductions in average detection error rate and forgetting rate, respectively. On open-world datasets, it improves the average detection accuracy by 9.47\% compared to the current SOTA method.</li>
</ul>

<h3>Title: DQ4FairIM: Fairness-aware Influence Maximization using Deep Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Akrati Saxena, Harshith Kumar Yadav, Bart Rutten, Shashi Shekhar Jha</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00545">https://arxiv.org/abs/2512.00545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00545">https://arxiv.org/pdf/2512.00545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00545]] DQ4FairIM: Fairness-aware Influence Maximization using Deep Reinforcement Learning(https://arxiv.org/abs/2512.00545)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, fair</a></li>
<li><strong>Abstract: </strong>The Influence Maximization (IM) problem aims to select a set of seed nodes within a given budget to maximize the spread of influence in a social network. However, real-world social networks have several structural inequalities, such as dominant majority groups and underrepresented minority groups. If these inequalities are not considered while designing IM algorithms, the outcomes might be biased, disproportionately benefiting majority groups while marginalizing minorities. In this work, we address this gap by designing a fairness-aware IM method using Reinforcement Learning (RL) that ensures equitable influence outreach across all communities, regardless of protected attributes. Fairness is incorporated using a maximin fairness objective, which prioritizes improving the outreach of the least-influenced group, pushing the solution toward an equitable influence distribution. We propose a novel fairness-aware deep RL method, called DQ4FairIM, that maximizes the expected number of influenced nodes by learning an RL policy. The learnt policy ensures that minority groups formulate the IM problem as a Markov Decision Process (MDP) and use deep Q-learning, combined with the Structure2Vec network embedding, earning together with Structure2Vec network embedding to solve the MDP. We perform extensive experiments on synthetic benchmarks and real-world networks to compare our method with fairness-agnostic and fairness-aware baselines. The results show that our method achieves a higher level of fairness while maintaining a better fairness-performance trade-off than baselines. Additionally, our approach learns effective seeding policies that generalize across problem instances without retraining, such as varying the network size or the number of seed nodes.</li>
</ul>

<h3>Title: Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions</h3>
<ul>
<li><strong>Authors: </strong>Sandika Biswas, Qianyi Wu, Biplab Banerjee, Hamid Rezatofighi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00547">https://arxiv.org/abs/2512.00547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00547">https://arxiv.org/pdf/2512.00547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00547]] Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions(https://arxiv.org/abs/2512.00547)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Real-world human-built environments are highly dynamic, involving multiple humans and their complex interactions with surrounding objects. While 3D geometry modeling of such scenes is crucial for applications like AR/VR, gaming, and embodied AI, it remains underexplored due to challenges like diverse motion patterns and frequent occlusions. Beyond novel view rendering, 3D Gaussian Splatting (GS) has demonstrated remarkable progress in producing detailed, high-quality surface geometry with fast optimization of the underlying structure. However, very few GS-based methods address multihuman, multiobject scenarios, primarily due to the above-mentioned inherent challenges. In a monocular setup, these challenges are further amplified, as maintaining structural consistency under severe occlusion becomes difficult when the scene is optimized solely based on GS-based rendering loss. To tackle the challenges of such a multihuman, multiobject dynamic scene, we propose a hybrid approach that effectively combines the advantages of 1) 3D generative models for generating high-fidelity meshes of the scene elements, 2) Semantic-aware deformation, \ie rigid transformation of the rigid objects and LBS-based deformation of the humans, and mapping of the deformed high-fidelity meshes in the dynamic scene, and 3) GS-based optimization of the individual elements for further refining their alignments in the scene. Such a hybrid approach helps maintain the object structures even under severe occlusion and can produce multiview and temporally consistent geometry. We choose HOI-M3 for evaluation, as, to the best of our knowledge, this is the only dataset featuring multihuman, multiobject interactions in a dynamic scene. Our method outperforms the state-of-the-art method in producing better surface reconstruction of such scenes.</li>
</ul>

<h3>Title: Catch Me If You Can: How Smaller Reasoning Models Pretend to Reason with Mathematical Fidelity</h3>
<ul>
<li><strong>Authors: </strong>Subramanyam Sahoo, Vinija Jain, Saanidhya Vats, Siddharth Mohapatra, Rui Min, Aman Chadha, Divya Chaudhary</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00552">https://arxiv.org/abs/2512.00552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00552">https://arxiv.org/pdf/2512.00552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00552]] Catch Me If You Can: How Smaller Reasoning Models Pretend to Reason with Mathematical Fidelity(https://arxiv.org/abs/2512.00552)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Current evaluation of mathematical reasoning in language models relies primarily on answer accuracy, potentially masking fundamental failures in logical computation. We introduce a diagnostic framework that distinguishes genuine mathematical reasoning from superficial pattern matching through four complementary axes: forward-backward consistency, transitivity coverage, counterfactual sensitivity, and perturbation robustness. Through a case study applying this framework to Qwen3-0.6B on the MenatQA dataset, we reveal a striking disconnect between surface performance and reasoning fidelity. While the model achieves reasonable answer accuracy (70%+), it demonstrates poor backward consistency (15%), limited transitivity coverage (32.2%), and brittle sensitivity to perturbations. Our diagnostics expose reasoning failures invisible to traditional accuracy metrics, suggesting that this small model relies heavily on pattern matching rather than genuine logical computation. While our empirical findings are based on a single 600M-parameter model, the diagnostic framework itself is model-agnostic and generalizable. We release our evaluation protocols to enable the research community to assess reasoning fidelity across different model scales and architectures, moving beyond surface-level accuracy toward verifiable mathematical reasoning.</li>
</ul>

<h3>Title: NeuroVolve: Evolving Visual Stimuli toward Programmable Neural Objectives</h3>
<ul>
<li><strong>Authors: </strong>Haomiao Chen, Keith W Jamison, Mert R. Sabuncu, Amy Kuceyeski</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00557">https://arxiv.org/abs/2512.00557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00557">https://arxiv.org/pdf/2512.00557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00557]] NeuroVolve: Evolving Visual Stimuli toward Programmable Neural Objectives(https://arxiv.org/abs/2512.00557)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>What visual information is encoded in individual brain regions, and how do distributed patterns combine to create their neural representations? Prior work has used generative models to replicate known category selectivity in isolated regions (e.g., faces in FFA), but these approaches offer limited insight into how regions interact during complex, naturalistic vision. We introduce NeuroVolve, a generative framework that provides brain-guided image synthesis via optimization of a neural objective function in the embedding space of a pretrained vision-language model. Images are generated under the guidance of a programmable neural objective, i.e., activating or deactivating single regions or multiple regions together. NeuroVolve is validated by recovering known selectivity for individual brain regions, while expanding to synthesize coherent scenes that satisfy complex, multi-region constraints. By tracking optimization steps, it reveals semantic trajectories through embedding space, unifying brain-guided image editing and preferred stimulus generation in a single process. We show that NeuroVolve can generate both low-level and semantic feature-specific stimuli for single ROIs, as well as stimuli aligned to curated neural objectives. These include co-activation and decorrelation between regions, exposing cooperative and antagonistic tuning relationships. Notably, the framework captures subject-specific preferences, supporting personalized brain-driven synthesis and offering interpretable constraints for mapping, analyzing, and probing neural representations of visual information.</li>
</ul>

<h3>Title: Integrating Skeleton Based Representations for Robust Yoga Pose Classification Using Deep Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Mohiuddin, Syed Mohammod Minhaz Hossain, Sumaiya Khanam, Prionkar Barua, Aparup Barua, MD Tamim Hossain</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00572">https://arxiv.org/abs/2512.00572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00572">https://arxiv.org/pdf/2512.00572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00572]] Integrating Skeleton Based Representations for Robust Yoga Pose Classification Using Deep Learning Models(https://arxiv.org/abs/2512.00572)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, interpretability</a></li>
<li><strong>Abstract: </strong>Yoga is a popular form of exercise worldwide due to its spiritual and physical health benefits, but incorrect postures can lead to injuries. Automated yoga pose classification has therefore gained importance to reduce reliance on expert practitioners. While human pose keypoint extraction models have shown high potential in action recognition, systematic benchmarking for yoga pose recognition remains limited, as prior works often focus solely on raw images or a single pose extraction model. In this study, we introduce a curated dataset, 'Yoga-16', which addresses limitations of existing datasets, and systematically evaluate three deep learning architectures (VGG16, ResNet50, and Xception) using three input modalities (direct images, MediaPipe Pose skeleton images, and YOLOv8 Pose skeleton images). Our experiments demonstrate that skeleton-based representations outperform raw image inputs, with the highest accuracy of 96.09% achieved by VGG16 with MediaPipe Pose skeleton input. Additionally, we provide interpretability analysis using Grad-CAM, offering insights into model decision-making for yoga pose classification with cross validation analysis.</li>
</ul>

<h3>Title: Non-Asymptotic Convergence of Discrete Diffusion Models: Masked and Random Walk dynamics</h3>
<ul>
<li><strong>Authors: </strong>Giovanni Conforti, Alain Durmus, Le-Tuyet-Nhi Pham</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00580">https://arxiv.org/abs/2512.00580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00580">https://arxiv.org/pdf/2512.00580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00580]] Non-Asymptotic Convergence of Discrete Diffusion Models: Masked and Random Walk dynamics(https://arxiv.org/abs/2512.00580)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We investigate the theoretical underpinnings of Discrete Diffusion Models (DDMs) on discrete state spaces. Unlike in the continuous setting-where diffusion models are well understood both theoretically and empirically-the discrete case poses significant challenges due to its combinatorial structure and the lack of rigorous analysis. In this work, we establish convergence guarantees for DDMs on both the finite space $\mathbb{Z}^d_m=\{0,...,m-1\}^d$ and the countably infinite space $\mathbb{N}^d$ under mild assumptions, focusing on forward masked and random walk dynamics. Similar to the continuous case, the backward process can be characterized by a discrete score function, whose monotonicity plays a central role in deriving the error bounds of the generated data. Notably, the complexity of our model scales linearly up to logarithmic factors, rather than exponentially, with the dimension, making it efficiently scalable to high-dimensional data. To the best of our knowledge, this study provides the first non-asymptotic convergence guarantees that do not rely on the boundedness of the estimated score-covering not only uniform noising processes on $\mathbb{Z}^d_m$ and on $\mathbb{N}^d$, but also masking-based noising dynamics.</li>
</ul>

<h3>Title: Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alla Chepurova, Aydar Bulatov, Yuri Kuratov, Mikhail Burtsev</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00590">https://arxiv.org/abs/2512.00590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00590">https://arxiv.org/pdf/2512.00590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00590]] Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models(https://arxiv.org/abs/2512.00590)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3$\times$ fewer than AriGraph and $<$1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs.</li>
</ul>

<h3>Title: Developing Fairness-Aware Task Decomposition to Improve Equity in Post-Spinal Fusion Complication Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yining Yuan, J. Ben Tamo, Wenqi Shi, Yishan Zhong, Micky C. Nnamdi, B. Randall Brenn, Steven W. Hwang, May D. Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00598">https://arxiv.org/abs/2512.00598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00598">https://arxiv.org/pdf/2512.00598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00598]] Developing Fairness-Aware Task Decomposition to Improve Equity in Post-Spinal Fusion Complication Prediction(https://arxiv.org/abs/2512.00598)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability</a></li>
<li><strong>Abstract: </strong>Fairness in clinical prediction models remains a persistent challenge, particularly in high-stakes applications such as spinal fusion surgery for scoliosis, where patient outcomes exhibit substantial heterogeneity. Many existing fairness approaches rely on coarse demographic adjustments or post-hoc corrections, which fail to capture the latent structure of clinical populations and may unintentionally reinforce bias. We propose FAIR-MTL, a fairness-aware multitask learning framework designed to provide equitable and fine-grained prediction of postoperative complication severity. Instead of relying on explicit sensitive attributes during model training, FAIR-MTL employs a data-driven subgroup inference mechanism. We extract a compact demographic embedding, and apply k-means clustering to uncover latent patient subgroups that may be differentially affected by traditional models. These inferred subgroup labels determine task routing within a shared multitask architecture. During training, subgroup imbalance is mitigated through inverse-frequency weighting, and regularization prevents overfitting to smaller groups. Applied to postoperative complication prediction with four severity levels, FAIR-MTL achieves an AUC of 0.86 and an accuracy of 75%, outperforming single-task baselines while substantially reducing bias. For gender, the demographic parity difference decreases to 0.055 and equalized odds to 0.094; for age, these values reduce to 0.056 and 0.148, respectively. Model interpretability is ensured through SHAP and Gini importance analyses, which consistently highlight clinically meaningful predictors such as hemoglobin, hematocrit, and patient weight. Our findings show that incorporating unsupervised subgroup discovery into a multitask framework enables more equitable, interpretable, and clinically actionable predictions for surgical risk stratification.</li>
</ul>

<h3>Title: Prism: A Minimal Compositional Metalanguage for Specifying Agent Behavior</h3>
<ul>
<li><strong>Authors: </strong>Franck Binard, Vanja Kljajevic</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00611">https://arxiv.org/abs/2512.00611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00611">https://arxiv.org/pdf/2512.00611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00611]] Prism: A Minimal Compositional Metalanguage for Specifying Agent Behavior(https://arxiv.org/abs/2512.00611)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Prism is a small, compositional metalanguage for specifying the behaviour of tool-using software agents. Rather than introducing ad hoc control constructs, Prism is built around a fixed core context, Core1, which provides a minimal background grammar of categories numbers, strings, user prompts, tools together with abstract combinators for booleans, predicates, pairs, and lists. Agent policies are written as ordinary expressions using a single abstraction operator so that conditionals appear as selections between alternatives instead of imperative if-else blocks. Domains extend the core by defining their own context-mini-grammars that introduce new categories, predicates, and external tools while reusing the same compositional machinery. We illustrate this with worked examples from thermostat control, home security, e-commerce recommendation, and medical monitoring, showing how natural language decision rules can be mapped to inspectable, executable policies. From a linguistic perspective, Prism enforces a clear separation between a reusable grammar-like core and domain specific lexicons and treats tools as bridges between internal policy representations and the external world. From an engineering perspective, it offers a compact interface language for agent control, making the space of possible actions explicit and amenable to analysis, verification, and safety constraints.</li>
</ul>

<h3>Title: Generalized Graph Transformer Variational Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Siddhant Karki</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00612">https://arxiv.org/abs/2512.00612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00612">https://arxiv.org/pdf/2512.00612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00612]] Generalized Graph Transformer Variational Autoencoder(https://arxiv.org/abs/2512.00612)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Graph link prediction has long been a central problem in graph representation learning in both network analysis and generative modeling. Recent progress in deep learning has introduced increasingly sophisticated architectures for capturing relational dependencies within graph-structured data. In this work, we propose the Generalized Graph Transformer Variational Autoencoder (GGT-VAE). Our model integrates Generalized Graph Transformer Architecture with Variational Autoencoder framework for link prediction. Unlike prior GraphVAE, GCN, or GNN approaches, GGT-VAE leverages transformer style global self-attention mechanism along with laplacian positional encoding to model structural patterns across nodes into a latent space without relying on message passing. Experimental results on several benchmark datasets demonstrate that GGT-VAE consistently achieves above-baseline performance in terms of ROC-AUC and Average Precision. To the best of our knowledge, this is among the first studies to explore graph structure generation using a generalized graph transformer backbone in a variational framework.</li>
</ul>

<h3>Title: ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization</h3>
<ul>
<li><strong>Authors: </strong>Omer Jauhar Khan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00617">https://arxiv.org/abs/2512.00617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00617">https://arxiv.org/pdf/2512.00617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00617]] ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization(https://arxiv.org/abs/2512.00617)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R22 values exceeding 0.96 in ELO rating convergence.</li>
</ul>

<h3>Title: Neuroscience-Inspired Memory Replay for Continual Learning: A Comparative Study of Predictive Coding and Backpropagation-Based Strategies</h3>
<ul>
<li><strong>Authors: </strong>Goutham Nalagatla, Shreyas Grandhe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00619">https://arxiv.org/abs/2512.00619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00619">https://arxiv.org/pdf/2512.00619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00619]] Neuroscience-Inspired Memory Replay for Continual Learning: A Comparative Study of Predictive Coding and Backpropagation-Based Strategies(https://arxiv.org/abs/2512.00619)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Continual learning remains a fundamental challenge in artificial intelligence, with catastrophic forgetting posing a significant barrier to deploying neural networks in dynamic environments. Inspired by biological memory consolidation mechanisms, we propose a novel framework for generative replay that leverages predictive coding principles to mitigate forgetting. We present a comprehensive comparison between predictive coding-based and backpropagation-based gen- erative replay strategies, evaluating their effectiveness on task retention and transfer efficiency across multiple benchmark datasets. Our experimental results demonstrate that predictive coding-based replay achieves superior retention performance (average 15.3% improvement) while maintaining competitive transfer efficiency, suggesting that biologically-inspired mechanisms can offer principled solutions to continual learning challenges. The proposed framework provides insights into the relationship between biological memory processes and artificial learning systems, opening new avenues for neuroscience-inspired AI research.</li>
</ul>

<h3>Title: Automatic Pith Detection in Tree Cross-Section Images Using Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Tzu-I Liao, Mahmoud Fakhry, Jibin Yesudas Varghese</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00625">https://arxiv.org/abs/2512.00625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00625">https://arxiv.org/pdf/2512.00625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00625]] Automatic Pith Detection in Tree Cross-Section Images Using Deep Learning(https://arxiv.org/abs/2512.00625)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Pith detection in tree cross-sections is essential for forestry and wood quality analysis but remains a manual, error-prone task. This study evaluates deep learning models -- YOLOv9, U-Net, Swin Transformer, DeepLabV3, and Mask R-CNN -- to automate the process efficiently. A dataset of 582 labeled images was dynamically augmented to improve generalization. Swin Transformer achieved the highest accuracy (0.94), excelling in fine segmentation. YOLOv9 performed well for bounding box detection but struggled with boundary precision. U-Net was effective for structured patterns, while DeepLabV3 captured multi-scale features with slight boundary imprecision. Mask R-CNN initially underperformed due to overlapping detections, but applying Non-Maximum Suppression (NMS) improved its IoU from 0.45 to 0.80. Generalizability was next tested using an oak dataset of 11 images from Oregon State University's Tree Ring Lab. Additionally, for exploratory analysis purposes, an additional dataset of 64 labeled tree cross-sections was used to train the worst-performing model to see if this would improve its performance generalizing to the unseen oak dataset. Key challenges included tensor mismatches and boundary inconsistencies, addressed through hyperparameter tuning and augmentation. Our results highlight deep learning's potential for tree cross-section pith detection, with model choice depending on dataset characteristics and application needs.</li>
</ul>

<h3>Title: XAI-Driven Skin Disease Classification: Leveraging GANs to Augment ResNet-50 Performance</h3>
<ul>
<li><strong>Authors: </strong>Kim Gerard A. Villanueva, Priyanka Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00626">https://arxiv.org/abs/2512.00626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00626">https://arxiv.org/pdf/2512.00626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00626]] XAI-Driven Skin Disease Classification: Leveraging GANs to Augment ResNet-50 Performance(https://arxiv.org/abs/2512.00626)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Accurate and timely diagnosis of multi-class skin lesions is hampered by subjective methods, inherent data imbalance in datasets like HAM10000, and the "black box" nature of Deep Learning (DL) models. This study proposes a trustworthy and highly accurate Computer-Aided Diagnosis (CAD) system to overcome these limitations. The approach utilizes Deep Convolutional Generative Adversarial Networks (DCGANs) for per class data augmentation to resolve the critical class imbalance problem. A fine-tuned ResNet-50 classifier is then trained on the augmented dataset to classify seven skin disease categories. Crucially, LIME and SHAP Explainable AI (XAI) techniques are integrated to provide transparency by confirming that predictions are based on clinically relevant features like irregular morphology. The system achieved a high overall Accuracy of 92.50 % and a Macro-AUC of 98.82 %, successfully outperforming various prior benchmarked architectures. This work successfully validates a verifiable framework that combines high performance with the essential clinical interpretability required for safe diagnostic deployment. Future research should prioritize enhancing discrimination for critical categories, such as Melanoma NOS (F1-Score is 0.8602).</li>
</ul>

<h3>Title: Financial Text Classification Based On rLoRA Finetuning On Qwen3-8B model</h3>
<ul>
<li><strong>Authors: </strong>Zhiming Lian</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.CP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00630">https://arxiv.org/abs/2512.00630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00630">https://arxiv.org/pdf/2512.00630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00630]] Financial Text Classification Based On rLoRA Finetuning On Qwen3-8B model(https://arxiv.org/abs/2512.00630)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Financial text classification has increasingly become an important aspect in quantitative trading systems and related tasks, such as financial sentiment analysis and the classification of financial news. In this paper, we assess the performance of the large language model Qwen3-8B on both tasks. Qwen3-8B is a state-of-the-art model that exhibits strong instruction-following and multilingual capabilities, and is distinct from standard models, primarily because it is specifically optimized for efficient fine tuning and high performance on reasoning-based benchmarks, making it suitable for financial applications. To adapt this model, we apply Noisy Embedding Instruction Finetuning and based on our previous work, this method increases robustness by injecting controlled noise into the embedding layers during supervised adaptation. We improve efficiency further with Rank-stabilized Low-Rank Adaptation low-rank optimization approach, and FlashAttention, which allow for faster training with lower GPU memory. For both tasks, we benchmark Qwen3-8B against standard classical transformer models, such as T5, BERT, and RoBERTa, and large models at scale, such as LLaMA1-7B, LLaMA2-7B, and Baichuan2-7B. The findings reveal that Qwen3-8B consistently surpasses these baselines by obtaining better classification accuracy and needing fewer training epochs. The synergy of instruction-based fine-tuning and memory-efficient optimization methods suggests Qwen3-8B can potentially serve as a scalable, economical option for real-time financial NLP applications. Qwen3-8B provides a very promising base for advancing dynamic quantitative trading systems in the future.</li>
</ul>

<h3>Title: Extended Abstract: Synthesizable Low-overhead Circuit-level Countermeasures and Pro-Active Detection Techniques for Power and EM SCA</h3>
<ul>
<li><strong>Authors: </strong>Archisman Ghosh</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00635">https://arxiv.org/abs/2512.00635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00635">https://arxiv.org/pdf/2512.00635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00635]] Extended Abstract: Synthesizable Low-overhead Circuit-level Countermeasures and Pro-Active Detection Techniques for Power and EM SCA(https://arxiv.org/abs/2512.00635)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>The gamut of todays internet-connected embedded devices has led to increased concerns regarding the security and confidentiality of data. Most internet-connected embedded devices employ mathematically secure cryptographic algorithms to address security vulnerabilities. Despite such mathematical guarantees, as these algorithms are often implemented in silicon, they leak critical information in terms of power consumption, electromagnetic (EM) radiation, timing, cache hits and misses, photonic emission and so on, leading to side-channel analysis (SCA) attacks. This thesis focuses on low overhead generic circuit-level yet synthesizable countermeasures against power and EM SCA. Existing countermeasures (including proposed) still have relatively high overhead which bars them from being used in energy-constraint IoT devices. We propose a zero-overhead integrated inductive sensor which is able to detect i)EM SCA ii) Clock glitch-based Fault Injection Attack (FIA), and iii) Voltage-glitch based Fault Injection Attack by using a simple ML algorithm. Advent of quantum computer research will open new possibilities for theoretical attacks against existing cryptographic protocols. National Institute of Standard & Technology (NIST) has standardized post-quantum cryptographic algorithms to secure crypto-systems against quantum adversary. I contribute to the standardization procedure by introducing the first silicon-verified Saber (a NIST finalist modulo Learning with Rounding scheme) which consumes lowest energy and area till date amongst all the candidates.</li>
</ul>

<h3>Title: Privacy Preserving Diffusion Models for Mixed-Type Tabular Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Timur Sattarov, Marco Schreyer, Damian Borth</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00638">https://arxiv.org/abs/2512.00638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00638">https://arxiv.org/pdf/2512.00638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00638]] Privacy Preserving Diffusion Models for Mixed-Type Tabular Data Generation(https://arxiv.org/abs/2512.00638)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion</a></li>
<li><strong>Abstract: </strong>We introduce DP-FinDiff, a differentially private diffusion framework for synthesizing mixed-type tabular data. DP-FinDiff employs embedding-based representations for categorical features, reducing encoding overhead and scaling to high-dimensional datasets. To adapt DP-training to the diffusion process, we propose two privacy-aware training strategies: an adaptive timestep sampler that aligns updates with diffusion dynamics, and a feature-aggregated loss that mitigates clipping-induced bias. Together, these enhancements improve fidelity and downstream utility without weakening privacy guarantees. On financial and medical datasets, DP-FinDiff achieves 16-42% higher utility than DP baselines at comparable privacy levels, demonstrating its promise for safe and effective data sharing in sensitive domains.</li>
</ul>

<h3>Title: Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Mahmoud El Hussieni</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CE, cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00639">https://arxiv.org/abs/2512.00639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00639">https://arxiv.org/pdf/2512.00639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00639]] Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation(https://arxiv.org/abs/2512.00639)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The increasing prevalence of thyroid cancer globally has led to the development of various computer-aided detection methods. Accurate segmentation of thyroid nodules is a critical first step in the development of AI-assisted clinical decision support systems. This study focuses on instance segmentation of thyroid nodules using YOLOv5 algorithms on ultrasound images. We evaluated multiple YOLOv5 variants (Nano, Small, Medium, Large, and XLarge) across two dataset versions, with and without doppler images. The YOLOv5-Large algorithm achieved the highest performance with a dice score of 91\% and mAP of 0.87 on the dataset including doppler images. Notably, our results demonstrate that doppler images, typically excluded by physicians, can significantly improve segmentation performance. The YOLOv5-Small model achieved 79\% dice score when doppler images were excluded, while including them improved performance across all model variants. These findings suggest that instance segmentation with YOLOv5 provides an effective real-time approach for thyroid nodule detection, with potential clinical applications in automated diagnostic systems.</li>
</ul>

<h3>Title: Graph-Attention Network with Adversarial Domain Alignment for Robust Cross-Domain Facial Expression Recognition</h3>
<ul>
<li><strong>Authors: </strong>Razieh Ghaedi, AmirReza BabaAhmadi, Reyer Zwiggelaar, Xinqi Fan, Nashid Alam</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00641">https://arxiv.org/abs/2512.00641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00641">https://arxiv.org/pdf/2512.00641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00641]] Graph-Attention Network with Adversarial Domain Alignment for Robust Cross-Domain Facial Expression Recognition(https://arxiv.org/abs/2512.00641)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Cross-domain facial expression recognition (CD-FER) remains difficult due to severe domain shift between training and deployment data. We propose Graph-Attention Network with Adversarial Domain Alignment (GAT-ADA), a hybrid framework that couples a ResNet-50 as backbone with a batch-level Graph Attention Network (GAT) to model inter-sample relations under shift. Each mini-batch is cast as a sparse ring graph so that attention aggregates cross-sample cues that are informative for adaptation. To align distributions, GAT-ADA combines adversarial learning via a Gradient Reversal Layer (GRL) with statistical alignment using CORAL and MMD. GAT-ADA is evaluated under a standard unsupervised domain adaptation protocol: training on one labeled source (RAF-DB) and adapting to multiple unlabeled targets (CK+, JAFFE, SFEW 2.0, FER2013, and ExpW). GAT-ADA attains 74.39% mean cross-domain accuracy. On RAF-DB to FER2013, it reaches 98.0% accuracy, corresponding to approximately a 36-point improvement over the best baseline we re-implemented with the same backbone and preprocessing.</li>
</ul>

<h3>Title: Blockchain-based vs. SQL Database Systems for Digital Twin Evidence Management: A Comparative Forensic Analysis</h3>
<ul>
<li><strong>Authors: </strong>Boyd Franken, Hong-Hanh Nguyen-Le, Nhien-An Le-Khac</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00645">https://arxiv.org/abs/2512.00645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00645">https://arxiv.org/pdf/2512.00645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00645]] Blockchain-based vs. SQL Database Systems for Digital Twin Evidence Management: A Comparative Forensic Analysis(https://arxiv.org/abs/2512.00645)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Digital forensics faces unprecedented challenges with the emergence of digital twins and metaverse technologies. This paper presents the first comparative analysis between blockchain-based and traditional database systems for managing digital twin evidence in forensic investigations. We conducted controlled experiments comparing the Ethereum blockchain with IPFS storage against traditional SQL databases for digital twin evidence management. Our findings reveal that while blockchain provides superior data integrity and immutability, crucial for forensic applications, traditional databases offer better performance consistency. The blockchain implementation showed faster average storage times but higher variability in retrieval operations. Both systems maintained forensic integrity through hash verification, though blockchain's immutable nature provides additional security guarantees essential for legal proceedings. This research contributes to the development of robust digital forensic methodologies for emerging technologies in the metaverse era.</li>
</ul>

<h3>Title: MambaScope: Coarse-to-Fine Scoping for Efficient Vision Mamba</h3>
<ul>
<li><strong>Authors: </strong>Shanhui Liu, Rui Xu, Yunke Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00647">https://arxiv.org/abs/2512.00647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00647">https://arxiv.org/pdf/2512.00647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00647]] MambaScope: Coarse-to-Fine Scoping for Efficient Vision Mamba(https://arxiv.org/abs/2512.00647)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vision Mamba has emerged as a promising and efficient alternative to Vision Transformers, yet its efficiency remains fundamentally constrained by the number of input tokens. Existing token reduction approaches typically adopt token pruning or merging to reduce computation. However, they inherently lead to information loss, as they discard or compress token representations. This problem is exacerbated when applied uniformly to fine-grained token representations across all images, regardless of visual complexity. We observe that not all inputs require fine-grained processing. Simple images can be effectively handled at coarse resolution, while only complex ones may warrant refinement. Based on this insight, we propose \textit{Coarse-to-Fine Vision Mamba (CF-ViM)}, an adaptive framework for efficient inference. CF-ViM first performs coarse-grained inference by dividing the input image into large patches, significantly reducing the token length and computation. When the model's prediction confidence is low, selected regions are re-processed at a finer resolution to recover critical visual details with minimal additional cost. This dynamic resolution assignment strategy allows CF-ViM to allocate computation adaptively according to image complexity, ensuring efficient processing without compromising essential visual information. Experiments on ImageNet demonstrate that CF-ViM outperforms both the baseline Vision Mamba and state-of-the-art token reduction techniques in terms of accuracy and efficiency.</li>
</ul>

<h3>Title: Sycophancy Claims about Language Models: The Missing Human-in-the-Loop</h3>
<ul>
<li><strong>Authors: </strong>Jan Batzner, Volker Stocker, Stefan Schmid, Gjergji Kasneci</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00656">https://arxiv.org/abs/2512.00656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00656">https://arxiv.org/pdf/2512.00656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00656]] Sycophancy Claims about Language Models: The Missing Human-in-the-Loop(https://arxiv.org/abs/2512.00656)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Sycophantic response patterns in Large Language Models (LLMs) have been increasingly claimed in the literature. We review methodological challenges in measuring LLM sycophancy and identify five core operationalizations. Despite sycophancy being inherently human-centric, current research does not evaluate human perception. Our analysis highlights the difficulties in distinguishing sycophantic responses from related concepts in AI alignment and offers actionable recommendations for future research.</li>
</ul>

<h3>Title: Graphing the Truth: Structured Visualizations for Automated Hallucination Detection in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tanmay Agrawal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00663">https://arxiv.org/abs/2512.00663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00663">https://arxiv.org/pdf/2512.00663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00663]] Graphing the Truth: Structured Visualizations for Automated Hallucination Detection in LLMs(https://arxiv.org/abs/2512.00663)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models have rapidly advanced in their ability to interpret and generate natural language. In enterprise settings, they are frequently augmented with closed-source domain knowledge to deliver more contextually informed responses. However, operational constraints such as limited context windows and inconsistencies between pre-training data and supplied knowledge often lead to hallucinations, some of which appear highly credible and escape routine human review. Current mitigation strategies either depend on costly, large-scale gold-standard Q\&A curation or rely on secondary model verification, neither of which offers deterministic assurance. This paper introduces a framework that organizes proprietary knowledge and model-generated content into interactive visual knowledge graphs. The objective is to provide end users with a clear, intuitive view of potential hallucination zones by linking model assertions to underlying sources of truth and indicating confidence levels. Through this visual interface, users can diagnose inconsistencies, identify weak reasoning chains, and supply corrective feedback. The resulting human-in-the-loop workflow creates a structured feedback loop that can enhance model reliability and continuously improve response quality.</li>
</ul>

<h3>Title: ML-Tool-Bench: Tool-Augmented Planning for ML Tasks</h3>
<ul>
<li><strong>Authors: </strong>Yaswanth Chittepu, Raghavendra Addanki, Tung Mai, Anup Rao, Branislav Kveton</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00672">https://arxiv.org/abs/2512.00672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00672">https://arxiv.org/pdf/2512.00672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00672]] ML-Tool-Bench: Tool-Augmented Planning for ML Tasks(https://arxiv.org/abs/2512.00672)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>The development of autonomous machine learning (ML) agents capable of end-to-end data science workflows represents a significant frontier in artificial intelligence. These agents must orchestrate complex sequences of data analysis, feature engineering, model selection, and hyperparameter optimization, tasks that require sophisticated planning and iteration. While recent work on building ML agents has explored using large language models (LLMs) for direct code generation, tool-augmented approaches offer greater modularity and reliability. However, existing tool-use benchmarks focus primarily on task-specific tool selection or argument extraction for tool invocation, failing to evaluate the sophisticated planning capabilities required for ML Agents. In this work, we introduce a comprehensive benchmark for evaluating tool-augmented ML agents using a curated set of 61 specialized tools and 15 tabular ML challenges from Kaggle. Our benchmark goes beyond traditional tool-use evaluation by incorporating an in-memory named object management, allowing agents to flexibly name, save, and retrieve intermediate results throughout the workflows. We demonstrate that standard ReAct-style approaches struggle to generate valid tool sequences for complex ML pipelines, and that tree search methods with LLM-based evaluation underperform due to inconsistent state scoring. To address these limitations, we propose two simple approaches: 1) using shaped deterministic rewards with structured textual feedback, and 2) decomposing the original problem into a sequence of sub-tasks, which significantly improves trajectory validity and task performance. Using GPT-4o, our approach improves over ReAct by 16.52 percentile positions, taking the median across all Kaggle challenges. We believe our work provides a foundation for developing more capable tool-augmented planning ML agents.</li>
</ul>

<h3>Title: A Comparison of Human and ChatGPT Classification Performance on Complex Social Media Data</h3>
<ul>
<li><strong>Authors: </strong>Breanna E. Green, Ashley L. Shea, Pengfei Zhao, Drew B. Margolin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00673">https://arxiv.org/abs/2512.00673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00673">https://arxiv.org/pdf/2512.00673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00673]] A Comparison of Human and ChatGPT Classification Performance on Complex Social Media Data(https://arxiv.org/abs/2512.00673)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative artificial intelligence tools, like ChatGPT, are an increasingly utilized resource among computational social scientists. Nevertheless, there remains space for improved understanding of the performance of ChatGPT in complex tasks such as classifying and annotating datasets containing nuanced language. Method. In this paper, we measure the performance of GPT-4 on one such task and compare results to human annotators. We investigate ChatGPT versions 3.5, 4, and 4o to examine performance given rapid changes in technological advancement of large language models. We craft four prompt styles as input and evaluate precision, recall, and F1 scores. Both quantitative and qualitative evaluations of results demonstrate that while including label definitions in prompts may help performance, overall GPT-4 has difficulty classifying nuanced language. Qualitative analysis reveals four specific findings. Our results suggest the use of ChatGPT in classification tasks involving nuanced language should be conducted with prudence.</li>
</ul>

<h3>Title: Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Dong In Lee, Hyungjun Doh, Seunggeun Chi, Runlin Duan, Sangpil Kim, Karthik Ramani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00677">https://arxiv.org/abs/2512.00677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00677">https://arxiv.org/pdf/2512.00677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00677]] Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer(https://arxiv.org/abs/2512.00677)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Recent progress in 4D representations, such as Dynamic NeRF and 4D Gaussian Splatting (4DGS), has enabled dynamic 4D scene reconstruction. However, text-driven 4D scene editing remains under-explored due to the challenge of ensuring both multi-view and temporal consistency across space and time during editing. Existing studies rely on 2D diffusion models that edit frames independently, often causing motion distortion, geometric drift, and incomplete editing. We introduce Dynamic-eDiTor, a training-free text-driven 4D editing framework leveraging Multimodal Diffusion Transformer (MM-DiT) and 4DGS. This mechanism consists of Spatio-Temporal Sub-Grid Attention (STGA) for locally consistent cross-view and temporal fusion, and Context Token Propagation (CTP) for global propagation via token inheritance and optical-flow-guided token replacement. Together, these components allow Dynamic-eDiTor to perform seamless, globally consistent multi-view video without additional training and directly optimize pre-trained source 4DGS. Extensive experiments on multi-view video dataset DyNeRF demonstrate that our method achieves superior editing fidelity and both multi-view and temporal consistency prior approaches. Project page for results and code: this https URL</li>
</ul>

<h3>Title: Using physics-inspired Singular Learning Theory to understand grokking & other phase transitions in modern neural networks</h3>
<ul>
<li><strong>Authors: </strong>Anish Lakkapragada</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00686">https://arxiv.org/abs/2512.00686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00686">https://arxiv.org/pdf/2512.00686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00686]] Using physics-inspired Singular Learning Theory to understand grokking & other phase transitions in modern neural networks(https://arxiv.org/abs/2512.00686)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Classical statistical inference and learning theory often fail to explain the success of modern neural networks. A key reason is that these models are non-identifiable (singular), violating core assumptions behind PAC bounds and asymptotic normality. Singular learning theory (SLT), a physics-inspired framework grounded in algebraic geometry, has gained popularity for its ability to close this theory-practice gap. In this paper, we empirically study SLT in toy settings relevant to interpretability and phase transitions. First, we understand the SLT free energy $\mathcal{F}_n$ by testing an Arrhenius-style rate hypothesis using both a grokking modulo-arithmetic model and Anthropic's Toy Models of Superposition. Second, we understand the local learning coefficient $\lambda_{\alpha}$ by measuring how it scales with problem difficulty across several controlled network families (polynomial regressors, low-rank linear networks, and low-rank autoencoders). Our experiments recover known scaling laws while others yield meaningful deviations from theoretical expectations. Overall, our paper illustrates the many merits of SLT for understanding neural network phase transitions, and poses open research questions for the field.</li>
</ul>

<h3>Title: Silhouette-based Gait Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Dingqiang Ye, Chao Fan, Kartik Narayan, Bingzhe Wu, Chengwen Luo, Jianqiang Li, Vishal M. Patel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00691">https://arxiv.org/abs/2512.00691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00691">https://arxiv.org/pdf/2512.00691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00691]] Silhouette-based Gait Foundation Model(https://arxiv.org/abs/2512.00691)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Gait patterns play a critical role in human identification and healthcare analytics, yet current progress remains constrained by small, narrowly designed models that fail to scale or generalize. Building a unified gait foundation model requires addressing two longstanding barriers: (a) Scalability. Why have gait models historically failed to follow scaling laws? (b) Generalization. Can one model serve the diverse gait tasks that have traditionally been studied in isolation? We introduce FoundationGait, the first scalable, self-supervised pretraining framework for gait understanding. Its largest version has nearly 0.13 billion parameters and is pretrained on 12 public gait datasets comprising over 2 million walking sequences. Extensive experiments demonstrate that FoundationGait, with or without fine-tuning, performs robustly across a wide spectrum of gait datasets, conditions, tasks (e.g., human identification, scoliosis screening, depression prediction, and attribute estimation), and even input modality. Notably, it achieves 48.0% zero-shot rank-1 accuracy on the challenging in-the-wild Gait3D dataset (1,000 test subjects) and 64.5% on the largest in-the-lab OU-MVLP dataset (5,000+ test subjects), setting a new milestone in robust gait recognition. Coming code and model: this https URL.</li>
</ul>

<h3>Title: Affordance-First Decomposition for Continual Learning in Video-Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Mengzhu Xu, Hanzhi Liu, Ningkang Peng, Qianyu Chen, Canran Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00694">https://arxiv.org/abs/2512.00694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00694">https://arxiv.org/pdf/2512.00694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00694]] Affordance-First Decomposition for Continual Learning in Video-Language Understanding(https://arxiv.org/abs/2512.00694)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Continual learning for video--language understanding is increasingly important as models face non-stationary data, domains, and query styles, yet prevailing solutions blur what should stay stable versus what should adapt, rely on static routing/capacity, or require replaying past videos. We aim to explicitly specify where stability lives and where plasticity should be focused under realistic memory and privacy constraints. We introduce Affordance-First Decomposition (AFD): videos are mapped to slowly varying affordance tokens that form a shared, time-aligned substrate, while a lightweight, query-routed, conflict-aware scheduler concentrates adaptation and grows capacity only when needed. The substrate is stabilized via weak alignment and teacher consistency, and training uses question-only replay. AFD achieves state-of-the-art across protocols: 51.6% average accuracy with -1.8% forgetting on domain-incremental VideoQA, ViLCo R@1@0.5 of 29.6% (MQ) and 20.7% (NLQ) with 18.4% stAP@0.25 (VQ), and 39.5% accuracy with -1.6% forgetting on time-incremental iVQA. Overall, AFD offers an explicit, interpretable split between a stable interaction-centered substrate and targeted adaptation.</li>
</ul>

<h3>Title: Flow Matching for Tabular Data Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Bahrul Ilmi Nasution, Floor Eijkelboom, Mark Elliot, Richard Allmendinger, Christian A. Naesseth</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00698">https://arxiv.org/abs/2512.00698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00698">https://arxiv.org/pdf/2512.00698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00698]] Flow Matching for Tabular Data Synthesis(https://arxiv.org/abs/2512.00698)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion</a></li>
<li><strong>Abstract: </strong>Synthetic data generation is an important tool for privacy-preserving data sharing. While diffusion models have set recent benchmarks, flow matching (FM) offers a promising alternative. This paper presents different ways to implement flow matching for tabular data synthesis. We provide a comprehensive empirical study that compares flow matching (FM and variational FM) with a state-of-the-art diffusion method (TabDDPM and TabSyn) in tabular data synthesis. We evaluate both the standard Optimal Transport (OT) and the Variance Preserving (VP) probability paths, and also compare deterministic and stochastic samplers -- something possible when learning to generate using \textit{variational} flow matching -- characterising the empirical relationship between data utility and privacy risk. Our key findings reveal that flow matching, particularly TabbyFlow, outperforms diffusion baselines. Flow matching methods also achieves better performance with remarkably low function evaluations ($\leq$ 100 steps), offering a substantial computational advantage. The choice of probability path is also crucial, as using the OT path demonstrates superior performance, while VP has potential for producing synthetic data with lower disclosure risk. Lastly, our results show that making flows stochastic not only preserves marginal distributions but, in some instances, enables the generation of high utility synthetic data with reduced disclosure risk.</li>
</ul>

<h3>Title: Optimizing LVLMs with On-Policy Data for Effective Hallucination Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Chengzhi Yu, Yifan Xu, Yifan Chen, Wenyi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00706">https://arxiv.org/abs/2512.00706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00706">https://arxiv.org/pdf/2512.00706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00706]] Optimizing LVLMs with On-Policy Data for Effective Hallucination Mitigation(https://arxiv.org/abs/2512.00706)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recently, large vision-language models (LVLMs) have risen to be a promising approach for multimodal tasks. However, principled hallucination mitigation remains a critical this http URL this work, we first analyze the data generation process in LVLM hallucination mitigation and affirm that on-policy data significantly outperforms off-policy data, which thus calls for efficient and reliable preference annotation of on-policy data. We then point out that, existing annotation methods introduce additional hallucination in training samples, which may enhance the model's hallucination patterns, to address this problem, we propose training a hallucination classifier giving binary annotations, which guarantee clean chosen samples for the subsequent alignment. To further harness of the power of on-policy data, we design a robust iterative direct preference optimization (DPO) algorithm adopting a dynamic sample reweighting scheme. We conduct comprehensive experiments on three benchmarks with comparison to 8 state-of-the-art baselines. In particular, our approach reduces the hallucination rate of LLaVA-1.5-7B on MMHalBench by 50.8% and the average hallucination rate on Object HalBench by 79.5%; more significantly, our method fully taps into the potential of open-source models, enabling LLaVA-1.5-13B to even surpass the performance of GPT-4V.</li>
</ul>

<h3>Title: Towards Precision Protein-Ligand Affinity Prediction Benchmark: A Complete and Modification-Aware DAVIS Dataset</h3>
<ul>
<li><strong>Authors: </strong>Ming-Hsiu Wu, Ziqian Xie, Shuiwang Ji, Degui Zhi</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00708">https://arxiv.org/abs/2512.00708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00708">https://arxiv.org/pdf/2512.00708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00708]] Towards Precision Protein-Ligand Affinity Prediction Benchmark: A Complete and Modification-Aware DAVIS Dataset(https://arxiv.org/abs/2512.00708)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Advancements in AI for science unlocks capabilities for critical drug discovery tasks such as protein-ligand binding affinity prediction. However, current models overfit to existing oversimplified datasets that does not represent naturally occurring and biologically relevant proteins with modifications. In this work, we curate a complete and modification-aware version of the widely used DAVIS dataset by incorporating 4,032 kinase-ligand pairs involving substitutions, insertions, deletions, and phosphorylation events. This enriched dataset enables benchmarking of predictive models under biologically realistic conditions. Based on this new dataset, we propose three benchmark settings-Augmented Dataset Prediction, Wild-Type to Modification Generalization, and Few-Shot Modification Generalization-designed to assess model robustness in the presence of protein modifications. Through extensive evaluation of both docking-free and docking-based methods, we find that docking-based model generalize better in zero-shot settings. In contrast, docking-free models tend to overfit to wild-type proteins and struggle with unseen modifications but show notable improvement when fine-tuned on a small set of modified examples. We anticipate that the curated dataset and benchmarks offer a valuable foundation for developing models that better generalize to protein modifications, ultimately advancing precision medicine in drug discovery. The benchmark is available at: this https URL</li>
</ul>

<h3>Title: Concept-Guided Backdoor Attack on Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Shen, Weimin Lyu, Haotian Xu, Tengfei Ma</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00713">https://arxiv.org/abs/2512.00713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00713">https://arxiv.org/pdf/2512.00713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00713]] Concept-Guided Backdoor Attack on Vision Language Models(https://arxiv.org/abs/2512.00713)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, steal</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have achieved impressive progress in multimodal text generation, yet their rapid adoption raises increasing concerns about security vulnerabilities. Existing backdoor attacks against VLMs primarily rely on explicit pixel-level triggers or imperceptible perturbations injected into images. While effective, these approaches reduce stealthiness and remain vulnerable to image-based defenses. We introduce concept-guided backdoor attacks, a new paradigm that operates at the semantic concept level rather than on raw pixels. We propose two different attacks. The first, Concept-Thresholding Poisoning (CTP), uses explicit concepts in natural images as triggers: only samples containing the target concept are poisoned, causing the model to behave normally in all other cases but consistently inject malicious outputs whenever the concept appears. The second, CBL-Guided Unseen Backdoor (CGUB), leverages a Concept Bottleneck Model (CBM) during training to intervene on internal concept activations, while discarding the CBM branch at inference time to keep the VLM unchanged. This design enables systematic replacement of a targeted label in generated text (for example, replacing "cat" with "dog"), even when the replacement behavior never appears in the training data. Experiments across multiple VLM architectures and datasets show that both CTP and CGUB achieve high attack success rates while maintaining moderate impact on clean-task performance. These findings highlight concept-level vulnerabilities as a critical new attack surface for VLMs.</li>
</ul>

<h3>Title: Deep Learning-Based Computer Vision Models for Early Cancer Detection Using Multimodal Medical Imaging and Radiogenomic Integration Frameworks</h3>
<ul>
<li><strong>Authors: </strong>Emmanuella Avwerosuoghene Oghenekaro</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00714">https://arxiv.org/abs/2512.00714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00714">https://arxiv.org/pdf/2512.00714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00714]] Deep Learning-Based Computer Vision Models for Early Cancer Detection Using Multimodal Medical Imaging and Radiogenomic Integration Frameworks(https://arxiv.org/abs/2512.00714)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Early cancer detection remains one of the most critical challenges in modern healthcare, where delayed diagnosis significantly reduces survival outcomes. Recent advancements in artificial intelligence, particularly deep learning, have enabled transformative progress in medical imaging analysis. Deep learning-based computer vision models, such as convolutional neural networks (CNNs), transformers, and hybrid attention architectures, can automatically extract complex spatial, morphological, and temporal patterns from multimodal imaging data including MRI, CT, PET, mammography, histopathology, and ultrasound. These models surpass traditional radiological assessment by identifying subtle tissue abnormalities and tumor microenvironment variations invisible to the human eye. At a broader scale, the integration of multimodal imaging with radiogenomics linking quantitative imaging features with genomics, transcriptomics, and epigenetic biomarkers has introduced a new paradigm for personalized oncology. This radiogenomic fusion allows the prediction of tumor genotype, immune response, molecular subtypes, and treatment resistance without invasive biopsies.</li>
</ul>

<h3>Title: Graph Data Augmentation with Contrastive Learning on Covariate Distribution Shift</h3>
<ul>
<li><strong>Authors: </strong>Fanlong Zeng, Wensheng Gan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00716">https://arxiv.org/abs/2512.00716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00716">https://arxiv.org/pdf/2512.00716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00716]] Graph Data Augmentation with Contrastive Learning on Covariate Distribution Shift(https://arxiv.org/abs/2512.00716)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Covariate distribution shift occurs when certain structural features present in the test set are absent from the training set. It is a common type of out-of-distribution (OOD) problem, frequently encountered in real-world graph data with complex structures. Existing research has revealed that most out-of-the-box graph neural networks (GNNs) fail to account for covariate shifts. Furthermore, we observe that existing methods aimed at addressing covariate shifts often fail to fully leverage the rich information contained within the latent space. Motivated by the potential of the latent space, we introduce a new method called MPAIACL for More Powerful Adversarial Invariant Augmentation using Contrastive Learning. MPAIACL leverages contrastive learning to unlock the full potential of vector representations by harnessing their intrinsic information. Through extensive experiments, MPAIACL demonstrates its robust generalization and effectiveness, as it performs well compared with other baselines across various public OOD datasets. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: RS-ISRefiner: Towards Better Adapting Vision Foundation Models for Interactive Segmentation of Remote Sensing Images</h3>
<ul>
<li><strong>Authors: </strong>Deliang Wang, Peng Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00718">https://arxiv.org/abs/2512.00718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00718">https://arxiv.org/pdf/2512.00718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00718]] RS-ISRefiner: Towards Better Adapting Vision Foundation Models for Interactive Segmentation of Remote Sensing Images(https://arxiv.org/abs/2512.00718)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Interactive image segmentation(IIS) plays a critical role in generating precise annotations for remote sensing imagery, where objects often exhibit scale variations, irregular boundaries and complex backgrounds. However, existing IIS methods, primarily designed for natural images, struggle to generalize to remote sensing domains due to limited annotated data and computational overhead. To address these challenges, we proposed RS-ISRefiner, a novel click-based IIS framework tailored for remote sensing images. The framework employs an adapter-based tuning strategy that preserves the general representations of Vision Foundation Models while enabling efficient learning of remote sensing-specific spatial and boundary characteristics. A hybrid attention mechanism integrating convolutional local modeling with Transformer-based global reasoning enhances robustness against scale diversity and scene complexity. Furthermore, an improved probability map modulation scheme effectively incorporates historical user interactions, yielding more stable iterative refinement and higher boundary fidelity. Comprehensive experiments on six remote sensing datasets, including iSAID, ISPRS Potsdam, SandBar, NWPU, LoveDA Urban and WHUBuilding, demonstrate that RS-ISRefiner consistently outperforms state-of-the-art IIS methods in terms of segmentation accuracy, efficiency and interaction cost. These results confirm the effectiveness and generalizability of our framework, making it highly suitable for high-quality instance segmentation in practical remote sensing scenarios.</li>
</ul>

<h3>Title: TrajDiff: End-to-end Autonomous Driving without Perception Annotation</h3>
<ul>
<li><strong>Authors: </strong>Xingtai Gui, Jianbo Zhao, Wencheng Han, Jikai Wang, Jiahao Gong, Feiyang Tan, Cheng-zhong Xu, Jianbing Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00723">https://arxiv.org/abs/2512.00723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00723">https://arxiv.org/pdf/2512.00723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00723]] TrajDiff: End-to-end Autonomous Driving without Perception Annotation(https://arxiv.org/abs/2512.00723)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>End-to-end autonomous driving systems directly generate driving policies from raw sensor inputs. While these systems can extract effective environmental features for planning, relying on auxiliary perception tasks, developing perception annotation-free planning paradigms has become increasingly critical due to the high cost of manual perception annotation. In this work, we propose TrajDiff, a Trajectory-oriented BEV Conditioned Diffusion framework that establishes a fully perception annotation-free generative method for end-to-end autonomous driving. TrajDiff requires only raw sensor inputs and future trajectory, constructing Gaussian BEV heatmap targets that inherently capture driving modalities. We design a simple yet effective trajectory-oriented BEV encoder to extract the TrajBEV feature without perceptual supervision. Furthermore, we introduce Trajectory-oriented BEV Diffusion Transformer (TB-DiT), which leverages ego-state information and the predicted TrajBEV features to directly generate diverse yet plausible trajectories, eliminating the need for handcrafted motion priors. Beyond architectural innovations, TrajDiff enables exploration of data scaling benefits in the annotation-free setting. Evaluated on the NAVSIM benchmark, TrajDiff achieves 87.5 PDMS, establishing state-of-the-art performance among all annotation-free methods. With data scaling, it further improves to 88.5 PDMS, which is comparable to advanced perception-based approaches. Our code and model will be made publicly available.</li>
</ul>

<h3>Title: Upcycled and Merged MoE Reward Model for Mitigating Reward Hacking</h3>
<ul>
<li><strong>Authors: </strong>Lingling Fu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00724">https://arxiv.org/abs/2512.00724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00724">https://arxiv.org/pdf/2512.00724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00724]] Upcycled and Merged MoE Reward Model for Mitigating Reward Hacking(https://arxiv.org/abs/2512.00724)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reward models play a critical role in Reinforcement Learning from Human Feedback (RLHF) by assessing the consistency between generated outputs and human preferences. However, conventional reward models are prone to reward hacking or over-optimization, where the policy exploits shortcut patterns to obtain high reward scores that do not reflect true human preference. Although Mixture-of-Experts (MoE)-based reward models can enhance discriminative capability, they typically introduce substantial computational overhead. To address these challenges, we propose an upcycle and merge MoE reward modeling approach. We first upcycle a dense reward model into a MoE architecture, where a shared expert captures general knowledge, while normal experts specialize in instruction-specific patterns. We then apply routing-weight normalization and merge experts back into a dense model through a learnable weight-averaging mechanism, preserving performance gains while significantly reducing inference cost. Experimental results demonstrate that our method effectively mitigates reward hacking across various model scales. Our work highlights the potential of upcycle and merge MoE structures for improving both robustness and efficiency of RLHF reward models.</li>
</ul>

<h3>Title: ESMC: MLLM-Based Embedding Selection for Explainable Multiple Clustering</h3>
<ul>
<li><strong>Authors: </strong>Xinyue Wang, Yuheng Jia, Hui Liu, Junhui Hou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00725">https://arxiv.org/abs/2512.00725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00725">https://arxiv.org/pdf/2512.00725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00725]] ESMC: MLLM-Based Embedding Selection for Explainable Multiple Clustering(https://arxiv.org/abs/2512.00725)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Typical deep clustering methods, while achieving notable progress, can only provide one clustering result per dataset. This limitation arises from their assumption of a fixed underlying data distribution, which may fail to meet user needs and provide unsatisfactory clustering outcomes. Our work investigates how multi-modal large language models (MLLMs) can be leveraged to achieve user-driven clustering, emphasizing their adaptability to user-specified semantic requirements. However, directly using MLLM output for clustering has risks for producing unstructured and generic image descriptions instead of feature-specific and concrete ones. To address these issues, our method first discovers that MLLMs' hidden states of text tokens are strongly related to the corresponding features, and leverages these embeddings to perform clusterings from any user-defined criteria. We also employ a lightweight clustering head augmented with pseudo-label learning, significantly enhancing clustering accuracy. Extensive experiments demonstrate its competitive performance on diverse datasets and metrics.</li>
</ul>

<h3>Title: Deep Learning for Modeling and Dispatching Hybrid Wind Farm Power Generation</h3>
<ul>
<li><strong>Authors: </strong>Zach Lawrence, Jessica Yao, Chris Qin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00728">https://arxiv.org/abs/2512.00728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00728">https://arxiv.org/pdf/2512.00728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00728]] Deep Learning for Modeling and Dispatching Hybrid Wind Farm Power Generation(https://arxiv.org/abs/2512.00728)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Wind farms with integrated energy storage, or hybrid wind farms, are able to store energy and dispatch it to the grid following an operational strategy. For individual wind farms with integrated energy storage capacity, data-driven dispatch strategies using localized grid demand and market conditions as input parameters stand to maximize wind energy value. Synthetic power generation data modeled on atmospheric conditions provide another avenue for improving the robustness of data-driven dispatch strategies. To these ends, the present work develops two deep learning frameworks: COVE-NN, an LSTM-based dispatch strategy tailored to individual wind farms, which reduced annual COVE by 32.3% over 43 years of simulated operations in a case study at the Pyron site; and a power generation modeling framework that reduced RMSE by 9.5% and improved power curve similarity by 18.9% when validated on the Palouse wind farm. Together, these models pave the way for more robust, data-driven dispatch strategies and potential extensions to other renewable energy systems.</li>
</ul>

<h3>Title: REM: Evaluating LLM Embodied Spatial Reasoning through Multi-Frame Trajectories</h3>
<ul>
<li><strong>Authors: </strong>Jacob Thompson, Emiliano Garcia-Lopez, Yonatan Bisk</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00736">https://arxiv.org/abs/2512.00736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00736">https://arxiv.org/pdf/2512.00736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00736]] REM: Evaluating LLM Embodied Spatial Reasoning through Multi-Frame Trajectories(https://arxiv.org/abs/2512.00736)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Humans build viewpoint-independent cognitive maps through navigation, enabling intuitive reasoning about object permanence and spatial relations. We argue that multimodal large language models (MLLMs), despite extensive video training, lack this fundamental spatial reasoning capability, a critical limitation for embodied applications. To demonstrate these limitations and drive research, we introduce REM (Reasoning over Embodied Multi-Frame Trajectories), a benchmark using controllable 3D environments for long-horizon embodied spatial reasoning. REM systematically evaluates key aspects like object permanence/distinction, spatial relationships, and numerical tracking across dynamic embodied viewpoints. Our evaluation shows that the best-performing current models exhibit promising overall performance, but become increasingly unreliable at even moderate complexity levels easily handled by humans. These findings highlight challenges MLLMs face in developing robust spatial representations from sequential visual input. Consequently, REM provides targeted metrics and diagnostics to foster improved spatial understanding in future models.</li>
</ul>

<h3>Title: MASCOT: Analyzing Malware Evolution Through A Well-Curated Source Code Dataset</h3>
<ul>
<li><strong>Authors: </strong>Bojing Li, Duo Zhong, Dharani Nadendla, Gabriel Terceros, Prajna Bhandar, Raguvir S, Charles Nicholas</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00741">https://arxiv.org/abs/2512.00741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00741">https://arxiv.org/pdf/2512.00741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00741]] MASCOT: Analyzing Malware Evolution Through A Well-Curated Source Code Dataset(https://arxiv.org/abs/2512.00741)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>In recent years, the explosion of malware and extensive code reuse have formed complex evolutionary connections among malware specimens. The rapid pace of development makes it challenging for existing studies to characterize recent evolutionary trends. In addition, intuitive tools to untangle these intricate connections between malware specimens or categories are urgently needed. This paper introduces a manually-reviewed malware source code dataset containing 6032 specimens. Building on and extending current research from a software engineering perspective, we systematically evaluate the scale, development costs, code quality, as well as security and dependencies of modern malware. We further introduce a multi-view genealogy analysis to clarify malware connections: at an overall view, this analysis quantifies the strength and direction of connections among specimens and categories; at a detailed view, it traces the evolutionary histories of individual specimens. Experimental results indicate that, despite persistent shortcomings in code quality, malware specimens exhibit an increasing complexity and standardization, in step with the development of mainstream software engineering practices. Meanwhile, our genealogy analysis intuitively reveals lineage expansion and evolution driven by code reuse, providing new evidence and tools for understanding the formation and evolution of the malware ecosystem.</li>
</ul>

<h3>Title: Joint Multi-scale Gated Transformer and Prior-guided Convolutional Network for Learned Image Compression</h3>
<ul>
<li><strong>Authors: </strong>Zhengxin Chen, Xiaohai He, Tingrong Zhang, Shuhua Xiong, Chao Ren</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00744">https://arxiv.org/abs/2512.00744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00744">https://arxiv.org/pdf/2512.00744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00744]] Joint Multi-scale Gated Transformer and Prior-guided Convolutional Network for Learned Image Compression(https://arxiv.org/abs/2512.00744)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recently, learned image compression methods have made remarkable achievements, some of which have outperformed the traditional image codec VVC. The advantages of learned image compression methods over traditional image codecs can be largely attributed to their powerful nonlinear transform coding. Convolutional layers and shifted window transformer (Swin-T) blocks are the basic units of neural networks, and their representation capabilities play an important role in nonlinear transform coding. In this paper, to improve the ability of the vanilla convolution to extract local features, we propose a novel prior-guided convolution (PGConv), where asymmetric convolutions (AConvs) and difference convolutions (DConvs) are introduced to strengthen skeleton elements and extract high-frequency information, respectively. A re-parameterization strategy is also used to reduce the computational complexity of PGConv. Moreover, to improve the ability of the Swin-T block to extract non-local features, we propose a novel multi-scale gated transformer (MGT), where dilated window-based multi-head self-attention blocks with different dilation rates and depth-wise convolution layers with different kernel sizes are used to extract multi-scale features, and a gate mechanism is introduced to enhance non-linearity. Finally, we propose a novel joint Multi-scale Gated Transformer and Prior-guided Convolutional Network (MGTPCN) for learned image compression. Experimental results show that our MGTPCN surpasses state-of-the-art algorithms with a better trade-off between performance and complexity.</li>
</ul>

<h3>Title: FastPOS: Language-Agnostic Scalable POS Tagging Framework Low-Resource Use Case</h3>
<ul>
<li><strong>Authors: </strong>Md Abdullah Al Kafi, Sumit Kumar Banshal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00745">https://arxiv.org/abs/2512.00745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00745">https://arxiv.org/pdf/2512.00745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00745]] FastPOS: Language-Agnostic Scalable POS Tagging Framework Low-Resource Use Case(https://arxiv.org/abs/2512.00745)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This study proposes a language-agnostic transformer-based POS tagging framework designed for low-resource languages, using Bangla and Hindi as case studies. With only three lines of framework-specific code, the model was adapted from Bangla to Hindi, demonstrating effective portability with minimal modification. The framework achieves 96.85 percent and 97 percent token-level accuracy across POS categories in Bangla and Hindi while sustaining strong F1 scores despite dataset imbalance and linguistic overlap. A performance discrepancy in a specific POS category underscores ongoing challenges in dataset curation. The strong results stem from the underlying transformer architecture, which can be replaced with limited code adjustments. Its modular and open-source design enables rapid cross-lingual adaptation while reducing model design and tuning overhead, allowing researchers to focus on linguistic preprocessing and dataset refinement, which are essential for advancing NLP in underrepresented languages.</li>
</ul>

<h3>Title: Probabilistic Modeling of Multi-rater Medical Image Segmentation for Diversity and Personalization</h3>
<ul>
<li><strong>Authors: </strong>Ke Liu, Shangde Gao, Yichao Fu, Shangqi Gao, Chunhua Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00748">https://arxiv.org/abs/2512.00748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00748">https://arxiv.org/pdf/2512.00748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00748]] Probabilistic Modeling of Multi-rater Medical Image Segmentation for Diversity and Personalization(https://arxiv.org/abs/2512.00748)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation is inherently influenced by data uncertainty, arising from ambiguous boundaries in medical scans and inter-observer variability in diagnosis. To address this challenge, previous works formulated the multi-rater medical image segmentation task, where multiple experts provide separate annotations for each image. However, existing models are typically constrained to either generate diverse segmentation that lacks expert specificity or to produce personalized outputs that merely replicate individual annotators. We propose Probabilistic modeling of multi-rater medical image Segmentation (ProSeg) that simultaneously enables both diversification and personalization. Specifically, we introduce two latent variables to model expert annotation preferences and image boundary ambiguity. Their conditional probabilistic distributions are then obtained through variational inference, allowing segmentation outputs to be generated by sampling from these distributions. Extensive experiments on both the nasopharyngeal carcinoma dataset (NPC) and the lung nodule dataset (LIDC-IDRI) demonstrate that our ProSeg achieves a new state-of-the-art performance, providing segmentation results that are both diverse and expert-personalized. Code can be found in this https URL.</li>
</ul>

<h3>Title: Charts Are Not Images: On the Challenges of Scientific Chart Editing</h3>
<ul>
<li><strong>Authors: </strong>Shawn Li, Ryan Rossi, Sungchul Kim, Sunav Choudhary, Franck Dernoncourt, Puneet Mathur, Zhengzhong Tu, Yue Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00752">https://arxiv.org/abs/2512.00752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00752">https://arxiv.org/pdf/2512.00752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00752]] Charts Are Not Images: On the Challenges of Scientific Chart Editing(https://arxiv.org/abs/2512.00752)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative models, such as diffusion and autoregressive approaches, have demonstrated impressive capabilities in editing natural images. However, applying these tools to scientific charts rests on a flawed assumption: a chart is not merely an arrangement of pixels but a visual representation of structured data governed by a graphical grammar. Consequently, chart editing is not a pixel-manipulation task but a structured transformation problem. To address this fundamental mismatch, we introduce \textit{FigEdit}, a large-scale benchmark for scientific figure editing comprising over 30,000 samples. Grounded in real-world data, our benchmark is distinguished by its diversity, covering 10 distinct chart types and a rich vocabulary of complex editing instructions. The benchmark is organized into five distinct and progressively challenging tasks: single edits, multi edits, conversational edits, visual-guidance-based edits, and style transfer. Our evaluation of a range of state-of-the-art models on this benchmark reveals their poor performance on scientific figures, as they consistently fail to handle the underlying structured transformations required for valid edits. Furthermore, our analysis indicates that traditional evaluation metrics (e.g., SSIM, PSNR) have limitations in capturing the semantic correctness of chart edits. Our benchmark demonstrates the profound limitations of pixel-level manipulation and provides a robust foundation for developing and evaluating future structure-aware models. By releasing \textit{FigEdit} (this https URL), we aim to enable systematic progress in structure-aware figure editing, provide a common ground for fair comparison, and encourage future research on models that understand both the visual and semantic layers of scientific charts.</li>
</ul>

<h3>Title: Preventing Model Collapse via Contraction-Conditioned Neural Filters</h3>
<ul>
<li><strong>Authors: </strong>Zongjian Han, Yiran Liang, Ruiwen Wang, Yiwei Luo, Yilin Huang, Xiaotong Song, Dongqing Wei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00757">https://arxiv.org/abs/2512.00757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00757">https://arxiv.org/pdf/2512.00757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00757]] Preventing Model Collapse via Contraction-Conditioned Neural Filters(https://arxiv.org/abs/2512.00757)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents a neural network filter method based on contraction operators to address model collapse in recursive training of generative models. Unlike \cite{xu2024probabilistic}, which requires superlinear sample growth ($O(t^{1+s})$), our approach completely eliminates the dependence on increasing sample sizes within an unbiased estimation framework by designing a neural filter that learns to satisfy contraction conditions. We develop specialized neural network architectures and loss functions that enable the filter to actively learn contraction conditions satisfying Assumption 2.3 in exponential family distributions, thereby ensuring practical application of our theoretical results. Theoretical analysis demonstrates that when the learned contraction conditions are satisfied, estimation errors converge probabilistically even with constant sample sizes, i.e., $\limsup_{t\to\infty}\mathbb{P}(\|\mathbf{e}_t\|>\delta)=0$ for any $\delta>0$. Experimental results show that our neural network filter effectively learns contraction conditions and prevents model collapse under fixed sample size settings, providing an end-to-end solution for practical applications.</li>
</ul>

<h3>Title: Forecasting India's Demographic Transition Under Fertility Policy Scenarios Using hybrid LSTM-PINN Model</h3>
<ul>
<li><strong>Authors: </strong>Subarna Khanra, Vijay Kumar Kukreja, Indu Bala</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00760">https://arxiv.org/abs/2512.00760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00760">https://arxiv.org/pdf/2512.00760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00760]] Forecasting India's Demographic Transition Under Fertility Policy Scenarios Using hybrid LSTM-PINN Model(https://arxiv.org/abs/2512.00760)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Demographic forecasting remains a fundamental challenge for policy planning in rapidly evolving nations such as India, where fertility transitions, policy interventions, and age structured dynamics interact in complex ways. In this study, we present a hybrid modelling framework that integrates policy-aware fertility functions into a Physics-Informed Neural Network (PINN) enhanced with Long Short-Term Memory (LSTM) networks to capture physical constraints and temporal dependencies in population dynamics. The model is applied to India's age structured population from 2024 to 2054 under three fertility-policy scenarios: continuation of current fertility decline, stricter population control, and relaxed fertility promotion. The governing transport-reaction partial differential equation is formulated with India-specific demographic indicators, including age-specific fertility and mortality rates. PINNs embed the core population equation and policy-driven fertility changes, while LSTM layers improve long-term forecasting across decades. Results show that fertility policies substantially shape future age distribution, dependency ratios, and workforce size. Stricter controls intensify ageing and reduce labour force participation, whereas relaxed policies support workforce growth but increase population pressure. Our findings suggest that the hybrid LSTM-PINN is an effective approach for demographic forecasting, offering accuracy with interpretability. Beyond methodological novelty, this work provides actionable insights for India's demographic policy debates, highlighting the need for balanced fertility interventions to ensure sustainable socio-economic development.</li>
</ul>

<h3>Title: The Outline of Deception: Physical Adversarial Attacks on Traffic Signs Using Edge Patches</h3>
<ul>
<li><strong>Authors: </strong>Haojie Jia, Te Hu, Haowen Li, Long Jin, Chongshi Xin, Yuchi Yao, Jiarui Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00765">https://arxiv.org/abs/2512.00765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00765">https://arxiv.org/pdf/2512.00765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00765]] The Outline of Deception: Physical Adversarial Attacks on Traffic Signs Using Edge Patches(https://arxiv.org/abs/2512.00765)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, steal, segmentation</a></li>
<li><strong>Abstract: </strong>Intelligent driving systems are vulnerable to physical adversarial attacks on traffic signs. These attacks can cause misclassification, leading to erroneous driving decisions that compromise road safety. Moreover, within V2X networks, such misinterpretations can propagate, inducing cascading failures that disrupt overall traffic flow and system stability. However, a key limitation of current physical attacks is their lack of stealth. Most methods apply perturbations to central regions of the sign, resulting in visually salient patterns that are easily detectable by human observers, thereby limiting their real-world practicality. This study proposes TESP-Attack, a novel stealth-aware adversarial patch method for traffic sign classification. Based on the observation that human visual attention primarily focuses on the central regions of traffic signs, we employ instance segmentation to generate edge-aligned masks that conform to the shape characteristics of the signs. A U-Net generator is utilized to craft adversarial patches, which are then optimized through color and texture constraints along with frequency domain analysis to achieve seamless integration with the background environment, resulting in highly effective visual concealment. The proposed method demonstrates outstanding attack success rates across traffic sign classification models with varied architectures, achieving over 90% under limited query budgets. It also exhibits strong cross-model transferability and maintains robust real-world performance that remains stable under varying angles and distances.</li>
</ul>

<h3>Title: Text Mining Analysis of Symptom Patterns in Medical Chatbot Conversations</h3>
<ul>
<li><strong>Authors: </strong>Hamed Razavi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00768">https://arxiv.org/abs/2512.00768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00768">https://arxiv.org/pdf/2512.00768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00768]] Text Mining Analysis of Symptom Patterns in Medical Chatbot Conversations(https://arxiv.org/abs/2512.00768)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The fast growth of digital health systems has led to a need to better comprehend how they interpret and represent patient-reported symptoms. Chatbots have been used in healthcare to provide clinical support and enhance the user experience, making it possible to provide meaningful clinical patterns from text-based data through chatbots. The proposed research utilises several different natural language processing methods to study the occurrences of symptom descriptions in medicine as well as analyse the patterns that emerge through these conversations within medical bots. Through the use of the Medical Conversations to Disease Dataset which contains 960 multi-turn dialogues divided into 24 Clinical Conditions, a standardised representation of conversations between patient and bot is created for further analysis by computational means. The multi-method approach uses a variety of tools, including Latent Dirichlet Allocation (LDA) to identify latent symptom themes, K-Means to group symptom descriptions by similarity, Transformer-based Named Entity Recognition (NER) to extract medical concepts, and the Apriori algorithm to discover frequent symptom pairs. Findings from the analysis indicate a coherent structure of clinically relevant topics, moderate levels of clustering cohesiveness and several high confidence rates on the relationships between symptoms like fever headache and rash itchiness. The results support the notion that conversational medical data can be a valuable diagnostic signal for early symptom interpretation, assist in strengthening decision support and improve how users interact with tele-health technology. By demonstrating a method for converting unstructured free-flowing dialogue into actionable knowledge regarding symptoms this work provides an extensible framework to further enhance future performance, dependability and clinical utility of selecting medical chatbots.</li>
</ul>

<h3>Title: AI Agent for Source Finding by SoFiA-2 for SKA-SDC2</h3>
<ul>
<li><strong>Authors: </strong>Xingchen Zhou, Nan Li, Peng Jia, Yingfeng Liu, Furen Deng, Shuanghao Shu, Ying Li, Liang Cao, Huanyuan Shan, Ayodeji Ibitoye</a></li>
<li><strong>Subjects: </strong>cs.LG, astro-ph.GA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00769">https://arxiv.org/abs/2512.00769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00769">https://arxiv.org/pdf/2512.00769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00769]] AI Agent for Source Finding by SoFiA-2 for SKA-SDC2(https://arxiv.org/abs/2512.00769)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Source extraction is crucial in analyzing data from next-generation, large-scale sky surveys in radio bands, such as the Square Kilometre Array (SKA). Several source extraction programs, including SoFiA and Aegean, have been developed to address this challenge. However, finding optimal parameter configurations when applying these programs to real observations is non-trivial. For example, the outcomes of SoFiA intensely depend on several key parameters across its preconditioning, source-finding, and reliability-filtering modules. To address this issue, we propose a framework to automatically optimize these parameters using an AI agent based on a state-of-the-art reinforcement learning (RL) algorithm, i.e., Soft Actor-Critic (SAC). The SKA Science Data Challenge 2 (SDC2) dataset is utilized to assess the feasibility and reliability of this framework. The AI agent interacts with the environment by adjusting parameters based on the feedback from the SDC2 score defined by the SDC2 Team, progressively learning to select parameter sets that yield improved performance. After sufficient training, the AI agent can automatically identify an optimal parameter configuration that outperform the benchmark set by Team SoFiA within only 100 evaluation steps and with reduced time consumption. Our approach could address similar problems requiring complex parameter tuning, beyond radio band surveys and source extraction. Yet, high-quality training sets containing representative observations and catalogs of ground truth are essential.</li>
</ul>

<h3>Title: EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes</h3>
<ul>
<li><strong>Authors: </strong>Xiaoshan Wu, Yifei Yu, Xiaoyang Lyu, Yihua Huang, Bo Wang, Baoheng Zhang, Zhongrui Wang, Xiaojuan Qi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00771">https://arxiv.org/abs/2512.00771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00771">https://arxiv.org/pdf/2512.00771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00771]] EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes(https://arxiv.org/abs/2512.00771)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose EAG3R, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks.</li>
</ul>

<h3>Title: DEJIMA: A Novel Large-scale Japanese Dataset for Image Captioning and Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Toshiki Katsube, Taiga Fukuhara, Kenichiro Ando, Yusuke Mukuta, Kohei Uehara, Tatsuya Harada</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00773">https://arxiv.org/abs/2512.00773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00773">https://arxiv.org/pdf/2512.00773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00773]] DEJIMA: A Novel Large-scale Japanese Dataset for Image Captioning and Visual Question Answering(https://arxiv.org/abs/2512.00773)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>This work addresses the scarcity of high-quality, large-scale resources for Japanese Vision-and-Language (V&L) modeling. We present a scalable and reproducible pipeline that integrates large-scale web collection with rigorous filtering/deduplication, object-detection-driven evidence extraction, and Large Language Model (LLM)-based refinement under grounding constraints. Using this pipeline, we build two resources: an image-caption dataset (DEJIMA-Cap) and a VQA dataset (DEJIMA-VQA), each containing 3.88M image-text pairs, far exceeding the size of existing Japanese V&L datasets. Human evaluations demonstrate that DEJIMA achieves substantially higher Japaneseness and linguistic naturalness than datasets constructed via translation or manual annotation, while maintaining factual correctness at a level comparable to human-annotated corpora. Quantitative analyses of image feature distributions further confirm that DEJIMA broadly covers diverse visual domains characteristic of Japan, complementing its linguistic and cultural representativeness. Models trained on DEJIMA exhibit consistent improvements across multiple Japanese multimodal benchmarks, confirming that culturally grounded, large-scale resources play a key role in enhancing model performance. All data sources and modules in our pipeline are licensed for commercial use, and we publicly release the resulting dataset and metadata to encourage further research and industrial applications in Japanese V&L modeling.</li>
</ul>

<h3>Title: What Is Preference Optimization Doing, How and Why?</h3>
<ul>
<li><strong>Authors: </strong>Yue Wang, Qizhou Wang, Zizhuo Zhang, Ang Li, Gang Niu, Bo Han, Masashi Sugiyama</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00778">https://arxiv.org/abs/2512.00778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00778">https://arxiv.org/pdf/2512.00778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00778]] What Is Preference Optimization Doing, How and Why?(https://arxiv.org/abs/2512.00778)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Preference optimization (PO) is indispensable for large language models (LLMs), with methods such as direct preference optimization (DPO) and proximal policy optimization (PPO) achieving great success. A common belief is that DPO is supervised learning while PPO is reinforcement learning, yet deeper analyses for the reasons underlying these differences remain lacking. To fill this gap, we analyze their optimization dynamics, revealing distinct algorithmic behaviors and comprehending their underlying causes. First, we examine the target directions of gradient-based updates and find that DPO follows stable targets, whereas PPO follows dynamic targets that balance exploration and exploitation, thus validating the common belief from a new perspective. Second, we examine the roles of positive learning, negative learning, and loss reweighting, which are three key components in PO methods. Our analyses reveal that these components play fairly different roles. In DPO, positive and negative learning jointly shape the learning targets meanwhile mutually offset each other. However, loss reweighting in DPO acts less as a reward signal but more as a regularizer to mitigate overfitting. In PPO, negative learning primarily supports exploration rather than determining the targets. Meanwhile, loss reweighting, related to absolute values of token-level advantages, indicates the distinct roles of token groups in updating targets. Given these findings, we conduct carefully designed ablation studies to further examine how controlling these dynamics impacts optimization efficiency and practical performance. The insights gained from our analyses not only deepen the understanding of PO methods but also inspire the development of more preference-aligned LLMs.</li>
</ul>

<h3>Title: Auxiliary-Hyperparameter-Free Sampling: Entropy Equilibrium for Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaodong Cai, Hai Lin, Shaoxiong Zhan, Weiqi Luo, Hong-Gee Kim, Hongyan Hao, Yu Yang, Hai-Tao Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00789">https://arxiv.org/abs/2512.00789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00789">https://arxiv.org/pdf/2512.00789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00789]] Auxiliary-Hyperparameter-Free Sampling: Entropy Equilibrium for Text Generation(https://arxiv.org/abs/2512.00789)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Token sampling strategies critically influence text generation quality in large language models (LLMs). However, existing methods introduce additional hyperparameters, requiring extensive tuning and complicating deployment. We present Entropy Equilibrium Sampling (EES), an auxiliary hyperparameter-free approach inspired by information theory that can dynamically adjust candidate sets by balancing normalized entropy with probability mass. We evaluate EES on both reasoning and generation tasks across a range of model architectures. Our results show that EES consistently performs well across temperature settings, delivering competitive accuracy and coherence while maintaining diversity. By eliminating the need for hyperparameter tuning, EES greatly simplifies deployment while improving performance. Code is available at this https URL</li>
</ul>

<h3>Title: Estimating the Effective Rank of Vision Transformers via Low-Rank Factorization</h3>
<ul>
<li><strong>Authors: </strong>Liyu Zerihun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00792">https://arxiv.org/abs/2512.00792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00792">https://arxiv.org/pdf/2512.00792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00792]] Estimating the Effective Rank of Vision Transformers via Low-Rank Factorization(https://arxiv.org/abs/2512.00792)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Deep networks are heavily over-parameterized, yet their learned representations often admit low-rank structure. We introduce a framework for estimating a model's intrinsic dimensionality by treating learned representations as projections onto a low-rank subspace of the model's full capacity. Our approach: train a full-rank teacher, factorize its weights at multiple ranks, and train each factorized student via distillation to measure performance as a function of rank. We define effective rank as a region, not a point: the smallest contiguous set of ranks for which the student reaches 85-95% of teacher accuracy. To stabilize estimates, we fit accuracy vs. rank with a monotone PCHIP interpolant and identify crossings of the normalized curve. We also define the effective knee as the rank maximizing perpendicular distance between the smoothed accuracy curve and its endpoint secant; an intrinsic indicator of where marginal gains concentrate. On ViT-B/32 fine-tuned on CIFAR-100 (one seed, due to compute constraints), factorizing linear blocks and training with distillation yields an effective-rank region of approximately [16, 34] and an effective knee at r* ~ 31. At rank 32, the student attains 69.46% top-1 accuracy vs. 73.35% for the teacher (~94.7% of baseline) while achieving substantial parameter compression. We provide a framework to estimate effective-rank regions and knees across architectures and datasets, offering a practical tool for characterizing the intrinsic dimensionality of deep models.</li>
</ul>

<h3>Title: CircleFlow: Flow-Guided Camera Blur Estimation using a Circle Grid Target</h3>
<ul>
<li><strong>Authors: </strong>Jiajian He, Enjie Hu, Shiqi Chen, Tianchen Qiu, Huajun Feng, Zhihai Xu, Yueting Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00796">https://arxiv.org/abs/2512.00796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00796">https://arxiv.org/pdf/2512.00796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00796]] CircleFlow: Flow-Guided Camera Blur Estimation using a Circle Grid Target(https://arxiv.org/abs/2512.00796)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The point spread function (PSF) serves as a fundamental descriptor linking the real-world scene to the captured signal, manifesting as camera blur. Accurate PSF estimation is crucial for both optical characterization and computational vision, yet remains challenging due to the inherent ambiguity and the ill-posed nature of intensity-based deconvolution. We introduce CircleFlow, a high-fidelity PSF estimation framework that employs flow-guided edge localization for precise blur characterization. CircleFlow begins with a structured capture that encodes locally anisotropic and spatially varying PSFs by imaging a circle grid target, while leveraging the target's binary luminance prior to decouple image and kernel estimation. The latent sharp image is then reconstructed through subpixel alignment of an initialized binary structure guided by optical flow, whereas the PSF is modeled as an energy-constrained implicit neural representation. Both components are jointly optimized within a demosaicing-aware differentiable framework, ensuring physically consistent and robust PSF estimation enabled by accurate edge localization. Extensive experiments on simulated and real-world data demonstrate that CircleFlow achieves state-of-the-art accuracy and reliability, validating its effectiveness for practical PSF calibration.</li>
</ul>

<h3>Title: Bias Injection Attacks on RAG Databases and Sanitization Defenses</h3>
<ul>
<li><strong>Authors: </strong>Hao Wu, Prateek Saxena</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00804">https://arxiv.org/abs/2512.00804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00804">https://arxiv.org/pdf/2512.00804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00804]] Bias Injection Attacks on RAG Databases and Sanitization Defenses(https://arxiv.org/abs/2512.00804)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>This paper explores attacks and defenses on vector databases in retrieval-augmented generation (RAG) systems. Prior work on knowledge poisoning attacks primarily inject false or toxic content, which fact-checking or linguistic analysis easily detects. We reveal a new and subtle threat: bias injection attacks, which insert factually correct yet semantically biased passages into the knowledge base to covertly influence the ideological framing of answers generated by large language models (LLMs). We demonstrate that these adversarial passages, though linguistically coherent and truthful, can systematically crowd out opposing views from the retrieved context and steer LLM answers toward the attacker's intended perspective. We precisely characterize this class of attacks and then develop a post-retrieval filtering defense, BiasDef. We construct a comprehensive benchmark based on public question answering datasets to evaluate them. Our results show that: (1) the proposed attack induces significant perspective shifts in LLM answers, effectively evading existing retrieval-based sanitization defenses; and (2) BiasDef outperforms existing methods by reducing adversarial passages retrieved by 15\% which mitigates perspective shift by 6.2\times in answers, while enabling the retrieval of 62\% more benign passages.</li>
</ul>

<h3>Title: Thinking with Drafts: Speculative Temporal Reasoning for Efficient Long Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Pengfei Hu, Meng Cao, Yingyao Wang, Yi Wang, Jiahua Dong, Jun Song, Yu Cheng, Bo Zheng, Xiaodan Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00805">https://arxiv.org/abs/2512.00805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00805">https://arxiv.org/pdf/2512.00805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00805]] Thinking with Drafts: Speculative Temporal Reasoning for Efficient Long Video Understanding(https://arxiv.org/abs/2512.00805)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long video understanding is essential for human-like intelligence, enabling coherent perception and reasoning over extended temporal contexts. While the emerging thinking-with-frames paradigm, which alternates between global temporal reasoning and local frame examination, has advanced the reasoning capabilities of video multi-modal large language models (MLLMs), it suffers from a significant efficiency bottleneck due to the progressively growing and redundant multi-modal context. To address this, we propose SpecTemp, a reinforcement learning-based Speculative Temporal reasoning framework that decouples temporal perception from reasoning via a cooperative dual-model design. In SpecTemp, a lightweight draft MLLM rapidly explores and proposes salient frames from densely sampled temporal regions, while a powerful target MLLM focuses on temporal reasoning and verifies the draft's proposals, iteratively refining its attention until convergence. This design mirrors the collaborative pathways of the human brain, balancing efficiency with accuracy. To support training, we construct the SpecTemp-80K dataset, featuring synchronized dual-level annotations for coarse evidence spans and fine-grained frame-level evidence. Experiments across multiple video understanding benchmarks demonstrate that SpecTemp not only maintains competitive accuracy but also significantly accelerates inference compared with existing thinking-with-frames methods.</li>
</ul>

<h3>Title: Causal Invariance and Counterfactual Learning Driven Cooperative Game for Multi-Label Classification</h3>
<ul>
<li><strong>Authors: </strong>Yijia Fan, Jusheng Zhang, Kaitong Cai, Jing Yang, Keze Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00812">https://arxiv.org/abs/2512.00812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00812">https://arxiv.org/pdf/2512.00812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00812]] Causal Invariance and Counterfactual Learning Driven Cooperative Game for Multi-Label Classification(https://arxiv.org/abs/2512.00812)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Multi-label classification (MLC) remains vulnerable to label imbalance, spurious correlations, and distribution shifts, challenges that are particularly detrimental to rare label prediction. To address these limitations, we introduce the Causal Cooperative Game (CCG) framework, which conceptualizes MLC as a cooperative multi-player interaction. CCG unifies explicit causal discovery via Neural Structural Equation Models with a counterfactual curiosity reward to drive robust feature learning. Furthermore, it incorporates a causal invariance loss to ensure generalization across diverse environments, complemented by a specialized enhancement strategy for rare labels. Extensive benchmarking demonstrates that CCG substantially outperforms strong baselines in both rare label prediction and overall robustness. Through rigorous ablation studies and qualitative analysis, we validate the efficacy and interpretability of our components, underscoring the potential of synergizing causal inference with cooperative game theory for advancing multi-label learning.</li>
</ul>

<h3>Title: Accelerating Bangla NLP Tasks with Automatic Mixed Precision: Resource-Efficient Training Preserving Model Efficacy</h3>
<ul>
<li><strong>Authors: </strong>Md Mehrab Hossain Opi, Sumaiya Khan, Moshammad Farzana Rahman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00829">https://arxiv.org/abs/2512.00829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00829">https://arxiv.org/pdf/2512.00829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00829]] Accelerating Bangla NLP Tasks with Automatic Mixed Precision: Resource-Efficient Training Preserving Model Efficacy(https://arxiv.org/abs/2512.00829)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Training models for Natural Language Processing (NLP) requires substantial computational resources and time, posing significant challenges, especially for NLP development in Bangla, where access to high-end hardware is often limited. In this work, we explore automatic mixed precision (AMP) training as a means to improve computational efficiency without sacrificing model performance. By leveraging a dynamic mix of 16-bit and 32-bit floating-point computations, AMP lowers GPU memory requirements and speeds up training without degrading model performance. We evaluate AMP across four standard Bangla NLP tasks, namely sentiment analysis, named entity recognition, error classification, and question answering, using four transformer-based models: BanglaBERT, BanglishBERT, XLM-R, and mBERT. Our results demonstrate that AMP accelerates training by 44.5% and reduces memory consumption by 17.6%, while maintaining F-1 score within 99.7% of the full-precision baselines. This empirical study highlights AMP's potential to democratize access to state-of-the-art NLP capabilities in hardware-constrained settings by lowering computational barriers.</li>
</ul>

<h3>Title: ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Zeng, Shuibai Zhang, Wonjun Kang, Shutong Wu, Lynnix Zou, Ying Fan, Heeju Kim, Ziqian Lin, Jungtaek Kim, Hyung Il Koo, Dimitris Papailiopoulos, Kangwook Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00831">https://arxiv.org/abs/2512.00831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00831">https://arxiv.org/pdf/2512.00831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00831]] ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning(https://arxiv.org/abs/2512.00831)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Reasoning Models (LRMs) are Large Language Models (LLMs) explicitly trained to generate long-form Chain-of-Thoughts (CoTs), achieving impressive success on challenging tasks like math and programming. However, their underlying reasoning "algorithms" remain poorly understood. To investigate this, we propose ReJump, which represents a reasoning trace as a visitation order over nodes in a tree of intermediate problem-solving steps. Transitions between nodes, which we term jumps, include adjacent moves that capture behaviors such as calculation, and non-adjacent moves that capture behaviors such as backtracking and verification. ReJump enables analyzing LLM reasoning with diverse metrics that quantify exploration, exploitation, overthinking, forgetting, and verification. Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying balance between exploration and exploitation). To further understand how learning strategies shape reasoning, we use ReJump to compare distilled LRMs with their teachers, CoT-prompted LLMs with LRMs, and to examine how the number of reasoning examples and reinforcement learning affect reasoning behavior. Finally, we show that ReJump can improve reasoning quality at test time through strategies such as ReJump-guided Best-of-N selection and prompt selection. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: Logic Encryption: This Time for Real</h3>
<ul>
<li><strong>Authors: </strong>Rupesh Raj Karn, Lakshmi Likhitha Mankali, Zeng Wang, Saideep Sreekumar, Prithwish Basu Roy, Ozgur Sinanoglu, Lilas Alrahis, Johann Knechtel</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00833">https://arxiv.org/abs/2512.00833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00833">https://arxiv.org/pdf/2512.00833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00833]] Logic Encryption: This Time for Real(https://arxiv.org/abs/2512.00833)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack</a></li>
<li><strong>Abstract: </strong>Modern circuits face various threats like reverse engineering, theft of intellectual property (IP), side-channel attacks, etc. Here, we present a novel approach for IP protection based on logic encryption (LE). Unlike established schemes for logic locking, our work obfuscates the circuit's structure and functionality by encoding and encrypting the logic itself. We devise an end-to-end method for practical LE implementation based on standard cryptographic algorithms, key-bit randomization, simple circuit design techniques, and system-level synthesis operations, all in a correct-by-construction manner. Our extensive analysis demonstrates the remarkable efficacy of our scheme, outperforming prior art against a range of oracle-less attacks covering crucial threat vectors, all with lower design overheads. We provide a full open-source release.</li>
</ul>

<h3>Title: WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yukang Lin, Jiahao Shao, Shuoran Jiang, Wentao Zhu, Bingjie Lu, Xiangping Wu, Joanna Siebert, Qingcai Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00837">https://arxiv.org/abs/2512.00837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00837">https://arxiv.org/pdf/2512.00837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00837]] WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models(https://arxiv.org/abs/2512.00837)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Watermarking acts as a critical safeguard in text generated by Large Language Models (LLMs). By embedding identifiable signals into model outputs, watermarking enables reliable attribution and enhances the security of machine-generated content. Existing approaches typically embed signals by manipulating token generation probabilities. Despite their effectiveness, these methods inherently face a trade-off between detectability and text quality: the signal strength and randomness required for robust watermarking tend to degrade the performance of downstream tasks. In this paper, we design a novel embedding scheme that controls seed pools to facilitate diverse parallel generation of watermarked text. Based on that scheme, we propose WaterSearch, a sentence-level, search-based watermarking framework adaptable to a wide range of existing methods. WaterSearch enhances text quality by jointly optimizing two key aspects: 1) distribution fidelity and 2) watermark signal characteristics. Furthermore, WaterSearch is complemented by a sentence-level detection method with strong attack robustness. We evaluate our method on three popular LLMs across ten diverse tasks. Extensive experiments demonstrate that our method achieves an average performance improvement of 51.01\% over state-of-the-art baselines at a watermark detectability strength of 95\%. In challenging scenarios such as short text generation and low-entropy output generation, our method yields performance gains of 47.78\% and 36.47\%, respectively. Moreover, under different attack senarios including insertion, synonym substitution and paraphrase attasks, WaterSearch maintains high detectability, further validating its robust anti-attack capabilities. Our code is available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Prediction-space knowledge markets for communication-efficient federated learning on multimedia tasks</h3>
<ul>
<li><strong>Authors: </strong>Wenzhang Du</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00841">https://arxiv.org/abs/2512.00841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00841">https://arxiv.org/pdf/2512.00841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00841]] Prediction-space knowledge markets for communication-efficient federated learning on multimedia tasks(https://arxiv.org/abs/2512.00841)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) enables collaborative training over distributed multimedia data but suffers acutely from statistical heterogeneity and communication constraints, especially when clients deploy large models. Classic parameter-averaging methods such as FedAvg transmit full model weights and can diverge under nonindependent and identically distributed (non-IID) data. We propose KTA v2, a prediction-space knowledge trading market for FL. Each round, clients locally train on their private data, then share only logits on a small public reference set. The server constructs a client-client similarity graph in prediction space, combines it with reference-set accuracy to form per-client teacher ensembles, and sends back personalized soft targets for a second-stage distillation update. This two-stage procedure can be interpreted as approximate block-coordinate descent on a unified objective with prediction-space regularization. Experiments on FEMNIST, CIFAR-10 and AG News show that, under comparable or much lower communication budgets, KTA v2 consistently outperforms a local-only baseline and strong parameter-based methods (FedAvg, FedProx), and substantially improves over a FedMD-style global teacher. On CIFAR-10 with ResNet-18, KTA v2 reaches 57.7% test accuracy using approximately 1/1100 of FedAvg's communication, while on AG News it attains 89.3% accuracy with approximately 1/300 of FedAvg's traffic.</li>
</ul>

<h3>Title: AFRAgent : An Adaptive Feature Renormalization Based High Resolution Aware GUI agent</h3>
<ul>
<li><strong>Authors: </strong>Neeraj Anand, Rishabh Jain, Sohan Patnaik, Balaji Krishnamurthy, Mausoom Sarkar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00846">https://arxiv.org/abs/2512.00846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00846">https://arxiv.org/pdf/2512.00846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00846]] AFRAgent : An Adaptive Feature Renormalization Based High Resolution Aware GUI agent(https://arxiv.org/abs/2512.00846)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>There is a growing demand for mobile user interface (UI) automation, driven by its broad applications across industries. With the advent of visual language models (VLMs), GUI automation has progressed from generating text-based instructions for humans to autonomously executing tasks, thus optimizing automation workflows. Recent approaches leverage VLMs for this problem due to their ability to 1) process on-screen content directly, 2) remain independent of device-specific APIs by utilizing human actions (e.g., clicks, typing), and 3) apply real-world contextual knowledge for task understanding. However, these models often have trouble accurately identifying widgets and determining actions due to limited spatial information in vision encoder features. Additionally, top-performing models are often large, requiring extensive training and resulting in inference delays. In this work, we introduce AFRAgent, an instruct-BLIP-based multimodal architecture that achieves superior performance in GUI automation while being less than one-fourth the size of its nearest competitor. To enhance image embeddings in the large language model (LLM) pipeline, we propose an adaptive feature renormalization-based (a token-level affine transformation) technique that effectively enriches low-resolution image embeddings and fuses high-resolution details. We evaluate AFRAgent on Meta-GUI and AITW benchmarks, establishing a new state-of-the-art baseline for smartphone automation.</li>
</ul>

<h3>Title: Topological Federated Clustering via Gravitational Potential Fields under Local Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Yunbo Long, Jiaquan Zhang, Xi Chen, Alexandra Brintrup</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00849">https://arxiv.org/abs/2512.00849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00849">https://arxiv.org/pdf/2512.00849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00849]] Topological Federated Clustering via Gravitational Potential Fields under Local Differential Privacy(https://arxiv.org/abs/2512.00849)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Clustering non-independent and identically distributed (non-IID) data under local differential privacy (LDP) in federated settings presents a critical challenge: preserving privacy while maintaining accuracy without iterative communication. Existing one-shot methods rely on unstable pairwise centroid distances or neighborhood rankings, degrading severely under strong LDP noise and data heterogeneity. We present Gravitational Federated Clustering (GFC), a novel approach to privacy-preserving federated clustering that overcomes the limitations of distance-based methods under varying LDP. Addressing the critical challenge of clustering non-IID data with diverse privacy guarantees, GFC transforms privatized client centroids into a global gravitational potential field where true cluster centers emerge as topologically persistent singularities. Our framework introduces two key innovations: (1) a client-side compactness-aware perturbation mechanism that encodes local cluster geometry as "mass" values, and (2) a server-side topological aggregation phase that extracts stable centroids through persistent homology analysis of the potential field's superlevel sets. Theoretically, we establish a closed-form bound between the privacy budget $\epsilon$ and centroid estimation error, proving the potential field's Lipschitz smoothing properties exponentially suppress noise in high-density regions. Empirically, GFC outperforms state-of-the-art methods on ten benchmarks, especially under strong LDP constraints ($\epsilon < 1$), while maintaining comparable performance at lower privacy budgets. By reformulating federated clustering as a topological persistence problem in a synthetic physics-inspired space, GFC achieves unprecedented privacy-accuracy trade-offs without iterative communication, providing a new perspective for privacy-preserving distributed learning.</li>
</ul>

<h3>Title: City-Conditioned Memory for Multi-City Traffic and Mobility Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Wenzhang Du</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00851">https://arxiv.org/abs/2512.00851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00851">https://arxiv.org/pdf/2512.00851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00851]] City-Conditioned Memory for Multi-City Traffic and Mobility Forecasting(https://arxiv.org/abs/2512.00851)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Deploying spatio-temporal forecasting models across many cities is difficult: traffic networks differ in size and topology, data availability can vary by orders of magnitude, and new cities may provide only a short history of logs. Existing deep traffic models are typically trained per city and backbone, creating high maintenance cost and poor transfer to data-scarce cities. We ask whether a single, backbone-agnostic layer can condition on "which city this sequence comes from", improve accuracy in full- and low-data regimes, and support better cross-city adaptation with minimal code changes. We propose CityCond, a light-weight city-conditioned memory layer that augments existing spatio-temporal backbones. CityCond combines a city-ID encoder with an optional shared memory bank (CityMem). Given a city index and backbone hidden states, it produces city-conditioned features fused through gated residual connections. We attach CityCond to five representative backbones (GRU, TCN, Transformer, GNN, STGCN) and evaluate three regimes: full-data, low-data, and cross-city few-shot transfer on METR-LA and PEMS-BAY. We also run auxiliary experiments on SIND, a drone-based multi-agent trajectory dataset from a signalized intersection in Tianjin (we focus on pedestrian tracks). Across more than fourteen model variants and three random seeds, CityCond yields consistent improvements, with the largest gains for high-capacity backbones such as Transformers and STGCNs. CityMem reduces Transformer error by roughly one third in full-data settings and brings substantial gains in low-data and cross-city transfer. On SIND, simple city-ID conditioning modestly improves low-data LSTM performance. CityCond can therefore serve as a reusable design pattern for scalable, multi-city forecasting under realistic data constraints.</li>
</ul>

<h3>Title: Robust Probabilistic Load Forecasting for a Single Household: A Comparative Study from SARIMA to Transformers on the REFIT Dataset</h3>
<ul>
<li><strong>Authors: </strong>Midhun Manoj</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00856">https://arxiv.org/abs/2512.00856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00856">https://arxiv.org/pdf/2512.00856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00856]] Robust Probabilistic Load Forecasting for a Single Household: A Comparative Study from SARIMA to Transformers on the REFIT Dataset(https://arxiv.org/abs/2512.00856)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Probabilistic forecasting is essential for modern risk management, allowing decision-makers to quantify uncertainty in critical systems. This paper tackles this challenge using the volatile REFIT household dataset, which is complicated by a large structural data gap. We first address this by conducting a rigorous comparative experiment to select a Seasonal Imputation method, demonstrating its superiority over linear interpolation in preserving the data's underlying distribution. We then systematically evaluate a hierarchy of models, progressing from classical baselines (SARIMA, Prophet) to machine learning (XGBoost) and advanced deep learning architectures (LSTM). Our findings reveal that classical models fail to capture the data's non-linear, regime-switching behavior. While the LSTM provided the most well-calibrated probabilistic forecast, the Temporal Fusion Transformer (TFT) emerged as the superior all-round model, achieving the best point forecast accuracy (RMSE 481.94) and producing safer, more cautious prediction intervals that effectively capture extreme volatility.</li>
</ul>

<h3>Title: HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ningning Chen, Weicai Ye, Ying Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00862">https://arxiv.org/abs/2512.00862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00862">https://arxiv.org/pdf/2512.00862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00862]] HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs(https://arxiv.org/abs/2512.00862)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce HBLLM, a wavelet-enhanced high-fidelity $1$-bit post-training quantization method for Large Language Models (LLMs). By leveraging Haar wavelet transforms to enhance expressive capacity through frequency decomposition, HBLLM significantly improves quantization fidelity while maintaining minimal overhead. This approach features two innovative structure-aware grouping strategies: (1) frequency-aware multi-parameter intra-row grouping and (2) $\ell_2$-norm-based saliency-driven column selection. For non-salient weights, a shared mean is employed across quantization groups within each frequency band to optimize storage efficiency. Experiments conducted on the OPT and LLaMA models demonstrate that HBLLM achieves state-of-the-art performance in $1$-bit quantization, attaining a perplexity of $6.71$ on LLaMA$2$-$13$B with an average weight storage of only $1.08$ bits. Code available at: this https URL.</li>
</ul>

<h3>Title: TAP-CT: 3D Task-Agnostic Pretraining of Computed Tomography Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Tim Veenboer, George Yiasemis, Eric Marcus, Vivien Van Veldhuizen, Cees G. M. Snoek, Jonas Teuwen, Kevin B. W. Groot Lipman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00872">https://arxiv.org/abs/2512.00872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00872">https://arxiv.org/pdf/2512.00872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00872]] TAP-CT: 3D Task-Agnostic Pretraining of Computed Tomography Foundation Models(https://arxiv.org/abs/2512.00872)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Existing foundation models (FMs) in the medical domain often require extensive fine-tuning or rely on training resource-intensive decoders, while many existing encoders are pretrained with objectives biased toward specific tasks. This illustrates a need for a strong, task-agnostic foundation model that requires minimal fine-tuning beyond feature extraction. In this work, we introduce a suite of task-agnostic pretraining of CT foundation models (TAP-CT): a simple yet effective adaptation of Vision Transformers (ViTs) and DINOv2 for volumetric data, enabling scalable self-supervised pretraining directly on 3D CT volumes. Our approach incorporates targeted modifications to patch embeddings, positional encodings, and volumetric augmentations, making the architecture depth-aware while preserving the simplicity of the underlying architectures. We show that large-scale 3D pretraining on an extensive in-house CT dataset (105K volumes) yields stable, robust frozen representations that generalize strongly across downstream tasks. To promote transparency and reproducibility, and to establish a powerful, low-resource baseline for future research in medical imaging, we will release all pretrained models, experimental configurations, and downstream benchmark code at this https URL.</li>
</ul>

<h3>Title: Less is More: Resource-Efficient Low-Rank Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Chunlin Tian, Xuyang Wei, Huanrong Liu, Zhijiang Guo, Li Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00878">https://arxiv.org/abs/2512.00878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00878">https://arxiv.org/pdf/2512.00878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00878]] Less is More: Resource-Efficient Low-Rank Adaptation(https://arxiv.org/abs/2512.00878)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), but it still incurs notable overhead and suffers from parameter interference in complex datasets. While re- cent works decouple LoRA update matrices to exploit matrix-wise asymmetry, training costs remain high. We revisit LoRA from the perspective of inter-matrix and intra-layer parameter redundancy and propose Resource-Efficient Low-Rank Adaptation, EffiLoRA, a lightweight and generalizable approach for language, multimodal, and diffusion models. EffiLoRA employs a unified A matrix across all transformer layers and introduces a runtime selective B matrices up- date to dynamically trade-off the system resource budget and model performance. EffiLoRA consistently outperforms LoRA across diverse modalities, including commonsense reasoning, visual instruction tuning, and image generation, demon- strating improved efficiency and robustness.</li>
</ul>

<h3>Title: Quantum-Inspired Spectral Geometry for Neural Operator Equivalence and Structured Pruning</h3>
<ul>
<li><strong>Authors: </strong>Haijian Shao, Wei Liu, Xing Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00880">https://arxiv.org/abs/2512.00880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00880">https://arxiv.org/pdf/2512.00880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00880]] Quantum-Inspired Spectral Geometry for Neural Operator Equivalence and Structured Pruning(https://arxiv.org/abs/2512.00880)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The rapid growth of multimodal intelligence on resource-constrained and heterogeneous domestic hardware exposes critical bottlenecks: multimodal feature heterogeneity, real-time requirements in dynamic scenarios, and hardware-specific operator redundancy. This work introduces a quantum-inspired geometric framework for neural operators that represents each operator by its normalized singular value spectrum on the Bloch hypersphere. We prove a tight spectral-to-functional equivalence theorem showing that vanishing Fubini--Study/Wasserstein-2 distance implies provable functional closeness, establishing the first rigorous foundation for cross-modal and cross-architecture operator substitutability. Based on this metric, we propose Quantum Metric-Driven Functional Redundancy Graphs (QM-FRG) and one-shot structured pruning. Controlled simulation validates the superiority of the proposed metric over magnitude and random baselines. An extensive experimental validation on large-scale multimodal transformers and domestic heterogeneous hardware (Huawei Ascend, Cambricon MLU, Kunlunxin) hardware is deferred to an extended journal version currently in preparation.</li>
</ul>

<h3>Title: HanDyVQA: A Video QA Benchmark for Fine-Grained Hand-Object Interaction Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Masatoshi Tateno, Gido Kato, Hirokatsu Kataoka, Yoichi Sato, Takuma Yagi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00885">https://arxiv.org/abs/2512.00885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00885">https://arxiv.org/pdf/2512.00885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00885]] HanDyVQA: A Video QA Benchmark for Fine-Grained Hand-Object Interaction Dynamics(https://arxiv.org/abs/2512.00885)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Hand-object interaction (HOI) inherently involves dynamics where human manipulations produce distinct spatio-temporal effects on objects. However, existing semantic HOI benchmarks focused either on manipulation or on the resulting effects at a coarse level, lacking fine-grained spatio-temporal reasoning to capture the underlying dynamics in HOI. We introduce HanDyVQA, a fine-grained video question-answering benchmark that comprehensively covers both the manipulation and effect aspects of HOI. HanDyVQA comprises six complementary question types (Action, Process, Objects, Location, State Change, and Object Parts), totalling 11.1K multiple-choice QA pairs. Collected QA pairs recognizing manipulation styles, hand/object motions, and part-level state changes. HanDyVQA also includes 10.3K segmentation masks for Objects and Object Parts questions, enabling the evaluation of object/part-level reasoning in video object segmentation. We evaluated recent video foundation models on our benchmark and found that even the best-performing model, Gemini-2.5-Pro, reached only 73% average accuracy, which is far from human performance (97%). Further analysis shows the remaining challenges in spatial relationship, motion, and part-level geometric understanding. We also found that integrating explicit HOI-related cues into visual features improves performance, offering insights for developing future models with a deeper understanding of HOI dynamics.</li>
</ul>

<h3>Title: Multilingual Training-Free Remote Sensing Image Captioning</h3>
<ul>
<li><strong>Authors: </strong>Carlos Rebelo, Gil Rocha, João Daniel Silva, Bruno Martins</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00887">https://arxiv.org/abs/2512.00887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00887">https://arxiv.org/pdf/2512.00887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00887]] Multilingual Training-Free Remote Sensing Image Captioning(https://arxiv.org/abs/2512.00887)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Remote sensing image captioning has advanced rapidly through encoder--decoder models, although the reliance on large annotated datasets and the focus on English restricts global applicability. To address these limitations, we propose the first training-free multilingual approach, based on retrieval-augmented prompting. For a given aerial image, we employ a domain-adapted SigLIP2 encoder to retrieve related captions and few-shot examples from a datastore, which are then provided to a language model. We explore two variants: an image-blind setup, where a multilingual Large Language Model (LLM) generates the caption from textual prompts alone, and an image-aware setup, where a Vision--Language Model (VLM) jointly processes the prompt and the input image. To improve the coherence of the retrieved content, we introduce a graph-based re-ranking strategy using PageRank on a graph of images and captions. Experiments on four benchmark datasets across ten languages demonstrate that our approach is competitive with fully supervised English-only systems and generalizes to other languages. Results also highlight the importance of re-ranking with PageRank, yielding up to 35% improvements in performance metrics. Additionally, it was observed that while VLMs tend to generate visually grounded but lexically diverse captions, LLMs can achieve stronger BLEU and CIDEr scores. Lastly, directly generating captions in the target language consistently outperforms other translation-based strategies. Overall, our work delivers one of the first systematic evaluations of multilingual, training-free captioning for remote sensing imagery, advancing toward more inclusive and scalable multimodal Earth observation systems.</li>
</ul>

<h3>Title: Accelerating Streaming Video Large Language Models via Hierarchical Token Compression</h3>
<ul>
<li><strong>Authors: </strong>Yiyu Wang, Xuyang Liu, Xiyan Gui, Xinying Lin, Boxue Yang, Chenfei Liao, Tailai Chen, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00891">https://arxiv.org/abs/2512.00891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00891">https://arxiv.org/pdf/2512.00891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00891]] Accelerating Streaming Video Large Language Models via Hierarchical Token Compression(https://arxiv.org/abs/2512.00891)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \textbf{S}treaming \textbf{T}oken \textbf{C}ompression (\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \textbf{99\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \textbf{24.5\%} and \textbf{45.3\%}.</li>
</ul>

<h3>Title: SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead</h3>
<ul>
<li><strong>Authors: </strong>Chaojun Ni, Cheng Chen, Xiaofeng Wang, Zheng Zhu, Wenzhao Zheng, Boyuan Wang, Tianrun Chen, Guosheng Zhao, Haoyun Li, Zhehao Dong, Qiang Zhang, Yun Ye, Yang Wang, Guan Huang, Wenjun Mei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00903">https://arxiv.org/abs/2512.00903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00903">https://arxiv.org/pdf/2512.00903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00903]] SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead(https://arxiv.org/abs/2512.00903)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.</li>
</ul>

<h3>Title: Beyond High-Entropy Exploration: Correctness-Aware Low-Entropy Segment-Based Advantage Shaping for Reasoning LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xinzhu Chen, Xuesheng Li, Zhongxiang Sun, Weijie Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00908">https://arxiv.org/abs/2512.00908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00908">https://arxiv.org/pdf/2512.00908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00908]] Beyond High-Entropy Exploration: Correctness-Aware Low-Entropy Segment-Based Advantage Shaping for Reasoning LLMs(https://arxiv.org/abs/2512.00908)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) has become a central approach for improving the reasoning ability of large language models. Recent work studies RLVR through token entropy, arguing that high-entropy tokens drive exploration and should receive stronger updates. However, they overlook the fact that most of a reasoning trajectory consists of low-entropy segments that encode stable and reusable structural patterns. Through qualitative and quantitative analyses, we find that the overlap of low-entropy segments across correct responses strongly correlates with model accuracy, while overlaps involving incorrect responses exhibit stable but unproductive patterns. Motivated by these findings, we propose LESS, a correctness-aware reinforcement framework that performs fine-grained advantage modulation over low-entropy segments. LESS amplifies segments unique to correct responses, suppresses those unique to incorrect ones, and neutralizes segments shared by both, while preserving high-entropy exploration in the underlying RL algorithm. Instantiated on top of the popular GRPO, LESS consistently improves accuracy over strong RL baselines across three backbones and six math benchmarks, achieves stronger robustness of the performance floor.</li>
</ul>

<h3>Title: TalkingPose: Efficient Face and Gesture Animation with Feedback-guided Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Alireza Javanmardi, Pragati Jaiswal, Tewodros Amberbir Habtegebrial, Christen Millerdurai, Shaoxiang Wang, Alain Pagani, Didier Stricker</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00909">https://arxiv.org/abs/2512.00909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00909">https://arxiv.org/pdf/2512.00909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00909]] TalkingPose: Efficient Face and Gesture Animation with Feedback-guided Diffusion Model(https://arxiv.org/abs/2512.00909)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in diffusion models have significantly improved the realism and generalizability of character-driven animation, enabling the synthesis of high-quality motion from just a single RGB image and a set of driving poses. Nevertheless, generating temporally coherent long-form content remains challenging. Existing approaches are constrained by computational and memory limitations, as they are typically trained on short video segments, thus performing effectively only over limited frame lengths and hindering their potential for extended coherent generation. To address these constraints, we propose TalkingPose, a novel diffusion-based framework specifically designed for producing long-form, temporally consistent human upper-body animations. TalkingPose leverages driving frames to precisely capture expressive facial and hand movements, transferring these seamlessly to a target actor through a stable diffusion backbone. To ensure continuous motion and enhance temporal coherence, we introduce a feedback-driven mechanism built upon image-based diffusion models. Notably, this mechanism does not incur additional computational costs or require secondary training stages, enabling the generation of animations with unlimited duration. Additionally, we introduce a comprehensive, large-scale dataset to serve as a new benchmark for human upper-body animation.</li>
</ul>

<h3>Title: ForamDeepSlice: A High-Accuracy Deep Learning Framework for Foraminifera Species Classification from 2D Micro-CT Slices</h3>
<ul>
<li><strong>Authors: </strong>Abdelghafour Halimi, Ali Alibrahim, Didier Barradas-Bautista, Ronell Sicat, Abdulkader M. Afifi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00912">https://arxiv.org/abs/2512.00912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00912">https://arxiv.org/pdf/2512.00912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00912]] ForamDeepSlice: A High-Accuracy Deep Learning Framework for Foraminifera Species Classification from 2D Micro-CT Slices(https://arxiv.org/abs/2512.00912)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This study presents a comprehensive deep learning pipeline for the automated classification of 12 foraminifera species using 2D micro-CT slices derived from 3D scans. We curated a scientifically rigorous dataset comprising 97 micro-CT scanned specimens across 27 species, selecting 12 species with sufficient representation for robust machine learning. To ensure methodological integrity and prevent data leakage, we employed specimen-level data splitting, resulting in 109,617 high-quality 2D slices (44,103 for training, 14,046 for validation, and 51,468 for testing). We evaluated seven state-of-the-art 2D convolutional neural network (CNN) architectures using transfer learning. Our final ensemble model, combining ConvNeXt-Large and EfficientNetV2-Small, achieved a test accuracy of 95.64%, with a top-3 accuracy of 99.6% and an area under the ROC curve (AUC) of 0.998 across all species. To facilitate practical deployment, we developed an interactive advanced dashboard that supports real-time slice classification and 3D slice matching using advanced similarity metrics, including SSIM, NCC, and the Dice coefficient. This work establishes new benchmarks for AI-assisted micropaleontological identification and provides a fully reproducible framework for foraminifera classification research, bridging the gap between deep learning and applied geosciences.</li>
</ul>

<h3>Title: Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments</h3>
<ul>
<li><strong>Authors: </strong>Junwoo Chang, Minwoo Park, Joohwan Seo, Roberto Horowitz, Jongmin Lee, Jongeun Choi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00915">https://arxiv.org/abs/2512.00915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00915">https://arxiv.org/pdf/2512.00915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00915]] Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments(https://arxiv.org/abs/2512.00915)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Group symmetries provide a powerful inductive bias for reinforcement learning (RL), enabling efficient generalization across symmetric states and actions via group-invariant Markov Decision Processes (MDPs). However, real-world environments almost never realize fully group-invariant MDPs; dynamics, actuation limits, and reward design usually break symmetries, often only locally. Under group-invariant Bellman backups for such cases, local symmetry-breaking introduces errors that propagate across the entire state-action space, resulting in global value estimation errors. To address this, we introduce Partially group-Invariant MDP (PI-MDP), which selectively applies group-invariant or standard Bellman backups depending on where symmetry holds. This framework mitigates error propagation from locally broken symmetries while maintaining the benefits of equivariance, thereby enhancing sample efficiency and generalizability. Building on this framework, we present practical RL algorithms -- Partially Equivariant (PE)-DQN for discrete control and PE-SAC for continuous control -- that combine the benefits of equivariance with robustness to symmetry-breaking. Experiments across Grid-World, locomotion, and manipulation benchmarks demonstrate that PE-DQN and PE-SAC significantly outperform baselines, highlighting the importance of selective symmetry exploitation for robust and sample-efficient RL.</li>
</ul>

<h3>Title: Reward Auditor: Inference on Reward Modeling Suitability in Real-World Perturbed Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Jianxiang Zang, Yongda Wei, Ruxue Bai, Shiyu Jiang, Nijia Mo, Binhong Li, Qiang Sun, Hui Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00920">https://arxiv.org/abs/2512.00920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00920">https://arxiv.org/pdf/2512.00920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00920]] Reward Auditor: Inference on Reward Modeling Suitability in Real-World Perturbed Scenarios(https://arxiv.org/abs/2512.00920)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Reliable reward models (RMs) are critical for ensuring the safe alignment of large language models (LLMs). However, current evaluation methods focus solely on preference perception accuracies in given specific scenarios, obscuring the critical vulnerabilities of RMs in real-world scenarios. We identify the true challenge lies in assessing a novel dimension: Suitability, defined as conditional reliability under specific real-world perturbations. To this end, we introduce Reward Auditor, a hypothesis-testing framework specifically designed for RM suitability inference. Rather than answering "How accurate is the RM's preference perception for given samples?", it employs scientific auditing to answer: "Can we infer RMs exhibit systematic vulnerabilities in specific real-world scenarios?". Under real-world perturbed scenarios, Reward Auditor quantifies statistical significance and effect size by auditing distribution degradation of RM preference perception confidence. This enables inference of both the certainty and severity of RM vulnerabilities across diverse real-world scenarios. This lays a solid foundation for building next-generation LLM alignment systems that are verifiably safe, more robust, and trustworthy.</li>
</ul>

<h3>Title: D-CTNet: A Dual-Branch Channel-Temporal Forecasting Network with Frequency-Domain Correction</h3>
<ul>
<li><strong>Authors: </strong>Shaoxun Wang, Xingjun Zhang, Kun Xia, Qianyang Li, Jiawei Cao, Zhendong Tan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00925">https://arxiv.org/abs/2512.00925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00925">https://arxiv.org/pdf/2512.00925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00925]] D-CTNet: A Dual-Branch Channel-Temporal Forecasting Network with Frequency-Domain Correction(https://arxiv.org/abs/2512.00925)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate Multivariate Time Series (MTS) forecasting is crucial for collaborative design of complex systems, Digital Twin building, and maintenance ahead of time. However, the collaborative industrial environment presents new challenges for MTS forecasting models: models should decouple complex inter-variable dependencies while addressing non-stationary distribution shift brought by environmental changes. To address these challenges and improve collaborative sensing reliability, we propose a Patch-Based Dual-Branch Channel-Temporal Forecasting Network (D-CTNet). Particularly, with a parallel dual-branch design incorporating linear temporal modeling layer and channel attention mechanism, our method explicitly decouples and jointly learns intra-channel temporal evolution patterns and dynamic multivariate correlations. Furthermore, a global patch attention fusion module goes beyond the local window scope to model long range dependencies. Most importantly, aiming at non-stationarity, a Frequency-Domain Stationarity Correction mechanism adaptively suppresses distribution shift impacts from environment change by spectrum alignment. Evaluations on seven benchmark datasets show that our model achieves better forecasting accuracy and robustness compared with state-of-the-art methods. Our work shows great promise as a new forecasting engine for industrial collaborative systems.</li>
</ul>

<h3>Title: LAHNet: Local Attentive Hashing Network for Point Cloud Registration</h3>
<ul>
<li><strong>Authors: </strong>Wentao Qu, Xiaoshui Huang, Liang Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00927">https://arxiv.org/abs/2512.00927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00927">https://arxiv.org/pdf/2512.00927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00927]] LAHNet: Local Attentive Hashing Network for Point Cloud Registration(https://arxiv.org/abs/2512.00927)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Most existing learning-based point cloud descriptors for point cloud registration focus on perceiving local information of point clouds to generate distinctive features. However, a reasonable and broader receptive field is essential for enhancing feature distinctiveness. In this paper, we propose a Local Attentive Hashing Network for point cloud registration, called LAHNet, which introduces a local attention mechanism with the inductive bias of locality of convolution-like operators into point cloud descriptors. Specifically, a Group Transformer is designed to capture reasonable long-range context between points. This employs a linear neighborhood search strategy, Locality-Sensitive Hashing, enabling uniformly partitioning point clouds into non-overlapping windows. Meanwhile, an efficient cross-window strategy is adopted to further expand the reasonable feature receptive field. Furthermore, building on this effective windowing strategy, we propose an Interaction Transformer to enhance the feature interactions of the overlap regions within point cloud pairs. This computes an overlap matrix to match overlap regions between point cloud pairs by representing each window as a global signal. Extensive results demonstrate that LAHNet can learn robust and distinctive features, achieving significant registration results on real-world indoor and outdoor benchmarks.</li>
</ul>

<h3>Title: Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study</h3>
<ul>
<li><strong>Authors: </strong>Imane Jaaouine, Ross D. King</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00931">https://arxiv.org/abs/2512.00931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00931">https://arxiv.org/pdf/2512.00931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00931]] Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study(https://arxiv.org/abs/2512.00931)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) produce context inconsistency hallucinations, which are LLM generated outputs that are misaligned with the user prompt. This research project investigates whether prompt engineering (PE) methods can mitigate context inconsistency hallucinations in zero-shot LLM summarisation of scientific texts, where zero-shot indicates that the LLM relies purely on its pre-training data. Across eight yeast biotechnology research paper abstracts, six instruction-tuned LLMs were prompted with seven methods: a base- line prompt, two levels of increasing instruction complexity (PE-1 and PE-2), two levels of context repetition (CR-K1 and CR-K2), and two levels of random addition (RA-K1 and RA-K2). Context repetition involved the identification and repetition of K key sentences from the abstract, whereas random addition involved the repetition of K randomly selected sentences from the abstract, where K is 1 or 2. A total of 336 LLM-generated summaries were evaluated using six metrics: ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, METEOR, and cosine similarity, which were used to compute the lexical and semantic alignment be- tween the summaries and the abstracts. Four hypotheses on the effects of prompt methods on summary alignment with the reference text were tested. Statistical analysis on 3744 collected datapoints was performed using bias-corrected and accelerated (BCa) bootstrap confidence intervals and Wilcoxon signed-rank tests with Bonferroni-Holm correction. The results demonstrated that CR and RA significantly improve the lexical alignment of LLM-generated summaries with the abstracts. These findings indicate that prompt engineering has the potential to impact hallucinations in zero-shot scientific summarisation tasks.</li>
</ul>

<h3>Title: DeformAr: Rethinking NER Evaluation through Component Analysis and Visual Analytics</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Mustafa Younes</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00938">https://arxiv.org/abs/2512.00938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00938">https://arxiv.org/pdf/2512.00938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00938]] DeformAr: Rethinking NER Evaluation through Component Analysis and Visual Analytics(https://arxiv.org/abs/2512.00938)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Transformer models have significantly advanced Natural Language Processing (NLP), demonstrating strong performance in English. However, their effectiveness in Arabic, particularly for Named Entity Recognition (NER), remains limited, even with larger pre-trained models. This performance gap stems from multiple factors, including tokenisation, dataset quality, and annotation inconsistencies. Existing studies often analyze these issues in isolation, failing to capture their joint effect on system behaviour and performance. We introduce DeformAr (Debugging and Evaluation Framework for Transformer-based NER Systems), a novel framework designed to investigate and explain the performance discrepancy between Arabic and English NER systems. DeformAr integrates a data extraction library and an interactive dashboard, supporting two modes of evaluation: cross-component analysis and behavioural analysis. The framework divides each language into dataset and model components to examine their interactions. The analysis proceeds in two stages. First, cross-component analysis provides systematic diagnostic measures across data and model subcomponents, addressing the "what," "how," and "why" behind observed discrepancies. The second stage applies behavioural analysis by combining interpretability techniques with token-level metrics, interactive visualisations, and representation space analysis. These stages enable a component-aware diagnostic process that detects model behaviours and explains them by linking them to underlying representational patterns and data factors. DeformAr is the first Arabic-specific, component-based interpretability tool, offering a crucial resource for advancing model analysis in under-resourced languages.</li>
</ul>

<h3>Title: Binary-Gaussian: Compact and Progressive Representation for 3D Gaussian Segmentation</h3>
<ul>
<li><strong>Authors: </strong>An Yang, Chenyu Liu, Jun Du, Jianqing Gao, Jia Pan, Jinshui Hu, Baocai Yin, Bing Yin, Cong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00944">https://arxiv.org/abs/2512.00944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00944">https://arxiv.org/pdf/2512.00944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00944]] Binary-Gaussian: Compact and Progressive Representation for 3D Gaussian Segmentation(https://arxiv.org/abs/2512.00944)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting (3D-GS) has emerged as an efficient 3D representation and a promising foundation for semantic tasks like segmentation. However, existing 3D-GS-based segmentation methods typically rely on high-dimensional category features, which introduce substantial memory overhead. Moreover, fine-grained segmentation remains challenging due to label space congestion and the lack of stable multi-granularity control mechanisms. To address these limitations, we propose a coarse-to-fine binary encoding scheme for per-Gaussian category representation, which compresses each feature into a single integer via the binary-to-decimal mapping, drastically reducing memory usage. We further design a progressive training strategy that decomposes panoptic segmentation into a series of independent sub-tasks, reducing inter-class conflicts and thereby enhancing fine-grained segmentation capability. Additionally, we fine-tune opacity during segmentation training to address the incompatibility between photometric rendering and semantic segmentation, which often leads to foreground-background confusion. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art segmentation performance while significantly reducing memory consumption and accelerating inference.</li>
</ul>

<h3>Title: Fine-tuning of lightweight large language models for sentiment classification on heterogeneous financial textual data</h3>
<ul>
<li><strong>Authors: </strong>Alvaro Paredes Amorin, Andre Python, Christoph Weisser</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00946">https://arxiv.org/abs/2512.00946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00946">https://arxiv.org/pdf/2512.00946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00946]] Fine-tuning of lightweight large language models for sentiment classification on heterogeneous financial textual data(https://arxiv.org/abs/2512.00946)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) play an increasingly important role in finan- cial markets analysis by capturing signals from complex and heterogeneous textual data sources, such as tweets, news articles, reports, and microblogs. However, their performance is dependent on large computational resources and proprietary datasets, which are costly, restricted, and therefore inacces- sible to many researchers and practitioners. To reflect realistic situations we investigate the ability of lightweight open-source LLMs - smaller and publicly available models designed to operate with limited computational resources - to generalize sentiment understanding from financial datasets of varying sizes, sources, formats, and languages. We compare the benchmark finance natural language processing (NLP) model, FinBERT, and three open-source lightweight LLMs, DeepSeek-LLM 7B, Llama3 8B Instruct, and Qwen3 8B on five publicly available datasets: FinancialPhraseBank, Financial Question Answering, Gold News Sentiment, Twitter Sentiment and Chinese Finance Sentiment. We find that LLMs, specially Qwen3 8B and Llama3 8B, perform best in most scenarios, even from using only 5% of the available training data. These results hold in zero-shot and few-shot learning scenarios. Our findings indicate that lightweight, open-source large language models (LLMs) consti- tute a cost-effective option, as they can achieve competitive performance on heterogeneous textual data even when trained on only a limited subset of the extensive annotated corpora that are typically deemed necessary.</li>
</ul>

<h3>Title: Table as a Modality for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Liyao Li, Chao Ye, Wentao Ye, Yifei Sun, Zhe Jiang, Haobo Wang, Jiaming Tian, Yiming Zhang, Ningtao Wang, Xing Fu, Gang Chen, Junbo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00947">https://arxiv.org/abs/2512.00947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00947">https://arxiv.org/pdf/2512.00947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00947]] Table as a Modality for Large Language Models(https://arxiv.org/abs/2512.00947)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To migrate the remarkable successes of Large Language Models (LLMs), the community has made numerous efforts to generalize them to the table reasoning tasks for the widely deployed tabular data. Despite that, in this work, by showing a probing experiment on our proposed StructQA benchmark, we postulate that even the most advanced LLMs (such as GPTs) may still fall short of coping with tabular data. More specifically, the current scheme often simply relies on serializing the tabular data, together with the meta information, then inputting them through the LLMs. We argue that the loss of structural information is the root of this shortcoming. In this work, we further propose TAMO, which bears an ideology to treat the tables as an independent modality integrated with the text tokens. The resulting model in TAMO is a multimodal framework consisting of a hypergraph neural network as the global table encoder seamlessly integrated with the mainstream LLM. Empirical results on various benchmarking datasets, including HiTab, WikiTQ, WikiSQL, FeTaQA, and StructQA, have demonstrated significant improvements on generalization with an average relative gain of 42.65%.</li>
</ul>

<h3>Title: Adaptive Evidential Learning for Temporal-Semantic Robustness in Moment Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Haojian Huang, Kaijing Ma, Jin Chen, Haodong Chen, Zhou Wu, Xianghao Zang, Han Fang, Chao Ban, Hao Sun, Mulin Chen, Zhongjiang He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00953">https://arxiv.org/abs/2512.00953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00953">https://arxiv.org/pdf/2512.00953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00953]] Adaptive Evidential Learning for Temporal-Semantic Robustness in Moment Retrieval(https://arxiv.org/abs/2512.00953)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>In the domain of moment retrieval, accurately identifying temporal segments within videos based on natural language queries remains challenging. Traditional methods often employ pre-trained models that struggle with fine-grained information and deterministic reasoning, leading to difficulties in aligning with complex or ambiguous moments. To overcome these limitations, we explore Deep Evidential Regression (DER) to construct a vanilla Evidential baseline. However, this approach encounters two major issues: the inability to effectively handle modality imbalance and the structural differences in DER's heuristic uncertainty regularizer, which adversely affect uncertainty estimation. This misalignment results in high uncertainty being incorrectly associated with accurate samples rather than challenging ones. Our observations indicate that existing methods lack the adaptability required for complex video scenarios. In response, we propose Debiased Evidential Learning for Moment Retrieval (DEMR), a novel framework that incorporates a Reflective Flipped Fusion (RFF) block for cross-modal alignment and a query reconstruction task to enhance text sensitivity, thereby reducing bias in uncertainty estimation. Additionally, we introduce a Geom-regularizer to refine uncertainty predictions, enabling adaptive alignment with difficult moments and improving retrieval accuracy. Extensive testing on standard datasets and debiased datasets ActivityNet-CD and Charades-CD demonstrates significant enhancements in effectiveness, robustness, and interpretability, positioning our approach as a promising solution for temporal-semantic robustness in moment retrieval. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: WUSH: Near-Optimal Adaptive Transforms for LLM Quantization</h3>
<ul>
<li><strong>Authors: </strong>Jiale Chen, Vage Egiazarian, Torsten Hoefler, Dan Alistarh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00956">https://arxiv.org/abs/2512.00956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00956">https://arxiv.org/pdf/2512.00956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00956]] WUSH: Near-Optimal Adaptive Transforms for LLM Quantization(https://arxiv.org/abs/2512.00956)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free, large language model</a></li>
<li><strong>Abstract: </strong>Quantization to low bitwidth is a standard approach for deploying large language models, however, a few extreme weights and activations stretch the dynamic range and reduce the effective resolution of the quantizer. A common mitigation approach is to apply some fixed orthogonal transforms, such as Hadamard matrices, before quantization, which typically reduces the dynamic range. Yet, these transforms ignore the statistics of the data, and their optimality is currently not understood. In this work, we derive, for the first time, closed-form optimal linear blockwise transforms for joint weight-activation quantization using standard data-free quantizers for common numerical formats. Specifically, we provide derivations of the optimal adaptive (data-aware) transforms for round-to-nearest (RTN), AbsMax-scaled block quantizers for both integer and floating-point formats. The resulting construction, which we call WUSH, combines a Hadamard backbone with a data-dependent component based on second-order moments, yielding a non-orthogonal transform that is provably optimal under mild assumptions and remains structured for efficient implementation. Preliminary experimental results show that our approach consistently improves upon the Hadamard transform for common formats.</li>
</ul>

<h3>Title: Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Boran Wen, Ye Lu, Keyan Wan, Sirui Wang, Jiahong Zhou, Junxuan Liang, Xinpeng Liu, Bang Xiao, Dingbang Huang, Ruiyang Liu, Yong-Lu Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00960">https://arxiv.org/abs/2512.00960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00960">https://arxiv.org/pdf/2512.00960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00960]] Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction(https://arxiv.org/abs/2512.00960)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Generalized robots must learn from diverse, large-scale human-object interactions (HOI) to operate robustly in the real world. Monocular internet videos offer a nearly limitless and readily available source of data, capturing an unparalleled diversity of human activities, objects, and environments. However, accurately and scalably extracting 4D interaction data from these in-the-wild videos remains a significant and unsolved challenge. Thus, in this work, we introduce 4DHOISolver, a novel and efficient optimization framework that constrains the ill-posed 4D HOI reconstruction problem by leveraging sparse, human-in-the-loop contact point annotations, while maintaining high spatio-temporal coherence and physical plausibility. Leveraging this framework, we introduce Open4DHOI, a new large-scale 4D HOI dataset featuring a diverse catalog of 144 object types and 103 actions. Furthermore, we demonstrate the effectiveness of our reconstructions by enabling an RL-based agent to imitate the recovered motions. However, a comprehensive benchmark of existing 3D foundation models indicates that automatically predicting precise human-object contact correspondences remains an unsolved problem, underscoring the immediate necessity of our human-in-the-loop strategy while posing an open challenge to the community. Data and code will be publicly available at this https URL</li>
</ul>

<h3>Title: Goal-Driven Reward by Video Diffusion Models for Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Qi Wang, Mian Wu, Yuyang Zhang, Mingqi Yuan, Wenyao Zhang, Haoxiang You, Yunbo Wang, Xin Jin, Xiaokang Yang, Wenjun Zeng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00961">https://arxiv.org/abs/2512.00961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00961">https://arxiv.org/pdf/2512.00961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00961]] Goal-Driven Reward by Video Diffusion Models for Reinforcement Learning(https://arxiv.org/abs/2512.00961)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has achieved remarkable success in various domains, yet it often relies on carefully designed programmatic reward functions to guide agent behavior. Designing such reward functions can be challenging and may not generalize well across different tasks. To address this limitation, we leverage the rich world knowledge contained in pretrained video diffusion models to provide goal-driven reward signals for RL agents without ad-hoc design of reward. Our key idea is to exploit off-the-shelf video diffusion models pretrained on large-scale video datasets as informative reward functions in terms of video-level and frame-level goals. For video-level rewards, we first finetune a pretrained video diffusion model on domain-specific datasets and then employ its video encoder to evaluate the alignment between the latent representations of agent's trajectories and the generated goal videos. To enable more fine-grained goal-achievement, we derive a frame-level goal by identifying the most relevant frame from the generated video using CLIP, which serves as the goal state. We then employ a learned forward-backward representation that represents the probability of visiting the goal state from a given state-action pair as frame-level reward, promoting more coherent and goal-driven trajectories. Experiments on various Meta-World tasks demonstrate the effectiveness of our approach.</li>
</ul>

<h3>Title: Mitigating Indirect Prompt Injection via Instruction-Following Intent Analysis</h3>
<ul>
<li><strong>Authors: </strong>Mintong Kang, Chong Xiang, Sanjay Kariyappa, Chaowei Xiao, Bo Li, Edward Suh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00966">https://arxiv.org/abs/2512.00966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00966">https://arxiv.org/pdf/2512.00966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00966]] Mitigating Indirect Prompt Injection via Instruction-Following Intent Analysis(https://arxiv.org/abs/2512.00966)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Indirect prompt injection attacks (IPIAs), where large language models (LLMs) follow malicious instructions hidden in input data, pose a critical threat to LLM-powered agents. In this paper, we present IntentGuard, a general defense framework based on instruction-following intent analysis. The key insight of IntentGuard is that the decisive factor in IPIAs is not the presence of malicious text, but whether the LLM intends to follow instructions from untrusted data. Building on this insight, IntentGuard leverages an instruction-following intent analyzer (IIA) to identify which parts of the input prompt the model recognizes as actionable instructions, and then flag or neutralize any overlaps with untrusted data segments. To instantiate the framework, we develop an IIA that uses three "thinking intervention" strategies to elicit a structured list of intended instructions from reasoning-enabled LLMs. These techniques include start-of-thinking prefilling, end-of-thinking refinement, and adversarial in-context demonstration. We evaluate IntentGuard on two agentic benchmarks (AgentDojo and Mind2Web) using two reasoning-enabled LLMs (Qwen-3-32B and gpt-oss-20B). Results demonstrate that IntentGuard achieves (1) no utility degradation in all but one setting and (2) strong robustness against adaptive prompt injection attacks (e.g., reducing attack success rates from 100% to 8.5% in a Mind2Web scenario).</li>
</ul>

<h3>Title: Advancing Academic Chatbots: Evaluation of Non Traditional Outputs</h3>
<ul>
<li><strong>Authors: </strong>Nicole Favero, Francesca Salute, Daniel Hardt</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00991">https://arxiv.org/abs/2512.00991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00991">https://arxiv.org/pdf/2512.00991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00991]] Advancing Academic Chatbots: Evaluation of Non Traditional Outputs(https://arxiv.org/abs/2512.00991)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Most evaluations of large language models focus on standard tasks such as factual question answering or short summarization. This research expands that scope in two directions: first, by comparing two retrieval strategies, Graph RAG, structured knowledge-graph based, and Advanced RAG, hybrid keyword-semantic search, for QA; and second, by evaluating whether LLMs can generate high quality non-traditional academic outputs, specifically slide decks and podcast scripts. We implemented a prototype combining Meta's LLaMA 3 70B open weight and OpenAI's GPT 4o mini API based. QA performance was evaluated using both human ratings across eleven quality dimensions and large language model judges for scalable cross validation. GPT 4o mini with Advanced RAG produced the most accurate responses. Graph RAG offered limited improvements and led to more hallucinations, partly due to its structural complexity and manual setup. Slide and podcast generation was tested with document grounded retrieval. GPT 4o mini again performed best, though LLaMA 3 showed promise in narrative coherence. Human reviewers were crucial for detecting layout and stylistic flaws, highlighting the need for combined human LLM evaluation in assessing emerging academic outputs.</li>
</ul>

<h3>Title: S2AM3D: Scale-controllable Part Segmentation of 3D Point Cloud</h3>
<ul>
<li><strong>Authors: </strong>Han Su, Tianyu Huang, Zichen Wan, Xiaohe Wu, Wangmeng Zuo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00995">https://arxiv.org/abs/2512.00995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00995">https://arxiv.org/pdf/2512.00995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00995]] S2AM3D: Scale-controllable Part Segmentation of 3D Point Cloud(https://arxiv.org/abs/2512.00995)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Part-level point cloud segmentation has recently attracted significant attention in 3D computer vision. Nevertheless, existing research is constrained by two major challenges: native 3D models lack generalization due to data scarcity, while introducing 2D pre-trained knowledge often leads to inconsistent segmentation results across different views. To address these challenges, we propose S2AM3D, which incorporates 2D segmentation priors with 3D consistent supervision. We design a point-consistent part encoder that aggregates multi-view 2D features through native 3D contrastive learning, producing globally consistent point features. A scale-aware prompt decoder is then proposed to enable real-time adjustment of segmentation granularity via continuous scale signals. Simultaneously, we introduce a large-scale, high-quality part-level point cloud dataset with more than 100k samples, providing ample supervision signals for model training. Extensive experiments demonstrate that S2AM3D achieves leading performance across multiple evaluation settings, exhibiting exceptional robustness and controllability when handling complex structures and parts with significant size variations.</li>
</ul>

<h3>Title: Provenance-Driven Reliable Semantic Medical Image Vector Reconstruction via Lightweight Blockchain-Verified Latent Fingerprints</h3>
<ul>
<li><strong>Authors: </strong>Mohsin Rasheed, Abdullah Al-Mamun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.00999">https://arxiv.org/abs/2512.00999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.00999">https://arxiv.org/pdf/2512.00999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.00999]] Provenance-Driven Reliable Semantic Medical Image Vector Reconstruction via Lightweight Blockchain-Verified Latent Fingerprints(https://arxiv.org/abs/2512.00999)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>Medical imaging is essential for clinical diagnosis, yet real-world data frequently suffers from corruption, noise, and potential tampering, challenging the reliability of AI-assisted interpretation. Conventional reconstruction techniques prioritize pixel-level recovery and may produce visually plausible outputs while compromising anatomical fidelity, an issue that can directly impact clinical outcomes. We propose a semantic-aware medical image reconstruction framework that integrates high-level latent embeddings with a hybrid U-Net architecture to preserve clinically relevant structures during restoration. To ensure trust and accountability, we incorporate a lightweight blockchain-based provenance layer using scale-free graph design, enabling verifiable recording of each reconstruction event without imposing significant overhead. Extensive evaluation across multiple datasets and corruption types demonstrates improved structural consistency, restoration accuracy, and provenance integrity compared with existing approaches. By uniting semantic-guided reconstruction with secure traceability, our solution advances dependable AI for medical imaging, enhancing both diagnostic confidence and regulatory compliance in healthcare environments.</li>
</ul>

<h3>Title: LISA-3D: Lifting Language-Image Segmentation to 3D via Multi-View Consistency</h3>
<ul>
<li><strong>Authors: </strong>Zhongbin Guo, Jiahe Liu, Wenyu Gao, Yushan Li, Chengzhi Li, Ping Jian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01008">https://arxiv.org/abs/2512.01008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01008">https://arxiv.org/pdf/2512.01008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01008]] LISA-3D: Lifting Language-Image Segmentation to 3D via Multi-View Consistency(https://arxiv.org/abs/2512.01008)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Text-driven 3D reconstruction demands a mask generator that simultaneously understands open-vocabulary instructions and remains consistent across viewpoints. We present LISA-3D, a two-stage framework that lifts language-image segmentation into 3D by retrofitting the instruction-following model LISA with geometry-aware Low-Rank Adaptation (LoRA) layers and reusing a frozen SAM-3D reconstructor. During training we exploit off-the-shelf RGB-D sequences and their camera poses to build a differentiable reprojection loss that enforces cross-view agreement without requiring any additional 3D-text supervision. The resulting masks are concatenated with RGB images to form RGBA prompts for SAM-3D, which outputs Gaussian splats or textured meshes without retraining. Across ScanRefer and Nr3D, LISA-3D improves language-to-3D accuracy by up to +15.6 points over single-view baselines while adapting only 11.6M parameters. The system is modular, data-efficient, and supports zero-shot deployment on unseen categories, providing a practical recipe for language-guided 3D content creation. Our code will be available at this https URL.</li>
</ul>

<h3>Title: Upper Approximation Bounds for Neural Oscillators</h3>
<ul>
<li><strong>Authors: </strong>Zifeng Huang, Konstantin M. Zuev, Yong Xia, Michael Beer</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS, math.FA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01015">https://arxiv.org/abs/2512.01015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01015">https://arxiv.org/pdf/2512.01015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01015]] Upper Approximation Bounds for Neural Oscillators(https://arxiv.org/abs/2512.01015)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural oscillators, originating from the second-order ordinary differential equations (ODEs), have demonstrated competitive performance in stably learning causal mappings between long-term sequences or continuous temporal functions. However, theoretically quantifying the capacities of their neural network architectures remains a significant challenge. In this study, the neural oscillator consisting of a second-order ODE followed by a multilayer perceptron (MLP) is considered. Its upper approximation bound for approximating causal and uniformly continuous operators between continuous temporal function spaces and that for approximating uniformly asymptotically incrementally stable second-order dynamical systems are derived. The established proof method of the approximation bound for approximating the causal continuous operators can also be directly applied to state-space models consisting of a linear time-continuous complex recurrent neural network followed by an MLP. Theoretical results reveal that the approximation error of the neural oscillator for approximating the second-order dynamical systems scales polynomially with the reciprocals of the widths of two utilized MLPs, thus mitigating the curse of parametric complexity. The decay rates of two established approximation error bounds are validated through two numerical cases. These results provide a robust theoretical foundation for the effective application of the neural oscillator in science and engineering.</li>
</ul>

<h3>Title: Operator-Theoretic Framework for Gradient-Free Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Mohit Kumar, Mathias Brucker, Alexander Valentinitsch, Adnan Husakovic, Ali Abbas, Manuela Geiß, Bernhard A. Moser</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01025">https://arxiv.org/abs/2512.01025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01025">https://arxiv.org/pdf/2512.01025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01025]] Operator-Theoretic Framework for Gradient-Free Federated Learning(https://arxiv.org/abs/2512.01025)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning must address heterogeneity, strict communication and computation limits, and privacy while ensuring performance. We propose an operator-theoretic framework that maps the $L^2$-optimal solution into a reproducing kernel Hilbert space (RKHS) via a forward operator, approximates it using available data, and maps back with the inverse operator, yielding a gradient-free scheme. Finite-sample bounds are derived using concentration inequalities over operator norms, and the framework identifies a data-dependent hypothesis space with guarantees on risk, error, robustness, and approximation. Within this space we design efficient kernel machines leveraging the space folding property of Kernel Affine Hull Machines. Clients transfer knowledge via a scalar space folding measure, reducing communication and enabling a simple differentially private protocol: summaries are computed from noise-perturbed data matrices in one step, avoiding per-round clipping and privacy accounting. The induced global rule requires only integer minimum and equality-comparison operations per test point, making it compatible with fully homomorphic encryption (FHE). Across four benchmarks, the gradient-free FL method with fixed encoder embeddings matches or outperforms strong gradient-based fine-tuning, with gains up to 23.7 points. In differentially private experiments, kernel smoothing mitigates accuracy loss in high-privacy regimes. The global rule admits an FHE realization using $Q \times C$ encrypted minimum and $C$ equality-comparison operations per test point, with operation-level benchmarks showing practical latencies. Overall, the framework provides provable guarantees with low communication, supports private knowledge transfer via scalar summaries, and yields an FHE-compatible prediction rule offering a mathematically grounded alternative to gradient-based federated learning under heterogeneity.</li>
</ul>

<h3>Title: Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model</h3>
<ul>
<li><strong>Authors: </strong>Jing He, Haodong Li, Mingzhi Sheng, Ying-Cong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01030">https://arxiv.org/abs/2512.01030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01030">https://arxiv.org/pdf/2512.01030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01030]] Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model(https://arxiv.org/abs/2512.01030)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recovering pixel-wise geometric properties from a single image is fundamentally ill-posed due to appearance ambiguity and non-injective mappings between 2D observations and 3D structures. While discriminative regression models achieve strong performance through large-scale supervision, their success is bounded by the scale, quality and diversity of available data and limited physical reasoning. Recent diffusion models exhibit powerful world priors that encode geometry and semantics learned from massive image-text data, yet directly reusing their stochastic generative formulation is suboptimal for deterministic geometric inference: the former is optimized for diverse and high-fidelity image generation, whereas the latter requires stable and accurate predictions. In this work, we propose Lotus-2, a two-stage deterministic framework for stable, accurate and fine-grained geometric dense prediction, aiming to provide an optimal adaption protocol to fully exploit the pre-trained generative priors. Specifically, in the first stage, the core predictor employs a single-step deterministic formulation with a clean-data objective and a lightweight local continuity module (LCM) to generate globally coherent structures without grid artifacts. In the second stage, the detail sharpener performs a constrained multi-step rectified-flow refinement within the manifold defined by the core predictor, enhancing fine-grained geometry through noise-free deterministic flow matching. Using only 59K training samples, less than 1% of existing large-scale datasets, Lotus-2 establishes new state-of-the-art results in monocular depth estimation and highly competitive surface normal prediction. These results demonstrate that diffusion models can serve as deterministic world priors, enabling high-quality geometric reasoning beyond traditional discriminative and generative paradigms.</li>
</ul>

<h3>Title: Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs</h3>
<ul>
<li><strong>Authors: </strong>MohammadParsa Dini, Human Jafari, Sajjad Amini, MohammadMahdi Mojahedian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01054">https://arxiv.org/abs/2512.01054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01054">https://arxiv.org/pdf/2512.01054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01054]] Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs(https://arxiv.org/abs/2512.01054)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, data-free, generative</a></li>
<li><strong>Abstract: </strong>Machine Unlearning is essential for large generative models (VAEs, DDPMs) to comply with the right to be forgotten and prevent undesired content generation without costly retraining. Existing approaches, such as Static-lambda SISS for diffusion models, rely on a fixed mixing weight lambda, which is suboptimal because the required unlearning strength varies across samples and training stages. We propose Adaptive-lambda SISS, a principled extension that turns lambda into a latent variable dynamically inferred at each training step. A lightweight inference network parameterizes an adaptive posterior over lambda, conditioned on contextual features derived from the instantaneous SISS loss terms (retain/forget losses and their gradients). This enables joint optimization of the diffusion model and the lambda-inference mechanism via a variational objective, yielding significantly better trade-offs. We further extend the adaptive-lambda principle to score-based unlearning and introduce a multi-class variant of Score Forgetting Distillation. In addition, we present two new directions: (i) a hybrid objective combining the data-free efficiency of Score Forgetting Distillation with the direct gradient control of SISS, and (ii) a Reinforcement Learning formulation that treats unlearning as a sequential decision process, learning an optimal policy over a state space defined by the model's current memory of the forget set. Experiments on an augmented MNIST benchmark show that Adaptive-lambda SISS substantially outperforms the original static-lambda SISS, achieving stronger removal of forgotten classes while better preserving generation quality on the retain set.</li>
</ul>

<h3>Title: Parameter Reduction Improves Vision Transformers: A Comparative Study of Sharing and Width Reduction</h3>
<ul>
<li><strong>Authors: </strong>Anantha Padmanaban Krishna Kumar (Boston University)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01059">https://arxiv.org/abs/2512.01059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01059">https://arxiv.org/pdf/2512.01059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01059]] Parameter Reduction Improves Vision Transformers: A Comparative Study of Sharing and Width Reduction(https://arxiv.org/abs/2512.01059)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Although scaling laws and many empirical results suggest that increasing the size of Vision Transformers often improves performance, model accuracy and training behavior are not always monotonically increasing with scale. Focusing on ViT-B/16 trained on ImageNet-1K, we study two simple parameter-reduction strategies applied to the MLP blocks, each removing 32.7\% of the baseline parameters. Our \emph{GroupedMLP} variant shares MLP weights between adjacent transformer blocks and achieves 81.47\% top-1 accuracy while maintaining the baseline computational cost. Our \emph{ShallowMLP} variant halves the MLP hidden dimension and reaches 81.25\% top-1 accuracy with a 38\% increase in inference throughput. Both models outperform the 86.6M-parameter baseline (81.05\%) and exhibit substantially improved training stability, reducing peak-to-final accuracy degradation from 0.47\% to the range 0.03\% to 0.06\%. These results suggest that, for ViT-B/16 on ImageNet-1K with a standard training recipe, the model operates in an overparameterized regime in which MLP capacity can be reduced without harming performance and can even slightly improve it. More broadly, our findings suggest that architectural constraints such as parameter sharing and reduced width may act as useful inductive biases, and highlight the importance of how parameters are allocated when designing Vision Transformers. All code is available at: this https URL.</li>
</ul>

<h3>Title: PIANO: Physics-informed Dual Neural Operator for Precipitation Nowcasting</h3>
<ul>
<li><strong>Authors: </strong>Seokhyun Chin, Junghwan Park, Woojin Cho</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01062">https://arxiv.org/abs/2512.01062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01062">https://arxiv.org/pdf/2512.01062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01062]] PIANO: Physics-informed Dual Neural Operator for Precipitation Nowcasting(https://arxiv.org/abs/2512.01062)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Precipitation nowcasting, key for early warning of disasters, currently relies on computationally expensive and restrictive methods that limit access to many countries. To overcome this challenge, we propose precipitation nowcasting using satellite imagery with physics constraints for improved accuracy and physical consistency. We use a novel physics-informed dual neural operator (PIANO) structure to enforce the fundamental equation of advection-diffusion during training to predict satellite imagery using a PINN loss. Then, we use a generative model to convert satellite images to radar images, which are used for precipitation nowcasting. Compared to baseline models, our proposed model shows a notable improvement in moderate (4mm/h) precipitation event prediction alongside short-term heavy (8mm/h) precipitation event prediction. It also demonstrates low seasonal variability in predictions, indicating robustness for generalization. This study suggests the potential of the PIANO and serves as a good baseline for physics-informed precipitation nowcasting.</li>
</ul>

<h3>Title: ELR-1000: A Community-Generated Dataset for Endangered Indic Indigenous Languages</h3>
<ul>
<li><strong>Authors: </strong>Neha Joshi, Pamir Gogoi, Aasim Mirza, Aayush Jansari, Aditya Yadavalli, Ayushi Pandey, Arunima Shukla, Deepthi Sudharsan, Kalika Bali, Vivek Seshadri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01077">https://arxiv.org/abs/2512.01077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01077">https://arxiv.org/pdf/2512.01077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01077]] ELR-1000: A Community-Generated Dataset for Endangered Indic Indigenous Languages(https://arxiv.org/abs/2512.01077)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present a culturally-grounded multimodal dataset of 1,060 traditional recipes crowdsourced from rural communities across remote regions of Eastern India, spanning 10 endangered languages. These recipes, rich in linguistic and cultural nuance, were collected using a mobile interface designed for contributors with low digital literacy. Endangered Language Recipes (ELR)-1000 -- captures not only culinary practices but also the socio-cultural context embedded in indigenous food traditions. We evaluate the performance of several state-of-the-art large language models (LLMs) on translating these recipes into English and find the following: despite the models' capabilities, they struggle with low-resource, culturally-specific language. However, we observe that providing targeted context -- including background information about the languages, translation examples, and guidelines for cultural preservation -- leads to significant improvements in translation quality. Our results underscore the need for benchmarks that cater to underrepresented languages and domains to advance equitable and culturally-aware language technologies. As part of this work, we release the ELR-1000 dataset to the NLP community, hoping it motivates the development of language technologies for endangered languages.</li>
</ul>

<h3>Title: Accelerating Inference of Masked Image Generators via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Pranav Subbaraman, Shufan Li, Siyan Zhao, Aditya Grover</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01094">https://arxiv.org/abs/2512.01094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01094">https://arxiv.org/pdf/2512.01094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01094]] Accelerating Inference of Masked Image Generators via Reinforcement Learning(https://arxiv.org/abs/2512.01094)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Masked Generative Models (MGM)s demonstrate strong capabilities in generating high-fidelity images. However, they need many sampling steps to create high-quality generations, resulting in slow inference speed. In this work, we propose Speed-RL, a novel paradigm for accelerating a pretrained MGMs to generate high-quality images in fewer steps. Unlike conventional distillation methods which formulate the acceleration problem as a distribution matching problem, where a few-step student model is trained to match the distribution generated by a many-step teacher model, we consider this problem as a reinforcement learning problem. Since the goal of acceleration is to generate high quality images in fewer steps, we can combine a quality reward with a speed reward and finetune the base model using reinforcement learning with the combined reward as the optimization target. Through extensive experiments, we show that the proposed method was able to accelerate the base model by a factor of 3x while maintaining comparable image quality.</li>
</ul>

<h3>Title: How do we measure privacy in text? A survey of text anonymization metrics</h3>
<ul>
<li><strong>Authors: </strong>Yaxuan Ren, Krithika Ramesh, Yaxing Yao, Anjalie Field</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01109">https://arxiv.org/abs/2512.01109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01109">https://arxiv.org/pdf/2512.01109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01109]] How do we measure privacy in text? A survey of text anonymization metrics(https://arxiv.org/abs/2512.01109)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust</a></li>
<li><strong>Abstract: </strong>In this work, we aim to clarify and reconcile metrics for evaluating privacy protection in text through a systematic survey. Although text anonymization is essential for enabling NLP research and model development in domains with sensitive data, evaluating whether anonymization methods sufficiently protect privacy remains an open challenge. In manually reviewing 47 papers that report privacy metrics, we identify and compare six distinct privacy notions, and analyze how the associated metrics capture different aspects of privacy risk. We then assess how well these notions align with legal privacy standards (HIPAA and GDPR), as well as user-centered expectations grounded in HCI studies. Our analysis offers practical guidance on navigating the landscape of privacy evaluation approaches further and highlights gaps in current practices. Ultimately, we aim to facilitate more robust, comparable, and legally aware privacy evaluations in text anonymization.</li>
</ul>

<h3>Title: Efficiently Learning Branching Networks for Multitask Algorithmic Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Dongyue Li, Zhenshuo Zhang, Minxuan Duan, Edgar Dobriban, Hongyang R. Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01113">https://arxiv.org/abs/2512.01113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01113">https://arxiv.org/pdf/2512.01113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01113]] Efficiently Learning Branching Networks for Multitask Algorithmic Reasoning(https://arxiv.org/abs/2512.01113)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Algorithmic reasoning -- the ability to perform step-by-step logical inference -- has become a core benchmark for evaluating reasoning in graph neural networks (GNNs) and large language models (LLMs). Ideally, one would like to design a single model capable of performing well on multiple algorithmic reasoning tasks simultaneously. However, this is challenging when the execution steps of algorithms differ from one another, causing negative interference when they are trained together. We propose branching neural networks, a principled architecture for multitask algorithmic reasoning. Searching for the optimal $k$-ary tree with $L$ layers over $n$ algorithmic tasks is combinatorial, requiring exploration of up to $k^{nL}$ possible structures. We develop AutoBRANE, an efficient algorithm that reduces this search to $O(nL)$ time by solving a convex relaxation at each layer to approximate an optimal task partition. The method clusters tasks using gradient-based affinity scores and can be used on top of any base model, including GNNs and LLMs. We validate AutoBRANE on a broad suite of graph-algorithmic and text-based reasoning benchmarks. We show that gradient features estimate true task performance within 5% error across four GNNs and four LLMs (up to 34B parameters). On the CLRS benchmark, it outperforms the strongest single multitask GNN by 3.7% and the best baseline by 1.2%, while reducing runtime by 48% and memory usage by 26%. The learned branching structures reveal an intuitively reasonable hierarchical clustering of related algorithms. On three text-based graph reasoning benchmarks, AutoBRANE improves over the best non-branching multitask baseline by 3.2%. Finally, on a large graph dataset with 21M edges and 500 tasks, AutoBRANE achieves a 28% accuracy gain over existing multitask and branching architectures, along with a 4.5$\times$ reduction in runtime.</li>
</ul>

<h3>Title: Sliced Rényi Pufferfish Privacy: Directional Additive Noise Mechanism and Private Learning with Gradient Clipping</h3>
<ul>
<li><strong>Authors: </strong>Tao Zhang, Yevgeniy Vorobeychik</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01115">https://arxiv.org/abs/2512.01115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01115">https://arxiv.org/pdf/2512.01115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01115]] Sliced Rényi Pufferfish Privacy: Directional Additive Noise Mechanism and Private Learning with Gradient Clipping(https://arxiv.org/abs/2512.01115)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>We study privatization mechanism design and privacy accounting in the Pufferfish family, addressing two practical gaps of Renyi Pufferfish Privacy (RPP): high-dimensional optimal transport (OT) calibration and the absence of a general, mechanism-agnostic composition rule for iterative learning. We introduce Sliced Renyi Pufferfish Privacy (SRPP), which replaces high-dimensional comparisons by directional ones over a set of unit vectors, enabling geometry-aware and tractable guarantees. To calibrate noise without high-dimensional OT, we propose sliced Wasserstein mechanisms that compute per-direction (1-D) sensitivities, yielding closed-form, statistically stable, and anisotropic calibrations. We further define SRPP Envelope (SRPE) as computable upper bounds that are tightly implementable by these sliced Wasserstein mechanisms. For iterative deep learning algorithms, we develop a decompose-then-compose SRPP-SGD scheme with gradient clipping based on a History-Uniform Cap (HUC), a pathwise bound on one-step directional changes that is uniform over optimization history, and a mean-square variant (ms-HUC) that leverages subsampling randomness to obtain on-average SRPP guarantees with improved utility. The resulting HUC and ms-HUC accountants aggregate per-iteration, per-direction Renyi costs and integrate naturally with moments-accountant style analyses. Finally, when multiple mechanisms are trained and privatized independently under a common slicing geometry, our analysis yields graceful additive composition in both worst-case and mean-square regimes. Our experiments indicate that the proposed SRPP-based methods achieve favorable privacy-utility trade-offs in both static and iterative settings.</li>
</ul>

<h3>Title: Structural Prognostic Event Modeling for Multimodal Cancer Survival Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yilan Zhang, Li Nanbo, Changchun Yang, Jürgen Schmidhuber, Xin Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01116">https://arxiv.org/abs/2512.01116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01116">https://arxiv.org/pdf/2512.01116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01116]] Structural Prognostic Event Modeling for Multimodal Cancer Survival Analysis(https://arxiv.org/abs/2512.01116)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>The integration of histology images and gene profiles has shown great promise for improving survival prediction in cancer. However, current approaches often struggle to model intra- and inter-modal interactions efficiently and effectively due to the high dimensionality and complexity of the inputs. A major challenge is capturing critical prognostic events that, though few, underlie the complexity of the observed inputs and largely determine patient outcomes. These events, manifested as high-level structural signals such as spatial histologic patterns or pathway co-activations, are typically sparse, patient-specific, and unannotated, making them inherently difficult to uncover. To address this, we propose SlotSPE, a slot-based framework for structural prognostic event modeling. Specifically, inspired by the principle of factorial coding, we compress each patient's multimodal inputs into compact, modality-specific sets of mutually distinctive slots using slot attention. By leveraging these slot representations as encodings for prognostic events, our framework enables both efficient and effective modeling of complex intra- and inter-modal interactions, while also facilitating seamless incorporation of biological priors that enhance prognostic relevance. Extensive experiments on ten cancer benchmarks show that SlotSPE outperforms existing methods in 8 out of 10 cohorts, achieving an overall improvement of 2.9%. It remains robust under missing genomic data and delivers markedly improved interpretability through structured event decomposition.</li>
</ul>

<h3>Title: World Model Robustness via Surprise Recognition</h3>
<ul>
<li><strong>Authors: </strong>Geigh Zollicoffer, Tanush Chopra, Mingkuan Yan, Xiaoxu Ma, Kenneth Eaton, Mark Riedl</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01119">https://arxiv.org/abs/2512.01119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01119">https://arxiv.org/pdf/2512.01119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01119]] World Model Robustness via Surprise Recognition(https://arxiv.org/abs/2512.01119)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>AI systems deployed in the real world must contend with distractions and out-of-distribution (OOD) noise that can destabilize their policies and lead to unsafe behavior. While robust training can reduce sensitivity to some forms of noise, it is infeasible to anticipate all possible OOD conditions. To mitigate this issue, we develop an algorithm that leverages a world model's inherent measure of surprise to reduce the impact of noise in world model--based reinforcement learning agents. We introduce both multi-representation and single-representation rejection sampling, enabling robustness to settings with multiple faulty sensors or a single faulty sensor. While the introduction of noise typically degrades agent performance, we show that our techniques preserve performance relative to baselines under varying types and levels of noise across multiple environments within self-driving simulation domains (CARLA and Safety Gymnasium). Furthermore, we demonstrate that our methods enhance the stability of two state-of-the-art world models with markedly different underlying architectures: Cosmos and DreamerV3. Together, these results highlight the robustness of our approach across world modeling domains. We release our code at this https URL .</li>
</ul>

<h3>Title: OmniFD: A Unified Model for Versatile Face Forgery Detection</h3>
<ul>
<li><strong>Authors: </strong>Haotian Liu, Haoyu Chen, Chenhui Pan, You Hu, Guoying Zhao, Xiaobai Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01128">https://arxiv.org/abs/2512.01128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01128">https://arxiv.org/pdf/2512.01128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01128]] OmniFD: A Unified Model for Versatile Face Forgery Detection(https://arxiv.org/abs/2512.01128)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Face forgery detection encompasses multiple critical tasks, including identifying forged images and videos and localizing manipulated regions and temporal segments. Current approaches typically employ task-specific models with independent architectures, leading to computational redundancy and ignoring potential correlations across related tasks. We introduce OmniFD, a unified framework that jointly addresses four core face forgery detection tasks within a single model, i.e., image and video classification, spatial localization, and temporal localization. Our architecture consists of three principal components: (1) a shared Swin Transformer encoder that extracts unified 4D spatiotemporal representations from both images and video inputs, (2) a cross-task interaction module with learnable queries that dynamically captures inter-task dependencies through attention-based reasoning, and (3) lightweight decoding heads that transform refined representations into corresponding predictions for all FFD tasks. Extensive experiments demonstrate OmniFD's advantage over task-specific models. Its unified design leverages multi-task learning to capture generalized representations across tasks, especially enabling fine-grained knowledge transfer that facilitates other tasks. For example, video classification accuracy improves by 4.63% when image data are incorporated. Furthermore, by unifying images, videos and the four tasks within one framework, OmniFD achieves superior performance across diverse benchmarks with high efficiency and scalability, e.g., reducing 63% model parameters and 50% training time. It establishes a practical and generalizable solution for comprehensive face forgery detection in real-world applications. The source code is made available at this https URL.</li>
</ul>

<h3>Title: Fiber Bundle Networks: A Geometric Machine Learning Paradigm</h3>
<ul>
<li><strong>Authors: </strong>Dong Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01151">https://arxiv.org/abs/2512.01151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01151">https://arxiv.org/pdf/2512.01151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01151]] Fiber Bundle Networks: A Geometric Machine Learning Paradigm(https://arxiv.org/abs/2512.01151)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We propose Fiber Bundle Networks (FiberNet), a novel machine learning framework integrating differential geometry with machine learning. Unlike traditional deep neural networks relying on black-box function fitting, we reformulate classification as interpretable geometric optimization on fiber bundles, where categories form the base space and wavelet-transformed features lie in the fibers above each category. We introduce two innovations: (1) learnable Riemannian metrics identifying important frequency feature components, (2) variational prototype optimization through energy function minimization. Classification is performed via Voronoi tessellation under the learned Riemannian metric, where each prototype defines a decision region and test samples are assigned to the nearest prototype, providing clear geometric interpretability. This work demonstrates that the integration of fiber bundle with machine learning provides interpretability and efficiency, which are difficult to obtain simultaneously in conventional deep learning.</li>
</ul>

<h3>Title: Open-Set Domain Adaptation Under Background Distribution Shift: Challenges and A Provably Efficient Solution</h3>
<ul>
<li><strong>Authors: </strong>Shravan Chaudhari, Yoav Wald, Suchi Saria</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01152">https://arxiv.org/abs/2512.01152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01152">https://arxiv.org/pdf/2512.01152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01152]] Open-Set Domain Adaptation Under Background Distribution Shift: Challenges and A Provably Efficient Solution(https://arxiv.org/abs/2512.01152)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As we deploy machine learning systems in the real world, a core challenge is to maintain a model that is performant even as the data shifts. Such shifts can take many forms: new classes may emerge that were absent during training, a problem known as open-set recognition, and the distribution of known categories may change. Guarantees on open-set recognition are mostly derived under the assumption that the distribution of known classes, which we call \emph{the background distribution}, is fixed. In this paper we develop \ours{}, a method that is guaranteed to solve open-set recognition even in the challenging case where the background distribution shifts. We prove that the method works under benign assumptions that the novel class is separable from the non-novel classes, and provide theoretical guarantees that it outperforms a representative baseline in a simplified overparameterized setting. We develop techniques to make \ours{} scalable and robust, and perform comprehensive empirical evaluations on image and text data. The results show that \ours{} significantly outperforms existing open-set recognition methods under background shift. Moreover, we provide new insights into how factors such as the size of the novel class influences performance, an aspect that has not been extensively explored in prior work.</li>
</ul>

<h3>Title: DPAC: Distribution-Preserving Adversarial Control for Diffusion Sampling</h3>
<ul>
<li><strong>Authors: </strong>Han-Jin Lee, Han-Ju Lee, Jin-Seong Kim, Seok-Hwan Choi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01153">https://arxiv.org/abs/2512.01153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01153">https://arxiv.org/pdf/2512.01153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01153]] DPAC: Distribution-Preserving Adversarial Control for Diffusion Sampling(https://arxiv.org/abs/2512.01153)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Adversarially guided diffusion sampling often achieves the target class, but sample quality degrades as deviations between the adversarially controlled and nominal trajectories accumulate. We formalize this degradation as a path-space Kullback-Leibler divergence(path-KL) between controlled and nominal (uncontrolled) diffusion processes, thereby showing via Girsanov's theorem that it exactly equals the control energy. Building on this stochastic optimal control (SOC) view, we theoretically establish that minimizing this path-KL simultaneously tightens upper bounds on both the 2-Wasserstein distance and Fréchet Inception Distance (FID), revealing a principled connection between adversarial control energy and perceptual fidelity. From a variational perspective, we derive a first-order optimality condition for the control: among all directions that yield the same classification gain, the component tangent to iso-(log-)density surfaces (i.e., orthogonal to the score) minimizes path-KL, whereas the normal component directly increases distributional drift. This leads to DPAC (Distribution-Preserving Adversarial Control), a diffusion guidance rule that projects adversarial gradients onto the tangent space defined by the generative score geometry. We further show that in discrete solvers, the tangent projection cancels the O({\Delta}t) leading error term in the Wasserstein distance, achieving an O({\Delta}t^2) quality gap; moreover, it remains second-order robust to score or metric approximation. Empirical studies on ImageNet-100 validate the theoretical predictions, confirming that DPAC achieves lower FID and estimated path-KL at matched attack success rates.</li>
</ul>

<h3>Title: 2D-ThermAl: Physics-Informed Framework for Thermal Analysis of Circuits using Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Soumyadeep Chandra, Sayeed Shafayet Chowdhury, Kaushik Roy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01163">https://arxiv.org/abs/2512.01163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01163">https://arxiv.org/pdf/2512.01163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01163]] 2D-ThermAl: Physics-Informed Framework for Thermal Analysis of Circuits using Generative AI(https://arxiv.org/abs/2512.01163)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Thermal analysis is increasingly critical in modern integrated circuits, where non-uniform power dissipation and high transistor densities can cause rapid temperature spikes and reliability concerns. Traditional methods, such as FEM-based simulations offer high accuracy but computationally prohibitive for early-stage design, often requiring multiple iterative redesign cycles to resolve late-stage thermal failures. To address these challenges, we propose 'ThermAl', a physics-informed generative AI framework which effectively identifies heat sources and estimates full-chip transient and steady-state thermal distributions directly from input activity profiles. ThermAl employs a hybrid U-Net architecture enhanced with positional encoding and a Boltzmann regularizer to maintain physical fidelity. Our model is trained on an extensive dataset of heat dissipation maps, ranging from simple logic gates (e.g., inverters, NAND, XOR) to complex designs, generated via COMSOL. Experimental results demonstrate that ThermAl delivers precise temperature mappings for large circuits, with a root mean squared error (RMSE) of only 0.71°C, and outperforms conventional FEM tools by running up to ~200 times faster. We analyze performance across diverse layouts and workloads, and discuss its applicability to large-scale EDA workflows. While thermal reliability assessments often extend beyond 85°C for post-layout signoff, our focus here is on early-stage hotspot detection and thermal pattern learning. To ensure generalization beyond the nominal operating range 25-55°C, we additionally performed cross-validation on an extended dataset spanning 25-95°C maintaining a high accuracy (<2.2% full-scale RMSE) even under elevated temperature conditions representative of peak power and stress scenarios.</li>
</ul>

<h3>Title: Reverse Engineering and Control-Aware Security Analysis of the ArduPilot UAV Framework</h3>
<ul>
<li><strong>Authors: </strong>Yasaswini Konapalli, Lotfi Ben Othmane, Cihan Tunc, Feras Benchellal, Likhita Mudagere</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01164">https://arxiv.org/abs/2512.01164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01164">https://arxiv.org/pdf/2512.01164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01164]] Reverse Engineering and Control-Aware Security Analysis of the ArduPilot UAV Framework(https://arxiv.org/abs/2512.01164)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Unmanned Aerial Vehicle (UAV) technologies are gaining high interest for many domains, which makes UAV security of utmost importance. ArduPilot is among the most widely used open-source autopilot UAV frameworks; yet, many studies demonstrate the vulnerabilities affecting such systems. Vulnerabilities within its communication subsystems (including WiFi, telemetry, or GPS) expose critical entry points, and vulnerabilities in Ardupilot can affect the control procedure. In this paper, we reconstruct the software architecture and the control models implemented by ArduPilot and then examine how these control models could potentially misused to induce malicious behaviors while relying on legitimate inputs.</li>
</ul>

<h3>Title: Real-Time On-the-Go Annotation Framework Using YOLO for Automated Dataset Generation</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Abdallah Salem (1), Ahmed Harb Rabia (1) ((1) North Dakota State University)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01165">https://arxiv.org/abs/2512.01165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01165">https://arxiv.org/pdf/2512.01165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01165]] Real-Time On-the-Go Annotation Framework Using YOLO for Automated Dataset Generation(https://arxiv.org/abs/2512.01165)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Efficient and accurate annotation of datasets remains a significant challenge for deploying object detection models such as You Only Look Once (YOLO) in real-world applications, particularly in agriculture where rapid decision-making is critical. Traditional annotation techniques are labor-intensive, requiring extensive manual labeling post data collection. This paper presents a novel real-time annotation approach leveraging YOLO models deployed on edge devices, enabling immediate labeling during image capture. To comprehensively evaluate the efficiency and accuracy of our proposed system, we conducted an extensive comparative analysis using three prominent YOLO architectures (YOLOv5, YOLOv8, YOLOv12) under various configurations: single-class versus multi-class annotation and pretrained versus scratch-based training. Our analysis includes detailed statistical tests and learning dynamics, demonstrating significant advantages of pretrained and single-class configurations in terms of model convergence, performance, and robustness. Results strongly validate the feasibility and effectiveness of our real-time annotation framework, highlighting its capability to drastically reduce dataset preparation time while maintaining high annotation quality.</li>
</ul>

<h3>Title: Data assimilation and discrepancy modeling with shallow recurrent decoders</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Bao, J. Nathan Kutz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.AP, nlin.CD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01170">https://arxiv.org/abs/2512.01170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01170">https://arxiv.org/pdf/2512.01170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01170]] Data assimilation and discrepancy modeling with shallow recurrent decoders(https://arxiv.org/abs/2512.01170)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The requirements of modern sensing are rapidly evolving, driven by increasing demands for data efficiency, real-time processing, and deployment under limited sensing coverage. Complex physical systems are often characterized through the integration of a limited number of point sensors in combination with scientific computations which approximate the dominant, full-state dynamics. Simulation models, however, inevitably neglect small-scale or hidden processes, are sensitive to perturbations, or oversimplify parameter correlations, leading to reconstructions that often diverge from the reality measured by sensors. This creates a critical need for data assimilation, the process of integrating observational data with predictive simulation models to produce coherent and accurate estimates of the full state of complex physical systems. We propose a machine learning framework for Data Assimilation with a SHallow REcurrent Decoder (DA-SHRED) which bridges the simulation-to-real (SIM2REAL) gap between computational modeling and experimental sensor data. For real-world physics systems modeling high-dimensional spatiotemporal fields, where the full state cannot be directly observed and must be inferred from sparse sensor measurements, we leverage the latent space learned from a reduced simulation model via SHRED, and update these latent variables using real sensor data to accurately reconstruct the full system state. Furthermore, our algorithm incorporates a sparse identification of nonlinear dynamics based regression model in the latent space to identify functionals corresponding to missing dynamics in the simulation model. We demonstrate that DA-SHRED successfully closes the SIM2REAL gap and additionally recovers missing dynamics in highly complex systems, demonstrating that the combination of efficient temporal encoding and physics-informed correction enables robust data assimilation.</li>
</ul>

<h3>Title: DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks</h3>
<ul>
<li><strong>Authors: </strong>Hyunjun Kim, Sooyoung Ryu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01174">https://arxiv.org/abs/2512.01174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01174">https://arxiv.org/pdf/2512.01174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01174]] DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks(https://arxiv.org/abs/2512.01174)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As agentic AI systems increasingly operate autonomously, establishing trust through verifiable evaluation becomes critical. Yet existing benchmarks lack the transparency and auditability needed to assess whether agents behave reliably. We present DrawingBench, a verification framework for evaluating the trustworthiness of agentic LLMs through spatial reasoning tasks that require generating sequences of low-level GUI actions. Unlike opaque evaluations, DrawingBench provides transparent, rule-based assessment: 8 objective criteria enable reproducible scoring, while action-level inspection allows stakeholders to audit agent behavior. Our framework comprises 250 diverse prompts across 20 categories and 4 difficulty levels, deterministic evaluation metrics, and an external oversight mechanism through multi-turn feedback that enables human control over agent refinement. Evaluating four state-of-the-art LLMs (Claude-4 Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash) across 1,000 tests, we establish both capabilities and limitations: models achieved 92.8% perfect performance with structured external feedback driving significant improvements (average +3.2%, up to +32.8% for complex scenes), but systematic error patterns emerged in tool state management and long-horizon planning. Notably, specification clarity proved more important than task complexity -- models achieved 100% perfect performance when given explicit, verifiable criteria. These findings demonstrate that transparent evaluation frameworks can establish trust in agentic systems, with external oversight proving more reliable than self-correction for guiding agent behavior. Our open-source framework provides a template for trustworthy agent assessment. Code and data: this https URL</li>
</ul>

<h3>Title: First On-Orbit Demonstration of a Geospatial Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Andrew Du, Roberto Del Prete, Alejandro Mousist, Nick Manser, Fabrice Marre, Andrew Barton, Carl Seubert, Gabriele Meoni, Tat-Jun Chin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01181">https://arxiv.org/abs/2512.01181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01181">https://arxiv.org/pdf/2512.01181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01181]] First On-Orbit Demonstration of a Geospatial Foundation Model(https://arxiv.org/abs/2512.01181)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Geospatial foundation models (GeoFMs) promise broad generalisation capacity for Earth observation (EO) tasks, particularly under data-limited conditions. However, their large size poses a barrier to deployment on resource-constrained space hardware. To address this, we present compact variants of a Vision Transformer (ViT)-based GeoFM that preserve downstream task performance while enabling onboard execution. Evaluation across five downstream tasks and validation in two representative flight environments show that model compression and domain adaptation are critical to reducing size and resource demands while maintaining high performance under operational conditions. We further demonstrate reliable on-orbit inference with the IMAGIN-e payload aboard the International Space Station. These results establish a pathway from large GeoFMs to flight-ready, resource-efficient deployments, expanding the feasibility of onboard AI for EO missions.</li>
</ul>

<h3>Title: TempPerturb-Eval: On the Joint Effects of Internal Temperature and External Perturbations in RAG Robustness</h3>
<ul>
<li><strong>Authors: </strong>Yongxin Zhou, Philippe Mulhem, Didier Schwab</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01183">https://arxiv.org/abs/2512.01183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01183">https://arxiv.org/pdf/2512.01183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01183]] TempPerturb-Eval: On the Joint Effects of Internal Temperature and External Perturbations in RAG Robustness(https://arxiv.org/abs/2512.01183)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The evaluation of Retrieval-Augmented Generation (RAG) systems typically examines retrieval quality and generation parameters like temperature in isolation, overlooking their interaction. This work presents a systematic investigation of how text perturbations (simulating noisy retrieval) interact with temperature settings across multiple LLM runs. We propose a comprehensive RAG Perturbation-Temperature Analysis Framework that subjects retrieved documents to three distinct perturbation types across varying temperature settings. Through extensive experiments on HotpotQA with both open-source and proprietary LLMs, we demonstrate that performance degradation follows distinct patterns: high-temperature settings consistently amplify vulnerability to perturbations, while certain perturbation types exhibit non-linear sensitivity across the temperature range. Our work yields three key contributions: (1) a diagnostic benchmark for assessing RAG robustness, (2) an analytical framework for quantifying perturbation-temperature interactions, and (3) practical guidelines for model selection and parameter tuning under noisy retrieval conditions.</li>
</ul>

<h3>Title: DefenSee: Dissecting Threat from Sight and Text - A Multi-View Defensive Pipeline for Multi-modal Jailbreaks</h3>
<ul>
<li><strong>Authors: </strong>Zihao Wang, Kar Wai Fok, Vrizlynn L. L. Thing</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01185">https://arxiv.org/abs/2512.01185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01185">https://arxiv.org/pdf/2512.01185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01185]] DefenSee: Dissecting Threat from Sight and Text - A Multi-View Defensive Pipeline for Multi-modal Jailbreaks(https://arxiv.org/abs/2512.01185)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Multi-modal large language models (MLLMs), capable of processing text, images, and audio, have been widely adopted in various AI applications. However, recent MLLMs integrating images and text remain highly vulnerable to coordinated jailbreaks. Existing defenses primarily focus on the text, lacking robust multi-modal protection. As a result, studies indicate that MLLMs are more susceptible to malicious or unsafe instructions, unlike their text-only counterparts. In this paper, we proposed DefenSee, a robust and lightweight multi-modal black-box defense technique that leverages image variants transcription and cross-modal consistency checks, mimicking human judgment. Experiments on popular multi-modal jailbreak and benign datasets show that DefenSee consistently enhances MLLM robustness while better preserving performance on benign tasks compared to SOTA defenses. It reduces the ASR of jailbreak attacks to below 1.70% on MiniGPT4 using the MM-SafetyBench benchmark, significantly outperforming prior methods under the same conditions.</li>
</ul>

<h3>Title: Teaching by Failure: Counter-Example-Driven Curricula for Transformer Self-Improvement</h3>
<ul>
<li><strong>Authors: </strong>Harshil Vejendla</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01187">https://arxiv.org/abs/2512.01187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01187">https://arxiv.org/pdf/2512.01187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01187]] Teaching by Failure: Counter-Example-Driven Curricula for Transformer Self-Improvement(https://arxiv.org/abs/2512.01187)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Transformer models often exhibit brittle extrapolation, failing on inputs that are longer or structurally more complex than those seen during training. We introduce Counter-Example-Driven Curricula (CEDC), an automated framework that improves model robustness by iteratively focusing on its own failures. At each step, CEDC uses the current model to generate a diverse set of candidate problems, employs a fast, executable verifier to identify incorrect predictions (counter-examples), and then fine-tunes the model on a dataset enriched with these discovered failures. We evaluate CEDC on a suite of algorithmic and natural language tasks, including integer addition, sorting, Dyck-2 language recognition, and three text classification benchmarks. Compared to static training and standard curriculum learning baselines, CEDC achieves up to 30x greater length extrapolation, is 3.75x more computationally efficient than uniform data augmentation, and requires no manual difficulty heuristics. We provide a detailed analysis of the counter-examples, showing how the curriculum naturally adapts to target progressively more complex error modes. Our findings establish verifier-guided, failure-driven learning as a simple, powerful, and efficient paradigm for enhancing the generalization capabilities of Transformer models.</li>
</ul>

<h3>Title: LGDC: Latent Graph Diffusion via Spectrum-Preserving Coarsening</h3>
<ul>
<li><strong>Authors: </strong>Nagham Osman, Keyue Jiang, Davide Buffelli, Xiaowen Dong, Laura Toni</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01190">https://arxiv.org/abs/2512.01190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01190">https://arxiv.org/pdf/2512.01190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01190]] LGDC: Latent Graph Diffusion via Spectrum-Preserving Coarsening(https://arxiv.org/abs/2512.01190)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Graph generation is a critical task across scientific domains. Existing methods fall broadly into two categories: autoregressive models, which iteratively expand graphs, and one-shot models, such as diffusion, which generate the full graph at once. In this work, we provide an analysis of these two paradigms and reveal a key trade-off: autoregressive models stand out in capturing fine-grained local structures, such as degree and clustering properties, whereas one-shot models excel at modeling global patterns, such as spectral distributions. Building on this, we propose LGDC (latent graph diffusion via spectrum-preserving coarsening), a hybrid framework that combines strengths of both approaches. LGDC employs a spectrum-preserving coarsening-decoarsening to bidirectionally map between graphs and a latent space, where diffusion efficiently generates latent graphs before expansion restores detail. This design captures both local and global properties with improved efficiency. Empirically, LGDC matches autoregressive models on locally structured datasets (Tree) and diffusion models on globally structured ones (Planar, Community-20), validating the benefits of hybrid generation.</li>
</ul>

<h3>Title: Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Krithik Vishwanath, Mrigayu Ghosh, Anton Alyakin, Daniel Alexander Alber, Yindalon Aphinyanaphongs, Eric Karl Oermann</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01191">https://arxiv.org/abs/2512.01191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01191">https://arxiv.org/pdf/2512.01191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01191]] Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks(https://arxiv.org/abs/2512.01191)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Specialized clinical AI assistants are rapidly entering medical practice, often framed as safer or more reliable than general-purpose large language models (LLMs). Yet, unlike frontier models, these clinical tools are rarely subjected to independent, quantitative evaluation, creating a critical evidence gap despite their growing influence on diagnosis, triage, and guideline interpretation. We assessed two widely deployed clinical AI systems (OpenEvidence and UpToDate Expert AI) against three state-of-the-art generalist LLMs (GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) using a 1,000-item mini-benchmark combining MedQA (medical knowledge) and HealthBench (clinician-alignment) tasks. Generalist models consistently outperformed clinical tools, with GPT-5 achieving the highest scores, while OpenEvidence and UpToDate demonstrated deficits in completeness, communication quality, context awareness, and systems-based safety reasoning. These findings reveal that tools marketed for clinical decision support may often lag behind frontier LLMs, underscoring the urgent need for transparent, independent evaluation before deployment in patient-facing workflows.</li>
</ul>

<h3>Title: Learning to Reconstruct Temperature Field from Sparse Observations with Implicit Physics Priors</h3>
<ul>
<li><strong>Authors: </strong>Shihang Li, Zhiqiang Gong, Weien Zhou, Yue Gao, Wen Yao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01196">https://arxiv.org/abs/2512.01196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01196">https://arxiv.org/pdf/2512.01196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01196]] Learning to Reconstruct Temperature Field from Sparse Observations with Implicit Physics Priors(https://arxiv.org/abs/2512.01196)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate reconstruction of temperature field of heat-source systems (TFR-HSS) is crucial for thermal monitoring and reliability assessment in engineering applications such as electronic devices and aerospace structures. However, the high cost of measurement acquisition and the substantial distributional shifts in temperature field across varying conditions present significant challenges for developing reconstruction models with robust generalization capabilities. Existing DNNs-based methods typically formulate TFR-HSS as a one-to-one regression problem based solely on target sparse measurements, without effectively leveraging reference simulation data that implicitly encode thermal knowledge. To address this limitation, we propose IPTR, an implicit physics-guided temperature field reconstruction framework that introduces sparse monitoring-temperature field pair from reference simulations as priors to enrich physical understanding. To integrate both reference and target information, we design a dual physics embedding module consisting of two complementary branches: an implicit physics-guided branch employing cross-attention to distill latent physics from the reference data, and an auxiliary encoding branch based on Fourier layers to capture the spatial characteristics of the target observation. The fused representation is then decoded to reconstruct the full temperature field. Extensive experiments under single-condition, multi-condition, and few-shot settings demonstrate that IPTR consistently outperforms existing methods, achieving state-of-the-art reconstruction accuracy and strong generalization capability.</li>
</ul>

<h3>Title: Know Thyself by Knowing Others: Learning Neuron Identity from Population Context</h3>
<ul>
<li><strong>Authors: </strong>Vinam Arora, Divyansha Lachi, Ian J. Knight, Mehdi Azabou, Blake Richards, Cole L. Hurwitz, Josh Siegle, Eva L. Dyer</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01199">https://arxiv.org/abs/2512.01199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01199">https://arxiv.org/pdf/2512.01199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01199]] Know Thyself by Knowing Others: Learning Neuron Identity from Population Context(https://arxiv.org/abs/2512.01199)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Neurons process information in ways that depend on their cell type, connectivity, and the brain region in which they are embedded. However, inferring these factors from neural activity remains a significant challenge. To build general-purpose representations that allow for resolving information about a neuron's identity, we introduce NuCLR, a self-supervised framework that aims to learn representations of neural activity that allow for differentiating one neuron from the rest. NuCLR brings together views of the same neuron observed at different times and across different stimuli and uses a contrastive objective to pull these representations together. To capture population context without assuming any fixed neuron ordering, we build a spatiotemporal transformer that integrates activity in a permutation-equivariant manner. Across multiple electrophysiology and calcium imaging datasets, a linear decoding evaluation on top of NuCLR representations achieves a new state-of-the-art for both cell type and brain region decoding tasks, and demonstrates strong zero-shot generalization to unseen animals. We present the first systematic scaling analysis for neuron-level representation learning, showing that increasing the number of animals used during pretraining consistently improves downstream performance. The learned representations are also label-efficient, requiring only a small fraction of labeled samples to achieve competitive performance. These results highlight how large, diverse neural datasets enable models to recover information about neuron identity that generalize across animals. Code is available at this https URL.</li>
</ul>

<h3>Title: TabletopGen: Instance-Level Interactive 3D Tabletop Scene Generation from Text or Single Image</h3>
<ul>
<li><strong>Authors: </strong>Ziqian Wang, Yonghao He, Licheng Yang, Wei Zou, Hongxuan Ma, Liu Liu, Wei Sui, Yuxin Guo, Hu Su</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01204">https://arxiv.org/abs/2512.01204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01204">https://arxiv.org/pdf/2512.01204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01204]] TabletopGen: Instance-Level Interactive 3D Tabletop Scene Generation from Text or Single Image(https://arxiv.org/abs/2512.01204)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Generating high-fidelity, physically interactive 3D simulated tabletop scenes is essential for embodied AI--especially for robotic manipulation policy learning and data synthesis. However, current text- or image-driven 3D scene generation methods mainly focus on large-scale scenes, struggling to capture the high-density layouts and complex spatial relations that characterize tabletop scenes. To address these challenges, we propose TabletopGen, a training-free, fully automatic framework that generates diverse, instance-level interactive 3D tabletop scenes. TabletopGen accepts a reference image as input, which can be synthesized by a text-to-image model to enhance scene diversity. We then perform instance segmentation and completion on the reference to obtain per-instance images. Each instance is reconstructed into a 3D model followed by canonical coordinate alignment. The aligned 3D models then undergo pose and scale estimation before being assembled into a collision-free, simulation-ready tabletop scene. A key component of our framework is a novel pose and scale alignment approach that decouples the complex spatial reasoning into two stages: a Differentiable Rotation Optimizer for precise rotation recovery and a Top-view Spatial Alignment mechanism for robust translation and scale estimation, enabling accurate 3D reconstruction from 2D reference. Extensive experiments and user studies show that TabletopGen achieves state-of-the-art performance, markedly surpassing existing methods in visual fidelity, layout accuracy, and physical plausibility, capable of generating realistic tabletop scenes with rich stylistic and spatial diversity. Our code will be publicly available.</li>
</ul>

<h3>Title: Pay Attention Later: From Vector Space Diffusion to Linearithmic Spectral Phase-Locking</h3>
<ul>
<li><strong>Authors: </strong>Alper Yıldırım, İbrahim Yücedağ</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01208">https://arxiv.org/abs/2512.01208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01208">https://arxiv.org/pdf/2512.01208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01208]] Pay Attention Later: From Vector Space Diffusion to Linearithmic Spectral Phase-Locking(https://arxiv.org/abs/2512.01208)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Standard Transformers suffer from a "Semantic Alignment Tax", a prohibitive optimization cost required to organize a chaotic initialization into a coherent geometric map via local gradient diffusion. We hypothesize that this reliance on diffusive learning creates "Catastrophic Rigidity", rendering models unable to adapt to novel concepts without destroying their pre-trained reasoning capabilities. To isolate this phenomenon, we introduce Iterative Semantic Map Refinement (ISMR), a diagnostic protocol revealing that alignment is a fixed geometric barrier that scaling cannot solve; a 20-layer model overcomes this barrier no faster than a 1-layer model. We introduce the Phase-Resonant Intelligent Spectral Model (PRISM). PRISM encodes semantic identity as resonant frequencies in the complex domain (C^d) and replaces quadratic self-attention with linearithmic O(N log N) Gated Harmonic Convolutions. We validate PRISM on the WMT14 translation task. While the Standard Transformer maintains a slight edge in general competence on static benchmarks (23.88 vs 21.40 BLEU), it fails the "Plasticity-Stability" stress test completely. When injected with novel concepts, the Transformer suffers Catastrophic Forgetting, degrading by -10.55 BLEU points while achieving only 60% acquisition. In contrast, PRISM demonstrates Lossless Plasticity, achieving 96% 5-shot acquisition with negligible degradation (-0.84 BLEU). These results suggest that harmonic representations effectively decouple memory from reasoning, offering a structural solution to the plasticity-stability dilemma in real-time knowledge adaptation.</li>
</ul>

<h3>Title: A Comparative Study of Machine Learning Algorithms for Electricity Price Forecasting with LIME-Based Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Xuanyi Zhao, Jiawen Ding, Xueting Huang, Yibo Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01212">https://arxiv.org/abs/2512.01212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01212">https://arxiv.org/pdf/2512.01212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01212]] A Comparative Study of Machine Learning Algorithms for Electricity Price Forecasting with LIME-Based Interpretability(https://arxiv.org/abs/2512.01212)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>With the rapid development of electricity markets, price volatility has significantly increased, making accurate forecasting crucial for power system operations and market decisions. Traditional linear models cannot capture the complex nonlinear characteristics of electricity pricing, necessitating advanced machine learning approaches. This study compares eight machine learning models using Spanish electricity market data, integrating consumption, generation, and meteorological variables. The models evaluated include linear regression, ridge regression, decision tree, KNN, random forest, gradient boosting, SVR, and XGBoost. Results show that KNN achieves the best performance with R^2 of 0.865, MAE of 3.556, and RMSE of 5.240. To enhance interpretability, LIME analysis reveals that meteorological factors and supply-demand indicators significantly influence price fluctuations through nonlinear relationships. This work demonstrates the effectiveness of machine learning models in electricity price forecasting while improving decision transparency through interpretability analysis.</li>
</ul>

<h3>Title: M4-BLIP: Advancing Multi-Modal Media Manipulation Detection through Face-Enhanced Local Analysis</h3>
<ul>
<li><strong>Authors: </strong>Hang Wu, Ke Sun, Jiayi Ji, Xiaoshuai Sun, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01214">https://arxiv.org/abs/2512.01214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01214">https://arxiv.org/pdf/2512.01214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01214]] M4-BLIP: Advancing Multi-Modal Media Manipulation Detection through Face-Enhanced Local Analysis(https://arxiv.org/abs/2512.01214)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>In the contemporary digital landscape, multi-modal media manipulation has emerged as a significant societal threat, impacting the reliability and integrity of information dissemination. Current detection methodologies in this domain often overlook the crucial aspect of localized information, despite the fact that manipulations frequently occur in specific areas, particularly in facial regions. In response to this critical observation, we propose the M4-BLIP framework. This innovative framework utilizes the BLIP-2 model, renowned for its ability to extract local features, as the cornerstone for feature extraction. Complementing this, we incorporate local facial information as prior knowledge. A specially designed alignment and fusion module within M4-BLIP meticulously integrates these local and global features, creating a harmonious blend that enhances detection accuracy. Furthermore, our approach seamlessly integrates with Large Language Models (LLM), significantly improving the interpretability of the detection outcomes. Extensive quantitative and visualization experiments validate the effectiveness of our framework against the state-of-the-art competitors.</li>
</ul>

<h3>Title: S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance</h3>
<ul>
<li><strong>Authors: </strong>Beining Xu, Siting Zhu, Zhao Jin, Junxian Li, Hesheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01223">https://arxiv.org/abs/2512.01223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01223">https://arxiv.org/pdf/2512.01223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01223]] S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance(https://arxiv.org/abs/2512.01223)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.</li>
</ul>

<h3>Title: CoSineVerifier: Tool-Augmented Answer Verification for Computation-Oriented Scientific Questions</h3>
<ul>
<li><strong>Authors: </strong>Ruixiang Feng, Zhenwei An, Yuntao Wen, Ran Le, Yiming Jia, Chen Yang, Zongchao Chen, Lisi Chen, Shen Gao, Shuo Shang, Yang Song, Tao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01224">https://arxiv.org/abs/2512.01224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01224">https://arxiv.org/pdf/2512.01224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01224]] CoSineVerifier: Tool-Augmented Answer Verification for Computation-Oriented Scientific Questions(https://arxiv.org/abs/2512.01224)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Answer verification methods are widely employed in language model training pipelines spanning data curation, evaluation, and reinforcement learning with verifiable rewards (RLVR). While prior work focus on developing unified verifiers applicable across multiple reasoning scenarios, significant challenges remain in computation-oriented scientific domains, such as algebraic equivalence checking and physical constant substitution. In this paper, we introduce \model, a tool-augmented verifier that leverages external executors to perform precise computations and symbolic simplifications. \model enables robust verification that goes beyond simple semantic matching. We propose a novel two-stage pipeline, which begin with cold-start fine-tuning and followed by multi-turn reinforcement learning with tool integration. Extensive experiments conducted on STEM subjects, general QA, and long-form reasoning tasks demonstrates strong generalization of \model. The results shows that the \model achieves state-of-the-art performance on VerifyBench-Hard and SCI-Bench. And we also employ our \model in RLVR as a reward model, the results show that it consistently outperforms both rubric-based and model-based verifiers on AIME'24 and AIME'25, demonstrating strong potential to enhance reasoning capabilities of LLM. Our model is released at \hyperlink{this https URL}{this https URL}.</li>
</ul>

<h3>Title: On the Tension Between Optimality and Adversarial Robustness in Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Haoran Li, Jiayu Lv, Congying Han, Zicheng Zhang, Anqi Li, Yan Liu, Tiande Guo, Nan Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01228">https://arxiv.org/abs/2512.01228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01228">https://arxiv.org/pdf/2512.01228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01228]] On the Tension Between Optimality and Adversarial Robustness in Policy Optimization(https://arxiv.org/abs/2512.01228)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Achieving optimality and adversarial robustness in deep reinforcement learning has long been regarded as conflicting goals. Nonetheless, recent theoretical insights presented in CAR suggest a potential alignment, raising the important question of how to realize this in practice. This paper first identifies a key gap between theory and practice by comparing standard policy optimization (SPO) and adversarially robust policy optimization (ARPO). Although they share theoretical consistency, a fundamental tension between robustness and optimality arises in practical policy gradient methods. SPO tends toward convergence to vulnerable first-order stationary policies (FOSPs) with strong natural performance, whereas ARPO typically favors more robust FOSPs at the expense of reduced returns. Furthermore, we attribute this tradeoff to the reshaping effect of the strongest adversary in ARPO, which significantly complicates the global landscape by inducing deceptive sticky FOSPs. This improves robustness but makes navigation more challenging. To alleviate this, we develop the BARPO, a bilevel framework unifying SPO and ARPO by modulating adversary strength, thereby facilitating navigability while preserving global optima. Extensive empirical results demonstrate that BARPO consistently outperforms vanilla ARPO, providing a practical approach to reconcile theoretical and empirical performance.</li>
</ul>

<h3>Title: CTF Archive: Capture, Curate, Learn Forever</h3>
<ul>
<li><strong>Authors: </strong>Pratham Gupta, Aditya Gabani, Connor Nelson, Yan Shoshitaishvili</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01233">https://arxiv.org/abs/2512.01233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01233">https://arxiv.org/pdf/2512.01233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01233]] CTF Archive: Capture, Curate, Learn Forever(https://arxiv.org/abs/2512.01233)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Capture the Flag (CTF) competitions represent a powerful experiential learning approach within cybersecurity education, blending diverse concepts into interactive challenges. However, the short duration (typically 24-48 hours) and ephemeral infrastructure of these events often impede sustained educational benefit. Learners face substantial barriers in revisiting unsolved challenges, primarily due to the cumbersome process of manually reconstructing and rehosting the challenges without comprehensive documentation or guidance. To address this critical gap, we introduce CTF Archive, a platform designed to preserve the educational value of CTF competitions by centralizing and archiving hundreds of challenges spanning over a decade in fully configured, ready-to-use environments. By removing the complexity of environment setup, CTF Archive allows learners to focus directly on conceptual understanding rather than technical troubleshooting. The availability of these preserved challenges encourages in-depth research and exploration at the learner's pace, significantly enhancing conceptual comprehension without the pressures of live competition. Additionally, public accessibility lowers entry barriers, promoting an inclusive educational experience. Overall, CTF Archive provides a scalable solution to integrate persistent, practical cybersecurity learning into academic curricula.</li>
</ul>

<h3>Title: Generative Adversarial Gumbel MCTS for Abstract Visual Composition Generation</h3>
<ul>
<li><strong>Authors: </strong>Zirui Zhao, Boye Niu, David Hsu, Wee Sun Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01242">https://arxiv.org/abs/2512.01242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01242">https://arxiv.org/pdf/2512.01242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01242]] Generative Adversarial Gumbel MCTS for Abstract Visual Composition Generation(https://arxiv.org/abs/2512.01242)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We study abstract visual composition, in which identity is primarily determined by the spatial configuration and relations among a small set of geometric primitives (e.g., parts, symmetry, topology). They are invariant primarily to texture and photorealistic detail. Composing such structures from fixed components under geometric constraints and vague goal specification (such as text) is non-trivial due to combinatorial placement choices, limited data, and discrete feasibility (overlap-free, allowable orientations), which create a sparse solution manifold ill-suited to purely statistical pixel-space generators. We propose a constraint-guided framework that combines explicit geometric reasoning with neural semantics. An AlphaGo-style search enforces feasibility, while a fine-tuned vision-language model scores semantic alignment as reward signals. Our algorithm uses a policy network as a heuristic in Monte-Carlo Tree Search and fine-tunes the network via search-generated plans. Inspired by the Generative Adversarial Network, we use the generated instances for adversarial reward refinement. Over time, the generation should approach the actual data more closely when the reward model cannot distinguish between generated instances and ground-truth. In the Tangram Assembly task, our approach yields higher validity and semantic fidelity than diffusion and auto-regressive baselines, especially as constraints tighten.</li>
</ul>

<h3>Title: TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition</h3>
<ul>
<li><strong>Authors: </strong>Junyuan Zhang, Bin Wang, Qintong Zhang, Fan Wu, Zichen Wen, Jialin Lu, Junjie Shan, Ziqi Zhao, Shuya Yang, Ziling Wang, Ziyang Miao, Huaping Zhong, Yuhang Zang, Xiaoyi Dong, Ka-Ho Chow, Conghui He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01248">https://arxiv.org/abs/2512.01248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01248">https://arxiv.org/pdf/2512.01248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01248]] TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition(https://arxiv.org/abs/2512.01248)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Table recognition (TR) aims to transform table images into semi-structured representations such as HTML or Markdown. As a core component of document parsing, TR has long relied on supervised learning, with recent efforts dominated by fine-tuning vision-language models (VLMs) using labeled data. While VLMs have brought TR to the next level, pushing performance further demands large-scale labeled data that is costly to obtain. Consequently, although proprietary models have continuously pushed the performance boundary, open-source models, often trained with limited resources and, in practice, the only viable option for many due to privacy regulations, still lag far behind. To bridge this gap, we introduce TRivia, a self-supervised fine-tuning method that enables pretrained VLMs to learn TR directly from unlabeled table images in the wild. Built upon Group Relative Policy Optimization, TRivia automatically identifies unlabeled samples that most effectively facilitate learning and eliminates the need for human annotations through a question-answering-based reward mechanism. An attention-guided module generates diverse questions for each table image, and the ability to interpret the recognition results and answer them correctly provides feedback to optimize the TR model. This closed-loop process allows the TR model to autonomously learn to recognize, structure, and reason over tables without labeled data. Leveraging this pipeline, we present TRivia-3B, an open-sourced, compact, and state-of-the-art TR model that surpasses existing systems (e.g., Gemini 2.5 Pro, MinerU2.5) on three popular benchmarks. Model and code are released at: this https URL</li>
</ul>

<h3>Title: Efficient Training of Diffusion Mixture-of-Experts Models: A Practical Recipe</h3>
<ul>
<li><strong>Authors: </strong>Yahui Liu, Yang Yue, Jingyuan Zhang, Chenxi Sun, Yang Zhou, Wencong Zeng, Ruiming Tang, Guorui Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01252">https://arxiv.org/abs/2512.01252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01252">https://arxiv.org/pdf/2512.01252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01252]] Efficient Training of Diffusion Mixture-of-Experts Models: A Practical Recipe(https://arxiv.org/abs/2512.01252)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Recent efforts on Diffusion Mixture-of-Experts (MoE) models have primarily focused on developing more sophisticated routing mechanisms. However, we observe that the underlying architectural configuration space remains markedly under-explored. Inspired by the MoE design paradigms established in large language models (LLMs), we identify a set of crucial architectural factors for building effective Diffusion MoE models--including DeepSeek-style expert modules, alternative intermediate widths, varying expert counts, and enhanced attention positional encodings. Our systematic study reveals that carefully tuning these configurations is essential for unlocking the full potential of Diffusion MoE models, often yielding gains that exceed those achieved by routing innovations alone. Through extensive experiments, we present novel architectures that can be efficiently applied to both latent and pixel-space diffusion frameworks, which provide a practical and efficient training recipe that enables Diffusion MoE models to surpass strong baselines while using equal or fewer activated parameters. All code and models are publicly available at: this https URL.</li>
</ul>

<h3>Title: Large Language Models Cannot Reliably Detect Vulnerabilities in JavaScript: The First Systematic Benchmark and Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Qingyuan Fei, Xin Liu, Song Li, Shujiang Wu, Jianwei Hou, Ping Chen, Zifeng Kang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01255">https://arxiv.org/abs/2512.01255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01255">https://arxiv.org/pdf/2512.01255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01255]] Large Language Models Cannot Reliably Detect Vulnerabilities in JavaScript: The First Systematic Benchmark and Evaluation(https://arxiv.org/abs/2512.01255)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Researchers have proposed numerous methods to detect vulnerabilities in JavaScript, especially those assisted by Large Language Models (LLMs). However, the actual capability of LLMs in JavaScript vulnerability detection remains questionable, necessitating systematic evaluation and comprehensive benchmarks. Unfortunately, existing benchmarks suffer from three critical limitations: (1) incomplete coverage, such as covering a limited subset of CWE types; (2) underestimation of LLM capabilities caused by unreasonable ground truth labeling; and (3) overestimation due to unrealistic cases such as using isolated vulnerable files rather than complete projects. In this paper, we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection that directly address these limitations: (1) comprehensiveness, (2) no underestimation, and (3) no overestimation. Guided by these principles, we propose FORGEJS, the first automatic benchmark generation framework for evaluating LLMs' capability in JavaScript vulnerability detection. Then, we use FORGEJS to construct ARENAJS-the first systematic benchmark for LLM-based JavaScript vulnerability detection-and further propose JUDGEJS, an automatic evaluation framework. We conduct the first systematic evaluation of LLMs for JavaScript vulnerability detection, leveraging JUDGEJS to assess seven popular commercial LLMs on ARENAJS. The results show that LLMs not only exhibit limited reasoning capabilities, but also suffer from severe robustness defects, indicating that reliable JavaScript vulnerability detection with LLMs remains an open challenge.</li>
</ul>

<h3>Title: ViscNet: Vision-Based In-line Viscometry for Fluid Mixing Process</h3>
<ul>
<li><strong>Authors: </strong>Jongwon Sohn, Juhyeon Moon, Hyunjoon Jung, Jaewook Nam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01268">https://arxiv.org/abs/2512.01268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01268">https://arxiv.org/pdf/2512.01268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01268]] ViscNet: Vision-Based In-line Viscometry for Fluid Mixing Process(https://arxiv.org/abs/2512.01268)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Viscosity measurement is essential for process monitoring and autonomous laboratory operation, yet conventional viscometers remain invasive and require controlled laboratory environments that differ substantially from real process conditions. We present a computer-vision-based viscometer that infers viscosity by exploiting how a fixed background pattern becomes optically distorted as light refracts through the mixing-driven, continuously deforming free surface. Under diverse lighting conditions, the system achieves a mean absolute error of 0.113 in log m2 s^-1 units for regression and reaches up to 81% accuracy in viscosity-class prediction. Although performance declines for classes with closely clustered viscosity values, a multi-pattern strategy improves robustness by providing enriched visual cues. To ensure sensor reliability, we incorporate uncertainty quantification, enabling viscosity predictions with confidence estimates. This stand-off viscometer offers a practical, automation-ready alternative to existing viscometry methods.</li>
</ul>

<h3>Title: nnMobileNet++: Towards Efficient Hybrid Networks for Retinal Image Analysis</h3>
<ul>
<li><strong>Authors: </strong>Xin Li, Wenhui Zhu, Xuanzhao Dong, Hao Wang, Yujian Xiong, Oana Dumitrascu, Yalin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01273">https://arxiv.org/abs/2512.01273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01273">https://arxiv.org/pdf/2512.01273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01273]] nnMobileNet++: Towards Efficient Hybrid Networks for Retinal Image Analysis(https://arxiv.org/abs/2512.01273)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Retinal imaging is a critical, non-invasive modality for the early detection and monitoring of ocular and systemic diseases. Deep learning, particularly convolutional neural networks (CNNs), has significant progress in automated retinal analysis, supporting tasks such as fundus image classification, lesion detection, and vessel segmentation. As a representative lightweight network, nnMobileNet has demonstrated strong performance across multiple retinal benchmarks while remaining computationally efficient. However, purely convolutional architectures inherently struggle to capture long-range dependencies and model the irregular lesions and elongated vascular patterns that characterize on retinal images, despite the critical importance of vascular features for reliable clinical diagnosis. To further advance this line of work and extend the original vision of nnMobileNet, we propose nnMobileNet++, a hybrid architecture that progressively bridges convolutional and transformer representations. The framework integrates three key components: (i) dynamic snake convolution for boundary-aware feature extraction, (ii) stage-specific transformer blocks introduced after the second down-sampling stage for global context modeling, and (iii) retinal image pretraining to improve generalization. Experiments on multiple public retinal datasets for classification, together with ablation studies, demonstrate that nnMobileNet++ achieves state-of-the-art or highly competitive accuracy while maintaining low computational cost, underscoring its potential as a lightweight yet effective framework for retinal image analysis.</li>
</ul>

<h3>Title: SUPERChem: A Multimodal Reasoning Benchmark in Chemistry</h3>
<ul>
<li><strong>Authors: </strong>Zehua Zhao, Zhixian Huang, Junren Li, Siyu Lin, Junting Zhou, Fengqi Cao, Kun Zhou, Rui Ge, Tingting Long, Yuexiang Zhu, Yan Liu, Jie Zheng, Junnian Wei, Rong Zhu, Peng Zou, Wenyu Li, Zekai Cheng, Tian Ding, Yaxuan Wang, Yizhao Yan, Tingru Wei, Haowei Ming, Weijie Mao, Chen Sun, Yiming Liu, Zichen Wang, Zuo Zhang, Tong Yang, Hao Ma, Zhen Gao, Jian Pei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01274">https://arxiv.org/abs/2512.01274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01274">https://arxiv.org/pdf/2512.01274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01274]] SUPERChem: A Multimodal Reasoning Benchmark in Chemistry(https://arxiv.org/abs/2512.01274)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Current benchmarks for evaluating the chemical reasoning capabilities of Large Language Models (LLMs) are limited by oversimplified tasks, lack of process-level evaluation, and misalignment with expert-level chemistry skills. To address these issues, we introduce SUPERChem, a benchmark of 500 expert-curated reasoning-intensive chemistry problems, covering diverse subfields and provided in both multimodal and text-only formats. Original content and an iterative curation pipeline eliminate flawed items and mitigate data contamination. Each problem is paired with an expert-authored solution path, enabling Reasoning Path Fidelity (RPF) scoring to evaluate reasoning quality beyond final-answer accuracy. Evaluations against a human baseline of 40.3% accuracy show that even the best-performing model, GPT-5 (High), reaches only 38.5%, followed closely by Gemini 2.5 Pro (37.9%) and DeepSeek-V3.1-Think (37.3%). SUPERChem elicits multi-step, multimodal reasoning, reveals model-dependent effects of visual information, and distinguishes high-fidelity reasoners from heuristic ones. By providing a challenging benchmark and a reliable evaluation framework, SUPERChem aims to facilitate the advancement of LLMs toward expert-level chemical intelligence. The dataset of the benchmark is available at this https URL.</li>
</ul>

<h3>Title: Generative Modeling with Continuous Flows: Sample Complexity of Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Mudit Gaur, Prashant Trivedi, Shuchin Aeron, Amrit Singh Bedi, George K. Atia, Vaneet Aggarwal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01286">https://arxiv.org/abs/2512.01286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01286">https://arxiv.org/pdf/2512.01286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01286]] Generative Modeling with Continuous Flows: Sample Complexity of Flow Matching(https://arxiv.org/abs/2512.01286)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Flow matching has recently emerged as a promising alternative to diffusion-based generative models, offering faster sampling and simpler training by learning continuous flows governed by ordinary differential equations. Despite growing empirical success, the theoretical understanding of flow matching remains limited, particularly in terms of sample complexity results. In this work, we provide the first analysis of the sample complexity for flow-matching based generative models without assuming access to the empirical risk minimizer (ERM) of the loss function for estimating the velocity field. Under standard assumptions on the loss function for velocity field estimation and boundedness of the data distribution, we show that a sufficiently expressive neural network can learn a velocity field such that with $\mathcal{O}(\epsilon^{-4})$ samples, such that the Wasserstein-2 distance between the learned and the true distribution is less than $\mathcal{O}(\epsilon)$. The key technical idea is to decompose the velocity field estimation error into neural-network approximation error, statistical error due to the finite sample size, and optimization error due to the finite number of optimization steps for estimating the velocity field. Each of these terms are then handled via techniques that may be of independent interest.</li>
</ul>

<h3>Title: milearn: A Python Package for Multi-Instance Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Dmitry Zankov, Pavlo Polishchuk, Michal Sobieraj, Mario Barbatti</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01287">https://arxiv.org/abs/2512.01287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01287">https://arxiv.org/pdf/2512.01287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01287]] milearn: A Python Package for Multi-Instance Machine Learning(https://arxiv.org/abs/2512.01287)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce milearn, a Python package for multi-instance learning (MIL) that follows the familiar scikit-learn fit/predict interface while providing a unified framework for both classical and neural-network-based MIL algorithms for regression and classification. The package also includes built-in hyperparameter optimization designed specifically for small MIL datasets, enabling robust model selection in data-scarce scenarios. We demonstrate the versatility of milearn across a broad range of synthetic MIL benchmark datasets, including digit classification and regression, molecular property prediction, and protein-protein interaction (PPI) prediction. Special emphasis is placed on the key instance detection (KID) problem, for which the package provides dedicated support.</li>
</ul>

<h3>Title: Supervised Contrastive Machine Unlearning of Background Bias in Sonar Image Classification with Fine-Grained Explainable AI</h3>
<ul>
<li><strong>Authors: </strong>Kamal Basha S, Athira Nambiar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01291">https://arxiv.org/abs/2512.01291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01291">https://arxiv.org/pdf/2512.01291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01291]] Supervised Contrastive Machine Unlearning of Background Bias in Sonar Image Classification with Fine-Grained Explainable AI(https://arxiv.org/abs/2512.01291)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust, interpretability</a></li>
<li><strong>Abstract: </strong>Acoustic sonar image analysis plays a critical role in object detection and classification, with applications in both civilian and defense domains. Despite the availability of real and synthetic datasets, existing AI models that achieve high accuracy often over-rely on seafloor features, leading to poor generalization. To mitigate this issue, we propose a novel framework that integrates two key modules: (i) a Targeted Contrastive Unlearning (TCU) module, which extends the traditional triplet loss to reduce seafloor-induced background bias and improve generalization, and (ii) the Unlearn to Explain Sonar Framework (UESF), which provides visual insights into what the model has deliberately forgotten while adapting the LIME explainer to generate more faithful and localized attributions for unlearning evaluation. Extensive experiments across both real and synthetic sonar datasets validate our approach, demonstrating significant improvements in unlearning effectiveness, model robustness, and interpretability.</li>
</ul>

<h3>Title: Diffusion Model in Latent Space for Medical Image Segmentation Task</h3>
<ul>
<li><strong>Authors: </strong>Huynh Trinh Ngoc, Toan Nguyen Hai, Ba Luong Son, Long Tran Quoc</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01292">https://arxiv.org/abs/2512.01292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01292">https://arxiv.org/pdf/2512.01292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01292]] Diffusion Model in Latent Space for Medical Image Segmentation Task(https://arxiv.org/abs/2512.01292)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation is crucial for clinical diagnosis and treatment planning. Traditional methods typically produce a single segmentation mask, failing to capture inherent uncertainty. Recent generative models enable the creation of multiple plausible masks per image, mimicking the collaborative interpretation of several clinicians. However, these approaches remain computationally heavy. We propose MedSegLatDiff, a diffusion based framework that combines a variational autoencoder (VAE) with a latent diffusion model for efficient medical image segmentation. The VAE compresses the input into a low dimensional latent space, reducing noise and accelerating training, while the diffusion process operates directly in this compact representation. We further replace the conventional MSE loss with weighted cross entropy in the VAE mask reconstruction path to better preserve tiny structures such as small nodules. MedSegLatDiff is evaluated on ISIC-2018 (skin lesions), CVC-Clinic (polyps), and LIDC-IDRI (lung nodules). It achieves state of the art or highly competitive Dice and IoU scores while simultaneously generating diverse segmentation hypotheses and confidence maps. This provides enhanced interpretability and reliability compared to deterministic baselines, making the model particularly suitable for clinical deployment.</li>
</ul>

<h3>Title: Systems Security Foundations for Agentic Computing</h3>
<ul>
<li><strong>Authors: </strong>Mihai Christodorescu, Earlence Fernandes, Ashish Hooda, Somesh Jha, Johann Rehberger, Khawaja Shams</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01295">https://arxiv.org/abs/2512.01295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01295">https://arxiv.org/pdf/2512.01295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01295]] Systems Security Foundations for Agentic Computing(https://arxiv.org/abs/2512.01295)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack</a></li>
<li><strong>Abstract: </strong>This paper articulates short- and long-term research problems in AI agent security and privacy, using the lens of computer systems security. This approach examines end-to-end security properties of entire systems, rather than AI models in isolation. While we recognize that hardening a single model is useful, it is important to realize that it is often insufficient. By way of an analogy, creating a model that is always helpful and harmless is akin to creating software that is always helpful and harmless. The collective experience of decades of cybersecurity research and practice shows that this is insufficient. Rather, constructing an informed and realistic attacker model before building a system, applying hard-earned lessons from software security, and continuous improvement of security posture is a tried-and-tested approach to securing real computer systems. A key goal is to examine where research challenges arise when applying traditional security principles in the context of AI agents. A secondary goal of this report is to distill these ideas for AI and ML practitioners and researchers. We discuss the challenges of applying security principles to agentic computing, present 11 case studies of real attacks on agentic systems, and define a series of new research problems specific to the security of agentic systems.</li>
</ul>

<h3>Title: EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly</h3>
<ul>
<li><strong>Authors: </strong>Xiaokun Pan, Zhenzhe Li, Zhichao Ye, Hongjia Zhai, Guofeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01296">https://arxiv.org/abs/2512.01296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01296">https://arxiv.org/pdf/2512.01296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01296]] EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly(https://arxiv.org/abs/2512.01296)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Real-time 3D reconstruction is a fundamental task in computer graphics. Recently, differentiable-rendering-based SLAM system has demonstrated significant potential, enabling photorealistic scene rendering through learnable scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Current differentiable rendering methods face dual challenges in real-time computation and sensor noise sensitivity, leading to degraded geometric fidelity in scene reconstruction and limited practicality. To address these challenges, we propose a novel real-time system EGG-Fusion, featuring robust sparse-to-dense camera tracking and a geometry-aware Gaussian surfel mapping module, introducing an information filter-based fusion method that explicitly accounts for sensor noise to achieve high-precision surface reconstruction. The proposed differentiable Gaussian surfel mapping effectively models multi-view consistent surfaces while enabling efficient parameter optimization. Extensive experimental results demonstrate that the proposed system achieves a surface reconstruction error of 0.6\textit{cm} on standardized benchmark datasets including Replica and ScanNet++, representing over 20\% improvement in accuracy compared to state-of-the-art (SOTA) GS-based methods. Notably, the system maintains real-time processing capabilities at 24 FPS, establishing it as one of the most accurate differentiable-rendering-based real-time reconstruction systems. Project Page: this https URL</li>
</ul>

<h3>Title: TBT-Former: Learning Temporal Boundary Distributions for Action Localization</h3>
<ul>
<li><strong>Authors: </strong>Thisara Rathnayaka, Uthayasanker Thayasivam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01298">https://arxiv.org/abs/2512.01298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01298">https://arxiv.org/pdf/2512.01298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01298]] TBT-Former: Learning Temporal Boundary Distributions for Action Localization(https://arxiv.org/abs/2512.01298)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Temporal Action Localization (TAL) remains a fundamental challenge in video understanding, aiming to identify the start time, end time, and category of all action instances within untrimmed videos. While recent single-stage, anchor-free models like ActionFormer have set a high standard by leveraging Transformers for temporal reasoning, they often struggle with two persistent issues: the precise localization of actions with ambiguous or "fuzzy" temporal boundaries and the effective fusion of multi-scale contextual information. In this paper, we introduce the Temporal Boundary Transformer (TBT-Former), a new architecture that directly addresses these limitations. TBT-Former enhances the strong ActionFormer baseline with three core contributions: (1) a higher-capacity scaled Transformer backbone with an increased number of attention heads and an expanded Multi-Layer Perceptron (MLP) dimension for more powerful temporal feature extraction; (2) a cross-scale feature pyramid network (FPN) that integrates a top-down pathway with lateral connections, enabling richer fusion of high-level semantics and low-level temporal details; and (3) a novel boundary distribution regression head. Inspired by the principles of Generalized Focal Loss (GFL), this new head recasts the challenging task of boundary regression as a more flexible probability distribution learning problem, allowing the model to explicitly represent and reason about boundary uncertainty. Within the paradigm of Transformer-based architectures, TBT-Former advances the formidable benchmark set by its predecessors, establishing a new level of performance on the highly competitive THUMOS14 and EPIC-Kitchens 100 datasets, while remaining competitive on the large-scale ActivityNet-1.3. Our code is available at this https URL</li>
</ul>

<h3>Title: DCText: Scheduled Attention Masking for Visual Text Generation via Divide-and-Conquer Strategy</h3>
<ul>
<li><strong>Authors: </strong>Jaewoo Song, Jooyoung Choi, Kanghyun Baek, Sangyub Lee, Daemin Park, Sungroh Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01302">https://arxiv.org/abs/2512.01302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01302">https://arxiv.org/pdf/2512.01302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01302]] DCText: Scheduled Attention Masking for Visual Text Generation via Divide-and-Conquer Strategy(https://arxiv.org/abs/2512.01302)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Despite recent text-to-image models achieving highfidelity text rendering, they still struggle with long or multiple texts due to diluted global attention. We propose DCText, a training-free visual text generation method that adopts a divide-and-conquer strategy, leveraging the reliable short-text generation of Multi-Modal Diffusion Transformers. Our method first decomposes a prompt by extracting and dividing the target text, then assigns each to a designated region. To accurately render each segment within their regions while preserving overall image coherence, we introduce two attention masks - Text-Focus and Context-Expansion - applied sequentially during denoising. Additionally, Localized Noise Initialization further improves text accuracy and region alignment without increasing computational cost. Extensive experiments on single- and multisentence benchmarks show that DCText achieves the best text accuracy without compromising image quality while also delivering the lowest generation latency.</li>
</ul>

<h3>Title: IVCR-200K: A Large-Scale Multi-turn Dialogue Benchmark for Interactive Video Corpus Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Ning Han, Yawen Zeng, Shaohua Long, Chengqing Li, Sijie Yang, Dun Tan, Jianfeng Dong, Jingjing Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01312">https://arxiv.org/abs/2512.01312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01312">https://arxiv.org/pdf/2512.01312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01312]] IVCR-200K: A Large-Scale Multi-turn Dialogue Benchmark for Interactive Video Corpus Retrieval(https://arxiv.org/abs/2512.01312)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, significant developments have been made in both video retrieval and video moment retrieval tasks, which respectively retrieve complete videos or moments for a given text query. These advancements have greatly improved user satisfaction during the search process. However, previous work has failed to establish meaningful "interaction" between the retrieval system and the user, and its one-way retrieval paradigm can no longer fully meet the personalization and dynamic needs of at least 80.8\% of users. In this paper, we introduce the Interactive Video Corpus Retrieval (IVCR) task, a more realistic setting that enables multi-turn, conversational, and realistic interactions between the user and the retrieval system. To facilitate research on this challenging task, we introduce IVCR-200K, a high-quality, bilingual, multi-turn, conversational, and abstract semantic dataset that supports video retrieval and even moment retrieval. Furthermore, we propose a comprehensive framework based on multi-modal large language models (MLLMs) to help users interact in several modes with more explainable solutions. The extensive experiments demonstrate the effectiveness of our dataset and framework.</li>
</ul>

<h3>Title: TokenPure: Watermark Removal through Tokenized Appearance and Structural Guidance</h3>
<ul>
<li><strong>Authors: </strong>Pei Yang, Yepeng Liu, Kelly Peng, Yuan Gao, Yiren Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01314">https://arxiv.org/abs/2512.01314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01314">https://arxiv.org/pdf/2512.01314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01314]] TokenPure: Watermark Removal through Tokenized Appearance and Structural Guidance(https://arxiv.org/abs/2512.01314)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, watermark, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>In the digital economy era, digital watermarking serves as a critical basis for ownership proof of massive replicable content, including AI-generated and other virtual assets. Designing robust watermarks capable of withstanding various attacks and processing operations is even more paramount. We introduce TokenPure, a novel Diffusion Transformer-based framework designed for effective and consistent watermark removal. TokenPure solves the trade-off between thorough watermark destruction and content consistency by leveraging token-based conditional reconstruction. It reframes the task as conditional generation, entirely bypassing the initial watermark-carrying noise. We achieve this by decomposing the watermarked image into two complementary token sets: visual tokens for texture and structural tokens for geometry. These tokens jointly condition the diffusion process, enabling the framework to synthesize watermark-free images with fine-grained consistency and structural integrity. Comprehensive experiments show that TokenPure achieves state-of-the-art watermark removal and reconstruction fidelity, substantially outperforming existing baselines in both perceptual quality and consistency.</li>
</ul>

<h3>Title: Rethinking Intracranial Aneurysm Vessel Segmentation: A Perspective from Computational Fluid Dynamics Applications</h3>
<ul>
<li><strong>Authors: </strong>Feiyang Xiao, Yichi Zhang, Xigui Li, Yuanye Zhou, Chen Jiang, Xin Guo, Limei Han, Yuxin Li, Fengping Zhu, Yuan Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01319">https://arxiv.org/abs/2512.01319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01319">https://arxiv.org/pdf/2512.01319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01319]] Rethinking Intracranial Aneurysm Vessel Segmentation: A Perspective from Computational Fluid Dynamics Applications(https://arxiv.org/abs/2512.01319)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The precise segmentation of intracranial aneurysms and their parent vessels (IA-Vessel) is a critical step for hemodynamic analyses, which mainly depends on computational fluid dynamics (CFD). However, current segmentation methods predominantly focus on image-based evaluation metrics, often neglecting their practical effectiveness in subsequent CFD applications. To address this deficiency, we present the Intracranial Aneurysm Vessel Segmentation (IAVS) dataset, the first comprehensive, multi-center collection comprising 641 3D MRA images with 587 annotations of aneurysms and IA-Vessels. In addition to image-mask pairs, IAVS dataset includes detailed hemodynamic analysis outcomes, addressing the limitations of existing datasets that neglect topological integrity and CFD applicability. To facilitate the development and evaluation of clinically relevant techniques, we construct two evaluation benchmarks including global localization of aneurysms (Stage I) and fine-grained segmentation of IA-Vessel (Stage II) and develop a simple and effective two-stage framework, which can be used as a out-of-the-box method and strong baseline. For comprehensive evaluation of applicability of segmentation results, we establish a standardized CFD applicability evaluation system that enables the automated and consistent conversion of segmentation masks into CFD models, offering an applicability-focused assessment of segmentation outcomes. The dataset, code, and model will be public available at this https URL.</li>
</ul>

<h3>Title: Securing Large Language Models (LLMs) from Prompt Injection Attacks</h3>
<ul>
<li><strong>Authors: </strong>Omar Farooq Khan Suri, John McCrae</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01326">https://arxiv.org/abs/2512.01326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01326">https://arxiv.org/pdf/2512.01326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01326]] Securing Large Language Models (LLMs) from Prompt Injection Attacks(https://arxiv.org/abs/2512.01326)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly being deployed in real-world applications, but their flexibility exposes them to prompt injection attacks. These attacks leverage the model's instruction-following ability to make it perform malicious tasks. Recent work has proposed JATMO, a task-specific fine-tuning approach that trains non-instruction-tuned base models to perform a single function, thereby reducing susceptibility to adversarial instructions. In this study, we evaluate the robustness of JATMO against HOUYI, a genetic attack framework that systematically mutates and optimizes adversarial prompts. We adapt HOUYI by introducing custom fitness scoring, modified mutation logic, and a new harness for local model testing, enabling a more accurate assessment of defense effectiveness. We fine-tuned LLaMA 2-7B, Qwen1.5-4B, and Qwen1.5-0.5B models under the JATMO methodology and compared them with a fine-tuned GPT-3.5-Turbo baseline. Results show that while JATMO reduces attack success rates relative to instruction-tuned models, it does not fully prevent injections; adversaries exploiting multilingual cues or code-related disruptors still bypass defenses. We also observe a trade-off between generation quality and injection vulnerability, suggesting that better task performance often correlates with increased susceptibility. Our results highlight both the promise and limitations of fine-tuning-based defenses and point toward the need for layered, adversarially informed mitigation strategies.</li>
</ul>

<h3>Title: Optimizing Stroke Risk Prediction: A Machine Learning Pipeline Combining ROS-Balanced Ensembles and XAI</h3>
<ul>
<li><strong>Authors: </strong>A S M Ahsanul Sarkar Akib, Raduana Khawla, Abdul Hasib</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01333">https://arxiv.org/abs/2512.01333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01333">https://arxiv.org/pdf/2512.01333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01333]] Optimizing Stroke Risk Prediction: A Machine Learning Pipeline Combining ROS-Balanced Ensembles and XAI(https://arxiv.org/abs/2512.01333)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Stroke is a major cause of death and permanent impairment, making it a major worldwide health concern. For prompt intervention and successful preventative tactics, early risk assessment is essential. To address this challenge, we used ensemble modeling and explainable AI (XAI) techniques to create an interpretable machine learning framework for stroke risk prediction. A thorough evaluation of 10 different machine learning models using 5-fold cross-validation across several datasets was part of our all-inclusive strategy, which also included feature engineering and data pretreatment (using Random Over-Sampling (ROS) to solve class imbalance). Our optimized ensemble model (Random Forest + ExtraTrees + XGBoost) performed exceptionally well, obtaining a strong 99.09% accuracy on the Stroke Prediction Dataset (SPD). We improved the model's transparency and clinical applicability by identifying three important clinical variables using LIME-based interpretability analysis: age, hypertension, and glucose levels. Through early prediction, this study highlights how combining ensemble learning with explainable AI (XAI) can deliver highly accurate and interpretable stroke risk assessment. By enabling data-driven prevention and personalized clinical decisions, our framework has the potential to transform stroke prediction and cardiovascular risk management.</li>
</ul>

<h3>Title: AlignVid: Training-Free Attention Scaling for Semantic Fidelity in Text-Guided Image-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yexin Liu, Wen-Jie Shu, Zile Huang, Haoze Zheng, Yueze Wang, Manyuan Zhang, Ser-Nam Lim, Harry Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01334">https://arxiv.org/abs/2512.01334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01334">https://arxiv.org/pdf/2512.01334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01334]] AlignVid: Training-Free Attention Scaling for Semantic Fidelity in Text-Guided Image-to-Video Generation(https://arxiv.org/abs/2512.01334)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Text-guided image-to-video (TI2V) generation has recently achieved remarkable progress, particularly in maintaining subject consistency and temporal coherence. However, existing methods still struggle to adhere to fine-grained prompt semantics, especially when prompts entail substantial transformations of the input image (e.g., object addition, deletion, or modification), a shortcoming we term semantic negligence. In a pilot study, we find that applying a Gaussian blur to the input image improves semantic adherence. Analyzing attention maps, we observe clearer foreground-background separation. From an energy perspective, this corresponds to a lower-entropy cross-attention distribution. Motivated by this, we introduce AlignVid, a training-free framework with two components: (i) Attention Scaling Modulation (ASM), which directly reweights attention via lightweight Q or K scaling, and (ii) Guidance Scheduling (GS), which applies ASM selectively across transformer blocks and denoising steps to reduce visual quality degradation. This minimal intervention improves prompt adherence while limiting aesthetic degradation. In addition, we introduce OmitI2V to evaluate semantic negligence in TI2V generation, comprising 367 human-annotated samples that span addition, deletion, and modification scenarios. Extensive experiments demonstrate that AlignVid can enhance semantic fidelity.</li>
</ul>

<h3>Title: EmoRAG: Evaluating RAG Robustness to Symbolic Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Xinyun Zhou, Xinfeng Li, Yinan Peng, Ming Xu, Xuanwang Zhang, Miao Yu, Yidong Wang, Xiaojun Jia, Kun Wang, Qingsong Wen, XiaoFeng Wang, Wei Dong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01335">https://arxiv.org/abs/2512.01335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01335">https://arxiv.org/pdf/2512.01335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01335]] EmoRAG: Evaluating RAG Robustness to Symbolic Perturbations(https://arxiv.org/abs/2512.01335)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) systems are increasingly central to robust AI, enhancing large language model (LLM) faithfulness by incorporating external knowledge. However, our study unveils a critical, overlooked vulnerability: their profound susceptibility to subtle symbolic perturbations, particularly through near-imperceptible emoticon tokens such as "(@_@)" that can catastrophically mislead retrieval, termed EmoRAG. We demonstrate that injecting a single emoticon into a query makes it nearly 100% likely to retrieve semantically unrelated texts that contain a matching emoticon. Our extensive experiment across general question-answering and code domains, using a range of state-of-the-art retrievers and generators, reveals three key findings: (I) Single-Emoticon Disaster: Minimal emoticon injections cause maximal disruptions, with a single emoticon almost 100% dominating RAG output. (II) Positional Sensitivity: Placing an emoticon at the beginning of a query can cause severe perturbation, with F1-Scores exceeding 0.92 across all datasets. (III) Parameter-Scale Vulnerability: Counterintuitively, models with larger parameters exhibit greater vulnerability to the interference. We provide an in-depth analysis to uncover the underlying mechanisms of these phenomena. Furthermore, we raise a critical concern regarding the robustness assumption of current RAG systems, envisioning a threat scenario where an adversary exploits this vulnerability to manipulate the RAG system. We evaluate standard defenses and find them insufficient against EmoRAG. To address this, we propose targeted defenses, analyzing their strengths and limitations in mitigating emoticon-based perturbations. Finally, we outline future directions for building robust RAG systems.</li>
</ul>

<h3>Title: EvalTalker: Learning to Evaluate Real-Portrait-Driven Multi-Subject Talking Humans</h3>
<ul>
<li><strong>Authors: </strong>Yingjie Zhou, Xilei Zhu, Siyu Ren, Ziyi Zhao, Ziwen Wang, Farong Wen, Yu Zhou, Jiezhang Cao, Xiongkuo Min, Fengjiao Chen, Xiaoyu Li, Xuezhi Cao, Guangtao Zhai, Xiaohong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01340">https://arxiv.org/abs/2512.01340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01340">https://arxiv.org/pdf/2512.01340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01340]] EvalTalker: Learning to Evaluate Real-Portrait-Driven Multi-Subject Talking Humans(https://arxiv.org/abs/2512.01340)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Speech-driven Talking Human (TH) generation, commonly known as "Talker," currently faces limitations in multi-subject driving capabilities. Extending this paradigm to "Multi-Talker," capable of animating multiple subjects simultaneously, introduces richer interactivity and stronger immersion in audiovisual communication. However, current Multi-Talkers still exhibit noticeable quality degradation caused by technical limitations, resulting in suboptimal user experiences. To address this challenge, we construct THQA-MT, the first large-scale Multi-Talker-generated Talking Human Quality Assessment dataset, consisting of 5,492 Multi-Talker-generated THs (MTHs) from 15 representative Multi-Talkers using 400 real portraits collected online. Through subjective experiments, we analyze perceptual discrepancies among different Multi-Talkers and identify 12 common types of distortion. Furthermore, we introduce EvalTalker, a novel TH quality assessment framework. This framework possesses the ability to perceive global quality, human characteristics, and identity consistency, while integrating Qwen-Sync to perceive multimodal synchrony. Experimental results demonstrate that EvalTalker achieves superior correlation with subjective scores, providing a robust foundation for future research on high-quality Multi-Talker generation and evaluation.</li>
</ul>

<h3>Title: InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision</h3>
<ul>
<li><strong>Authors: </strong>Chenting Wang, Yuhan Zhu, Yicheng Xu, Jiange Yang, Ziang Yan, Yali Wang, Yi Wang, Limin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01342">https://arxiv.org/abs/2512.01342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01342">https://arxiv.org/pdf/2512.01342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01342]] InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision(https://arxiv.org/abs/2512.01342)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Large-scale video-text pretraining achieves strong performance but depends on noisy, synthetic captions with limited semantic coverage, often overlooking implicit world knowledge such as object motion, 3D geometry, and physical cues. In contrast, masked video modeling (MVM) directly exploits spatiotemporal structures but trails text-supervised methods on general tasks. We find this gap arises from overlooked architectural issues: pixel-level reconstruction struggles with convergence and its low-level requirement often conflicts with semantics, while latent prediction often encourages shortcut learning. To address these, we disentangle the traditional encoder-decoder design into an Encoder-Predictor-Decoder (EPD) framework, where the predictor acts as a latent world model, and propose InternVideo-Next, a two-stage pretraining scheme that builds a semantically consistent yet detail-preserving latent space for this world model. First, conventional linear decoder in pixel MVM enforces the predictor output latent to be linearly projected to, thus separable in pixel space, causing the conflict with semantic abstraction. Our Stage 1 proposes a conditional diffusion decoder and injects reliable image-level semantic priors to enhance semantics and convergence, thus bridging pixel-level fidelity with high-level semantic abstraction. Stage 2 further learns world knowledge by predicting frozen Stage 1 targets within this space, mitigating shortcut learning. Trained on public, unlabeled videos, InternVideo-Next achieves state-of-the-art results across benchmarks and provides a scalable path toward general video representation learning.</li>
</ul>

<h3>Title: Intrinsic Structure as a Proxy for Saliency: SVD-Based Weight Preservation for Mixed-Precision Quantization in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shashank Landge, Abhishek Patil, Tejas kamble, Bhushan Buddhivant, Priyanka Joshi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01343">https://arxiv.org/abs/2512.01343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01343">https://arxiv.org/pdf/2512.01343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01343]] Intrinsic Structure as a Proxy for Saliency: SVD-Based Weight Preservation for Mixed-Precision Quantization in Large Language Models(https://arxiv.org/abs/2512.01343)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, data-free, large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) continue to scale in parameter count, deploying them on commodity hardware has become increasingly challenging. Post-Training Quantization (PTQ) addresses this by reducing the precision of model weights, typically to 4-bit or lower. However, uniform quantization often leads to significant performance degradation due to the presence of ``outlier features'' -- weights that, while few in number, are critical for maintaining model accuracy. Current state-of-the-art methods such as AWQ (Activation-aware Weight Quantization) and SpQR (Sparse Quantization Representations) rely on calibration data to identify these salient weights via activation magnitudes or Hessian sensitivity. In scenarios where data privacy is paramount or calibration data is unavailable, these methods are inapplicable. In this work, we propose a data-free, structure-aware hypothesis: that the weights identified as Principal Components via Singular Value Decomposition (SVD) are intrinsically important to the model's downstream performance. We introduce a novel selection heuristic that preserves the top-$k$ weights aligned with the principal components in FP32, while aggressively quantizing the residual weights. We compare our method against activation-aware (AWQ) and second-order (SpQR) methods across GLUE benchmarks (MRPC, RTE, QNLI) using a DistilBERT backbone. Our experiments reveal that structural importance is highly correlated with functional importance. On the challenging RTE task, our SVD-based method achieves an accuracy of 66.06\%, outperforming both AWQ (65.34\%) and SpQR (65.34\%) at high protection budgets, validating that intrinsic matrix structure can serve as a robust proxy for weight saliency without the need for forward passes or calibration data.</li>
</ul>

<h3>Title: Handwritten Text Recognition for Low Resource Languages</h3>
<ul>
<li><strong>Authors: </strong>Sayantan Dey, Alireza Alaei, Partha Pratim Roy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01348">https://arxiv.org/abs/2512.01348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01348">https://arxiv.org/pdf/2512.01348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01348]] Handwritten Text Recognition for Low Resource Languages(https://arxiv.org/abs/2512.01348)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Despite considerable progress in handwritten text recognition, paragraph-level handwritten text recognition, especially in low-resource languages, such as Hindi, Urdu and similar scripts, remains a challenging problem. These languages, often lacking comprehensive linguistic resources, require special attention to develop robust systems for accurate optical character recognition (OCR). This paper introduces BharatOCR, a novel segmentation-free paragraph-level handwritten Hindi and Urdu text recognition. We propose a ViT-Transformer Decoder-LM architecture for handwritten text recognition, where a Vision Transformer (ViT) extracts visual features, a Transformer decoder generates text sequences, and a pre-trained language model (LM) refines the output to improve accuracy, fluency, and coherence. Our model utilizes a Data-efficient Image Transformer (DeiT) model proposed for masked image modeling in this research work. In addition, we adopt a RoBERTa architecture optimized for masked language modeling (MLM) to enhance the linguistic comprehension and generative capabilities of the proposed model. The transformer decoder generates text sequences from visual embeddings. This model is designed to iteratively process a paragraph image line by line, called implicit line segmentation. The proposed model was evaluated using our custom dataset ('Parimal Urdu') and ('Parimal Hindi'), introduced in this research work, as well as two public datasets. The proposed model achieved benchmark results in the NUST-UHWR, PUCIT-OUHL, and Parimal-Urdu datasets, achieving character recognition rates of 96.24%, 92.05%, and 94.80%, respectively. The model also provided benchmark results using the Hindi dataset achieving a character recognition rate of 80.64%. The results obtained from our proposed model indicated that it outperformed several state-of-the-art Urdu text recognition methods.</li>
</ul>

<h3>Title: A Wolf in Sheep's Clothing: Bypassing Commercial LLM Guardrails via Harmless Prompt Weaving and Adaptive Tree Search</h3>
<ul>
<li><strong>Authors: </strong>Rongzhe Wei, Peizhi Niu, Xinjie Shen, Tony Tu, Yifan Li, Ruihan Wu, Eli Chien, Olgica Milenkovic, Pan Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01353">https://arxiv.org/abs/2512.01353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01353">https://arxiv.org/pdf/2512.01353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01353]] A Wolf in Sheep's Clothing: Bypassing Commercial LLM Guardrails via Harmless Prompt Weaving and Adaptive Tree Search(https://arxiv.org/abs/2512.01353)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) remain vulnerable to jailbreak attacks that bypass safety guardrails to elicit harmful outputs. Existing approaches overwhelmingly operate within the prompt-optimization paradigm: whether through traditional algorithmic search or recent agent-based workflows, the resulting prompts typically retain malicious semantic signals that modern guardrails are primed to detect. In contrast, we identify a deeper, largely overlooked vulnerability stemming from the highly interconnected nature of an LLM's internal knowledge. This structure allows harmful objectives to be realized by weaving together sequences of benign sub-queries, each of which individually evades detection. To exploit this loophole, we introduce the Correlated Knowledge Attack Agent (CKA-Agent), a dynamic framework that reframes jailbreaking as an adaptive, tree-structured exploration of the target model's knowledge base. The CKA-Agent issues locally innocuous queries, uses model responses to guide exploration across multiple paths, and ultimately assembles the aggregated information to achieve the original harmful objective. Evaluated across state-of-the-art commercial LLMs (Gemini2.5-Flash/Pro, GPT-oss-120B, Claude-Haiku-4.5), CKA-Agent consistently achieves over 95% success rates even against strong guardrails, underscoring the severity of this vulnerability and the urgent need for defenses against such knowledge-decomposition attacks. Our codes are available at this https URL.</li>
</ul>

<h3>Title: A Fine Evaluation Method for Cube Copying Test for Early Detection of Alzheimer's Disease</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Jiang, Cuiyun Gao, Wenda Huang, Yiyang Jiang, Binwen Luo, Yuxin Jiang, Mengting Wang, Haoran Wen, Yang Zhao, Xuemei Chen, Songqun Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01367">https://arxiv.org/abs/2512.01367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01367">https://arxiv.org/pdf/2512.01367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01367]] A Fine Evaluation Method for Cube Copying Test for Early Detection of Alzheimer's Disease(https://arxiv.org/abs/2512.01367)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Background: Impairment of visual spatial cognitive function is the most common early clinical manifestation of Alzheimer's Disease (AD). When the Montreal Cognitive Assessment (MoCA) uses the "0/1" binary method ("pass/fail") to evaluate the visual spatial cognitive ability represented by the Cube Copying Test(CCT), the elder with less formal education generally score 0 point, resulting in serious bias in the evaluation results. Therefore, this study proposes a fine evaluation method for CCT based on dynamic handwriting feature extraction of DH-SCSM-BLA. method : The Cogni-CareV3.0 software independently developed by our team was used to collect dynamic handwriting data of CCT. Then, the spatial and motion features of segmented dynamic handwriting were extracted, and feature matrix with unequal dimensions were normalized. Finally, a bidirectional long short-term memory network model combined with attention mechanism (BiLSTM-Attention) was adopted for classification. Result: The experimental results showed that: The proposed method has significant superiority compared to similar studies, with a classification accuracy of 86.69%. The distribution of cube drawing ability scores has significant regularity for three aspects such as MCI patients and healthy control group, age, and levels of education. It was also found that score for each cognitive task including cube drawing ability score is negatively correlated with age. Score for each cognitive task including cube drawing ability score, but positively correlated with levels of education significantly. Conclusion: This study provides a relatively objective and comprehensive evaluation method for early screening and personalized intervention of visual spatial cognitive impairment.</li>
</ul>

<h3>Title: MARSAD: A Multi-Functional Tool for Real-Time Social Media Analysis</h3>
<ul>
<li><strong>Authors: </strong>Md. Rafiul Biswas, Firoj Alam, Wajdi Zaghouani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01369">https://arxiv.org/abs/2512.01369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01369">https://arxiv.org/pdf/2512.01369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01369]] MARSAD: A Multi-Functional Tool for Real-Time Social Media Analysis(https://arxiv.org/abs/2512.01369)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>MARSAD is a multifunctional natural language processing (NLP) platform designed for real-time social media monitoring and analysis, with a particular focus on the Arabic-speaking world. It enables researchers and non-technical users alike to examine both live and archived social media content, producing detailed visualizations and reports across various dimensions, including sentiment analysis, emotion analysis, propaganda detection, fact-checking, and hate speech detection. The platform also provides secure data-scraping capabilities through API keys for accessing public social media data. MARSAD's backend architecture integrates flexible document storage with structured data management, ensuring efficient processing of large and multimodal datasets. Its user-friendly frontend supports seamless data upload and interaction.</li>
</ul>

<h3>Title: Beyond Loss Guidance: Using PDE Residuals as Spectral Attention in Diffusion Neural Operators</h3>
<ul>
<li><strong>Authors: </strong>Medha Sawhney, Abhilash Neog, Mridul Khurana, Anuj Karpatne</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.NA, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01370">https://arxiv.org/abs/2512.01370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01370">https://arxiv.org/pdf/2512.01370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01370]] Beyond Loss Guidance: Using PDE Residuals as Spectral Attention in Diffusion Neural Operators(https://arxiv.org/abs/2512.01370)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based solvers for partial differential equations (PDEs) are often bottle-necked by slow gradient-based test-time optimization routines that use PDE residuals for loss guidance. They additionally suffer from optimization instabilities and are unable to dynamically adapt their inference scheme in the presence of noisy PDE residuals. To address these limitations, we introduce PRISMA (PDE Residual Informed Spectral Modulation with Attention), a conditional diffusion neural operator that embeds PDE residuals directly into the model's architecture via attention mechanisms in the spectral domain, enabling gradient-descent free inference. In contrast to previous methods that use PDE loss solely as external optimization targets, PRISMA integrates PDE residuals as integral architectural features, making it inherently fast, robust, accurate, and free from sensitive hyperparameter tuning. We show that PRISMA has competitive accuracy, at substantially lower inference costs, compared to previous methods across five benchmark PDEs, especially with noisy observations, while using 10x to 100x fewer denoising steps, leading to 15x to 250x faster inference.</li>
</ul>

<h3>Title: SRAM: Shape-Realism Alignment Metric for No Reference 3D Shape Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Sheng Liu, Tianyu Luan, Phani Nuney, Xuelu Feng, Junsong Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01373">https://arxiv.org/abs/2512.01373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01373">https://arxiv.org/pdf/2512.01373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01373]] SRAM: Shape-Realism Alignment Metric for No Reference 3D Shape Evaluation(https://arxiv.org/abs/2512.01373)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>3D generation and reconstruction techniques have been widely used in computer games, film, and other content creation areas. As the application grows, there is a growing demand for 3D shapes that look truly realistic. Traditional evaluation methods rely on a ground truth to measure mesh fidelity. However, in many practical cases, a shape's realism does not depend on having a ground truth reference. In this work, we propose a Shape-Realism Alignment Metric that leverages a large language model (LLM) as a bridge between mesh shape information and realism evaluation. To achieve this, we adopt a mesh encoding approach that converts 3D shapes into the language token space. A dedicated realism decoder is designed to align the language model's output with human perception of realism. Additionally, we introduce a new dataset, RealismGrading, which provides human-annotated realism scores without the need for ground truth shapes. Our dataset includes shapes generated by 16 different algorithms on over a dozen objects, making it more representative of practical 3D shape distributions. We validate our metric's performance and generalizability through k-fold cross-validation across different objects. Experimental results show that our metric correlates well with human perceptions and outperforms existing methods, and has good generalizability.</li>
</ul>

<h3>Title: Stabilizing Reinforcement Learning with LLMs: Formulation and Practices</h3>
<ul>
<li><strong>Authors: </strong>Chujie Zheng, Kai Dang, Bowen Yu, Mingze Li, Huiqiang Jiang, Junrong Lin, Yuqiong Liu, An Yang, Jingren Zhou, Junyang Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01374">https://arxiv.org/abs/2512.01374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01374">https://arxiv.org/pdf/2512.01374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01374]] Stabilizing Reinforcement Learning with LLMs: Formulation and Practices(https://arxiv.org/abs/2512.01374)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.</li>
</ul>

<h3>Title: PointNet4D: A Lightweight 4D Point Cloud Video Backbone for Online and Offline Perception in Robotic Applications</h3>
<ul>
<li><strong>Authors: </strong>Yunze Liu, Zifan Wang, Peiran Wu, Jiayang Ao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01383">https://arxiv.org/abs/2512.01383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01383">https://arxiv.org/pdf/2512.01383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01383]] PointNet4D: A Lightweight 4D Point Cloud Video Backbone for Online and Offline Perception in Robotic Applications(https://arxiv.org/abs/2512.01383)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Understanding dynamic 4D environments-3D space evolving over time-is critical for robotic and interactive systems. These applications demand systems that can process streaming point cloud video in real-time, often under resource constraints, while also benefiting from past and present observations when available. However, current 4D backbone networks rely heavily on spatiotemporal convolutions and Transformers, which are often computationally intensive and poorly suited to real-time applications. We propose PointNet4D, a lightweight 4D backbone optimized for both online and offline settings. At its core is a Hybrid Mamba-Transformer temporal fusion block, which integrates the efficient state-space modeling of Mamba and the bidirectional modeling power of Transformers. This enables PointNet4D to handle variable-length online sequences efficiently across different deployment scenarios. To enhance temporal understanding, we introduce 4DMAP, a frame-wise masked auto-regressive pretraining strategy that captures motion cues across frames. Our extensive evaluations across 9 tasks on 7 datasets, demonstrating consistent improvements across diverse domains. We further demonstrate PointNet4D's utility by building two robotic application systems: 4D Diffusion Policy and 4D Imitation Learning, achieving substantial gains on the RoboTwin and HandoverSim benchmarks.</li>
</ul>

<h3>Title: Consistency Flow Model Achieves One-step Denoising Error Correction Codes</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Lei, Chin Wa Lau, Kaiwen Zhou, Nian Guo, Farzan Farnia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01389">https://arxiv.org/abs/2512.01389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01389">https://arxiv.org/pdf/2512.01389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01389]] Consistency Flow Model Achieves One-step Denoising Error Correction Codes(https://arxiv.org/abs/2512.01389)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Error Correction Codes (ECC) are fundamental to reliable digital communication, yet designing neural decoders that are both accurate and computationally efficient remains challenging. Recent denoising diffusion decoders with transformer backbones achieve state-of-the-art performance, but their iterative sampling limits practicality in low-latency settings. We introduce the Error Correction Consistency Flow Model (ECCFM), an architecture-agnostic training framework for high-fidelity one-step decoding. By casting the reverse denoising process as a Probability Flow Ordinary Differential Equation (PF-ODE) and enforcing smoothness through a differential time regularization, ECCFM learns to map noisy signals along the decoding trajectory directly to the original codeword in a single inference step. Across multiple decoding benchmarks, ECCFM attains lower bit-error rates (BER) than autoregressive and diffusion-based baselines, with notable improvements on longer codes, while delivering inference speeds up from 30x to 100x faster than denoising diffusion decoders.</li>
</ul>

<h3>Title: FRAMER: Frequency-Aligned Self-Distillation with Adaptive Modulation Leveraging Diffusion Priors for Real-World Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Seungho Choi, Jeahun Sung, Jihyong Oh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01390">https://arxiv.org/abs/2512.01390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01390">https://arxiv.org/pdf/2512.01390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01390]] FRAMER: Frequency-Aligned Self-Distillation with Adaptive Modulation Leveraging Diffusion Priors for Real-World Image Super-Resolution(https://arxiv.org/abs/2512.01390)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Real-image super-resolution (Real-ISR) seeks to recover HR images from LR inputs with mixed, unknown degradations. While diffusion models surpass GANs in perceptual quality, they under-reconstruct high-frequency (HF) details due to a low-frequency (LF) bias and a depth-wise "low-first, high-later" hierarchy. We introduce FRAMER, a plug-and-play training scheme that exploits diffusion priors without changing the backbone or inference. At each denoising step, the final-layer feature map teaches all intermediate layers. Teacher and student feature maps are decomposed into LF/HF bands via FFT masks to align supervision with the model's internal frequency hierarchy. For LF, an Intra Contrastive Loss (IntraCL) stabilizes globally shared structure. For HF, an Inter Contrastive Loss (InterCL) sharpens instance-specific details using random-layer and in-batch negatives. Two adaptive modulators, Frequency-based Adaptive Weight (FAW) and Frequency-based Alignment Modulation (FAM), reweight per-layer LF/HF signals and gate distillation by current similarity. Across U-Net and DiT backbones (e.g., Stable Diffusion 2, 3), FRAMER consistently improves PSNR/SSIM and perceptual metrics (LPIPS, NIQE, MANIQA, MUSIQ). Ablations validate the final-layer teacher and random-layer negatives.</li>
</ul>

<h3>Title: INFERMAL: Inferential analysis of maliciously registered domains</h3>
<ul>
<li><strong>Authors: </strong>Yevheniya Nosyk, Maciej Korczyński, Carlos Gañán, Sourena Maroofi, Jan Bayer, Zul Odgerel, Samaneh Tajalizadehkhoob, Andrzej Duda</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01391">https://arxiv.org/abs/2512.01391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01391">https://arxiv.org/pdf/2512.01391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01391]] INFERMAL: Inferential analysis of maliciously registered domains(https://arxiv.org/abs/2512.01391)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Cybercriminals have long depended on domain names for phishing, spam, malware distribution, and botnet operation. To facilitate the malicious activities, they continually register new domain names for exploitation. Previous work revealed an abnormally high concentration of malicious registrations in a handful of domain name registrars and top-level domains (TLDs). Anecdotal evidence suggests that low registration prices attract cybercriminals, implying that higher costs may potentially discourage them. However, no existing study has systematically analyzed the factors driving abuse, leaving a critical gap in understanding how different variables influence malicious registrations. In this report, we carefully distill the inclinations and aversions of malicious actors during the registration of new phishing domain names. We compile a comprehensive list of 73 features encompassing three main latent factors: registration attributes, proactive verification, and reactive security practices. Through a GLM regression analysis, we find that each dollar reduction in registration fees corresponds to a 49% increase in malicious domains. The availability of free services, such as web hosting, drives an 88% surge in phishing activities. Conversely, stringent restrictions cut down abuse by 63%, while registrars providing API access for domain registration or account creation experience a staggering 401% rise in malicious domains. This exploration may assist intermediaries involved in domain registration to develop tailored anti-abuse practices, yet aligning them with their economic incentives.</li>
</ul>

<h3>Title: RE-LLM: Integrating Large Language Models into Renewable Energy Systems</h3>
<ul>
<li><strong>Authors: </strong>Ali Forootani, Mohammad Sadr, Danial Esmaeili Aliabadi, Daniela Thraen</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01392">https://arxiv.org/abs/2512.01392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01392">https://arxiv.org/pdf/2512.01392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01392]] RE-LLM: Integrating Large Language Models into Renewable Energy Systems(https://arxiv.org/abs/2512.01392)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Energy system models are increasingly employed to guide long-term planning in multi-sectoral environments where decisions span electricity, heat, transport, land use, and industry. While these models provide rigorous quantitative insights, their outputs are often highly technical, making them difficult to interpret for non-expert stakeholders such as policymakers, planners, and the public. This communication gap limits the accessibility and practical impact of scenario-based modeling, particularly as energy transitions grow more complex with rising shares of renewables, sectoral integration, and deep uncertainties. To address this challenge, we propose the Renewable Energy Large Language Model (RE-LLM), a hybrid framework that integrates Large Language Models (LLMs) directly into the energy system modeling workflow. RE-LLM combines three core elements: (i) optimization-based scenario exploration, (ii) machine learning surrogates that accelerate computationally intensive simulations, and (iii) LLM-powered natural language generation that translates complex results into clear, stakeholder-oriented explanations. This integrated design not only reduces computational burden but also enhances inter-pretability, enabling real-time reasoning about trade-offs, sensitivities, and policy implications. The framework is adaptable across different optimization platforms and energy system models, ensuring broad applicability beyond the case study presented. By merging speed, rigor, and interpretability, RE-LLM advances a new paradigm of human-centric energy modeling. It enables interactive, multilingual, and accessible engagement with future energy pathways, ultimately bridging the final gap between data-driven analysis and actionable decision-making for sustainable transitions.</li>
</ul>

<h3>Title: On Global Applicability and Location Transferability of Generative Deep Learning Models for Precipitation Downscaling</h3>
<ul>
<li><strong>Authors: </strong>Paula Harder, Christian Lessig, Matthew Chantry, Francis Pelletier, David Rolnick</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01400">https://arxiv.org/abs/2512.01400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01400">https://arxiv.org/pdf/2512.01400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01400]] On Global Applicability and Location Transferability of Generative Deep Learning Models for Precipitation Downscaling(https://arxiv.org/abs/2512.01400)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep learning offers promising capabilities for the statistical downscaling of climate and weather forecasts, with generative approaches showing particular success in capturing fine-scale precipitation patterns. However, most existing models are region-specific, and their ability to generalize to unseen geographic areas remains largely unexplored. In this study, we evaluate the generalization performance of generative downscaling models across diverse regions. Using a global framework, we employ ERA5 reanalysis data as predictors and IMERG precipitation estimates at $0.1^\circ$ resolution as targets. A hierarchical location-based data split enables a systematic assessment of model performance across 15 regions around the world.</li>
</ul>

<h3>Title: Fantastic Features and Where to Find Them: A Probing Method to combine Features from Multiple Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Ramtoula, Pierre-Yves Lajoie, Paul Newman, Daniele De Martini</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01405">https://arxiv.org/abs/2512.01405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01405">https://arxiv.org/pdf/2512.01405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01405]] Fantastic Features and Where to Find Them: A Probing Method to combine Features from Multiple Foundation Models(https://arxiv.org/abs/2512.01405)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Foundation models (FMs) trained with different objectives and data learn diverse representations, making some more effective than others for specific downstream tasks. Existing adaptation strategies, such as parameter-efficient fine-tuning, focus on individual models and do not exploit the complementary strengths across models. Probing methods offer a promising alternative by extracting information from frozen models, but current techniques do not scale well with large feature sets and often rely on dataset-specific hyperparameter tuning. We propose Combined backBones (ComBo), a simple and scalable probing-based adapter that effectively integrates features from multiple models and layers. ComBo compresses activations from layers of one or more FMs into compact token-wise representations and processes them with a lightweight transformer for task-specific prediction. Crucially, ComBo does not require dataset-specific tuning or backpropagation through the backbone models. However, not all models are equally relevant for all tasks. To address this, we introduce a mechanism that leverages ComBo's joint multi-backbone probing to efficiently evaluate each backbone's task-relevance, enabling both practical model comparison and improved performance through selective adaptation. On the 19 tasks of the VTAB-1k benchmark, ComBo outperforms previous probing methods, matches or surpasses more expensive alternatives, such as distillation-based model merging, and enables efficient probing of tuned models. Our results demonstrate that ComBo offers a practical and general-purpose framework for combining diverse representations from multiple FMs.</li>
</ul>

<h3>Title: A Self-explainable Model of Long Time Series by Extracting Informative Structured Causal Patterns</h3>
<ul>
<li><strong>Authors: </strong>Ziqian Wang, Yuxiao Cheng, Jinli Suo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01412">https://arxiv.org/abs/2512.01412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01412">https://arxiv.org/pdf/2512.01412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01412]] A Self-explainable Model of Long Time Series by Extracting Informative Structured Causal Patterns(https://arxiv.org/abs/2512.01412)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Explainability is essential for neural networks that model long time series, yet most existing explainable AI methods only produce point-wise importance scores and fail to capture temporal structures such as trends, cycles, and regime changes. This limitation weakens human interpretability and trust in long-horizon models. To address these issues, we identify four key requirements for interpretable time-series modeling: temporal continuity, pattern-centric explanation, causal disentanglement, and faithfulness to the model's inference process. We propose EXCAP, a unified framework that satisfies all four requirements. EXCAP combines an attention-based segmenter that extracts coherent temporal patterns, a causally structured decoder guided by a pre-trained causal graph, and a latent aggregation mechanism that enforces representation stability. Our theoretical analysis shows that EXCAP provides smooth and stable explanations over time and is robust to perturbations in causal masks. Extensive experiments on classification and forecasting benchmarks demonstrate that EXCAP achieves strong predictive accuracy while generating coherent and causally grounded explanations. These results show that EXCAP offers a principled and scalable approach to interpretable modeling of long time series with relevance to high-stakes domains such as healthcare and finance.</li>
</ul>

<h3>Title: PromptBridge: Cross-Model Prompt Transfer for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yaxuan Wang, Quan Liu, Zhenting Wang, Zichao Li, Wei Wei, Yang Liu, Yujia Bao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01420">https://arxiv.org/abs/2512.01420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01420">https://arxiv.org/pdf/2512.01420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01420]] PromptBridge: Cross-Model Prompt Transfer for Large Language Models(https://arxiv.org/abs/2512.01420)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) underpin applications in code generation, mathematical reasoning, and agent-based workflows. In practice, systems access LLMs via commercial APIs or open-source deployments, and the model landscape (e.g., GPT, Claude, Llama) evolves rapidly. This rapid evolution forces frequent model switches driven by capability, cost, deployment constraints, and privacy. Yet prompts are highly model-sensitive: reusing a prompt engineered for one model on another often yields substantially worse performance than a prompt optimized for the target model. We term this phenomenon Model Drifting. Through extensive empirical analysis across diverse LLM configurations, we show that model drifting is both common and severe. To address this challenge, we introduce PromptBridge, a training-free framework that preserves prompt effectiveness under model switches, enabling cross-model prompt transfer without costly per-task or per-model re-optimization. PromptBridge requires only a small set of alignment tasks for calibration. It first applies Model-Adaptive Reflective Prompt Evolution (MAP-RPE) to obtain task- and model-specific optimal prompts via iterative reflective refinement and quantitative evaluation. Using the resulting calibrated prompt pairs for the source and target models, PromptBridge learns a cross-model prompt mapping. At test time, i.e., for an unseen task, given a source-model prompt, this mapping directly produces an optimized prompt for the target model. Experiments in single-agent and multi-agent settings show that PromptBridge consistently improves downstream accuracy while reducing migration effort. The code will be available soon.</li>
</ul>

<h3>Title: MDiff4STR: Mask Diffusion Model for Scene Text Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yongkun Du, Miaomiao Zhao, Songlin Fan, Zhineng Chen, Caiyan Jia, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01422">https://arxiv.org/abs/2512.01422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01422">https://arxiv.org/pdf/2512.01422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01422]] MDiff4STR: Mask Diffusion Model for Scene Text Recognition(https://arxiv.org/abs/2512.01422)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Mask Diffusion Models (MDMs) have recently emerged as a promising alternative to auto-regressive models (ARMs) for vision-language tasks, owing to their flexible balance of efficiency and accuracy. In this paper, for the first time, we introduce MDMs into the Scene Text Recognition (STR) task. We show that vanilla MDM lags behind ARMs in terms of accuracy, although it improves recognition efficiency. To bridge this gap, we propose MDiff4STR, a Mask Diffusion model enhanced with two key improvement strategies tailored for STR. Specifically, we identify two key challenges in applying MDMs to STR: noising gap between training and inference, and overconfident predictions during inference. Both significantly hinder the performance of MDMs. To mitigate the first issue, we develop six noising strategies that better align training with inference behavior. For the second, we propose a token-replacement noise mechanism that provides a non-mask noise type, encouraging the model to reconsider and revise overly confident but incorrect predictions. We conduct extensive evaluations of MDiff4STR on both standard and challenging STR benchmarks, covering diverse scenarios including irregular, artistic, occluded, and Chinese text, as well as whether the use of pretraining. Across these settings, MDiff4STR consistently outperforms popular STR models, surpassing state-of-the-art ARMs in accuracy, while maintaining fast inference with only three denoising steps. Code: this https URL.</li>
</ul>

<h3>Title: \textit{ViRectify}: A Challenging Benchmark for Video Reasoning Correction with Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xusen Hei, Jiali Chen, Jinyu Yang, Mengchen Zhao, Yi Cai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01424">https://arxiv.org/abs/2512.01424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01424">https://arxiv.org/pdf/2512.01424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01424]] \textit{ViRectify}: A Challenging Benchmark for Video Reasoning Correction with Multimodal Large Language Models(https://arxiv.org/abs/2512.01424)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As multimodal large language models (MLLMs) frequently exhibit errors in complex video reasoning scenarios, correcting these errors is critical for uncovering their weaknesses and improving performance. However, existing benchmarks lack systematic evaluation of MLLMs' ability to identify and correct these video reasoning errors. To bridge this gap, we propose \textit{ViRectify}, a comprehensive benchmark to evaluate their fine-grained correction capability. Through an AI-assisted annotation pipeline with human verification, we construct a dataset of over 30\textit{K} instances spanning dynamic perception, scientific reasoning, and embodied decision-making domains. In \textit{ViRectify}, we challenge MLLMs to perform step-wise error identification and generate rationales with key video evidence grounding. In addition, we further propose the trajectory evidence-driven correction framework, comprising step-wise error trajectory and reward modeling on visual evidence-grounded correction. It encourages the model to explicitly concentrate on error propagation and key timestamps for correction. Extensive evaluation across 16 advanced MLLMs demonstrates that our \textit{ViRectify} serves as a challenging testbed, where GPT-5 achieves only 31.94\% correction accuracy. Our framework enables a Qwen2.5-VL-7B to consistently outperform the variants of 72B on \textit{ViRectify}, showing the effectiveness of our approach. Further analysis uncovers systematic asymmetries in error correction across models, and our dataset is also a valuable data resource to perform reflection learning. We believe \textit{ViRectify} provides a new direction for comprehensively evaluating the advanced MLLMs in video reasoning.</li>
</ul>

<h3>Title: ResDiT: Evoking the Intrinsic Resolution Scalability in Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Yiyang Ma, Feng Zhou, Xuedan Yin, Pu Cao, Yonghao Dang, Jianqin Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01426">https://arxiv.org/abs/2512.01426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01426">https://arxiv.org/pdf/2512.01426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01426]] ResDiT: Evoking the Intrinsic Resolution Scalability in Diffusion Transformers(https://arxiv.org/abs/2512.01426)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Leveraging pre-trained Diffusion Transformers (DiTs) for high-resolution (HR) image synthesis often leads to spatial layout collapse and degraded texture fidelity. Prior work mitigates these issues with complex pipelines that first perform a base-resolution (i.e., training-resolution) denoising process to guide HR generation. We instead explore the intrinsic generative mechanisms of DiTs and propose ResDiT, a training-free method that scales resolution efficiently. We identify the core factor governing spatial layout, position embeddings (PEs), and show that the original PEs encode incorrect positional information when extrapolated to HR, which triggers layout collapse. To address this, we introduce a PE scaling technique that rectifies positional encoding under resolution changes. To further remedy low-fidelity details, we develop a local-enhancement mechanism grounded in base-resolution local attention. We design a patch-level fusion module that aggregates global and local cues, together with a Gaussian-weighted splicing strategy that eliminates grid artifacts. Comprehensive evaluations demonstrate that ResDiT consistently delivers high-fidelity, high-resolution image synthesis and integrates seamlessly with downstream tasks, including spatially controlled generation.</li>
</ul>

<h3>Title: Language-Guided Open-World Anomaly Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Klara Reichard, Nikolas Brasch, Nassir Navab, Federico Tombari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01427">https://arxiv.org/abs/2512.01427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01427">https://arxiv.org/pdf/2512.01427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01427]] Language-Guided Open-World Anomaly Segmentation(https://arxiv.org/abs/2512.01427)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>Open-world and anomaly segmentation methods seek to enable autonomous driving systems to detect and segment both known and unknown objects in real-world scenes. However, existing methods do not assign semantically meaningful labels to unknown regions, and distinguishing and learning representations for unknown classes remains difficult. While open-vocabulary segmentation methods show promise in generalizing to novel classes, they require a fixed inference vocabulary and thus cannot be directly applied to anomaly segmentation where unknown classes are unconstrained. We propose Clipomaly, the first CLIP-based open-world and anomaly segmentation method for autonomous driving. Our zero-shot approach requires no anomaly-specific training data and leverages CLIP's shared image-text embedding space to both segment unknown objects and assign human-interpretable names to them. Unlike open-vocabulary methods, our model dynamically extends its vocabulary at inference time without retraining, enabling robust detection and naming of anomalies beyond common class definitions such as those in Cityscapes. Clipomaly achieves state-of-the-art performance on established anomaly segmentation benchmarks while providing interpretability and flexibility essential for practical deployment.</li>
</ul>

<h3>Title: Inside Qubic's Selfish Mining Campaign on Monero: Evidence, Tactics, and Limits</h3>
<ul>
<li><strong>Authors: </strong>Suhyeon Lee, Hyeongyeong Kim</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01437">https://arxiv.org/abs/2512.01437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01437">https://arxiv.org/pdf/2512.01437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01437]] Inside Qubic's Selfish Mining Campaign on Monero: Evidence, Tactics, and Limits(https://arxiv.org/abs/2512.01437)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, segmentation</a></li>
<li><strong>Abstract: </strong>We analyze Qubic's advertised selfish mining campaign on Monero in 2025. Combining data from Monero nodes, and the Qubic pool API, we reconstruct Qubic-attributed blocks and hashrate and detect ten intervals consistent with selfish mining strategies. In these intervals, Qubic's average hashrate share rises to the 23-34\% range, yet sustained 51\% control is never observed. We evaluate the campaign against the classical selfish mining model and a modified Markov-chain model that reflects Qubic's conservative release strategy: both predict lower revenue than honest mining at the inferred parameters, and the data largely confirms this while still showing noticeable deviations from the predicted curve. We interpret this gap between model and measurements in terms of Qubic's time-varying hashrate and coarse-grained attack segmentation.</li>
</ul>

<h3>Title: MEGConformer: Conformer-Based MEG Decoder for Robust Speech and Phoneme Classification</h3>
<ul>
<li><strong>Authors: </strong>Xabier de Zuazo, Ibon Saratxaga, Eva Navas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.NE, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01443">https://arxiv.org/abs/2512.01443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01443">https://arxiv.org/pdf/2512.01443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01443]] MEGConformer: Conformer-Based MEG Decoder for Robust Speech and Phoneme Classification(https://arxiv.org/abs/2512.01443)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present Conformer-based decoders for the LibriBrain 2025 PNPL competition, targeting two foundational MEG tasks: Speech Detection and Phoneme Classification. Our approach adapts a compact Conformer to raw 306-channel MEG signals, with a lightweight convolutional projection layer and task-specific heads. For Speech Detection, a MEG-oriented SpecAugment provided a first exploration of MEG-specific augmentation. For Phoneme Classification, we used inverse-square-root class weighting and a dynamic grouping loader to handle 100-sample averaged examples. In addition, a simple instance-level normalization proved critical to mitigate distribution shifts on the holdout split. Using the official Standard track splits and F1-macro for model selection, our best systems achieved 88.9% (Speech) and 65.8% (Phoneme) on the leaderboard, surpassing the competition baselines and ranking within the top-10 in both tasks. For further implementation details, the technical documentation, source code, and checkpoints are available at this https URL.</li>
</ul>

<h3>Title: ZIP-RC: Zero-overhead Inference-time Prediction of Reward and Cost for Adaptive and Interpretable Generation</h3>
<ul>
<li><strong>Authors: </strong>Rohin Manvi, Joey Hong, Tim Seyde, Maxime Labonne, Mathias Lechner, Sergey Levine</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01457">https://arxiv.org/abs/2512.01457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01457">https://arxiv.org/pdf/2512.01457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01457]] ZIP-RC: Zero-overhead Inference-time Prediction of Reward and Cost for Adaptive and Interpretable Generation(https://arxiv.org/abs/2512.01457)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, an adaptive inference method that equips models with zero-overhead inference-time predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning.</li>
</ul>

<h3>Title: Stay Unique, Stay Efficient: Preserving Model Personality in Multi-Task Merging</h3>
<ul>
<li><strong>Authors: </strong>Kuangpu Guo, Yuhe Ding, Jian Liang, Zilei Wang, Ran He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01461">https://arxiv.org/abs/2512.01461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01461">https://arxiv.org/pdf/2512.01461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01461]] Stay Unique, Stay Efficient: Preserving Model Personality in Multi-Task Merging(https://arxiv.org/abs/2512.01461)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free</a></li>
<li><strong>Abstract: </strong>Model merging has emerged as a promising paradigm for enabling multi-task capabilities without additional training. However, existing methods often experience substantial performance degradation compared with individually fine-tuned models, even on similar tasks, underscoring the need to preserve task-specific information. This paper proposes Decomposition, Thresholding, and Scaling (DTS), an approximation-based personalized merging framework that preserves task-specific information with minimal storage overhead. DTS first applies singular value decomposition to the task-specific information and retains only a small subset of singular values and vectors. It then introduces a novel thresholding strategy that partitions singular vector elements into groups and assigns a scaling factor to each group. To enable generalization to unseen tasks, we further extend DTS with a variant that fuses task-specific information in a data-free manner based on the semantic similarity of task characteristics. Extensive experiments demonstrate that DTS consistently outperforms state-of-the-art baselines while requiring only 1\% additional storage per task. Furthermore, experiments on unseen tasks show that the DTS variant achieves significantly better generalization performance. Our code is available at this https URL.</li>
</ul>

<h3>Title: A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data</h3>
<ul>
<li><strong>Authors: </strong>Hongnan Si, Tong Li, Yujie Chen, Xin Liao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01465">https://arxiv.org/abs/2512.01465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01465">https://arxiv.org/pdf/2512.01465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01465]] A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data(https://arxiv.org/abs/2512.01465)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Water quality monitoring is a core component of ecological environmental protection. However, due to sensor failure or other inevitable factors, data missing often exists in long-term monitoring, posing great challenges in water quality analysis. This paper proposes a Neural Tucker Convolutional Network (NTCN) model for water quality data imputation, which features the following key components: a) Encode different mode entities into respective embedding vectors, and construct a Tucker interaction tensor by outer product operations to capture the complex mode-wise feature interactions; b) Use 3D convolution to extract fine-grained spatiotemporal features from the interaction tensor. Experiments on three real-world water quality datasets show that the proposed NTCN model outperforms several state-of-the-art imputation models in terms of accuracy.</li>
</ul>

<h3>Title: CourtMotion: Learning Event-Driven Motion Representations from Skeletal Data for Basketball</h3>
<ul>
<li><strong>Authors: </strong>Omer Sela (1 and 2), Michael Chertok (1), Lior Wolf (2) ((1) Amazon, (2) Tel Aviv University)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01478">https://arxiv.org/abs/2512.01478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01478">https://arxiv.org/pdf/2512.01478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01478]] CourtMotion: Learning Event-Driven Motion Representations from Skeletal Data for Basketball(https://arxiv.org/abs/2512.01478)</code><input type="text"></li>
<li><strong>Keywords: </strong>steal, transformer</a></li>
<li><strong>Abstract: </strong>This paper presents CourtMotion, a spatiotemporal modeling framework for analyzing and predicting game events and plays as they develop in professional basketball. Anticipating basketball events requires understanding both physical motion patterns and their semantic significance in the context of the game. Traditional approaches that use only player positions fail to capture crucial indicators such as body orientation, defensive stance, or shooting preparation motions. Our two-stage approach first processes skeletal tracking data through Graph Neural Networks to capture nuanced motion patterns, then employs a Transformer architecture with specialized attention mechanisms to model player interactions. We introduce event projection heads that explicitly connect player movements to basketball events like passes, shots, and steals, training the model to associate physical motion patterns with their tactical purposes. Experiments on NBA tracking data demonstrate significant improvements over position-only baselines: 35% reduction in trajectory prediction error compared to state-of-the-art position-based models and consistent performance gains across key basketball analytics tasks. The resulting pretrained model serves as a powerful foundation for multiple downstream tasks, with pick detection, shot taker identification, assist prediction, shot location classification, and shot type recognition demonstrating substantial improvements over existing methods.</li>
</ul>

<h3>Title: ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling</h3>
<ul>
<li><strong>Authors: </strong>Qisen Wang, Yifan Zhao, Peisen Shen, Jialu Li, Jia Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01481">https://arxiv.org/abs/2512.01481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01481">https://arxiv.org/pdf/2512.01481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01481]] ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling(https://arxiv.org/abs/2512.01481)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Although prevailing camera-controlled video generation models can produce cinematic results, lifting them directly to the generation of 3D-consistent and high-fidelity time-synchronized multi-view videos remains challenging, which is a pivotal capability for taming 4D worlds. Some works resort to data augmentation or test-time optimization, but these strategies are constrained by limited model generalization and scalability issues. To this end, we propose ChronosObserver, a training-free method including World State Hyperspace to represent the spatiotemporal constraints of a 4D world scene, and Hyperspace Guided Sampling to synchronize the diffusion sampling trajectories of multiple views using the hyperspace. Experimental results demonstrate that our method achieves high-fidelity and 3D-consistent time-synchronized multi-view videos generation without training or fine-tuning for diffusion models.</li>
</ul>

<h3>Title: Multi-view diffusion geometry using intertwined diffusion trajectories</h3>
<ul>
<li><strong>Authors: </strong>Gwendal Debaussart-Joniec (CB), Argyris Kalogeratos (CB)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01484">https://arxiv.org/abs/2512.01484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01484">https://arxiv.org/pdf/2512.01484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01484]] Multi-view diffusion geometry using intertwined diffusion trajectories(https://arxiv.org/abs/2512.01484)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper introduces a comprehensive unified framework for constructing multi-view diffusion geometries through intertwined multi-view diffusion trajectories (MDTs), a class of inhomogeneous diffusion processes that iteratively combine the random walk operators of multiple data views. Each MDT defines a trajectory-dependent diffusion operator with a clear probabilistic and geometric interpretation, capturing over time the interplay between data views. Our formulation encompasses existing multi-view diffusion models, while providing new degrees of freedom for view interaction and fusion. We establish theoretical properties under mild assumptions, including ergodicity of both the point-wise operator and the process in itself. We also derive MDT-based diffusion distances, and associated embeddings via singular value decompositions. Finally, we propose various strategies for learning MDT operators within the defined operator space, guided by internal quality measures. Beyond enabling flexible model design, MDTs also offer a neutral baseline for evaluating diffusion-based approaches through comparison with randomly selected MDTs. Experiments show the practical impact of the MDT operators in a manifold learning and data clustering context.</li>
</ul>

<h3>Title: A variational method for curve extraction with curvature-dependent energies</h3>
<ul>
<li><strong>Authors: </strong>Majid Arthaud (ENPC, MOKAPLAN, UMich), Antonin Chambolle (CEREMADE, MOKAPLAN), Vincent Duval (MOKAPLAN)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01494">https://arxiv.org/abs/2512.01494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01494">https://arxiv.org/pdf/2512.01494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01494]] A variational method for curve extraction with curvature-dependent energies(https://arxiv.org/abs/2512.01494)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>We introduce a variational approach for extracting curves between a list of possible endpoints, based on the discretization of an energy and Smirnov's decomposition theorem for vector fields. It is used to design a bi-level minimization approach to automatically extract curves and 1D structures from an image, which is mostly unsupervised. We extend then the method to curvature-dependent energies, using a now classical lifting of the curves in the space of positions and orientations equipped with an appropriate sub-Riemanian or Finslerian metric.</li>
</ul>

<h3>Title: ELVIS: Enhance Low-Light for Video Instance Segmentation in the Dark</h3>
<ul>
<li><strong>Authors: </strong>Joanne Lin, Ruirui Lin, Yini Li, David Bull, Nantheera Anantrasirichai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01495">https://arxiv.org/abs/2512.01495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01495">https://arxiv.org/pdf/2512.01495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01495]] ELVIS: Enhance Low-Light for Video Instance Segmentation in the Dark(https://arxiv.org/abs/2512.01495)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Video instance segmentation (VIS) for low-light content remains highly challenging for both humans and machines alike, due to adverse imaging conditions including noise, blur and low-contrast. The lack of large-scale annotated datasets and the limitations of current synthetic pipelines, particularly in modeling temporal degradations, further hinder progress. Moreover, existing VIS methods are not robust to the degradations found in low-light videos and, as a result, perform poorly even when finetuned on low-light data. In this paper, we introduce \textbf{ELVIS} (\textbf{E}nhance \textbf{L}ow-light for \textbf{V}ideo \textbf{I}nstance \textbf{S}egmentation), a novel framework that enables effective domain adaptation of state-of-the-art VIS models to low-light scenarios. ELVIS comprises an unsupervised synthetic low-light video pipeline that models both spatial and temporal degradations, a calibration-free degradation profile synthesis network (VDP-Net) and an enhancement decoder head that disentangles degradations from content features. ELVIS improves performances by up to \textbf{+3.7AP} on the synthetic low-light YouTube-VIS 2019 dataset. Code will be released upon acceptance.</li>
</ul>

<h3>Title: Winning Solutions for the Rayan AI Contest: Compositional Retrieval, Zero-Shot Anomaly Detection, and Backdoor Detection</h3>
<ul>
<li><strong>Authors: </strong>Ali Nafisi, Sina Asghari, Mohammad Saeed Arvenaghi, Hossein Shakibania</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01498">https://arxiv.org/abs/2512.01498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01498">https://arxiv.org/pdf/2512.01498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01498]] Winning Solutions for the Rayan AI Contest: Compositional Retrieval, Zero-Shot Anomaly Detection, and Backdoor Detection(https://arxiv.org/abs/2512.01498)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>This report presents solutions to three machine learning challenges: compositional image retrieval, zero-shot anomaly detection, and backdoored model detection. In compositional image retrieval, we developed a system that processes visual and textual inputs to retrieve relevant images, achieving 95.38\% accuracy and ranking first with a clear margin over the second team. For zero-shot anomaly detection, we designed a model that identifies and localizes anomalies in images without prior exposure to abnormal examples, securing 1st place with 73.14\% accuracy. In the backdoored model detection task, we proposed a method to detect hidden backdoor triggers in neural networks, reaching an accuracy of 78\%, which placed our approach in second place. These results demonstrate the effectiveness of our methods in addressing key challenges related to retrieval, anomaly detection, and model security, with implications for real-world applications in industries such as healthcare, manufacturing, and cybersecurity. Code for all solutions is available online.</li>
</ul>

<h3>Title: Semantic-aware Random Convolution and Source Matching for Domain Generalization in Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Franz Thaler, Martin Urschler, Mateusz Kozinski, Matthias AF Gsell, Gernot Plank, Darko Stern</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01510">https://arxiv.org/abs/2512.01510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01510">https://arxiv.org/pdf/2512.01510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01510]] Semantic-aware Random Convolution and Source Matching for Domain Generalization in Medical Image Segmentation(https://arxiv.org/abs/2512.01510)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We tackle the challenging problem of single-source domain generalization (DG) for medical image segmentation. To this end, we aim for training a network on one domain (e.g., CT) and directly apply it to a different domain (e.g., MR) without adapting the model and without requiring images or annotations from the new domain during training. We propose a novel method for promoting DG when training deep segmentation networks, which we call SRCSM. During training, our method diversifies the source domain through semantic-aware random convolution, where different regions of a source image are augmented differently, based on their annotation labels. At test-time, we complement the randomization of the training domain via mapping the intensity of target domain images, making them similar to source domain data. We perform a comprehensive evaluation on a variety of cross-modality and cross-center generalization settings for abdominal, whole-heart and prostate segmentation, where we outperform previous DG techniques in a vast majority of experiments. Additionally, we also investigate our method when training on whole-heart CT or MR data and testing on the diastolic and systolic phase of cine MR data captured with different scanner hardware, where we make a step towards closing the domain gap in this even more challenging setting. Overall, our evaluation shows that SRCSM can be considered a new state-of-the-art in DG for medical image segmentation and, moreover, even achieves a segmentation performance that matches the performance of the in-domain baseline in several settings.</li>
</ul>

<h3>Title: MCAT: Scaling Many-to-Many Speech-to-Text Translation with MLLMs to 70 Languages</h3>
<ul>
<li><strong>Authors: </strong>Yexing Du, Kaiyuan Liu, Youcheng Pan, Bo Yang, Keqi Deng, Xie Chen, Yang Xiang, Ming Liu, Bin Qin, YaoWei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01512">https://arxiv.org/abs/2512.01512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01512">https://arxiv.org/pdf/2512.01512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01512]] MCAT: Scaling Many-to-Many Speech-to-Text Translation with MLLMs to 70 Languages(https://arxiv.org/abs/2512.01512)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have achieved great success in Speech-to-Text Translation (S2TT) tasks. However, current research is constrained by two key challenges: language coverage and efficiency. Most of the popular S2TT datasets are substantially English-centric, which restricts the scaling-up of MLLMs' many-to-many translation capabilities. Moreover, the inference speed of MLLMs degrades dramatically when the speech is converted into long sequences (e.g., 750 tokens). To address these limitations, we propose a Multilingual Cost-effective Accelerated Speech-to-Text Translator (MCAT) framework, which includes two innovations. First, a language scaling method that leverages curriculum learning and a data balancing strategy is introduced to extend the language coverage supported by MLLMs to 70 languages and achieve mutual translation among these languages. Second, an optimized speech adapter module is designed to reduce the length of the speech sequence to only 30 tokens. Extensive experiments were conducted on MLLMs of different scales (9B and 27B). The experimental results demonstrate that MCAT not only surpasses state-of-the-art end-to-end models on the FLEURS dataset across 70x69 directions but also enhances batch inference efficiency. This is achieved with only ~100M trainable parameters and by using only 10 hours of S2TT data per language. Furthermore, we have released MCAT as open-source to promote the development of MLLMs for robust S2TT capabilities. The code and models are released at this https URL.</li>
</ul>

<h3>Title: Diffusion Fuzzy System: Fuzzy Rule Guided Latent Multi-Path Diffusion Modeling</h3>
<ul>
<li><strong>Authors: </strong>Hailong Yang, Te Zhang, Kup-sze Choi, Zhaohong Deng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01533">https://arxiv.org/abs/2512.01533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01533">https://arxiv.org/pdf/2512.01533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01533]] Diffusion Fuzzy System: Fuzzy Rule Guided Latent Multi-Path Diffusion Modeling(https://arxiv.org/abs/2512.01533)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as a leading technique for generating images due to their ability to create high-resolution and realistic images. Despite their strong performance, diffusion models still struggle in managing image collections with significant feature differences. They often fail to capture complex features and produce conflicting results. Research has attempted to address this issue by learning different regions of an image through multiple diffusion paths and then combining them. However, this approach leads to inefficient coordination among multiple paths and high computational costs. To tackle these issues, this paper presents a Diffusion Fuzzy System (DFS), a latent-space multi-path diffusion model guided by fuzzy rules. DFS offers several advantages. First, unlike traditional multi-path diffusion methods, DFS uses multiple diffusion paths, each dedicated to learning a specific class of image features. By assigning each path to a different feature type, DFS overcomes the limitations of multi-path models in capturing heterogeneous image features. Second, DFS employs rule-chain-based reasoning to dynamically steer the diffusion process and enable efficient coordination among multiple paths. Finally, DFS introduces a fuzzy membership-based latent-space compression mechanism to reduce the computational costs of multi-path diffusion effectively. We tested our method on three public datasets: LSUN Bedroom, LSUN Church, and MS COCO. The results show that DFS achieves more stable training and faster convergence than existing single-path and multi-path diffusion models. Additionally, DFS surpasses baseline models in both image quality and alignment between text and images, and also shows improved accuracy when comparing generated images to target references.</li>
</ul>

<h3>Title: Deep Unsupervised Anomaly Detection in Brain Imaging: Large-Scale Benchmarking and Bias Analysis</h3>
<ul>
<li><strong>Authors: </strong>Alexander Frotscher, Christian F. Baumgartner, Thomas Wolfers</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01534">https://arxiv.org/abs/2512.01534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01534">https://arxiv.org/pdf/2512.01534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01534]] Deep Unsupervised Anomaly Detection in Brain Imaging: Large-Scale Benchmarking and Bias Analysis(https://arxiv.org/abs/2512.01534)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Deep unsupervised anomaly detection in brain magnetic resonance imaging offers a promising route to identify pathological deviations without requiring lesion-specific annotations. Yet, fragmented evaluations, heterogeneous datasets, and inconsistent metrics have hindered progress toward clinical translation. Here, we present a large-scale, multi-center benchmark of deep unsupervised anomaly detection for brain imaging. The training cohort comprised 2,976 T1 and 2,972 T2-weighted scans from healthy individuals across six scanners, with ages ranging from 6 to 89 years. Validation used 92 scans to tune hyperparameters and estimate unbiased thresholds. Testing encompassed 2,221 T1w and 1,262 T2w scans spanning healthy datasets and diverse clinical cohorts. Across all algorithms, the Dice-based segmentation performance varied between 0.03 and 0.65, indicating substantial variability. To assess robustness, we systematically evaluated the impact of different scanners, lesion types and sizes, as well as demographics (age, sex). Reconstruction-based methods, particularly diffusion-inspired approaches, achieved the strongest lesion segmentation performance, while feature-based methods showed greater robustness under distributional shifts. However, systematic biases, such as scanner-related effects, were observed for the majority of algorithms, including that small and low-contrast lesions were missed more often, and that false positives varied with age and sex. Increasing healthy training data yields only modest gains, underscoring that current unsupervised anomaly detection frameworks are limited algorithmically rather than by data availability. Our benchmark establishes a transparent foundation for future research and highlights priorities for clinical translation, including image native pretraining, principled deviation measures, fairness-aware modeling, and robust domain adaptation.</li>
</ul>

<h3>Title: FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention</h3>
<ul>
<li><strong>Authors: </strong>Zipeng Wang, Dan Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01540">https://arxiv.org/abs/2512.01540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01540">https://arxiv.org/pdf/2512.01540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01540]] FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention(https://arxiv.org/abs/2512.01540)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at this https URL.</li>
</ul>

<h3>Title: Language Diversity: Evaluating Language Usage and AI Performance on African Languages in Digital Spaces</h3>
<ul>
<li><strong>Authors: </strong>Edward Ajayi, Eudoxie Umwari, Mawuli Deku, Prosper Singadi, Jules Udahemuka, Bekalu Tadele, Chukuemeka Edeh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01557">https://arxiv.org/abs/2512.01557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01557">https://arxiv.org/pdf/2512.01557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01557]] Language Diversity: Evaluating Language Usage and AI Performance on African Languages in Digital Spaces(https://arxiv.org/abs/2512.01557)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This study examines the digital representation of African languages and the challenges this presents for current language detection tools. We evaluate their performance on Yoruba, Kinyarwanda, and Amharic. While these languages are spoken by millions, their online usage on conversational platforms is often sparse, heavily influenced by English, and not representative of the authentic, monolingual conversations prevalent among native speakers. This lack of readily available authentic data online creates a challenge of scarcity of conversational data for training language models. To investigate this, data was collected from subreddits and local news sources for each language. The analysis showed a stark contrast between the two sources. Reddit data was minimal and characterized by heavy code-switching. Conversely, local news media offered a robust source of clean, monolingual language data, which also prompted more user engagement in the local language on the news publishers social media pages. Language detection models, including the specialized AfroLID and a general LLM, performed with near-perfect accuracy on the clean news data but struggled with the code-switched Reddit posts. The study concludes that professionally curated news content is a more reliable and effective source for training context-rich AI models for African languages than data from conversational platforms. It also highlights the need for future models that can process clean and code-switched text to improve the detection accuracy for African languages.</li>
</ul>

<h3>Title: TimePred: efficient and interpretable offline change point detection for high volume data - with application to industrial process monitoring</h3>
<ul>
<li><strong>Authors: </strong>Simon Leszek</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01562">https://arxiv.org/abs/2512.01562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01562">https://arxiv.org/pdf/2512.01562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01562]] TimePred: efficient and interpretable offline change point detection for high volume data - with application to industrial process monitoring(https://arxiv.org/abs/2512.01562)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Change-point detection (CPD) in high-dimensional, large-volume time series is challenging for statistical consistency, scalability, and interpretability. We introduce TimePred, a self-supervised framework that reduces multivariate CPD to univariate mean-shift detection by predicting each sample's normalized time index. This enables efficient offline CPD using existing algorithms and supports the integration of XAI attribution methods for feature-level explanations. Our experiments show competitive CPD performance while reducing computational cost by up to two orders of magnitude. In an industrial manufacturing case study, we demonstrate improved detection accuracy and illustrate the practical value of interpretable change-point insights.</li>
</ul>

<h3>Title: MasHeNe: A Benchmark for Head and Neck CT Mass Segmentation using Window-Enhanced Mamba with Frequency-Domain Integration</h3>
<ul>
<li><strong>Authors: </strong>Thao Thi Phuong Dao, Tan-Cong Nguyen, Nguyen Chi Thanh, Truong Hoang Viet, Trong-Le Do, Mai-Khiem Tran, Minh-Khoi Pham, Trung-Nghia Le, Minh-Triet Tran, Thanh Dinh Le</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01563">https://arxiv.org/abs/2512.01563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01563">https://arxiv.org/pdf/2512.01563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01563]] MasHeNe: A Benchmark for Head and Neck CT Mass Segmentation using Window-Enhanced Mamba with Frequency-Domain Integration(https://arxiv.org/abs/2512.01563)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, fair, segmentation</a></li>
<li><strong>Abstract: </strong>Head and neck masses are space-occupying lesions that can compress the airway and esophagus and may affect nerves and blood vessels. Available public datasets primarily focus on malignant lesions and often overlook other space-occupying conditions in this region. To address this gap, we introduce MasHeNe, an initial dataset of 3,779 contrast-enhanced CT slices that includes both tumors and cysts with pixel-level annotations. We also establish a benchmark using standard segmentation baselines and report common metrics to enable fair comparison. In addition, we propose the Windowing-Enhanced Mamba with Frequency integration (WEMF) model. WEMF applies tri-window enhancement to enrich the input appearance before feature extraction. It further uses multi-frequency attention to fuse information across skip connections within a U-shaped Mamba backbone. On MasHeNe, WEMF attains the best performance among evaluated methods, with a Dice of 70.45%, IoU of 66.89%, NSD of 72.33%, and HD95 of 5.12 mm. This model indicates stable and strong results on this challenging task. MasHeNe provides a benchmark for head-and-neck mass segmentation beyond malignancy-only datasets. The observed error patterns also suggest that this task remains challenging and requires further research. Our dataset and code are available at this https URL.</li>
</ul>

<h3>Title: Do Large Language Models Walk Their Talk? Measuring the Gap Between Implicit Associations, Self-Report, and Behavioral Altruism</h3>
<ul>
<li><strong>Authors: </strong>Sandro Andric</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01568">https://arxiv.org/abs/2512.01568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01568">https://arxiv.org/pdf/2512.01568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01568]] Do Large Language Models Walk Their Talk? Measuring the Gap Between Implicit Associations, Self-Report, and Behavioral Altruism(https://arxiv.org/abs/2512.01568)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We investigate whether Large Language Models (LLMs) exhibit altruistic tendencies, and critically, whether their implicit associations and self-reports predict actual altruistic behavior. Using a multi-method approach inspired by human social psychology, we tested 24 frontier LLMs across three paradigms: (1) an Implicit Association Test (IAT) measuring implicit altruism bias, (2) a forced binary choice task measuring behavioral altruism, and (3) a self-assessment scale measuring explicit altruism beliefs. Our key findings are: (1) All models show strong implicit pro-altruism bias (mean IAT = 0.87, p < .0001), confirming models "know" altruism is good. (2) Models behave more altruistically than chance (65.6% vs. 50%, p < .0001), but with substantial variation (48-85%). (3) Implicit associations do not predict behavior (r = .22, p = .29). (4) Most critically, models systematically overestimate their own altruism, claiming 77.5% altruism while acting at 65.6% (p < .0001, Cohen's d = 1.08). This "virtue signaling gap" affects 75% of models tested. Based on these findings, we recommend the Calibration Gap (the discrepancy between self-reported and behavioral values) as a standardized alignment metric. Well-calibrated models are more predictable and behaviorally consistent; only 12.5% of models achieve the ideal combination of high prosocial behavior and accurate self-knowledge.</li>
</ul>

<h3>Title: Reconstructing Multi-Scale Physical Fields from Extremely Sparse Measurements with an Autoencoder-Diffusion Cascade</h3>
<ul>
<li><strong>Authors: </strong>Letian Yi, Tingpeng Zhang, Mingyuan Zhou, Guannan Wang, Quanke Su, Zhilu Lai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.app-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01572">https://arxiv.org/abs/2512.01572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01572">https://arxiv.org/pdf/2512.01572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01572]] Reconstructing Multi-Scale Physical Fields from Extremely Sparse Measurements with an Autoencoder-Diffusion Cascade(https://arxiv.org/abs/2512.01572)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Reconstructing full fields from extremely sparse and random measurements is a longstanding ill-posed inverse problem. A powerful framework for addressing such challenges is hierarchical probabilistic modeling, where uncertainty is represented by intermediate variables and resolved through marginalization during inference. Inspired by this principle, we propose Cascaded Sensing (Cas-Sensing), a hierarchical reconstruction framework that integrates an autoencoder-diffusion cascade. First, a neural operator-based functional autoencoder reconstructs the dominant structures of the original field - including large-scale components and geometric boundaries - from arbitrary sparse inputs, serving as an intermediate variable. Then, a conditional diffusion model, trained with a mask-cascade strategy, generates fine-scale details conditioned on these large-scale structures. To further enhance fidelity, measurement consistency is enforced via the manifold constrained gradient based on Bayesian posterior sampling during the generation process. This cascaded pipeline substantially alleviates ill-posedness, delivering accurate and robust reconstructions. Experiments on both simulation and real-world datasets demonstrate that Cas-Sensing generalizes well across varying sensor configurations and geometric boundaries, making it a promising tool for practical deployment in scientific and engineering applications.</li>
</ul>

<h3>Title: IVE: An Accelerator for Single-Server Private Information Retrieval Using Versatile Processing Elements</h3>
<ul>
<li><strong>Authors: </strong>Sangpyo Kim, Hyesung Ji, Jongmin Kim, Wonseok Choi, Jaiyoung Park, Jung Ho Ahn</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01574">https://arxiv.org/abs/2512.01574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01574">https://arxiv.org/pdf/2512.01574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01574]] IVE: An Accelerator for Single-Server Private Information Retrieval Using Versatile Processing Elements(https://arxiv.org/abs/2512.01574)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Private information retrieval (PIR) is an essential cryptographic protocol for privacy-preserving applications, enabling a client to retrieve a record from a server's database without revealing which record was requested. Single-server PIR based on homomorphic encryption has particularly gained immense attention for its ease of deployment and reduced trust assumptions. However, single-server PIR remains impractical due to its high computational and memory bandwidth demands. Specifically, reading the entirety of large databases from storage, such as SSDs, severely limits its performance. To address this, we propose IVE, an accelerator for single-server PIR with a systematic extension that enables practical retrieval from large databases using DRAM. Recent advances in DRAM capacity allow PIR for large databases to be served entirely from DRAM, removing its dependence on storage bandwidth. Although the memory bandwidth bottleneck still remains, multi-client batching effectively amortizes database access costs across concurrent requests to improve throughput. However, client-specific data remains a bottleneck, whose bandwidth requirements ultimately limits performance. IVE overcomes this by employing a large on-chip scratchpad with an operation scheduling algorithm that maximizes data reuse, further boosting throughput. Additionally, we introduce sysNTTU, a versatile functional unit that enhances area efficiency without sacrificing performance. We also propose a heterogeneous memory system architecture, which enables a linear scaling of database sizes without a throughput degradation. Consequently, IVE achieves up to 1,275x higher throughput compared to prior PIR hardware solutions.</li>
</ul>

<h3>Title: Beyond the Hype: A Large-Scale Empirical Analysis of On-Chain Transactions in NFT Scams</h3>
<ul>
<li><strong>Authors: </strong>Wenkai Li, Zongwei Li, Xiaoqi Li, Chunyi Zhang, Xiaoyan Zhang, Yuqing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01577">https://arxiv.org/abs/2512.01577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01577">https://arxiv.org/pdf/2512.01577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01577]] Beyond the Hype: A Large-Scale Empirical Analysis of On-Chain Transactions in NFT Scams(https://arxiv.org/abs/2512.01577)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Non-fungible tokens (NFTs) serve as a representative form of digital asset ownership and have attracted numerous investors, creators, and tech enthusiasts in recent years. However, related fraud activities, especially phishing scams, have caused significant property losses. There are many graph analysis methods to detect malicious scam incidents, but no research on the transaction patterns of the NFT scams. Therefore, to fill this gap, we are the first to systematically explore NFT phishing frauds through graph analysis, aiming to comprehensively investigate the characteristics and patterns of NFT phishing frauds on the transaction graph. During the research process, we collect transaction records, log data, and security reports related to NFT phishing incidents published on multiple platforms. After collecting, sanitizing, and unifying the data, we construct a transaction graph and analyze the distribution, transaction features, and interaction patterns of NFT phishing scams. We find that normal transactions on the blockchain accounted for 96.71% of all transactions. Although phishing-related accounts accounted for only 0.94% of the total accounts, they appeared in 8.36% of the transaction scenarios, and their interaction probability with normal accounts is significantly higher in large-scale transaction networks. Moreover, NFT phishing scammers often carry out fraud in a collective manner, targeting specific accounts, tend to interact with victims through multiple token standards, have shorter transaction cycles than normal transactions, and involve more multi-party transactions. This study reveals the core behavioral features of NFT phishing scams, providing important references for the detection and prevention of NFT phishing scams in the future.</li>
</ul>

<h3>Title: RoleMotion: A Large-Scale Dataset towards Robust Scene-Specific Role-Playing Motion Synthesis with Fine-grained Descriptions</h3>
<ul>
<li><strong>Authors: </strong>Junran Peng, Yiheng Huang, Silei Shen, Zeji Wei, Jingwei Yang, Baojie Wang, Yonghao He, Chuanchen Luo, Man Zhang, Xucheng Yin, Wei Sui</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01582">https://arxiv.org/abs/2512.01582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01582">https://arxiv.org/pdf/2512.01582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01582]] RoleMotion: A Large-Scale Dataset towards Robust Scene-Specific Role-Playing Motion Synthesis with Fine-grained Descriptions(https://arxiv.org/abs/2512.01582)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce RoleMotion, a large-scale human motion dataset that encompasses a wealth of role-playing and functional motion data tailored to fit various specific scenes. Existing text datasets are mainly constructed decentrally as amalgamation of assorted subsets that their data are nonfunctional and isolated to work together to cover social activities in various scenes. Also, the quality of motion data is inconsistent, and textual annotation lacks fine-grained details in these datasets. In contrast, RoleMotion is meticulously designed and collected with a particular focus on scenes and roles. The dataset features 25 classic scenes, 110 functional roles, over 500 behaviors, and 10296 high-quality human motion sequences of body and hands, annotated with 27831 fine-grained text descriptions. We build an evaluator stronger than existing counterparts, prove its reliability, and evaluate various text-to-motion methods on our dataset. Finally, we explore the interplay of motion generation of body and hands. Experimental results demonstrate the high-quality and functionality of our dataset on text-driven whole-body generation.</li>
</ul>

<h3>Title: Toward Content-based Indexing and Retrieval of Head and Neck CT with Abscess Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Thao Thi Phuong Dao, Tan-Cong Nguyen, Trong-Le Do, Truong Hoang Viet, Nguyen Chi Thanh, Huynh Nguyen Thuan, Do Vo Cong Nguyen, Minh-Khoi Pham, Mai-Khiem Tran, Viet-Tham Huynh, Trong-Thuan Nguyen, Trung-Nghia Le, Vo Thanh Toan, Tam V. Nguyen, Minh-Triet Tran, Thanh Dinh Le</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01589">https://arxiv.org/abs/2512.01589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01589">https://arxiv.org/pdf/2512.01589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01589]] Toward Content-based Indexing and Retrieval of Head and Neck CT with Abscess Segmentation(https://arxiv.org/abs/2512.01589)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Abscesses in the head and neck represent an acute infectious process that can potentially lead to sepsis or mortality if not diagnosed and managed promptly. Accurate detection and delineation of these lesions on imaging are essential for diagnosis, treatment planning, and surgical intervention. In this study, we introduce AbscessHeNe, a curated and comprehensively annotated dataset comprising 4,926 contrast-enhanced CT slices with clinically confirmed head and neck abscesses. The dataset is designed to facilitate the development of robust semantic segmentation models that can accurately delineate abscess boundaries and evaluate deep neck space involvement, thereby supporting informed clinical decision-making. To establish performance baselines, we evaluate several state-of-the-art segmentation architectures, including CNN, Transformer, and Mamba-based models. The highest-performing model achieved a Dice Similarity Coefficient of 0.39, Intersection-over-Union of 0.27, and Normalized Surface Distance of 0.67, indicating the challenges of this task and the need for further research. Beyond segmentation, AbscessHeNe is structured for future applications in content-based multimedia indexing and case-based retrieval. Each CT scan is linked with pixel-level annotations and clinical metadata, providing a foundation for building intelligent retrieval systems and supporting knowledge-driven clinical workflows. The dataset will be made publicly available at this https URL.</li>
</ul>

<h3>Title: Scaling and context steer LLMs along the same computational path as the human brain</h3>
<ul>
<li><strong>Authors: </strong>Joséphine Raugel, Stéphane d'Ascoli, Jérémy Rapin, Valentin Wyart, Jean-Rémi King</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01591">https://arxiv.org/abs/2512.01591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01591">https://arxiv.org/pdf/2512.01591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01591]] Scaling and context steer LLMs along the same computational path as the human brain(https://arxiv.org/abs/2512.01591)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recent studies suggest that the representations learned by large language models (LLMs) are partially aligned to those of the human brain. However, whether and why this alignment score arises from a similar sequence of computations remains elusive. In this study, we explore this question by examining temporally-resolved brain signals of participants listening to 10 hours of an audiobook. We study these neural dynamics jointly with a benchmark encompassing 22 LLMs varying in size and architecture type. Our analyses confirm that LLMs and the brain generate representations in a similar order: specifically, activations in the initial layers of LLMs tend to best align with early brain responses, while the deeper layers of LLMs tend to best align with later brain responses. This brain-LLM alignment is consistent across transformers and recurrent architectures. However, its emergence depends on both model size and context length. Overall, this study sheds light on the sequential nature of computations and the factors underlying the partial convergence between biological and artificial neural networks.</li>
</ul>

<h3>Title: Confidential, Attestable, and Efficient Inter-CVM Communication with Arm CCA</h3>
<ul>
<li><strong>Authors: </strong>Sina Abdollahi, Amir Al Sadi, Marios Kogias, David Kotz, Hamed Haddadi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.OS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01594">https://arxiv.org/abs/2512.01594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01594">https://arxiv.org/pdf/2512.01594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01594]] Confidential, Attestable, and Efficient Inter-CVM Communication with Arm CCA(https://arxiv.org/abs/2512.01594)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, protect</a></li>
<li><strong>Abstract: </strong>Confidential Virtual Machines (CVMs) are increasingly adopted to protect sensitive workloads from privileged adversaries such as the hypervisor. While they provide strong isolation guarantees, existing CVM architectures lack first-class mechanisms for inter-CVM data sharing due to their disjoint memory model, making inter-CVM data exchange a performance bottleneck in compartmentalized or collaborative multi-CVM systems. Under this model, a CVM's accessible memory is either shared with the hypervisor or protected from both the hypervisor and all other CVMs. This design simplifies reasoning about memory ownership; however, it fundamentally precludes plaintext data sharing between CVMs because all inter-CVM communication must pass through hypervisor-accessible memory, requiring costly encryption and decryption to preserve confidentiality and integrity. In this paper, we introduce CAEC, a system that enables protected memory sharing between CVMs. CAEC builds on Arm Confidential Compute Architecture (CCA) and extends its firmware to support Confidential Shared Memory (CSM), a memory region securely shared between multiple CVMs while remaining inaccessible to the hypervisor and all non-participating CVMs. CAEC's design is fully compatible with CCA hardware and introduces only a modest increase (4\%) in CCA firmware code size. CAEC delivers substantial performance benefits across a range of workloads. For instance, inter-CVM communication over CAEC achieves up to 209$\times$ reduction in CPU cycles compared to encryption-based mechanisms over hypervisor-accessible shared memory. By combining high performance, strong isolation guarantees, and attestable sharing semantics, CAEC provides a practical and scalable foundation for the next generation of trusted multi-CVM services across both edge and cloud environments.</li>
</ul>

<h3>Title: WhiteLie: A Robust System for Spoofing User Data in Android Platforms</h3>
<ul>
<li><strong>Authors: </strong>Harish Yadav, Vikas Maurya, Abhilash Jindal, Vireshwar Kumar</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01595">https://arxiv.org/abs/2512.01595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01595">https://arxiv.org/pdf/2512.01595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01595]] WhiteLie: A Robust System for Spoofing User Data in Android Platforms(https://arxiv.org/abs/2512.01595)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Android employs a permission framework that empowers users to either accept or deny sharing their private data (for example, location) with an app. However, many apps tend to crash when they are denied permission, leaving users no choice but to allow access to their data in order to use the app. In this paper, we introduce a comprehensive and robust user data spoofing system, WhiteLie, that can spoof a variety of user data and feed it to target apps. Additionally, it detects privacy-violating behaviours, automatically responding by supplying spoofed data instead of the user's real data, without crashing or disrupting the apps. Unlike prior approaches, WhiteLie requires neither device rooting nor altering the app's binary, making it deployable on stock Android devices. Through experiments on more than 70 popular Android apps, we demonstrate that WhiteLie is able to deceive apps into accepting spoofed data without getting detected. Our evaluation further demonstrates that WhiteLie introduces negligible overhead in terms of battery usage, CPU consumption, and app execution latency. Our findings underscore the feasibility of implementing user-centric privacy-enhancing mechanisms within the existing Android ecosystem.</li>
</ul>

<h3>Title: Towards a Multi-Layer Defence Framework for Securing Near-Real-Time Operations in Open RAN</h3>
<ul>
<li><strong>Authors: </strong>Hamed Alimohammadi, Samara Mayhoub, Sotiris Chatzimiltis, Mohammad Shojafar, Muhammad Nasir Mumtaz Bhutta</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01596">https://arxiv.org/abs/2512.01596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01596">https://arxiv.org/pdf/2512.01596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01596]] Towards a Multi-Layer Defence Framework for Securing Near-Real-Time Operations in Open RAN(https://arxiv.org/abs/2512.01596)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>Securing the near-real-time (near-RT) control operations in Open Radio Access Networks (Open RAN) is increasingly critical, yet remains insufficiently addressed, as new runtime threats target the control loop while the system is operational. In this paper, we propose a multi-layer defence framework designed to enhance the security of near-RT RAN Intelligent Controller (RIC) operations. We classify operational-time threats into three categories, message-level, data-level, and control logic-level, and design and implement a dedicated detection and mitigation component for each: a signature-based E2 message inspection module performing structural and semantic validation of signalling exchanges, a telemetry poisoning detector based on temporal anomaly scoring using an LSTM network, and a runtime xApp attestation mechanism based on execution-time hash challenge-response. The framework is evaluated on an O-RAN testbed comprising FlexRIC and a commercial RAN emulator, demonstrating effective detection rates, low latency overheads, and practical integration feasibility. Results indicate that the proposed safeguards can operate within near-RT time constraints while significantly improving protection against runtime attacks, introducing less than 80 ms overhead for a network with 500 User Equipment (UEs). Overall, this work lays the foundation for deployable, layered, and policy-driven runtime security architectures for the near-RT RIC control loop in Open RAN, and provides an extensible framework into which future mitigation policies and threat-specific modules can be integrated.</li>
</ul>

<h3>Title: MAC-SLU: Multi-Intent Automotive Cabin Spoken Language Understanding Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Yuezhang Peng, Chonghao Cai, Ziang Liu, Shuai Fan, Sheng Jiang, Hua Xu, Yuxin Liu, Qiguang Chen, Kele Xu, Yao Li, Sheng Wang, Libo Qin, Xie Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01603">https://arxiv.org/abs/2512.01603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01603">https://arxiv.org/pdf/2512.01603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01603]] MAC-SLU: Multi-Intent Automotive Cabin Spoken Language Understanding Benchmark(https://arxiv.org/abs/2512.01603)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Spoken Language Understanding (SLU), which aims to extract user semantics to execute downstream tasks, is a crucial component of task-oriented dialog systems. Existing SLU datasets generally lack sufficient diversity and complexity, and there is an absence of a unified benchmark for the latest Large Language Models (LLMs) and Large Audio Language Models (LALMs). This work introduces MAC-SLU, a novel Multi-Intent Automotive Cabin Spoken Language Understanding Dataset, which increases the difficulty of the SLU task by incorporating authentic and complex multi-intent data. Based on MAC-SLU, we conducted a comprehensive benchmark of leading open-source LLMs and LALMs, covering methods like in-context learning, supervised fine-tuning (SFT), and end-to-end (E2E) and pipeline paradigms. Our experiments show that while LLMs and LALMs have the potential to complete SLU tasks through in-context learning, their performance still lags significantly behind SFT. Meanwhile, E2E LALMs demonstrate performance comparable to pipeline approaches and effectively avoid error propagation from speech recognition. Code\footnote{this https URL\_SLU} and datasets\footnote{this http URL\_SLU} are released publicly.</li>
</ul>

<h3>Title: On the Context-Hiding Property of Shamir-Based Homomorphic Secret Sharing</h3>
<ul>
<li><strong>Authors: </strong>Shuai Feng, Liang Feng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01604">https://arxiv.org/abs/2512.01604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01604">https://arxiv.org/pdf/2512.01604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01604]] On the Context-Hiding Property of Shamir-Based Homomorphic Secret Sharing(https://arxiv.org/abs/2512.01604)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Homomorphic secret sharing (HSS) allows multiple input clients to secretly share their private inputs to a function among several servers such that each server can homomorphically compute the function over its share to produce a share of the function's output. In HSS-enabled applications such as secure multi-party computation (MPC), security requires that the output shares leak no more information about the inputs than the function output. Such security is ensured by the context-hiding property of HSS. The typical rerandomization technique achieves context hiding but increases the share size. To address this, we formalize the context-hiding property of HSS for individual functions, examine the context-hiding property of Shamir-based HSS for monomials, and extend the study to polynomials.</li>
</ul>

<h3>Title: SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Yumeng He, Ying Jiang, Jiayin Lu, Yin Yang, Chenfanfu Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01629">https://arxiv.org/abs/2512.01629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01629">https://arxiv.org/pdf/2512.01629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01629]] SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge(https://arxiv.org/abs/2512.01629)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Articulated 3D objects are critical for embodied AI, robotics, and interactive scene understanding, yet creating simulation-ready assets remains labor-intensive and requires expert modeling of part hierarchies and motion structures. We introduce SPARK, a framework for reconstructing physically consistent, kinematic part-level articulated objects from a single RGB image. Given an input image, we first leverage VLMs to extract coarse URDF parameters and generate part-level reference images. We then integrate the part-image guidance and the inferred structure graph into a generative diffusion transformer to synthesize consistent part and complete shapes of articulated objects. To further refine the URDF parameters, we incorporate differentiable forward kinematics and differentiable rendering to optimize joint types, axes, and origins under VLM-generated open-state supervision. Extensive experiments show that SPARK produces high-quality, simulation-ready articulated assets across diverse categories, enabling downstream applications such as robotic manipulation and interaction modeling.</li>
</ul>

<h3>Title: Generative Editing in the Joint Vision-Language Space for Zero-Shot Composed Image Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Xin Wang, Haipeng Zhang, Mang Li, Zhaohui Xia, Yueguo Chen, Yu Zhang, Chunyu Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01636">https://arxiv.org/abs/2512.01636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01636">https://arxiv.org/pdf/2512.01636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01636]] Generative Editing in the Joint Vision-Language Space for Zero-Shot Composed Image Retrieval(https://arxiv.org/abs/2512.01636)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Composed Image Retrieval (CIR) enables fine-grained visual search by combining a reference image with a textual modification. While supervised CIR methods achieve high accuracy, their reliance on costly triplet annotations motivates zero-shot solutions. The core challenge in zero-shot CIR (ZS-CIR) stems from a fundamental dilemma: existing text-centric or diffusion-based approaches struggle to effectively bridge the vision-language modality gap. To address this, we propose Fusion-Diff, a novel generative editing framework with high effectiveness and data efficiency designed for multimodal alignment. First, it introduces a multimodal fusion feature editing strategy within a joint vision-language (VL) space, substantially narrowing the modality gap. Second, to maximize data efficiency, the framework incorporates a lightweight Control-Adapter, enabling state-of-the-art performance through fine-tuning on only a limited-scale synthetic dataset of 200K samples. Extensive experiments on standard CIR benchmarks (CIRR, FashionIQ, and CIRCO) demonstrate that Fusion-Diff significantly outperforms prior zero-shot approaches. We further enhance the interpretability of our model by visualizing the fused multimodal representations.</li>
</ul>

<h3>Title: ViT$^3$: Unlocking Test-Time Training in Vision</h3>
<ul>
<li><strong>Authors: </strong>Dongchen Han, Yining Li, Tianyu Li, Zixuan Cao, Ziming Wang, Jun Song, Yu Cheng, Bo Zheng, Gao Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01643">https://arxiv.org/abs/2512.01643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01643">https://arxiv.org/pdf/2512.01643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01643]] ViT$^3$: Unlocking Test-Time Training in Vision(https://arxiv.org/abs/2512.01643)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Test-Time Training (TTT) has recently emerged as a promising direction for efficient sequence modeling. TTT reformulates attention operation as an online learning problem, constructing a compact inner model from key-value pairs at test time. This reformulation opens a rich and flexible design space while achieving linear computational complexity. However, crafting a powerful visual TTT design remains challenging: fundamental choices for the inner module and inner training lack comprehensive understanding and practical guidelines. To bridge this critical gap, in this paper, we present a systematic empirical study of TTT designs for visual sequence modeling. From a series of experiments and analyses, we distill six practical insights that establish design principles for effective visual TTT and illuminate paths for future improvement. These findings culminate in the Vision Test-Time Training (ViT$^3$) model, a pure TTT architecture that achieves linear complexity and parallelizable computation. We evaluate ViT$^3$ across diverse visual tasks, including image classification, image generation, object detection, and semantic segmentation. Results show that ViT$^3$ consistently matches or outperforms advanced linear-complexity models (e.g., Mamba and linear attention variants) and effectively narrows the gap to highly optimized vision Transformers. We hope this study and the ViT$^3$ baseline can facilitate future work on visual TTT models. Code is available at this https URL.</li>
</ul>

<h3>Title: In-context Inverse Optimality for Fair Digital Twins: A Preference-based approach</h3>
<ul>
<li><strong>Authors: </strong>Daniele Masti, Francesco Basciani, Arianna Fedeli, Girgio Gnecco, Francesco Smarra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SE, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01650">https://arxiv.org/abs/2512.01650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01650">https://arxiv.org/pdf/2512.01650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01650]] In-context Inverse Optimality for Fair Digital Twins: A Preference-based approach(https://arxiv.org/abs/2512.01650)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Digital Twins (DTs) are increasingly used as autonomous decision-makers in complex socio-technical systems. Their mathematically optimal decisions often diverge from human expectations, exposing a persistent gap between algorithmic and bounded human rationality. This work addresses this gap by proposing a framework that operationalizes fairness as a learnable objective within optimization-based Digital Twins. We introduce a preference-driven learning pipeline that infers latent fairness objectives directly from human pairwise preferences over feasible decisions. A novel Siamese neural network is developed to generate convex quadratic cost functions conditioned on contextual information. The resulting surrogate objectives align optimization outcomes with human-perceived fairness while maintaining computational efficiency. The approach is demonstrated on a COVID-19 hospital resource allocation scenario. This study provides an actionable path toward embedding human-centered fairness in the design of autonomous decision-making systems.</li>
</ul>

<h3>Title: Rethinking Cybersecurity Ontology Classification and Evaluation: Towards a Credibility-Centered Framework</h3>
<ul>
<li><strong>Authors: </strong>Antoine Leblanc, Jacques Robin, Nourhène Ben Rabah, Zequan Huang, Bénédicte Le Grand</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01651">https://arxiv.org/abs/2512.01651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01651">https://arxiv.org/pdf/2512.01651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01651]] Rethinking Cybersecurity Ontology Classification and Evaluation: Towards a Credibility-Centered Framework(https://arxiv.org/abs/2512.01651)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>This paper analyzes the proliferation of cybersecurity ontologies, arguing that this surge cannot be explained solely by technical shortcomings related to quality, but also by a credibility deficit - a lack of trust, endorsement, and adoption by users. This conclusion is based on our first contribution, which is a state-of-the-art review and categorization of cybersecurity ontologies using the Framework for Ontologies Classification framework. To address this gap, we propose a revised framework for assessing credibility, introducing indicators such as institutional support, academic recognition, day-to-day practitioner validation, and industrial adoption. Based on these new credibility indicators, we construct a classification scheme designed to guide the selection of ontologies that are relevant to specific security needs. We then apply this framework to a concrete use case: the Franco-Luxembourgish research project ANCILE, which illustrates how a credibility-aware evaluation can reshape ontology selection for operational contexts.</li>
</ul>

<h3>Title: DB-KAUNet: An Adaptive Dual Branch Kolmogorov-Arnold UNet for Retinal Vessel Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Xu, Panpan Meng, Meng Wang, Dayu Hu, Liming Liang, Xiaoqi Sheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01657">https://arxiv.org/abs/2512.01657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01657">https://arxiv.org/pdf/2512.01657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01657]] DB-KAUNet: An Adaptive Dual Branch Kolmogorov-Arnold UNet for Retinal Vessel Segmentation(https://arxiv.org/abs/2512.01657)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of retinal vessels is crucial for the clinical diagnosis of numerous ophthalmic and systemic diseases. However, traditional Convolutional Neural Network (CNN) methods exhibit inherent limitations, struggling to capture long-range dependencies and complex nonlinear relationships. To address the above limitations, an Adaptive Dual Branch Kolmogorov-Arnold UNet (DB-KAUNet) is proposed for retinal vessel segmentation. In DB-KAUNet, we design a Heterogeneous Dual-Branch Encoder (HDBE) that features parallel CNN and Transformer pathways. The HDBE strategically interleaves standard CNN and Transformer blocks with novel KANConv and KAT blocks, enabling the model to form a comprehensive feature representation. To optimize feature processing, we integrate several critical components into the HDBE. First, a Cross-Branch Channel Interaction (CCI) module is embedded to facilitate efficient interaction of channel features between the parallel pathways. Second, an attention-based Spatial Feature Enhancement (SFE) module is employed to enhance spatial features and fuse the outputs from both branches. Building upon the SFE module, an advanced Spatial Feature Enhancement with Geometrically Adaptive Fusion (SFE-GAF) module is subsequently developed. In the SFE-GAF module, adaptive sampling is utilized to focus on true vessel morphology precisely. The adaptive process strengthens salient vascular features while significantly reducing background noise and computational overhead. Extensive experiments on the DRIVE, STARE, and CHASE_DB1 datasets validate that DB-KAUNet achieves leading segmentation performance and demonstrates exceptional robustness.</li>
</ul>

<h3>Title: HalluGraph: Auditable Hallucination Detection for Legal RAG Systems via Knowledge Graph Alignment</h3>
<ul>
<li><strong>Authors: </strong>Valentin Noël, Elimane Yassine Seidou, Charly Ken Capo-Chichi, Ghanem Amari</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01659">https://arxiv.org/abs/2512.01659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01659">https://arxiv.org/pdf/2512.01659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01659]] HalluGraph: Auditable Hallucination Detection for Legal RAG Systems via Knowledge Graph Alignment(https://arxiv.org/abs/2512.01659)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Legal AI systems powered by retrieval-augmented generation (RAG) face a critical accountability challenge: when an AI assistant cites case law, statutes, or contractual clauses, practitioners need verifiable guarantees that generated text faithfully represents source documents. Existing hallucination detectors rely on semantic similarity metrics that tolerate entity substitutions, a dangerous failure mode when confusing parties, dates, or legal provisions can have material consequences. We introduce HalluGraph, a graph-theoretic framework that quantifies hallucinations through structural alignment between knowledge graphs extracted from context, query, and response. Our approach produces bounded, interpretable metrics decomposed into \textit{Entity Grounding} (EG), measuring whether entities in the response appear in source documents, and \textit{Relation Preservation} (RP), verifying that asserted relationships are supported by context. On structured control documents, HalluGraph achieves near-perfect discrimination ($>$400 words, $>$20 entities), HalluGraph achieves $AUC = 0.979$, while maintaining robust performance ($AUC \approx 0.89$) on challenging generative legal task, consistently outperforming semantic similarity baselines. The framework provides the transparency and traceability required for high-stakes legal applications, enabling full audit trails from generated assertions back to source passages.</li>
</ul>

<h3>Title: Bridging the Scale Gap: Balanced Tiny and General Object Detection in Remote Sensing Imagery</h3>
<ul>
<li><strong>Authors: </strong>Zhicheng Zhao, Yin Huang, Lingma Sun, Chenglong Li, Jin Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01665">https://arxiv.org/abs/2512.01665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01665">https://arxiv.org/pdf/2512.01665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01665]] Bridging the Scale Gap: Balanced Tiny and General Object Detection in Remote Sensing Imagery(https://arxiv.org/abs/2512.01665)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Tiny object detection in remote sensing imagery has attracted significant research interest in recent years. Despite recent progress, achieving balanced detection performance across diverse object scales remains a formidable challenge, particularly in scenarios where dense tiny objects and large objects coexist. Although large foundation models have revolutionized general vision tasks, their application to tiny object detection remains unexplored due to the extreme scale variation and density distribution inherent to remote sensing imagery. To bridge this scale gap, we propose ScaleBridge-Det, to the best of our knowledge, the first large detection framework designed for tiny objects, which could achieve balanced performance across diverse scales through scale-adaptive expert routing and density-guided query allocation. Specifically, we introduce a Routing-Enhanced Mixture Attention (REM) module that dynamically selects and fuses scale-specific expert features via adaptive routing to address the tendency of standard MoE models to favor dominant scales. REM generates complementary and discriminative multi-scale representations suitable for both tiny and large objects. Furthermore, we present a Density-Guided Dynamic Query (DGQ) module that predicts object density to adaptively adjust query positions and numbers, enabling efficient resource allocation for objects of varying scales. The proposed framework allows ScaleBridge-Det to simultaneously optimize performance for both dense tiny and general objects without trade-offs. Extensive experiments on benchmark and cross-domain datasets demonstrate that ScaleBridge-Det achieves state-of-the-art performance on AI-TOD-V2 and DTOD, while exhibiting superior cross-domain robustness on VisDrone.</li>
</ul>

<h3>Title: Demystifying Feature Engineering in Malware Analysis of API Call Sequences</h3>
<ul>
<li><strong>Authors: </strong>Tianheng Qu, Hongsong Zhu, Limin Sun, Haining Wang, Haiqiang Fei, Zheng He, Zhi Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01666">https://arxiv.org/abs/2512.01666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01666">https://arxiv.org/pdf/2512.01666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01666]] Demystifying Feature Engineering in Malware Analysis of API Call Sequences(https://arxiv.org/abs/2512.01666)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Machine learning (ML) has been widely used to analyze API call sequences in malware analysis, which typically requires the expertise of domain specialists to extract relevant features from raw data. The extracted features play a critical role in malware analysis. Traditional feature extraction is based on human domain knowledge, while there is a trend of using natural language processing (NLP) for automatic feature extraction. This raises a question: how do we effectively select features for malware analysis based on API call sequences? To answer it, this paper presents a comprehensive study of investigating the impact of feature engineering upon malware this http URL first conducted a comparative performance evaluation under three models, Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and Transformer, with respect to knowledge-based and NLP-based feature engineering methods. We observed that models with knowledge-based feature engineering inputs generally outperform those using NLP-based across all metrics, especially under smaller sample sizes. Then we analyzed a complete set of data features from API call sequences, our analysis reveals that models often focus on features such as handles and virtual addresses, which vary across executions and are difficult for human analysts to interpret.</li>
</ul>

<h3>Title: ICAD-LLM: One-for-All Anomaly Detection via In-Context Learning with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhongyuan Wu, Jingyuan Wang, Zexuan Cheng, Yilong Zhou, Weizhi Wang, Juhua Pu, Chao Li, Changqing Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01672">https://arxiv.org/abs/2512.01672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01672">https://arxiv.org/pdf/2512.01672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01672]] ICAD-LLM: One-for-All Anomaly Detection via In-Context Learning with Large Language Models(https://arxiv.org/abs/2512.01672)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Anomaly detection (AD) is a fundamental task of critical importance across numerous domains. Current systems increasingly operate in rapidly evolving environments that generate diverse yet interconnected data modalities -- such as time series, system logs, and tabular records -- as exemplified by modern IT systems. Effective AD methods in such environments must therefore possess two critical capabilities: (1) the ability to handle heterogeneous data formats within a unified framework, allowing the model to process and detect multiple modalities in a consistent manner during anomalous events; (2) a strong generalization ability to quickly adapt to new scenarios without extensive retraining. However, most existing methods fall short of these requirements, as they typically focus on single modalities and lack the flexibility to generalize across domains. To address this gap, we introduce a novel paradigm: In-Context Anomaly Detection (ICAD), where anomalies are defined by their dissimilarity to a relevant reference set of normal samples. Under this paradigm, we propose ICAD-LLM, a unified AD framework leveraging Large Language Models' in-context learning abilities to process heterogeneous data within a single model. Extensive experiments demonstrate that ICAD-LLM achieves competitive performance with task-specific AD methods and exhibits strong generalization to previously unseen tasks, which substantially reduces deployment costs and enables rapid adaptation to new environments. To the best of our knowledge, ICAD-LLM is the first model capable of handling anomaly detection tasks across diverse domains and modalities.</li>
</ul>

<h3>Title: GRASP: Guided Residual Adapters with Sample-wise Partitioning</h3>
<ul>
<li><strong>Authors: </strong>Felix Nützel, Mischa Dombrowski, Bernhard Kainz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01675">https://arxiv.org/abs/2512.01675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01675">https://arxiv.org/pdf/2512.01675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01675]] GRASP: Guided Residual Adapters with Sample-wise Partitioning(https://arxiv.org/abs/2512.01675)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-image diffusion models enable high-fidelity generation across diverse prompts. However, these models falter in long-tail settings, such as medical imaging, where rare pathologies comprise a small fraction of data. This results in mode collapse: tail-class outputs lack quality and diversity, undermining the goal of synthetic data augmentation for underrepresented conditions. We pinpoint gradient conflicts between frequent head and rare tail classes as the primary culprit, a factor unaddressed by existing sampling or conditioning methods that mainly steer inference without altering the learned distribution. To resolve this, we propose GRASP: Guided Residual Adapters with Sample-wise Partitioning. GRASP uses external priors to statically partition samples into clusters that minimize intra-group gradient clashes. It then fine-tunes pre-trained models by injecting cluster-specific residual adapters into transformer feedforward layers, bypassing learned gating for stability and efficiency. On the long-tail MIMIC-CXR-LT dataset, GRASP yields superior FID and diversity metrics, especially for rare classes, outperforming baselines like vanilla fine-tuning and Mixture of Experts variants. Downstream classification on NIH-CXR-LT improves considerably for tail labels. Generalization to ImageNet-LT confirms broad applicability. Our method is lightweight, scalable, and readily integrates with diffusion pipelines.</li>
</ul>

<h3>Title: Open-world Hand-Object Interaction Video Generation Based on Structure and Contact-aware Representation</h3>
<ul>
<li><strong>Authors: </strong>Haodong Yan, Hang Yu, Zhide Zhong, Weilin Yuan, Xin Gong, Zehang Luo, Chengxi Heyu, Junfeng Li, Wenxuan Song, Shunbo Zhou, Haoang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01677">https://arxiv.org/abs/2512.01677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01677">https://arxiv.org/pdf/2512.01677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01677]] Open-world Hand-Object Interaction Video Generation Based on Structure and Contact-aware Representation(https://arxiv.org/abs/2512.01677)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generating realistic hand-object interactions (HOI) videos is a significant challenge due to the difficulty of modeling physical constraints (e.g., contact and occlusion between hands and manipulated objects). Current methods utilize HOI representation as an auxiliary generative objective to guide video synthesis. However, there is a dilemma between 2D and 3D representations that cannot simultaneously guarantee scalability and interaction fidelity. To address this limitation, we propose a structure and contact-aware representation that captures hand-object contact, hand-object occlusion, and holistic structure context without 3D annotations. This interaction-oriented and scalable supervision signal enables the model to learn fine-grained interaction physics and generalize to open-world scenarios. To fully exploit the proposed representation, we introduce a joint-generation paradigm with a share-and-specialization strategy that generates interaction-oriented representations and videos. Extensive experiments demonstrate that our method outperforms state-of-the-art methods on two real-world datasets in generating physics-realistic and temporally coherent HOI videos. Furthermore, our approach exhibits strong generalization to challenging open-world scenarios, highlighting the benefit of our scalable design. Our project page is this https URL.</li>
</ul>

<h3>Title: DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models</h3>
<ul>
<li><strong>Authors: </strong>Patrick Kwon, Chen Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01686">https://arxiv.org/abs/2512.01686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01686">https://arxiv.org/pdf/2512.01686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01686]] DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models(https://arxiv.org/abs/2512.01686)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Current story visualization methods tend to position subjects solely by text and face challenges in maintaining artistic consistency. To address these limitations, we introduce DreamingComics, a layout-aware story visualization framework. We build upon a pretrained video diffusion-transformer (DiT) model, leveraging its spatiotemporal priors to enhance identity and style consistency. For layout-based position control, we propose RegionalRoPE, a region-aware positional encoding scheme that re-indexes embeddings based on the target layout. Additionally, we introduce a masked condition loss to further constrain each subject's visual features to their designated region. To infer layouts from natural language scripts, we integrate an LLM-based layout generator trained to produce comic-style layouts, enabling flexible and controllable layout conditioning. We present a comprehensive evaluation of our approach, showing a 29.2% increase in character consistency and a 36.2% increase in style similarity compared to previous methods, while displaying high spatial accuracy. Our project page is available at this https URL</li>
</ul>

<h3>Title: SSR: Semantic and Spatial Rectification for CLIP-based Weakly Supervised Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xiuli Bi, Die Xiao, Junchao Fan, Bin Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01701">https://arxiv.org/abs/2512.01701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01701">https://arxiv.org/pdf/2512.01701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01701]] SSR: Semantic and Spatial Rectification for CLIP-based Weakly Supervised Segmentation(https://arxiv.org/abs/2512.01701)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In recent years, Contrastive Language-Image Pretraining (CLIP) has been widely applied to Weakly Supervised Semantic Segmentation (WSSS) tasks due to its powerful cross-modal semantic understanding capabilities. This paper proposes a novel Semantic and Spatial Rectification (SSR) method to address the limitations of existing CLIP-based weakly supervised semantic segmentation approaches: over-activation in non-target foreground regions and background areas. Specifically, at the semantic level, the Cross-Modal Prototype Alignment (CMPA) establishes a contrastive learning mechanism to enforce feature space alignment across modalities, reducing inter-class overlap while enhancing semantic correlations, to rectify over-activation in non-target foreground regions effectively; at the spatial level, the Superpixel-Guided Correction (SGC) leverages superpixel-based spatial priors to precisely filter out interference from non-target regions during affinity propagation, significantly rectifying background over-activation. Extensive experiments on the PASCAL VOC and MS COCO datasets demonstrate that our method outperforms all single-stage approaches, as well as more complex multi-stage approaches, achieving mIoU scores of 79.5% and 50.6%, respectively.</li>
</ul>

<h3>Title: A unified framework for geometry-independent operator learning in cardiac electrophysiology simulations</h3>
<ul>
<li><strong>Authors: </strong>Bei Zhou, Cesare Corrado, Shuang Qian, Maximilian Balmus, Angela W. C. Lee, Cristobal Rodero, Marco J.W. Gotte, Luuk H.G.A. Hopman, Mengyun Qiao, Steven Niederer</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01702">https://arxiv.org/abs/2512.01702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01702">https://arxiv.org/pdf/2512.01702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01702]] A unified framework for geometry-independent operator learning in cardiac electrophysiology simulations(https://arxiv.org/abs/2512.01702)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate maps of atrial electrical activation are essential for personalised treatment of arrhythmias, yet biophysically detailed simulations remain computationally intensive for real-time clinical use or population-scale analyses. Here we introduce a geometry-independent operator-learning framework that predicts local activation time (LAT) fields across diverse left atrial anatomies with near-instantaneous inference. We generated a dataset of 308,700 simulations using a GPU-accelerated electrophysiology solver, systematically varying multiple pacing sites and physiologically varied conduction properties across 147 patient-specific geometries derived from two independent clinical cohorts. All anatomical and functional data are expressed in a Universal Atrium Coordinate system, providing a consistent representation that decouples electrophysiological patterns from mesh topology. Within this coordinate space, we designed a neural operator with a vision-transformer backbone to learn the mapping from structural and electrophysiological inputs to LAT fields. With a mean prediction error of 5.1 ms over a 455 ms maximum simulation time, the model outperforms established operator-learning approaches and performs inference in 0.12 ms per sample. Our framework establishes a general strategy for learning domain-invariant biophysical mappings across variable anatomical domains and enables integration of computational electrophysiology into real-time and large-scale clinical workflows.</li>
</ul>

<h3>Title: StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos</h3>
<ul>
<li><strong>Authors: </strong>Daeun Lee, Subhojyoti Mukherjee, Branislav Kveton, Ryan A. Rossi, Viet Dac Lai, Seunghyun Yoon, Trung Bui, Franck Dernoncourt, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01707">https://arxiv.org/abs/2512.01707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01707">https://arxiv.org/pdf/2512.01707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01707]] StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos(https://arxiv.org/abs/2512.01707)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.</li>
</ul>

<h3>Title: MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications</h3>
<ul>
<li><strong>Authors: </strong>Stefano Zeppieri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01710">https://arxiv.org/abs/2512.01710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01710">https://arxiv.org/pdf/2512.01710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01710]] MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications(https://arxiv.org/abs/2512.01710)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting layers: conversational, long-term user, episodic and event-linked, sensory and context-aware, and short-term working memory. Drawing inspiration from cognitive psychology, we map these layers to technical components and outline strategies for coordination, prioritization, and conflict resolution. We demonstrate the approach through its implementation in the Heero conversational agent, where encrypted long-term bios and conversational history already improve engagement and retention. We further discuss implementation concerns around storage, retrieval, privacy, and latency, and highlight open challenges. MMAG provides a foundation for building memory-rich language agents that are more coherent, proactive, and aligned with human needs.</li>
</ul>

<h3>Title: Beware of Reasoning Overconfidence: Pitfalls in the Reasoning Process for Multi-solution Tasks</h3>
<ul>
<li><strong>Authors: </strong>Jiannan Guan, Qiguang Chen, Libo Qin, Dengyun Peng, Jinhao Liu, Liangyu Huo, Jian Xie, Wanxiang Che</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01725">https://arxiv.org/abs/2512.01725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01725">https://arxiv.org/pdf/2512.01725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01725]] Beware of Reasoning Overconfidence: Pitfalls in the Reasoning Process for Multi-solution Tasks(https://arxiv.org/abs/2512.01725)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel in reasoning tasks requiring a single correct answer, but they perform poorly in multi-solution tasks that require generating comprehensive and diverse answers. We attribute this limitation to \textbf{reasoning overconfidence}: a tendency to express undue certainty in an incomplete solution set. To examine the effect, we introduce \textit{MuSoBench}, a benchmark of multi-solution problems. Experiments show that the conventional short chain-of-thought (Short-CoT) prompting paradigm exhibits pronounced overconfidence, whereas the emerging long chain-of-thought (Long-CoT) approach mitigates it through iterative exploration and self-reflection. We further characterise observable behaviours and influential factors. To probe the underlying cause, we propose the \textbf{cognitive-rigidity hypothesis}, which posits that overconfidence arises when the reasoning process prematurely converges on a narrow set of thought paths. An attention-entropy analysis offers preliminary support for this view. These findings provide tools for assessing the completeness of LLM reasoning and highlight the need to move evaluation beyond single-answer accuracy toward comprehensive exploration.</li>
</ul>

<h3>Title: AI-Driven Cybersecurity Testbed for Nuclear Infrastructure: Comprehensive Evaluation Using METL Operational Data</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Blakely, Yeni Li, Akshay Dave, Derek Kultgen, Rick Vilim</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01727">https://arxiv.org/abs/2512.01727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01727">https://arxiv.org/pdf/2512.01727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01727]] AI-Driven Cybersecurity Testbed for Nuclear Infrastructure: Comprehensive Evaluation Using METL Operational Data(https://arxiv.org/abs/2512.01727)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>Advanced nuclear reactor systems face increasing cybersecurity threats as sophisticated attackers exploit cyber-physical interfaces to manipulate control systems while evading traditional IT security measures. This research presents a comprehensive evaluation of artificial intelligence approaches for cybersecurity protection in nuclear infrastructure, using Argonne National Laboratory's Mechanisms Engineering Test Loop (METL) as an experimental platform. We developed a systematic evaluation framework encompassing four machine learning detection paradigms: Change Point Detection, LSTM-based Anomaly Detection, Dependency Violation analysis, and Autoencoder reconstruction methods. Our comprehensive attack taxonomy includes 15 distinct scenarios targeting reactor control systems, each implemented across five severity tiers to evaluate detection performance under varying attack intensities. The experimental evaluation encompassed 300 rigorous experiments using realistic METL operational data. Change Point Detection emerged as the leading approach with mean AUC performance of 0.785, followed by LSTM Anomaly Detection (0.636), Dependency Violation (0.621), and Autoencoder methods (0.580). Attack detectability varied significantly, with multi-site coordinated attacks proving most detectable (AUC = 0.739) while precision trust decay attacks presented the greatest detection challenge (AUC = 0.592). This work delivers practical performance benchmarks and reference architecture that advance AI-based cybersecurity capabilities for critical nuclear infrastructure, providing essential foundations for operational deployment and enhanced threat response in cyber-physical systems.</li>
</ul>

<h3>Title: Beyond Scaffold: A Unified Spatio-Temporal Gradient Tracking Method</h3>
<ul>
<li><strong>Authors: </strong>Yan Huang, Jinming Xu, Jiming Chen, Karl Henrik Johansson</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01732">https://arxiv.org/abs/2512.01732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01732">https://arxiv.org/pdf/2512.01732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01732]] Beyond Scaffold: A Unified Spatio-Temporal Gradient Tracking Method(https://arxiv.org/abs/2512.01732)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>In distributed and federated learning algorithms, communication overhead is often reduced by performing multiple local updates between communication rounds. However, due to data heterogeneity across nodes and the local gradient noise within each node, this strategy can lead to the drift of local models away from the global optimum. To address this issue, we revisit the well-known federated learning method Scaffold (Karimireddy et al., 2020) under a gradient tracking perspective, and propose a unified spatio-temporal gradient tracking algorithm, termed ST-GT, for distributed stochastic optimization over time-varying graphs. ST-GT tracks the global gradient across neighboring nodes to mitigate data heterogeneity, while maintaining a running average of local gradients to substantially suppress noise, with slightly more storage overhead. Without assuming bounded data heterogeneity, we prove that ST-GT attains a linear convergence rate for strongly convex problems and a sublinear rate for nonconvex cases. Notably, ST-GT achieves the first linear speed-up in communication complexity with respect to the number of local updates per round $\tau$ for the strongly-convex setting. Compared to traditional gradient tracking methods, ST-GT reduces the topology-dependent noise term from $\sigma^2$ to $\sigma^2/\tau$, where $\sigma^2$ denotes the noise level, thereby improving communication efficiency.</li>
</ul>

<h3>Title: Automating modeling in mechanics: LLMs as designers of physics-constrained neural networks for constitutive modeling of materials</h3>
<ul>
<li><strong>Authors: </strong>Marius Tacke, Matthias Busch, Kian Abdolazizi, Jonas Eichinger, Kevin Linka, Christian Cyron, Roland Aydin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01735">https://arxiv.org/abs/2512.01735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01735">https://arxiv.org/pdf/2512.01735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01735]] Automating modeling in mechanics: LLMs as designers of physics-constrained neural networks for constitutive modeling of materials(https://arxiv.org/abs/2512.01735)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM)-based agentic frameworks increasingly adopt the paradigm of dynamically generating task-specific agents. We suggest that not only agents but also specialized software modules for scientific and engineering tasks can be generated on demand. We demonstrate this concept in the field of solid mechanics. There, so-called constitutive models are required to describe the relationship between mechanical stress and body deformation. Constitutive models are essential for both the scientific understanding and industrial application of materials. However, even recent data-driven methods of constitutive modeling, such as constitutive artificial neural networks (CANNs), still require substantial expert knowledge and human labor. We present a framework in which an LLM generates a CANN on demand, tailored to a given material class and dataset provided by the user. The framework covers LLM-based architecture selection, integration of physical constraints, and complete code generation. Evaluation on three benchmark problems demonstrates that LLM-generated CANNs achieve accuracy comparable to or greater than manually engineered counterparts, while also exhibiting reliable generalization to unseen loading scenarios and extrapolation to large deformations. These findings indicate that LLM-based generation of physics-constrained neural networks can substantially reduce the expertise required for constitutive modeling and represent a step toward practical end-to-end automation.</li>
</ul>

<h3>Title: MSPT: Efficient Large-Scale Physical Modeling via Parallelized Multi-Scale Attention</h3>
<ul>
<li><strong>Authors: </strong>Pedro M. P. Curvo, Jan-Willem van de Meent, Maksim Zhdanov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01738">https://arxiv.org/abs/2512.01738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01738">https://arxiv.org/pdf/2512.01738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01738]] MSPT: Efficient Large-Scale Physical Modeling via Parallelized Multi-Scale Attention(https://arxiv.org/abs/2512.01738)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>A key scalability challenge in neural solvers for industrial-scale physics simulations is efficiently capturing both fine-grained local interactions and long-range global dependencies across millions of spatial elements. We introduce the Multi-Scale Patch Transformer (MSPT), an architecture that combines local point attention within patches with global attention to coarse patch-level representations. To partition the input domain into spatially-coherent patches, we employ ball trees, which handle irregular geometries efficiently. This dual-scale design enables MSPT to scale to millions of points on a single GPU. We validate our method on standard PDE benchmarks (elasticity, plasticity, fluid dynamics, porous flow) and large-scale aerodynamic datasets (ShapeNet-Car, Ahmed-ML), achieving state-of-the-art accuracy with substantially lower memory footprint and computational cost.</li>
</ul>

<h3>Title: SA-ADP: Sensitivity-Aware Adaptive Differential Privacy for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Stella Etuk, Ashraf Matrawy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01748">https://arxiv.org/abs/2512.01748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01748">https://arxiv.org/pdf/2512.01748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01748]] SA-ADP: Sensitivity-Aware Adaptive Differential Privacy for Large Language Models(https://arxiv.org/abs/2512.01748)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, large language model</a></li>
<li><strong>Abstract: </strong>Despite advances in the use of large language models (LLMs) in downstream tasks, their ability to memorize information has raised privacy concerns. Therefore, protecting personally identifiable information (PII) during LLM training remains a fundamental challenge. Conventional methods like Differential Privacy-Stochastic Gradient Descent (DP-SGD) provide robust privacy protection via uniform noising, protecting PII regardless of its distinct sensitivity. This comes at the expense of the model's utility, leading to a trade-off. In this paper, we propose SA-ADP, a sensitivity-aware approach that allocates noise based on the sensitivity of individual PII. We evaluated our method on four datasets (ABCD, CUSTOMERSIM, Wikitext-2, and UNSW-NB15 ). Our results show that SA-ADP achieves results comparable to the baseline (No-DP) and the conventional DP-SGD. This means that our method did not degrade the model's utility while still maintaining strong privacy protection.</li>
</ul>

<h3>Title: FreqEdit: Preserving High-Frequency Features for Robust Multi-Turn Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Liao, Jiajun Liang, Kaiqian Cui, Baoquan Zhao, Haoran Xie, Wei Liu, Qing Li, Xudong Mao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01755">https://arxiv.org/abs/2512.01755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01755">https://arxiv.org/pdf/2512.01755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01755]] FreqEdit: Preserving High-Frequency Features for Robust Multi-Turn Image Editing(https://arxiv.org/abs/2512.01755)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Instruction-based image editing through natural language has emerged as a powerful paradigm for intuitive visual manipulation. While recent models achieve impressive results on single edits, they suffer from severe quality degradation under multi-turn editing. Through systematic analysis, we identify progressive loss of high-frequency information as the primary cause of this quality degradation. We present FreqEdit, a training-free framework that enables stable editing across 10+ consecutive iterations. Our approach comprises three synergistic components: (1) high-frequency feature injection from reference velocity fields to preserve fine-grained details, (2) an adaptive injection strategy that spatially modulates injection strength for precise region-specific control, and (3) a path compensation mechanism that periodically recalibrates the editing trajectory to prevent over-constraint. Extensive experiments demonstrate that FreqEdit achieves superior performance in both identity preservation and instruction following compared to seven state-of-the-art baselines.</li>
</ul>

<h3>Title: Mofasa: A Step Change in Metal-Organic Framework Generation</h3>
<ul>
<li><strong>Authors: </strong>Vaidotas Simkus, Anders Christensen, Steven Bennett, Ian Johnson, Mark Neumann, James Gin, Jonathan Godwin, Benjamin Rhodes</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01756">https://arxiv.org/abs/2512.01756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01756">https://arxiv.org/pdf/2512.01756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01756]] Mofasa: A Step Change in Metal-Organic Framework Generation(https://arxiv.org/abs/2512.01756)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Mofasa is an all-atom latent diffusion model with state-of-the-art performance for generating Metal-Organic Frameworks (MOFs). These are highly porous crystalline materials used to harvest water from desert air, capture carbon dioxide, store toxic gases and catalyse chemical reactions. In recognition of their value, the development of MOFs recently received a Nobel Prize in Chemistry. In many ways, MOFs are well-suited for exploiting generative models in chemistry: they are rationally-designable materials with a large combinatorial design space and strong structure-property couplings. And yet, to date, a high performance generative model has been lacking. To fill this gap, we introduce Mofasa, a general-purpose latent diffusion model that jointly samples positions, atom-types and lattice vectors for systems as large as 500 atoms. Mofasa avoids handcrafted assembly algorithms common in the literature, unlocking the simultaneous discovery of metal nodes, linkers and topologies. To help the scientific community build on our work, we release MofasaDB, an annotated library of hundreds of thousands of sampled MOF structures, along with a user-friendly web interface for search and discovery: this https URL .</li>
</ul>

<h3>Title: Weight Space Representation Learning with Neural Fields</h3>
<ul>
<li><strong>Authors: </strong>Zhuoqian Yang, Mathieu Salzmann, Sabine Süsstrunk</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01759">https://arxiv.org/abs/2512.01759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01759">https://arxiv.org/pdf/2512.01759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01759]] Weight Space Representation Learning with Neural Fields(https://arxiv.org/abs/2512.01759)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this work, we investigate the potential of weights to serve as effective representations, focusing on neural fields. Our key insight is that constraining the optimization space through a pre-trained base model and low-rank adaptation (LoRA) can induce structure in weight space. Across reconstruction, generation, and analysis tasks on 2D and 3D data, we find that multiplicative LoRA weights achieve high representation quality while exhibiting distinctiveness and semantic structure. When used with latent diffusion models, multiplicative LoRA weights enable higher-quality generation than existing weight-space methods.</li>
</ul>

<h3>Title: On the Unreasonable Effectiveness of Last-layer Retraining</h3>
<ul>
<li><strong>Authors: </strong>John C. Hill, Tyler LaBonte, Xinchen Zhang, Vidya Muthukumar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01766">https://arxiv.org/abs/2512.01766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01766">https://arxiv.org/pdf/2512.01766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01766]] On the Unreasonable Effectiveness of Last-layer Retraining(https://arxiv.org/abs/2512.01766)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Last-layer retraining (LLR) methods -- wherein the last layer of a neural network is reinitialized and retrained on a held-out set following ERM training -- have garnered interest as an efficient approach to rectify dependence on spurious correlations and improve performance on minority groups. Surprisingly, LLR has been found to improve worst-group accuracy even when the held-out set is an imbalanced subset of the training set. We initially hypothesize that this ``unreasonable effectiveness'' of LLR is explained by its ability to mitigate neural collapse through the held-out set, resulting in the implicit bias of gradient descent benefiting robustness. Our empirical investigation does not support this hypothesis. Instead, we present strong evidence for an alternative hypothesis: that the success of LLR is primarily due to better group balance in the held-out set. We conclude by showing how the recent algorithms CB-LLR and AFR perform implicit group-balancing to elicit a robustness improvement.</li>
</ul>

<h3>Title: VideoScoop: A Non-Traditional Domain-Independent Framework For Video Analysis</h3>
<ul>
<li><strong>Authors: </strong>Hafsa Billah</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01769">https://arxiv.org/abs/2512.01769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01769">https://arxiv.org/pdf/2512.01769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01769]] VideoScoop: A Non-Traditional Domain-Independent Framework For Video Analysis(https://arxiv.org/abs/2512.01769)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Automatically understanding video contents is important for several applications in Civic Monitoring (CM), general Surveillance (SL), Assisted Living (AL), etc. Decades of Image and Video Analysis (IVA) research have advanced tasks such as content extraction (e.g., object recognition and tracking). Identifying meaningful activities or situations (e.g., two objects coming closer) remains difficult and cannot be achieved by content extraction alone. Currently, Video Situation Analysis (VSA) is done manually with a human in the loop, which is error-prone and labor-intensive, or through custom algorithms designed for specific video types or situations. These algorithms are not general-purpose and require a new algorithm/software for each new situation or video from a new domain. This report proposes a general-purpose VSA framework that overcomes the above limitations. Video contents are extracted once using state-of-the-art Video Content Extraction technologies. They are represented using two alternative models -- the extended relational model (R++) and graph models. When represented using R++, the extracted contents can be used as data streams, enabling Continuous Query Processing via the proposed Continuous Query Language for Video Analysis. The graph models complement this by enabling the detection of situations that are difficult or impossible to detect using the relational model alone. Existing graph algorithms and newly developed algorithms support a wide variety of situation detection. To support domain independence, primitive situation variants across domains are identified and expressed as parameterized templates. Extensive experiments were conducted across several interesting situations from three domains -- AL, CM, and SL-- to evaluate the accuracy, efficiency, and robustness of the proposed approach using a dataset of videos of varying lengths from these domains.</li>
</ul>

<h3>Title: Robust Rigid and Non-Rigid Medical Image Registration Using Learnable Edge Kernels</h3>
<ul>
<li><strong>Authors: </strong>Ahsan Raza Siyal, Markus Haltmeier, Ruth Steiger, Malik Galijasevic, Elke Ruth Gizewski, Astrid Ellen Grams</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01771">https://arxiv.org/abs/2512.01771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01771">https://arxiv.org/pdf/2512.01771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01771]] Robust Rigid and Non-Rigid Medical Image Registration Using Learnable Edge Kernels(https://arxiv.org/abs/2512.01771)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Medical image registration is crucial for various clinical and research applications including disease diagnosis or treatment planning which require alignment of images from different modalities, time points, or subjects. Traditional registration techniques often struggle with challenges such as contrast differences, spatial distortions, and modality-specific variations. To address these limitations, we propose a method that integrates learnable edge kernels with learning-based rigid and non-rigid registration techniques. Unlike conventional layers that learn all features without specific bias, our approach begins with a predefined edge detection kernel, which is then perturbed with random noise. These kernels are learned during training to extract optimal edge features tailored to the task. This adaptive edge detection enhances the registration process by capturing diverse structural features critical in medical imaging. To provide clearer insight into the contribution of each component in our design, we introduce four variant models for rigid registration and four variant models for non-rigid registration. We evaluated our approach using a dataset provided by the Medical University across three setups: rigid registration without skull removal, with skull removal, and non-rigid registration. Additionally, we assessed performance on two publicly available datasets. Across all experiments, our method consistently outperformed state-of-the-art techniques, demonstrating its potential to improve multi-modal image alignment and anatomical structure analysis.</li>
</ul>

<h3>Title: Evaluating SAM2 for Video Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Syed Hesham Syed Ariff, Yun Liu, Guolei Sun, Jing Yang, Henghui Ding, Xue Geng, Xudong Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01774">https://arxiv.org/abs/2512.01774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01774">https://arxiv.org/pdf/2512.01774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01774]] Evaluating SAM2 for Video Semantic Segmentation(https://arxiv.org/abs/2512.01774)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The Segmentation Anything Model 2 (SAM2) has proven to be a powerful foundation model for promptable visual object segmentation in both images and videos, capable of storing object-aware memories and transferring them temporally through memory blocks. While SAM2 excels in video object segmentation by providing dense segmentation masks based on prompts, extending it to dense Video Semantic Segmentation (VSS) poses challenges due to the need for spatial accuracy, temporal consistency, and the ability to track multiple objects with complex boundaries and varying scales. This paper explores the extension of SAM2 for VSS, focusing on two primary approaches and highlighting firsthand observations and common challenges faced during this process. The first approach involves using SAM2 to extract unique objects as masks from a given image, with a segmentation network employed in parallel to generate and refine initial predictions. The second approach utilizes the predicted masks to extract unique feature vectors, which are then fed into a simple network for classification. The resulting classifications and masks are subsequently combined to produce the final segmentation. Our experiments suggest that leveraging SAM2 enhances overall performance in VSS, primarily due to its precise predictions of object boundaries.</li>
</ul>

<h3>Title: How Does RL Post-training Induce Skill Composition? A Case Study on Countdown</h3>
<ul>
<li><strong>Authors: </strong>Simon Park, Simran Kaur, Sanjeev Arora</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01775">https://arxiv.org/abs/2512.01775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01775">https://arxiv.org/pdf/2512.01775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01775]] How Does RL Post-training Induce Skill Composition? A Case Study on Countdown(https://arxiv.org/abs/2512.01775)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While reinforcement learning (RL) successfully enhances reasoning in large language models, its role in fostering compositional generalization (the ability to synthesize novel skills from known components) is often conflated with mere length generalization. To this end, we study what RL post-training teaches about skill composition and how the structure of the composition affects the skill transfer. We focus on the Countdown task (given n numbers and a target, form an expression that evaluates to the target) and analyze model solutions as expression trees, where each subtree corresponds to a reusable subtask and thus can be viewed as a ``skill.'' Tracking tree shapes and their success rates over training, we find: (i) out-of-distribution (OOD) generalization to larger n and to unseen tree shapes, indicating compositional reuse of subtasks; (ii) a structure-dependent hierarchy of learnability -- models master shallow balanced trees (workload is balanced between subtasks) before deep unbalanced ones, with persistent fragility on right-heavy structures (even when the composition depth is the same as some left-heavy structures). Our diagnostic reveals what is learned, in what order, and where generalization fails, clarifying how RL-only post-training induces OOD generalization beyond what standard metrics such as pass@k reveal.</li>
</ul>

<h3>Title: Dual Randomized Smoothing: Beyond Global Noise Variance</h3>
<ul>
<li><strong>Authors: </strong>Chenhao Sun, Yuhao Mao, Martin Vechev</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01782">https://arxiv.org/abs/2512.01782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01782">https://arxiv.org/pdf/2512.01782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01782]] Dual Randomized Smoothing: Beyond Global Noise Variance(https://arxiv.org/abs/2512.01782)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Randomized Smoothing (RS) is a prominent technique for certifying the robustness of neural networks against adversarial perturbations. With RS, achieving high accuracy at small radii requires a small noise variance, while achieving high accuracy at large radii requires a large noise variance. However, the global noise variance used in the standard RS formulation leads to a fundamental limitation: there exists no global noise variance that simultaneously achieves strong performance at both small and large radii. To break through the global variance limitation, we propose a dual RS framework which enables input-dependent noise variances. To achieve that, we first prove that RS remains valid with input-dependent noise variances, provided the variance is locally constant around each input. Building on this result, we introduce two components which form our dual RS framework: (i) a variance estimator first predicts an optimal noise variance for each input, (ii) this estimated variance is then used by a standard RS classifier. The variance estimator is independently smoothed via RS to ensure local constancy, enabling flexible design. We also introduce training strategies to iteratively optimize the two components. Extensive experiments on CIFAR-10 show that our dual RS method provides strong performance for both small and large radii-unattainable with global noise variance-while incurring only a 60% computational overhead at inference. Moreover, it consistently outperforms prior input-dependent noise approaches across most radii, with particularly large gains at radii 0.5, 0.75, and 1.0, achieving relative improvements of 19%, 24%, and 21%, respectively. On ImageNet, dual RS remains effective across all radii. Additionally, the dual RS framework naturally provides a routing perspective for certified robustness, improving the accuracy-robustness trade-off with off-the-shelf expert RS models.</li>
</ul>

<h3>Title: Learned Image Compression for Earth Observation: Implications for Downstream Segmentation Tasks</h3>
<ul>
<li><strong>Authors: </strong>Christian Mollière, Iker Cumplido, Marco Zeulner, Lukas Liesenhoff, Matthias Schubert, Julia Gottfriedsen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01788">https://arxiv.org/abs/2512.01788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01788">https://arxiv.org/pdf/2512.01788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01788]] Learned Image Compression for Earth Observation: Implications for Downstream Segmentation Tasks(https://arxiv.org/abs/2512.01788)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The rapid growth of data from satellite-based Earth observation (EO) systems poses significant challenges in data transmission and storage. We evaluate the potential of task-specific learned compression algorithms in this context to reduce data volumes while retaining crucial information. In detail, we compare traditional compression (JPEG 2000) versus a learned compression approach (Discretized Mixed Gaussian Likelihood) on three EO segmentation tasks: Fire, cloud, and building detection. Learned compression notably outperforms JPEG 2000 for large-scale, multi-channel optical imagery in both reconstruction quality (PSNR) and segmentation accuracy. However, traditional codecs remain competitive on smaller, single-channel thermal infrared datasets due to limited data and architectural constraints. Additionally, joint end-to-end optimization of compression and segmentation models does not improve performance over standalone optimization.</li>
</ul>

<h3>Title: Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos</h3>
<ul>
<li><strong>Authors: </strong>Xavier Thomas, Youngsun Lim, Ananya Srinivasan, Audrey Zheng, Deepti Ghadiyaram</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01803">https://arxiv.org/abs/2512.01803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01803">https://arxiv.org/pdf/2512.01803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01803]] Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos(https://arxiv.org/abs/2512.01803)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Despite rapid advances in video generative models, robust metrics for evaluating visual and temporal correctness of complex human actions remain elusive. Critically, existing pure-vision encoders and Multimodal Large Language Models (MLLMs) are strongly appearance-biased, lack temporal understanding, and thus struggle to discern intricate motion dynamics and anatomical implausibilities in generated videos. We tackle this gap by introducing a novel evaluation metric derived from a learned latent space of real-world human actions. Our method first captures the nuances, constraints, and temporal smoothness of real-world motion by fusing appearance-agnostic human skeletal geometry features with appearance-based features. We posit that this combined feature space provides a robust representation of action plausibility. Given a generated video, our metric quantifies its action quality by measuring the distance between its underlying representations and this learned real-world action distribution. For rigorous validation, we develop a new multi-faceted benchmark specifically designed to probe temporally challenging aspects of human action fidelity. Through extensive experiments, we show that our metric achieves substantial improvement of more than 68% compared to existing state-of-the-art methods on our benchmark, performs competitively on established external benchmarks, and has a stronger correlation with human perception. Our in-depth analysis reveals critical limitations in current video generative models and establishes a new standard for advanced research in video generation.</li>
</ul>

<h3>Title: DeepCAVE: A Visualization and Analysis Tool for Automated Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Sarah Segel, Helena Graf, Edward Bergman, Kristina Thieme, Marcel Wever, Alexander Tornede, Frank Hutter, Marius Lindauer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01810">https://arxiv.org/abs/2512.01810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01810">https://arxiv.org/pdf/2512.01810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01810]] DeepCAVE: A Visualization and Analysis Tool for Automated Machine Learning(https://arxiv.org/abs/2512.01810)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Hyperparameter optimization (HPO), as a central paradigm of AutoML, is crucial for leveraging the full potential of machine learning (ML) models; yet its complexity poses challenges in understanding and debugging the optimization process. We present DeepCAVE, a tool for interactive visualization and analysis, providing insights into HPO. Through an interactive dashboard, researchers, data scientists, and ML engineers can explore various aspects of the HPO process and identify issues, untouched potentials, and new insights about the ML model being tuned. By empowering users with actionable insights, DeepCAVE contributes to the interpretability of HPO and ML on a design level and aims to foster the development of more robust and efficient methodologies in the future.</li>
</ul>

<h3>Title: Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling</h3>
<ul>
<li><strong>Authors: </strong>Meng Cao, Haokun Lin, Haoyuan Li, Haoran Tang, Rongtao Xu, Dong An, Xue Liu, Ian Reid, Xiaodan Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01821">https://arxiv.org/abs/2512.01821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01821">https://arxiv.org/pdf/2512.01821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01821]] Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling(https://arxiv.org/abs/2512.01821)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM's symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.</li>
</ul>

<h3>Title: InnoGym: Benchmarking the Innovation Potential of AI Agents</h3>
<ul>
<li><strong>Authors: </strong>Jintian Zhang, Kewei Xu, Jingsheng Zheng, Zhuoyun Yu, Yuqi Zhu, Yujie Luo, Lanning Wei, Shuofei Qiao, Lun Du, Da Zheng, Shumin Deng, Huajun Chen, Ningyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01822">https://arxiv.org/abs/2512.01822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01822">https://arxiv.org/pdf/2512.01822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01822]] InnoGym: Benchmarking the Innovation Potential of AI Agents(https://arxiv.org/abs/2512.01822)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.</li>
</ul>

<h3>Title: OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic</h3>
<ul>
<li><strong>Authors: </strong>Songyan Zhang, Wenhui Huang, Zhan Chen, Chua Jiahao Collister, Qihang Huang, Chen Lv</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01830">https://arxiv.org/abs/2512.01830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01830">https://arxiv.org/pdf/2512.01830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01830]] OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic(https://arxiv.org/abs/2512.01830)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, two-stage fine-tuning strategies, e.g., acquiring essential driving knowledge through supervised fine-tuning (SFT) and further enhancing decision-making and planning via reinforcement fine-tuning (RFT), have shown strong potential in advancing the knowledge-driven autonomous driving (AD) paradigm. However, the learning nature of SFT still limits the generalization of reasoning, thereby constraining the full potential of driving performance. Meanwhile, current RFT approaches are primarily applied to downstream tasks, since scene understanding is an open-ended problem where corresponding rewards are difficult to quantify. To address these limitations, we propose OpenREAD, an OPEN-ended REasoning reinforced vision-language model (VLM)-based autonomous driving (AD) framework that enables end-to-end RFT across the full spectrum from high-level reasoning to low-level trajectory planning. Specifically, we begin by constructing large-scale Chain-of-Thought (CoT) annotations on open-source driving-related knowledge datasets, and employ the powerful Qwen3 large language model (LLM) as the critic in RFT to quantify reasoning quality for open-ended questions during reward modeling. Extensive experiments confirm that joint end-to-end RFT yields substantial improvements in both upstream and downstream tasks, enabling OpenREAD to achieve state-of-the-art performance on reasoning and planning benchmarks.</li>
</ul>

<h3>Title: Deconstructing Generative Diversity: An Information Bottleneck Analysis of Discrete Latent Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Yudi Wu, Wenhao Zhao, Dianbo Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01831">https://arxiv.org/abs/2512.01831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01831">https://arxiv.org/pdf/2512.01831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01831]] Deconstructing Generative Diversity: An Information Bottleneck Analysis of Discrete Latent Generative Models(https://arxiv.org/abs/2512.01831)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative diversity varies significantly across discrete latent generative models such as AR, MIM, and Diffusion. We propose a diagnostic framework, grounded in Information Bottleneck (IB) theory, to analyze the underlying strategies resolving this behavior. The framework models generation as a conflict between a 'Compression Pressure' - a drive to minimize overall codebook entropy - and a 'Diversity Pressure' - a drive to maximize conditional entropy given an input. We further decompose this diversity into two primary sources: 'Path Diversity', representing the choice of high-level generative strategies, and 'Execution Diversity', the randomness in executing a chosen strategy. To make this decomposition operational, we introduce three zero-shot, inference-time interventions that directly perturb the latent generative process and reveal how models allocate and express diversity. Application of this probe-based framework to representative AR, MIM, and Diffusion systems reveals three distinct strategies: "Diversity-Prioritized" (MIM), "Compression-Prioritized" (AR), and "Decoupled" (Diffusion). Our analysis provides a principled explanation for their behavioral differences and informs a novel inference-time diversity enhancement technique.</li>
</ul>

<h3>Title: A Privacy-Preserving Information-Sharing Protocol for Federated Authentication</h3>
<ul>
<li><strong>Authors: </strong>Francesco Buccafurri, Carmen Licciardi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01832">https://arxiv.org/abs/2512.01832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01832">https://arxiv.org/pdf/2512.01832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01832]] A Privacy-Preserving Information-Sharing Protocol for Federated Authentication(https://arxiv.org/abs/2512.01832)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>This paper presents a privacy-preserving protocol for identity registration and information sharing in federated authentication systems. The goal is to enable Identity Providers (IdPs) to detect duplicate or fraudulent identity enrollments without revealing users personal data or enabling cross-domain correlation. The protocol relies on Oblivious Pseudorandom Functions (OPRFs) combined with domain-specific transformations, ensuring that each IdP generates independent pseudonymous identifiers derived from a shared cryptographic service while maintaining full input confidentiality. A central authority maintains a blind registry that records successful and failed identity verifications using only pseudonymous identifiers, allowing global consistency checks without exposing sensitive information or linking users across domains. The proposed construction provides a general and abstract framework suitable for a wide range of federated authentication systems, achieving strong privacy guarantees while supporting effective fraud-prevention mechanisms during identity registration.</li>
</ul>

<h3>Title: Mitigating Gender Bias in Depression Detection via Counterfactual Inference</h3>
<ul>
<li><strong>Authors: </strong>Mingxuan Hu, Hongbo Ma, Xinlan Wu, Ziqi Liu, Jiaqi Liu, Yangbin Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01834">https://arxiv.org/abs/2512.01834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01834">https://arxiv.org/pdf/2512.01834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01834]] Mitigating Gender Bias in Depression Detection via Counterfactual Inference(https://arxiv.org/abs/2512.01834)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Audio-based depression detection models have demonstrated promising performance but often suffer from gender bias due to imbalanced training data. Epidemiological statistics show a higher prevalence of depression in females, leading models to learn spurious correlations between gender and depression. Consequently, models tend to over-diagnose female patients while underperforming on male patients, raising significant fairness concerns. To address this, we propose a novel Counterfactual Debiasing Framework grounded in causal inference. We construct a causal graph to model the decision-making process and identify gender bias as the direct causal effect of gender on the prediction. During inference, we employ counterfactual inference to estimate and subtract this direct effect, ensuring the model relies primarily on authentic acoustic pathological features. Extensive experiments on the DAIC-WOZ dataset using two advanced acoustic backbones demonstrate that our framework not only significantly reduces gender bias but also improves overall detection performance compared to existing debiasing strategies.</li>
</ul>

<h3>Title: Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability</h3>
<ul>
<li><strong>Authors: </strong>Jinghan Jia, Nathalie Baracaldo, Sijia Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01848">https://arxiv.org/abs/2512.01848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01848">https://arxiv.org/pdf/2512.01848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01848]] Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability(https://arxiv.org/abs/2512.01848)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large reasoning models (LRMs) extend large language models by generating explicit chain-of-thought (CoT) reasoning, significantly improving mathematical and logical problem solving. However, this explicit reasoning process also introduces new safety risks, as unsafe behaviors often emerge within intermediate reasoning trajectories, even when final answers appear harmless. Existing safety alignment approaches primarily rely on supervised fine-tuning (SFT) over safety-oriented long CoT datasets. While intuitive, we find that SFT produces inconsistent safety improvements, degrades reasoning ability, and generalizes poorly across model families. These limitations suggest that purely supervised approaches are insufficient for robust safety alignment in LRMs. To address this, we investigate reinforcement learning (RL) as a complementary optimization framework for LRM safety training. Unlike SFT, RL directly optimizes model policies with reward feedback, enabling more adaptive and stable alignment. Extensive experiments across multiple model families and benchmarks show that RL achieves stronger and more consistent safety gains while maintaining reasoning competence. Further analysis of reflection dynamics and token-level entropy reveals that RL suppresses unsafe exploratory reasoning while preserving reflective depth, leading to safer and more reliable reasoning processes.</li>
</ul>

<h3>Title: BHRAM-IL: A Benchmark for Hallucination Recognition and Assessment in Multiple Indian Languages</h3>
<ul>
<li><strong>Authors: </strong>Hrishikesh Terdalkar, Kirtan Bhojani, Aryan Dongare, Omm Aditya Behera</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01852">https://arxiv.org/abs/2512.01852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01852">https://arxiv.org/pdf/2512.01852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01852]] BHRAM-IL: A Benchmark for Hallucination Recognition and Assessment in Multiple Indian Languages(https://arxiv.org/abs/2512.01852)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly deployed in multilingual applications but often generate plausible yet incorrect or misleading outputs, known as hallucinations. While hallucination detection has been studied extensively in English, under-resourced Indian languages remain largely unexplored. We present BHRAM-IL, a benchmark for hallucination recognition and assessment in multiple Indian languages, covering Hindi, Gujarati, Marathi, Odia, along with English. The benchmark comprises 36,047 curated questions across nine categories spanning factual, numerical, reasoning, and linguistic tasks. We evaluate 14 state-of-the-art multilingual LLMs on a benchmark subset of 10,265 questions, analyzing cross-lingual and factual hallucinations across languages, models, scales, categories, and domains using category-specific metrics normalized to (0,1) range. Aggregation over all categories and models yields a primary score of 0.23 and a language-corrected fuzzy score of 0.385, demonstrating the usefulness of BHRAM-IL for hallucination-focused evaluation. The dataset, and the code for generation and evaluation are available on GitHub (this https URL) and HuggingFace (this https URL) to support future research in multilingual hallucination detection and mitigation.</li>
</ul>

<h3>Title: COACH: Collaborative Agents for Contextual Highlighting - A Multi-Agent Framework for Sports Video Analysis</h3>
<ul>
<li><strong>Authors: </strong>Tsz-To Wong, Ching-Chun Huang, Hong-Han Shuai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01853">https://arxiv.org/abs/2512.01853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01853">https://arxiv.org/pdf/2512.01853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01853]] COACH: Collaborative Agents for Contextual Highlighting - A Multi-Agent Framework for Sports Video Analysis(https://arxiv.org/abs/2512.01853)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, generative</a></li>
<li><strong>Abstract: </strong>Intelligent sports video analysis demands a comprehensive understanding of temporal context, from micro-level actions to macro-level game strategies. Existing end-to-end models often struggle with this temporal hierarchy, offering solutions that lack generalization, incur high development costs for new tasks, and suffer from poor interpretability. To overcome these limitations, we propose a reconfigurable Multi-Agent System (MAS) as a foundational framework for sports video understanding. In our system, each agent functions as a distinct "cognitive tool" specializing in a specific aspect of analysis. The system's architecture is not confined to a single temporal dimension or task. By leveraging iterative invocation and flexible composition of these agents, our framework can construct adaptive pipelines for both short-term analytic reasoning (e.g., Rally QA) and long-term generative summarization (e.g., match summaries). We demonstrate the adaptability of this framework using two representative tasks in badminton analysis, showcasing its ability to bridge fine-grained event detection and global semantic organization. This work presents a paradigm shift towards a flexible, scalable, and interpretable system for robust, cross-task sports video this http URL project homepage is available at this https URL</li>
</ul>

<h3>Title: Cross-Lingual Interleaving for Speech Language Models</h3>
<ul>
<li><strong>Authors: </strong>Adel Moumen, Guangzhi Sun, Philip C. Woodland</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01865">https://arxiv.org/abs/2512.01865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01865">https://arxiv.org/pdf/2512.01865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01865]] Cross-Lingual Interleaving for Speech Language Models(https://arxiv.org/abs/2512.01865)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Spoken Language Models (SLMs) aim to learn linguistic competence directly from speech using discrete units, widening access to Natural Language Processing (NLP) technologies for languages with limited written resources. However, progress has been largely English-centric due to scarce spoken evaluation benchmarks and training data, making cross-lingual learning difficult. We present a cross-lingual interleaving method that mixes speech tokens across languages without textual supervision. We also release an EN-FR training dataset, TinyStories (~42k hours), together with EN-FR spoken StoryCloze and TopicCloze benchmarks for cross-lingual semantic evaluation, both synthetically generated using GPT-4. On 360M and 1B SLMs under matched training-token budgets, interleaving improves monolingual semantic accuracy, enables robust cross-lingual continuation, and strengthens cross-lingual hidden-state alignment. Taken together, these results indicate that cross-lingual interleaving is a simple, scalable route to building multilingual SLMs that understand and converse across languages. All resources will be made open-source to support reproducibility.</li>
</ul>

<h3>Title: The Mean-Field Dynamics of Transformers</h3>
<ul>
<li><strong>Authors: </strong>Philippe Rigollet</a></li>
<li><strong>Subjects: </strong>cs.LG, math-ph, math.DS, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01868">https://arxiv.org/abs/2512.01868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01868">https://arxiv.org/pdf/2512.01868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01868]] The Mean-Field Dynamics of Transformers(https://arxiv.org/abs/2512.01868)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We develop a mathematical framework that interprets Transformer attention as an interacting particle system and studies its continuum (mean-field) limits. By idealizing attention continuous on the sphere, we connect Transformer dynamics to Wasserstein gradient flows, synchronization models (Kuramoto), and mean-shift clustering. Central to our results is a global clustering phenomenon whereby tokens cluster asymptotically after long metastable states where they are arranged into multiple clusters. We further analyze a tractable equiangular reduction to obtain exact clustering rates, show how commonly used normalization schemes alter contraction speeds, and identify a phase transition for long-context attention. The results highlight both the mechanisms that drive representation collapse and the regimes that preserve expressive, multi-cluster structure in deep attention architectures.</li>
</ul>

<h3>Title: Unifying Sign and Magnitude for Optimizing Deep Vision Networks via ThermoLion</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Nebli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01881">https://arxiv.org/abs/2512.01881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01881">https://arxiv.org/pdf/2512.01881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01881]] Unifying Sign and Magnitude for Optimizing Deep Vision Networks via ThermoLion(https://arxiv.org/abs/2512.01881)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The training of deep vision models is fundamentally a signal recovery problem amidst high-dimensional stochastic noise. Current optimization paradigms impose a static compromise on information channel capacity. For instance, magnitude-based methods, such as AdamW, operate on the assumption that gradient norms are high-fidelity curvature signals. While this allows for precision in smooth regimes, it leads to catastrophic noise amplification when applied to rugged, non-convex landscapes. Conversely, sign-based methods (e.g., Lion) perform a radical 1-bit quantization of the gradient, which aims to provide robust regularization at the cost of discarding fine-grained descent information. We propose that optimal convergence requires neither static prior, but rather a dynamic modulation of the update bitrate. We introduce \textbf{ThermoLion}, a vision-centric framework that utilizes local Signal-to-Noise Ratio (SNR) gating to autonomously transition parameters between a "low-bit" exploration phase and a "high-precision" exploitation phase. Furthermore, we introduce a Momentum Alignment mechanism that detects constructive interference between historical drift and instantaneous gradients to accelerate convergence during stable trajectories. Empirical benchmarks across 12 diverse vision datasets (including CIFAR, SVHN, and GTSRB) demonstrate that ThermoLion serves as a hyperparameter-free generalist, surpassing both AdamW and Lion in convergence speed and terminal accuracy without architecture-specific tuning.</li>
</ul>

<h3>Title: New Spiking Architecture for Multi-Modal Decision-Making in Autonomous Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Aref Ghoreishee, Abhishek Mishra, Lifeng Zhou, John Walsh, Nagarajan Kandasamy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01882">https://arxiv.org/abs/2512.01882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01882">https://arxiv.org/pdf/2512.01882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01882]] New Spiking Architecture for Multi-Modal Decision-Making in Autonomous Vehicles(https://arxiv.org/abs/2512.01882)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This work proposes an end-to-end multi-modal reinforcement learning framework for high-level decision-making in autonomous vehicles. The framework integrates heterogeneous sensory input, including camera images, LiDAR point clouds, and vehicle heading information, through a cross-attention transformer-based perception module. Although transformers have become the backbone of modern multi-modal architectures, their high computational cost limits their deployment in resource-constrained edge environments. To overcome this challenge, we propose a spiking temporal-aware transformer-like architecture that uses ternary spiking neurons for computationally efficient multi-modal fusion. Comprehensive evaluations across multiple tasks in the Highway Environment demonstrate the effectiveness and efficiency of the proposed approach for real-time autonomous decision-making.</li>
</ul>

<h3>Title: TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals</h3>
<ul>
<li><strong>Authors: </strong>Florian Bürger, Martim Dias Gomes, Nica Gutu, Adrián E. Granada, Noémie Moreau, Katarzyna Bozek</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.CB, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01885">https://arxiv.org/abs/2512.01885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01885">https://arxiv.org/pdf/2512.01885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01885]] TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals(https://arxiv.org/abs/2512.01885)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Tracking cells in time-lapse videos is an essential technique for monitoring cell population dynamics at a single-cell level. Current methods for cell tracking are developed on videos with mostly single, constant signals and do not detect pivotal events such as cell death. Here, we present TransientTrack, a deep learning-based framework for cell tracking in multi-channel microscopy video data with transient fluorescent signals that fluctuate over time following processes such as the circadian rhythm of cells. By identifying key cellular events - mitosis (cell division) and apoptosis (cell death) our method allows us to build complete trajectories, including cell lineage information. TransientTrack is lightweight and performs matching on cell detection embeddings directly, without the need for quantification of tracking-specific cell features. Furthermore, our approach integrates Transformer Networks, multi-stage matching using all detection boxes, and the interpolation of missing tracklets with the Kalman Filter. This unified framework achieves strong performance across diverse conditions, effectively tracking cells and capturing cell division and death. We demonstrate the use of TransientTrack in an analysis of the efficacy of a chemotherapeutic drug at a single-cell level. The proposed framework could further advance quantitative studies of cancer cell dynamics, enabling detailed characterization of treatment response and resistance mechanisms. The code is available at this https URL.</li>
</ul>

<h3>Title: KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM</h3>
<ul>
<li><strong>Authors: </strong>Zaid Nasser, Mikhail Iumanov, Tianhao Li, Maxim Popov, Jaafar Mahmoud, Malik Mohrat, Ilya Obrubov, Ekaterina Derevyanka, Ivan Sosin, Sergey Kolyubin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01889">https://arxiv.org/abs/2512.01889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01889">https://arxiv.org/pdf/2512.01889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01889]] KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM(https://arxiv.org/abs/2512.01889)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments. Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training. KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views). The system performs simultaneous online localization and open-vocabulary semantic mapping by fusing geometric and deep visual features aligned with language embeddings. Our results are competitive with state-of-the-art approaches, while existing solutions either operate offline, need depth data and/or odometry estimation, or lack dynamic scene robustness. KM-ViPE benefits from internet-scale training and uniquely combines online operation, uncalibrated monocular input, and robust handling of dynamic scenes, which makes it a good fit for autonomous robotics and AR/VR applications and advances practical spatial intelligence capabilities for embodied AI.</li>
</ul>

<h3>Title: Behind the Curtain: How Shared Hosting Providers Respond to Vulnerability Notifications</h3>
<ul>
<li><strong>Authors: </strong>Giada Stivala, Rafael Mrowczynski, Maria Hellenthal, Giancarlo Pellegrino</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01891">https://arxiv.org/abs/2512.01891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01891">https://arxiv.org/pdf/2512.01891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01891]] Behind the Curtain: How Shared Hosting Providers Respond to Vulnerability Notifications(https://arxiv.org/abs/2512.01891)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Large-scale vulnerability notifications (VNs) can help hosting provider organizations (HPOs) identify and remediate security vulnerabilities that attackers can exploit in data breaches or phishing campaigns. Previous VN studies have primarily focused on factors under the control of reporters, such as sender reputation, email formatting, and communication channels. Despite these efforts, remediation rates for vulnerability notifications continue to remain consistently low. This paper presents the first in-depth study of how HPOs process vulnerability notifications internally and what organizational and operational factors influence VN effectiveness. We examine the problem from a different perspective to provide the first detailed understanding of the reasons behind persistently low remediation rates. Instead of manipulating parameters of VN campaigns, we interview hosting providers directly, investigating how they handle vulnerability notifications and what factors may influence VN effectiveness, such as VN awareness and reachability, HPOs' service models, and perceived security risks. We conducted semi-structured interviews with 24 HPOs across shared hosting and web development services, representing varied company sizes and operator roles. Our findings reveal practical insights on VN processing and abuse workflows. While some providers remain hard to reach due to complex infrastructures, most report routinely handling VNs. However, limited remediation often stems from strict responsibility boundaries, where web application issues are seen as the customer's domain. Low hosting fees and high volumes of daily compromises further discourage both proactive and reactive measures. Our findings show that HPOs blame negligent website owners, and prior works on website owners confirms they often undervalue their sites or lack security know-how.</li>
</ul>

<h3>Title: Exploring Human Perceptions of AI Responses: Insights from a Mixed-Methods Study on Risk Mitigation in Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Heloisa Candello, Muneeza Azmat, Uma Sushmitha Gunturi, Raya Horesh, Rogerio Abreu de Paula, Heloisa Pimentel, Marcelo Carpinette Grave, Aminat Adebiyi, Tiago Machado, Maysa Malfiza Garcia de Macedo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01892">https://arxiv.org/abs/2512.01892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01892">https://arxiv.org/pdf/2512.01892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01892]] Exploring Human Perceptions of AI Responses: Insights from a Mixed-Methods Study on Risk Mitigation in Generative Models(https://arxiv.org/abs/2512.01892)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative</a></li>
<li><strong>Abstract: </strong>With the rapid uptake of generative AI, investigating human perceptions of generated responses has become crucial. A major challenge is their `aptitude' for hallucinating and generating harmful contents. Despite major efforts for implementing guardrails, human perceptions of these mitigation strategies are largely unknown. We conducted a mixed-method experiment for evaluating the responses of a mitigation strategy across multiple-dimensions: faithfulness, fairness, harm-removal capacity, and relevance. In a within-subject study design, 57 participants assessed the responses under two conditions: harmful response plus its mitigation and solely mitigated response. Results revealed that participants' native language, AI work experience, and annotation familiarity significantly influenced evaluations. Participants showed high sensitivity to linguistic and contextual attributes, penalizing minor grammar errors while rewarding preserved semantic contexts. This contrasts with how language is often treated in the quantitative evaluation of LLMs. We also introduced new metrics for training and evaluating mitigation strategies and insights for human-AI evaluation studies.</li>
</ul>

<h3>Title: Improving Phishing Resilience with AI-Generated Training: Evidence on Prompting, Personalization, and Duration</h3>
<ul>
<li><strong>Authors: </strong>Francesco Greco, Giuseppe Desolda, Cesare Tucci, Andrea Esposito, Antonio Curci, Antonio Piccinno</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01893">https://arxiv.org/abs/2512.01893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01893">https://arxiv.org/pdf/2512.01893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01893]] Improving Phishing Resilience with AI-Generated Training: Evidence on Prompting, Personalization, and Duration(https://arxiv.org/abs/2512.01893)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, generative, large language model</a></li>
<li><strong>Abstract: </strong>Phishing remains a persistent cybersecurity threat; however, developing scalable and effective user training is labor-intensive and challenging to maintain. Generative Artificial Intelligence offers an interesting opportunity, but empirical evidence on its instructional efficacy remains scarce. This paper provides an experimental validation of Large Language Models (LLMs) as autonomous engines for generating phishing resilience training. Across two controlled studies (N=480), we demonstrate that AI-generated content yields significant pre-post learning gains regardless of the specific prompting strategy employed. Study 1 (N=80) compares four prompting techniques, finding that even a straightforward "direct-profile" strategy--simply embedding user traits into the prompt--produces effective training material. Study 2 (N=400) investigates the scalability of this approach by testing personalization and training duration. Results show that complex psychometric personalization offers no measurable advantage over well-designed generic content, while longer training duration provides a modest boost in accuracy. These findings suggest that organizations can leverage LLMs to generate high-quality, effective training at scale without the need for complex user profiling, relying instead on the inherent capabilities of the model.</li>
</ul>

<h3>Title: StyleYourSmile: Cross-Domain Face Retargeting Without Paired Multi-Style Data</h3>
<ul>
<li><strong>Authors: </strong>Avirup Dey, Vinay Namboodiri</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01895">https://arxiv.org/abs/2512.01895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01895">https://arxiv.org/pdf/2512.01895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01895]] StyleYourSmile: Cross-Domain Face Retargeting Without Paired Multi-Style Data(https://arxiv.org/abs/2512.01895)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Cross-domain face retargeting requires disentangled control over identity, expressions, and domain-specific stylistic attributes. Existing methods, typically trained on real-world faces, either fail to generalize across domains, need test-time optimizations, or require fine-tuning with carefully curated multi-style datasets to achieve domain-invariant identity representations. In this work, we introduce \textit{StyleYourSmile}, a novel one-shot cross-domain face retargeting method that eliminates the need for curated multi-style paired data. We propose an efficient data augmentation strategy alongside a dual-encoder framework, for extracting domain-invariant identity cues and capturing domain-specific stylistic variations. Leveraging these disentangled control signals, we condition a diffusion model to retarget facial expressions across domains. Extensive experiments demonstrate that \textit{StyleYourSmile} achieves superior identity preservation and retargeting fidelity across a wide range of visual domains.</li>
</ul>

<h3>Title: OPOR-Bench: Evaluating Large Language Models on Online Public Opinion Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinzheng Yu, Yang Xu, Haozhen Li, Junqi Li, Yifan Feng, Ligu Zhu, Hao Shen, Lei Shi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01896">https://arxiv.org/abs/2512.01896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01896">https://arxiv.org/pdf/2512.01896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01896]] OPOR-Bench: Evaluating Large Language Models on Online Public Opinion Report Generation(https://arxiv.org/abs/2512.01896)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Online Public Opinion Reports consolidate news and social media for timely crisis management by governments and enterprises. While large language models have made automated report generation technically feasible, systematic research in this specific area remains notably absent, particularly lacking formal task definitions and corresponding benchmarks. To bridge this gap, we define the Automated Online Public Opinion Report Generation (OPOR-GEN) task and construct OPOR-BENCH, an event-centric dataset covering 463 crisis events with their corresponding news articles, social media posts, and a reference summary. To evaluate report quality, we propose OPOR-EVAL, a novel agent-based framework that simulates human expert evaluation by analyzing generated reports in context. Experiments with frontier models demonstrate that our framework achieves high correlation with human judgments. Our comprehensive task definition, benchmark dataset, and evaluation framework provide a solid foundation for future research in this critical domain.</li>
</ul>

<h3>Title: Latent Debate: A Surrogate Framework for Interpreting LLM Thinking</h3>
<ul>
<li><strong>Authors: </strong>Lihu Chen, Xiang Yin, Francesca Toni</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01909">https://arxiv.org/abs/2512.01909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01909">https://arxiv.org/pdf/2512.01909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01909]] Latent Debate: A Surrogate Framework for Interpreting LLM Thinking(https://arxiv.org/abs/2512.01909)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Understanding the internal thinking process of Large Language Models (LLMs) and the cause of hallucinations remains a key challenge. To this end, we introduce latent debate, a novel framework for interpreting model predictions through the lens of implicit internal arguments. Unlike the current work of self-consistency and multi-agent debate, which relies on explicit debates among multiple answers or multiple models, latent debate captures the hidden supporting and attacking signals that arise within a single model during a single inference. We first present a model- and task-agnostic conceptual framework, and then instantiate it symbolically to approximate the thinking process of LLMs on True/False prediction tasks. Empirical studies demonstrate that latent debate is a faithful structured surrogate model that has highly consistent predictions with the original LLM. Beyond interpretability, we demonstrate that latent debate provides a strong baseline for hallucination detection. Further analysis reveals strong correlations between hallucinations and debate patterns, such as a high degree of latent debates in the middle layers is linked to a higher risk of hallucinations. These findings position latent debate as a potential framework for understanding internal mechanisms of LLMs, especially for scenarios where internal (dis)agreements appear during the inference steps.</li>
</ul>

<h3>Title: Rectifying LLM Thought from Lens of Optimization</h3>
<ul>
<li><strong>Authors: </strong>Junnan Liu, Hongwei Liu, Songyang Zhang, Kai Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01925">https://arxiv.org/abs/2512.01925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01925">https://arxiv.org/pdf/2512.01925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01925]] Rectifying LLM Thought from Lens of Optimization(https://arxiv.org/abs/2512.01925)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.</li>
</ul>

<h3>Title: SVRG and Beyond via Posterior Correction</h3>
<ul>
<li><strong>Authors: </strong>Nico Daheim, Thomas Möllenhoff, Ming Liang Ang, Mohammad Emtiyaz Khan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01930">https://arxiv.org/abs/2512.01930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01930">https://arxiv.org/pdf/2512.01930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01930]] SVRG and Beyond via Posterior Correction(https://arxiv.org/abs/2512.01930)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Stochastic Variance Reduced Gradient (SVRG) and its variants aim to speed-up training by using gradient corrections, but have seen limited success in deep learning. Here, we show surprising new foundational connections of SVRG to a recently proposed Bayesian method called posterior correction. Specifically, we show that SVRG is recovered as a special case of posterior correction over the isotropic-Gaussian family, while novel extensions are automatically obtained by using more flexible exponential families. We derive two new SVRG variants by using Gaussian families: First, a Newton-like variant that employs novel Hessian corrections, and second, an Adam-like extension that improves pretraining and finetuning of Transformer language models. This is the first work to connect SVRG to Bayes and use it to boost variational training for deep networks.</li>
</ul>

<h3>Title: Physical ID-Transfer Attacks against Multi-Object Tracking via Adversarial Trajectory</h3>
<ul>
<li><strong>Authors: </strong>Chenyi Wang, Yanmao Man, Raymond Muller, Ming Li, Z. Berkay Celik, Ryan Gerdes, Jonathan Petit</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01934">https://arxiv.org/abs/2512.01934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01934">https://arxiv.org/pdf/2512.01934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01934]] Physical ID-Transfer Attacks against Multi-Object Tracking via Adversarial Trajectory(https://arxiv.org/abs/2512.01934)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Multi-Object Tracking (MOT) is a critical task in computer vision, with applications ranging from surveillance systems to autonomous driving. However, threats to MOT algorithms have yet been widely studied. In particular, incorrect association between the tracked objects and their assigned IDs can lead to severe consequences, such as wrong trajectory predictions. Previous attacks against MOT either focused on hijacking the trackers of individual objects, or manipulating the tracker IDs in MOT by attacking the integrated object detection (OD) module in the digital domain, which are model-specific, non-robust, and only able to affect specific samples in offline datasets. In this paper, we present AdvTraj, the first online and physical ID-manipulation attack against tracking-by-detection MOT, in which an attacker uses adversarial trajectories to transfer its ID to a targeted object to confuse the tracking system, without attacking OD. Our simulation results in CARLA show that AdvTraj can fool ID assignments with 100% success rate in various scenarios for white-box attacks against SORT, which also have high attack transferability (up to 93% attack success rate) against state-of-the-art (SOTA) MOT algorithms due to their common design principles. We characterize the patterns of trajectories generated by AdvTraj and propose two universal adversarial maneuvers that can be performed by a human walker/driver in daily scenarios. Our work reveals under-explored weaknesses in the object association phase of SOTA MOT systems, and provides insights into enhancing the robustness of such systems.</li>
</ul>

<h3>Title: Agentic Policy Optimization via Instruction-Policy Co-Evolution</h3>
<ul>
<li><strong>Authors: </strong>Han Zhou, Xingchen Wan, Ivan Vulić, Anna Korhonen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01945">https://arxiv.org/abs/2512.01945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01945">https://arxiv.org/pdf/2512.01945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01945]] Agentic Policy Optimization via Instruction-Policy Co-Evolution(https://arxiv.org/abs/2512.01945)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-integrated reasoning. While instructions serve as the primary protocol for defining agents, RLVR typically relies on static and manually designed instructions. However, those instructions may be suboptimal for the base model, and the optimal instruction may change as the agent's policy improves and explores the interaction with the environment. To bridge the gap, we introduce INSPO, a novel Instruction-Policy co-evolution framework that integrates instruction optimization as a dynamic component of the reinforcement learning (RL) loop. INSPO maintains a dynamic population of instruction candidates that are sampled with questions, where reward signals in RL loops are automatically attributed to each instruction, and low performers are periodically pruned. New instructions are generated and verified through an on-policy reflection mechanism, where an LLM-based optimizer analyzes past experience from a replay buffer and evolves more effective strategies given the current policy. We conduct extensive experiments on multi-turn retrieval and reasoning tasks, demonstrating that INSPO substantially outperforms strong baselines relying on static instructions. INSPO discovers innovative instructions that guide the agent toward more strategic reasoning paths, achieving substantial performance gains with only a marginal increase in computational overhead.</li>
</ul>

<h3>Title: Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhongyu Yang, Dannong Xu, Wei Pang, Yingfang Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01949">https://arxiv.org/abs/2512.01949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01949">https://arxiv.org/pdf/2512.01949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01949]] Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models(https://arxiv.org/abs/2512.01949)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid growth of visual tokens in multimodal large language models (MLLMs) leads to excessive memory consumption and inference latency, especially when handling high-resolution images and videos. Token pruning is a technique used to mitigate this issue by removing redundancy, but existing methods often ignore relevance to the user query or suffer from the limitations of attention mechanisms, reducing their adaptability and effectiveness. To address these challenges, we propose Script, a plug-and-play pruning method that requires no retraining and generalizes across diverse MLLMs. Script comprises two modules: a graph-structured pruning module that removes visually redundant tokens, and a query-conditioned semantic pruning module that preserves query-relevant visual information. Together, they enhance performance on multimodal tasks. Experiments on fourteen benchmarks across image and video understanding tasks show that Script consistently achieves higher model efficiency and predictive accuracy compared to existing pruning methods. On LLaVA-NeXT-7B, it achieves up to 6.8x prefill speedup and 10x FLOP reduction, while retaining 96.88% of the original performance.</li>
</ul>

<h3>Title: GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment</h3>
<ul>
<li><strong>Authors: </strong>Haoyang He, Jay Patrikar, Dong-Ki Kim, Max Smith, Daniel McGann, Ali-akbar Agha-mohammadi, Shayegan Omidshafiei, Sebastian Scherer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01952">https://arxiv.org/abs/2512.01952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01952">https://arxiv.org/pdf/2512.01952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01952]] GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment(https://arxiv.org/abs/2512.01952)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.</li>
</ul>

<h3>Title: KV Pareto: Systems-Level Optimization of KV Cache and Model Compression for Long Context Inference</h3>
<ul>
<li><strong>Authors: </strong>Sai Gokhale, Devleena Das, Rajeev Patwari, Ashish Sirasao, Elliott Delaye</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01953">https://arxiv.org/abs/2512.01953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01953">https://arxiv.org/pdf/2512.01953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01953]] KV Pareto: Systems-Level Optimization of KV Cache and Model Compression for Long Context Inference(https://arxiv.org/abs/2512.01953)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long-context Large Language Models (LLMs) face significant memory bottlenecks during inference due to the linear growth of key-value (KV) cache with sequence length. While individual optimization techniques like KV cache quantization, chunked prefill, and model weight quantization have shown promise, their joint effects and optimal configurations for edge deployment remain underexplored. We introduce KV Pareto, a systems-level framework that systematically maps the trade-off frontier between total memory consumption and task accuracy across these three complementary optimization techniques. Our framework evaluates multiple LLM architectures (Qwen, Llama, Mistral) with varying KV quantization schemes (int2/4/8, mixed-precision), granularities (per-token, per-tensor, per-block), and 4-bit weight quantization via AWQ. Our framework identifies model-specific Pareto-optimal configurations that achieve 68-78% total memory reduction with minimal (1-3%) accuracy degradation on long-context tasks. We additionally verify the selected frontiers on additional benchmarks of Needle-in-a-Haystack, GSM8k and MMLU as well as extended context lengths of up to 128k to demonstrate the practical need of joint optimization for efficient LLM inference.</li>
</ul>

<h3>Title: SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Zisu Li, Hengye Lyu, Jiaxin Shi, Yufeng Zeng, Mingming Fan, Hanwang Zhang, Chen Liang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01960">https://arxiv.org/abs/2512.01960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01960">https://arxiv.org/pdf/2512.01960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01960]] SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation(https://arxiv.org/abs/2512.01960)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modeling and synthesizing complex hand-object interactions remains a significant challenge, even for state-of-the-art physics engines. Conventional simulation-based approaches rely on explicitly defined rigid object models and pre-scripted hand gestures, making them inadequate for capturing dynamic interactions with non-rigid or articulated entities such as deformable fabrics, elastic materials, hinge-based structures, furry surfaces, or even living creatures. In this paper, we present SpriteHand, an autoregressive video generation framework for real-time synthesis of versatile hand-object interaction videos across a wide range of object types and motion patterns. SpriteHand takes as input a static object image and a video stream in which the hands are imagined to interact with the virtual object embedded in a real-world scene, and generates corresponding hand-object interaction effects in real time. Our model employs a causal inference architecture for autoregressive generation and leverages a hybrid post-training approach to enhance visual realism and temporal coherence. Our 1.3B model supports real-time streaming generation at around 18 FPS and 640x368 resolution, with an approximate 150 ms latency on a single NVIDIA RTX 5090 GPU, and more than a minute of continuous output. Experiments demonstrate superior visual quality, physical plausibility, and interaction fidelity compared to both generative and engine-based baselines.</li>
</ul>

<h3>Title: SGDiff: Scene Graph Guided Diffusion Model for Image Collaborative SegCaptioning</h3>
<ul>
<li><strong>Authors: </strong>Xu Zhang, Jin Yuan, Hanwang Zhang, Guojin Zhong, Yongsheng Zang, Jiacheng Lin, Zhiyong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01975">https://arxiv.org/abs/2512.01975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01975">https://arxiv.org/pdf/2512.01975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01975]] SGDiff: Scene Graph Guided Diffusion Model for Image Collaborative SegCaptioning(https://arxiv.org/abs/2512.01975)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Controllable image semantic understanding tasks, such as captioning or segmentation, necessitate users to input a prompt (e.g., text or bounding boxes) to predict a unique outcome, presenting challenges such as high-cost prompt input or limited information output. This paper introduces a new task ``Image Collaborative Segmentation and Captioning'' (SegCaptioning), which aims to translate a straightforward prompt, like a bounding box around an object, into diverse semantic interpretations represented by (caption, masks) pairs, allowing flexible result selection by users. This task poses significant challenges, including accurately capturing a user's intention from a minimal prompt while simultaneously predicting multiple semantically aligned caption words and masks. Technically, we propose a novel Scene Graph Guided Diffusion Model that leverages structured scene graph features for correlated mask-caption prediction. Initially, we introduce a Prompt-Centric Scene Graph Adaptor to map a user's prompt to a scene graph, effectively capturing his intention. Subsequently, we employ a diffusion process incorporating a Scene Graph Guided Bimodal Transformer to predict correlated caption-mask pairs by uncovering intricate correlations between them. To ensure accurate alignment, we design a Multi-Entities Contrastive Learning loss to explicitly align visual and textual entities by considering inter-modal similarity, resulting in well-aligned caption-mask pairs. Extensive experiments conducted on two datasets demonstrate that SGDiff achieves superior performance in SegCaptioning, yielding promising results for both captioning and segmentation tasks with minimal prompt input.</li>
</ul>

<h3>Title: Low-Rank Prehab: Preparing Neural Networks for SVD Compression</h3>
<ul>
<li><strong>Authors: </strong>Haoran Qin, Shansita Sharma, Ali Abbasi, Chayne Thrash, Soheil Kolouri</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01980">https://arxiv.org/abs/2512.01980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01980">https://arxiv.org/pdf/2512.01980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01980]] Low-Rank Prehab: Preparing Neural Networks for SVD Compression(https://arxiv.org/abs/2512.01980)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Low-rank approximation methods such as singular value decomposition (SVD) and its variants (e.g., Fisher-weighted SVD, Activation SVD) have recently emerged as effective tools for neural network compression. In this setting, decomposition acts as a "surgical" intervention, followed by fine-tuning that serves as "rehab" to recover accuracy. Inspired by prehabilitation in surgery, we introduce a pre-compression fine-tuning stage, Low-Rank Prehab, that explicitly encourages low-rank structure in weight matrices while preserving task performance. By conditioning the model before SVD, Prehab steers weights toward spectrally compact regions of the parameter space, enabling smoother low-rank approximation and improved recovery. Experiments on large language models (LLMs) and other Transformer-based architectures, including Vision Transformers (ViTs), show that Prehab substantially reduces the immediate accuracy drop after compression and consistently improves post-finetuning performance. Across a wide range of compression ratios, our method outperforms state-of-the-art SVD-based techniques such as SVD-LLM, highlighting the importance of preparing models for compression rather than only improving the compression and recovery stages. Source code is available at this https URL</li>
</ul>

<h3>Title: Feature-Based Semantics-Aware Scheduling for Energy-Harvesting Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Eunjeong Jeong, Giovanni Perin, Howard H. Yang, Nikolaos Pappas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, cs.IT, cs.NI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01983">https://arxiv.org/abs/2512.01983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01983">https://arxiv.org/pdf/2512.01983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01983]] Feature-Based Semantics-Aware Scheduling for Energy-Harvesting Federated Learning(https://arxiv.org/abs/2512.01983)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) on resource-constrained edge devices faces a critical challenge: The computational energy required for training Deep Neural Networks (DNNs) often dominates communication costs. However, most existing Energy-Harvesting FL (EHFL) strategies fail to account for this reality, resulting in wasted energy due to redundant local computations. For efficient and proactive resource management, algorithms that predict local update contributions must be devised. We propose a lightweight client scheduling framework using the Version Age of Information (VAoI), a semantics-aware metric that quantifies update timeliness and significance. Crucially, we overcome VAoI's typical prohibitive computational cost, which requires statistical distance over the entire parameter space, by introducing a feature-based proxy. This proxy estimates model redundancy using intermediate-layer extraction from a single forward pass, dramatically reducing computational complexity. Experiments conducted under extreme non-IID data distributions and scarce energy availability demonstrate superior learning performance while achieving energy reduction compared to existing baseline selection policies. Our framework establishes semantics-aware scheduling as a practical and vital solution for EHFL in realistic scenarios where training costs dominate transmission costs.</li>
</ul>

<h3>Title: Forecasting in Offline Reinforcement Learning for Non-stationary Environments</h3>
<ul>
<li><strong>Authors: </strong>Suzan Ece Ada, Georg Martius, Emre Ugur, Erhan Oztop</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01987">https://arxiv.org/abs/2512.01987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01987">https://arxiv.org/pdf/2512.01987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01987]] Forecasting in Offline Reinforcement Learning for Non-stationary Environments(https://arxiv.org/abs/2512.01987)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Offline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time, assumptions that often fail in real-world scenarios characterized by abrupt, time-varying offsets. These offsets can lead to partial observability, causing agents to misperceive their true state and degrade performance. To overcome this challenge, we introduce Forecasting in Non-stationary Offline RL (FORL), a framework that unifies (i) conditional diffusion-based candidate state generation, trained without presupposing any specific pattern of future non-stationarity, and (ii) zero-shot time-series foundation models. FORL targets environments prone to unexpected, potentially non-Markovian offsets, requiring robust agent performance from the onset of each episode. Empirical evaluations on offline RL benchmarks, augmented with real-world time-series data to simulate realistic non-stationarity, demonstrate that FORL consistently improves performance compared to competitive baselines. By integrating zero-shot forecasting with the agent's experience, we aim to bridge the gap between offline RL and the complexities of real-world, non-stationary environments.</li>
</ul>

<h3>Title: PAI-Bench: A Comprehensive Benchmark For Physical AI</h3>
<ul>
<li><strong>Authors: </strong>Fengzhe Zhou, Jiannan Huang, Jialuo Li, Deva Ramanan, Humphrey Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.01989">https://arxiv.org/abs/2512.01989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.01989">https://arxiv.org/pdf/2512.01989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.01989]] PAI-Bench: A Comprehensive Benchmark For Physical AI(https://arxiv.org/abs/2512.01989)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.</li>
</ul>

<h3>Title: AlignSAE: Concept-Aligned Sparse Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Minglai Yang, Xinyu Guo, Mihai Surdeanu, Liangming Pan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.02004">https://arxiv.org/abs/2512.02004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.02004">https://arxiv.org/pdf/2512.02004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.02004]] AlignSAE: Concept-Aligned Sparse Autoencoders(https://arxiv.org/abs/2512.02004)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) encode factual knowledge within hidden parametric spaces that are difficult to inspect or control. While Sparse Autoencoders (SAEs) can decompose hidden activations into more fine-grained, interpretable features, they often struggle to reliably align these features with human-defined concepts, resulting in entangled and distributed feature representations. To address this, we introduce AlignSAE, a method that aligns SAE features with a defined ontology through a "pre-train, then post-train" curriculum. After an initial unsupervised training phase, we apply supervised post-training to bind specific concepts to dedicated latent slots while preserving the remaining capacity for general reconstruction. This separation creates an interpretable interface where specific relations can be inspected and controlled without interference from unrelated features. Empirical results demonstrate that AlignSAE enables precise causal interventions, such as reliable "concept swaps", by targeting single, semantically aligned slots.</li>
</ul>

<h3>Title: The Art of Scaling Test-Time Compute for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aradhye Agarwal, Ayan Sengupta, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.02008">https://arxiv.org/abs/2512.02008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.02008">https://arxiv.org/pdf/2512.02008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.02008]] The Art of Scaling Test-Time Compute for Large Language Models(https://arxiv.org/abs/2512.02008)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Test-time scaling (TTS) -- the dynamic allocation of compute during inference -- is a promising direction for improving reasoning in large language models (LLMs). However, a systematic comparison of well-known TTS strategies under identical conditions is missing, and the influence of model type and problem difficulty on performance remains unclear. To address these gaps, we conduct the first large-scale study of TTS, spanning over thirty billion tokens generated using eight open-source LLMs (7B to 235B parameters), across four reasoning datasets. We observe three consistent trends: (1) no single TTS strategy universally dominates; (2) reasoning models exhibit distinct trace-quality patterns across problem difficulty and trace length, forming short-horizon and long-horizon categories; and (3) for a given model type, the optimal TTS performance scales monotonically with compute budget. Based on these insights, we provide a practical recipe for selecting the best TTS strategy, considering problem difficulty, model type, and compute budget, providing a practical guide to effective inference-time scaling.</li>
</ul>

<h3>Title: Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling</h3>
<ul>
<li><strong>Authors: </strong>Jack Cook, Junxian Guo, Guangxuan Xiao, Yujun Lin, Song Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.02010">https://arxiv.org/abs/2512.02010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.02010">https://arxiv.org/pdf/2512.02010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.02010]] Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling(https://arxiv.org/abs/2512.02010)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>As large language models have grown larger, low-precision numerical formats such as NVFP4 have become increasingly popular due to the speed and memory benefits they provide. However, to accelerate computation with NVFP4, all matrix multiplication operands--weights and activations in the forward pass, and weights, activations, and gradients in the backward pass--must be quantized to NVFP4, often leading to divergence during training and performance degradation during inference. NVFP4 by evaluating multiple potential scale factors for each block of values. To address this issue, in this work we introduce Four Over Six (4/6), a modification to the NVFP4 quantization algorithm that evaluates two potential scale factors for each block of values. Unlike integer formats, floating-point formats such as FP4 have the most quantization error on near-maximal values in each block, which we find to be primarily responsible for downstream performance degradation. We find that for some blocks, scaling to smaller FP4 values makes the distribution of representable values more uniform, improving representation of near-maximal values. Importantly, 4/6 can be implemented efficiently on NVIDIA Blackwell GPUs, making it viable to use while training LLMs with NVFP4. In pre-training experiments with transformer and hybrid model architectures, we find that 4/6 prevents divergence in several cases, bringing training loss significantly closer to BF16 compared to models trained with current state-of-the-art NVFP4 training recipes. We also find that 4/6 can be easily incorporated into many different post-training quantization methods and generally improves downstream accuracy. We hope this inspires future work in training and deploying models with NVFP4.</li>
</ul>

<h3>Title: Improved Mean Flows: On the Challenges of Fastforward Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Zhengyang Geng, Yiyang Lu, Zongze Wu, Eli Shechtman, J. Zico Kolter, Kaiming He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.02012">https://arxiv.org/abs/2512.02012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.02012">https://arxiv.org/pdf/2512.02012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.02012]] Improved Mean Flows: On the Challenges of Fastforward Generative Models(https://arxiv.org/abs/2512.02012)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>MeanFlow (MF) has recently been established as a framework for one-step generative modeling. However, its ``fastforward'' nature introduces key challenges in both the training objective and the guidance mechanism. First, the original MF's training target depends not only on the underlying ground-truth fields but also on the network itself. To address this issue, we recast the objective as a loss on the instantaneous velocity $v$, re-parameterized by a network that predicts the average velocity $u$. Our reformulation yields a more standard regression problem and improves the training stability. Second, the original MF fixes the classifier-free guidance scale during training, which sacrifices flexibility. We tackle this issue by formulating guidance as explicit conditioning variables, thereby retaining flexibility at test time. The diverse conditions are processed through in-context conditioning, which reduces model size and benefits performance. Overall, our $\textbf{improved MeanFlow}$ ($\textbf{iMF}$) method, trained entirely from scratch, achieves $\textbf{1.72}$ FID with a single function evaluation (1-NFE) on ImageNet 256$\times$256. iMF substantially outperforms prior methods of this kind and closes the gap with multi-step methods while using no distillation. We hope our work will further advance fastforward generative modeling as a stand-alone paradigm.</li>
</ul>

<h3>Title: Generative Video Motion Editing with 3D Point Tracks</h3>
<ul>
<li><strong>Authors: </strong>Yao-Chih Lee, Zhoutong Zhang, Jiahui Huang, Jui-Hsien Wang, Joon-Young Lee, Jia-Bin Huang, Eli Shechtman, Zhengqi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.02015">https://arxiv.org/abs/2512.02015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.02015">https://arxiv.org/pdf/2512.02015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.02015]] Generative Video Motion Editing with 3D Point Tracks(https://arxiv.org/abs/2512.02015)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.</li>
</ul>

<h3>Title: Data-Centric Visual Development for Self-Driving Labs</h3>
<ul>
<li><strong>Authors: </strong>Anbang Liu, Guanzhong Hu, Jiayi Wang, Ping Guo, Han Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.02018">https://arxiv.org/abs/2512.02018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.02018">https://arxiv.org/pdf/2512.02018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.02018]] Data-Centric Visual Development for Self-Driving Labs(https://arxiv.org/abs/2512.02018)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Self-driving laboratories offer a promising path toward reducing the labor-intensive, time-consuming, and often irreproducible workflows in the biological sciences. Yet their stringent precision requirements demand highly robust models whose training relies on large amounts of annotated data. However, this kind of data is difficult to obtain in routine practice, especially negative samples. In this work, we focus on pipetting, the most critical and precision sensitive action in SDLs. To overcome the scarcity of training data, we build a hybrid pipeline that fuses real and virtual data generation. The real track adopts a human-in-the-loop scheme that couples automated acquisition with selective human verification to maximize accuracy with minimal effort. The virtual track augments the real data using reference-conditioned, prompt-guided image generation, which is further screened and validated for reliability. Together, these two tracks yield a class-balanced dataset that enables robust bubble detection training. On a held-out real test set, a model trained entirely on automatically acquired real images reaches 99.6% accuracy, and mixing real and generated data during training sustains 99.4% accuracy while reducing collection and review load. Our approach offers a scalable and cost-effective strategy for supplying visual feedback data to SDL workflows and provides a practical solution to data scarcity in rare event detection and broader vision tasks.</li>
</ul>

<h3>Title: A Diffusion Model Framework for Maximum Entropy Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Sanokowski, Kaustubh Patil, Alois Knoll</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.02019">https://arxiv.org/abs/2512.02019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.02019">https://arxiv.org/pdf/2512.02019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.02019]] A Diffusion Model Framework for Maximum Entropy Reinforcement Learning(https://arxiv.org/abs/2512.02019)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable success in data-driven learning and in sampling from complex, unnormalized target distributions. Building on this progress, we reinterpret Maximum Entropy Reinforcement Learning (MaxEntRL) as a diffusion model-based sampling problem. We tackle this problem by minimizing the reverse Kullback-Leibler (KL) divergence between the diffusion policy and the optimal policy distribution using a tractable upper bound. By applying the policy gradient theorem to this objective, we derive a modified surrogate objective for MaxEntRL that incorporates diffusion dynamics in a principled way. This leads to simple diffusion-based variants of Soft Actor-Critic (SAC), Proximal Policy Optimization (PPO) and Wasserstein Policy Optimization (WPO), termed DiffSAC, DiffPPO and DiffWPO. All of these methods require only minor implementation changes to their base algorithm. We find that on standard continuous control benchmarks, DiffSAC, DiffPPO and DiffWPO achieve better returns and higher sample efficiency than SAC and PPO.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
