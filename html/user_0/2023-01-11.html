<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Earn While You Reveal: Private Set Intersection that Rewards Participants. (arXiv:2301.03889v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03889">http://arxiv.org/abs/2301.03889</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03889] Earn While You Reveal: Private Set Intersection that Rewards Participants](http://arxiv.org/abs/2301.03889) #secure</code></li>
<li>Summary: <p>In Private Set Intersection protocols (PSIs), a non-empty result always
reveals something about the private input sets of the parties. Moreover, in
various variants of PSI, not all parties necessarily receive or are interested
in the result. Nevertheless, to date, the literature has assumed that those
parties who do not receive or are not interested in the result still contribute
their private input sets to the PSI for free, although doing so would cost them
their privacy. In this work, for the first time, we propose a multi-party PSI,
called "Anesidora", that rewards parties who contribute their private input
sets to the protocol. Anesidora is efficient; it mainly relies on symmetric key
primitives and its computation and communication complexities are linear with
the number of parties and set cardinality. It remains secure even if the
majority of parties are corrupted by active colluding adversaries.
</p></li>
</ul>

<h3>Title: IronForge: An Open, Secure, Fair, Decentralized Federated Learning. (arXiv:2301.04006v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.04006">http://arxiv.org/abs/2301.04006</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.04006] IronForge: An Open, Secure, Fair, Decentralized Federated Learning](http://arxiv.org/abs/2301.04006) #secure</code></li>
<li>Summary: <p>Federated learning (FL) provides an effective machine learning (ML)
architecture to protect data privacy in a distributed manner. However, the
inevitable network asynchrony, the over-dependence on a central coordinator,
and the lack of an open and fair incentive mechanism collectively hinder its
further development. We propose \textsc{IronForge}, a new generation of FL
framework, that features a Directed Acyclic Graph (DAG)-based data structure
and eliminates the need for central coordinators to achieve fully decentralized
operations. \textsc{IronForge} runs in a public and open network, and launches
a fair incentive mechanism by enabling state consistency in the DAG, so that
the system fits in networks where training resources are unevenly distributed.
In addition, dedicated defense strategies against prevalent FL attacks on
incentive fairness and data privacy are presented to ensure the security of
\textsc{IronForge}. Experimental results based on a newly developed testbed
FLSim highlight the superiority of \textsc{IronForge} to the existing prevalent
FL frameworks under various specifications in performance, fairness, and
security. To the best of our knowledge, \textsc{IronForge} is the first secure
and fully decentralized FL framework that can be applied in open networks with
realistic network and training settings.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Network Message Field Type Classification and Recognition for Unknown Binary Protocols. (arXiv:2301.03584v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03584">http://arxiv.org/abs/2301.03584</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03584] Network Message Field Type Classification and Recognition for Unknown Binary Protocols](http://arxiv.org/abs/2301.03584) #security</code></li>
<li>Summary: <p>Reverse engineering of unknown network protocols based on recorded traffic
traces enables security analyses and debugging of undocumented network
services. In particular for binary protocols, existing approaches (1) lack
comprehensive methods to classify or determine the data type of a discovered
segment in a message, e.,g., a number, timestamp, or network address, that
would allow for a semantic interpretation and (2) have strong assumptions that
prevent analysis of lower-layer protocols often found in IoT or mobile systems.
In this paper, we propose the first generic method for analyzing unknown
messages from binary protocols to reveal the data types in message fields. To
this end, we split messages into segments of bytes and use their vector
interpretation to calculate similarities. These can be used to create clusters
of segments with the same type and, moreover, to recognize specific data types
based on the clusters' characteristics. Our extensive evaluation shows that our
method provides precise classification in most cases and a
data-type-recognition precision of up to 100% at reasonable recall, improving
the state-of-the-art by a factor between 1.3 and 3.7 in realistic scenarios. We
open-source our implementation to facilitate follow-up works.
</p></li>
</ul>

<h3>Title: Refining Network Message Segmentation with Principal Component Analysis. (arXiv:2301.03585v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03585">http://arxiv.org/abs/2301.03585</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03585] Refining Network Message Segmentation with Principal Component Analysis](http://arxiv.org/abs/2301.03585) #security</code></li>
<li>Summary: <p>Reverse engineering of undocumented protocols is a common task in security
analyses of networked services. The communication itself, captured in traffic
traces, contains much of the necessary information to perform such a protocol
reverse engineering. The comprehension of the format of unknown messages is of
particular interest for binary protocols that are not human-readable. One major
challenge is to discover probable fields in a message as the basis for further
analyses. Given a set of messages, split into segments of bytes by an existing
segmenter, we propose a method to refine the approximation of the field
inference. We use principle component analysis (PCA) to discover linearly
correlated variance between sets of message segments. We relocate the
boundaries of the initial coarse segmentation to more accurately match with the
true fields. We perform different evaluations of our method to show its benefit
for the message format inference and subsequent analysis tasks from literature
that depend on the message format. We can achieve a median improvement of the
message format accuracy across different real-world protocols by up to 100 %.
</p></li>
</ul>

<h3>Title: Fast, Cheap and Good: Lightweight Methods Are Undervalued. (arXiv:2301.03593v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03593">http://arxiv.org/abs/2301.03593</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03593] Fast, Cheap and Good: Lightweight Methods Are Undervalued](http://arxiv.org/abs/2301.03593) #security</code></li>
<li>Summary: <p>Engineering techniques to address the endless parade of security issues are
an important area of research. Properties of practices in industrial use are
rarely studied. Security workers satisfice. There is a widespread perception
that security work must be cumbersome, and thus there's no value to assessing
levels of effort. This is complemented by a belief that the nth day of work
will produce value equal to the first. These perceptions impact both practice
and research. This paper expands the acceptable paradigms for security analysis
to include the fast, cheap and good enough. "Nothing" is often enough for
industry. This paper makes a case for valuing lightweight ("fast and cheap")
methods, presents a set of case studies and evaluation criteria for such tools,
including card decks and role playing games.
</p></li>
</ul>

<h3>Title: Quantifying User Password Exposure to Third-Party CDNs. (arXiv:2301.03690v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03690">http://arxiv.org/abs/2301.03690</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03690] Quantifying User Password Exposure to Third-Party CDNs](http://arxiv.org/abs/2301.03690) #security</code></li>
<li>Summary: <p>Web services commonly employ Content Distribution Networks (CDNs) for
performance and security. As web traffic is becoming 100% HTTPS, more and more
websites allow CDNs to terminate their HTTPS connections. This practice may
expose a website's user sensitive information such as a user's login password
to a third-party CDN. In this paper, we measure and quantify the extent of user
password exposure to third-party CDNs. We find that among Alexa top 50K
websites, at least 12,451 of them use CDNs and contain user login entrances.
Among those websites, 33% of them expose users' passwords to the CDNs, and a
popular CDN may observe passwords from more than 40% of its customers. This
result suggests that if a CDN infrastructure has a security vulnerability or an
insider attack, many users' accounts will be at risk. A simple fix to this
security vulnerability is for a website to encrypt a user's password inside the
HTTPS request. Our measurement shows that less than 17% of the websites adopt
this solution.
</p></li>
</ul>

<h3>Title: Chatbots in a Honeypot World. (arXiv:2301.03771v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03771">http://arxiv.org/abs/2301.03771</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03771] Chatbots in a Honeypot World](http://arxiv.org/abs/2301.03771) #security</code></li>
<li>Summary: <p>Question-and-answer agents like ChatGPT offer a novel tool for use as a
potential honeypot interface in cyber security. By imitating Linux, Mac, and
Windows terminal commands and providing an interface for TeamViewer, nmap, and
ping, it is possible to create a dynamic environment that can adapt to the
actions of attackers and provide insight into their tactics, techniques, and
procedures (TTPs). The paper illustrates ten diverse tasks that a
conversational agent or large language model might answer appropriately to the
effects of command-line attacker. The original result features feasibility
studies for ten model tasks meant for defensive teams to mimic expected
honeypot interfaces with minimal risks. Ultimately, the usefulness outside of
forensic activities stems from whether the dynamic honeypot can extend the
time-to-conquer or otherwise delay attacker timelines short of reaching key
network assets like databases or confidential information. While ongoing
maintenance and monitoring may be required, ChatGPT's ability to detect and
deflect malicious activity makes it a valuable option for organizations seeking
to enhance their cyber security posture. Future work will focus on
cybersecurity layers, including perimeter security, host virus detection, and
data security.
</p></li>
</ul>

<h3>Title: A Practical Runtime Security Policy Transformation Framework for Software Defined Networks. (arXiv:2301.03790v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03790">http://arxiv.org/abs/2301.03790</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03790] A Practical Runtime Security Policy Transformation Framework for Software Defined Networks](http://arxiv.org/abs/2301.03790) #security</code></li>
<li>Summary: <p>Software-defined networking (SDN) has been widely utilized to enforce the
security of traditional networks, thereby promoting the process of transforming
traditional networks into SDN networks. However, SDN-based security enforcement
mechanisms rely heavily on the security policies containing the underlying
information of data plane. With increasing the scale of underlying network, the
current security policy management mechanism will confront more and more
challenges. The security policy transformation for SDN networks is to research
how to transform the high-level security policy without containing the
underlying information of data plane into the practical flow entries used by
the OpenFlow switches automatically, thereby implementing the automation of
security policy management. Based on this insight, a practical runtime security
policy transformation framework is proposed in this paper. First of all, we
specify the security policies used by SDN networks as a system model of
security policy (SPM). From the theoretical level, we establish the system
model for SDN network and propose a formal method to transform SPM into the
system model of flow entries automatically. From the practical level, we
propose a runtime security policy transformation framework to solve the problem
of how to find a connected path for each relationship of SPM in the data plane,
as well as how to generate the practical flow entries according to the system
model of flow entries. In order to validate the feasibility and effectiveness
of the framework, we set up an experimental system and implement the framework
with POX controller and Mininet emulator.
</p></li>
</ul>

<h3>Title: BLE Protocol in IoT Devices and Smart Wearable Devices: Security and Privacy Threats. (arXiv:2301.03852v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03852">http://arxiv.org/abs/2301.03852</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03852] BLE Protocol in IoT Devices and Smart Wearable Devices: Security and Privacy Threats](http://arxiv.org/abs/2301.03852) #security</code></li>
<li>Summary: <p>Bluetooth Low Energy (BLE) has become the primary transmission media due to
its extremely low energy consumption, good network scope, and data transfer
speed for the Internet of Things (IoT) and smart wearable devices. With the
exponential boom of the Internet of Things (IoT) and the Bluetooth Low Energy
(BLE) connection protocol, a requirement to discover defensive techniques to
protect it with practical security analysis. Unfortunately, IoT-BLE is at risk
of spoofing assaults where an attacker can pose as a gadget and provide its
users a harmful information. Furthermore, due to the simplified strategy of
this protocol, there were many security and privacy vulnerabilities. Justifying
this quantitative security analysis with STRIDE Methodology change to create a
framework to deal with protection issues for the IoT-BLE sensors. Therefore,
providing probable attack scenarios for various exposures in this analysis, and
offer mitigating strategies. In light of this authors performed STRIDE threat
modeling to understand the attack surface for smart wearable devices supporting
BLE. The study evaluates different exploitation scenarios Denial of Service
(DoS), Elevation of privilege, Information disclosure, spoofing, Tampering, and
repudiation on MI Band, One plus Band, Boat Storm smartwatch, and Fire Bolt
Invincible.
</p></li>
</ul>

<h3>Title: Balanced Datasets for IoT IDS. (arXiv:2301.04008v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.04008">http://arxiv.org/abs/2301.04008</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.04008] Balanced Datasets for IoT IDS](http://arxiv.org/abs/2301.04008) #security</code></li>
<li>Summary: <p>As the Internet of Things (IoT) continues to grow, cyberattacks are becoming
increasingly common. The security of IoT networks relies heavily on intrusion
detection systems (IDSs). The development of an IDS that is accurate and
efficient is a challenging task. As a result, this challenge is made more
challenging by the absence of balanced datasets for training and testing the
proposed IDS. In this study, four commonly used datasets are visualized and
analyzed visually. Moreover, it proposes a sampling algorithm that generates a
sample that represents the original dataset. In addition, it proposes an
algorithm to generate a balanced dataset. Researchers can use this paper as a
starting point when investigating cybersecurity and machine learning. The
proposed sampling algorithms showed reliability in generating well-representing
and balanced samples from NSL-KDD, UNSW-NB15, BotNetIoT-01, and BoTIoT
datasets.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: A Privacy Preserving Method with a Random Orthogonal Matrix for ConvMixer Models. (arXiv:2301.03843v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03843">http://arxiv.org/abs/2301.03843</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03843] A Privacy Preserving Method with a Random Orthogonal Matrix for ConvMixer Models](http://arxiv.org/abs/2301.03843) #privacy</code></li>
<li>Summary: <p>In this paper, a privacy preserving image classification method is proposed
under the use of ConvMixer models. To protect the visual information of test
images, a test image is divided into blocks, and then every block is encrypted
by using a random orthogonal matrix. Moreover, a ConvMixer model trained with
plain images is transformed by the random orthogonal matrix used for encrypting
test images, on the basis of the embedding structure of ConvMixer. The proposed
method allows us not only to use the same classification accuracy as that of
ConvMixer models without considering privacy protection but to also enhance
robustness against various attacks compared to conventional privacy-preserving
learning.
</p></li>
</ul>

<h3>Title: Privacy-Preserving Record Linkage for Cardinality Counting. (arXiv:2301.04000v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.04000">http://arxiv.org/abs/2301.04000</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.04000] Privacy-Preserving Record Linkage for Cardinality Counting](http://arxiv.org/abs/2301.04000) #privacy</code></li>
<li>Summary: <p>Several applications require counting the number of distinct items in the
data, which is known as the cardinality counting problem. Example applications
include health applications such as rare disease patients counting for adequate
awareness and funding, and counting the number of cases of a new disease for
outbreak detection, marketing applications such as counting the visibility
reached for a new product, and cybersecurity applications such as tracking the
number of unique views of social media posts. The data needed for the counting
is however often personal and sensitive, and need to be processed using
privacy-preserving techniques. The quality of data in different databases, for
example typos, errors and variations, poses additional challenges for accurate
cardinality estimation. While privacy-preserving cardinality counting has
gained much attention in the recent times and a few privacy-preserving
algorithms have been developed for cardinality estimation, no work has so far
been done on privacy-preserving cardinality counting using record linkage
techniques with fuzzy matching and provable privacy guarantees. We propose a
novel privacy-preserving record linkage algorithm using unsupervised clustering
techniques to link and count the cardinality of individuals in multiple
datasets without compromising their privacy or identity. In addition, existing
Elbow methods to find the optimal number of clusters as the cardinality are far
from accurate as they do not take into account the purity and completeness of
generated clusters. We propose a novel method to find the optimal number of
clusters in unsupervised learning. Our experimental results on real and
synthetic datasets are highly promising in terms of significantly smaller error
rate of less than 0.1 with a privacy budget {\epsilon} = 1.0 compared to the
state-of-the-art fuzzy matching and clustering method.
</p></li>
</ul>

<h3>Title: Improving unlinkability in C-ITS: a methodology for optimal obfuscation. (arXiv:2301.04130v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.04130">http://arxiv.org/abs/2301.04130</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.04130] Improving unlinkability in C-ITS: a methodology for optimal obfuscation](http://arxiv.org/abs/2301.04130) #privacy</code></li>
<li>Summary: <p>In this paper, we develop a new methodology to provide high assurance about
privacy in Cooperative Intelligent Transport Systems (C-ITS). Our focus lies on
vehicle-to-everything (V2X) communications enabled by Cooperative Awareness
Basic Service. Our research motivation is developed based on the analysis of
unlinkability provision methods indicating a gap. To address this, we propose a
Hidden Markov Model (HMM) to express unlinkability for the situation where two
cars are communicating with a Roadside Unit (RSU) using Cooperative Awareness
Messages (CAMs). Our HMM has labeled states specifying distinct origins of the
CAMs observable by a passive attacker. We then demonstrate that a high
assurance about the degree of uncertainty (e.g., entropy) about labeled states
can be obtained for the attacker under the assumption that he knows actual
positions of the vehicles (e.g., hidden states in HMM). We further demonstrate
how unlinkability can be increased in C-ITS: we propose a joint probability
distribution that both drivers must use to obfuscate their actual data jointly.
This obfuscated data is then encapsulated in their CAMs. Finally, our findings
are incorporated into an obfuscation algorithm whose complexity is linear in
the number of discrete time steps in HMM.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: On the Susceptibility and Robustness of Time Series Models through Adversarial Attack and Defense. (arXiv:2301.03703v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03703">http://arxiv.org/abs/2301.03703</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03703] On the Susceptibility and Robustness of Time Series Models through Adversarial Attack and Defense](http://arxiv.org/abs/2301.03703) #defense</code></li>
<li>Summary: <p>Under adversarial attacks, time series regression and classification are
vulnerable. Adversarial defense, on the other hand, can make the models more
resilient. It is important to evaluate how vulnerable different time series
models are to attacks and how well they recover using defense. The sensitivity
to various attacks and the robustness using the defense of several time series
models are investigated in this study. Experiments are run on seven-time series
models with three adversarial attacks and one adversarial defense. According to
the findings, all models, particularly GRU and RNN, appear to be vulnerable.
LSTM and GRU also have better defense recovery. FGSM exceeds the competitors in
terms of attacks. PGD attacks are more difficult to recover from than other
sorts of attacks.
</p></li>
</ul>

<h3>Title: SoK: Hardware Defenses Against Speculative Execution Attacks. (arXiv:2301.03724v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03724">http://arxiv.org/abs/2301.03724</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03724] SoK: Hardware Defenses Against Speculative Execution Attacks](http://arxiv.org/abs/2301.03724) #defense</code></li>
<li>Summary: <p>Speculative execution attacks leverage the speculative and out-of-order
execution features in modern computer processors to access secret data or
execute code that should not be executed. Secret information can then be leaked
through a covert channel. While software patches can be installed for
mitigation on existing hardware, these solutions can incur big performance
overhead. Hardware mitigation is being studied extensively by the computer
architecture community. It has the benefit of preserving software compatibility
and the potential for much smaller performance overhead than software
solutions.
</p></li>
</ul>

<p>This paper presents a systematization of the hardware defenses against
speculative execution attacks that have been proposed. We show that speculative
execution attacks consist of 6 critical attack steps. We propose defense
strategies, each of which prevents a critical attack step from happening, thus
preventing the attack from succeeding. We then summarize 20 hardware defenses
and overhead-reducing features that have been proposed. We show that each
defense proposed can be classified under one of our defense strategies, which
also explains why it can thwart the attack from succeeding. We discuss the
scope of the defenses, their performance overhead, and the security-performance
trade-offs that can be made.
</p>

<h2>attack</h2>
<h3>Title: AdvBiom: Adversarial Attacks on Biometric Matchers. (arXiv:2301.03966v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03966">http://arxiv.org/abs/2301.03966</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03966] AdvBiom: Adversarial Attacks on Biometric Matchers](http://arxiv.org/abs/2301.03966) #attack</code></li>
<li>Summary: <p>With the advent of deep learning models, face recognition systems have
achieved impressive recognition rates. The workhorses behind this success are
Convolutional Neural Networks (CNNs) and the availability of large training
datasets. However, we show that small human-imperceptible changes to face
samples can evade most prevailing face recognition systems. Even more alarming
is the fact that the same generator can be extended to other traits in the
future. In this work, we present how such a generator can be trained and also
extended to other biometric modalities, such as fingerprint recognition
systems.
</p></li>
</ul>

<h3>Title: RingAuth: Wearable Authentication using a Smart Ring. (arXiv:2301.03594v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03594">http://arxiv.org/abs/2301.03594</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03594] RingAuth: Wearable Authentication using a Smart Ring](http://arxiv.org/abs/2301.03594) #attack</code></li>
<li>Summary: <p>In this paper, we show that by using inertial sensor data generated by a
smart ring, worn on the finger, the user can be authenticated when making
mobile payments or when knocking on a door (for access control). The proposed
system can be deployed purely in software and does not require updates to
existing payment terminals or infrastructure. We also demonstrate that smart
ring data can authenticate smartwatch gestures, and vice versa, allowing either
device to act as an implicit second factor for the other. To validate the
system, we conduct a user study (n=21) to collect inertial sensor data from
users as they perform gestures, and we evaluate the system against an active
impersonation attacker. Based on this data, we develop payment and access
control authentication models for which we achieve EERs of 0.04 and 0.02,
respectively.
</p></li>
</ul>

<h3>Title: White-box Inference Attacks against Centralized Machine Learning and Federated Learning. (arXiv:2301.03595v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03595">http://arxiv.org/abs/2301.03595</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03595] White-box Inference Attacks against Centralized Machine Learning and Federated Learning](http://arxiv.org/abs/2301.03595) #attack</code></li>
<li>Summary: <p>With the development of information science and technology, various
industries have generated massive amounts of data, and machine learning is
widely used in the analysis of big data. However, if the privacy of machine
learning applications' customers cannot be guaranteed, it will cause security
threats and losses to users' personal privacy information and service
providers. Therefore, the issue of privacy protection of machine learning has
received wide attention. For centralized machine learning models, we evaluate
the impact of different neural network layers, gradient, gradient norm, and
fine-tuned models on member inference attack performance with prior knowledge;
For the federated learning model, we discuss the location of the attacker in
the target model and its attack mode. The results show that the centralized
machine learning model shows more serious member information leakage in all
aspects, and the accuracy of the attacker in the central parameter server is
significantly higher than the local Inference attacks as participants.
</p></li>
</ul>

<h3>Title: Membership Inference Attacks Against Latent Factor Model. (arXiv:2301.03596v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03596">http://arxiv.org/abs/2301.03596</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03596] Membership Inference Attacks Against Latent Factor Model](http://arxiv.org/abs/2301.03596) #attack</code></li>
<li>Summary: <p>The advent of the information age has led to the problems of information
overload and unclear demands. As an information filtering system, personalized
recommendation systems predict users' behavior and preference for items and
improves users' information acquisition efficiency. However, recommendation
systems usually use highly sensitive user data for training. In this paper, we
use the latent factor model as the recommender to get the list of recommended
items, and we representing users from relevant items Compared with the
traditional member inference against machine learning classifiers. We construct
a multilayer perceptron model with two hidden layers as the attack model to
complete the member inference. Moreover, a shadow recommender is established to
derive the labeled training data for the attack model. The attack model is
trained on the dataset generated by the shadow recommender and tested on the
dataset generated by the target recommender. The experimental data show that
the AUC index of our attack model can reach 0.857 on the real dataset
MovieLens, which shows that the attack model has good performance.
</p></li>
</ul>

<h3>Title: Over-The-Air Adversarial Attacks on Deep Learning Wi-Fi Fingerprinting. (arXiv:2301.03760v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03760">http://arxiv.org/abs/2301.03760</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03760] Over-The-Air Adversarial Attacks on Deep Learning Wi-Fi Fingerprinting](http://arxiv.org/abs/2301.03760) #attack</code></li>
<li>Summary: <p>Empowered by deep neural networks (DNNs), Wi-Fi fingerprinting has recently
achieved astonishing localization performance to facilitate many
security-critical applications in wireless networks, but it is inevitably
exposed to adversarial attacks, where subtle perturbations can mislead DNNs to
wrong predictions. Such vulnerability provides new security breaches to
malicious devices for hampering wireless network security, such as
malfunctioning geofencing or asset management. The prior adversarial attack on
localization DNNs uses additive perturbations on channel state information
(CSI) measurements, which is impractical in Wi-Fi transmissions. To transcend
this limitation, this paper presents FooLoc, which fools Wi-Fi CSI
fingerprinting DNNs over the realistic wireless channel between the attacker
and the victim access point (AP). We observe that though uplink CSIs are
unknown to the attacker, the accessible downlink CSIs could be their reasonable
substitutes at the same spot. We thoroughly investigate the multiplicative and
repetitive properties of over-the-air perturbations and devise an efficient
optimization problem to generate imperceptible yet robust adversarial
perturbations. We implement FooLoc using commercial Wi-Fi APs and Wireless
Open-Access Research Platform (WARP) v3 boards in offline and online
experiments, respectively. The experimental results show that FooLoc achieves
overall attack success rates of about 70% in targeted attacks and of above 90%
in untargeted attacks with small perturbation-to-signal ratios of about -18dB.
</p></li>
</ul>

<h3>Title: Deep learning approach for interruption attacks detection in LEO satellite networks. (arXiv:2301.03998v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03998">http://arxiv.org/abs/2301.03998</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03998] Deep learning approach for interruption attacks detection in LEO satellite networks](http://arxiv.org/abs/2301.03998) #attack</code></li>
<li>Summary: <p>The developments of satellite communication in network systems require strong
and effective security plans. Attacks such as denial of service (DoS) can be
detected through the use of machine learning techniques, especially under
normal operational conditions. This work aims to provide an interruption
detection strategy for Low Earth Orbit (\textsf{LEO}) satellite networks using
deep learning algorithms. Both the training, and the testing of the proposed
models are carried out with our own communication datasets, created by
utilizing a satellite traffic (benign and malicious) that was generated using
satellite networks simulation platforms, Omnet++ and Inet. We test different
deep learning algorithms including Multi Layer Perceptron (MLP), Convolutional
Neural Network (CNN), Recurrent Neural Network (RNN), Gated Recurrent Units
(GRU), and Long Short-term Memory (LSTM). Followed by a full analysis and
investigation of detection rate in both binary classification, and
multi-classes classification that includes different interruption categories
such as Distributed DoS (DDoS), Network Jamming, and meteorological
disturbances. Simulation results for both classification types surpassed 99.33%
in terms of detection rate in scenarios of full network surveillance. However,
in more realistic scenarios, the best-recorded performance was 96.12% for the
detection of binary traffic and 94.35% for the detection of multi-class traffic
with a false positive rate of 3.72%, using a hybrid model that combines MLP and
GRU. This Deep Learning approach efficiency calls for the necessity of using
machine learning methods to improve security and to give more awareness to
search for solutions that facilitate data collection in LEO satellite networks.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Look Beyond Bias with Entropic Adversarial Data Augmentation. (arXiv:2301.03844v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03844">http://arxiv.org/abs/2301.03844</a></li>
<li>Code URL: <a href="https://github.com/liris-tduboudin/look-beyond-bias">https://github.com/liris-tduboudin/look-beyond-bias</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03844] Look Beyond Bias with Entropic Adversarial Data Augmentation](http://arxiv.org/abs/2301.03844) #robust</code></li>
<li>Summary: <p>Deep neural networks do not discriminate between spurious and causal
patterns, and will only learn the most predictive ones while ignoring the
others. This shortcut learning behaviour is detrimental to a network's ability
to generalize to an unknown test-time distribution in which the spurious
correlations do not hold anymore. Debiasing methods were developed to make
networks robust to such spurious biases but require to know in advance if a
dataset is biased and make heavy use of minority counterexamples that do not
display the majority bias of their class. In this paper, we argue that such
samples should not be necessarily needed because the ''hidden'' causal
information is often also contained in biased images. To study this idea, we
propose 3 publicly released synthetic classification benchmarks, exhibiting
predictive classification shortcuts, each of a different and challenging
nature, without any minority samples acting as counterexamples. First, we
investigate the effectiveness of several state-of-the-art strategies on our
benchmarks and show that they do not yield satisfying results on them. Then, we
propose an architecture able to succeed on our benchmarks, despite their
unusual properties, using an entropic adversarial data augmentation training
scheme. An encoder-decoder architecture is tasked to produce images that are
not recognized by a classifier, by maximizing the conditional entropy of its
outputs, and keep as much as possible of the initial content. A precise control
of the information destroyed, via a disentangling process, enables us to remove
the shortcut and leave everything else intact. Furthermore, results competitive
with the state-of-the-art on the BAR dataset ensure the applicability of our
method in real-life situations.
</p></li>
</ul>

<h3>Title: ROBUSfT: Robust Real-Time Shape-from-Template, a C++ Library. (arXiv:2301.04037v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.04037">http://arxiv.org/abs/2301.04037</a></li>
<li>Code URL: <a href="https://github.com/mrshetab/robusft">https://github.com/mrshetab/robusft</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2301.04037] ROBUSfT: Robust Real-Time Shape-from-Template, a C++ Library](http://arxiv.org/abs/2301.04037) #robust</code></li>
<li>Summary: <p>Tracking the 3D shape of a deforming object using only monocular 2D vision is
a challenging problem. This is because one should (i) infer the 3D shape from a
2D image, which is a severely underconstrained problem, and (ii) implement the
whole solution pipeline in real-time. The pipeline typically requires feature
detection and matching, mismatch filtering, 3D shape inference and feature
tracking algorithms. We propose ROBUSfT, a conventional pipeline based on a
template containing the object's rest shape, texturemap and deformation law.
ROBUSfT is ready-to-use, wide-baseline, capable of handling large deformations,
fast up to 30 fps, free of training, and robust against partial occlusions and
discontinuity in video frames. It outperforms the state-of-the-art methods in
challenging datasets. ROBUSfT is implemented as a publicly available C++
library and we provide a tutorial on how to use it in
https://github.com/mrshetab/ROBUSfT
</p></li>
</ul>

<h3>Title: Benchmarking Robustness in Neural Radiance Fields. (arXiv:2301.04075v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.04075">http://arxiv.org/abs/2301.04075</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.04075] Benchmarking Robustness in Neural Radiance Fields](http://arxiv.org/abs/2301.04075) #robust</code></li>
<li>Summary: <p>Neural Radiance Field (NeRF) has demonstrated excellent quality in novel view
synthesis, thanks to its ability to model 3D object geometries in a concise
formulation. However, current approaches to NeRF-based models rely on clean
images with accurate camera calibration, which can be difficult to obtain in
the real world, where data is often subject to corruption and distortion. In
this work, we provide the first comprehensive analysis of the robustness of
NeRF-based novel view synthesis algorithms in the presence of different types
of corruptions.
</p></li>
</ul>

<p>We find that NeRF-based models are significantly degraded in the presence of
corruption, and are more sensitive to a different set of corruptions than image
recognition models. Furthermore, we analyze the robustness of the feature
encoder in generalizable methods, which synthesize images using neural features
extracted via convolutional neural networks or transformers, and find that it
only contributes marginally to robustness. Finally, we reveal that standard
data augmentation techniques, which can significantly improve the robustness of
recognition models, do not help the robustness of NeRF-based models. We hope
that our findings will attract more researchers to study the robustness of
NeRF-based approaches and help to improve their performance in the real world.
</p>

<h3>Title: Language Models sounds the Death Knell of Knowledge Graphs. (arXiv:2301.03980v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03980">http://arxiv.org/abs/2301.03980</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03980] Language Models sounds the Death Knell of Knowledge Graphs](http://arxiv.org/abs/2301.03980) #robust</code></li>
<li>Summary: <p>Healthcare domain generates a lot of unstructured and semi-structured text.
Natural Language processing (NLP) has been used extensively to process this
data. Deep Learning based NLP especially Large Language Models (LLMs) such as
BERT have found broad acceptance and are used extensively for many
applications. A Language Model is a probability distribution over a word
sequence. Self-supervised Learning on a large corpus of data automatically
generates deep learning-based language models. BioBERT and Med-BERT are
language models pre-trained for the healthcare domain. Healthcare uses typical
NLP tasks such as question answering, information extraction, named entity
recognition, and search to simplify and improve processes. However, to ensure
robust application of the results, NLP practitioners need to normalize and
standardize them. One of the main ways of achieving normalization and
standardization is the use of Knowledge Graphs. A Knowledge Graph captures
concepts and their relationships for a specific domain, but their creation is
time-consuming and requires manual intervention from domain experts, which can
prove expensive. SNOMED CT (Systematized Nomenclature of Medicine -- Clinical
Terms), Unified Medical Language System (UMLS), and Gene Ontology (GO) are
popular ontologies from the healthcare domain. SNOMED CT and UMLS capture
concepts such as disease, symptoms and diagnosis and GO is the world's largest
source of information on the functions of genes. Healthcare has been dealing
with an explosion in information about different types of drugs, diseases, and
procedures. This paper argues that using Knowledge Graphs is not the best
solution for solving problems in this domain. We present experiments using LLMs
for the healthcare domain to demonstrate that language models provide the same
functionality as knowledge graphs, thereby making knowledge graphs redundant.
</p></li>
</ul>

<h3>Title: On adversarial robustness and the use of Wasserstein ascent-descent dynamics to enforce it. (arXiv:2301.03662v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03662">http://arxiv.org/abs/2301.03662</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03662] On adversarial robustness and the use of Wasserstein ascent-descent dynamics to enforce it](http://arxiv.org/abs/2301.03662) #robust</code></li>
<li>Summary: <p>We propose iterative algorithms to solve adversarial problems in a variety of
supervised learning settings of interest. Our algorithms, which can be
interpreted as suitable ascent-descent dynamics in Wasserstein spaces, take the
form of a system of interacting particles. These interacting particle dynamics
are shown to converge toward appropriate mean-field limit equations in certain
large number of particles regimes. In turn, we prove that, under certain
regularity assumptions, these mean-field equations converge, in the large time
limit, toward approximate Nash equilibria of the original adversarial learning
problems. We present results for nonconvex-nonconcave settings, as well as for
nonconvex-concave ones. Numerical experiments illustrate our results.
</p></li>
</ul>

<h3>Title: On the Robustness of AlphaFold: A COVID-19 Case Study. (arXiv:2301.04093v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.04093">http://arxiv.org/abs/2301.04093</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.04093] On the Robustness of AlphaFold: A COVID-19 Case Study](http://arxiv.org/abs/2301.04093) #robust</code></li>
<li>Summary: <p>Protein folding neural networks (PFNNs) such as AlphaFold predict remarkably
accurate structures of proteins compared to other approaches. However, the
robustness of such networks has heretofore not been explored. This is
particularly relevant given the broad social implications of such technologies
and the fact that biologically small perturbations in the protein sequence do
not generally lead to drastic changes in the protein structure. In this paper,
we demonstrate that AlphaFold does not exhibit such robustness despite its high
accuracy. This raises the challenge of detecting and quantifying the extent to
which these predicted protein structures can be trusted. To measure the
robustness of the predicted structures, we utilize (i) the root-mean-square
deviation (RMSD) and (ii) the Global Distance Test (GDT) similarity measure
between the predicted structure of the original sequence and the structure of
its adversarially perturbed version. We prove that the problem of minimally
perturbing protein sequences to fool protein folding neural networks is
NP-complete. Based on the well-established BLOSUM62 sequence alignment scoring
matrix, we generate adversarial protein sequences and show that the RMSD
between the predicted protein structure and the structure of the original
sequence are very large when the adversarial changes are bounded by (i) 20
units in the BLOSUM62 distance, and (ii) five residues (out of hundreds or
thousands of residues) in the given protein sequence. In our experimental
evaluation, we consider 111 COVID-19 proteins in the Universal Protein resource
(UniProt), a central resource for protein data managed by the European
Bioinformatics Institute, Swiss Institute of Bioinformatics, and the US Protein
Information Resource. These result in an overall GDT similarity test score
average of around 34%, demonstrating a substantial drop in the performance of
AlphaFold.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Is Federated Learning a Practical PET Yet?. (arXiv:2301.04017v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.04017">http://arxiv.org/abs/2301.04017</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.04017] Is Federated Learning a Practical PET Yet?](http://arxiv.org/abs/2301.04017) #federate</code></li>
<li>Summary: <p>Federated learning (FL) is a framework for users to jointly train a machine
learning model. FL is promoted as a privacy-enhancing technology (PET) that
provides data minimization: data never "leaves" personal devices and users
share only model updates with a server (e.g., a company) coordinating the
distributed training. We assess the realistic (i.e., worst-case) privacy
guarantees that are provided to users who are unable to trust the server. To
this end, we propose an attack against FL protected with distributed
differential privacy (DDP) and secure aggregation (SA). The attack method is
based on the introduction of Sybil devices that deviate from the protocol to
expose individual users' data for reconstruction by the server. The underlying
root cause for the vulnerability to our attack is the power imbalance. The
server orchestrates the whole protocol and users are given little guarantees
about the selection of other users participating in the protocol. Moving
forward, we discuss requirements for an FL protocol to guarantee DDP without
asking users to trust the server. We conclude that such systems are not yet
practical.
</p></li>
</ul>

<h3>Title: Federated Learning for Energy Constrained IoT devices: A systematic mapping study. (arXiv:2301.03720v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03720">http://arxiv.org/abs/2301.03720</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03720] Federated Learning for Energy Constrained IoT devices: A systematic mapping study](http://arxiv.org/abs/2301.03720) #federate</code></li>
<li>Summary: <p>Federated Machine Learning (Fed ML) is a new distributed machine learning
technique applied to collaboratively train a global model using clients local
data without transmitting it. Nodes only send parameter updates (e.g., weight
updates in the case of neural networks), which are fused together by the server
to build the global model. By not divulging node data, Fed ML guarantees its
confidentiality, a crucial aspect of network security, which enables it to be
used in the context of data-sensitive Internet of Things (IoT) and mobile
applications, such as smart Geo-location and the smart grid. However, most IoT
devices are particularly energy constrained, which raises the need to optimize
the Fed ML process for efficient training tasks and optimized power
consumption. In this paper, we conduct, to the best of our knowledge, the first
Systematic Mapping Study (SMS) on Fed ML optimization techniques for
energy-constrained IoT devices. From a total of more than 800 papers, we select
67 that satisfy our criteria and give a structured overview of the field using
a set of carefully chosen research questions. Finally, we attempt to provide an
analysis of the energy-constrained Fed ML state of the art and try to outline
some potential recommendations for the research community.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Sequential Fair Resource Allocation under a Markov Decision Process Framework. (arXiv:2301.03758v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03758">http://arxiv.org/abs/2301.03758</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03758] Sequential Fair Resource Allocation under a Markov Decision Process Framework](http://arxiv.org/abs/2301.03758) #fair</code></li>
<li>Summary: <p>We study the sequential decision-making problem of allocating a limited
resource to agents that reveal their stochastic demands on arrival over a
finite horizon. Our goal is to design fair allocation algorithms that exhaust
the available resource budget. This is challenging in sequential settings where
information on future demands is not available at the time of decision-making.
We formulate the problem as a discrete time Markov decision process (MDP). We
propose a new algorithm, SAFFE, that makes fair allocations with respect to the
entire demands revealed over the horizon by accounting for expected future
demands at each arrival time. The algorithm introduces regularization which
enables the prioritization of current revealed demands over future potential
demands depending on the uncertainty in agents' future demands. Using the MDP
formulation, we show that SAFFE optimizes allocations based on an upper bound
on the Nash Social Welfare fairness objective, and we bound its gap to
optimality with the use of concentration bounds on total future demands. Using
synthetic and real data, we compare the performance of SAFFE against existing
approaches and a reinforcement learning policy trained on the MDP. We show that
SAFFE leads to more fair and efficient allocations and achieves
close-to-optimal performance in settings with dense arrivals.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Learning Support and Trivial Prototypes for Interpretable Image Classification. (arXiv:2301.04011v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.04011">http://arxiv.org/abs/2301.04011</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.04011] Learning Support and Trivial Prototypes for Interpretable Image Classification](http://arxiv.org/abs/2301.04011) #interpretability</code></li>
<li>Summary: <p>Prototypical part network (ProtoPNet) methods have been designed to achieve
interpretable classification by associating predictions with a set of training
prototypes, which we refer to as trivial (i.e., easy-to-learn) prototypes
because they are trained to lie far from the classification boundary in the
feature space. Note that it is possible to make an analogy between ProtoPNet
and support vector machine (SVM) given that the classification from both
methods relies on computing similarity with a set of training points (i.e.,
trivial prototypes in ProtoPNet, and support vectors in SVM). However, while
trivial prototypes are located far from the classification boundary, support
vectors are located close to this boundary, and we argue that this discrepancy
with the well-established SVM theory can result in ProtoPNet models with
suboptimal classification accuracy. In this paper, we aim to improve the
classification accuracy of ProtoPNet with a new method to learn support
prototypes that lie near the classification boundary in the feature space, as
suggested by the SVM theory. In addition, we target the improvement of
classification interpretability with a new model, named ST-ProtoPNet, which
exploits our support prototypes and the trivial prototypes to provide
complementary interpretability information. Experimental results on
CUB-200-2011, Stanford Cars, and Stanford Dogs datasets demonstrate that the
proposed method achieves state-of-the-art classification accuracy and produces
more visually meaningful and diverse prototypes.
</p></li>
</ul>

<h3>Title: Differentiable modeling to unify machine learning and physical models and advance Geosciences. (arXiv:2301.04027v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.04027">http://arxiv.org/abs/2301.04027</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.04027] Differentiable modeling to unify machine learning and physical models and advance Geosciences](http://arxiv.org/abs/2301.04027) #interpretability</code></li>
<li>Summary: <p>Process-Based Modeling (PBM) and Machine Learning (ML) are often perceived as
distinct paradigms in the geosciences. Here we present differentiable
geoscientific modeling as a powerful pathway toward dissolving the perceived
barrier between them and ushering in a paradigm shift. For decades, PBM offered
benefits in interpretability and physical consistency but struggled to
efficiently leverage large datasets. ML methods, especially deep networks,
presented strong predictive skills yet lacked the ability to answer specific
scientific questions. While various methods have been proposed for ML-physics
integration, an important underlying theme -- differentiable modeling -- is not
sufficiently recognized. Here we outline the concepts, applicability, and
significance of differentiable geoscientific modeling (DG). "Differentiable"
refers to accurately and efficiently calculating gradients with respect to
model variables, critically enabling the learning of high-dimensional unknown
relationships. DG refers to a range of methods connecting varying amounts of
prior knowledge to neural networks and training them together, capturing a
different scope than physics-guided machine learning and emphasizing first
principles. Preliminary evidence suggests DG offers better interpretability and
causality than ML, improved generalizability and extrapolation capability, and
strong potential for knowledge discovery, while approaching the performance of
purely data-driven ML. DG models require less training data while scaling
favorably in performance and efficiency with increasing amounts of data. With
DG, geoscientists may be better able to frame and investigate questions, test
hypotheses, and discover unrecognized linkages.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: DiffTalk: Crafting Diffusion Models for Generalized Talking Head Synthesis. (arXiv:2301.03786v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03786">http://arxiv.org/abs/2301.03786</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03786] DiffTalk: Crafting Diffusion Models for Generalized Talking Head Synthesis](http://arxiv.org/abs/2301.03786) #diffusion</code></li>
<li>Summary: <p>Talking head synthesis is a promising approach for the video production
industry. Recently, a lot of effort has been devoted in this research area to
improve the generation quality or enhance the model generalization. However,
there are few works able to address both issues simultaneously, which is
essential for practical applications. To this end, in this paper, we turn
attention to the emerging powerful Latent Diffusion Models, and model the
Talking head generation as an audio-driven temporally coherent denoising
process (DiffTalk). More specifically, instead of employing audio signals as
the single driving factor, we investigate the control mechanism of the talking
face, and incorporate reference face images and landmarks as conditions for
personality-aware generalized synthesis. In this way, the proposed DiffTalk is
capable of producing high-quality talking head videos in synchronization with
the source audio, and more importantly, it can be naturally generalized across
different identities without any further fine-tuning. Additionally, our
DiffTalk can be gracefully tailored for higher-resolution synthesis with
negligible extra computational cost. Extensive experiments show that the
proposed DiffTalk efficiently synthesizes high-fidelity audio-driven talking
head videos for generalized novel identities. For more video results, please
refer to this demonstration
\url{https://cloud.tsinghua.edu.cn/f/e13f5aad2f4c4f898ae7/}.
</p></li>
</ul>

<h3>Title: Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models. (arXiv:2301.03949v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.03949">http://arxiv.org/abs/2301.03949</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.03949] Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models](http://arxiv.org/abs/2301.03949) #diffusion</code></li>
<li>Summary: <p>Diffusion-based generative models have recently emerged as powerful solutions
for high-quality synthesis in multiple domains. Leveraging the bidirectional
Markov chains, diffusion probabilistic models generate samples by inferring the
reversed Markov chain based on the learned distribution mapping at the forward
diffusion process. In this work, we propose Modiff, a conditional paradigm that
benefits from the denoising diffusion probabilistic model (DDPM) to tackle the
problem of realistic and diverse action-conditioned 3D skeleton-based motion
generation. We are a pioneering attempt that uses DDPM to synthesize a variable
number of motion sequences conditioned on a categorical action. We evaluate our
approach on the large-scale NTU RGB+D dataset and show improvements over
state-of-the-art motion generation methods.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
