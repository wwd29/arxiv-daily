<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-04-01</h1>
<h3>Title: AttentionStore: Cost-effective Attention Reuse across Multi-turn  Conversations in Large Language Model Serving</h3>
<ul>
<li><strong>Authors: </strong>Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje Jevdjic, Junbo Deng, Xingkun Yang, Zhou Yu, Pengfei Zuo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19708">https://arxiv.org/abs/2403.19708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19708">https://arxiv.org/pdf/2403.19708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19708]] AttentionStore: Cost-effective Attention Reuse across Multi-turn  Conversations in Large Language Model Serving(https://arxiv.org/abs/2403.19708)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Interacting with humans through multi-turn conversations is a fundamental feature of large language models (LLMs). However, existing LLM serving engines for executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs. To address the problem, this paper proposes AttentionStore, a new attention mechanism that enables the reuse of KV caches (i.e., attention reuse) across multi-turn conversations, significantly reducing the repetitive computation overheads. AttentionStore maintains a hierarchical KV caching system that leverages cost-effective memory/storage mediums to save KV caches for all requests. To reduce KV cache access overheads from slow mediums, AttentionStore employs layer-wise pre-loading and asynchronous saving schemes to overlap the KV cache access with the GPU computation. To ensure that the KV caches to be accessed are placed in the fastest hierarchy, AttentionStore employs scheduler-aware fetching and eviction schemes to consciously place the KV caches in different layers based on the hints from the inference job scheduler. To avoid the invalidation of the saved KV caches incurred by context window overflow, AttentionStore enables the saved KV caches to remain valid via decoupling the positional encoding and effectively truncating the KV caches. Extensive experimental results demonstrate that AttentionStore significantly decreases the time to the first token (TTFT) by up to 88%, improves the prompt prefilling throughput by 8.2$\times$ for multi-turn conversations, and reduces the end-to-end inference cost by up to 56%. For long sequence inference, AttentionStore reduces the TTFT by up to 95% and improves the prompt prefilling throughput by 22$\times$.</li>
</ul>

<h3>Title: STRUM-LLM: Attributed and Structured Contrastive Summarization</h3>
<ul>
<li><strong>Authors: </strong>Beliz Gunel, James B. Wendt, Jing Xie, Yichao Zhou, Nguyen Vo, Zachary Fisher, Sandeep Tata</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19710">https://arxiv.org/abs/2403.19710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19710">https://arxiv.org/pdf/2403.19710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19710]] STRUM-LLM: Attributed and Structured Contrastive Summarization(https://arxiv.org/abs/2403.19710)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Users often struggle with decision-making between two options (A vs B), as it usually requires time-consuming research across multiple web pages. We propose STRUM-LLM that addresses this challenge by generating attributed, structured, and helpful contrastive summaries that highlight key differences between the two options. STRUM-LLM identifies helpful contrast: the specific attributes along which the two options differ significantly and which are most likely to influence the user's decision. Our technique is domain-agnostic, and does not require any human-labeled data or fixed attribute list as supervision. STRUM-LLM attributes all extractions back to the input sources along with textual evidence, and it does not have a limit on the length of input sources that it can process. STRUM-LLM Distilled has 100x more throughput than the models with comparable performance while being 10x smaller. In this paper, we provide extensive evaluations for our method and lay out future directions for our currently deployed system.</li>
</ul>

<h3>Title: Capability-aware Prompt Reformulation Learning for Text-to-Image  Generation</h3>
<ul>
<li><strong>Authors: </strong>Jingtao Zhan, Qingyao Ai, Yiqun Liu, Jia Chen, Shaoping Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19716">https://arxiv.org/abs/2403.19716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19716">https://arxiv.org/pdf/2403.19716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19716]] Capability-aware Prompt Reformulation Learning for Text-to-Image  Generation(https://arxiv.org/abs/2403.19716)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Text-to-image generation systems have emerged as revolutionary tools in the realm of artistic creation, offering unprecedented ease in transforming textual prompts into visual art. However, the efficacy of these systems is intricately linked to the quality of user-provided prompts, which often poses a challenge to users unfamiliar with prompt crafting. This paper addresses this challenge by leveraging user reformulation data from interaction logs to develop an automatic prompt reformulation model. Our in-depth analysis of these logs reveals that user prompt reformulation is heavily dependent on the individual user's capability, resulting in significant variance in the quality of reformulation pairs. To effectively use this data for training, we introduce the Capability-aware Prompt Reformulation (CAPR) framework. CAPR innovatively integrates user capability into the reformulation process through two key components: the Conditional Reformulation Model (CRM) and Configurable Capability Features (CCF). CRM reformulates prompts according to a specified user capability, as represented by CCF. The CCF, in turn, offers the flexibility to tune and guide the CRM's behavior. This enables CAPR to effectively learn diverse reformulation strategies across various user capacities and to simulate high-capability user reformulation during inference. Extensive experiments on standard text-to-image generation benchmarks showcase CAPR's superior performance over existing baselines and its remarkable robustness on unseen systems. Furthermore, comprehensive analyses validate the effectiveness of different components. CAPR can facilitate user-friendly interaction with text-to-image systems and make advanced artistic creation more achievable for a broader range of users.</li>
</ul>

<h3>Title: A Picture is Worth 500 Labels: A Case Study of Demographic Disparities  in Local Machine Learning Models for Instagram and TikTok</h3>
<ul>
<li><strong>Authors: </strong>Jack West, Lea Thiemt, Shimaa Ahmed, Maggie Bartig, Kassem Fawaz, Suman Banerjee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19717">https://arxiv.org/abs/2403.19717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19717">https://arxiv.org/pdf/2403.19717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19717]] A Picture is Worth 500 Labels: A Case Study of Demographic Disparities  in Local Machine Learning Models for Instagram and TikTok(https://arxiv.org/abs/2403.19717)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, extraction</a></li>
<li><strong>Abstract: </strong>Mobile apps have embraced user privacy by moving their data processing to the user's smartphone. Advanced machine learning (ML) models, such as vision models, can now locally analyze user images to extract insights that drive several functionalities. Capitalizing on this new processing model of locally analyzing user images, we analyze two popular social media apps, TikTok and Instagram, to reveal (1) what insights vision models in both apps infer about users from their image and video data and (2) whether these models exhibit performance disparities with respect to demographics. As vision models provide signals for sensitive technologies like age verification and facial recognition, understanding potential biases in these models is crucial for ensuring that users receive equitable and accurate services. We develop a novel method for capturing and evaluating ML tasks in mobile apps, overcoming challenges like code obfuscation, native code execution, and scalability. Our method comprises ML task detection, ML pipeline reconstruction, and ML performance assessment, specifically focusing on demographic disparities. We apply our methodology to TikTok and Instagram, revealing significant insights. For TikTok, we find issues in age and gender prediction accuracy, particularly for minors and Black individuals. In Instagram, our analysis uncovers demographic disparities in the extraction of over 500 visual concepts from images, with evidence of spurious correlations between demographic features and certain concepts.</li>
</ul>

<h3>Title: Computationally and Memory-Efficient Robust Predictive Analytics Using  Big Data</h3>
<ul>
<li><strong>Authors: </strong>Daniel Menges, Adil Rasheed</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19721">https://arxiv.org/abs/2403.19721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19721">https://arxiv.org/pdf/2403.19721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19721]] Computationally and Memory-Efficient Robust Predictive Analytics Using  Big Data(https://arxiv.org/abs/2403.19721)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In the current data-intensive era, big data has become a significant asset for Artificial Intelligence (AI), serving as a foundation for developing data-driven models and providing insight into various unknown fields. This study navigates through the challenges of data uncertainties, storage limitations, and predictive data-driven modeling using big data. We utilize Robust Principal Component Analysis (RPCA) for effective noise reduction and outlier elimination, and Optimal Sensor Placement (OSP) for efficient data compression and storage. The proposed OSP technique enables data compression without substantial information loss while simultaneously reducing storage needs. While RPCA offers an enhanced alternative to traditional Principal Component Analysis (PCA) for high-dimensional data management, the scope of this work extends its utilization, focusing on robust, data-driven modeling applicable to huge data sets in real-time. For that purpose, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network, are applied to model and predict data based on a low-dimensional subset obtained from OSP, leading to a crucial acceleration of the training phase. LSTMs are feasible for capturing long-term dependencies in time series data, making them particularly suited for predicting the future states of physical systems on historical data. All the presented algorithms are not only theorized but also simulated and validated using real thermal imaging data mapping a ship's engine.</li>
</ul>

<h3>Title: HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for  Few-shot Complex Table Understanding</h3>
<ul>
<li><strong>Authors: </strong>Rihui Jin, Yu Li, Guilin Qi, Nan Hu, Yuan-Fang Li, Jiaoyan Chen, Jianan Wang, Yongrui Chen, Dehai Min</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19723">https://arxiv.org/abs/2403.19723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19723">https://arxiv.org/pdf/2403.19723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19723]] HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for  Few-shot Complex Table Understanding(https://arxiv.org/abs/2403.19723)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Table understanding (TU) has achieved promising advancements, but it faces the challenges of the scarcity of manually labeled tables and the presence of complex table structures.To address these challenges, we propose HGT, a framework with a heterogeneous graph (HG)-enhanced large language model (LLM) to tackle few-shot TU tasks.It leverages the LLM by aligning the table semantics with the LLM's parametric knowledge through soft prompts and instruction turning and deals with complex tables by a multi-task pre-training scheme involving three novel multi-granularity self-supervised HG pre-training objectives.We empirically demonstrate the effectiveness of HGT, showing that it outperforms the SOTA for few-shot complex TU on several benchmarks.</li>
</ul>

<h3>Title: MUGC: Machine Generated versus User Generated Content Detection</h3>
<ul>
<li><strong>Authors: </strong>Yaqi Xie, Anjali Rawal, Yujing Cen, Dixuan Zhao, Sunil K Narang, Shanu Sushmita</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19725">https://arxiv.org/abs/2403.19725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19725">https://arxiv.org/pdf/2403.19725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19725]] MUGC: Machine Generated versus User Generated Content Detection(https://arxiv.org/abs/2403.19725)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>As advanced modern systems like deep neural networks (DNNs) and generative AI continue to enhance their capabilities in producing convincing and realistic content, the need to distinguish between user-generated and machine generated content is becoming increasingly evident. In this research, we undertake a comparative evaluation of eight traditional machine-learning algorithms to distinguish between machine-generated and human-generated data across three diverse datasets: Poems, Abstracts, and Essays. Our results indicate that traditional methods demonstrate a high level of accuracy in identifying machine-generated data, reflecting the documented effectiveness of popular pre-trained models like RoBERT. We note that machine-generated texts tend to be shorter and exhibit less word variety compared to human-generated content. While specific domain-related keywords commonly utilized by humans, albeit disregarded by current LLMs (Large Language Models), may contribute to this high detection accuracy, we show that deeper word representations like word2vec can capture subtle semantic variances. Furthermore, readability, bias, moral, and affect comparisons reveal a discernible contrast between machine-generated and human generated content. There are variations in expression styles and potentially underlying biases in the data sources (human and machine-generated). This study provides valuable insights into the advancing capacities and challenges associated with machine-generated content across various domains.</li>
</ul>

<h3>Title: A Benchmark Evaluation of Clinical Named Entity Recognition in French</h3>
<ul>
<li><strong>Authors: </strong>Nesrine Bannour (STL), Christophe Servan (STL), Aurélie Névéol (STL), Xavier Tannier (LIMICS)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19726">https://arxiv.org/abs/2403.19726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19726">https://arxiv.org/pdf/2403.19726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19726]] A Benchmark Evaluation of Clinical Named Entity Recognition in French(https://arxiv.org/abs/2403.19726)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Background: Transformer-based language models have shown strong performance on many Natural LanguageProcessing (NLP) tasks. Masked Language Models (MLMs) attract sustained interest because they can be adaptedto different languages and sub-domains through training or fine-tuning on specific corpora while remaining lighterthan modern Large Language Models (LLMs). Recently, several MLMs have been released for the biomedicaldomain in French, and experiments suggest that they outperform standard French counterparts. However, nosystematic evaluation comparing all models on the same corpora is available. Objective: This paper presentsan evaluation of masked language models for biomedical French on the task of clinical named entity recognition.Material and methods: We evaluate biomedical models CamemBERT-bio and DrBERT and compare them tostandard French models CamemBERT, FlauBERT and FrALBERT as well as multilingual mBERT using three publicallyavailable corpora for clinical named entity recognition in French. The evaluation set-up relies on gold-standardcorpora as released by the corpus developers. Results: Results suggest that CamemBERT-bio outperformsDrBERT consistently while FlauBERT offers competitive performance and FrAlBERT achieves the lowest carbonfootprint. Conclusion: This is the first benchmark evaluation of biomedical masked language models for Frenchclinical entity recognition that compares model performance consistently on nested entity recognition using metricscovering performance and environmental impact.</li>
</ul>

<h3>Title: MIST: Mitigating Intersectional Bias with Disentangled Cross-Attention  Editing in Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Hidir Yesiltepe, Kiymet Akdemir, Pinar Yanardag</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19738">https://arxiv.org/abs/2403.19738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19738">https://arxiv.org/pdf/2403.19738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19738]] MIST: Mitigating Intersectional Bias with Disentangled Cross-Attention  Editing in Text-to-Image Diffusion Models(https://arxiv.org/abs/2403.19738)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based text-to-image models have rapidly gained popularity for their ability to generate detailed and realistic images from textual descriptions. However, these models often reflect the biases present in their training data, especially impacting marginalized groups. While prior efforts to debias language models have focused on addressing specific biases, such as racial or gender biases, efforts to tackle intersectional bias have been limited. Intersectional bias refers to the unique form of bias experienced by individuals at the intersection of multiple social identities. Addressing intersectional bias is crucial because it amplifies the negative effects of discrimination based on race, gender, and other identities. In this paper, we introduce a method that addresses intersectional bias in diffusion-based text-to-image models by modifying cross-attention maps in a disentangled manner. Our approach utilizes a pre-trained Stable Diffusion model, eliminates the need for an additional set of reference images, and preserves the original quality for unaltered concepts. Comprehensive experiments demonstrate that our method surpasses existing approaches in mitigating both single and intersectional biases across various attributes. We make our source code and debiased models for various attributes available to encourage fairness in generative models and to support further research.</li>
</ul>

<h3>Title: Using Deep Learning to Increase Eye-Tracking Robustness, Accuracy, and  Precision in Virtual Reality</h3>
<ul>
<li><strong>Authors: </strong>Kevin Barkevich, Reynold Bailey, Gabriel J. Diaz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19768">https://arxiv.org/abs/2403.19768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19768">https://arxiv.org/pdf/2403.19768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19768]] Using Deep Learning to Increase Eye-Tracking Robustness, Accuracy, and  Precision in Virtual Reality(https://arxiv.org/abs/2403.19768)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Algorithms for the estimation of gaze direction from mobile and video-based eye trackers typically involve tracking a feature of the eye that moves through the eye camera image in a way that covaries with the shifting gaze direction, such as the center or boundaries of the pupil. Tracking these features using traditional computer vision techniques can be difficult due to partial occlusion and environmental reflections. Although recent efforts to use machine learning (ML) for pupil tracking have demonstrated superior results when evaluated using standard measures of segmentation performance, little is known of how these networks may affect the quality of the final gaze estimate. This work provides an objective assessment of the impact of several contemporary ML-based methods for eye feature tracking when the subsequent gaze estimate is produced using either feature-based or model-based methods. Metrics include the accuracy and precision of the gaze estimate, as well as drop-out rate.</li>
</ul>

<h3>Title: ShapeFusion: A 3D diffusion model for localized shape editing</h3>
<ul>
<li><strong>Authors: </strong>Rolandos Alexandros Potamias, Michail Tarasiou Stylianos Ploumpis, Stefanos Zafeiriou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19773">https://arxiv.org/abs/2403.19773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19773">https://arxiv.org/pdf/2403.19773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19773]] ShapeFusion: A 3D diffusion model for localized shape editing(https://arxiv.org/abs/2403.19773)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In the realm of 3D computer vision, parametric models have emerged as a ground-breaking methodology for the creation of realistic and expressive 3D avatars. Traditionally, they rely on Principal Component Analysis (PCA), given its ability to decompose data to an orthonormal space that maximally captures shape variations. However, due to the orthogonality constraints and the global nature of PCA's decomposition, these models struggle to perform localized and disentangled editing of 3D shapes, which severely affects their use in applications requiring fine control such as face sculpting. In this paper, we leverage diffusion models to enable diverse and fully localized edits on 3D meshes, while completely preserving the un-edited regions. We propose an effective diffusion masking training strategy that, by design, facilitates localized manipulation of any shape region, without being limited to predefined regions or to sparse sets of predefined control vertices. Following our framework, a user can explicitly set their manipulation region of choice and define an arbitrary set of vertices as handles to edit a 3D mesh. Compared to the current state-of-the-art our method leads to more interpretable shape manipulations than methods relying on latent code state, greater localization and generation diversity while offering faster inference than optimization based approaches. Project page: https://rolpotamias.github.io/Shapefusion/</li>
</ul>

<h3>Title: ENet-21: An Optimized light CNN Structure for Lane Detection</h3>
<ul>
<li><strong>Authors: </strong>Seyed Rasoul Hosseini, Mohammad Teshnehlab</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19782">https://arxiv.org/abs/2403.19782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19782">https://arxiv.org/pdf/2403.19782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19782]] ENet-21: An Optimized light CNN Structure for Lane Detection(https://arxiv.org/abs/2403.19782)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Lane detection for autonomous vehicles is an important concept, yet it is a challenging issue of driver assistance systems in modern vehicles. The emergence of deep learning leads to significant progress in self-driving cars. Conventional deep learning-based methods handle lane detection problems as a binary segmentation task and determine whether a pixel belongs to a line. These methods rely on the assumption of a fixed number of lanes, which does not always work. This study aims to develop an optimal structure for the lane detection problem, offering a promising solution for driver assistance features in modern vehicles by utilizing a machine learning method consisting of binary segmentation and Affinity Fields that can manage varying numbers of lanes and lane change scenarios. In this approach, the Convolutional Neural Network (CNN), is selected as a feature extractor, and the final output is obtained through clustering of the semantic segmentation and Affinity Field outputs. Our method uses less complex CNN architecture than exi</li>
</ul>

<h3>Title: Zero-shot Prompt-based Video Encoder for Surgical Gesture Recognition</h3>
<ul>
<li><strong>Authors: </strong>Mingxing Rao, Yinhong Qin, Soheil Kolouri, Jie Ying Wu, Daniel Moyer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19786">https://arxiv.org/abs/2403.19786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19786">https://arxiv.org/pdf/2403.19786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19786]] Zero-shot Prompt-based Video Encoder for Surgical Gesture Recognition(https://arxiv.org/abs/2403.19786)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Purpose: Surgical video is an important data stream for gesture recognition. Thus, robust visual encoders for those data-streams is similarly important. Methods: Leveraging the Bridge-Prompt framework, we fine-tune a pre-trained vision-text model (CLIP) for gesture recognition in surgical videos. This can utilize extensive outside video data such as text, but also make use of label meta-data and weakly supervised contrastive losses. Results: Our experiments show that prompt-based video encoder outperforms standard encoders in surgical gesture recognition tasks. Notably, it displays strong performance in zero-shot scenarios, where gestures/tasks that were not provided during the encoder training phase are included in the prediction phase. Additionally, we measure the benefit of inclusion text descriptions in the feature extractor training schema. Conclusion: Bridge-Prompt and similar pre-trained+fine-tuned video encoder models present significant visual representation for surgical robotics, especially in gesture recognition tasks. Given the diverse range of surgical tasks (gestures), the ability of these models to zero-shot transfer without the need for any task (gesture) specific retraining makes them invaluable.</li>
</ul>

<h3>Title: JIST: Joint Image and Sequence Training for Sequential Visual Place  Recognition</h3>
<ul>
<li><strong>Authors: </strong>Gabriele Berton, Gabriele Trivigno, Barbara Caputo, Carlo Masone</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19787">https://arxiv.org/abs/2403.19787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19787">https://arxiv.org/pdf/2403.19787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19787]] JIST: Joint Image and Sequence Training for Sequential Visual Place  Recognition(https://arxiv.org/abs/2403.19787)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Visual Place Recognition aims at recognizing previously visited places by relying on visual clues, and it is used in robotics applications for SLAM and localization. Since typically a mobile robot has access to a continuous stream of frames, this task is naturally cast as a sequence-to-sequence localization problem. Nevertheless, obtaining sequences of labelled data is much more expensive than collecting isolated images, which can be done in an automated way with little supervision. As a mitigation to this problem, we propose a novel Joint Image and Sequence Training protocol (JIST) that leverages large uncurated sets of images through a multi-task learning framework. With JIST we also introduce SeqGeM, an aggregation layer that revisits the popular GeM pooling to produce a single robust and compact embedding from a sequence of single-frame embeddings. We show that our model is able to outperform previous state of the art while being faster, using 8 times smaller descriptors, having a lighter architecture and allowing to process sequences of various lengths. Code is available at https://github.com/ga1i13o/JIST</li>
</ul>

<h3>Title: MAPL: Model Agnostic Peer-to-peer Learning</h3>
<ul>
<li><strong>Authors: </strong>Sayak Mukherjee, Andrea Simonetto, Hadi Jamali-Rad</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19792">https://arxiv.org/abs/2403.19792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19792">https://arxiv.org/pdf/2403.19792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19792]] MAPL: Model Agnostic Peer-to-peer Learning(https://arxiv.org/abs/2403.19792)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Effective collaboration among heterogeneous clients in a decentralized setting is a rather unexplored avenue in the literature. To structurally address this, we introduce Model Agnostic Peer-to-peer Learning (coined as MAPL) a novel approach to simultaneously learn heterogeneous personalized models as well as a collaboration graph through peer-to-peer communication among neighboring clients. MAPL is comprised of two main modules: (i) local-level Personalized Model Learning (PML), leveraging a combination of intra- and inter-client contrastive losses; (ii) network-wide decentralized Collaborative Graph Learning (CGL) dynamically refining collaboration weights in a privacy-preserving manner based on local task similarities. Our extensive experimentation demonstrates the efficacy of MAPL and its competitive (or, in most cases, superior) performance compared to its centralized model-agnostic counterparts, without relying on any central server. Our code is available and can be accessed here: https://github.com/SayakMukherjee/MAPL</li>
</ul>

<h3>Title: Efficient 3D Instance Mapping and Localization with Neural Fields</h3>
<ul>
<li><strong>Authors: </strong>George Tang, Krishna Murthy Jatavallabhula, Antonio Torralba</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19797">https://arxiv.org/abs/2403.19797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19797">https://arxiv.org/pdf/2403.19797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19797]] Efficient 3D Instance Mapping and Localization with Neural Fields(https://arxiv.org/abs/2403.19797)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We tackle the problem of learning an implicit scene representation for 3D instance segmentation from a sequence of posed RGB images. Towards this, we introduce 3DIML, a novel framework that efficiently learns a label field that may be rendered from novel viewpoints to produce view-consistent instance segmentation masks. 3DIML significantly improves upon training and inference runtimes of existing implicit scene representation based methods. Opposed to prior art that optimizes a neural field in a self-supervised manner, requiring complicated training procedures and loss function design, 3DIML leverages a two-phase process. The first phase, InstanceMap, takes as input 2D segmentation masks of the image sequence generated by a frontend instance segmentation model, and associates corresponding masks across images to 3D labels. These almost view-consistent pseudolabel masks are then used in the second phase, InstanceLift, to supervise the training of a neural label field, which interpolates regions missed by InstanceMap and resolves ambiguities. Additionally, we introduce InstanceLoc, which enables near realtime localization of instance masks given a trained label field and an off-the-shelf image segmentation model by fusing outputs from both. We evaluate 3DIML on sequences from the Replica and ScanNet datasets and demonstrate 3DIML's effectiveness under mild assumptions for the image sequences. We achieve a 14-24x speedup over existing implicit scene representation methods with comparable quality, showcasing its potential to facilitate faster and more effective 3D scene understanding.</li>
</ul>

<h3>Title: Developing Healthcare Language Model Embedding Spaces</h3>
<ul>
<li><strong>Authors: </strong>Niall Taylor, Dan Schofield, Andrey Kormilitzin, Dan W Joyce, Alejo Nevado-Holgado</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19802">https://arxiv.org/abs/2403.19802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19802">https://arxiv.org/pdf/2403.19802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19802]] Developing Healthcare Language Model Embedding Spaces(https://arxiv.org/abs/2403.19802)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Pre-trained Large Language Models (LLMs) often struggle on out-of-domain datasets like healthcare focused text. We explore specialized pre-training to adapt smaller LLMs to different healthcare datasets. Three methods are assessed: traditional masked language modeling, Deep Contrastive Learning for Unsupervised Textual Representations (DeCLUTR), and a novel pre-training objective utilizing metadata categories from the healthcare settings. These schemes are evaluated on downstream document classification tasks for each dataset, with additional analysis of the resultant embedding spaces. Contrastively trained models outperform other approaches on the classification tasks, delivering strong performance from limited labeled data and with fewer model parameter updates required. While metadata-based pre-training does not further improve classifications across the datasets, it yields interesting embedding cluster separability. All domain adapted LLMs outperform their publicly available general base LLM, validating the importance of domain-specialization. This research illustrates efficient approaches to instill healthcare competency in compact LLMs even under tight computational budgets, an essential capability for responsible and sustainable deployment in local healthcare settings. We provide pre-training guidelines for specialized healthcare LLMs, motivate continued inquiry into contrastive objectives, and demonstrates adaptation techniques to align small LLMs with privacy-sensitive medical tasks.</li>
</ul>

<h3>Title: Vulnerabilities of smart contracts and mitigation schemes: A  Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Wejdene Haouari, Abdelhakim Senhaji Hafid, Marios Fokaefs</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19805">https://arxiv.org/abs/2403.19805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19805">https://arxiv.org/pdf/2403.19805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19805]] Vulnerabilities of smart contracts and mitigation schemes: A  Comprehensive Survey(https://arxiv.org/abs/2403.19805)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Ethereum smart contracts are highly powerful; they are immutable and retain massive amounts of tokens. However, smart contracts keep attracting attackers to benefit from smart contract flaws and Ethereum's unexpected behaviour. Thus, methodologies and tools have been proposed to help implementing secure smart contracts and to evaluate the security of smart contracts already deployed. Most related surveys focus on tools without discussing the logic behind them; in addition, they assess the tools based on papers rather than testing the tools and collecting community feedback. Other surveys lack guidelines on how to use tools specific to smart contract functionalities. This paper presents a literature review combined with an experimental report, that aims to assist developers in developing secure smarts, with a novel emphasis on the challenges and vulnerabilities introduced by NFT fractionalization by addressing the unique risks of dividing NFT ownership into tradeable units called fractions. It provides a list of frequent vulnerabilities and corresponding mitigation solutions. In addition, it evaluates the community's most widely used tools by executing and testing them on sample smart contracts. Finally, a complete guidance on how to secure smart contracts is presented.</li>
</ul>

<h3>Title: Feature-Based Echo-State Networks: A Step Towards Interpretability and  Minimalism in Reservoir Computer</h3>
<ul>
<li><strong>Authors: </strong>Debdipta Goswami</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19806">https://arxiv.org/abs/2403.19806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19806">https://arxiv.org/pdf/2403.19806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19806]] Feature-Based Echo-State Networks: A Step Towards Interpretability and  Minimalism in Reservoir Computer(https://arxiv.org/abs/2403.19806)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>This paper proposes a novel and interpretable recurrent neural-network structure using the echo-state network (ESN) paradigm for time-series prediction. While the traditional ESNs perform well for dynamical systems prediction, it needs a large dynamic reservoir with increased computational complexity. It also lacks interpretability to discern contributions from different input combinations to the output. Here, a systematic reservoir architecture is developed using smaller parallel reservoirs driven by different input combinations, known as features, and then they are nonlinearly combined to produce the output. The resultant feature-based ESN (Feat-ESN) outperforms the traditional single-reservoir ESN with less reservoir nodes. The predictive capability of the proposed architecture is demonstrated on three systems: two synthetic datasets from chaotic dynamical systems and a set of real-time traffic data.</li>
</ul>

<h3>Title: Evaluating Explanatory Capabilities of Machine Learning Models in  Medical Diagnostics: A Human-in-the-Loop Approach</h3>
<ul>
<li><strong>Authors: </strong>José Bobes-Bascarán (1), Eduardo Mosqueira-Rey (1), Ángel Fernández-Leal (1), Elena Hernández-Pereira (1), David Alonso-Ríos (1), Vicente Moret-Bonillo (1), Israel Figueirido-Arnoso (1), Yolanda Vidal-Ínsua (2) ((1) University of Coruña (CITIC), (2) Complejo Hospitalario (CHUS))</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19820">https://arxiv.org/abs/2403.19820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19820">https://arxiv.org/pdf/2403.19820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19820]] Evaluating Explanatory Capabilities of Machine Learning Models in  Medical Diagnostics: A Human-in-the-Loop Approach(https://arxiv.org/abs/2403.19820)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>This paper presents a comprehensive study on the evaluation of explanatory capabilities of machine learning models, with a focus on Decision Trees, Random Forest and XGBoost models using a pancreatic cancer dataset. We use Human-in-the-Loop related techniques and medical guidelines as a source of domain knowledge to establish the importance of the different features that are relevant to establish a pancreatic cancer treatment. These features are not only used as a dimensionality reduction approach for the machine learning models, but also as way to evaluate the explainability capabilities of the different models using agnostic and non-agnostic explainability techniques. To facilitate interpretation of explanatory results, we propose the use of similarity measures such as the Weighted Jaccard Similarity coefficient. The goal is to not only select the best performing model but also the one that can best explain its conclusions and aligns with human domain knowledge.</li>
</ul>

<h3>Title: Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case  of the Missing AANNs</h3>
<ul>
<li><strong>Authors: </strong>Kanishka Misra, Kyle Mahowald</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19827">https://arxiv.org/abs/2403.19827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19827">https://arxiv.org/pdf/2403.19827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19827]] Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case  of the Missing AANNs(https://arxiv.org/abs/2403.19827)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Language models learn rare syntactic phenomena, but it has been argued that they rely on rote memorization, as opposed to grammatical generalization. Training on a corpus of human-scale in size (100M words), we iteratively trained transformer language models on systematically manipulated corpora and then evaluated their learning of a particular rare grammatical phenomenon: the English Article+Adjective+Numeral+Noun (AANN) construction (``a beautiful five days''). We first compared how well this construction was learned on the default corpus relative to a counterfactual corpus in which the AANN sentences were removed. AANNs were still learned better than systematically perturbed variants of the construction. Using additional counterfactual corpora, we suggest that this learning occurs through generalization from related constructions (e.g., ``a few days''). An additional experiment showed that this learning is enhanced when there is more variability in the input. Taken together, our results provide an existence proof that models learn rare grammatical phenomena by generalization from less rare phenomena. Code available at https://github.com/kanishkamisra/aannalysis</li>
</ul>

<h3>Title: Target Span Detection for Implicit Harmful Content</h3>
<ul>
<li><strong>Authors: </strong>Nazanin Jafari, James Allan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19836">https://arxiv.org/abs/2403.19836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19836">https://arxiv.org/pdf/2403.19836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19836]] Target Span Detection for Implicit Harmful Content(https://arxiv.org/abs/2403.19836)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, large language model</a></li>
<li><strong>Abstract: </strong>Identifying the targets of hate speech is a crucial step in grasping the nature of such speech and, ultimately, in improving the detection of offensive posts on online forums. Much harmful content on online platforms uses implicit language especially when targeting vulnerable and protected groups such as using stereotypical characteristics instead of explicit target names, making it harder to detect and mitigate the language. In this study, we focus on identifying implied targets of hate speech, essential for recognizing subtler hate speech and enhancing the detection of harmful content on digital platforms. We define a new task aimed at identifying the targets even when they are not explicitly stated. To address that task, we collect and annotate target spans in three prominent implicit hate speech datasets: SBIC, DynaHate, and IHC. We call the resulting merged collection Implicit-Target-Span. The collection is achieved using an innovative pooling method with matching scores based on human annotations and Large Language Models (LLMs). Our experiments indicate that Implicit-Target-Span provides a challenging test bed for target span detection methods.</li>
</ul>

<h3>Title: Multi-Frame, Lightweight & Efficient Vision-Language Models for Question  Answering in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Akshay Gopalkrishnan, Ross Greer, Mohan Trivedi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19838">https://arxiv.org/abs/2403.19838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19838">https://arxiv.org/pdf/2403.19838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19838]] Multi-Frame, Lightweight & Efficient Vision-Language Models for Question  Answering in Autonomous Driving(https://arxiv.org/abs/2403.19838)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) and Multi-Modal Language models (MMLMs) have become prominent in autonomous driving research, as these models can provide interpretable textual reasoning and responses for end-to-end autonomous driving safety tasks using traffic scene images and other data modalities. However, current approaches to these systems use expensive large language model (LLM) backbones and image encoders, making such systems unsuitable for real-time autonomous driving systems where tight memory constraints exist and fast inference time is necessary. To address these previous issues, we develop EM-VLM4AD, an efficient, lightweight, multi-frame vision language model which performs Visual Question Answering for autonomous driving. In comparison to previous approaches, EM-VLM4AD requires at least 10 times less memory and floating point operations, while also achieving higher BLEU-4, METEOR, CIDEr, and ROGUE scores than the existing baseline on the DriveLM dataset. EM-VLM4AD also exhibits the ability to extract relevant information from traffic views related to prompts and can answer questions for various autonomous driving subtasks. We release our code to train and evaluate our model at https://github.com/akshaygopalkr/EM-VLM4AD.</li>
</ul>

<h3>Title: Biased Over-the-Air Federated Learning under Wireless Heterogeneity</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Faraz Ul Abrar, Nicolò Michelusi</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19849">https://arxiv.org/abs/2403.19849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19849">https://arxiv.org/pdf/2403.19849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19849]] Biased Over-the-Air Federated Learning under Wireless Heterogeneity(https://arxiv.org/abs/2403.19849)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Recently, Over-the-Air (OTA) computation has emerged as a promising federated learning (FL) paradigm that leverages the waveform superposition properties of the wireless channel to realize fast model updates. Prior work focused on the OTA device ``pre-scaler" design under \emph{homogeneous} wireless conditions, in which devices experience the same average path loss, resulting in zero-bias solutions. Yet, zero-bias designs are limited by the device with the worst average path loss and hence may perform poorly in \emph{heterogeneous} wireless settings. In this scenario, there may be a benefit in designing \emph{biased} solutions, in exchange for a lower variance in the model updates. To optimize this trade-off, we study the design of OTA device pre-scalers by focusing on the OTA-FL convergence. We derive an upper bound on the model ``optimality error", which explicitly captures the effect of bias and variance in terms of the choice of the pre-scalers. Based on this bound, we identify two solutions of interest: minimum noise variance, and minimum noise variance zero-bias solutions. Numerical evaluations show that using OTA device pre-scalers that minimize the variance of FL updates, while allowing a small bias, can provide high gains over existing schemes.</li>
</ul>

<h3>Title: Secure Link State Routing for Mobile Ad Hoc Networks</h3>
<ul>
<li><strong>Authors: </strong>Panagiotis Papadimitratos, Zygmunt J. Haas</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19859">https://arxiv.org/abs/2403.19859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19859">https://arxiv.org/pdf/2403.19859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19859]] Secure Link State Routing for Mobile Ad Hoc Networks(https://arxiv.org/abs/2403.19859)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust</a></li>
<li><strong>Abstract: </strong>The secure operation of the routing protocol is one of the major challenges to be met for the proliferation of the Mobile Ad hoc Networking (MANET) paradigm. Nevertheless, security enhancements have been proposed mostly for reactive MANET protocols. The proposed here Secure Link State Routing Protocol (SLSP) provides secure proactive topology discovery, which can be multiply beneficial to the network operation. SLSP can be employed as a stand-alone protocol, or fit naturally into a hybrid routing framework, when combined with a reactive protocol. SLSP is robust against individual attackers, it is capable of adjusting its scope between local and network-wide topology discovery, and it is capable of operating in networks of frequently changing topology and membership.</li>
</ul>

<h3>Title: DeNetDM: Debiasing by Network Depth Modulation</h3>
<ul>
<li><strong>Authors: </strong>Silpa Vadakkeeveetil Sreelatha, Adarsh Kappiyath, Anjan Dutta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19863">https://arxiv.org/abs/2403.19863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19863">https://arxiv.org/pdf/2403.19863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19863]] DeNetDM: Debiasing by Network Depth Modulation(https://arxiv.org/abs/2403.19863)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>When neural networks are trained on biased datasets, they tend to inadvertently learn spurious correlations, leading to challenges in achieving strong generalization and robustness. Current approaches to address such biases typically involve utilizing bias annotations, reweighting based on pseudo-bias labels, or enhancing diversity within bias-conflicting data points through augmentation techniques. We introduce DeNetDM, a novel debiasing method based on the observation that shallow neural networks prioritize learning core attributes, while deeper ones emphasize biases when tasked with acquiring distinct information. Using a training paradigm derived from Product of Experts, we create both biased and debiased branches with deep and shallow architectures and then distill knowledge to produce the target debiased model. Extensive experiments and analyses demonstrate that our approach outperforms current debiasing techniques, achieving a notable improvement of around 5% in three datasets, encompassing both synthetic and real-world data. Remarkably, DeNetDM accomplishes this without requiring annotations pertaining to bias labels or bias types, while still delivering performance on par with supervised counterparts. Furthermore, our approach effectively harnesses the diversity of bias-conflicting points within the data, surpassing previous methods and obviating the need for explicit augmentation-based methods to enhance the diversity of such bias-conflicting points. The source code will be available upon acceptance.</li>
</ul>

<h3>Title: Is Synthetic Image Useful for Transfer Learning? An Investigation into  Data Generation, Volume, and Utilization</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Li, Xin Dong, Chen Chen, Jingtao Li, Yuxin Wen, Michael Spranger, Lingjuan Lyu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19866">https://arxiv.org/abs/2403.19866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19866">https://arxiv.org/pdf/2403.19866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19866]] Is Synthetic Image Useful for Transfer Learning? An Investigation into  Data Generation, Volume, and Utilization(https://arxiv.org/abs/2403.19866)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative</a></li>
<li><strong>Abstract: </strong>Synthetic image data generation represents a promising avenue for training deep learning models, particularly in the realm of transfer learning, where obtaining real images within a specific domain can be prohibitively expensive due to privacy and intellectual property considerations. This work delves into the generation and utilization of synthetic images derived from text-to-image generative models in facilitating transfer learning paradigms. Despite the high visual fidelity of the generated images, we observe that their naive incorporation into existing real-image datasets does not consistently enhance model performance due to the inherent distribution gap between synthetic and real images. To address this issue, we introduce a novel two-stage framework called bridged transfer, which initially employs synthetic images for fine-tuning a pre-trained model to improve its transferability and subsequently uses real data for rapid adaptation. Alongside, We propose dataset style inversion strategy to improve the stylistic alignment between synthetic and real images. Our proposed methods are evaluated across 10 different datasets and 5 distinct models, demonstrating consistent improvements, with up to 30% accuracy increase on classification tasks. Intriguingly, we note that the enhancements were not yet saturated, indicating that the benefits may further increase with an expanded volume of synthetic data.</li>
</ul>

<h3>Title: Towards Stable Machine Learning Model Retraining via Slowly Varying  Sequences</h3>
<ul>
<li><strong>Authors: </strong>Vassilis Digalakis Jr, Yu Ma, Phevos Paschalidis, Dimitris Bertsimas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19871">https://arxiv.org/abs/2403.19871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19871">https://arxiv.org/pdf/2403.19871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19871]] Towards Stable Machine Learning Model Retraining via Slowly Varying  Sequences(https://arxiv.org/abs/2403.19871)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Retraining machine learning models remains an important task for real-world machine learning model deployment. Existing methods focus largely on greedy approaches to find the best-performing model without considering the stability of trained model structures across different retraining evolutions. In this study, we develop a mixed integer optimization algorithm that holistically considers the problem of retraining machine learning models across different data batch updates. Our method focuses on retaining consistent analytical insights - which is important to model interpretability, ease of implementation, and fostering trust with users - by using custom-defined distance metrics that can be directly incorporated into the optimization problem. Importantly, our method shows stronger stability than greedily trained models with a small, controllable sacrifice in model performance in a real-world production case study. Finally, important analytical insights, as demonstrated using SHAP feature importance, are shown to be consistent across retraining iterations.</li>
</ul>

<h3>Title: Jamba: A Hybrid Transformer-Mamba Language Model</h3>
<ul>
<li><strong>Authors: </strong>Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, Yoav Shoham</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19887">https://arxiv.org/abs/2403.19887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19887">https://arxiv.org/pdf/2403.19887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19887]] Jamba: A Hybrid Transformer-Mamba Language Model(https://arxiv.org/abs/2403.19887)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. We also describe several interesting properties of these architectures which the training and evaluation of Jamba have revealed, and plan to release checkpoints from various ablation runs, to encourage further exploration of this novel architecture. We make the weights of our implementation of Jamba publicly available under a permissive license.</li>
</ul>

<h3>Title: MambaMixer: Efficient Selective State Space Models with Dual Token and  Channel Selection</h3>
<ul>
<li><strong>Authors: </strong>Ali Behrouz, Michele Santacatterina, Ramin Zabih</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19888">https://arxiv.org/abs/2403.19888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19888">https://arxiv.org/pdf/2403.19888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19888]] MambaMixer: Efficient Selective State Space Models with Dual Token and  Channel Selection(https://arxiv.org/abs/2403.19888)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Recent advances in deep learning have mainly relied on Transformers due to their data dependency and ability to learn at scale. The attention module in these architectures, however, exhibits quadratic time and space in input size, limiting their scalability for long-sequence modeling. Despite recent attempts to design efficient and effective architecture backbone for multi-dimensional data, such as images and multivariate time series, existing models are either data independent, or fail to allow inter- and intra-dimension communication. Recently, State Space Models (SSMs), and more specifically Selective State Space Models, with efficient hardware-aware implementation, have shown promising potential for long sequence modeling. Motivated by the success of SSMs, we present MambaMixer, a new architecture with data-dependent weights that uses a dual selection mechanism across tokens and channels, called Selective Token and Channel Mixer. MambaMixer connects selective mixers using a weighted averaging mechanism, allowing layers to have direct access to early features. As a proof of concept, we design Vision MambaMixer (ViM2) and Time Series MambaMixer (TSM2) architectures based on the MambaMixer block and explore their performance in various vision and time series forecasting tasks. Our results underline the importance of selective mixing across both tokens and channels. In ImageNet classification, object detection, and semantic segmentation tasks, ViM2 achieves competitive performance with well-established vision models and outperforms SSM-based vision models. In time series forecasting, TSM2 achieves outstanding performance compared to state-of-the-art methods while demonstrating significantly improved computational cost. These results show that while Transformers, cross-channel attention, and MLPs are sufficient for good performance in time series forecasting, neither is necessary.</li>
</ul>

<h3>Title: Towards a Robust Retrieval-Based Summarization System</h3>
<ul>
<li><strong>Authors: </strong>Shengjie Liu, Jing Wu, Jingyuan Bao, Wenyi Wang, Naira Hovakimyan, Christopher G Healey</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19889">https://arxiv.org/abs/2403.19889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19889">https://arxiv.org/pdf/2403.19889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19889]] Towards a Robust Retrieval-Based Summarization System(https://arxiv.org/abs/2403.19889)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper describes an investigation of the robustness of large language models (LLMs) for retrieval augmented generation (RAG)-based summarization tasks. While LLMs provide summarization capabilities, their performance in complex, real-world scenarios remains under-explored. Our first contribution is LogicSumm, an innovative evaluation framework incorporating realistic scenarios to assess LLM robustness during RAG-based summarization. Based on limitations identified by LogiSumm, we then developed SummRAG, a comprehensive system to create training dialogues and fine-tune a model to enhance robustness within LogicSumm's scenarios. SummRAG is an example of our goal of defining structured methods to test the capabilities of an LLM, rather than addressing issues in a one-off fashion. Experimental results confirm the power of SummRAG, showcasing improved logical coherence and summarization quality. Data, corresponding model weights, and Python code are available online.</li>
</ul>

<h3>Title: Disentangling Racial Phenotypes: Fine-Grained Control of Race-related  Facial Phenotype Characteristics</h3>
<ul>
<li><strong>Authors: </strong>Seyma Yucer, Amir Atapour Abarghouei, Noura Al Moubayed, Toby P. Breckon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19897">https://arxiv.org/abs/2403.19897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19897">https://arxiv.org/pdf/2403.19897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19897]] Disentangling Racial Phenotypes: Fine-Grained Control of Race-related  Facial Phenotype Characteristics(https://arxiv.org/abs/2403.19897)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Achieving an effective fine-grained appearance variation over 2D facial images, whilst preserving facial identity, is a challenging task due to the high complexity and entanglement of common 2D facial feature encoding spaces. Despite these challenges, such fine-grained control, by way of disentanglement is a crucial enabler for data-driven racial bias mitigation strategies across multiple automated facial analysis tasks, as it allows to analyse, characterise and synthesise human facial diversity. In this paper, we propose a novel GAN framework to enable fine-grained control over individual race-related phenotype attributes of the facial images. Our framework factors the latent (feature) space into elements that correspond to race-related facial phenotype representations, thereby separating phenotype aspects (e.g. skin, hair colour, nose, eye, mouth shapes), which are notoriously difficult to annotate robustly in real-world facial data. Concurrently, we also introduce a high quality augmented, diverse 2D face image dataset drawn from CelebA-HQ for GAN training. Unlike prior work, our framework only relies upon 2D imagery and related parameters to achieve state-of-the-art individual control over race-related phenotype attributes with improved photo-realistic output.</li>
</ul>

<h3>Title: Structure Matters: Tackling the Semantic Discrepancy in Diffusion Models  for Image Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Haipeng Liu, Yang Wang, Biao Qian, Meng Wang, Yong Rui</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19898">https://arxiv.org/abs/2403.19898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19898">https://arxiv.org/pdf/2403.19898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19898]] Structure Matters: Tackling the Semantic Discrepancy in Diffusion Models  for Image Inpainting(https://arxiv.org/abs/2403.19898)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Denoising diffusion probabilistic models for image inpainting aim to add the noise to the texture of image during the forward process and recover masked regions with unmasked ones of the texture via the reverse denoising process.Despite the meaningful semantics generation,the existing arts suffer from the semantic discrepancy between masked and unmasked regions, since the semantically dense unmasked texture fails to be completely degraded while the masked regions turn to the pure noise in diffusion process,leading to the large discrepancy between them.In this paper,we aim to answer how unmasked semantics guide texture denoising process;together with how to tackle the semantic discrepancy,to facilitate the consistent and meaningful semantics generation.To this end,we propose a novel structure-guided diffusion model named StrDiffusion,to reformulate the conventional texture denoising process under structure guidance to derive a simplified denoising objective for image inpainting,while revealing:1) the semantically sparse structure is beneficial to tackle semantic discrepancy in early stage, while dense texture generates reasonable semantics in late stage;2) the semantics from unmasked regions essentially offer the time-dependent structure guidance for the texture denoising process,benefiting from the time-dependent sparsity of the structure semantics.For the denoising process,a structure-guided neural network is trained to estimate the simplified denoising objective by exploiting the consistency of the denoised structure between masked and unmasked regions.Besides,we devise an adaptive resampling strategy as a formal criterion as whether structure is competent to guide the texture denoising process,while regulate their semantic correlations.Extensive experiments validate the merits of StrDiffusion over the state-of-the-arts.Our code is available at https://github.com/htyjers/StrDiffusion.</li>
</ul>

<h3>Title: Fully Geometric Panoramic Localization</h3>
<ul>
<li><strong>Authors: </strong>Junho Kim, Jiwon Jeong, Young Min Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19904">https://arxiv.org/abs/2403.19904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19904">https://arxiv.org/pdf/2403.19904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19904]] Fully Geometric Panoramic Localization(https://arxiv.org/abs/2403.19904)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>We introduce a lightweight and accurate localization method that only utilizes the geometry of 2D-3D lines. Given a pre-captured 3D map, our approach localizes a panorama image, taking advantage of the holistic 360 view. The system mitigates potential privacy breaches or domain discrepancies by avoiding trained or hand-crafted visual descriptors. However, as lines alone can be ambiguous, we express distinctive yet compact spatial contexts from relationships between lines, namely the dominant directions of parallel lines and the intersection between non-parallel lines. The resulting representations are efficient in processing time and memory compared to conventional visual descriptor-based methods. Given the groups of dominant line directions and their intersections, we accelerate the search process to test thousands of pose candidates in less than a millisecond without sacrificing accuracy. We empirically show that the proposed 2D-3D matching can localize panoramas for challenging scenes with similar structures, dramatic domain shifts or illumination changes. Our fully geometric approach does not involve extensive parameter tuning or neural network training, making it a practical algorithm that can be readily deployed in the real world. Project page including the code is available through this link: https://82magnolia.github.io/fgpl/.</li>
</ul>

<h3>Title: Automated Identification and Segmentation of Hi Sources in CRAFTS Using  Deep Learning Method</h3>
<ul>
<li><strong>Authors: </strong>Zihao Song, Huaxi Chen, Donghui Quan, Di Li, Yinghui Zheng, Shulei Ni, Yunchuan Chen, Yun Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, astro-ph.GA, astro-ph.IM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19912">https://arxiv.org/abs/2403.19912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19912">https://arxiv.org/pdf/2403.19912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19912]] Automated Identification and Segmentation of Hi Sources in CRAFTS Using  Deep Learning Method(https://arxiv.org/abs/2403.19912)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We introduce a machine learning-based method for extracting HI sources from 3D spectral data, and construct a dedicated dataset of HI sources from CRAFTS. Our custom dataset provides comprehensive resources for HI source detection. Utilizing the 3D-Unet segmentation architecture, our method reliably identifies and segments HI sources, achieving notable performance metrics with recall rates reaching 91.6% and accuracy levels at 95.7%. These outcomes substantiate the value of our custom dataset and the efficacy of our proposed network in identifying HI source. Our code is publicly available at https://github.com/fishszh/HISF.</li>
</ul>

<h3>Title: MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Peng Ding, Jiading Fang, Peng Li, Kangrui Wang, Xiaochen Zhou, Mo Yu, Jing Li, Matthew R. Walter, Hongyuan Mei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19913">https://arxiv.org/abs/2403.19913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19913">https://arxiv.org/pdf/2403.19913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19913]] MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of  Large Language Models(https://arxiv.org/abs/2403.19913)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models such as ChatGPT and GPT-4 have recently achieved astonishing performance on a variety of natural language processing tasks. In this paper, we propose MANGO, a benchmark to evaluate their capabilities to perform text-based mapping and navigation. Our benchmark includes 53 mazes taken from a suite of textgames: each maze is paired with a walkthrough that visits every location but does not cover all possible paths. The task is question-answering: for each maze, a large language model reads the walkthrough and answers hundreds of mapping and navigation questions such as "How should you go to Attic from West of House?" and "Where are we if we go north and east from Cellar?". Although these questions are easy to humans, it turns out that even GPT-4, the best-to-date language model, performs poorly at answering them. Further, our experiments suggest that a strong mapping and navigation ability would benefit large language models in performing relevant downstream tasks, such as playing textgames. Our MANGO benchmark will facilitate future research on methods that improve the mapping and navigation capabilities of language models. We host our leaderboard, data, code, and evaluation program at https://mango.ttic.edu and https://github.com/oaklight/mango/.</li>
</ul>

<h3>Title: Diff-Reg v1: Diffusion Matching Model for Registration Problem</h3>
<ul>
<li><strong>Authors: </strong>Qianliang Wu, Haobo Jiang, Lei Luo, Jun Li, Yaqing Ding, Jin Xie, Jian Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19919">https://arxiv.org/abs/2403.19919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19919">https://arxiv.org/pdf/2403.19919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19919]] Diff-Reg v1: Diffusion Matching Model for Registration Problem(https://arxiv.org/abs/2403.19919)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, diffusion</a></li>
<li><strong>Abstract: </strong>Establishing reliable correspondences is essential for registration tasks such as 3D and 2D3D registration. Existing methods commonly leverage geometric or semantic point features to generate potential correspondences. However, these features may face challenges such as large deformation, scale inconsistency, and ambiguous matching problems (e.g., symmetry). Additionally, many previous methods, which rely on single-pass prediction, may struggle with local minima in complex scenarios. To mitigate these challenges, we introduce a diffusion matching model for robust correspondence construction. Our approach treats correspondence estimation as a denoising diffusion process within the doubly stochastic matrix space, which gradually denoises (refines) a doubly stochastic matching matrix to the ground-truth one for high-quality correspondence estimation. It involves a forward diffusion process that gradually introduces Gaussian noise into the ground truth matching matrix and a reverse denoising process that iteratively refines the noisy matching matrix. In particular, the feature extraction from the backbone occurs only once during the inference phase. Our lightweight denoising module utilizes the same feature at each reverse sampling step. Evaluation of our method on both 3D and 2D3D registration tasks confirms its effectiveness.</li>
</ul>

<h3>Title: MI-NeRF: Learning a Single Face NeRF from Multiple Identities</h3>
<ul>
<li><strong>Authors: </strong>Aggelina Chatziagapi, Grigorios G. Chrysos, Dimitris Samaras</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19920">https://arxiv.org/abs/2403.19920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19920">https://arxiv.org/pdf/2403.19920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19920]] MI-NeRF: Learning a Single Face NeRF from Multiple Identities(https://arxiv.org/abs/2403.19920)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this work, we introduce a method that learns a single dynamic neural radiance field (NeRF) from monocular talking face videos of multiple identities. NeRFs have shown remarkable results in modeling the 4D dynamics and appearance of human faces. However, they require per-identity optimization. Although recent approaches have proposed techniques to reduce the training and rendering time, increasing the number of identities can be expensive. We introduce MI-NeRF (multi-identity NeRF), a single unified network that models complex non-rigid facial motion for multiple identities, using only monocular videos of arbitrary length. The core premise in our method is to learn the non-linear interactions between identity and non-identity specific information with a multiplicative module. By training on multiple videos simultaneously, MI-NeRF not only reduces the total training time compared to standard single-identity NeRFs, but also demonstrates robustness in synthesizing novel expressions for any input identity. We present results for both facial expression transfer and talking face video synthesis. Our method can be further personalized for a target identity given only a short video.</li>
</ul>

<h3>Title: SceneTracker: Long-term Scene Flow Estimation Network</h3>
<ul>
<li><strong>Authors: </strong>Bo Wang, Jian Li, Yang Yu, Li Liu, Zhenping Sun, Dewen Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19924">https://arxiv.org/abs/2403.19924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19924">https://arxiv.org/pdf/2403.19924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19924]] SceneTracker: Long-term Scene Flow Estimation Network(https://arxiv.org/abs/2403.19924)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Considering the complementarity of scene flow estimation in the spatial domain's focusing capability and 3D object tracking in the temporal domain's coherence, this study aims to address a comprehensive new task that can simultaneously capture fine-grained and long-term 3D motion in an online manner: long-term scene flow estimation (LSFE). We introduce SceneTracker, a novel learning-based LSFE network that adopts an iterative approach to approximate the optimal trajectory. Besides, it dynamically indexes and constructs appearance and depth correlation features simultaneously and employs the Transformer to explore and utilize long-range connections within and between trajectories. With detailed experiments, SceneTracker shows superior capabilities in handling 3D spatial occlusion and depth noise interference, highly tailored to the LSFE task's needs. The code for SceneTracker is available at https://github.com/wwsource/SceneTracker.</li>
</ul>

<h3>Title: Decision Mamba: Reinforcement Learning via Sequence Modeling with  Selective State Spaces</h3>
<ul>
<li><strong>Authors: </strong>Toshihiro Ota</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19925">https://arxiv.org/abs/2403.19925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19925">https://arxiv.org/pdf/2403.19925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19925]] Decision Mamba: Reinforcement Learning via Sequence Modeling with  Selective State Spaces(https://arxiv.org/abs/2403.19925)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Decision Transformer, a promising approach that applies Transformer architectures to reinforcement learning, relies on causal self-attention to model sequences of states, actions, and rewards. While this method has shown competitive results, this paper investigates the integration of the Mamba framework, known for its advanced capabilities in efficient and effective sequence modeling, into the Decision Transformer architecture, focusing on the potential performance enhancements in sequential decision-making tasks. Our study systematically evaluates this integration by conducting a series of experiments across various decision-making environments, comparing the modified Decision Transformer, Decision Mamba, with its traditional counterpart. This work contributes to the advancement of sequential decision-making models, suggesting that the architecture and training methodology of neural networks can significantly impact their performance in complex tasks, and highlighting the potential of Mamba as a valuable tool for improving the efficacy of Transformer-based models in reinforcement learning scenarios.</li>
</ul>

<h3>Title: DiJiang: Efficient Large Language Models through Compact Kernelization</h3>
<ul>
<li><strong>Authors: </strong>Hanting Chen, Zhicheng Liu, Xutao Wang, Yuchuan Tian, Yunhe Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19928">https://arxiv.org/abs/2403.19928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19928">https://arxiv.org/pdf/2403.19928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19928]] DiJiang: Efficient Large Language Models through Compact Kernelization(https://arxiv.org/abs/2403.19928)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>In an effort to reduce the computational load of Transformers, research on linear attention has gained significant momentum. However, the improvement strategies for attention mechanisms typically necessitate extensive retraining, which is impractical for large language models with a vast array of parameters. In this paper, we present DiJiang, a novel Frequency Domain Kernelization approach that enables the transformation of a pre-trained vanilla Transformer into a linear complexity model with little training costs. By employing a weighted Quasi-Monte Carlo method for sampling, the proposed approach theoretically offers superior approximation efficiency. To further reduce the training computational complexity, our kernelization is based on Discrete Cosine Transform (DCT) operations. Extensive experiments demonstrate that the proposed method achieves comparable performance to the original Transformer, but with significantly reduced training costs and much faster inference speeds. Our DiJiang-7B achieves comparable performance with LLaMA2-7B on various benchmark while requires only about 1/50 training cost. Code is available at https://github.com/YuchuanTian/DiJiang.</li>
</ul>

<h3>Title: Are LLMs Effective Backbones for Fine-tuning? An Experimental  Investigation of Supervised LLMs on Chinese Short Text Matching</h3>
<ul>
<li><strong>Authors: </strong>Shulin Liu, Chengcheng Xu, Hao Liu, Tinghao Yu, Tao Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19930">https://arxiv.org/abs/2403.19930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19930">https://arxiv.org/pdf/2403.19930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19930]] Are LLMs Effective Backbones for Fine-tuning? An Experimental  Investigation of Supervised LLMs on Chinese Short Text Matching(https://arxiv.org/abs/2403.19930)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The recent success of Large Language Models (LLMs) has garnered significant attention in both academia and industry. Prior research on LLMs has primarily focused on enhancing or leveraging their generalization capabilities in zero- and few-shot settings. However, there has been limited investigation into effectively fine-tuning LLMs for a specific natural language understanding task in supervised settings. In this study, we conduct an experimental analysis by fine-tuning LLMs for the task of Chinese short text matching. We explore various factors that influence performance when fine-tuning LLMs, including task modeling methods, prompt formats, and output formats.</li>
</ul>

<h3>Title: FairCLIP: Harnessing Fairness in Vision-Language Learning</h3>
<ul>
<li><strong>Authors: </strong>Yan Luo, Min Shi, Muhammad Osama Khan, Muhammad Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu Tian, Luo Song, Ava Kouhana, Tobias Elze, Yi Fang, Mengyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19949">https://arxiv.org/abs/2403.19949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19949">https://arxiv.org/pdf/2403.19949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19949]] FairCLIP: Harnessing Fairness in Vision-Language Learning(https://arxiv.org/abs/2403.19949)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, fair</a></li>
<li><strong>Abstract: </strong>Fairness is a critical concern in deep learning, especially in healthcare, where these models influence diagnoses and treatment decisions. Although fairness has been investigated in the vision-only domain, the fairness of medical vision-language (VL) models remains unexplored due to the scarcity of medical VL datasets for studying fairness. To bridge this research gap, we introduce the first fair vision-language medical dataset FairVLMed that provides detailed demographic attributes, ground-truth labels, and clinical notes to facilitate an in-depth examination of fairness within VL foundation models. Using FairVLMed, we conduct a comprehensive fairness analysis of two widely-used VL models (CLIP and BLIP2), pre-trained on both natural and medical domains, across four different protected attributes. Our results highlight significant biases in all VL models, with Asian, Male, Non-Hispanic, and Spanish being the preferred subgroups across the protected attributes of race, gender, ethnicity, and language, respectively. In order to alleviate these biases, we propose FairCLIP, an optimal-transport-based approach that achieves a favorable trade-off between performance and fairness by reducing the Sinkhorn distance between the overall sample distribution and the distributions corresponding to each demographic group. As the first VL dataset of its kind, FairVLMed holds the potential to catalyze advancements in the development of machine learning models that are both ethically aware and clinically effective. Our dataset and code are available at https://ophai.hms.harvard.edu/datasets/fairvlmed10k.</li>
</ul>

<h3>Title: Enhancing the General Agent Capabilities of Low-Parameter LLMs through  Tuning and Multi-Branch Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Qinhao Zhou, Zihan Zhang, Xiang Xiang, Ke Wang, Yuchuan Wu, Yongbin Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19962">https://arxiv.org/abs/2403.19962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19962">https://arxiv.org/pdf/2403.19962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19962]] Enhancing the General Agent Capabilities of Low-Parameter LLMs through  Tuning and Multi-Branch Reasoning(https://arxiv.org/abs/2403.19962)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Open-source pre-trained Large Language Models (LLMs) exhibit strong language understanding and generation capabilities, making them highly successful in a variety of tasks. However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance. Various methods have been proposed to enhance the agent capabilities of LLMs. On the one hand, methods involve constructing agent-specific data and fine-tuning the models. On the other hand, some methods focus on designing prompts that effectively activate the reasoning abilities of the LLMs. We explore both strategies on the 7B and 13B models. We propose a comprehensive method for constructing agent-specific data using GPT-4. Through supervised fine-tuning with constructed data, we find that for these models with a relatively small number of parameters, supervised fine-tuning can significantly reduce hallucination outputs and formatting errors in agent tasks. Furthermore, techniques such as multi-path reasoning and task decomposition can effectively decrease problem complexity and enhance the performance of LLMs as agents. We evaluate our method on five agent tasks of AgentBench and achieve satisfactory results.</li>
</ul>

<h3>Title: FairRAG: Fair Human Generation via Fair Retrieval Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Robik Shrestha, Yang Zou, Qiuyu Chen, Zhiheng Li, Yusheng Xie, Siqi Deng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19964">https://arxiv.org/abs/2403.19964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19964">https://arxiv.org/pdf/2403.19964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19964]] FairRAG: Fair Human Generation via Fair Retrieval Augmentation(https://arxiv.org/abs/2403.19964)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative</a></li>
<li><strong>Abstract: </strong>Existing text-to-image generative models reflect or even amplify societal biases ingrained in their training data. This is especially concerning for human image generation where models are biased against certain demographic groups. Existing attempts to rectify this issue are hindered by the inherent limitations of the pre-trained models and fail to substantially improve demographic diversity. In this work, we introduce Fair Retrieval Augmented Generation (FairRAG), a novel framework that conditions pre-trained generative models on reference images retrieved from an external image database to improve fairness in human generation. FairRAG enables conditioning through a lightweight linear module that projects reference images into the textual space. To enhance fairness, FairRAG applies simple-yet-effective debiasing strategies, providing images from diverse demographic groups during the generative process. Extensive experiments demonstrate that FairRAG outperforms existing methods in terms of demographic diversity, image-text alignment, and image fidelity while incurring minimal computational overhead during inference.</li>
</ul>

<h3>Title: Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output  Channel Pruning on Computer Vision Tasks</h3>
<ul>
<li><strong>Authors: </strong>Guanhua Ding, Zexi Ye, Zhen Zhong, Gang Li, David Shao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19969">https://arxiv.org/abs/2403.19969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19969">https://arxiv.org/pdf/2403.19969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19969]] Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output  Channel Pruning on Computer Vision Tasks(https://arxiv.org/abs/2403.19969)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Deep Neural Network (DNN) pruning has emerged as a key strategy to reduce model size, improve inference latency, and lower power consumption on DNN accelerators. Among various pruning techniques, block and output channel pruning have shown significant potential in accelerating hardware performance. However, their accuracy often requires further improvement. In response to this challenge, we introduce a separate, dynamic and differentiable (SMART) pruner. This pruner stands out by utilizing a separate, learnable probability mask for weight importance ranking, employing a differentiable Top k operator to achieve target sparsity, and leveraging a dynamic temperature parameter trick to escape from non-sparse local minima. In our experiments, the SMART pruner consistently demonstrated its superiority over existing pruning methods across a wide range of tasks and models on block and output channel pruning. Additionally, we extend our testing to Transformer-based models in N:M pruning scenarios, where SMART pruner also yields state-of-the-art results, demonstrating its adaptability and robustness across various neural network architectures, and pruning types.</li>
</ul>

<h3>Title: Semantically-Shifted Incremental Adapter-Tuning is A Continual  ViTransformer</h3>
<ul>
<li><strong>Authors: </strong>Yuwen Tan, Qinhao Zhou, Xiang Xiang, Ke Wang, Yuchuan Wu, Yongbin Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19979">https://arxiv.org/abs/2403.19979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19979">https://arxiv.org/pdf/2403.19979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19979]] Semantically-Shifted Incremental Adapter-Tuning is A Continual  ViTransformer(https://arxiv.org/abs/2403.19979)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Class-incremental learning (CIL) aims to enable models to continuously learn new classes while overcoming catastrophic forgetting. The introduction of pre-trained models has brought new tuning paradigms to CIL. In this paper, we revisit different parameter-efficient tuning (PET) methods within the context of continual learning. We observe that adapter tuning demonstrates superiority over prompt-based methods, even without parameter expansion in each learning session. Motivated by this, we propose incrementally tuning the shared adapter without imposing parameter update constraints, enhancing the learning capacity of the backbone. Additionally, we employ feature sampling from stored prototypes to retrain a unified classifier, further improving its performance. We estimate the semantic shift of old prototypes without access to past samples and update stored prototypes session by session. Our proposed method eliminates model expansion and avoids retaining any image samples. It surpasses previous pre-trained model-based CIL methods and demonstrates remarkable continual learning capabilities. Experimental results on five CIL benchmarks validate the effectiveness of our approach, achieving state-of-the-art (SOTA) performance.</li>
</ul>

<h3>Title: A Parallel Attention Network for Cattle Face Recognition</h3>
<ul>
<li><strong>Authors: </strong>Jiayu Li, Xuechao Zou, Shiying Wang, Ben Chen, Junliang Xing, Pin Tao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19980">https://arxiv.org/abs/2403.19980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19980">https://arxiv.org/pdf/2403.19980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19980]] A Parallel Attention Network for Cattle Face Recognition(https://arxiv.org/abs/2403.19980)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Cattle face recognition holds paramount significance in domains such as animal husbandry and behavioral research. Despite significant progress in confined environments, applying these accomplishments in wild settings remains challenging. Thus, we create the first large-scale cattle face recognition dataset, ICRWE, for wild environments. It encompasses 483 cattle and 9,816 high-resolution image samples. Each sample undergoes annotation for face features, light conditions, and face orientation. Furthermore, we introduce a novel parallel attention network, PANet. Comprising several cascaded Transformer modules, each module incorporates two parallel Position Attention Modules (PAM) and Feature Mapping Modules (FMM). PAM focuses on local and global features at each image position through parallel channel attention, and FMM captures intricate feature patterns through non-linear mappings. Experimental results indicate that PANet achieves a recognition accuracy of 88.03% on the ICRWE dataset, establishing itself as the current state-of-the-art approach. The source code is available in the supplementary materials.</li>
</ul>

<h3>Title: Grounding and Enhancing Grid-based Models for Neural Fields</h3>
<ul>
<li><strong>Authors: </strong>Zelin Zhao, Fenglei Fan, Wenlong Liao, Junchi Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20002">https://arxiv.org/abs/2403.20002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20002">https://arxiv.org/pdf/2403.20002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20002]] Grounding and Enhancing Grid-based Models for Neural Fields(https://arxiv.org/abs/2403.20002)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Many contemporary studies utilize grid-based models for neural field representation, but a systematic analysis of grid-based models is still missing, hindering the improvement of those models. Therefore, this paper introduces a theoretical framework for grid-based models. This framework points out that these models' approximation and generalization behaviors are determined by grid tangent kernels (GTK), which are intrinsic properties of grid-based models. The proposed framework facilitates a consistent and systematic analysis of diverse grid-based models. Furthermore, the introduced framework motivates the development of a novel grid-based model named the Multiplicative Fourier Adaptive Grid (MulFAGrid). The numerical analysis demonstrates that MulFAGrid exhibits a lower generalization bound than its predecessors, indicating its robust generalization performance. Empirical studies reveal that MulFAGrid achieves state-of-the-art performance in various tasks, including 2D image fitting, 3D signed distance field (SDF) reconstruction, and novel view synthesis, demonstrating superior representation ability. The project website is available at https://sites.google.com/view/cvpr24-2034-submission/home.</li>
</ul>

<h3>Title: Large Language Model based Situational Dialogues for Second Language  Learning</h3>
<ul>
<li><strong>Authors: </strong>Shuyao Xu, Long Qin, Tianyang Chen, Zhenzhou Zha, Bingxue Qiu, Weizhi Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20005">https://arxiv.org/abs/2403.20005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20005">https://arxiv.org/pdf/2403.20005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20005]] Large Language Model based Situational Dialogues for Second Language  Learning(https://arxiv.org/abs/2403.20005)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In second language learning, scenario-based conversation practice is important for language learners to achieve fluency in speaking, but students often lack sufficient opportunities to practice their conversational skills with qualified instructors or native speakers. To bridge this gap, we propose situational dialogue models for students to engage in conversational practice. Our situational dialogue models are fine-tuned on large language models (LLMs), with the aim of combining the engaging nature of an open-ended conversation with the focused practice of scenario-based tasks. Leveraging the generalization capabilities of LLMs, we demonstrate that our situational dialogue models perform effectively not only on training topics but also on topics not encountered during training. This offers a promising solution to support a wide range of conversational topics without extensive manual work. Additionally, research in the field of dialogue systems still lacks reliable automatic evaluation metrics, leading to human evaluation as the gold standard (Smith et al., 2022), which is typically expensive. To address the limitations of existing evaluation methods, we present a novel automatic evaluation method that employs fine-tuned LLMs to efficiently and effectively assess the performance of situational dialogue models.</li>
</ul>

<h3>Title: On Large Language Models' Hallucination with Regard to Known Facts</h3>
<ul>
<li><strong>Authors: </strong>Che Jiang, Biqing Qi, Xiangyu Hong, Dayuan Fu, Yang Cheng, Fandong Meng, Mo Yu, Bowen Zhou, Jie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20009">https://arxiv.org/abs/2403.20009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20009">https://arxiv.org/pdf/2403.20009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20009]] On Large Language Models' Hallucination with Regard to Known Facts(https://arxiv.org/abs/2403.20009)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models are successful in answering factoid questions but are also prone to hallucination.We investigate the phenomenon of LLMs possessing correct answer knowledge yet still hallucinating from the perspective of inference dynamics, an area not previously covered in studies on hallucinations.We are able to conduct this analysis via two key ideas.First, we identify the factual questions that query the same triplet knowledge but result in different answers. The difference between the model behaviors on the correct and incorrect outputs hence suggests the patterns when hallucinations happen. Second, to measure the pattern, we utilize mappings from the residual streams to vocabulary space. We reveal the different dynamics of the output token probabilities along the depths of layers between the correct and hallucinated cases. In hallucinated cases, the output token's information rarely demonstrates abrupt increases and consistent superiority in the later stages of the model. Leveraging the dynamic curve as a feature, we build a classifier capable of accurately detecting hallucinatory predictions with an 88\% success rate. Our study shed light on understanding the reasons for LLMs' hallucinations on their known facts, and more importantly, on accurately predicting when they are hallucinating.</li>
</ul>

<h3>Title: NeSLAM: Neural Implicit Mapping and Self-Supervised Feature Tracking  With Depth Completion and Denoising</h3>
<ul>
<li><strong>Authors: </strong>Tianchen Deng, Yanbo Wang, Hongle Xie, Hesheng Wang, Jingchuan Wang, Danwei Wang, Weidong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20034">https://arxiv.org/abs/2403.20034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20034">https://arxiv.org/pdf/2403.20034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20034]] NeSLAM: Neural Implicit Mapping and Self-Supervised Feature Tracking  With Depth Completion and Denoising(https://arxiv.org/abs/2403.20034)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In recent years, there have been significant advancements in 3D reconstruction and dense RGB-D SLAM systems. One notable development is the application of Neural Radiance Fields (NeRF) in these systems, which utilizes implicit neural representation to encode 3D scenes. This extension of NeRF to SLAM has shown promising results. However, the depth images obtained from consumer-grade RGB-D sensors are often sparse and noisy, which poses significant challenges for 3D reconstruction and affects the accuracy of the representation of the scene geometry. Moreover, the original hierarchical feature grid with occupancy value is inaccurate for scene geometry representation. Furthermore, the existing methods select random pixels for camera tracking, which leads to inaccurate localization and is not robust in real-world indoor environments. To this end, we present NeSLAM, an advanced framework that achieves accurate and dense depth estimation, robust camera tracking, and realistic synthesis of novel views. First, a depth completion and denoising network is designed to provide dense geometry prior and guide the neural implicit representation optimization. Second, the occupancy scene representation is replaced with Signed Distance Field (SDF) hierarchical scene representation for high-quality reconstruction and view synthesis. Furthermore, we also propose a NeRF-based self-supervised feature tracking algorithm for robust real-time tracking. Experiments on various indoor datasets demonstrate the effectiveness and accuracy of the system in reconstruction, tracking quality, and novel view synthesis.</li>
</ul>

<h3>Title: Transformer-Lite: High-efficiency Deployment of Large Language Models on  Mobile Phone GPUs</h3>
<ul>
<li><strong>Authors: </strong>Luchang Li, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, Qin Xie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20041">https://arxiv.org/abs/2403.20041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20041">https://arxiv.org/pdf/2403.20041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20041]] Transformer-Lite: High-efficiency Deployment of Large Language Models on  Mobile Phone GPUs(https://arxiv.org/abs/2403.20041)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The Large Language Model (LLM) is widely employed for tasks such as intelligent assistants, text summarization, translation, and multi-modality on mobile phones. However, the current methods for on-device LLM deployment maintain slow inference speed, which causes poor user experience. To facilitate high-efficiency LLM deployment on device GPUs, we propose four optimization techniques: (a) a symbolic expression-based approach to support dynamic shape model inference; (b) operator optimizations and execution priority setting to enhance inference speed and reduce phone lagging; (c) an FP4 quantization method termed M0E4 to reduce dequantization overhead; (d) a sub-tensor-based technique to eliminate the need for copying KV cache after LLM inference. Furthermore, we implement these methods in our mobile inference engine, Transformer-Lite, which is compatible with both Qualcomm and MTK processors. We evaluated Transformer-Lite's performance using LLMs with varied architectures and parameters ranging from 2B to 14B. Specifically, we achieved prefill and decoding speeds of 121 token/s and 14 token/s for ChatGLM2 6B, and 330 token/s and 30 token/s for smaller Gemma 2B, respectively. Compared with CPU-based FastLLM and GPU-based MLC-LLM, our engine attains over 10x speedup for the prefill speed and 2~3x speedup for the decoding speed.</li>
</ul>

<h3>Title: Cross-Lingual Transfer Robustness to Lower-Resource Languages on  Adversarial Datasets</h3>
<ul>
<li><strong>Authors: </strong>Shadi Manafi, Nikhil Krishnaswamy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20056">https://arxiv.org/abs/2403.20056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20056">https://arxiv.org/pdf/2403.20056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20056]] Cross-Lingual Transfer Robustness to Lower-Resource Languages on  Adversarial Datasets(https://arxiv.org/abs/2403.20056)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multilingual Language Models (MLLMs) exhibit robust cross-lingual transfer capabilities, or the ability to leverage information acquired in a source language and apply it to a target language. These capabilities find practical applications in well-established Natural Language Processing (NLP) tasks such as Named Entity Recognition (NER). This study aims to investigate the effectiveness of a source language when applied to a target language, particularly in the context of perturbing the input test set. We evaluate on 13 pairs of languages, each including one high-resource language (HRL) and one low-resource language (LRL) with a geographic, genetic, or borrowing relationship. We evaluate two well-known MLLMs--MBERT and XLM-R--on these pairs, in native LRL and cross-lingual transfer settings, in two tasks, under a set of different perturbations. Our findings indicate that NER cross-lingual transfer depends largely on the overlap of entity chunks. If a source and target language have more entities in common, the transfer ability is stronger. Models using cross-lingual transfer also appear to be somewhat more robust to certain perturbations of the input, perhaps indicating an ability to leverage stronger representations derived from the HRL. Our research provides valuable insights into cross-lingual transfer and its implications for NLP applications, and underscores the need to consider linguistic nuances and potential limitations when employing MLLMs across distinct languages.</li>
</ul>

<h3>Title: Optimal s-boxes against alternative operations</h3>
<ul>
<li><strong>Authors: </strong>Marco Calderini, Roberto Civino, Riccardo Invernizzi</a></li>
<li><strong>Subjects: </strong>cs.CR, math.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20059">https://arxiv.org/abs/2403.20059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20059">https://arxiv.org/pdf/2403.20059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20059]] Optimal s-boxes against alternative operations(https://arxiv.org/abs/2403.20059)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, diffusion</a></li>
<li><strong>Abstract: </strong>Civino et al. have characterised diffusion layers that expose an SPN to vulnerability from differential cryptanalysis when employing alternative operations coming from groups isomorphic to the translation group on the message space. In this study, we present a classification of diffusion layers that exhibit linearity in parallel alternative operations for ciphers with 4-bit s-boxes, enabling the possibility of an alternative differential attack simultaneously targeting all the s-boxes within the block. Furthermore, we investigate the differential behaviour with respect to alternative operations for all classes of optimal 4-bit s-boxes, as defined by Leander and Poschmann (2007). Our examination reveals that certain classes contain weak permutations w.r.t. alternative differential attacks, and we leverage these vulnerabilities to execute a series of experiments.</li>
</ul>

<h3>Title: Adaptive Decentralized Federated Learning in Energy and Latency  Constrained Wireless Networks</h3>
<ul>
<li><strong>Authors: </strong>Zhigang Yan, Dong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20075">https://arxiv.org/abs/2403.20075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20075">https://arxiv.org/pdf/2403.20075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20075]] Adaptive Decentralized Federated Learning in Energy and Latency  Constrained Wireless Networks(https://arxiv.org/abs/2403.20075)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>In Federated Learning (FL), with parameter aggregated by a central node, the communication overhead is a substantial concern. To circumvent this limitation and alleviate the single point of failure within the FL framework, recent studies have introduced Decentralized Federated Learning (DFL) as a viable alternative. Considering the device heterogeneity, and energy cost associated with parameter aggregation, in this paper, the problem on how to efficiently leverage the limited resources available to enhance the model performance is investigated. Specifically, we formulate a problem that minimizes the loss function of DFL while considering energy and latency constraints. The proposed solution involves optimizing the number of local training rounds across diverse devices with varying resource budgets. To make this problem tractable, we first analyze the convergence of DFL with edge devices with different rounds of local training. The derived convergence bound reveals the impact of the rounds of local training on the model performance. Then, based on the derived bound, the closed-form solutions of rounds of local training in different devices are obtained. Meanwhile, since the solutions require the energy cost of aggregation as low as possible, we modify different graph-based aggregation schemes to solve this energy consumption minimization problem, which can be applied to different communication scenarios. Finally, a DFL framework which jointly considers the optimized rounds of local training and the energy-saving aggregation scheme is proposed. Simulation results show that, the proposed algorithm achieves a better performance than the conventional schemes with fixed rounds of local training, and consumes less energy than other traditional aggregation schemes.</li>
</ul>

<h3>Title: Negative Label Guided OOD Detection with Pretrained Vision-Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Xue Jiang, Feng Liu, Zhen Fang, Hong Chen, Tongliang Liu, Feng Zheng, Bo Han</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20078">https://arxiv.org/abs/2403.20078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20078">https://arxiv.org/pdf/2403.20078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20078]] Negative Label Guided OOD Detection with Pretrained Vision-Language  Models(https://arxiv.org/abs/2403.20078)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Out-of-distribution (OOD) detection aims at identifying samples from unknown classes, playing a crucial role in trustworthy models against errors on unexpected inputs. Extensive research has been dedicated to exploring OOD detection in the vision modality. Vision-language models (VLMs) can leverage both textual and visual information for various multi-modal applications, whereas few OOD detection methods take into account information from the text modality. In this paper, we propose a novel post hoc OOD detection method, called NegLabel, which takes a vast number of negative labels from extensive corpus databases. We design a novel scheme for the OOD score collaborated with negative labels. Theoretical analysis helps to understand the mechanism of negative labels. Extensive experiments demonstrate that our method NegLabel achieves state-of-the-art performance on various OOD detection benchmarks and generalizes well on multiple VLM architectures. Furthermore, our method NegLabel exhibits remarkable robustness against diverse domain shifts. The codes are available at https://github.com/tmlr-group/NegLabel.</li>
</ul>

<h3>Title: SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior</h3>
<ul>
<li><strong>Authors: </strong>Zhongrui Yu, Haoran Wang, Jinze Yang, Hanzhang Wang, Zeke Xie, Yunfeng Cai, Jiale Cao, Zhong Ji, Mingming Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20079">https://arxiv.org/abs/2403.20079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20079">https://arxiv.org/pdf/2403.20079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20079]] SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior(https://arxiv.org/abs/2403.20079)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Novel View Synthesis (NVS) for street scenes play a critical role in the autonomous driving simulation. The current mainstream technique to achieve it is neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although thrilling progress has been made, when handling street scenes, current methods struggle to maintain rendering quality at the viewpoint that deviates significantly from the training viewpoints. This issue stems from the sparse training views captured by a fixed camera on a moving vehicle. To tackle this problem, we propose a novel approach that enhances the capacity of 3DGS by leveraging prior from a Diffusion Model along with complementary multi-modal data. Specifically, we first fine-tune a Diffusion Model by adding images from adjacent frames as condition, meanwhile exploiting depth data from LiDAR point clouds to supply additional spatial information. Then we apply the Diffusion Model to regularize the 3DGS at unseen views during training. Experimental results validate the effectiveness of our method compared with current state-of-the-art models, and demonstrate its advance in rendering images from broader views.</li>
</ul>

<h3>Title: Mixed-precision Supernet Training from Vision Foundation Models using  Low Rank Adapter</h3>
<ul>
<li><strong>Authors: </strong>Yuiko Sakuma, Masakazu Yoshimura, Junji Otsuka, Atsushi Irie, Takeshi Ohashi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20080">https://arxiv.org/abs/2403.20080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20080">https://arxiv.org/pdf/2403.20080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20080]] Mixed-precision Supernet Training from Vision Foundation Models using  Low Rank Adapter(https://arxiv.org/abs/2403.20080)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Compression of large and performant vision foundation models (VFMs) into arbitrary bit-wise operations (BitOPs) allows their deployment on various hardware. We propose to fine-tune a VFM to a mixed-precision quantized supernet. The supernet-based neural architecture search (NAS) can be adopted for this purpose, which trains a supernet, and then subnets within arbitrary hardware budgets can be extracted. However, existing methods face difficulties in optimizing the mixed-precision search space and incurring large memory costs during training. To tackle these challenges, first, we study the effective search space design for fine-tuning a VFM by comparing different operators (such as resolution, feature size, width, depth, and bit-widths) in terms of performance and BitOPs reduction. Second, we propose memory-efficient supernet training using a low-rank adapter (LoRA) and a progressive training strategy. The proposed method is evaluated for the recently proposed VFM, Segment Anything Model, fine-tuned on segmentation tasks. The searched model yields about a 95% reduction in BitOPs without incurring performance degradation.</li>
</ul>

<h3>Title: Selective Attention-based Modulation for Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Giovanni Bellitto, Federica Proietto Salanitri, Matteo Pennisi, Matteo Boschini, Angelo Porrello, Simone Calderara, Simone Palazzo, Concetto Spampinato</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20086">https://arxiv.org/abs/2403.20086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20086">https://arxiv.org/pdf/2403.20086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20086]] Selective Attention-based Modulation for Continual Learning(https://arxiv.org/abs/2403.20086)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>We present SAM, a biologically-plausible selective attention-driven modulation approach to enhance classification models in a continual learning setting. Inspired by neurophysiological evidence that the primary visual cortex does not contribute to object manifold untangling for categorization and that primordial attention biases are still embedded in the modern brain, we propose to employ auxiliary saliency prediction features as a modulation signal to drive and stabilize the learning of a sequence of non-i.i.d. classification tasks. Experimental results confirm that SAM effectively enhances the performance (in some cases up to about twenty percent points) of state-of-the-art continual learning methods, both in class-incremental and task-incremental settings. Moreover, we show that attention-based modulation successfully encourages the learning of features that are more robust to the presence of spurious features and to adversarial attacks than baseline methods. Code is available at: https://github.com/perceivelab/SAM.</li>
</ul>

<h3>Title: Modeling Weather Uncertainty for Multi-weather Co-Presence Estimation</h3>
<ul>
<li><strong>Authors: </strong>Qi Bi, Shaodi You, Theo Gevers</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20092">https://arxiv.org/abs/2403.20092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20092">https://arxiv.org/pdf/2403.20092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20092]] Modeling Weather Uncertainty for Multi-weather Co-Presence Estimation(https://arxiv.org/abs/2403.20092)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Images from outdoor scenes may be taken under various weather conditions. It is well studied that weather impacts the performance of computer vision algorithms and needs to be handled properly. However, existing algorithms model weather condition as a discrete status and estimate it using multi-label classification. The fact is that, physically, specifically in meteorology, weather are modeled as a continuous and transitional status. Instead of directly implementing hard classification as existing multi-weather classification methods do, we consider the physical formulation of multi-weather conditions and model the impact of physical-related parameter on learning from the image appearance. In this paper, we start with solid revisit of the physics definition of weather and how it can be described as a continuous machine learning and computer vision task. Namely, we propose to model the weather uncertainty, where the level of probability and co-existence of multiple weather conditions are both considered. A Gaussian mixture model is used to encapsulate the weather uncertainty and a uncertainty-aware multi-weather learning scheme is proposed based on prior-posterior learning. A novel multi-weather co-presence estimation transformer (MeFormer) is proposed. In addition, a new multi-weather co-presence estimation (MePe) dataset, along with 14 fine-grained weather categories and 16,078 samples, is proposed to benchmark both conventional multi-label weather classification task and multi-weather co-presence estimation task. Large scale experiments show that the proposed method achieves state-of-the-art performance and substantial generalization capabilities on both the conventional multi-label weather classification task and the proposed multi-weather co-presence estimation task. Besides, modeling weather uncertainty also benefits adverse-weather semantic segmentation.</li>
</ul>

<h3>Title: RealKIE: Five Novel Datasets for Enterprise Key Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Townsend, Madison May, Christopher Wells</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20101">https://arxiv.org/abs/2403.20101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20101">https://arxiv.org/pdf/2403.20101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20101]] RealKIE: Five Novel Datasets for Enterprise Key Information Extraction(https://arxiv.org/abs/2403.20101)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>We introduce RealKIE, a benchmark of five challenging datasets aimed at advancing key information extraction methods, with an emphasis on enterprise applications. The datasets include a diverse range of documents including SEC S1 Filings, US Non-disclosure Agreements, UK Charity Reports, FCC Invoices, and Resource Contracts. Each presents unique challenges: poor text serialization, sparse annotations in long documents, and complex tabular layouts. These datasets provide a realistic testing ground for key information extraction tasks like investment analysis and legal data processing. In addition to presenting these datasets, we offer an in-depth description of the annotation process, document processing techniques, and baseline modeling approaches. This contribution facilitates the development of NLP models capable of handling practical challenges and supports further research into information extraction technologies applicable to industry-specific problems. The annotated data and OCR outputs are available to download at https://indicodatasolutions.github.io/RealKIE/ code to reproduce the baselines will be available shortly.</li>
</ul>

<h3>Title: FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion  Models</h3>
<ul>
<li><strong>Authors: </strong>Barbara Toniella Corradini, Mustafa Shukor, Paul Couairon, Guillaume Couairon, Franco Scarselli, Matthieu Cord</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20105">https://arxiv.org/abs/2403.20105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20105">https://arxiv.org/pdf/2403.20105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20105]] FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion  Models(https://arxiv.org/abs/2403.20105)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Foundation models have exhibited unprecedented capabilities in tackling many domains and tasks. Models such as CLIP are currently widely used to bridge cross-modal representations, and text-to-image diffusion models are arguably the leading models in terms of realistic image generation. Image generative models are trained on massive datasets that provide them with powerful internal spatial representations. In this work, we explore the potential benefits of such representations, beyond image generation, in particular, for dense visual prediction tasks. We focus on the task of image segmentation, which is traditionally solved by training models on closed-vocabulary datasets, with pixel-level annotations. To avoid the annotation cost or training large diffusion models, we constraint our setup to be zero-shot and training-free. In a nutshell, our pipeline leverages different and relatively small-sized, open-source foundation models for zero-shot open-vocabulary segmentation. The pipeline is as follows: the image is passed to both a captioner model (i.e. BLIP) and a diffusion model (i.e., Stable Diffusion Model) to generate a text description and visual representation, respectively. The features are clustered and binarized to obtain class agnostic masks for each object. These masks are then mapped to a textual class, using the CLIP model to support open-vocabulary. Finally, we add a refinement step that allows to obtain a more precise segmentation mask. Our approach (dubbed FreeSeg-Diff), which does not rely on any training, outperforms many training-based approaches on both Pascal VOC and COCO datasets. In addition, we show very competitive results compared to the recent weakly-supervised segmentation approaches. We provide comprehensive experiments showing the superiority of diffusion model features compared to other pretrained models. Project page: https://bcorrad.github.io/freesegdiff/</li>
</ul>

<h3>Title: Aggregating Local and Global Features via Selective State Spaces Model  for Efficient Image Deblurring</h3>
<ul>
<li><strong>Authors: </strong>Hu Gao, Depeng Dang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20106">https://arxiv.org/abs/2403.20106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20106">https://arxiv.org/pdf/2403.20106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20106]] Aggregating Local and Global Features via Selective State Spaces Model  for Efficient Image Deblurring(https://arxiv.org/abs/2403.20106)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Image deblurring is a process of restoring a high quality image from the corresponding blurred image. Significant progress in this field has been made possible by the emergence of various effective deep learning models, including CNNs and Transformers. However, these methods often face the dilemma between eliminating long-range blur degradation perturbations and maintaining computational efficiency, which hinders their practical application. To address this issue, we propose an efficient image deblurring network that leverages selective structured state spaces model to aggregate enriched and accurate features. Specifically, we design an aggregate local and global block (ALGBlock) to capture and fuse both local invariant properties and non-local information. The ALGBlock consists of two blocks: (1) The local block models local connectivity using simplified channel attention. (2) The global block captures long-range dependency features with linear complexity through selective structured state spaces. Nevertheless, we note that the image details are local features of images, we accentuate the local part for restoration by recalibrating the weight when aggregating the two branches for recovery. Experimental results demonstrate that the proposed method outperforms state-of-the-art approaches on widely used benchmarks, highlighting its superior performance.</li>
</ul>

<h3>Title: Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic  Rewards for Goal-directed Molecular Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinyeong Park, Jaegyoon Ahn, Jonghwan Choi, Jibum Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20109">https://arxiv.org/abs/2403.20109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20109">https://arxiv.org/pdf/2403.20109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20109]] Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic  Rewards for Goal-directed Molecular Generation(https://arxiv.org/abs/2403.20109)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Optimizing techniques for discovering molecular structures with desired properties is crucial in artificial intelligence(AI)-based drug discovery. Combining deep generative models with reinforcement learning has emerged as an effective strategy for generating molecules with specific properties. Despite its potential, this approach is ineffective in exploring the vast chemical space and optimizing particular chemical properties. To overcome these limitations, we present Mol-AIR, a reinforcement learning-based framework using adaptive intrinsic rewards for effective goal-directed molecular generation. Mol-AIR leverages the strengths of both history-based and learning-based intrinsic rewards by exploiting random distillation network and counting-based strategies. In benchmark tests, Mol-AIR demonstrates superior performance over existing approaches in generating molecules with desired properties without any prior knowledge, including penalized LogP, QED, and celecoxib similarity. We believe that Mol-AIR represents a significant advancement in drug discovery, offering a more efficient path to discovering novel therapeutics.</li>
</ul>

<h3>Title: Segmentation, Classification and Interpretation of Breast Cancer Medical  Images using Human-in-the-Loop Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>David Vázquez-Lema (1), Eduardo Mosqueira-Rey (1), Elena Hernández-Pereira (1), Carlos Fernández-Lozano (1), Fernando Seara-Romera (1), Jorge Pombo-Otero (2) ((1) University of Coruña (CITIC), (2) Complejo Hospitalario Universitario de A Coruña (CHUAC))</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20112">https://arxiv.org/abs/2403.20112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20112">https://arxiv.org/pdf/2403.20112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20112]] Segmentation, Classification and Interpretation of Breast Cancer Medical  Images using Human-in-the-Loop Machine Learning(https://arxiv.org/abs/2403.20112)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper explores the application of Human-in-the-Loop (HITL) strategies in training machine learning models in the medical domain. In this case a doctor-in-the-loop approach is proposed to leverage human expertise in dealing with large and complex data. Specifically, the paper deals with the integration of genomic data and Whole Slide Imaging (WSI) analysis of breast cancer. Three different tasks were developed: segmentation of histopathological images, classification of this images regarding the genomic subtype of the cancer and, finally, interpretation of the machine learning results. The involvement of a pathologist helped us to develop a better segmentation model and to enhance the explainatory capabilities of the models, but the classification results were suboptimal, highlighting the limitations of this approach: despite involving human experts, complex domains can still pose challenges, and a HITL approach may not always be effective.</li>
</ul>

<h3>Title: Privacy-Preserving Data Aggregation Techniques for Enhanced Efficiency  and Security in Wireless Sensor Networks: A Comprehensive Analysis and  Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Ayush Rastogi, Harsh Rastogi, Yash Rastogi, Divyansh Dubey</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20120">https://arxiv.org/abs/2403.20120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20120">https://arxiv.org/pdf/2403.20120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20120]] Privacy-Preserving Data Aggregation Techniques for Enhanced Efficiency  and Security in Wireless Sensor Networks: A Comprehensive Analysis and  Evaluation(https://arxiv.org/abs/2403.20120)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack</a></li>
<li><strong>Abstract: </strong>In this paper, we present a multidimensional, highly effective method for aggregating data for wireless sensor networks while maintaining privacy. The suggested system is resistant to data loss and secure against both active and passive privacy compromising attacks, such as the coalition attack from a rogue base station and kidnapped sensor nodes. With regard to cluster size, it achieves consistent communication overhead, which is helpful in large-scale WSNs. Due to its constant size communication overhead, the suggested strategy outperforms the previous privacy-preserving data aggregation scheme not only in terms of privacy preservation but also in terms of communication complexity and energy costs.</li>
</ul>

<h3>Title: ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with  Visual Prompt Tuning</h3>
<ul>
<li><strong>Authors: </strong>Beomyoung Kim, Joonsang Yu, Sung Ju Hwang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20126">https://arxiv.org/abs/2403.20126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20126">https://arxiv.org/pdf/2403.20126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20126]] ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with  Visual Prompt Tuning(https://arxiv.org/abs/2403.20126)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Panoptic segmentation, combining semantic and instance segmentation, stands as a cutting-edge computer vision task. Despite recent progress with deep learning models, the dynamic nature of real-world applications necessitates continual learning, where models adapt to new classes (plasticity) over time without forgetting old ones (catastrophic forgetting). Current continual segmentation methods often rely on distillation strategies like knowledge distillation and pseudo-labeling, which are effective but result in increased training complexity and computational overhead. In this paper, we introduce a novel and efficient method for continual panoptic segmentation based on Visual Prompt Tuning, dubbed ECLIPSE. Our approach involves freezing the base model parameters and fine-tuning only a small set of prompt embeddings, addressing both catastrophic forgetting and plasticity and significantly reducing the trainable parameters. To mitigate inherent challenges such as error propagation and semantic drift in continual segmentation, we propose logit manipulation to effectively leverage common knowledge across the classes. Experiments on ADE20K continual panoptic segmentation benchmark demonstrate the superiority of ECLIPSE, notably its robustness against catastrophic forgetting and its reasonable plasticity, achieving a new state-of-the-art. The code is available at https://github.com/clovaai/ECLIPSE.</li>
</ul>

<h3>Title: User Modeling Challenges in Interactive AI Assistant Systems</h3>
<ul>
<li><strong>Authors: </strong>Megan Su, Yuwei Bao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20134">https://arxiv.org/abs/2403.20134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20134">https://arxiv.org/pdf/2403.20134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20134]] User Modeling Challenges in Interactive AI Assistant Systems(https://arxiv.org/abs/2403.20134)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Interactive Artificial Intelligent(AI) assistant systems are designed to offer timely guidance to help human users to complete a variety tasks. One of the remaining challenges is to understand user's mental states during the task for more personalized guidance. In this work, we analyze users' mental states during task executions and investigate the capabilities and challenges for large language models to interpret user profiles for more personalized user guidance.</li>
</ul>

<h3>Title: Differentiated Security Architecture for Secure and Efficient  Infotainment Data Communication in IoV Networks</h3>
<ul>
<li><strong>Authors: </strong>Jiani Fan, Lwin Khin Shar, Jiale Guo, Wenzhuo Yang, Dusit Niyato, Kwok-Yan Lam</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20136">https://arxiv.org/abs/2403.20136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20136">https://arxiv.org/pdf/2403.20136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20136]] Differentiated Security Architecture for Secure and Efficient  Infotainment Data Communication in IoV Networks(https://arxiv.org/abs/2403.20136)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack</a></li>
<li><strong>Abstract: </strong>This paper aims to provide differentiated security protection for infotainment data communication in Internet-of-Vehicle (IoV) networks. The IoV is a network of vehicles that uses various sensors, software, built-in hardware, and communication technologies to enable information exchange between pedestrians, cars, and urban infrastructure. Negligence on the security of infotainment data communication in IoV networks can unintentionally open an easy access point for social engineering attacks. The attacker can spread false information about traffic conditions, mislead drivers in their directions, and interfere with traffic management. Such attacks can also cause distractions to the driver, which has a potential implication for the safety of driving. The existing literature on IoV communication and network security focuses mainly on generic solutions. In a heterogeneous communication network where different types of communication coexist, we can improve the efficiency of security solutions by considering the different security and efficiency requirements of data communications. Hence, we propose a differentiated security mechanism for protecting infotainment data communication in IoV networks. In particular, we first classify data communication in the IoV network, examine the security focus of each data communication, and then develop a differentiated security architecture to provide security protection on a file-to-file basis. Our architecture leverages Named Data Networking (NDN) so that infotainment files can be efficiently circulated throughout the network where any node can own a copy of the file, thus improving the hit ratio for user file requests. In addition, we propose a time-sensitive Key-Policy Attribute-Based Encryption (KP-ABE) scheme for sharing subscription-based infotainment data...</li>
</ul>

<h3>Title: Fine-tuning Large Language Models for Automated Diagnostic Screening  Summaries</h3>
<ul>
<li><strong>Authors: </strong>Manjeet Yadav, Nilesh Kumar Sahu, Mudita Chaturvedi, Snehil Gupta, Haroon R Lone</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20145">https://arxiv.org/abs/2403.20145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20145">https://arxiv.org/pdf/2403.20145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20145]] Fine-tuning Large Language Models for Automated Diagnostic Screening  Summaries(https://arxiv.org/abs/2403.20145)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Improving mental health support in developing countries is a pressing need. One potential solution is the development of scalable, automated systems to conduct diagnostic screenings, which could help alleviate the burden on mental health professionals. In this work, we evaluate several state-of-the-art Large Language Models (LLMs), with and without fine-tuning, on our custom dataset for generating concise summaries from mental state examinations. We rigorously evaluate four different models for summary generation using established ROUGE metrics and input from human evaluators. The results highlight that our top-performing fine-tuned model outperforms existing models, achieving ROUGE-1 and ROUGE-L values of 0.810 and 0.764, respectively. Furthermore, we assessed the fine-tuned model's generalizability on a publicly available D4 dataset, and the outcomes were promising, indicating its potential applicability beyond our custom dataset.</li>
</ul>

<h3>Title: IndiBias: A Benchmark Dataset to Measure Social Biases in Language  Models for Indian Context</h3>
<ul>
<li><strong>Authors: </strong>Nihar Ranjan Sahoo, Pranamya Prashant Kulkarni, Narjis Asad, Arif Ahmad, Tanu Goyal, Aparna Garimella, Pushpak Bhattacharyya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20147">https://arxiv.org/abs/2403.20147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20147">https://arxiv.org/pdf/2403.20147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20147]] IndiBias: A Benchmark Dataset to Measure Social Biases in Language  Models for Indian Context(https://arxiv.org/abs/2403.20147)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The pervasive influence of social biases in language data has sparked the need for benchmark datasets that capture and evaluate these biases in Large Language Models (LLMs). Existing efforts predominantly focus on English language and the Western context, leaving a void for a reliable dataset that encapsulates India's unique socio-cultural nuances. To bridge this gap, we introduce IndiBias, a comprehensive benchmarking dataset designed specifically for evaluating social biases in the Indian context. We filter and translate the existing CrowS-Pairs dataset to create a benchmark dataset suited to the Indian context in Hindi language. Additionally, we leverage LLMs including ChatGPT and InstructGPT to augment our dataset with diverse societal biases and stereotypes prevalent in India. The included bias dimensions encompass gender, religion, caste, age, region, physical appearance, and occupation. We also build a resource to address intersectional biases along three intersectional dimensions. Our dataset contains 800 filtered sentences from the CrowS-Pairs dataset and tuples for bias measurement across different demographics. It is made available in English and Hindi languages, providing a size comparable to existing benchmark datasets. Furthermore, using IndiBias we compare ten different language models on multiple bias measurement metrics. We observed that the language models exhibit more bias across a majority of the intersectional groups.</li>
</ul>

<h3>Title: TFB: Towards Comprehensive and Fair Benchmarking of Time Series  Forecasting Methods</h3>
<ul>
<li><strong>Authors: </strong>Xiangfei Qiu, Jilin Hu, Lekui Zhou, Xingjian Wu, Junyang Du, Buang Zhang, Chenjuan Guo, Aoying Zhou, Christian S. Jensen, Zhenli Sheng, Bin Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20150">https://arxiv.org/abs/2403.20150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20150">https://arxiv.org/pdf/2403.20150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20150]] TFB: Towards Comprehensive and Fair Benchmarking of Time Series  Forecasting Methods(https://arxiv.org/abs/2403.20150)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series characterization to ensure that the selected datasets are comprehensive. To remove biases against some methods, we include a diverse range of methods, including statistical learning, machine learning, and deep learning methods, and we also support a variety of evaluation strategies and metrics to ensure a more comprehensive evaluations of different methods. To support the integration of different methods into the benchmark and enable fair comparisons, TFB features a flexible and scalable pipeline that eliminates biases. Next, we employ TFB to perform a thorough evaluation of 21 Univariate Time Series Forecasting (UTSF) methods on 8,068 univariate time series and 14 Multivariate Time Series Forecasting (MTSF) methods on 25 datasets. The benchmark code and data are available at https://github.com/decisionintelligence/TFB.</li>
</ul>

<h3>Title: Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D  Generative Prior</h3>
<ul>
<li><strong>Authors: </strong>Jaehoon Ko, Kyusun Cho, Joungbin Lee, Heeji Yoon, Sangmin Lee, Sangjun Ahn, Seungryong Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20153">https://arxiv.org/abs/2403.20153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20153">https://arxiv.org/pdf/2403.20153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20153]] Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D  Generative Prior(https://arxiv.org/abs/2403.20153)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent methods for audio-driven talking head synthesis often optimize neural radiance fields (NeRF) on a monocular talking portrait video, leveraging its capability to render high-fidelity and 3D-consistent novel-view frames. However, they often struggle to reconstruct complete face geometry due to the absence of comprehensive 3D information in the input monocular videos. In this paper, we introduce a novel audio-driven talking head synthesis framework, called Talk3D, that can faithfully reconstruct its plausible facial geometries by effectively adopting the pre-trained 3D-aware generative prior. Given the personalized 3D generative model, we present a novel audio-guided attention U-Net architecture that predicts the dynamic face variations in the NeRF space driven by audio. Furthermore, our model is further modulated by audio-unrelated conditioning tokens which effectively disentangle variations unrelated to audio features. Compared to existing methods, our method excels in generating realistic facial geometries even under extreme head poses. We also conduct extensive experiments showing our approach surpasses state-of-the-art benchmarks in terms of both quantitative and qualitative evaluations.</li>
</ul>

<h3>Title: CAESAR: Enhancing Federated RL in Heterogeneous MDPs through  Convergence-Aware Sampling with Screening</h3>
<ul>
<li><strong>Authors: </strong>Hei Yi Mak, Flint Xiaofeng Fan, Luca A. Lanzendörfer, Cheston Tan, Wei Tsang Ooi, Roger Wattenhofer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20156">https://arxiv.org/abs/2403.20156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20156">https://arxiv.org/pdf/2403.20156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20156]] CAESAR: Enhancing Federated RL in Heterogeneous MDPs through  Convergence-Aware Sampling with Screening(https://arxiv.org/abs/2403.20156)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>In this study, we delve into Federated Reinforcement Learning (FedRL) in the context of value-based agents operating across diverse Markov Decision Processes (MDPs). Existing FedRL methods typically aggregate agents' learning by averaging the value functions across them to improve their performance. However, this aggregation strategy is suboptimal in heterogeneous environments where agents converge to diverse optimal value functions. To address this problem, we introduce the Convergence-AwarE SAmpling with scReening (CAESAR) aggregation scheme designed to enhance the learning of individual agents across varied MDPs. CAESAR is an aggregation strategy used by the server that combines convergence-aware sampling with a screening mechanism. By exploiting the fact that agents learning in identical MDPs are converging to the same optimal value function, CAESAR enables the selective assimilation of knowledge from more proficient counterparts, thereby significantly enhancing the overall learning efficiency. We empirically validate our hypothesis and demonstrate the effectiveness of CAESAR in enhancing the learning efficiency of agents, using both a custom-built GridWorld environment and the classical FrozenLake-v1 task, each presenting varying levels of environmental heterogeneity.</li>
</ul>

<h3>Title: A Systematic Analysis of Subwords and Cross-Lingual Transfer in  Multilingual Translation</h3>
<ul>
<li><strong>Authors: </strong>Francois Meyer, Jan Buys</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20157">https://arxiv.org/abs/2403.20157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20157">https://arxiv.org/pdf/2403.20157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20157]] A Systematic Analysis of Subwords and Cross-Lingual Transfer in  Multilingual Translation(https://arxiv.org/abs/2403.20157)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Multilingual modelling can improve machine translation for low-resource languages, partly through shared subword representations. This paper studies the role of subword segmentation in cross-lingual transfer. We systematically compare the efficacy of several subword methods in promoting synergy and preventing interference across different linguistic typologies. Our findings show that subword regularisation boosts synergy in multilingual modelling, whereas BPE more effectively facilitates transfer during cross-lingual fine-tuning. Notably, our results suggest that differences in orthographic word boundary conventions (the morphological granularity of written words) may impede cross-lingual transfer more significantly than linguistic unrelatedness. Our study confirms that decisions around subword modelling can be key to optimising the benefits of multilingual modelling.</li>
</ul>

<h3>Title: ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zehao Wen, Rabih Younes</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20158">https://arxiv.org/abs/2403.20158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20158">https://arxiv.org/pdf/2403.20158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20158]] ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned  Language Models(https://arxiv.org/abs/2403.20158)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In our rapidly evolving digital sphere, the ability to discern media bias becomes crucial as it can shape public sentiment and influence pivotal decisions. The advent of large language models (LLMs), such as ChatGPT, noted for their broad utility in various natural language processing (NLP) tasks, invites exploration of their efficacy in media bias detection. Can ChatGPT detect media bias? This study seeks to answer this question by leveraging the Media Bias Identification Benchmark (MBIB) to assess ChatGPT's competency in distinguishing six categories of media bias, juxtaposed against fine-tuned models such as BART, ConvBERT, and GPT-2. The findings present a dichotomy: ChatGPT performs at par with fine-tuned models in detecting hate speech and text-level context bias, yet faces difficulties with subtler elements of other bias detections, namely, fake news, racial, gender, and cognitive biases.</li>
</ul>

<h3>Title: MCNet: A crowd denstity estimation network based on integrating  multiscale attention module</h3>
<ul>
<li><strong>Authors: </strong>Qiang Guo, Rubo Zhang, Di Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20173">https://arxiv.org/abs/2403.20173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20173">https://arxiv.org/pdf/2403.20173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20173]] MCNet: A crowd denstity estimation network based on integrating  multiscale attention module(https://arxiv.org/abs/2403.20173)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Aiming at the metro video surveillance system has not been able to effectively solve the metro crowd density estimation problem, a Metro Crowd density estimation Network (called MCNet) is proposed to automatically classify crowd density level of passengers. Firstly, an Integrating Multi-scale Attention (IMA) module is proposed to enhance the ability of the plain classifiers to extract semantic crowd texture features to accommodate to the characteristics of the crowd texture feature. The innovation of the IMA module is to fuse the dilation convolution, multiscale feature extraction and attention mechanism to obtain multi-scale crowd feature activation from a larger receptive field with lower computational cost, and to strengthen the crowds activation state of convolutional features in top layers. Secondly, a novel lightweight crowd texture feature extraction network is proposed, which can directly process video frames and automatically extract texture features for crowd density estimation, while its faster image processing speed and fewer network parameters make it flexible to be deployed on embedded platforms with limited hardware resources. Finally, this paper integrates IMA module and the lightweight crowd texture feature extraction network to construct the MCNet, and validate the feasibility of this network on image classification dataset: Cifar10 and four crowd density datasets: PETS2009, Mall, QUT and SH_METRO to validate the MCNet whether can be a suitable solution for crowd density estimation in metro video surveillance where there are image processing challenges such as high density, high occlusion, perspective distortion and limited hardware resources.</li>
</ul>

<h3>Title: Measuring Taiwanese Mandarin Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Po-Heng Chen, Sijia Cheng, Wei-Lin Chen, Yen-Ting Lin, Yun-Nung Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20180">https://arxiv.org/abs/2403.20180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20180">https://arxiv.org/pdf/2403.20180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20180]] Measuring Taiwanese Mandarin Language Understanding(https://arxiv.org/abs/2403.20180)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The evaluation of large language models (LLMs) has drawn substantial attention in the field recently. This work focuses on evaluating LLMs in a Chinese context, specifically, for Traditional Chinese which has been largely underrepresented in existing benchmarks. We present TMLU, a holistic evaluation suit tailored for assessing the advanced knowledge and reasoning capability in LLMs, under the context of Taiwanese Mandarin. TMLU consists of an array of 37 subjects across social science, STEM, humanities, Taiwan-specific content, and others, ranging from middle school to professional levels. In addition, we curate chain-of-thought-like few-shot explanations for each subject to facilitate the evaluation of complex reasoning skills. To establish a comprehensive baseline, we conduct extensive experiments and analysis on 24 advanced LLMs. The results suggest that Chinese open-weight models demonstrate inferior performance comparing to multilingual proprietary ones, and open-weight models tailored for Taiwanese Mandarin lag behind the Simplified-Chinese counterparts. The findings indicate great headrooms for improvement, and emphasize the goal of TMLU to foster the development of localized Taiwanese-Mandarin LLMs. We release the benchmark and evaluation scripts for the community to promote future research.</li>
</ul>

<h3>Title: HARMamba: Efficient Wearable Sensor Human Activity Recognition Based on  Bidirectional Selective SSM</h3>
<ul>
<li><strong>Authors: </strong>Shuangjian Li, Tao Zhu, Furong Duan, Liming Chen, Huansheng Ning, Yaping Wan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20183">https://arxiv.org/abs/2403.20183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20183">https://arxiv.org/pdf/2403.20183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20183]] HARMamba: Efficient Wearable Sensor Human Activity Recognition Based on  Bidirectional Selective SSM(https://arxiv.org/abs/2403.20183)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Wearable sensor human activity recognition (HAR) is a crucial area of research in activity sensing. While transformer-based temporal deep learning models have been extensively studied and implemented, their large number of parameters present significant challenges in terms of system computing load and memory usage, rendering them unsuitable for real-time mobile activity recognition applications. Recently, an efficient hardware-aware state space model (SSM) called Mamba has emerged as a promising alternative. Mamba demonstrates strong potential in long sequence modeling, boasts a simpler network architecture, and offers an efficient hardware-aware design. Leveraging SSM for activity recognition represents an appealing avenue for exploration. In this study, we introduce HARMamba, which employs a more lightweight selective SSM as the foundational model architecture for activity recognition. The goal is to address the computational resource constraints encountered in real-time activity recognition scenarios. Our approach involves processing sensor data flow by independently learning each channel and segmenting the data into "patches". The marked sensor sequence's position embedding serves as the input token for the bidirectional state space model, ultimately leading to activity categorization through the classification head. Compared to established activity recognition frameworks like Transformer-based models, HARMamba achieves superior performance while also reducing computational and memory overhead. Furthermore, our proposed method has been extensively tested on four public activity datasets: PAMAP2, WISDM, UNIMIB, and UCI, demonstrating impressive performance in activity recognition tasks.</li>
</ul>

<h3>Title: Sketch-to-Architecture: Generative AI-aided Architectural Design</h3>
<ul>
<li><strong>Authors: </strong>Pengzhi Li, Baijuan Li, Zhiheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20186">https://arxiv.org/abs/2403.20186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20186">https://arxiv.org/pdf/2403.20186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20186]] Sketch-to-Architecture: Generative AI-aided Architectural Design(https://arxiv.org/abs/2403.20186)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recently, the development of large-scale models has paved the way for various interdisciplinary research, including architecture. By using generative AI, we present a novel workflow that utilizes AI models to generate conceptual floorplans and 3D models from simple sketches, enabling rapid ideation and controlled generation of architectural renderings based on textual descriptions. Our work demonstrates the potential of generative AI in the architectural design process, pointing towards a new direction of computer-aided architectural design. Our project website is available at: https://zrealli.github.io/sketch2arc</li>
</ul>

<h3>Title: Homomorphic WiSARDs: Efficient Weightless Neural Network training over  encrypted data</h3>
<ul>
<li><strong>Authors: </strong>Leonardo Neumann, Antonio Guimarães, Diego F. Aranha, Edson Borin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20190">https://arxiv.org/abs/2403.20190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20190">https://arxiv.org/pdf/2403.20190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20190]] Homomorphic WiSARDs: Efficient Weightless Neural Network training over  encrypted data(https://arxiv.org/abs/2403.20190)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The widespread application of machine learning algorithms is a matter of increasing concern for the data privacy research community, and many have sought to develop privacy-preserving techniques for it. Among existing approaches, the homomorphic evaluation of ML algorithms stands out by performing operations directly over encrypted data, enabling strong guarantees of confidentiality. The homomorphic evaluation of inference algorithms is practical even for relatively deep Convolution Neural Networks (CNNs). However, training is still a major challenge, with current solutions often resorting to lightweight algorithms that can be unfit for solving more complex problems, such as image recognition. This work introduces the homomorphic evaluation of Wilkie, Stonham, and Aleksander's Recognition Device (WiSARD) and subsequent Weightless Neural Networks (WNNs) for training and inference on encrypted data. Compared to CNNs, WNNs offer better performance with a relatively small accuracy drop. We develop a complete framework for it, including several building blocks that can be of independent interest. Our framework achieves 91.7% accuracy on the MNIST dataset after only 3.5 minutes of encrypted training (multi-threaded), going up to 93.8% in 3.5 hours. For the HAM10000 dataset, we achieve 67.9% accuracy in just 1.5 minutes, going up to 69.9% after 1 hour. Compared to the state of the art on the HE evaluation of CNN training, Glyph (Lou et al., NeurIPS 2020), these results represent a speedup of up to 1200 times with an accuracy loss of at most 5.4%. For HAM10000, we even achieved a 0.65% accuracy improvement while being 60 times faster than Glyph. We also provide solutions for small-scale encrypted training. In a single thread on a desktop machine using less than 200MB of memory, we train over 1000 MNIST images in 12 minutes or over the entire Wisconsin Breast Cancer dataset in just 11 seconds.</li>
</ul>

<h3>Title: Motion Inversion for Video Customization</h3>
<ul>
<li><strong>Authors: </strong>Luozhou Wang, Guibao Shen, Yixun Liang, Xin Tao, Pengfei Wan, Di Zhang, Yijun Li, Yingcong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20193">https://arxiv.org/abs/2403.20193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20193">https://arxiv.org/pdf/2403.20193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20193]] Motion Inversion for Video Customization(https://arxiv.org/abs/2403.20193)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>In this research, we present a novel approach to motion customization in video generation, addressing the widespread gap in the thorough exploration of motion representation within video generative models. Recognizing the unique challenges posed by video's spatiotemporal nature, our method introduces Motion Embeddings, a set of explicit, temporally coherent one-dimensional embeddings derived from a given video. These embeddings are designed to integrate seamlessly with the temporal transformer modules of video diffusion models, modulating self-attention computations across frames without compromising spatial integrity. Our approach offers a compact and efficient solution to motion representation and enables complex manipulations of motion characteristics through vector arithmetic in the embedding space. Furthermore, we identify the Temporal Discrepancy in video generative models, which refers to variations in how different motion modules process temporal relationships between frames. We leverage this understanding to optimize the integration of our motion embeddings. Our contributions include the introduction of a tailored motion embedding for customization tasks, insights into the temporal processing differences in video models, and a demonstration of the practical advantages and effectiveness of our method through extensive experiments.</li>
</ul>

<h3>Title: Automatic Alignment of Discourse Relations of Different Discourse  Annotation Frameworks</h3>
<ul>
<li><strong>Authors: </strong>Yingxue Fu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20196">https://arxiv.org/abs/2403.20196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20196">https://arxiv.org/pdf/2403.20196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20196]] Automatic Alignment of Discourse Relations of Different Discourse  Annotation Frameworks(https://arxiv.org/abs/2403.20196)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Existing discourse corpora are annotated based on different frameworks, which show significant dissimilarities in definitions of arguments and relations and structural constraints. Despite surface differences, these frameworks share basic understandings of discourse relations. The relationship between these frameworks has been an open research question, especially the correlation between relation inventories utilized in different frameworks. Better understanding of this question is helpful for integrating discourse theories and enabling interoperability of discourse corpora annotated under different frameworks. However, studies that explore correlations between discourse relation inventories are hindered by different criteria of discourse segmentation, and expert knowledge and manual examination are typically needed. Some semi-automatic methods have been proposed, but they rely on corpora annotated in multiple frameworks in parallel. In this paper, we introduce a fully automatic approach to address the challenges. Specifically, we extend the label-anchored contrastive learning method introduced by Zhang et al. (2022b) to learn label embeddings during a classification task. These embeddings are then utilized to map discourse relations from different frameworks. We show experimental results on RST-DT (Carlson et al., 2001) and PDTB 3.0 (Prasad et al., 2018).</li>
</ul>

<h3>Title: Unleashing the Potential of Large Language Models for Predictive Tabular  Tasks in Data Science</h3>
<ul>
<li><strong>Authors: </strong>Yazheng Yang, Yuqi Wang, Sankalok Sen, Lei Li, Qi Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20208">https://arxiv.org/abs/2403.20208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20208">https://arxiv.org/pdf/2403.20208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20208]] Unleashing the Potential of Large Language Models for Predictive Tabular  Tasks in Data Science(https://arxiv.org/abs/2403.20208)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improvements over existing benchmarks. These advancements highlight the efficacy of tailoring LLM training to solve table-related problems in data science, thereby establishing a new benchmark in the utilization of LLMs for enhancing tabular intelligence.</li>
</ul>

<h3>Title: Decentralized Multimedia Data Sharing in IoV: A Learning-based  Equilibrium of Supply and Demand</h3>
<ul>
<li><strong>Authors: </strong>Jiani Fan, Minrui Xu, Jiale Guo, Lwin Khin Shar, Jiawen Kang, Dusit Niyato, Kwok-Yan Lam</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20218">https://arxiv.org/abs/2403.20218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20218">https://arxiv.org/pdf/2403.20218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20218]] Decentralized Multimedia Data Sharing in IoV: A Learning-based  Equilibrium of Supply and Demand(https://arxiv.org/abs/2403.20218)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect</a></li>
<li><strong>Abstract: </strong>The Internet of Vehicles (IoV) has great potential to transform transportation systems by enhancing road safety, reducing traffic congestion, and improving user experience through onboard infotainment applications. Decentralized data sharing can improve security, privacy, reliability, and facilitate infotainment data sharing in IoVs. However, decentralized data sharing may not achieve the expected efficiency if there are IoV users who only want to consume the shared data but are not willing to contribute their own data to the community, resulting in incomplete information observed by other vehicles and infrastructure, which can introduce additional transmission latency. Therefore, in this article, by modeling the data sharing ecosystem as a data trading market, we propose a decentralized data-sharing incentive mechanism based on multi-intelligent reinforcement learning to learn the supply-demand balance in markets and minimize transmission latency. Our proposed mechanism takes into account the dynamic nature of IoV markets, which can experience frequent fluctuations in supply and demand. We propose a time-sensitive Key-Policy Attribute-Based Encryption (KP-ABE) mechanism coupled with Named Data Networking (NDN) to protect data in IoVs, which adds a layer of security to our proposed solution. Additionally, we design a decentralized market for efficient data sharing in IoVs, where continuous double auctions are adopted. The proposed mechanism based on multi-agent deep reinforcement learning can learn the supply-demand equilibrium in markets, thus improving the efficiency and sustainability of markets. Theoretical analysis and experimental results show that our proposed learning-based incentive mechanism outperforms baselines by 10% in determining the equilibrium of supply and demand while reducing transmission latency by 20%.</li>
</ul>

<h3>Title: Graph Neural Aggregation-diffusion with Metastability</h3>
<ul>
<li><strong>Authors: </strong>Kaiyuan Cui, Xinyan Wang, Zicheng Zhang, Weichen Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20221">https://arxiv.org/abs/2403.20221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20221">https://arxiv.org/pdf/2403.20221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20221]] Graph Neural Aggregation-diffusion with Metastability(https://arxiv.org/abs/2403.20221)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Continuous graph neural models based on differential equations have expanded the architecture of graph neural networks (GNNs). Due to the connection between graph diffusion and message passing, diffusion-based models have been widely studied. However, diffusion naturally drives the system towards an equilibrium state, leading to issues like over-smoothing. To this end, we propose GRADE inspired by graph aggregation-diffusion equations, which includes the delicate balance between nonlinear diffusion and aggregation induced by interaction potentials. The node representations obtained through aggregation-diffusion equations exhibit metastability, indicating that features can aggregate into multiple clusters. In addition, the dynamics within these clusters can persist for long time periods, offering the potential to alleviate over-smoothing effects. This nonlinear diffusion in our model generalizes existing diffusion-based models and establishes a connection with classical GNNs. We prove that GRADE achieves competitive performance across various benchmarks and alleviates the over-smoothing issue in GNNs evidenced by the enhanced Dirichlet energy.</li>
</ul>

<h3>Title: Long-Tailed Anomaly Detection with Learnable Class Names</h3>
<ul>
<li><strong>Authors: </strong>Chih-Hui Ho, Kuan-Chuan Peng, Nuno Vasconcelos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20236">https://arxiv.org/abs/2403.20236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20236">https://arxiv.org/pdf/2403.20236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20236]] Long-Tailed Anomaly Detection with Learnable Class Names(https://arxiv.org/abs/2403.20236)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Anomaly detection (AD) aims to identify defective images and localize their defects (if any). Ideally, AD models should be able to detect defects over many image classes; without relying on hard-coded class names that can be uninformative or inconsistent across datasets; learn without anomaly supervision; and be robust to the long-tailed distributions of real-world applications. To address these challenges, we formulate the problem of long-tailed AD by introducing several datasets with different levels of class imbalance and metrics for performance evaluation. We then propose a novel method, LTAD, to detect defects from multiple and long-tailed classes, without relying on dataset class names. LTAD combines AD by reconstruction and semantic AD modules. AD by reconstruction is implemented with a transformer-based reconstruction module. Semantic AD is implemented with a binary classifier, which relies on learned pseudo class names and a pretrained foundation model. These modules are learned over two phases. Phase 1 learns the pseudo-class names and a variational autoencoder (VAE) for feature synthesis that augments the training data to combat long-tails. Phase 2 then learns the parameters of the reconstruction and classification modules of LTAD. Extensive experiments using the proposed long-tailed datasets show that LTAD substantially outperforms the state-of-the-art methods for most forms of dataset imbalance. The long-tailed dataset split is available at https://zenodo.org/records/10854201 .</li>
</ul>

<h3>Title: Enhancing Dimension-Reduced Scatter Plots with Class and Feature  Centroids</h3>
<ul>
<li><strong>Authors: </strong>Daniel B. Hier, Tayo Obafemi-Ajayi, Gayla R. Olbricht, Devin M. Burns, Sasha Petrenko, Donald C. Wunsch II</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20246">https://arxiv.org/abs/2403.20246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20246">https://arxiv.org/pdf/2403.20246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20246]] Enhancing Dimension-Reduced Scatter Plots with Class and Feature  Centroids(https://arxiv.org/abs/2403.20246)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Dimension reduction is increasingly applied to high-dimensional biomedical data to improve its interpretability. When datasets are reduced to two dimensions, each observation is assigned an x and y coordinates and is represented as a point on a scatter plot. A significant challenge lies in interpreting the meaning of the x and y axes due to the complexities inherent in dimension reduction. This study addresses this challenge by using the x and y coordinates derived from dimension reduction to calculate class and feature centroids, which can be overlaid onto the scatter plots. This method connects the low-dimension space to the original high-dimensional space. We illustrate the utility of this approach with data derived from the phenotypes of three neurogenetic diseases and demonstrate how the addition of class and feature centroids increases the interpretability of scatter plots.</li>
</ul>

<h3>Title: Relation Rectification in Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Yinwei Wu, Xingyi Yang, Xinchao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20249">https://arxiv.org/abs/2403.20249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20249">https://arxiv.org/pdf/2403.20249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20249]] Relation Rectification in Diffusion Model(https://arxiv.org/abs/2403.20249)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Despite their exceptional generative abilities, large text-to-image diffusion models, much like skilled but careless artists, often struggle with accurately depicting visual relationships between objects. This issue, as we uncover through careful analysis, arises from a misaligned text encoder that struggles to interpret specific relationships and differentiate the logical order of associated objects. To resolve this, we introduce a novel task termed Relation Rectification, aiming to refine the model to accurately represent a given relationship it initially fails to generate. To address this, we propose an innovative solution utilizing a Heterogeneous Graph Convolutional Network (HGCN). It models the directional relationships between relation terms and corresponding objects within the input prompts. Specifically, we optimize the HGCN on a pair of prompts with identical relational words but reversed object orders, supplemented by a few reference images. The lightweight HGCN adjusts the text embeddings generated by the text encoder, ensuring the accurate reflection of the textual relation in the embedding space. Crucially, our method retains the parameters of the text encoder and diffusion model, preserving the model's robust performance on unrelated descriptions. We validated our approach on a newly curated dataset of diverse relational data, demonstrating both quantitative and qualitative enhancements in generating images with precise visual relations. Project page: https://wuyinwei-hah.github.io/rrnet.github.io/.</li>
</ul>

<h3>Title: Latent Embedding Clustering for Occlusion Robust Head Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>José Celestino, Manuel Marques, Jacinto C. Nascimento</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20251">https://arxiv.org/abs/2403.20251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20251">https://arxiv.org/pdf/2403.20251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20251]] Latent Embedding Clustering for Occlusion Robust Head Pose Estimation(https://arxiv.org/abs/2403.20251)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Head pose estimation has become a crucial area of research in computer vision given its usefulness in a wide range of applications, including robotics, surveillance, or driver attention monitoring. One of the most difficult challenges in this field is managing head occlusions that frequently take place in real-world scenarios. In this paper, we propose a novel and efficient framework that is robust in real world head occlusion scenarios. In particular, we propose an unsupervised latent embedding clustering with regression and classification components for each pose angle. The model optimizes latent feature representations for occluded and non-occluded images through a clustering term while improving fine-grained angle predictions. Experimental evaluation on in-the-wild head pose benchmark datasets reveal competitive performance in comparison to state-of-the-art methodologies with the advantage of having a significant data reduction. We observe a substantial improvement in occluded head pose estimation. Also, an ablation study is conducted to ascertain the impact of the clustering term within our proposed framework.</li>
</ul>

<h3>Title: Using LLMs to Model the Beliefs and Preferences of Targeted Populations</h3>
<ul>
<li><strong>Authors: </strong>Keiichi Namikoshi, Alex Filipowicz, David A. Shamma, Rumen Iliev, Candice L. Hogan, Nikos Arechiga</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20252">https://arxiv.org/abs/2403.20252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20252">https://arxiv.org/pdf/2403.20252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20252]] Using LLMs to Model the Beliefs and Preferences of Targeted Populations(https://arxiv.org/abs/2403.20252)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We consider the problem of aligning a large language model (LLM) to model the preferences of a human population. Modeling the beliefs, preferences, and behaviors of a specific population can be useful for a variety of different applications, such as conducting simulated focus groups for new products, conducting virtual surveys, and testing behavioral interventions, especially for interventions that are expensive, impractical, or unethical. Existing work has had mixed success using LLMs to accurately model human behavior in different contexts. We benchmark and evaluate two well-known fine-tuning approaches and evaluate the resulting populations on their ability to match the preferences of real human respondents on a survey of preferences for battery electric vehicles (BEVs). We evaluate our models against their ability to match population-wide statistics as well as their ability to match individual responses, and we investigate the role of temperature in controlling the trade-offs between these two. Additionally, we propose and evaluate a novel loss term to improve model performance on responses that require a numeric response.</li>
</ul>

<h3>Title: MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image  Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20253">https://arxiv.org/abs/2403.20253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20253">https://arxiv.org/pdf/2403.20253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20253]] MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image  Segmentation(https://arxiv.org/abs/2403.20253)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation of anatomical structures and pathology is crucial in modern clinical diagnosis, disease study, and treatment planning. To date, great progress has been made in deep learning-based segmentation techniques, but most methods still lack data efficiency, generalizability, and interactability. Consequently, the development of new, precise segmentation methods that demand fewer labeled datasets is of utmost importance in medical image analysis. Recently, the emergence of foundation models, such as CLIP and Segment-Anything-Model (SAM), with comprehensive cross-domain representation opened the door for interactive and universal image segmentation. However, exploration of these models for data-efficient medical image segmentation is still limited, but is highly necessary. In this paper, we propose a novel framework, called MedCLIP-SAM that combines CLIP and SAM models to generate segmentation of clinical scans using text prompts in both zero-shot and weakly supervised settings. To achieve this, we employed a new Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE) loss to fine-tune the BiomedCLIP model and the recent gScoreCAM to generate prompts to obtain segmentation masks from SAM in a zero-shot setting. Additionally, we explored the use of zero-shot segmentation labels in a weakly supervised paradigm to improve the segmentation quality further. By extensively testing three diverse segmentation tasks and medical image modalities (breast tumor ultrasound, brain tumor MRI, and lung X-ray), our proposed framework has demonstrated excellent accuracy.</li>
</ul>

<h3>Title: Benchmarking the Robustness of Temporal Action Detection Models Against  Temporal Corruptions</h3>
<ul>
<li><strong>Authors: </strong>Runhao Zeng, Xiaoyong Chen, Jiaming Liang, Huisi Wu, Guangzhong Cao, Yong Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20254">https://arxiv.org/abs/2403.20254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20254">https://arxiv.org/pdf/2403.20254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20254]] Benchmarking the Robustness of Temporal Action Detection Models Against  Temporal Corruptions(https://arxiv.org/abs/2403.20254)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Temporal action detection (TAD) aims to locate action positions and recognize action categories in long-term untrimmed videos. Although many methods have achieved promising results, their robustness has not been thoroughly studied. In practice, we observe that temporal information in videos can be occasionally corrupted, such as missing or blurred frames. Interestingly, existing methods often incur a significant performance drop even if only one frame is affected. To formally evaluate the robustness, we establish two temporal corruption robustness benchmarks, namely THUMOS14-C and ActivityNet-v1.3-C. In this paper, we extensively analyze the robustness of seven leading TAD methods and obtain some interesting findings: 1) Existing methods are particularly vulnerable to temporal corruptions, and end-to-end methods are often more susceptible than those with a pre-trained feature extractor; 2) Vulnerability mainly comes from localization error rather than classification error; 3) When corruptions occur in the middle of an action instance, TAD models tend to yield the largest performance drop. Besides building a benchmark, we further develop a simple but effective robust training method to defend against temporal corruptions, through the FrameDrop augmentation and Temporal-Robust Consistency loss. Remarkably, our approach not only improves robustness but also yields promising improvements on clean data. We believe that this study will serve as a benchmark for future research in robust video analysis. Source code and models are available at https://github.com/Alvin-Zeng/temporal-robustness-benchmark.</li>
</ul>

<h3>Title: ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Thibaut Thonet, Jos Rozen, Laurent Besacier</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20262">https://arxiv.org/abs/2403.20262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20262">https://arxiv.org/pdf/2403.20262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20262]] ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language  Models(https://arxiv.org/abs/2403.20262)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending models' context size to better capture dependencies within long documents. While benchmarks have been proposed to assess long-range abilities, existing efforts primarily considered generic tasks that are not necessarily aligned with real-world applications. In contrast, our work proposes a new benchmark for long-context LLMs focused on a practical meeting assistant scenario. In this scenario, the long contexts consist of transcripts obtained by automatic speech recognition, presenting unique challenges for LLMs due to the inherent noisiness and oral nature of such data. Our benchmark, named ELITR-Bench, augments the existing ELITR corpus' transcripts with 271 manually crafted questions and their ground-truth answers. Our experiments with recent long-context LLMs on ELITR-Bench highlight a gap between open-source and proprietary models, especially when questions are asked sequentially within a conversation. We also provide a thorough analysis of our GPT-4-based evaluation method, encompassing insights from a crowdsourcing study. Our findings suggest that while GPT-4's evaluation scores are correlated with human judges', its ability to differentiate among more than three score levels may be limited.</li>
</ul>

<h3>Title: Latxa: An Open Language Model and Evaluation Suite for Basque</h3>
<ul>
<li><strong>Authors: </strong>Julen Etxaniz, Oscar Sainz, Naiara Perez, Itziar Aldabe, German Rigau, Eneko Agirre, Aitor Ormazabal, Mikel Artetxe, Aitor Soroa</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20266">https://arxiv.org/abs/2403.20266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20266">https://arxiv.org/pdf/2403.20266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20266]] Latxa: An Open Language Model and Evaluation Suite for Basque(https://arxiv.org/abs/2403.20266)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well as our new pretraining corpora and evaluation datasets, are publicly available under open licenses at https://github.com/hitz-zentroa/latxa. Our suite enables reproducible research on methods to build LLMs for low-resource languages.</li>
</ul>

<h3>Title: Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to  Comprehend What You Want</h3>
<ul>
<li><strong>Authors: </strong>Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20271">https://arxiv.org/abs/2403.20271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20271">https://arxiv.org/pdf/2403.20271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20271]] Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to  Comprehend What You Want(https://arxiv.org/abs/2403.20271)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The interaction between humans and artificial intelligence (AI) is a crucial factor that reflects the effectiveness of multimodal large language models (MLLMs). However, current MLLMs primarily focus on image-level comprehension and limit interaction to textual instructions, thereby constraining their flexibility in usage and depth of response. In this paper, we introduce the Draw-and-Understand project: a new model, a multi-domain dataset, and a challenging benchmark for visual prompting. Specifically, we propose SPHINX-V, a new end-to-end trained Multimodal Large Language Model (MLLM) that connects a vision encoder, a visual prompt encoder and an LLM for various visual prompts (points, bounding boxes, and free-form shape) and language understanding. To advance visual prompting research for MLLMs, we introduce MDVP-Data and MDVP-Bench. MDVP-Data features a multi-domain dataset containing 1.6M unique image-visual prompt-text instruction-following samples, including natural images, document images, OCR images, mobile screenshots, web screenshots, and multi-panel images. Furthermore, we present MDVP-Bench, a comprehensive and challenging benchmark to assess a model's capability in understanding visual prompting instructions. Our experiments demonstrate SPHINX-V's impressive multimodal interaction capabilities through visual prompting, revealing significant improvements in detailed pixel-level description and question-answering abilities.</li>
</ul>

<h3>Title: LUQ: Long-text Uncertainty Quantification for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Caiqi Zhang, Fangyu Liu, Marco Basaldella, Nigel Collier</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20279">https://arxiv.org/abs/2403.20279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20279">https://arxiv.org/pdf/2403.20279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20279]] LUQ: Long-text Uncertainty Quantification for LLMs(https://arxiv.org/abs/2403.20279)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capability in a variety of NLP tasks. Despite their effectiveness, these models are prone to generate nonfactual content. Uncertainty Quantification (UQ) is pivotal in enhancing our understanding of a model's confidence in its generated content, thereby aiding in the mitigation of nonfactual outputs. Existing research on UQ predominantly targets short text generation, typically yielding brief, word-limited responses. However, real-world applications frequently necessitate much longer responses. Our study first highlights the limitations of current UQ methods in handling long text generation. We then introduce \textsc{Luq}, a novel sampling-based UQ approach specifically designed for long text. Our findings reveal that \textsc{Luq} outperforms existing baseline methods in correlating with the model's factuality scores (negative coefficient of -0.85 observed for Gemini Pro). With \textsc{Luq} as the tool for UQ, we investigate behavior patterns of several popular LLMs' response confidence spectrum and how that interplays with the response' factuality. We identify that LLMs lack confidence in generating long text for rare facts and a factually strong model (i.e. GPT-4) tends to reject questions it is not sure about. To further improve the factual accuracy of LLM responses, we propose a method called \textsc{Luq-Ensemble} that ensembles responses from multiple models and selects the response with the least uncertainty. The ensembling method greatly improves the response factuality upon the best standalone LLM.</li>
</ul>

<h3>Title: Sparse multimodal fusion with modal channel attention</h3>
<ul>
<li><strong>Authors: </strong>Josiah Bjorgaard</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20280">https://arxiv.org/abs/2403.20280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20280">https://arxiv.org/pdf/2403.20280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20280]] Sparse multimodal fusion with modal channel attention(https://arxiv.org/abs/2403.20280)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>The ability of masked multimodal transformer architectures to learn a robust embedding space when modality samples are sparsely aligned is studied by measuring the quality of generated embedding spaces as a function of modal sparsity. An extension to the masked multimodal transformer model is proposed which incorporates modal-incomplete channels in the multihead attention mechanism called modal channel attention (MCA). Two datasets with 4 modalities are used, CMU-MOSEI for multimodal sentiment recognition and TCGA for multiomics. Models are shown to learn uniform and aligned embedding spaces with only two out of four modalities in most samples. It was found that, even with no modal sparsity, the proposed MCA mechanism improves the quality of generated embedding spaces, recall metrics, and subsequent performance on downstream tasks.</li>
</ul>

<h3>Title: LayerNorm: A key component in parameter-efficient fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Taha ValizadehAslani, Hualou Liang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20284">https://arxiv.org/abs/2403.20284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20284">https://arxiv.org/pdf/2403.20284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20284]] LayerNorm: A key component in parameter-efficient fine-tuning(https://arxiv.org/abs/2403.20284)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Fine-tuning a pre-trained model, such as Bidirectional Encoder Representations from Transformers (BERT), has been proven to be an effective method for solving many natural language processing (NLP) tasks. However, due to the large number of parameters in many state-of-the-art NLP models, including BERT, the process of fine-tuning is computationally expensive. One attractive solution to this issue is parameter-efficient fine-tuning, which involves modifying only a minimal segment of the model while keeping the remainder unchanged. Yet, it remains unclear which segment of the BERT model is crucial for fine-tuning. In this paper, we first analyze different components in the BERT model to pinpoint which one undergoes the most significant changes after fine-tuning. We find that output LayerNorm changes more than any other components when fine-tuned for different General Language Understanding Evaluation (GLUE) tasks. Then we show that only fine-tuning the LayerNorm can reach comparable, or in some cases better, performance to full fine-tuning and other parameter-efficient fine-tuning methods. Moreover, we use Fisher information to determine the most critical subset of LayerNorm and demonstrate that many NLP tasks in the GLUE benchmark can be solved by fine-tuning only a small portion of LayerNorm with negligible performance degradation.</li>
</ul>

<h3>Title: Benchmarking Counterfactual Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Thomas Melistas, Nikos Spyrou, Nefeli Gkouti, Pedro Sanchez, Athanasios Vlontzos, Giorgos Papanastasiou, Sotirios A. Tsaftaris</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20287">https://arxiv.org/abs/2403.20287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20287">https://arxiv.org/pdf/2403.20287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20287]] Benchmarking Counterfactual Image Generation(https://arxiv.org/abs/2403.20287)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Counterfactual image generation is pivotal for understanding the causal relations of variables, with applications in interpretability and generation of unbiased synthetic data. However, evaluating image generation is a long-standing challenge in itself. The need to evaluate counterfactual generation compounds on this challenge, precisely because counterfactuals, by definition, are hypothetical scenarios without observable ground truths. In this paper, we present a novel comprehensive framework aimed at benchmarking counterfactual image generation methods. We incorporate metrics that focus on evaluating diverse aspects of counterfactuals, such as composition, effectiveness, minimality of interventions, and image realism. We assess the performance of three distinct conditional image generation model types, based on the Structural Causal Model paradigm. Our work is accompanied by a user-friendly Python package which allows to further evaluate and benchmark existing and future counterfactual image generation methods. Our framework is extendable to additional SCM and other causal methods, generative models, and datasets.</li>
</ul>

<h3>Title: Can LLMs Correct Physicians, Yet? Investigating Effective Interaction  Methods in the Medical Domain</h3>
<ul>
<li><strong>Authors: </strong>Burcu Sayin, Pasquale Minervini, Jacopo Staiano, Andrea Passerini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20288">https://arxiv.org/abs/2403.20288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20288">https://arxiv.org/pdf/2403.20288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20288]] Can LLMs Correct Physicians, Yet? Investigating Effective Interaction  Methods in the Medical Domain(https://arxiv.org/abs/2403.20288)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We explore the potential of Large Language Models (LLMs) to assist and potentially correct physicians in medical decision-making tasks. We evaluate several LLMs, including Meditron, Llama2, and Mistral, to analyze the ability of these models to interact effectively with physicians across different scenarios. We consider questions from PubMedQA and several tasks, ranging from binary (yes/no) responses to long answer generation, where the answer of the model is produced after an interaction with a physician. Our findings suggest that prompt design significantly influences the downstream accuracy of LLMs and that LLMs can provide valuable feedback to physicians, challenging incorrect diagnoses and contributing to more accurate decision-making. For example, when the physician is accurate 38% of the time, Mistral can produce the correct answer, improving accuracy up to 74% depending on the prompt being used, while Llama2 and Meditron models exhibit greater sensitivity to prompt choice. Our analysis also uncovers the challenges of ensuring that LLM-generated suggestions are pertinent and useful, emphasizing the need for further research in this area.</li>
</ul>

<h3>Title: Convolutional Prompting meets Language Models for Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Anurag Roy, Riddhiman Moulick, Vinay K. Verma, Saptarshi Ghosh, Abir Das</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20317">https://arxiv.org/abs/2403.20317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20317">https://arxiv.org/pdf/2403.20317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20317]] Convolutional Prompting meets Language Models for Continual Learning(https://arxiv.org/abs/2403.20317)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Continual Learning (CL) enables machine learning models to learn from continuously shifting new training data in absence of data from old tasks. Recently, pretrained vision transformers combined with prompt tuning have shown promise for overcoming catastrophic forgetting in CL. These approaches rely on a pool of learnable prompts which can be inefficient in sharing knowledge across tasks leading to inferior performance. In addition, the lack of fine-grained layer specific prompts does not allow these to fully express the strength of the prompts for CL. We address these limitations by proposing ConvPrompt, a novel convolutional prompt creation mechanism that maintains layer-wise shared embeddings, enabling both layer-specific learning and better concept transfer across tasks. The intelligent use of convolution enables us to maintain a low parameter overhead without compromising performance. We further leverage Large Language Models to generate fine-grained text descriptions of each category which are used to get task similarity and dynamically decide the number of prompts to be learned. Extensive experiments demonstrate the superiority of ConvPrompt and improves SOTA by ~3% with significantly less parameter overhead. We also perform strong ablation over various modules to disentangle the importance of different components.</li>
</ul>

<h3>Title: SeaBird: Segmentation in Bird's View with Dice Loss Improves Monocular  3D Detection of Large Objects</h3>
<ul>
<li><strong>Authors: </strong>Abhinav Kumar, Yuliang Guo, Xinyu Huang, Liu Ren, Xiaoming Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20318">https://arxiv.org/abs/2403.20318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20318">https://arxiv.org/pdf/2403.20318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20318]] SeaBird: Segmentation in Bird's View with Dice Loss Improves Monocular  3D Detection of Large Objects(https://arxiv.org/abs/2403.20318)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Monocular 3D detectors achieve remarkable performance on cars and smaller objects. However, their performance drops on larger objects, leading to fatal accidents. Some attribute the failures to training data scarcity or their receptive field requirements of large objects. In this paper, we highlight this understudied problem of generalization to large objects. We find that modern frontal detectors struggle to generalize to large objects even on nearly balanced datasets. We argue that the cause of failure is the sensitivity of depth regression losses to noise of larger objects. To bridge this gap, we comprehensively investigate regression and dice losses, examining their robustness under varying error levels and object sizes. We mathematically prove that the dice loss leads to superior noise-robustness and model convergence for large objects compared to regression losses for a simplified case. Leveraging our theoretical insights, we propose SeaBird (Segmentation in Bird's View) as the first step towards generalizing to large objects. SeaBird effectively integrates BEV segmentation on foreground objects for 3D detection, with the segmentation head trained with the dice loss. SeaBird achieves SoTA results on the KITTI-360 leaderboard and improves existing detectors on the nuScenes leaderboard, particularly for large objects. Code and models at https://github.com/abhi1kumar/SeaBird</li>
</ul>

<h3>Title: MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Agiza, Marina Neseem, Sherief Reda</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20320">https://arxiv.org/abs/2403.20320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20320">https://arxiv.org/pdf/2403.20320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20320]] MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning(https://arxiv.org/abs/2403.20320)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Adapting models pre-trained on large-scale datasets to a variety of downstream tasks is a common strategy in deep learning. Consequently, parameter-efficient fine-tuning methods have emerged as a promising way to adapt pre-trained models to different tasks while training only a minimal number of parameters. While most of these methods are designed for single-task adaptation, parameter-efficient training in Multi-Task Learning (MTL) architectures is still unexplored. In this paper, we introduce MTLoRA, a novel framework for parameter-efficient training of MTL models. MTLoRA employs Task-Agnostic and Task-Specific Low-Rank Adaptation modules, which effectively disentangle the parameter space in MTL fine-tuning, thereby enabling the model to adeptly handle both task specialization and interaction within MTL contexts. We applied MTLoRA to hierarchical-transformer-based MTL architectures, adapting them to multiple downstream dense prediction tasks. Our extensive experiments on the PASCAL dataset show that MTLoRA achieves higher accuracy on downstream tasks compared to fully fine-tuning the MTL model while reducing the number of trainable parameters by 3.6x. Furthermore, MTLoRA establishes a Pareto-optimal trade-off between the number of trainable parameters and the accuracy of the downstream tasks, outperforming current state-of-the-art parameter-efficient training methods in both accuracy and efficiency. Our code is publicly available.</li>
</ul>

<h3>Title: Localising the Seizure Onset Zone from Single-Pulse Electrical  Stimulation Responses with a Transformer</h3>
<ul>
<li><strong>Authors: </strong>Jamie Norris, Aswin Chari, Gerald Cooray, Martin Tisdall, Karl Friston, Richard Rosch</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20324">https://arxiv.org/abs/2403.20324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20324">https://arxiv.org/pdf/2403.20324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20324]] Localising the Seizure Onset Zone from Single-Pulse Electrical  Stimulation Responses with a Transformer(https://arxiv.org/abs/2403.20324)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Epilepsy is one of the most common neurological disorders, and many patients require surgical intervention when medication fails to control seizures. For effective surgical outcomes, precise localisation of the epileptogenic focus - often approximated through the Seizure Onset Zone (SOZ) - is critical yet remains a challenge. Active probing through electrical stimulation is already standard clinical practice for identifying epileptogenic areas. This paper advances the application of deep learning for SOZ localisation using Single Pulse Electrical Stimulation (SPES) responses. We achieve this by introducing Transformer models that incorporate cross-channel attention. We evaluate these models on held-out patient test sets to assess their generalisability to unseen patients and electrode placements. Our study makes three key contributions: Firstly, we implement an existing deep learning model to compare two SPES analysis paradigms - namely, divergent and convergent. These paradigms evaluate outward and inward effective connections, respectively. Our findings reveal a notable improvement in moving from a divergent (AUROC: 0.574) to a convergent approach (AUROC: 0.666), marking the first application of the latter in this context. Secondly, we demonstrate the efficacy of the Transformer models in handling heterogeneous electrode placements, increasing the AUROC to 0.730. Lastly, by incorporating inter-trial variability, we further refine the Transformer models, with an AUROC of 0.745, yielding more consistent predictions across patients. These advancements provide a deeper insight into SOZ localisation and represent a significant step in modelling patient-specific intracranial EEG electrode placements in SPES. Future work will explore integrating these models into clinical decision-making processes to bridge the gap between deep learning research and practical healthcare applications.</li>
</ul>

<h3>Title: Gecko: Versatile Text Embeddings Distilled from Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R. Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher Karthik Duddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, Aditya Kusupati, Prateek Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang, Iftekhar Naim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20327">https://arxiv.org/abs/2403.20327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20327">https://arxiv.org/pdf/2403.20327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20327]] Gecko: Versatile Text Embeddings Distilled from Large Language Models(https://arxiv.org/abs/2403.20327)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present Gecko, a compact and versatile text embedding model. Gecko achieves strong retrieval performance by leveraging a key idea: distilling knowledge from large language models (LLMs) into a retriever. Our two-step distillation process begins with generating diverse, synthetic paired data using an LLM. Next, we further refine the data quality by retrieving a set of candidate passages for each query, and relabeling the positive and hard negative passages using the same LLM. The effectiveness of our approach is demonstrated by the compactness of the Gecko. On the Massive Text Embedding Benchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existing entries with 768 embedding size. Gecko with 768 embedding dimensions achieves an average score of 66.31, competing with 7x larger models and 5x higher dimensional embeddings.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
