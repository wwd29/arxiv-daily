<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-06</h1>
<h3>Title: Hybrid Horizons: Policy for Post-Quantum Security</h3>
<ul>
<li><strong>Authors: </strong>Anais Jaikissoon</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02317">https://arxiv.org/abs/2510.02317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02317">https://arxiv.org/pdf/2510.02317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02317]] Hybrid Horizons: Policy for Post-Quantum Security(https://arxiv.org/abs/2510.02317)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The Age of Artificial Intelligence is here. In 2025, there are few regulations governing artificial intelligence. While the expansion of artificial intelligence is going in a relatively good direction, there is a risk that it can be misused. Misuse of technology is nothing new and will continue to happen. The lack of regulation in artificial intelligence is necessary because it raises the question of how we can move forward without knowing what the limits are. While artificial intelligence dominates the technology industry, new technology is starting to emerge. Quantum cryptography is expected to replace classical cryptography; however, the transition from classical to quantum cryptography is expected to occur within the next 10 years. The ability to transition from classical to quantum cryptography requires hybrid cryptography. Hybrid cryptography can be used now; however, similar to artificial intelligence, there is no regulation or support for the regulatory infrastructure regarding hybrid machines. This paper will explore the regulatory gaps in hybrid cryptography. The paper will also offer solutions to fix the gaps and ensure the transition from classical to quantum cryptography is safely and effectively completed.</li>
</ul>

<h3>Title: Modeling the Attack: Detecting AI-Generated Text by Quantifying Adversarial Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Lekkala Sai Teja, Annepaka Yadagiri, Sangam Sai Anish, Siva Gopala Krishna Nuthakki, Partha Pakray</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02319">https://arxiv.org/abs/2510.02319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02319">https://arxiv.org/pdf/2510.02319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02319]] Modeling the Attack: Detecting AI-Generated Text by Quantifying Adversarial Perturbations(https://arxiv.org/abs/2510.02319)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>The growth of highly advanced Large Language Models (LLMs) constitutes a huge dual-use problem, making it necessary to create dependable AI-generated text detection systems. Modern detectors are notoriously vulnerable to adversarial attacks, with paraphrasing standing out as an effective evasion technique that foils statistical detection. This paper presents a comparative study of adversarial robustness, first by quantifying the limitations of standard adversarial training and then by introducing a novel, significantly more resilient detection framework: Perturbation-Invariant Feature Engineering (PIFE), a framework that enhances detection by first transforming input text into a standardized form using a multi-stage normalization pipeline, it then quantifies the transformation's magnitude using metrics like Levenshtein distance and semantic similarity, feeding these signals directly to the classifier. We evaluate both a conventionally hardened Transformer and our PIFE-augmented model against a hierarchical taxonomy of character-, word-, and sentence-level attacks. Our findings first confirm that conventional adversarial training, while resilient to syntactic noise, fails against semantic attacks, an effect we term "semantic evasion threshold", where its True Positive Rate at a strict 1% False Positive Rate plummets to 48.8%. In stark contrast, our PIFE model, which explicitly engineers features from the discrepancy between a text and its canonical form, overcomes this limitation. It maintains a remarkable 82.6% TPR under the same conditions, effectively neutralizing the most sophisticated semantic attacks. This superior performance demonstrates that explicitly modeling perturbation artifacts, rather than merely training on them, is a more promising path toward achieving genuine robustness in the adversarial arms race.</li>
</ul>

<h3>Title: Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning</h3>
<ul>
<li><strong>Authors: </strong>Wannan Yang, Xinchi Qiu, Lei Yu, Yuchen Zhang, Oliver Aobo Yang, Narine Kokhlikyan, Nicola Cancedda, Diego Garcia-Olano</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02324">https://arxiv.org/abs/2510.02324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02324">https://arxiv.org/pdf/2510.02324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02324]] Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning(https://arxiv.org/abs/2510.02324)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit impressive capabilities but often hallucinate, confidently providing incorrect answers instead of admitting ignorance. Prior work has shown that models encode linear representations of their own knowledge and that activation steering can reduce hallucinations. These approaches, however, require real-time monitoring and intervention during inference. We introduce Contrastive Activation Steering for Amortized Learning (CASAL), an efficient algorithm that connects interpretability with amortized optimization. CASAL directly bakes the benefits of activation steering into model's weights. Once trained, LLMs answer questions they know while abstaining from answering those they do not. CASAL's light-weight design requires training only a submodule of a single transformer layer and yet reduces hallucination by 30%-40% across multiple short-form QA benchmarks. CASAL is 30x more compute-efficient and 20x more data-efficient than strong LoRA-based baselines such as SFT and DPO, boosting its practical applicability in data scarce domains. Importantly, CASAL also generalizes effectively to out-of-distribution (OOD) domains. We showcase CASAL's flexibility in mitigating hallucinations in both text-only and vision-language models. To our knowledge, CASAL is the first steering-based training method that has been shown to be effective for both dense and Mixture-of-Experts (MoE) models. CASAL represents a promising step forward for applying interpretability-inspired method for practical deployment in production systems.</li>
</ul>

<h3>Title: Agentic-AI Healthcare: Multilingual, Privacy-First Framework with MCP Agents</h3>
<ul>
<li><strong>Authors: </strong>Mohammed A. Shehab</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02325">https://arxiv.org/abs/2510.02325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02325">https://arxiv.org/pdf/2510.02325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02325]] Agentic-AI Healthcare: Multilingual, Privacy-First Framework with MCP Agents(https://arxiv.org/abs/2510.02325)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces Agentic-AI Healthcare, a privacy-aware, multilingual, and explainable research prototype developed as a single-investigator project. The system leverages the emerging Model Context Protocol (MCP) to orchestrate multiple intelligent agents for patient interaction, including symptom checking, medication suggestions, and appointment scheduling. The platform integrates a dedicated Privacy and Compliance Layer that applies role-based access control (RBAC), AES-GCM field-level encryption, and tamper-evident audit logging, aligning with major healthcare data protection standards such as HIPAA (US), PIPEDA (Canada), and PHIPA (Ontario). Example use cases demonstrate multilingual patient-doctor interaction (English, French, Arabic) and transparent diagnostic reasoning powered by large language models. As an applied AI contribution, this work highlights the feasibility of combining agentic orchestration, multilingual accessibility, and compliance-aware architecture in healthcare applications. This platform is presented as a research prototype and is not a certified medical device.</li>
</ul>

<h3>Title: Hallucination-Resistant, Domain-Specific Research Assistant with Self-Evaluation and Vector-Grounded Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Vivek Bhavsar, Joseph Ereifej, Aravanan Gurusami</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02326">https://arxiv.org/abs/2510.02326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02326">https://arxiv.org/pdf/2510.02326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02326]] Hallucination-Resistant, Domain-Specific Research Assistant with Self-Evaluation and Vector-Grounded Retrieval(https://arxiv.org/abs/2510.02326)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large language models accelerate literature synthesis but can hallucinate and mis-cite, limiting their usefulness in expert workflows. We present RA-FSM (Research Assistant - Finite State Machine), a modular GPT-based research assistant that wraps generation in a finite-state control loop: Relevance -> Confidence -> Knowledge. The system is grounded in vector retrieval and a deterministic citation pipeline. The controller filters out-of-scope queries, scores answerability, decomposes questions, and triggers retrieval only when needed, and emits answers with confidence labels and in-corpus, de-duplicated references. A ranked-tier ingestion workflow constructs a domain knowledge base from journals, conferences, indices, preprints, and patents, writing both to a dense vector index and to a relational store of normalized metrics. We implement the system for photonics and evaluate it on six task categories: analytical reasoning, numerical analysis, methodological critique, comparative synthesis, factual extraction, and application design. In blinded A/B reviews, domain experts prefer RA-FSM to both a strong Notebook LM (NLM) and a vanilla Default GPT API call single-pass baseline, citing stronger boundary-condition handling and more defensible evidence use. Coverage and novelty analyses indicate that RA-FSM explores beyond the NLM while incurring tunable latency and cost overheads. The design emphasizes transparent, well-cited answers for high-stakes technical work and is generalizable to other scientific domains.</li>
</ul>

<h3>Title: KAME: Tandem Architecture for Enhancing Knowledge in Real-Time Speech-to-Speech Conversational AI</h3>
<ul>
<li><strong>Authors: </strong>So Kuroki, Yotaro Kubo, Takuya Akiba, Yujin Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02327">https://arxiv.org/abs/2510.02327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02327">https://arxiv.org/pdf/2510.02327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02327]] KAME: Tandem Architecture for Enhancing Knowledge in Real-Time Speech-to-Speech Conversational AI(https://arxiv.org/abs/2510.02327)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Real-time speech-to-speech (S2S) models excel at generating natural, low-latency conversational responses but often lack deep knowledge and semantic understanding. Conversely, cascaded systems combining automatic speech recognition, a text-based Large Language Model (LLM), and text-to-speech synthesis offer superior knowledge representation at the cost of high latency, which disrupts the flow of natural interaction. This paper introduces a novel hybrid architecture that bridges the gap between these two paradigms. Our framework processes user speech through an S2S transformer for immediate responsiveness while concurrently relaying the query to a powerful back-end LLM. The LLM's text-based response is then injected in real time to guide the S2S model's speech generation, effectively infusing its output with rich knowledge without the full latency penalty of a cascaded system. We evaluated our method using a speech-synthesized variant of the MT-Bench benchmark that consists of multi-turn question-answering sessions. The results demonstrate that our system substantially outperforms a baseline S2S model in response correctness, approaching that of a cascaded system, while maintaining a latency on par with the baseline.</li>
</ul>

<h3>Title: AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Ziqing Wang, Chengsheng Mao, Xiaole Wen, Yuan Luo, Kaize Ding</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02328">https://arxiv.org/abs/2510.02328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02328">https://arxiv.org/pdf/2510.02328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02328]] AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering(https://arxiv.org/abs/2510.02328)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise in medical visual question answering (Med-VQA). However, when deployed in low-resource settings where abundant labeled data are unavailable, existing Med-MLLMs commonly fail due to their medical reasoning capability bottlenecks: (i) the intrinsic reasoning bottleneck that ignores the details from the medical image; (ii) the extrinsic reasoning bottleneck that fails to incorporate specialized medical knowledge. To address those limitations, we propose AMANDA, a training-free agentic framework that performs medical knowledge augmentation via LLM agents. Specifically, our intrinsic medical knowledge augmentation focuses on coarse-to-fine question decomposition for comprehensive diagnosis, while extrinsic medical knowledge augmentation grounds the reasoning process via biomedical knowledge graph retrieval. Extensive experiments across eight Med-VQA benchmarks demonstrate substantial improvements in both zero-shot and few-shot Med-VQA settings. The code is available at this https URL.</li>
</ul>

<h3>Title: A High-Capacity and Secure Disambiguation Algorithm for Neural Linguistic Steganography</h3>
<ul>
<li><strong>Authors: </strong>Yapei Feng, Feng Jiang, Shanhao Wu, Hua Zhong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02332">https://arxiv.org/abs/2510.02332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02332">https://arxiv.org/pdf/2510.02332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02332]] A High-Capacity and Secure Disambiguation Algorithm for Neural Linguistic Steganography(https://arxiv.org/abs/2510.02332)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Neural linguistic steganography aims to embed information into natural text while preserving statistical undetectability. A fundamental challenge in this ffeld stems from tokenization ambiguity in modern tokenizers, which can lead to catastrophic decoding failures. The recent method, SyncPool, addresses this ambiguity by employing a coarse-grained synchronization mechanism over groups of ambiguous candidates. However, SyncPool sacriffces embedding capacity, as it utilizes the entire Shannon entropy of an ambiguous group solely for synchronization rather than for payload embedding. We propose a method named look-ahead Sync, which overcomes the capacity limitation of SyncPool while retaining its provable security guarantees. Our approach performs minimal synchronized sampling only on truly indistinguishable token sequences, while strategically preserving all other discernible paths to maximize embedding capacity. We provide theoretical proofs for the security of our method and analyze the gap between its achievable embedding capacity and the theoretical upper bound. Experiments on English (using Llama 3) and Chinese (using Qwen 2.5) benchmarks show that our method consistently approaches the theoretical capacity upper bound and signiffcantly outperforms SyncPool. The improvement in embedding rate exceeds 160% in English and 25% in Chinese, particularly in settings with larger candidate pools. This work represents a signiffcant step toward practical high-capacity provably secure linguistic steganography.</li>
</ul>

<h3>Title: Human Mobility Datasets Enriched With Contextual and Social Dimensions</h3>
<ul>
<li><strong>Authors: </strong>Chiara Pugliese, Francesco Lettich, Guido Rocchietti, Chiara Renso, Fabio Pinelli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02333">https://arxiv.org/abs/2510.02333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02333">https://arxiv.org/pdf/2510.02333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02333]] Human Mobility Datasets Enriched With Contextual and Social Dimensions(https://arxiv.org/abs/2510.02333)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>In this resource paper, we present two publicly available datasets of semantically enriched human trajectories, together with the pipeline to build them. The trajectories are publicly available GPS traces retrieved from OpenStreetMap. Each dataset includes contextual layers such as stops, moves, points of interest (POIs), inferred transportation modes, and weather data. A novel semantic feature is the inclusion of synthetic, realistic social media posts generated by Large Language Models (LLMs), enabling multimodal and semantic mobility analysis. The datasets are available in both tabular and Resource Description Framework (RDF) formats, supporting semantic reasoning and FAIR data practices. They cover two structurally distinct, large cities: Paris and New York. Our open source reproducible pipeline allows for dataset customization, while the datasets support research tasks such as behavior modeling, mobility prediction, knowledge graph construction, and LLM-based applications. To our knowledge, our resource is the first to combine real-world movement, structured semantic enrichment, LLM-generated text, and semantic web compatibility in a reusable framework.</li>
</ul>

<h3>Title: Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing</h3>
<ul>
<li><strong>Authors: </strong>Zhe Li, Wei Zhao, Yige Li, Jun Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02334">https://arxiv.org/abs/2510.02334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02334">https://arxiv.org/pdf/2510.02334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02334]] Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing(https://arxiv.org/abs/2510.02334)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their deployment is frequently undermined by undesirable behaviors such as generating harmful content, factual inaccuracies, and societal biases. Diagnosing the root causes of these failures poses a critical challenge for AI safety. Existing attribution methods, particularly those based on parameter gradients, often fall short due to prohibitive noisy signals and computational complexity. In this work, we introduce a novel and efficient framework that diagnoses a range of undesirable LLM behaviors by analyzing representation and its gradients, which operates directly in the model's activation space to provide a semantically meaningful signal linking outputs to their training data. We systematically evaluate our method for tasks that include tracking harmful content, detecting backdoor poisoning, and identifying knowledge contamination. The results demonstrate that our approach not only excels at sample-level attribution but also enables fine-grained token-level analysis, precisely identifying the specific samples and phrases that causally influence model behavior. This work provides a powerful diagnostic tool to understand, audit, and ultimately mitigate the risks associated with LLMs. The code is available at this https URL.</li>
</ul>

<h3>Title: FormalML: A Benchmark for Evaluating Formal Subgoal Completion in Machine Learning Theory</h3>
<ul>
<li><strong>Authors: </strong>Xiao-Wen Yang, Zihao Zhang, Jianuo Cao, Zhi Zhou, Zenan Li, Lan-Zhe Guo, Yuan Yao, Taolue Chen, Yu-Feng Li, Xiaoxing Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02335">https://arxiv.org/abs/2510.02335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02335">https://arxiv.org/pdf/2510.02335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02335]] FormalML: A Benchmark for Evaluating Formal Subgoal Completion in Machine Learning Theory(https://arxiv.org/abs/2510.02335)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently demonstrated remarkable progress in formal theorem proving. Yet their ability to serve as practical assistants for mathematicians, filling in missing steps within complex proofs, remains underexplored. We identify this challenge as the task of subgoal completion, where an LLM must discharge short but nontrivial proof obligations left unresolved in a human-provided sketch. To study this problem, we introduce FormalML, a Lean 4 benchmark built from foundational theories of machine learning. Using a translation tactic that converts procedural proofs into declarative form, we extract 4937 problems spanning optimization and probability inequalities, with varying levels of difficulty. FormalML is the first subgoal completion benchmark to combine premise retrieval and complex research-level contexts. Evaluation of state-of-the-art provers highlights persistent limitations in accuracy and efficiency, underscoring the need for more capable LLM-based theorem provers for effective subgoal completion,</li>
</ul>

<h3>Title: Optimizing Long-Form Clinical Text Generation with Claim-Based Rewards</h3>
<ul>
<li><strong>Authors: </strong>Samyak Jhaveri, Praphul Singh, Jangwon Kim, Tara Taghavi, Krishnaram Kenthapadi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02338">https://arxiv.org/abs/2510.02338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02338">https://arxiv.org/pdf/2510.02338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02338]] Optimizing Long-Form Clinical Text Generation with Claim-Based Rewards(https://arxiv.org/abs/2510.02338)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automating clinical documentation with large language models requires precise alignment with priorities such as completeness and factual grounding. We present an evaluation-integrated reinforcement learning framework for long-form clinical text generation that couples Group Relative Policy Optimization (GRPO) with DocLens, a claim-level evaluator that provides deterministic, dialogue-grounded rewards. Our method directly optimizes factual grounding and completeness without training a separate reward model or relying on human-authored references. Empirically, the approach improves clinical note quality and reduces training cost via a simple reward-gating strategy. An independent GPT-5 qualitative evaluation further supports these gains, showing higher preference for GRPO outputs in factuality, completeness, and brevity, with fewer omissions and hallucinations. Because the benchmarks are relatively clean and the base model already well aligned, these improvements likely represent a conservative lower bound. The framework is scalable to real-world settings and can incorporate custom objectives such as guideline adherence or billing preferences.</li>
</ul>

<h3>Title: Evaluating Uncertainty Quantification Methods in Argumentative Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kevin Zhou, Adam Dejl, Gabriel Freedman, Lihu Chen, Antonio Rago, Francesca Toni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02339">https://arxiv.org/abs/2510.02339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02339">https://arxiv.org/pdf/2510.02339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02339]] Evaluating Uncertainty Quantification Methods in Argumentative Large Language Models(https://arxiv.org/abs/2510.02339)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Research in uncertainty quantification (UQ) for large language models (LLMs) is increasingly important towards guaranteeing the reliability of this groundbreaking technology. We explore the integration of LLM UQ methods in argumentative LLMs (ArgLLMs), an explainable LLM framework for decision-making based on computational argumentation in which UQ plays a critical role. We conduct experiments to evaluate ArgLLMs' performance on claim verification tasks when using different LLM UQ methods, inherently performing an assessment of the UQ methods' effectiveness. Moreover, the experimental procedure itself is a novel way of evaluating the effectiveness of UQ methods, especially when intricate and potentially contentious statements are present. Our results demonstrate that, despite its simplicity, direct prompting is an effective UQ strategy in ArgLLMs, outperforming considerably more complex approaches.</li>
</ul>

<h3>Title: Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs</h3>
<ul>
<li><strong>Authors: </strong>Xin Gao, Ruiyi Zhang, Daniel Du, Saurabh Mahindre, Sai Ashish Somayajula, Pengtao Xie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02340">https://arxiv.org/abs/2510.02340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02340">https://arxiv.org/pdf/2510.02340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02340]] Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs(https://arxiv.org/abs/2510.02340)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are widely used for temporal prediction, but their reliance on pretraining data raises contamination concerns, as accurate predictions on pre-cutoff test data may reflect memorization rather than reasoning, leading to an overestimation of their generalization capability. With the recent emergence of prompting-based unlearning techniques, a natural question arises: Can LLMs be prompted to simulate an earlier knowledge cutoff? In this work, we investigate the capability of prompting to simulate earlier knowledge cutoff in LLMs. We construct three evaluation datasets to assess the extent to which LLMs can forget (1) direct factual knowledge, (2) semantic shifts, and (3) causally related knowledge. Results demonstrate that while prompt-based simulated knowledge cutoffs show effectiveness when directly queried with the information after that date, they struggle to induce forgetting when the forgotten content is not directly asked but causally related to the query. These findings highlight the need for more rigorous evaluation settings when applying LLMs for temporal prediction tasks. The full dataset and evaluation code are available at this https URL.</li>
</ul>

<h3>Title: DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning</h3>
<ul>
<li><strong>Authors: </strong>Yifan Wang, Bolian Li, Junlin Wu, Zhaoxuan Tan, Zheli Liu, Ruqi Zhang, Ananth Grama, Qingkai Zeng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02341">https://arxiv.org/abs/2510.02341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02341">https://arxiv.org/pdf/2510.02341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02341]] DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning(https://arxiv.org/abs/2510.02341)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Real-world large language model deployments (e.g., conversational AI systems, code generation assistants) naturally generate abundant implicit user dissatisfaction (DSAT) signals, as users iterate toward better answers through refinements, corrections, and expressed preferences, while explicit satisfaction (SAT) feedback is scarce. Existing preference learning approaches are poorly aligned with this data profile, as they rely on costly human annotations or assume plentiful positive responses. In this paper, we introduce \textbf{DRIFT} (\textbf{D}issatisfaction-\textbf{R}efined \textbf{I}terative pre\textbf{F}erence \textbf{T}raining), which anchors training on real-world DSAT signals and samples positives dynamically from the evolving policy. Empirically, DRIFT models trained on real-world \textit{WildFeedback} datasets and synthetic \textit{UltraFeedback} datasets achieve up to +6.23\% (7B) / +7.61\% (14B) on WildBench Task Score and up to +8.95\% (7B) / +12.29\% (14B) on AlpacaEval2 win rate over base models, outperforming strong baseline methods such as iterative DPO and SPIN. At larger scales, the improvements are particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on WildBench. Further analysis shows that DRIFT also preserves exploratory capacity, yielding more diverse high-reward solutions rather than collapsing to narrow subsets. Theoretically, we demonstrate that this design preserves preference margins and avoids the gradient degeneration. These results show that DRIFT is an effective and scalable recipe for real-world post-training that leverages the most abundant and informative signal. The code and data are available at this https URL.</li>
</ul>

<h3>Title: CATMark: A Context-Aware Thresholding Framework for Robust Cross-Task Watermarking in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yu Zhang, Shuliang Liu, Xu Yang, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02342">https://arxiv.org/abs/2510.02342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02342">https://arxiv.org/pdf/2510.02342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02342]] CATMark: A Context-Aware Thresholding Framework for Robust Cross-Task Watermarking in Large Language Models(https://arxiv.org/abs/2510.02342)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Watermarking algorithms for Large Language Models (LLMs) effectively identify machine-generated content by embedding and detecting hidden statistical features in text. However, such embedding leads to a decline in text quality, especially in low-entropy scenarios where performance needs improvement. Existing methods that rely on entropy thresholds often require significant computational resources for tuning and demonstrate poor adaptability to unknown or cross-task generation scenarios. We propose \textbf{C}ontext-\textbf{A}ware \textbf{T}hreshold watermarking ($\myalgo$), a novel framework that dynamically adjusts watermarking intensity based on real-time semantic context. $\myalgo$ partitions text generation into semantic states using logits clustering, establishing context-aware entropy thresholds that preserve fidelity in structured content while embedding robust watermarks. Crucially, it requires no pre-defined thresholds or task-specific tuning. Experiments show $\myalgo$ improves text quality in cross-tasks without sacrificing detection accuracy.</li>
</ul>

<h3>Title: $\texttt{BluePrint}$: A Social Media User Dataset for LLM Persona Evaluation and Training</h3>
<ul>
<li><strong>Authors: </strong>Aurélien Bück-Kaeffer, Je Qin Chooi, Dan Zhao, Maximilian Puelma Touzel, Kellin Pelrine, Jean-François Godbout, Reihaneh Rabbany, Zachary Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02343">https://arxiv.org/abs/2510.02343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02343">https://arxiv.org/pdf/2510.02343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02343]] $\texttt{BluePrint}$: A Social Media User Dataset for LLM Persona Evaluation and Training(https://arxiv.org/abs/2510.02343)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) offer promising capabilities for simulating social media dynamics at scale, enabling studies that would be ethically or logistically challenging with human subjects. However, the field lacks standardized data resources for fine-tuning and evaluating LLMs as realistic social media agents. We address this gap by introducing SIMPACT, the SIMulation-oriented Persona and Action Capture Toolkit, a privacy respecting framework for constructing behaviorally-grounded social media datasets suitable for training agent models. We formulate next-action prediction as a task for training and evaluating LLM-based agents and introduce metrics at both the cluster and population levels to assess behavioral fidelity and stylistic realism. As a concrete implementation, we release BluePrint, a large-scale dataset built from public Bluesky data focused on political discourse. BluePrint clusters anonymized users into personas of aggregated behaviours, capturing authentic engagement patterns while safeguarding privacy through pseudonymization and removal of personally identifiable information. The dataset includes a sizable action set of 12 social media interaction types (likes, replies, reposts, etc.), each instance tied to the posting activity preceding it. This supports the development of agents that use context-dependence, not only in the language, but also in the interaction behaviours of social media to model social media users. By standardizing data and evaluation protocols, SIMPACT provides a foundation for advancing rigorous, ethically responsible social media simulations. BluePrint serves as both an evaluation benchmark for political discourse modeling and a template for building domain specific datasets to study challenges such as misinformation and polarization.</li>
</ul>

<h3>Title: Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression</h3>
<ul>
<li><strong>Authors: </strong>Peijun Zhu, Ning Yang, Jiayu Wei, Jinghang Wu, Haijun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DC, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02345">https://arxiv.org/abs/2510.02345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02345">https://arxiv.org/pdf/2510.02345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02345]] Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression(https://arxiv.org/abs/2510.02345)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load imbalance, parameter redundancy, and communication overhead. We introduce a unified framework based on dynamic expert clustering and structured compression to address these issues cohesively. Our method employs an online clustering procedure that periodically regroups experts using a fused metric of parameter and activation similarity, which stabilizes expert utilization. To our knowledge, this is one of the first frameworks to leverage the semantic embedding capability of the router to dynamically reconfigure the model's architecture during training for substantial efficiency gains. Within each cluster, we decompose expert weights into a shared base matrix and extremely low-rank residual adapters, achieving up to fivefold parameter reduction per group while preserving specialization. This structure enables a two-stage hierarchical routing strategy: tokens are first assigned to a cluster, then to specific experts within it, drastically reducing the routing search space and the volume of all-to-all communication. Furthermore, a heterogeneous precision scheme, which stores shared bases in FP16 and residual factors in INT4, coupled with dynamic offloading of inactive clusters, reduces peak memory consumption to levels comparable to dense models. Evaluated on GLUE and WikiText-103, our framework matches the quality of standard MoE models while reducing total parameters by approximately 80%, improving throughput by 10% to 20%, and lowering expert load variance by a factor of over three. Our work demonstrates that structural reorganization is a principled path toward scalable, efficient, and memory-effective MoE LLMs.</li>
</ul>

<h3>Title: Small Language Models for Curriculum-based Guidance</h3>
<ul>
<li><strong>Authors: </strong>Konstantinos Katharakis, Sippo Rossi, Raghava Rao Mukkamala</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02347">https://arxiv.org/abs/2510.02347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02347">https://arxiv.org/pdf/2510.02347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02347]] Small Language Models for Curriculum-based Guidance(https://arxiv.org/abs/2510.02347)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative, large language model</a></li>
<li><strong>Abstract: </strong>The adoption of generative AI and large language models (LLMs) in education is still emerging. In this study, we explore the development and evaluation of AI teaching assistants that provide curriculum-based guidance using a retrieval-augmented generation (RAG) pipeline applied to selected open-source small language models (SLMs). We benchmarked eight SLMs, including LLaMA 3.1, IBM Granite 3.3, and Gemma 3 (7-17B parameters), against GPT-4o. Our findings show that with proper prompting and targeted retrieval, SLMs can match LLMs in delivering accurate, pedagogically aligned responses. Importantly, SLMs offer significant sustainability benefits due to their lower computational and energy requirements, enabling real-time use on consumer-grade hardware without depending on cloud infrastructure. This makes them not only cost-effective and privacy-preserving but also environmentally responsible, positioning them as viable AI teaching assistants for educational institutions aiming to scale personalized learning in a sustainable and energy-efficient manner.</li>
</ul>

<h3>Title: mini-vec2vec: Scaling Universal Geometry Alignment with Linear Transformations</h3>
<ul>
<li><strong>Authors: </strong>Guy Dar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02348">https://arxiv.org/abs/2510.02348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02348">https://arxiv.org/pdf/2510.02348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02348]] mini-vec2vec: Scaling Universal Geometry Alignment with Linear Transformations(https://arxiv.org/abs/2510.02348)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We build upon vec2vec, a procedure designed to align text embedding spaces without parallel data. vec2vec finds a near-perfect alignment, but it is expensive and unstable. We present mini-vec2vec, a simple and efficient alternative that requires substantially lower computational cost and is highly robust. Moreover, the learned mapping is a linear transformation. Our method consists of three main stages: a tentative matching of pseudo-parallel embedding vectors, transformation fitting, and iterative refinement. Our linear alternative exceeds the original instantiation of vec2vec by orders of magnitude in efficiency, while matching or exceeding their results. The method's stability and interpretable algorithmic steps facilitate scaling and unlock new opportunities for adoption in new domains and fields.</li>
</ul>

<h3>Title: An Investigation into the Performance of Non-Contrastive Self-Supervised Learning Methods for Network Intrusion Detection</h3>
<ul>
<li><strong>Authors: </strong>Hamed Fard, Tobias Schalau, Gerhard Wunder</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02349">https://arxiv.org/abs/2510.02349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02349">https://arxiv.org/pdf/2510.02349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02349]] An Investigation into the Performance of Non-Contrastive Self-Supervised Learning Methods for Network Intrusion Detection(https://arxiv.org/abs/2510.02349)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Network intrusion detection, a well-explored cybersecurity field, has predominantly relied on supervised learning algorithms in the past two decades. However, their limitations in detecting only known anomalies prompt the exploration of alternative approaches. Motivated by the success of self-supervised learning in computer vision, there is a rising interest in adapting this paradigm for network intrusion detection. While prior research mainly delved into contrastive self-supervised methods, the efficacy of non-contrastive methods, in conjunction with encoder architectures serving as the representation learning backbone and augmentation strategies that determine what is learned, remains unclear for effective attack detection. This paper compares the performance of five non-contrastive self-supervised learning methods using three encoder architectures and six augmentation strategies. Ninety experiments are systematically conducted on two network intrusion detection datasets, UNSW-NB15 and 5G-NIDD. For each self-supervised model, the combination of encoder architecture and augmentation method yielding the highest average precision, recall, F1-score, and AUCROC is reported. Furthermore, by comparing the best-performing models to two unsupervised baselines, DeepSVDD, and an Autoencoder, we showcase the competitiveness of the non-contrastive methods for attack detection. Code at: this https URL</li>
</ul>

<h3>Title: LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL</h3>
<ul>
<li><strong>Authors: </strong>Dzmitry Pihulski, Karol Charchut, Viktoria Novogrodskaia, Jan Kocoń</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02350">https://arxiv.org/abs/2510.02350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02350">https://arxiv.org/pdf/2510.02350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02350]] LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL(https://arxiv.org/abs/2510.02350)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Converting natural language questions into SQL queries (Text-to-SQL) enables non-expert users to interact with relational databases and has long been a central task for natural language interfaces to data. While the WikiSQL dataset played a key role in early NL2SQL research, its usage has declined due to structural and annotation issues, including case sensitivity inconsistencies, data type mismatches, syntax errors, and unanswered questions. We present LLMSQL, a systematic revision and transformation of WikiSQL designed for the LLM era. We classify these errors and implement automated methods for cleaning and re-annotation. To assess the impact of these improvements, we evaluated multiple large language models (LLMs), including Gemma 3, LLaMA 3.2, Mistral 7B, gpt-oss 20B, Phi-3.5 Mini, Qwen 2.5, OpenAI o4-mini, DeepSeek R1 and others. Rather than serving as an update, LLMSQL is introduced as an LLM-ready benchmark: unlike the original WikiSQL, tailored for pointer-network models selecting tokens from input, LLMSQL provides clean natural language questions and full SQL queries as plain text, enabling straightforward generation and evaluation for modern natural language-to-SQL models.</li>
</ul>

<h3>Title: Language, Culture, and Ideology: Personalizing Offensiveness Detection in Political Tweets with Reasoning LLMs</h3>
<ul>
<li><strong>Authors: </strong>Dzmitry Pihulski, Jan Kocoń</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02351">https://arxiv.org/abs/2510.02351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02351">https://arxiv.org/pdf/2510.02351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02351]] Language, Culture, and Ideology: Personalizing Offensiveness Detection in Political Tweets with Reasoning LLMs(https://arxiv.org/abs/2510.02351)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>We explore how large language models (LLMs) assess offensiveness in political discourse when prompted to adopt specific political and cultural perspectives. Using a multilingual subset of the MD-Agreement dataset centered on tweets from the 2020 US elections, we evaluate several recent LLMs - including DeepSeek-R1, o4-mini, GPT-4.1-mini, Qwen3, Gemma, and Mistral - tasked with judging tweets as offensive or non-offensive from the viewpoints of varied political personas (far-right, conservative, centrist, progressive) across English, Polish, and Russian contexts. Our results show that larger models with explicit reasoning abilities (e.g., DeepSeek-R1, o4-mini) are more consistent and sensitive to ideological and cultural variation, while smaller models often fail to capture subtle distinctions. We find that reasoning capabilities significantly improve both the personalization and interpretability of offensiveness judgments, suggesting that such mechanisms are key to adapting LLMs for nuanced sociopolitical text classification across languages and ideologies.</li>
</ul>

<h3>Title: Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations</h3>
<ul>
<li><strong>Authors: </strong>Yihao Wu, Tianrui Wang, Yizhou Peng, Yi-Wen Chao, Xuyi Zhuang, Xinsheng Wang, Shunshun Yin, Ziyang Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02352">https://arxiv.org/abs/2510.02352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02352">https://arxiv.org/pdf/2510.02352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02352]] Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations(https://arxiv.org/abs/2510.02352)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>While biases in large language models (LLMs), such as stereotypes and cultural tendencies in outputs, have been examined and identified, their presence and characteristics in spoken dialogue models (SDMs) with audio input and output remain largely unexplored. Paralinguistic features, such as age, gender, and accent, can affect model outputs; when compounded by multi-turn conversations, these effects may exacerbate biases, with potential implications for fairness in decision-making and recommendation tasks. In this paper, we systematically evaluate biases in speech LLMs and study the impact of multi-turn dialogues with repeated negative feedback. Bias is measured using Group Unfairness Score (GUS) for decisions and similarity-based normalized statistics rate (SNSR) for recommendations, across both open-source models like Qwen2.5-Omni and GLM-4-Voice, as well as closed-source APIs such as GPT-4o Audio and Gemini-2.5-Flash. Our analysis reveals that closed-source models generally exhibit lower bias, while open-source models are more sensitive to age and gender, and recommendation tasks tend to amplify cross-group disparities. We found that biased decisions may persist in multi-turn conversations. This work provides the first systematic study of biases in end-to-end spoken dialogue models, offering insights towards fair and reliable audio-based interactive systems. To facilitate further research, we release the FairDialogue dataset and evaluation code.</li>
</ul>

<h3>Title: An Senegalese Legal Texts Structuration Using LLM-augmented Knowledge Graph</h3>
<ul>
<li><strong>Authors: </strong>Oumar Kane, Mouhamad M. Allaya, Dame Samb, Mamadou Bousso</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02353">https://arxiv.org/abs/2510.02353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02353">https://arxiv.org/pdf/2510.02353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02353]] An Senegalese Legal Texts Structuration Using LLM-augmented Knowledge Graph(https://arxiv.org/abs/2510.02353)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>This study examines the application of artificial intelligence (AI) and large language models (LLM) to improve access to legal texts in Senegal's judicial system. The emphasis is on the difficulties of extracting and organizing legal documents, highlighting the need for better access to judicial information. The research successfully extracted 7,967 articles from various legal documents, particularly focusing on the Land and Public Domain Code. A detailed graph database was developed, which contains 2,872 nodes and 10,774 relationships, aiding in the visualization of interconnections within legal texts. In addition, advanced triple extraction techniques were utilized for knowledge, demonstrating the effectiveness of models such as GPT-4o, GPT-4, and Mistral-Large in identifying relationships and relevant metadata. Through these technologies, the aim is to create a solid framework that allows Senegalese citizens and legal professionals to more effectively understand their rights and responsibilities.</li>
</ul>

<h3>Title: Modeling the language cortex with form-independent and enriched representations of sentence meaning reveals remarkable semantic abstractness</h3>
<ul>
<li><strong>Authors: </strong>Shreya Saha, Shurui Li, Greta Tuckute, Yuanning Li, Ru-Yuan Zhang, Leila Wehbe, Evelina Fedorenko, Meenakshi Khosla</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02354">https://arxiv.org/abs/2510.02354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02354">https://arxiv.org/pdf/2510.02354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02354]] Modeling the language cortex with form-independent and enriched representations of sentence meaning reveals remarkable semantic abstractness(https://arxiv.org/abs/2510.02354)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The human language system represents both linguistic forms and meanings, but the abstractness of the meaning representations remains debated. Here, we searched for abstract representations of meaning in the language cortex by modeling neural responses to sentences using representations from vision and language models. When we generate images corresponding to sentences and extract vision model embeddings, we find that aggregating across multiple generated images yields increasingly accurate predictions of language cortex responses, sometimes rivaling large language models. Similarly, averaging embeddings across multiple paraphrases of a sentence improves prediction accuracy compared to any single paraphrase. Enriching paraphrases with contextual details that may be implicit (e.g., augmenting "I had a pancake" to include details like "maple syrup") further increases prediction accuracy, even surpassing predictions based on the embedding of the original sentence, suggesting that the language system maintains richer and broader semantic representations than language models. Together, these results demonstrate the existence of highly abstract, form-independent meaning representations within the language cortex.</li>
</ul>

<h3>Title: Measuring Physical-World Privacy Awareness of Large Language Models: An Evaluation Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Xinjie Shen, Mufei Li, Pan Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02356">https://arxiv.org/abs/2510.02356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02356">https://arxiv.org/pdf/2510.02356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02356]] Measuring Physical-World Privacy Awareness of Large Language Models: An Evaluation Benchmark(https://arxiv.org/abs/2510.02356)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, large language model</a></li>
<li><strong>Abstract: </strong>The deployment of Large Language Models (LLMs) in embodied agents creates an urgent need to measure their privacy awareness in the physical world. Existing evaluation methods, however, are confined to natural language based scenarios. To bridge this gap, we introduce EAPrivacy, a comprehensive evaluation benchmark designed to quantify the physical-world privacy awareness of LLM-powered agents. EAPrivacy utilizes procedurally generated scenarios across four tiers to test an agent's ability to handle sensitive objects, adapt to changing environments, balance task execution with privacy constraints, and resolve conflicts with social norms. Our measurements reveal a critical deficit in current models. The top-performing model, Gemini 2.5 Pro, achieved only 59\% accuracy in scenarios involving changing physical environments. Furthermore, when a task was accompanied by a privacy request, models prioritized completion over the constraint in up to 86\% of cases. In high-stakes situations pitting privacy against critical social norms, leading models like GPT-4o and Claude-3.5-haiku disregarded the social norm over 15\% of the time. These findings, demonstrated by our benchmark, underscore a fundamental misalignment in LLMs regarding physically grounded privacy and establish the need for more robust, physically-aware alignment.</li>
</ul>

<h3>Title: Privacy in the Age of AI: A Taxonomy of Data Risks</h3>
<ul>
<li><strong>Authors: </strong>Grace Billiris, Asif Gill, Madhushi Bandara</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02357">https://arxiv.org/abs/2510.02357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02357">https://arxiv.org/pdf/2510.02357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02357]] Privacy in the Age of AI: A Taxonomy of Data Risks(https://arxiv.org/abs/2510.02357)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence (AI) systems introduce unprecedented privacy challenges as they process increasingly sensitive data. Traditional privacy frameworks prove inadequate for AI technologies due to unique characteristics such as autonomous learning and black-box decision-making. This paper presents a taxonomy classifying AI privacy risks, synthesised from 45 studies identified through systematic review. We identify 19 key risks grouped under four categories: Dataset-Level, Model-Level, Infrastructure-Level, and Insider Threat Risks. Findings reveal a balanced distribution across these dimensions, with human error (9.45%) emerging as the most significant factor. This taxonomy challenges conventional security approaches that typically prioritise technical controls over human factors, highlighting gaps in holistic understanding. By bridging technical and behavioural dimensions of AI privacy, this paper contributes to advancing trustworthy AI development and provides a foundation for future research.</li>
</ul>

<h3>Title: DiffuSpec: Unlocking Diffusion Language Models for Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Guanghao Li, Zhihui Fu, Min Fang, Qibin Zhao, Ming Tang, Chun Yuan, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02358">https://arxiv.org/abs/2510.02358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02358">https://arxiv.org/pdf/2510.02358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02358]] DiffuSpec: Unlocking Diffusion Language Models for Speculative Decoding(https://arxiv.org/abs/2510.02358)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) scale up, accuracy improves, but the autoregressive (AR) nature of decoding increases latency since each token requires a serial forward pass. Speculative decoding addresses this by employing a fast drafter to propose multi-token drafts, which are then verified in parallel by the target model. However, many deployments still rely on AR drafters, where sequential passes limit wall-clock gains. We revisit the drafting stage and present DiffuSpec, a training-free drop-in framework that uses a pretrained diffusion language model (DLM) to produce multi-token drafts in a single forward pass, while remaining compatible with standard AR verifiers. Because DLM drafts are generated under bidirectional conditioning, parallel per-position candidates form a token lattice in which the locally highest-probability token at each position need not form a causal left-to-right path. Moreover, DLM drafting requires pre-specifying a draft length, inducing a speed-quality trade-off. To address these challenges, we introduce two practical components: (i) a causal-consistency path search (CPS) over this lattice that extracts a left-to-right path aligned with AR verification; and (ii) an adaptive draft-length (ADL) controller that adjusts next proposal size based on recent acceptance feedback and realized generated length. Across benchmarks, DiffuSpec yields up to 3x wall-clock speedup, establishing diffusion-based drafting as a robust alternative to autoregressive drafters for speculative decoding.</li>
</ul>

<h3>Title: Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis</h3>
<ul>
<li><strong>Authors: </strong>Jiashu Ye, Tong Wu, Weiwen Chen, Hao Zhang, Zeteng Lin, Xingxing Li, Shujuan Weng, Manni Zhu, Xin Yuan, Xinlong Hong, Jingjie Li, Junyu Zheng, Zhijiong Huang, Jing Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02359">https://arxiv.org/abs/2510.02359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02359">https://arxiv.org/pdf/2510.02359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02359]] Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis(https://arxiv.org/abs/2510.02359)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Improving air quality and addressing climate change relies on accurate understanding and analysis of air pollutant and greenhouse gas emissions. However, emission-related knowledge is often fragmented and highly specialized, while existing methods for accessing and compiling emissions data remain inefficient. These issues hinder the ability of non-experts to interpret emissions information, posing challenges to research and management. To address this, we present Emission-GPT, a knowledge-enhanced large language model agent tailored for the atmospheric emissions domain. Built on a curated knowledge base of over 10,000 documents (including standards, reports, guidebooks, and peer-reviewed literature), Emission-GPT integrates prompt engineering and question completion to support accurate domain-specific question answering. Emission-GPT also enables users to interactively analyze emissions data via natural language, such as querying and visualizing inventories, analyzing source contributions, and recommending emission factors for user-defined scenarios. A case study in Guangdong Province demonstrates that Emission-GPT can extract key insights--such as point source distributions and sectoral trends--directly from raw data with simple prompts. Its modular and extensible architecture facilitates automation of traditionally manual workflows, positioning Emission-GPT as a foundational tool for next-generation emission inventory development and scenario-based assessment.</li>
</ul>

<h3>Title: Spiral of Silence in Large Language Model Agents</h3>
<ul>
<li><strong>Authors: </strong>Mingze Zhong, Meng Fang, Zijing Shi, Yuxuan Huang, Shunfeng Zheng, Yali Du, Ling Chen, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02360">https://arxiv.org/abs/2510.02360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02360">https://arxiv.org/pdf/2510.02360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02360]] Spiral of Silence in Large Language Model Agents(https://arxiv.org/abs/2510.02360)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Spiral of Silence (SoS) theory holds that individuals with minority views often refrain from speaking out for fear of social isolation, enabling majority positions to dominate public discourse. When the 'agents' are large language models (LLMs), however, the classical psychological explanation is not directly applicable, since SoS was developed for human societies. This raises a central question: can SoS-like dynamics nevertheless emerge from purely statistical language generation in LLM collectives? We propose an evaluation framework for examining SoS in LLM agents. Specifically, we consider four controlled conditions that systematically vary the availability of 'History' and 'Persona' signals. Opinion dynamics are assessed using trend tests such as Mann-Kendall and Spearman's rank, along with concentration measures including kurtosis and interquartile range. Experiments across open-source and closed-source models show that history and persona together produce strong majority dominance and replicate SoS patterns; history signals alone induce strong anchoring; and persona signals alone foster diverse but uncorrelated opinions, indicating that without historical anchoring, SoS dynamics cannot emerge. The work bridges computational sociology and responsible AI design, highlighting the need to monitor and mitigate emergent conformity in LLM-agent systems.</li>
</ul>

<h3>Title: ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs Inference</h3>
<ul>
<li><strong>Authors: </strong>Haojie Ouyang, Jianwei Lv, Lei Ren, Chen Wei, Xiaojie Wang, Fangxiang Feng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02361">https://arxiv.org/abs/2510.02361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02361">https://arxiv.org/pdf/2510.02361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02361]] ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs Inference(https://arxiv.org/abs/2510.02361)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based large models excel in natural language processing and computer vision, but face severe computational inefficiencies due to the self-attention's quadratic complexity with input tokens. Recently, researchers have proposed a series of methods based on block selection and compression to alleviate this problem, but they either have issues with semantic incompleteness or poor training-inference efficiency. To comprehensively address these challenges, we propose ChunkLLM, a lightweight and pluggable training framework. Specifically, we introduce two components: QK Adapter (Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each Transformer layer, serving dual purposes of feature compression and chunk attention acquisition. The latter operates at the bottommost layer of the model, functioning to detect chunk boundaries by leveraging contextual semantic information. During the training phase, the parameters of the backbone remain frozen, with only the QK Adapter and Chunk Adapter undergoing training. Notably, we design an attention distillation method for training the QK Adapter, which enhances the recall rate of key chunks. During the inference phase, chunk selection is triggered exclusively when the current token is detected as a chunk boundary, thereby accelerating model inference. Experimental evaluations are conducted on a diverse set of long-text and short-text benchmark datasets spanning multiple tasks. ChunkLLM not only attains comparable performance on short-text benchmarks but also maintains 98.64% of the performance on long-context benchmarks while preserving a 48.58% key-value cache retention rate. Particularly, ChunkLLM attains a maximum speedup of 4.48x in comparison to the vanilla Transformer in the processing of 120K long texts.</li>
</ul>

<h3>Title: A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History</h3>
<ul>
<li><strong>Authors: </strong>Matei-Iulian Cocu, Răzvan-Cosmin Cristia, Adrian Marius Dumitran</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02362">https://arxiv.org/abs/2510.02362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02362">https://arxiv.org/pdf/2510.02362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02362]] A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History(https://arxiv.org/abs/2510.02362)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this case study, we select a set of controversial Romanian historical questions and ask multiple Large Language Models to answer them across languages and contexts, in order to assess their biases. Besides being a study mainly performed for educational purposes, the motivation also lies in the recognition that history is often presented through altered perspectives, primarily influenced by the culture and ideals of a state, even through large language models. Since they are often trained on certain data sets that may present certain ambiguities, the lack of neutrality is subsequently instilled in users. The research process was carried out in three stages, to confirm the idea that the type of response expected can influence, to a certain extent, the response itself; after providing an affirmative answer to some given question, an LLM could shift its way of thinking after being asked the same question again, but being told to respond with a numerical value of a scale. Results show that binary response stability is relatively high but far from perfect and varies by language. Models often flip stance across languages or between formats; numeric ratings frequently diverge from the initial binary choice, and the most consistent models are not always those judged most accurate or neutral. Our research brings to light the predisposition of models to such inconsistencies, within a specific contextualization of the language for the question asked.</li>
</ul>

<h3>Title: Bootstrapping as a Morphism: An Arithmetic Geometry Approach to Asymptotically Faster Homomorphic Encryption</h3>
<ul>
<li><strong>Authors: </strong>Dongfang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CR, math.AG, math.NT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02365">https://arxiv.org/abs/2510.02365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02365">https://arxiv.org/pdf/2510.02365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02365]] Bootstrapping as a Morphism: An Arithmetic Geometry Approach to Asymptotically Faster Homomorphic Encryption(https://arxiv.org/abs/2510.02365)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>Fully Homomorphic Encryption (FHE) provides a powerful paradigm for secure computation, but its practical adoption is severely hindered by the prohibitive computational cost of its bootstrapping procedure. The complexity of all current bootstrapping methods is fundamentally tied to the multiplicative depth of the decryption circuit, denoted $L_{dec}$, making it the primary performance bottleneck. This paper introduces a new approach to bootstrapping that completely bypasses the traditional circuit evaluation model. We apply the tools of modern arithmetic geometry to reframe the bootstrapping operation as a direct geometric projection. Our framework models the space of ciphertexts as an affine scheme and rigorously defines the loci of decryptable and fresh ciphertexts as distinct closed subschemes. The bootstrapping transformation is then realized as a morphism between these two spaces. Computationally, this projection is equivalent to solving a specific Closest Vector Problem (CVP) instance on a highly structured ideal lattice, which we show can be done efficiently using a technique we call algebraic folding. The primary result of our work is a complete and provably correct bootstrapping algorithm with a computational complexity of $O(d \cdot \text{poly}(\log q))$, where $d$ is the ring dimension and $q$ is the ciphertext modulus. The significance of this result lies in the complete elimination of the factor $L_{dec}$ from the complexity, representing a fundamental asymptotic improvement over the state of the art. This geometric perspective offers a new and promising pathway toward achieving truly practical and high-performance FHE.</li>
</ul>

<h3>Title: Beyond Manuals and Tasks: Instance-Level Context Learning for LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Kuntai Cai, Juncheng Liu, Xianglin Yang, Zhaojie Niu, Xiaokui Xiao, Xing Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02369">https://arxiv.org/abs/2510.02369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02369">https://arxiv.org/pdf/2510.02369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02369]] Beyond Manuals and Tasks: Instance-Level Context Learning for LLM Agents(https://arxiv.org/abs/2510.02369)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) agents typically receive two kinds of context: (i) environment-level manuals that define interaction interfaces and global rules, and (ii) task-level guidance or demonstrations tied to specific goals. In this work, we identify a crucial but overlooked third type of context, instance-level context, which consists of verifiable and reusable facts tied to a specific environment instance, such as object locations, crafting recipes, and local rules. We argue that the absence of instance-level context is a common source of failure for LLM agents in complex tasks, as success often depends not only on reasoning over global rules or task prompts but also on making decisions based on precise and persistent facts. Acquiring such context requires more than memorization: the challenge lies in efficiently exploring, validating, and formatting these facts under tight interaction budgets. We formalize this problem as Instance-Level Context Learning (ILCL) and introduce our task-agnostic method to solve it. Our method performs a guided exploration, using a compact TODO forest to intelligently prioritize its next actions and a lightweight plan-act-extract loop to execute them. This process automatically produces a high-precision context document that is reusable across many downstream tasks and agents, thereby amortizing the initial exploration cost. Experiments across TextWorld, ALFWorld, and Crafter demonstrate consistent gains in both success and efficiency: for instance, ReAct's mean success rate in TextWorld rises from 37% to 95%, while IGE improves from 81% to 95%. By transforming one-off exploration into persistent, reusable knowledge, our method complements existing contexts to enable more reliable and efficient LLM agents.</li>
</ul>

<h3>Title: Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Minsung Kim, Dong-Kyum Kim, Jea Kwon, Nakyeong Yang, Kyomin Jung, Meeyoung Cha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02370">https://arxiv.org/abs/2510.02370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02370">https://arxiv.org/pdf/2510.02370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02370]] Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models(https://arxiv.org/abs/2510.02370)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models often encounter conflicts between in-context knowledge retrieved at inference time and parametric knowledge acquired during pretraining. Models that accept external knowledge uncritically are vulnerable to misinformation, whereas models that adhere rigidly to parametric knowledge fail to benefit from retrieval. Despite the widespread adoption of retrieval-augmented generation, we still lack a systematic understanding of what shapes knowledge-arbitration strategies during training. This gap risks producing pretrained models with undesirable arbitration behaviors and, consequently, wasting substantial computational resources after the pretraining budget has already been spent. To address this problem, we present the first controlled study of how training conditions influence models' use of in-context and parametric knowledge, and how they arbitrate between them. We train transformer-based language models on a synthetic biographies corpus while systematically controlling various conditions. Our experiments reveal that intra-document repetition of facts fosters the development of both parametric and in-context capabilities. Moreover, training on a corpus that contains inconsistent information or distributional skew encourages models to develop robust strategies for leveraging parametric and in-context knowledge. Rather than viewing these non-ideal properties as artifacts to remove, our results indicate that they are important for learning robust arbitration. These insights offer concrete, empirical guidance for pretraining models that harmoniously integrate parametric and in-context knowledge.</li>
</ul>

<h3>Title: Federated Spatiotemporal Graph Learning for Passive Attack Detection in Smart Grids</h3>
<ul>
<li><strong>Authors: </strong>Bochra Al Agha, Razane Tajeddine</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02371">https://arxiv.org/abs/2510.02371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02371">https://arxiv.org/pdf/2510.02371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02371]] Federated Spatiotemporal Graph Learning for Passive Attack Detection in Smart Grids(https://arxiv.org/abs/2510.02371)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, steal, federate</a></li>
<li><strong>Abstract: </strong>Smart grids are exposed to passive eavesdropping, where attackers listen silently to communication links. Although no data is actively altered, such reconnaissance can reveal grid topology, consumption patterns, and operational behavior, creating a gateway to more severe targeted attacks. Detecting this threat is difficult because the signals it produces are faint, short-lived, and often disappear when traffic is examined by a single node or along a single timeline. This paper introduces a graph-centric, multimodal detector that fuses physical-layer and behavioral indicators over ego-centric star subgraphs and short temporal windows to detect passive attacks. To capture stealthy perturbations, a two-stage encoder is introduced: graph convolution aggregates spatial context across ego-centric star subgraphs, while a bidirectional GRU models short-term temporal dependencies. The encoder transforms heterogeneous features into a unified spatio-temporal representation suitable for classification. Training occurs in a federated learning setup under FedProx, improving robustness to heterogeneous local raw data and contributing to the trustworthiness of decentralized training; raw measurements remain on client devices. A synthetic, standards-informed dataset is generated to emulate heterogeneous HAN/NAN/WAN communications with wireless-only passive perturbations, event co-occurrence, and leak-safe splits. The model achieves a testing accuracy of 98.32% per-timestep (F1_{attack}=0.972) and 93.35% per-sequence at 0.15% FPR using a simple decision rule with run-length m=2 and threshold $\tau=0.55$. The results demonstrate that combining spatial and temporal context enables reliable detection of stealthy reconnaissance while maintaining low false-positive rates, making the approach suitable for non-IID federated smart-grid deployments.</li>
</ul>

<h3>Title: A-MemGuard: A Proactive Defense Framework for LLM-Based Agent Memory</h3>
<ul>
<li><strong>Authors: </strong>Qianshan Wei, Tengchao Yang, Yaochen Wang, Xinfeng Li, Lijun Li, Zhenfei Yin, Yi Zhan, Thorsten Holz, Zhiqiang Lin, XiaoFeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02373">https://arxiv.org/abs/2510.02373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02373">https://arxiv.org/pdf/2510.02373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02373]] A-MemGuard: A Proactive Defense Framework for LLM-Based Agent Memory(https://arxiv.org/abs/2510.02373)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) agents use memory to learn from past interactions, enabling autonomous planning and decision-making in complex environments. However, this reliance on memory introduces a critical security risk: an adversary can inject seemingly harmless records into an agent's memory to manipulate its future behavior. This vulnerability is characterized by two core aspects: First, the malicious effect of injected records is only activated within a specific context, making them hard to detect when individual memory entries are audited in isolation. Second, once triggered, the manipulation can initiate a self-reinforcing error cycle: the corrupted outcome is stored as precedent, which not only amplifies the initial error but also progressively lowers the threshold for similar attacks in the future. To address these challenges, we introduce A-MemGuard (Agent-Memory Guard), the first proactive defense framework for LLM agent memory. The core idea of our work is the insight that memory itself must become both self-checking and self-correcting. Without modifying the agent's core architecture, A-MemGuard combines two mechanisms: (1) consensus-based validation, which detects anomalies by comparing reasoning paths derived from multiple related memories and (2) a dual-memory structure, where detected failures are distilled into ``lessons'' stored separately and consulted before future actions, breaking error cycles and enabling adaptation. Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. This work shifts LLM memory security from static filtering to a proactive, experience-driven model where defenses strengthen over time. Our code is available in this https URL</li>
</ul>

<h3>Title: A Hybrid CAPTCHA Combining Generative AI with Keystroke Dynamics for Enhanced Bot Detection</h3>
<ul>
<li><strong>Authors: </strong>Ayda Aghaei Nia</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02374">https://arxiv.org/abs/2510.02374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02374">https://arxiv.org/pdf/2510.02374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02374]] A Hybrid CAPTCHA Combining Generative AI with Keystroke Dynamics for Enhanced Bot Detection(https://arxiv.org/abs/2510.02374)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, biometric, extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>Completely Automated Public Turing tests to tell Computers and Humans Apart (CAPTCHAs) are a foundational component of web security, yet traditional implementations suffer from a trade-off between usability and resilience against AI-powered bots. This paper introduces a novel hybrid CAPTCHA system that synergizes the cognitive challenges posed by Large Language Models (LLMs) with the behavioral biometric analysis of keystroke dynamics. Our approach generates dynamic, unpredictable questions that are trivial for humans but non-trivial for automated agents, while simultaneously analyzing the user's typing rhythm to distinguish human patterns from robotic input. We present the system's architecture, formalize the feature extraction methodology for keystroke analysis, and report on an experimental evaluation. The results indicate that our dual-layered approach achieves a high degree of accuracy in bot detection, successfully thwarting both paste-based and script-based simulation attacks, while maintaining a high usability score among human participants. This work demonstrates the potential of combining cognitive and behavioral tests to create a new generation of more secure and user-friendly CAPTCHAs.</li>
</ul>

<h3>Title: Pretraining with hierarchical memories: separating long-tail and common knowledge</h3>
<ul>
<li><strong>Authors: </strong>Hadi Pouransari, David Grangier, C Thomas, Michael Kirchhof, Oncel Tuzel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02375">https://arxiv.org/abs/2510.02375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02375">https://arxiv.org/pdf/2510.02375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02375]] Pretraining with hierarchical memories: separating long-tail and common knowledge(https://arxiv.org/abs/2510.02375)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>The impressive performance gains of modern language models currently rely on scaling parameters: larger models store more world knowledge and reason better. Yet compressing all world knowledge into parameters is unnecessary, as only a fraction is used per prompt, and impractical for edge devices with limited inference-time memory and compute. We address this shortcoming by a memory-augmented architecture and a pretraining strategy aligned with existing hardware paradigms. We introduce small language models that access large hierarchical parametric memory banks encoding world knowledge. During pretraining and inference, we fetch a small, context-dependent memory block and add it to the model. Our pretraining learns to store long-tail world knowledge in the memory parameters, while the small language model acts as an anchor capturing common knowledge and general reasoning abilities. Through trillion-token-scale experiments, we show significant gains: a 160M-parameters model augmented with an 18M-parameters memory fetched from a 4.6B memory bank obtains comparable performance to a regular model with more than 2x the parameters. Through extensive experiments, we study the optimal type and size of parametric memories in transformers, scaling them to over 21B parameters. We find that our proposed hierarchical feed-forward memories work robustly across transformer architectures, whether added during pretraining or post-hoc.</li>
</ul>

<h3>Title: Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems</h3>
<ul>
<li><strong>Authors: </strong>Aakriti Agrawal, Rohith Aralikatti, Anirudh Satheesh, Souradip Chakraborty, Amrit Singh Bedi, Furong Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02377">https://arxiv.org/abs/2510.02377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02377">https://arxiv.org/pdf/2510.02377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02377]] Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems(https://arxiv.org/abs/2510.02377)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated exceptional capabilities, yet selecting the most reliable response from multiple LLMs remains a challenge, particularly in resource-constrained settings. Existing approaches often depend on costly external verifiers, human evaluators, or self-consistency techniques that require multiple samples from a single model. While multi-LLM systems produce more diverse responses than single models and thus have greater potential, they often underperform compared to single LLM self-consistency. We propose a principled, novel and computationally efficient method to select the best response from multiple different LLMs using a calibrated log-likelihood score, implicitly leveraging the inherent knowledge and confidence of these models. Our method demonstrates improvements of approx. 4%, 3%, and 5% across both debate (multi-round LLM discussions) and non-debate (Best-of-N with multiple LLMs) settings on GSM8K, MMLU (6 subsets), and ARC datasets respectively.</li>
</ul>

<h3>Title: Hybrid Schemes of NIST Post-Quantum Cryptography Standard Algorithms and Quantum Key Distribution for Key Exchange and Digital Signature</h3>
<ul>
<li><strong>Authors: </strong>Abel C. H. Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.PF, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02379">https://arxiv.org/abs/2510.02379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02379">https://arxiv.org/pdf/2510.02379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02379]] Hybrid Schemes of NIST Post-Quantum Cryptography Standard Algorithms and Quantum Key Distribution for Key Exchange and Digital Signature(https://arxiv.org/abs/2510.02379)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Since the security of post-quantum cryptography (PQC) algorithms is based on the hardness of mathematical problems, while the security of quantum key distribution (QKD) relies on the fundamental principles of quantum physics, each approach possesses distinct advantages and limitations that can complement one another. Consequently, recent studies have proposed hybrid schemes that combine QKD and PQC to establish a dual-layered security model. In response to this trend, this study proposes hybrid schemes that integrate QKD with the National Institute of Standards and Technology (NIST) standardized PQC algorithms. These hybrid schemes include two core components: a hybrid QKD-PQC key exchange protocol and a hybrid QKD-PQC digital signature scheme. For the hybrid key exchange protocol, this study combines Module-Lattice-based Key Encapsulation Mechanisms (ML-KEM) with QKD protocols, specifically BB84 and E91, to construct a secure key exchange protocol. In the design of the hybrid digital signature scheme, this study utilizes Module-Lattice-based Digital Signature Algorithms (ML-DSA) and Stateless Hash-based Digital Signature Algorithms (SLH-DSA) to generate signature reconstruction values. These values are verified using confirmation codes transmitted via the BB84 and E91 protocols. The proposed hybrid key exchange protocol is evaluated by examining the shared secret key it produces, particularly with respect to entropy and whether the output is independent and identically distributed (IID). Furthermore, the computation time and message lengths of the proposed hybrid schemes are evaluated.</li>
</ul>

<h3>Title: Selmer-Inspired Elliptic Curve Generation</h3>
<ul>
<li><strong>Authors: </strong>Awnon Bhowmik</a></li>
<li><strong>Subjects: </strong>cs.CR, math.NT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02383">https://arxiv.org/abs/2510.02383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02383">https://arxiv.org/pdf/2510.02383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02383]] Selmer-Inspired Elliptic Curve Generation(https://arxiv.org/abs/2510.02383)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Elliptic curve cryptography (ECC) is foundational to modern secure communication, yet existing standard curves have faced scrutiny for opaque parameter-generation practices. This work introduces a Selmer-inspired framework for constructing elliptic curves that is both transparent and auditable. Drawing from $2$- and $3$-descent methods, we derive binary quartics and ternary cubics whose classical invariants deterministically yield candidate $(c_4,c_6)$ parameters. Local solubility checks, modeled on Selmer admissibility, filter candidates prior to reconciliation into short-Weierstrass form over prime fields. We then apply established cryptographic validations, including group-order factorization, cofactor bounds, twist security, and embedding-degree heuristics. A proof-of-concept implementation demonstrates that the pipeline functions as a retry-until-success Las Vegas algorithm, with complete transcripts enabling independent verification. Unlike seed-based or purely efficiency-driven designs, our approach embeds arithmetic structure into parameter selection while remaining compatible with constant-time, side-channel resistant implementations. This work broadens the design space for elliptic curves, showing that descent techniques from arithmetic geometry can underpin trust-enhancing, standardization-ready constructions.</li>
</ul>

<h3>Title: Secure and Robust Watermarking for AI-generated Images: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Jie Cao, Qi Li, Zelin Zhang, Jianbing Ni</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02384">https://arxiv.org/abs/2510.02384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02384">https://arxiv.org/pdf/2510.02384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02384]] Secure and Robust Watermarking for AI-generated Images: A Comprehensive Survey(https://arxiv.org/abs/2510.02384)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, protect, attack, robust, watermark, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of generative artificial intelligence (Gen-AI) has facilitated the effortless creation of high-quality images, while simultaneously raising critical concerns regarding intellectual property protection, authenticity, and accountability. Watermarking has emerged as a promising solution to these challenges by distinguishing AI-generated images from natural content, ensuring provenance, and fostering trustworthy digital ecosystems. This paper presents a comprehensive survey of the current state of AI-generated image watermarking, addressing five key dimensions: (1) formalization of image watermarking systems; (2) an overview and comparison of diverse watermarking techniques; (3) evaluation methodologies with respect to visual quality, capacity, and detectability; (4) vulnerabilities to malicious attacks; and (5) prevailing challenges and future directions. The survey aims to equip researchers with a holistic understanding of AI-generated image watermarking technologies, thereby promoting their continued development.</li>
</ul>

<h3>Title: On The Fragility of Benchmark Contamination Detection in Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Han Wang, Haoyu Li, Brian Ko, Huan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02386">https://arxiv.org/abs/2510.02386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02386">https://arxiv.org/pdf/2510.02386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02386]] On The Fragility of Benchmark Contamination Detection in Reasoning Models(https://arxiv.org/abs/2510.02386)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Leaderboards for LRMs have turned evaluation into a competition, incentivizing developers to optimize directly on benchmark suites. A shortcut to achieving higher rankings is to incorporate evaluation benchmarks into the training data, thereby yielding inflated performance, known as benchmark contamination. Surprisingly, our studies find that evading contamination detections for LRMs is alarmingly easy. We focus on the two scenarios where contamination may occur in practice: (I) when the base model evolves into LRM via SFT and RL, we find that contamination during SFT can be originally identified by contamination detection methods. Yet, even a brief GRPO training can markedly conceal contamination signals that most detection methods rely on. Further empirical experiments and theoretical analysis indicate that PPO style importance sampling and clipping objectives are the root cause of this detection concealment, indicating that a broad class of RL methods may inherently exhibit similar concealment capability; (II) when SFT contamination with CoT is applied to advanced LRMs as the final stage, most contamination detection methods perform near random guesses. Without exposure to non-members, contaminated LRMs would still have more confidence when responding to those unseen samples that share similar distributions to the training set, and thus, evade existing memorization-based detection methods. Together, our findings reveal the unique vulnerability of LRMs evaluations: Model developers could easily contaminate LRMs to achieve inflated leaderboards performance while leaving minimal traces of contamination, thereby strongly undermining the fairness of evaluation and threatening the integrity of public leaderboards. This underscores the urgent need for advanced contamination detection methods and trustworthy evaluation protocols tailored to LRMs.</li>
</ul>

<h3>Title: Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Haoyue Bai, Haoyu Wang, Shengyu Chen, Zhengzhang Chen, Lu-An Tang, Wei Cheng, Haifeng Chen, Yanjie Fu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02388">https://arxiv.org/abs/2510.02388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02388">https://arxiv.org/pdf/2510.02388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02388]] Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation(https://arxiv.org/abs/2510.02388)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable performance on general Question Answering (QA), yet they often struggle in domain-specific scenarios where accurate and up-to-date information is required. Retrieval-Augmented Generation (RAG) addresses this limitation by enriching LLMs with external knowledge, but existing systems primarily rely on unstructured documents, while largely overlooking relational databases, which provide precise, timely, and efficiently queryable factual information, serving as indispensable infrastructure in domains such as finance, healthcare, and scientific research. Motivated by this gap, we conduct a systematic analysis that reveals three central observations: (i) databases and documents offer complementary strengths across queries, (ii) naively combining both sources introduces noise and cost without consistent accuracy gains, and (iii) selecting the most suitable source for each query is crucial to balance effectiveness and efficiency. We further observe that query types show consistent regularities in their alignment with retrieval paths, suggesting that routing decisions can be effectively guided by systematic rules that capture these patterns. Building on these insights, we propose a rule-driven routing framework. A routing agent scores candidate augmentation paths based on explicit rules and selects the most suitable one; a rule-making expert agent refines the rules over time using QA feedback to maintain adaptability; and a path-level meta-cache reuses past routing decisions for semantically similar queries to reduce latency and cost. Experiments on three QA benchmarks demonstrate that our framework consistently outperforms static strategies and learned routing baselines, achieving higher accuracy while maintaining moderate computational cost.</li>
</ul>

<h3>Title: LLM-Generated Samples for Android Malware Detection</h3>
<ul>
<li><strong>Authors: </strong>Nik Rollinson, Nikolaos Polatidis</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02391">https://arxiv.org/abs/2510.02391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02391">https://arxiv.org/pdf/2510.02391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02391]] LLM-Generated Samples for Android Malware Detection(https://arxiv.org/abs/2510.02391)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, large language model</a></li>
<li><strong>Abstract: </strong>Android malware continues to evolve through obfuscation and polymorphism, posing challenges for both signature-based defenses and machine learning models trained on limited and imbalanced datasets. Synthetic data has been proposed as a remedy for scarcity, yet the role of large language models (LLMs) in generating effective malware data for detection tasks remains underexplored. In this study, we fine-tune GPT-4.1-mini to produce structured records for three malware families: BankBot, Locker/SLocker, and Airpush/StopSMS, using the KronoDroid dataset. After addressing generation inconsistencies with prompt engineering and post-processing, we evaluate multiple classifiers under three settings: training with real data only, real-plus-synthetic data, and synthetic data alone. Results show that real-only training achieves near perfect detection, while augmentation with synthetic data preserves high performance with only minor degradations. In contrast, synthetic-only training produces mixed outcomes, with effectiveness varying across malware families and fine-tuning strategies. These findings suggest that LLM-generated malware can enhance scarce datasets without compromising detection accuracy, but remains insufficient as a standalone training source.</li>
</ul>

<h3>Title: KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing and Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Yinyi Luo, Zhexian Zhou, Hao Chen, Kai Qiu, Marios Savvides, Yixuan Li, Jindong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02392">https://arxiv.org/abs/2510.02392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02392">https://arxiv.org/pdf/2510.02392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02392]] KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing and Unlearning(https://arxiv.org/abs/2510.02392)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Knowledge editing and machine unlearning are two popular approaches for large language models (LLMs) to stay up-to-date. However, the knowledge updating mechanism of LLMs remains largely unexplored due to insufficient, isolated, and small-scale evaluation. For instance, are LLMs similar to humans in modifying certain knowledge? What differs editing and unlearning as training data increases? This paper proposes KnowledgeSmith, a unified framework to systematically understand the updating mechanism of LLMs. We first cast editing and unlearning as instances of one constrained optimization problem. Then, we propose an automatic dataset generator that provides structured interventions across multiple graph levels and data scales, enabling controlled studies of how different modification strategies propagate through model knowledge. Extensive experiments demonstrate nuanced insights over knowledge propagation, plasticity scaling, consistency, and robustness. For instance, our results show that LLMs do not exhibit similar updating as humans for different levels of knowledge, and there exists consistency-capacity trade-off. We hope our findings can offer suggestions to the design of more reliable and scalable strategies. Code: this https URL</li>
</ul>

<h3>Title: Retrieval and Augmentation of Domain Knowledge for Text-to-SQL Semantic Parsing</h3>
<ul>
<li><strong>Authors: </strong>Manasi Patwardhan, Ayush Agarwal, Shabbirhussain Bhaisaheb, Aseem Arora, Lovekesh Vig, Sunita Sarawagi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02394">https://arxiv.org/abs/2510.02394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02394">https://arxiv.org/pdf/2510.02394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02394]] Retrieval and Augmentation of Domain Knowledge for Text-to-SQL Semantic Parsing(https://arxiv.org/abs/2510.02394)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The performance of Large Language Models (LLMs) for translating Natural Language (NL) queries into SQL varies significantly across databases (DBs). NL queries are often expressed using a domain specific vocabulary, and mapping these to the correct SQL requires an understanding of the embedded domain expressions, their relationship to the DB schema structure. Existing benchmarks rely on unrealistic, ad-hoc query specific textual hints for expressing domain knowledge. In this paper, we propose a systematic framework for associating structured domain statements at the database level. We present retrieval of relevant structured domain statements given a user query using sub-string level match. We evaluate on eleven realistic DB schemas covering diverse domains across five open-source and proprietary LLMs and demonstrate that (1) DB level structured domain statements are more practical and accurate than existing ad-hoc query specific textual domain statements, and (2) Our sub-string match based retrieval of relevant domain statements provides significantly higher accuracy than other retrieval approaches.</li>
</ul>

<h3>Title: PolyLink: A Blockchain Based Decentralized Edge AI Platform for LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Hongbo Liu, Jiannong Cao, Bo Yang, Dongbin Bai, Yinfeng Cao, Xiaoming Shen, Yinan Zhang, Jinwen Liang, Shan Jiang, Mingjin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02395">https://arxiv.org/abs/2510.02395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02395">https://arxiv.org/pdf/2510.02395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02395]] PolyLink: A Blockchain Based Decentralized Edge AI Platform for LLM Inference(https://arxiv.org/abs/2510.02395)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) in recent years has revolutionized the AI landscape. However, the deployment model and usage of LLM services remain highly centralized, creating significant trust issues and costs for end users and developers. To address these issues, we propose PolyLink, a blockchain-based decentralized AI platform that decentralizes LLM development and inference. Specifically, PolyLink introduces a decentralized crowdsourcing architecture that supports single-device and cross-device model deployment and inference across heterogeneous devices at the edge. Moreover, to ensure the inference integrity, we design the TIQE protocol, which combines a lightweight cross-encoder model and an LLM-as-a-Judge for a high-accuracy inference evaluation. Lastly, we integrate a comprehensive token-based incentive model with dynamic pricing and reward mechanisms for all participants. We have deployed PolyLink and conducted an extensive real-world evaluation through geo-distributed deployment across heterogeneous devices. Results indicate that the inference and verification latency is practical. Our security analysis demonstrates that the system is resistant to model degradation attacks and validator corruptions. PolyLink is now available at this https URL.</li>
</ul>

<h3>Title: Extreme value forecasting using relevance-based data augmentation with deep learning models</h3>
<ul>
<li><strong>Authors: </strong>Junru Hua, Rahul Ahluwalia, Rohitash Chandra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02407">https://arxiv.org/abs/2510.02407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02407">https://arxiv.org/pdf/2510.02407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02407]] Extreme value forecasting using relevance-based data augmentation with deep learning models(https://arxiv.org/abs/2510.02407)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Data augmentation with generative adversarial networks (GANs) has been popular for class imbalance problems, mainly for pattern classification and computer vision-related applications. Extreme value forecasting is a challenging field that has various applications from finance to climate change problems. In this study, we present a data augmentation framework for extreme value forecasting. In this framework, our focus is on forecasting extreme values using deep learning models in combination with data augmentation models such as GANs and synthetic minority oversampling technique (SMOTE). We use deep learning models such as convolutional long short-term memory (Conv-LSTM) and bidirectional long short-term memory (BD-LSTM) networks for multistep ahead prediction featuring extremes. We investigate which data augmentation models are the most suitable, taking into account the prediction accuracy overall and at extreme regions, along with computational efficiency. We also present novel strategies for incorporating data augmentation, considering extreme values based on a relevance function. Our results indicate that the SMOTE-based strategy consistently demonstrated superior adaptability, leading to improved performance across both short- and long-horizon forecasts. Conv-LSTM and BD-LSTM exhibit complementary strengths: the former excels in periodic, stable datasets, while the latter performs better in chaotic or non-stationary sequences.</li>
</ul>

<h3>Title: Dynamic Target Attack</h3>
<ul>
<li><strong>Authors: </strong>Kedong Xiu, Churui Zeng, Tianhang Zheng, Xinzhe Huang, Xiaojun Jia, Di Wang, Puning Zhao, Zhan Qin, Kui Ren</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02422">https://arxiv.org/abs/2510.02422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02422">https://arxiv.org/pdf/2510.02422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02422]] Dynamic Target Attack(https://arxiv.org/abs/2510.02422)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Existing gradient-based jailbreak attacks typically optimize an adversarial suffix to induce a fixed affirmative response. However, this fixed target usually resides in an extremely low-density region of a safety-aligned LLM's output distribution conditioned on diverse harmful inputs. Due to the substantial discrepancy between the target and the original output, existing attacks require numerous iterations to optimize the adversarial prompt, which might still fail to induce the low-probability target response from the target LLM. In this paper, we propose Dynamic Target Attack (DTA), a new jailbreaking framework relying on the target LLM's own responses as targets to optimize the adversarial prompts. In each optimization round, DTA iteratively samples multiple candidate responses directly from the output distribution conditioned on the current prompt, and selects the most harmful response as a temporary target for prompt optimization. In contrast to existing attacks, DTA significantly reduces the discrepancy between the target and the output distribution, substantially easing the optimization process to search for an effective adversarial prompt. Extensive experiments demonstrate the superior effectiveness and efficiency of DTA: under the white-box setting, DTA only needs 200 optimization iterations to achieve an average attack success rate (ASR) of over 87\% on recent safety-aligned LLMs, exceeding the state-of-the-art baselines by over 15\%. The time cost of DTA is 2-26 times less than existing baselines. Under the black-box setting, DTA uses Llama-3-8B-Instruct as a surrogate model for target sampling and achieves an ASR of 85\% against the black-box target model Llama-3-70B-Instruct, exceeding its counterparts by over 25\%.</li>
</ul>

<h3>Title: Adaptive Deception Framework with Behavioral Analysis for Enhanced Cybersecurity Defense</h3>
<ul>
<li><strong>Authors: </strong>Basil Abdullah AL-Zahrani</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02424">https://arxiv.org/abs/2510.02424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02424">https://arxiv.org/pdf/2510.02424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02424]] Adaptive Deception Framework with Behavioral Analysis for Enhanced Cybersecurity Defense(https://arxiv.org/abs/2510.02424)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>This paper presents CADL (Cognitive-Adaptive Deception Layer), an adaptive deception framework achieving 99.88% detection rate with 0.13% false positive rate on the CICIDS2017 dataset. The framework employs ensemble machine learning (Random Forest, XGBoost, Neural Networks) combined with behavioral profiling to identify and adapt responses to network intrusions. Through a coordinated signal bus architecture, security components share real-time intelligence, enabling collective decision-making. The system profiles attackers based on temporal patterns and deploys customized deception strategies across five escalation levels. Evaluation on 50,000 CICIDS2017 test samples demonstrates that CADL significantly outperforms traditional intrusion detection systems (Snort: 71.2%, Suricata: 68.5%) while maintaining production-ready false positive rates. The framework's behavioral analysis achieves 89% accuracy in classifying attacker profiles. We provide open-source implementation and transparent performance metrics, offering an accessible alternative to commercial deception platforms costing $150-400 per host annually.</li>
</ul>

<h3>Title: Words That Make Language Models Perceive</h3>
<ul>
<li><strong>Authors: </strong>Sophie L. Wang, Phillip Isola, Brian Cheung</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02425">https://arxiv.org/abs/2510.02425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02425">https://arxiv.org/pdf/2510.02425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02425]] Words That Make Language Models Perceive(https://arxiv.org/abs/2510.02425)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) trained purely on text ostensibly lack any direct perceptual experience, yet their internal representations are implicitly shaped by multimodal regularities encoded in language. We test the hypothesis that explicit sensory prompting can surface this latent structure, bringing a text-only LLM into closer representational alignment with specialist vision and audio encoders. When a sensory prompt tells the model to 'see' or 'hear', it cues the model to resolve its next-token predictions as if they were conditioned on latent visual or auditory evidence that is never actually supplied. Our findings reveal that lightweight prompt engineering can reliably activate modality-appropriate representations in purely text-trained LLMs.</li>
</ul>

<h3>Title: How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models</h3>
<ul>
<li><strong>Authors: </strong>Parth Asawa, Alan Zhu, Matei Zaharia, Alexandros G. Dimakis, Joseph E. Gonzalez</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02453">https://arxiv.org/abs/2510.02453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02453">https://arxiv.org/pdf/2510.02453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02453]] How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models(https://arxiv.org/abs/2510.02453)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Foundation models are increasingly deployed as black-box services, where model weights cannot be modified and customization is limited to prompting. While static prompt optimization has shown promise, it produces a single fixed prompt that fails to adapt to different inputs, users, or environments. We introduce Advisor Models, lightweight parametric policies trained with reinforcement learning to reactively issue natural language steering instructions in-context to black-box models. The advisor is a second small model that sits between the input and the model, shaping behavior on a per-instance basis using reward signals from the environment. Across multiple domains involving reasoning and personalization, we show that Advisor Models outperform static prompt optimizers, discovering environment dynamics and improving downstream task performance. We also demonstrate the generalizability of advisors by transferring them across black-box models, as well as the framework's ability to achieve specialization while retaining robustness to out-of-distribution inputs. Viewed more broadly, Advisor Models provide a learnable interface to black-box systems where the advisor acts as a parametric, environment-specific memory. We argue that dynamic optimization of black-box models via Advisor Models is a promising direction for enabling personalization and environment-adaptable AI with frontier-level capabilities.</li>
</ul>

<h3>Title: Assessing the Potential for Catastrophic Failure in Dynamic Post-Training Quantization</h3>
<ul>
<li><strong>Authors: </strong>Logan Frank, Paul Ardis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02457">https://arxiv.org/abs/2510.02457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02457">https://arxiv.org/pdf/2510.02457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02457]] Assessing the Potential for Catastrophic Failure in Dynamic Post-Training Quantization(https://arxiv.org/abs/2510.02457)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Post-training quantization (PTQ) has recently emerged as an effective tool for reducing the computational complexity and memory usage of a neural network by representing its weights and activations with lower precision. While this paradigm has shown great success in lowering compute and storage costs, there is the potential for drastic performance reduction depending upon the distribution of inputs experienced in inference. When considering possible deployment in safety-critical environments, it is important to investigate the extent of potential performance reduction, and what characteristics of input distributions may give rise to this reduction. In this work, we explore the idea of extreme failure stemming from dynamic PTQ and formulate a knowledge distillation and reinforcement learning task to learn a network and bit-width policy pair such that catastrophic failure under quantization is analyzed in terms of worst case potential. Our results confirm the existence of this "detrimental" network-policy pair, with several instances demonstrating performance reductions in the range of 10-65% in accuracy, compared to their "robust" counterparts encountering a <2% decrease. From systematic experimentation and analyses, we also provide an initial exploration into points at highest vulnerability. While our results represent an initial step toward understanding failure cases introduced by PTQ, our findings ultimately emphasize the need for caution in real-world deployment scenarios. We hope this work encourages more rigorous examinations of robustness and a greater emphasis on safety considerations for future works within the broader field of deep learning.</li>
</ul>

<h3>Title: CLARITY: Clinical Assistant for Routing, Inference, and Triage</h3>
<ul>
<li><strong>Authors: </strong>Vladimir Shaposhnikov, Aleksandr Nesterov, Ilia Kopanichuk, Ivan Bakulin, Egor Zhelvakov, Ruslan Abramov, Ekaterina Tsapieva, Dmitry V. Dylov, Ivan Oseledets</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02463">https://arxiv.org/abs/2510.02463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02463">https://arxiv.org/pdf/2510.02463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02463]] CLARITY: Clinical Assistant for Routing, Inference, and Triage(https://arxiv.org/abs/2510.02463)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We present CLARITY (Clinical Assistant for Routing, Inference, and Triage), an AI-driven platform designed to facilitate patient-to-specialist routing, clinical consultations, and severity assessment of patients' conditions. Its hybrid architecture combines a Finite State Machine (FSM) for structured dialogue flows with collaborative agents that employ Large Language Model (LLM) to analyze symptoms and prioritize referrals to appropriate specialists. Built on a modular microservices framework, CLARITY ensures safe, efficient, and robust performance, flexible and readily scalable to meet the demands of existing workflows and IT solutions in healthcare. We report integration of our clinical assistant into a large-scale nation-wide inter-hospital IT platform, with over 55,000 content-rich user dialogues completed within the two months of deployment, 2,500 of which were expert-annotated for a consequent validation. The validation results show that CLARITY surpasses human-level performance in terms of the first-attempt routing precision, naturally requiring up to 3 times shorter duration of the consultation than with a human.</li>
</ul>

<h3>Title: Rigorous Evaluation of Microarchitectural Side-Channels with Statistical Model Checking</h3>
<ul>
<li><strong>Authors: </strong>Weihang Li, Pete Crowley, Arya Tschand, Yu Wang, Miroslav Pajic, Daniel Sorin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02475">https://arxiv.org/abs/2510.02475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02475">https://arxiv.org/pdf/2510.02475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02475]] Rigorous Evaluation of Microarchitectural Side-Channels with Statistical Model Checking(https://arxiv.org/abs/2510.02475)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Rigorous quantitative evaluation of microarchitectural side channels is challenging for two reasons. First, the processors, attacks, and defenses often exhibit probabilistic behaviors. These probabilistic behaviors arise due to natural noise in systems (e.g., from co-running processes), probabilistic side channel attacks, and probabilistic obfuscation defenses. Second, microprocessors are extremely complex. Previous evaluation methods have relied on abstract or simplified models, which are necessarily less detailed than real systems or cycle-by-cycle simulators, and these models may miss important phenomena. Whereas a simple model may suffice for estimating performance, security issues frequently manifest in the details. We address this challenge by introducing Statistical Model Checking (SMC) to the quantitative evaluation of microarchitectural side channels. SMC is a rigorous statistical technique that can process the results of probabilistic experiments and provide statistical guarantees, and it has been used in computing applications that depend heavily on statistical guarantees (e.g., medical implants, vehicular computing). With SMC, we can treat processors as opaque boxes, and we do not have to abstract or simplify them. We demonstrate the effectiveness of SMC through three case studies, in which we experimentally show that SMC can evaluate existing security vulnerabilities and defenses and provide qualitatively similar conclusions with greater statistical rigor, while making no simplifying assumptions or abstractions. We also show that SMC can enable a defender to quantify the amount of noise necessary to have a desired level of confidence that she has reduced an attacker's probability of success to less than a desired threshold, thus providing the defender with an actionable plan for obfuscation via noise injection.</li>
</ul>

<h3>Title: Litespark Technical Report: High-Throughput, Energy-Efficient LLM Training Framework</h3>
<ul>
<li><strong>Authors: </strong>Nii Osae Osae Dade, Moinul Hossain Rahat</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02483">https://arxiv.org/abs/2510.02483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02483">https://arxiv.org/pdf/2510.02483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02483]] Litespark Technical Report: High-Throughput, Energy-Efficient LLM Training Framework(https://arxiv.org/abs/2510.02483)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Training Large Language Models (LLMs) is plagued by long training times and massive energy consumption, with modern models requiring months of computation and gigawatt-hours of electricity. In light of these challenges,we introduce Litespark, a novel pre-training framework that addresses these inefficiencies through targeted optimizations to transformer attention and MLP layers. Our approach combines architectural improvements with algorithmic enhancements to maximize Model FLOPs Utilization (MFU) while maintaining compatibility with standard transformer implementations. Comprehensive benchmarking on 3B and 30B parameter Llama models using the SlimPajama-627B dataset demonstrates substantial performance gains: 2x-6x training throughput improvement and $55\%-83$% energy consumption reduction across multi-node H200 GPU clusters. These optimizations are model- and hardware-agnostic, enabling broad applicability across transformer architectures and extending to post-training phases including supervised fine-tuning and direct preference optimization.</li>
</ul>

<h3>Title: Improved Robustness of Deep Reinforcement Learning for Control of Time-Varying Systems by Bounded Extremum Seeking</h3>
<ul>
<li><strong>Authors: </strong>Shaifalee Saxena, Alan Williams, Rafael Fierro, Alexander Scheinker</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02490">https://arxiv.org/abs/2510.02490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02490">https://arxiv.org/pdf/2510.02490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02490]] Improved Robustness of Deep Reinforcement Learning for Control of Time-Varying Systems by Bounded Extremum Seeking(https://arxiv.org/abs/2510.02490)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we study the use of robust model independent bounded extremum seeking (ES) feedback control to improve the robustness of deep reinforcement learning (DRL) controllers for a class of nonlinear time-varying systems. DRL has the potential to learn from large datasets to quickly control or optimize the outputs of many-parameter systems, but its performance degrades catastrophically when the system model changes rapidly over time. Bounded ES can handle time-varying systems with unknown control directions, but its convergence speed slows down as the number of tuned parameters increases and, like all local adaptive methods, it can get stuck in local minima. We demonstrate that together, DRL and bounded ES result in a hybrid controller whose performance exceeds the sum of its parts with DRL taking advantage of historical data to learn how to quickly control a many-parameter system to a desired setpoint while bounded ES ensures its robustness to time variations. We present a numerical study of a general time-varying system and a combined ES-DRL controller for automatic tuning of the Low Energy Beam Transport section at the Los Alamos Neutron Science Center linear particle accelerator.</li>
</ul>

<h3>Title: TLoRa: Implementing TLS Over LoRa for Secure HTTP Communication in IoT</h3>
<ul>
<li><strong>Authors: </strong>Atonu Ghosh, Akhilesh Mohanasundaram, Srishivanth R F, Sudip Misra</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02519">https://arxiv.org/abs/2510.02519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02519">https://arxiv.org/pdf/2510.02519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02519]] TLoRa: Implementing TLS Over LoRa for Secure HTTP Communication in IoT(https://arxiv.org/abs/2510.02519)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>We present TLoRa, an end-to-end architecture for HTTPS communication over LoRa by integrating TCP tunneling and a complete TLS 1.3 handshake. It enables a seamless and secure communication channel between WiFi-enabled end devices and the Internet over LoRa using an End Hub (EH) and a Net Relay (NR). The EH tethers a WiFi hotspot and a captive portal for user devices to connect and request URLs. The EH forwards the requested URLs to the NR using a secure tunnel over LoRa. The NR, which acts as a server-side proxy, receives and resolves the request from the Internet-based server. It then relays back the encrypted response from the server over the same secure tunnel. TLoRa operates in three phases -session setup, secure tunneling, and rendering. In the first phase, it manages the TCP socket and initiates the TLS handshake. In the second, it creates a secure tunnel and transfers encrypted TLS data over LoRa. Finally, it delivers the URL content to the user. TLoRa also implements a lightweight TLS record reassembly layer and a queuing mechanism for session multiplexing. We evaluate TLoRa on real hardware using multiple accesses to a web API. Results indicate that it provides a practical solution by successfully establishing a TLS session over LoRa in 9.9 seconds and takes 3.58 seconds to fulfill API requests. To the best of our knowledge, this is the first work to comprehensively design, implement, and evaluate the performance of HTTPS access over LoRa using full TLS.</li>
</ul>

<h3>Title: Graph Generation with Spectral Geodesic Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Xikun Huang, Tianyu Ruan, Chihao Zhang, Shihua Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02520">https://arxiv.org/abs/2510.02520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02520">https://arxiv.org/pdf/2510.02520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02520]] Graph Generation with Spectral Geodesic Flow Matching(https://arxiv.org/abs/2510.02520)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Graph generation is a fundamental task with wide applications in modeling complex systems. Although existing methods align the spectrum or degree profile of the target graph, they often ignore the geometry induced by eigenvectors and the global structure of the graph. In this work, we propose Spectral Geodesic Flow Matching (SFMG), a novel framework that uses spectral eigenmaps to embed both input and target graphs into continuous Riemannian manifolds. We then define geodesic flows between embeddings and match distributions along these flows to generate output graphs. Our method yields several advantages: (i) captures geometric structure beyond eigenvalues, (ii) supports flexible generation of diverse graphs, and (iii) scales efficiently. Empirically, SFMG matches the performance of state-of-the-art approaches on graphlet, degree, and spectral metrics across diverse benchmarks. In particular, it achieves up to 30$\times$ speedup over diffusion-based models, offering a substantial advantage in scalability and training efficiency. We also demonstrate its ability to generalize to unseen graph scales. Overall, SFMG provides a new approach to graph synthesis by integrating spectral geometry with flow matching.</li>
</ul>

<h3>Title: Unraveling Syntax: How Language Models Learn Context-Free Grammars</h3>
<ul>
<li><strong>Authors: </strong>Laura Ying Schulz, Daniel Mitropolsky, Tomaso Poggio</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.FL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02524">https://arxiv.org/abs/2510.02524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02524">https://arxiv.org/pdf/2510.02524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02524]] Unraveling Syntax: How Language Models Learn Context-Free Grammars(https://arxiv.org/abs/2510.02524)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>We introduce a new framework for understanding how language models acquire syntax. While large models achieve impressive results, little is known about their learning dynamics. Our approach starts with the observation that most domains of interest, such as natural language syntax, coding languages, arithmetic problems, are captured by probabilistic context-free grammars (PCFGs). We study the learning dynamics of small models trained on synthetic languages generated from PCFGs, enabling precise control over grammar complexity, recursion depth, and subgrammar structure. We prove several general, recursive formulae for the training loss and Kullback-Leibler divergence over the subgrammar structure of a PCFG. Empirically, we find that unlike children, who first master simple substructures before progressing to more complex constructions, transformers reduce loss across all subgrammars in parallel. We further show that subgrammar pretraining can improve the final loss for smaller models, and that pretrained models develop internal representations more aligned with the grammar's substructure. Finally, we demonstrate that models struggle with deeper recursive structures (a limitation even of large language models), revealing fundamental challenges in how neural networks represent hierarchical syntax. Overall, our work initiates the study of the learning dynamics of transformers on PCFGs as a versatile testbed for probing learning in language models, opening a research direction with many open questions.</li>
</ul>

<h3>Title: Hierarchical Semantic Retrieval with Cobweb</h3>
<ul>
<li><strong>Authors: </strong>Anant Gupta, Karthik Singaravadivelan, Zekun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02539">https://arxiv.org/abs/2510.02539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02539">https://arxiv.org/pdf/2510.02539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02539]] Hierarchical Semantic Retrieval with Cobweb(https://arxiv.org/abs/2510.02539)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural document retrieval often treats a corpus as a flat cloud of vectors scored at a single granularity, leaving corpus structure underused and explanations opaque. We use Cobweb--a hierarchy-aware framework--to organize sentence embeddings into a prototype tree and rank documents via coarse-to-fine traversal. Internal nodes act as concept prototypes, providing multi-granular relevance signals and a transparent rationale through retrieval paths. We instantiate two inference approaches: a generalized best-first search and a lightweight path-sum ranker. We evaluate our approaches on MS MARCO and QQP with encoder (e.g., BERT/T5) and decoder (GPT-2) representations. Our results show that our retrieval approaches match the dot product search on strong encoder embeddings while remaining robust when kNN degrades: with GPT-2 vectors, dot product performance collapses whereas our approaches still retrieve relevant results. Overall, our experiments suggest that Cobweb provides competitive effectiveness, improved robustness to embedding quality, scalability, and interpretable retrieval via hierarchical prototypes.</li>
</ul>

<h3>Title: Knowledge-Graph Based RAG System Evaluation Framework</h3>
<ul>
<li><strong>Authors: </strong>Sicheng Dong, Vahid Zolfaghari, Nenad Petrovic, Alois Knoll</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02549">https://arxiv.org/abs/2510.02549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02549">https://arxiv.org/pdf/2510.02549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02549]] Knowledge-Graph Based RAG System Evaluation Framework(https://arxiv.org/abs/2510.02549)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) has become a significant research focus and is utilized in various fields, such as text generation and dialog systems. One of the most essential applications of LLM is Retrieval Augmented Generation (RAG), which greatly enhances generated content's reliability and relevance. However, evaluating RAG systems remains a challenging task. Traditional evaluation metrics struggle to effectively capture the key features of modern LLM-generated content that often exhibits high fluency and naturalness. Inspired by the RAGAS tool, a well-known RAG evaluation framework, we extended this framework into a KG-based evaluation paradigm, enabling multi-hop reasoning and semantic community clustering to derive more comprehensive scoring metrics. By incorporating these comprehensive evaluation criteria, we gain a deeper understanding of RAG systems and a more nuanced perspective on their performance. To validate the effectiveness of our approach, we compare its performance with RAGAS scores and construct a human-annotated subset to assess the correlation between human judgments and automated metrics. In addition, we conduct targeted experiments to demonstrate that our KG-based evaluation method is more sensitive to subtle semantic differences in generated outputs. Finally, we discuss the key challenges in evaluating RAG systems and highlight potential directions for future research.</li>
</ul>

<h3>Title: ToolTweak: An Attack on Tool Selection in LLM-based Agents</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Sneh, Ruomei Yan, Jialin Yu, Philip Torr, Yarin Gal, Sunando Sengupta, Eric Sommerlade, Alasdair Paren, Adel Bibi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02554">https://arxiv.org/abs/2510.02554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02554">https://arxiv.org/pdf/2510.02554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02554]] ToolTweak: An Attack on Tool Selection in LLM-based Agents(https://arxiv.org/abs/2510.02554)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, fair</a></li>
<li><strong>Abstract: </strong>As LLMs increasingly power agents that interact with external tools, tool use has become an essential mechanism for extending their capabilities. These agents typically select tools from growing databases or marketplaces to solve user tasks, creating implicit competition among tool providers and developers for visibility and usage. In this paper, we show that this selection process harbors a critical vulnerability: by iteratively manipulating tool names and descriptions, adversaries can systematically bias agents toward selecting specific tools, gaining unfair advantage over equally capable alternatives. We present ToolTweak, a lightweight automatic attack that increases selection rates from a baseline of around 20% to as high as 81%, with strong transferability between open-source and closed-source models. Beyond individual tools, we show that such attacks cause distributional shifts in tool usage, revealing risks to fairness, competition, and security in emerging tool ecosystems. To mitigate these risks, we evaluate two defenses: paraphrasing and perplexity filtering, which reduce bias and lead agents to select functionally similar tools more equally. All code will be open-sourced upon acceptance.</li>
</ul>

<h3>Title: Who's Wearing? Ear Canal Biometric Key Extraction for User Authentication on Wireless Earbuds</h3>
<ul>
<li><strong>Authors: </strong>Chenpei Huang, Lingfeng Yao, Hui Zhong, Kyu In Lee, Lan Zhang, Xiaoyong Yuan, Tomoaki Ohtsuki, Miao Pan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02563">https://arxiv.org/abs/2510.02563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02563">https://arxiv.org/pdf/2510.02563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02563]] Who's Wearing? Ear Canal Biometric Key Extraction for User Authentication on Wireless Earbuds(https://arxiv.org/abs/2510.02563)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, attack, robust, biometric, extraction</a></li>
<li><strong>Abstract: </strong>Ear canal scanning/sensing (ECS) has emerged as a novel biometric authentication method for mobile devices paired with wireless earbuds. Existing studies have demonstrated the uniqueness of ear canals by training and testing machine learning classifiers on ECS data. However, implementing practical ECS-based authentication requires preventing raw biometric data leakage and designing computationally efficient protocols suitable for resource-constrained earbuds. To address these challenges, we propose an ear canal key extraction protocol, \textbf{EarID}. Without relying on classifiers, EarID extracts unique binary keys directly on the earbuds during authentication. These keys further allow the use of privacy-preserving fuzzy commitment scheme that verifies the wearer's key on mobile devices. Our evaluation results demonstrate that EarID achieves a 98.7\% authentication accuracy, comparable to machine learning classifiers. The mobile enrollment time (160~ms) and earbuds processing time (226~ms) are negligible in terms of wearer's experience. Moreover, our approach is robust and attack-resistant, maintaining a false acceptance rate below 1\% across all adversarial scenarios. We believe the proposed EarID offers a practical and secure solution for next-generation wireless earbuds.</li>
</ul>

<h3>Title: On The Expressive Power of GNN Derivatives</h3>
<ul>
<li><strong>Authors: </strong>Yam Eitan, Moshe Eliasof, Yoav Gelberg, Fabrizio Frasca, Guy Bar-Shalom, Haggai Maron</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02565">https://arxiv.org/abs/2510.02565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02565">https://arxiv.org/pdf/2510.02565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02565]] On The Expressive Power of GNN Derivatives(https://arxiv.org/abs/2510.02565)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Despite significant advances in Graph Neural Networks (GNNs), their limited expressivity remains a fundamental challenge. Research on GNN expressivity has produced many expressive architectures, leading to architecture hierarchies with models of increasing expressive power. Separately, derivatives of GNNs with respect to node features have been widely studied in the context of the oversquashing and over-smoothing phenomena, GNN explainability, and more. To date, these derivatives remain unexplored as a means to enhance GNN expressivity. In this paper, we show that these derivatives provide a natural way to enhance the expressivity of GNNs. We introduce High-Order Derivative GNN (HOD-GNN), a novel method that enhances the expressivity of Message Passing Neural Networks (MPNNs) by leveraging high-order node derivatives of the base model. These derivatives generate expressive structure-aware node embeddings processed by a second GNN in an end-to-end trainable architecture. Theoretically, we show that the resulting architecture family's expressive power aligns with the WL hierarchy. We also draw deep connections between HOD-GNN, Subgraph GNNs, and popular structural encoding schemes. For computational efficiency, we develop a message-passing algorithm for computing high-order derivatives of MPNNs that exploits graph sparsity and parallelism. Evaluations on popular graph learning benchmarks demonstrate HOD-GNN's strong performance on popular graph learning tasks.</li>
</ul>

<h3>Title: PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Qiao Feng, Yiming Huang, Yufu Wang, Jiatao Gu, Lingjie Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02566">https://arxiv.org/abs/2510.02566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02566">https://arxiv.org/pdf/2510.02566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02566]] PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction(https://arxiv.org/abs/2510.02566)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reconstructing physically plausible human motion from monocular videos remains a challenging problem in computer vision and graphics. Existing methods primarily focus on kinematics-based pose estimation, often leading to unrealistic results due to the lack of physical constraints. To address such artifacts, prior methods have typically relied on physics-based post-processing following the initial kinematics-based motion estimation. However, this two-stage design introduces error accumulation, ultimately limiting the overall reconstruction quality. In this paper, we present PhysHMR, a unified framework that directly learns a visual-to-action policy for humanoid control in a physics-based simulator, enabling motion reconstruction that is both physically grounded and visually aligned with the input video. A key component of our approach is the pixel-as-ray strategy, which lifts 2D keypoints into 3D spatial rays and transforms them into global space. These rays are incorporated as policy inputs, providing robust global pose guidance without depending on noisy 3D root predictions. This soft global grounding, combined with local visual features from a pretrained encoder, allows the policy to reason over both detailed pose and global positioning. To overcome the sample inefficiency of reinforcement learning, we further introduce a distillation scheme that transfers motion knowledge from a mocap-trained expert to the vision-conditioned policy, which is then refined using physically motivated reinforcement learning rewards. Extensive experiments demonstrate that PhysHMR produces high-fidelity, physically plausible motion across diverse scenarios, outperforming prior approaches in both visual accuracy and physical realism.</li>
</ul>

<h3>Title: Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tolúl\d{o}pé Ògúnrèmí, Christopher D. Manning, Dan Jurafsky, Karen Livescu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02569">https://arxiv.org/abs/2510.02569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02569">https://arxiv.org/pdf/2510.02569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02569]] Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models(https://arxiv.org/abs/2510.02569)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Spoken language models (SLMs) that integrate speech with large language models (LMs) rely on modality adapters (MAs) to map the output of speech encoders to a representation that is understandable to the decoder LM. Yet we know very little about how these crucial MAs transform representations. Here we examine the MA output representation in three SLMs (SALMONN, Qwen2-Audio and Phi-4-Multimodal-Instruct). By finding the nearest decoder LM token to an MA representation, we uncover two strategies for MA representations. For models using a Whisper encoder, MAs appear to represent the meaning of the input using an English-based interlingua, allowing them to handle languages unseen in instruction tuning. For models that don't, like Phi-4-Multimodal-Instruct, MAs instead represent the phonetics of the input, but expressed with English words. We hypothesise that which arises depends on whether the speech encoder is trained only for speech recognition or also for translation.</li>
</ul>

<h3>Title: How Confident are Video Models? Empowering Video Models to Express their Uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Zhiting Mei, Ola Shorinwa, Anirudha Majumdar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02571">https://arxiv.org/abs/2510.02571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02571">https://arxiv.org/pdf/2510.02571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02571]] How Confident are Video Models? Empowering Video Models to Express their Uncertainty(https://arxiv.org/abs/2510.02571)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative video models demonstrate impressive text-to-video capabilities, spurring widespread adoption in many real-world applications. However, like large language models (LLMs), video generation models tend to hallucinate, producing plausible videos even when they are factually wrong. Although uncertainty quantification (UQ) of LLMs has been extensively studied in prior work, no UQ method for video models exists, raising critical safety concerns. To our knowledge, this paper represents the first work towards quantifying the uncertainty of video models. We present a framework for uncertainty quantification of generative video models, consisting of: (i) a metric for evaluating the calibration of video models based on robust rank correlation estimation with no stringent modeling assumptions; (ii) a black-box UQ method for video models (termed S-QUBED), which leverages latent modeling to rigorously decompose predictive uncertainty into its aleatoric and epistemic components; and (iii) a UQ dataset to facilitate benchmarking calibration in video models. By conditioning the generation task in the latent space, we disentangle uncertainty arising due to vague task specifications from that arising from lack of knowledge. Through extensive experiments on benchmark video datasets, we demonstrate that S-QUBED computes calibrated total uncertainty estimates that are negatively correlated with the task accuracy and effectively computes the aleatoric and epistemic constituents.</li>
</ul>

<h3>Title: PEO: Training-Free Aesthetic Quality Enhancement in Pre-Trained Text-to-Image Diffusion Models with Prompt Embedding Optimization</h3>
<ul>
<li><strong>Authors: </strong>Hovhannes Margaryan, Bo Wan, Tinne Tuytelaars</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02599">https://arxiv.org/abs/2510.02599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02599">https://arxiv.org/pdf/2510.02599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02599]] PEO: Training-Free Aesthetic Quality Enhancement in Pre-Trained Text-to-Image Diffusion Models with Prompt Embedding Optimization(https://arxiv.org/abs/2510.02599)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel approach to aesthetic quality improvement in pre-trained text-to-image diffusion models when given a simple prompt. Our method, dubbed Prompt Embedding Optimization (PEO), leverages a pre-trained text-to-image diffusion model as a backbone and optimizes the text embedding of a given simple and uncurated prompt to enhance the visual quality of the generated image. We achieve this by a tripartite objective function that improves the aesthetic fidelity of the generated image, ensures adherence to the optimized text embedding, and minimal divergence from the initial prompt. The latter is accomplished through a prompt preservation term. Additionally, PEO is training-free and backbone-independent. Quantitative and qualitative evaluations confirm the effectiveness of the proposed method, exceeding or equating the performance of state-of-the-art text-to-image and prompt adaptation methods.</li>
</ul>

<h3>Title: Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Beijia Lu, Ziyi Chen, Jing Xiao, Jun-Yan Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02617">https://arxiv.org/abs/2510.02617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02617">https://arxiv.org/pdf/2510.02617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02617]] Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation(https://arxiv.org/abs/2510.02617)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models can synthesize realistic co-speech video from audio for various applications, such as video creation and virtual agents. However, existing diffusion-based methods are slow due to numerous denoising steps and costly attention mechanisms, preventing real-time deployment. In this work, we distill a many-step diffusion video model into a few-step student model. Unfortunately, directly applying recent diffusion distillation methods degrades video quality and falls short of real-time performance. To address these issues, our new video distillation method leverages input human pose conditioning for both attention and loss functions. We first propose using accurate correspondence between input human pose keypoints to guide attention to relevant regions, such as the speaker's face, hands, and upper body. This input-aware sparse attention reduces redundant computations and strengthens temporal correspondences of body parts, improving inference efficiency and motion coherence. To further enhance visual quality, we introduce an input-aware distillation loss that improves lip synchronization and hand motion realism. By integrating our input-aware sparse attention and distillation loss, our method achieves real-time performance with improved visual quality compared to recent audio-driven and input-driven methods. We also conduct extensive experiments showing the effectiveness of our algorithmic design choices.</li>
</ul>

<h3>Title: TabImpute: Accurate and Fast Zero-Shot Missing-Data Imputation with a Pre-Trained Transformer</h3>
<ul>
<li><strong>Authors: </strong>Jacob Feitelberg, Dwaipayan Saha, Kyuseong Choi, Zaid Ahmad, Anish Agarwal, Raaz Dwivedi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02625">https://arxiv.org/abs/2510.02625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02625">https://arxiv.org/pdf/2510.02625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02625]] TabImpute: Accurate and Fast Zero-Shot Missing-Data Imputation with a Pre-Trained Transformer(https://arxiv.org/abs/2510.02625)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>Missing data is a pervasive problem in tabular settings. Existing solutions range from simple averaging to complex generative adversarial networks. However, due to huge variance in performance across real-world domains and time-consuming hyperparameter tuning, no default imputation method exists. Building on TabPFN, a recent tabular foundation model for supervised learning, we propose TabImpute, a pre-trained transformer that delivers accurate and fast zero-shot imputations requiring no fitting or hyperparameter tuning at inference-time. To train and evaluate TabImpute, we introduce (i) an entry-wise featurization for tabular settings, which enables a $100\times$ speedup over the previous TabPFN imputation method, (ii) a synthetic training data generation pipeline incorporating realistic missingness patterns, which boosts test-time performance, and (iii) MissBench, a comprehensive benchmark for evaluation of imputation methods with $42$ OpenML datasets and $13$ missingness patterns. MissBench spans domains such as medicine, finance, and engineering, showcasing TabImpute's robust performance compared to $11$ established imputation methods.</li>
</ul>

<h3>Title: Evaluation Framework for Highlight Explanations of Context Utilisation in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jingyi Sun, Pepa Atanasova, Sagnik Ray Choudhury, Sekh Mainul Islam, Isabelle Augenstein</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02629">https://arxiv.org/abs/2510.02629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02629">https://arxiv.org/pdf/2510.02629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02629]] Evaluation Framework for Highlight Explanations of Context Utilisation in Language Models(https://arxiv.org/abs/2510.02629)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Context utilisation, the ability of Language Models (LMs) to incorporate relevant information from the provided context when generating responses, remains largely opaque to users, who cannot determine whether models draw from parametric memory or provided context, nor identify which specific context pieces inform the response. Highlight explanations (HEs) offer a natural solution as they can point the exact context pieces and tokens that influenced model outputs. However, no existing work evaluates their effectiveness in accurately explaining context utilisation. We address this gap by introducing the first gold standard HE evaluation framework for context attribution, using controlled test cases with known ground-truth context usage, which avoids the limitations of existing indirect proxy evaluations. To demonstrate the framework's broad applicability, we evaluate four HE methods -- three established techniques and MechLight, a mechanistic interpretability approach we adapt for this task -- across four context scenarios, four datasets, and five LMs. Overall, we find that MechLight performs best across all context scenarios. However, all methods struggle with longer contexts and exhibit positional biases, pointing to fundamental challenges in explanation accuracy that require new approaches to deliver reliable context utilisation explanations at scale.</li>
</ul>

<h3>Title: HyperAdaLoRA: Accelerating LoRA Rank Allocation During Training via Hypernetworks without Sacrificing Performance</h3>
<ul>
<li><strong>Authors: </strong>Hao Zhang, Zhenjia Li, Runfeng Bao, Yifan Gao, Xi Xiao, Bo Huang, Yuhang Wu, Tianyang Wang, Hao Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02630">https://arxiv.org/abs/2510.02630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02630">https://arxiv.org/pdf/2510.02630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02630]] HyperAdaLoRA: Accelerating LoRA Rank Allocation During Training via Hypernetworks without Sacrificing Performance(https://arxiv.org/abs/2510.02630)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation (LoRA), has emerged as a promising approach to fine-tuning large language models(LLMs) while reducing computational and memory overhead. However, LoRA assumes a uniform rank \textit{r} for each incremental matrix, not accounting for the varying significance of weight matrices across different modules and layers. AdaLoRA leverages Singular Value Decomposition (SVD) to parameterize updates and employs pruning of singular values to introduce dynamic rank allocation, thereby enhancing adaptability. However, during the training process, it often encounters issues of slow convergence speed and high computational overhead. To address this issue, we propose HyperAdaLoRA, a novel framework that accelerates the convergence of AdaLoRA by leveraging a hypernetwork. Instead of directly optimizing the components of Singular Value Decomposition $(P, \Lambda, Q)$, HyperAdaLoRA employs a hypernetwork based on attention mechanisms to dynamically generate these parameters. By pruning the outputs of the hypernetwork that generates the singular values, dynamic rank allocation is achieved. Comprehensive experiments on various datasets and models demonstrate that our method achieves faster convergence without sacrificing performance. Additionally, further extension experiments on other LoRA-based approaches validate the broad applicability of our method.</li>
</ul>

<h3>Title: Deep Generative Continual Learning using Functional LoRA: FunLoRA</h3>
<ul>
<li><strong>Authors: </strong>Victor Enescu, Hichem Sahbi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02631">https://arxiv.org/abs/2510.02631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02631">https://arxiv.org/pdf/2510.02631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02631]] Deep Generative Continual Learning using Functional LoRA: FunLoRA(https://arxiv.org/abs/2510.02631)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Continual adaptation of deep generative models holds tremendous potential and critical importance, given their rapid and expanding usage in text and vision based applications. Incremental training, however, remains highly challenging due to catastrophic forgetting phenomenon, which makes it difficult for neural networks to effectively incorporate new knowledge. A common strategy consists in retraining the generative model on its own synthetic data in order to mitigate forgetting. Yet, such an approach faces two major limitations: (i) the continually increasing training time eventually becomes intractable, and (ii) reliance on synthetic data inevitably leads to long-term performance degradation, since synthetic samples lack the richness of real training data. In this paper, we attenuate these issues by designing a novel and more expressive conditioning mechanism for generative models based on low rank adaptation (LoRA), that exclusively employs rank 1 matrices, whose reparametrized matrix rank is functionally increased using carefully selected functions -- and dubbed functional LoRA: FunLoRA. Using this dynamic conditioning, the generative model is guaranteed to avoid catastrophic forgetting and needs only to be trained on data from the current task. Extensive experiments using flow-matching based models trained from scratch, showcase that our proposed parameter-efficient fine-tuning (PEFT) method surpasses prior state-of-the-art results based on diffusion models, reaching higher classification accuracy scores, while only requiring a fraction of the memory cost and sampling time.</li>
</ul>

<h3>Title: Sequence-Preserving Dual-FoV Defense for Traffic Sign and Light Recognition in Autonomous Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Joshi, Jahnavi Krishna Koda, Abhishek Phadke</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02642">https://arxiv.org/abs/2510.02642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02642">https://arxiv.org/pdf/2510.02642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02642]] Sequence-Preserving Dual-FoV Defense for Traffic Sign and Light Recognition in Autonomous Vehicles(https://arxiv.org/abs/2510.02642)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Traffic light and sign recognition are key for Autonomous Vehicles (AVs) because perception mistakes directly influence navigation and safety. In addition to digital adversarial attacks, models are vulnerable to existing perturbations (glare, rain, dirt, or graffiti), which could lead to dangerous misclassifications. The current work lacks consideration of temporal continuity, multistatic field-of-view (FoV) sensing, and robustness to both digital and natural degradation. This study proposes a dual FoV, sequence-preserving robustness framework for traffic lights and signs in the USA based on a multi-source dataset built on aiMotive, Udacity, Waymo, and self-recorded videos from the region of Texas. Mid and long-term sequences of RGB images are temporally aligned for four operational design domains (ODDs): highway, night, rainy, and urban. Over a series of experiments on a real-life application of anomaly detection, this study outlines a unified three-layer defense stack framework that incorporates feature squeezing, defensive distillation, and entropy-based anomaly detection, as well as sequence-wise temporal voting for further enhancement. The evaluation measures included accuracy, attack success rate (ASR), risk-weighted misclassification severity, and confidence stability. Physical transferability was confirmed using probes for recapture. The results showed that the Unified Defense Stack achieved 79.8mAP and reduced the ASR to 18.2%, which is superior to YOLOv8, YOLOv9, and BEVFormer, while reducing the high-risk misclassification to 32%.</li>
</ul>

<h3>Title: Mind the Gap: Linguistic Divergence and Adaptation Strategies in Human-LLM Assistant vs. Human-Human Interactions</h3>
<ul>
<li><strong>Authors: </strong>Fulei Zhang, Zhou Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02645">https://arxiv.org/abs/2510.02645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02645">https://arxiv.org/pdf/2510.02645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02645]] Mind the Gap: Linguistic Divergence and Adaptation Strategies in Human-LLM Assistant vs. Human-Human Interactions(https://arxiv.org/abs/2510.02645)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) are increasingly deployed in customer-facing applications, a critical yet underexplored question is how users communicate differently with LLM chatbots compared to human agent. In this study, we present empirical evidence that users adopt distinct communication styles when users interact with chatbots versus human agents. Our analysis reveals significant differences in grammatical fluency, politeness, and lexical diversity in user language between the two settings. These findings suggest that models trained exclusively on human-human interaction data may not adequately accommodate the communication style shift that occurs once an LLM chatbot is deployed. To enhance LLM robustness to post-launch communication style changes, we experimented with two strategies: (1) data augmentation during the post-training phase and (2) inference-time user message reformulation. Our results indicate that models trained on stylistically diverse datasets significantly outperform those trained exclusively on original or stylistically uniform datasets, while inference-time reformulation proved less effective. These insights help us to better adapt our models for improved LLM-user interaction experiences.</li>
</ul>

<h3>Title: SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Rui Qi, Zhibo Man, Yufeng Chen, Fengran Mo, Jinan Xu, Kaiyu Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02648">https://arxiv.org/abs/2510.02648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02648">https://arxiv.org/pdf/2510.02648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02648]] SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models(https://arxiv.org/abs/2510.02648)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent developments have enabled Large Language Models (LLMs) to engage in complex reasoning tasks through deep thinking. However, the capacity of reasoning has not been successfully transferred to non-high-resource languages due to resource constraints, which struggles with multilingual reasoning tasks. To this end, we propose Structured-of-Thought (SoT), a training-free method that improves the performance on multilingual reasoning through a multi-step transformation: Language Thinking Transformation and Structured Knowledge Transformation. The SoT method converts language-specific semantic information into language-agnostic structured representations, enabling the models to understand the query in different languages more sophisticated. Besides, SoT effectively guides LLMs toward more concentrated reasoning to maintain consistent underlying reasoning pathways when handling cross-lingual variations in expression. Experimental results demonstrate that SoT outperforms several strong baselines on multiple multilingual reasoning benchmarks when adapting to various backbones of LLMs. It can also be integrated with other training-free strategies for further improvements. Our code is available at this https URL.</li>
</ul>

<h3>Title: TutorBench: A Benchmark To Assess Tutoring Capabilities Of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Rakshith S Srinivasa, Zora Che, Chen Bo Calvin Zhang, Diego Mares, Ernesto Hernandez, Jayeon Park, Dean Lee, Guillermo Mangialardi, Charmaine Ng, Ed-Yeremai Hernandez Cardona, Anisha Gunjal, Yunzhong He, Bing Liu, Chen Xing</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02663">https://arxiv.org/abs/2510.02663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02663">https://arxiv.org/pdf/2510.02663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02663]] TutorBench: A Benchmark To Assess Tutoring Capabilities Of Large Language Models(https://arxiv.org/abs/2510.02663)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As students increasingly adopt large language models (LLMs) as learning aids, it is crucial to build models that are adept at handling the nuances of tutoring: they need to identify the core needs of students, be adaptive, provide personalized guidance, and be accurate. To this end, we introduce TutorBench, a dataset and evaluation benchmark designed to rigorously evaluate the core tutoring skills of LLMs. The dataset comprises 1,490 samples curated by human experts, focused on high-school and AP-level curricula. The samples are drawn from three common tutoring tasks: (i) generating adaptive explanations tailored to a student's confusion, (ii) providing actionable feedback on a student's work, and (iii) promoting active learning through effective hint generation. To account for the inherent complexity of tutoring, samples are accompanied by sample-specific rubrics which are used to judge model responses during evaluation. TutorBench uses a reliable and fine-grained automatic evaluation method that uses an LLM-judge and the sample-specific rubrics. We evaluate 16 frontier LLMs on TutorBench and present a detailed analysis of their performance and behavior. Our results show that none of the frontier LLMs achieve a score of greater than $56\%$, showing a large room for improvement. We find that LLMs fall short in exhibiting the full range of tutoring skills needed to guide, diagnose, and support students effectively, with all the frontier models achieving less than a $60\%$ pass rate on rubric criteria related to these skills. We also find that different model families exhibit varied strengths and limitations: the Claude models outperform others in supporting active learning, while they lag behind in the other two use cases. By releasing TutorBench, we provide a comprehensive and unsaturated benchmark to guide the development of the next-generation of AI tutors.</li>
</ul>

<h3>Title: Self-Improvement in Multimodal Large Language Models: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Shijian Deng, Kai Wang, Tianyu Yang, Harsh Singh, Yapeng Tian</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02665">https://arxiv.org/abs/2510.02665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02665">https://arxiv.org/pdf/2510.02665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02665]] Self-Improvement in Multimodal Large Language Models: A Survey(https://arxiv.org/abs/2510.02665)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in self-improvement for Large Language Models (LLMs) have efficiently enhanced model capabilities without significantly increasing costs, particularly in terms of human effort. While this area is still relatively young, its extension to the multimodal domain holds immense potential for leveraging diverse data sources and developing more general self-improving models. This survey is the first to provide a comprehensive overview of self-improvement in Multimodal LLMs (MLLMs). We provide a structured overview of the current literature and discuss methods from three perspectives: 1) data collection, 2) data organization, and 3) model optimization, to facilitate the further development of self-improvement in MLLMs. We also include commonly used evaluations and downstream applications. Finally, we conclude by outlining open challenges and future research directions.</li>
</ul>

<h3>Title: Uncertainty as Feature Gaps: Epistemic Uncertainty Quantification of LLMs in Contextual Question-Answering</h3>
<ul>
<li><strong>Authors: </strong>Yavuz Bakman, Sungmin Kang, Zhiqi Huang, Duygu Nur Yaldiz, Catarina G. Belém, Chenyang Zhu, Anoop Kumar, Alfy Samuel, Salman Avestimehr, Daben Liu, Sai Praneeth Karimireddy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02671">https://arxiv.org/abs/2510.02671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02671">https://arxiv.org/pdf/2510.02671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02671]] Uncertainty as Feature Gaps: Epistemic Uncertainty Quantification of LLMs in Contextual Question-Answering(https://arxiv.org/abs/2510.02671)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Uncertainty Quantification (UQ) research has primarily focused on closed-book factual question answering (QA), while contextual QA remains unexplored, despite its importance in real-world applications. In this work, we focus on UQ for the contextual QA task and propose a theoretically grounded approach to quantify epistemic uncertainty. We begin by introducing a task-agnostic, token-level uncertainty measure defined as the cross-entropy between the predictive distribution of the given model and the unknown true distribution. By decomposing this measure, we isolate the epistemic component and approximate the true distribution by a perfectly prompted, idealized model. We then derive an upper bound for epistemic uncertainty and show that it can be interpreted as semantic feature gaps in the given model's hidden representations relative to the ideal model. We further apply this generic framework to the contextual QA task and hypothesize that three features approximate this gap: context-reliance (using the provided context rather than parametric knowledge), context comprehension (extracting relevant information from context), and honesty (avoiding intentional lies). Using a top-down interpretability approach, we extract these features by using only a small number of labeled samples and ensemble them to form a robust uncertainty score. Experiments on multiple QA benchmarks in both in-distribution and out-of-distribution settings show that our method substantially outperforms state-of-the-art unsupervised (sampling-free and sampling-based) and supervised UQ methods, achieving up to a 13-point PRR improvement while incurring a negligible inference overhead.</li>
</ul>

<h3>Title: To Compress or Not? Pushing the Frontier of Lossless GenAI Model Weights Compression with Exponent Concentration</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Yang, Tianyi Zhang, Jianwen Xie, Chuan Li, Zhaozhuo Xu, Anshumali Shrivastava</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02676">https://arxiv.org/abs/2510.02676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02676">https://arxiv.org/pdf/2510.02676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02676]] To Compress or Not? Pushing the Frontier of Lossless GenAI Model Weights Compression with Exponent Concentration(https://arxiv.org/abs/2510.02676)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The scaling of Generative AI (GenAI) models into the hundreds of billions of parameters makes low-precision computation indispensable for efficient deployment. We argue that the fundamental solution lies in developing low-precision floating-point formats, which inherently provide numerical stability, memory savings, and hardware efficiency without dequantization overhead. In this paper, we present a theoretical and empirical study of an exponent concentration phenomenon in GenAI weights: exponents consistently exhibit low entropy across architectures and modalities. We show that this arises naturally from $\alpha$-stable distributions induced by stochastic gradient descent, and we prove tight bounds on the entropy of exponents. Our analysis establishes a theoretical compression limit near FP4.67, which motivates the design of a practical FP8 format. Building on these insights, we propose Exponent-Concentrated FP8 (ECF8), a lossless compression framework with entropy-aware encoding and GPU-optimized decoding. Experiments on LLMs and DiTs up to 671B parameters demonstrate up to 26.9% memory savings and 177.1% throughput acceleration, with perfectly lossless computations, i.e., no deviation in model outputs. Our results establish exponent concentration as a statistical law of trained models and open a principled path for lossless low-precision floating-point design in the FP8 era.</li>
</ul>

<h3>Title: EvoSpeak: Large Language Models for Interpretable Genetic Programming-Evolved Heuristics</h3>
<ul>
<li><strong>Authors: </strong>Meng Xu, Jiao Liu, Yew Soon Ong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02686">https://arxiv.org/abs/2510.02686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02686">https://arxiv.org/pdf/2510.02686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02686]] EvoSpeak: Large Language Models for Interpretable Genetic Programming-Evolved Heuristics(https://arxiv.org/abs/2510.02686)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative, large language model</a></li>
<li><strong>Abstract: </strong>Genetic programming (GP) has demonstrated strong effectiveness in evolving tree-structured heuristics for complex optimization problems. Yet, in dynamic and large-scale scenarios, the most effective heuristics are often highly complex, hindering interpretability, slowing convergence, and limiting transferability across tasks. To address these challenges, we present EvoSpeak, a novel framework that integrates GP with large language models (LLMs) to enhance the efficiency, transparency, and adaptability of heuristic evolution. EvoSpeak learns from high-quality GP heuristics, extracts knowledge, and leverages this knowledge to (i) generate warm-start populations that accelerate convergence, (ii) translate opaque GP trees into concise natural-language explanations that foster interpretability and trust, and (iii) enable knowledge transfer and preference-aware heuristic generation across related tasks. We verify the effectiveness of EvoSpeak through extensive experiments on dynamic flexible job shop scheduling (DFJSS), under both single- and multi-objective formulations. The results demonstrate that EvoSpeak produces more effective heuristics, improves evolutionary efficiency, and delivers human-readable reports that enhance usability. By coupling the symbolic reasoning power of GP with the interpretative and generative strengths of LLMs, EvoSpeak advances the development of intelligent, transparent, and user-aligned heuristics for real-world optimization problems.</li>
</ul>

<h3>Title: FSFSplatter: Build Surface and Novel Views with Sparse-Views within 3min</h3>
<ul>
<li><strong>Authors: </strong>Yibin Zhao, Yihan Pan, Jun Nan, Jianjun Yi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02691">https://arxiv.org/abs/2510.02691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02691">https://arxiv.org/pdf/2510.02691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02691]] FSFSplatter: Build Surface and Novel Views with Sparse-Views within 3min(https://arxiv.org/abs/2510.02691)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Gaussian Splatting has become a leading reconstruction technique, known for its high-quality novel view synthesis and detailed reconstruction. However, most existing methods require dense, calibrated views. Reconstructing from free sparse images often leads to poor surface due to limited overlap and overfitting. We introduce FSFSplatter, a new approach for fast surface reconstruction from free sparse images. Our method integrates end-to-end dense Gaussian initialization, camera parameter estimation, and geometry-enhanced scene optimization. Specifically, FSFSplatter employs a large Transformer to encode multi-view images and generates a dense and geometrically consistent Gaussian scene initialization via a self-splitting Gaussian head. It eliminates local floaters through contribution-based pruning and mitigates overfitting to limited views by leveraging depth and multi-view feature supervision with differentiable camera parameters during rapid optimization. FSFSplatter outperforms current state-of-the-art methods on widely used DTU and Replica.</li>
</ul>

<h3>Title: Fine-Tuning Diffusion Models via Intermediate Distribution Shaping</h3>
<ul>
<li><strong>Authors: </strong>Gautham Govind Anil, Shaan Ul Haque, Nithish Kannen, Dheeraj Nagaraj, Sanjay Shakkottai, Karthikeyan Shanmugam</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02692">https://arxiv.org/abs/2510.02692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02692">https://arxiv.org/pdf/2510.02692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02692]] Fine-Tuning Diffusion Models via Intermediate Distribution Shaping(https://arxiv.org/abs/2510.02692)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are widely used for generative tasks across domains. While pre-trained diffusion models effectively capture the training data distribution, it is often desirable to shape these distributions using reward functions to align with downstream applications. Policy gradient methods, such as Proximal Policy Optimization (PPO), are widely used in the context of autoregressive generation. However, the marginal likelihoods required for such methods are intractable for diffusion models, leading to alternative proposals and relaxations. In this context, we unify variants of Rejection sAmpling based Fine-Tuning (RAFT) as GRAFT, and show that this implicitly performs PPO with reshaped rewards. We then introduce P-GRAFT to shape distributions at intermediate noise levels and demonstrate empirically that this can lead to more effective fine-tuning. We mathematically explain this via a bias-variance tradeoff. Motivated by this, we propose inverse noise correction to improve flow models without leveraging explicit rewards. We empirically evaluate our methods on text-to-image(T2I) generation, layout generation, molecule generation and unconditional image generation. Notably, our framework, applied to Stable Diffusion 2, improves over policy gradient methods on popular T2I benchmarks in terms of VQAScore and shows an $8.81\%$ relative improvement over the base model. For unconditional image generation, inverse noise correction improves FID of generated images at lower FLOPs/image.</li>
</ul>

<h3>Title: MALF: A Multi-Agent LLM Framework for Intelligent Fuzzing of Industrial Control Protocols</h3>
<ul>
<li><strong>Authors: </strong>Bowei Ning, Xuejun Zong, Kan He</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02694">https://arxiv.org/abs/2510.02694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02694">https://arxiv.org/pdf/2510.02694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02694]] MALF: A Multi-Agent LLM Framework for Intelligent Fuzzing of Industrial Control Protocols(https://arxiv.org/abs/2510.02694)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Industrial control systems (ICS) are vital to modern infrastructure but increasingly vulnerable to cybersecurity threats, particularly through weaknesses in their communication protocols. This paper presents MALF (Multi-Agent LLM Fuzzing Framework), an advanced fuzzing solution that integrates large language models (LLMs) with multi-agent coordination to identify vulnerabilities in industrial control protocols (ICPs). By leveraging Retrieval-Augmented Generation (RAG) for domain-specific knowledge and QLoRA fine-tuning for protocol-aware input generation, MALF enhances fuzz testing precision and adaptability. The multi-agent framework optimizes seed generation, mutation strategies, and feedback-driven refinement, leading to improved vulnerability discovery. Experiments on protocols like Modbus/TCP, S7Comm, and Ethernet/IP demonstrate that MALF surpasses traditional methods, achieving a test case pass rate (TCPR) of 88-92% and generating more exception triggers (ETN). MALF also maintains over 90% seed coverage and Shannon entropy values between 4.2 and 4.6 bits, ensuring diverse, protocol-compliant mutations. Deployed in a real-world Industrial Attack-Defense Range for power plants, MALF identified critical vulnerabilities, including three zero-day flaws, one confirmed and registered by CNVD. These results validate MALF's effectiveness in real-world fuzzing applications. This research highlights the transformative potential of multi-agent LLMs in ICS cybersecurity, offering a scalable, automated framework that sets a new standard for vulnerability discovery and strengthens critical infrastructure security against emerging threats.</li>
</ul>

<h3>Title: RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization</h3>
<ul>
<li><strong>Authors: </strong>Kai Fukazawa, Kunal Mundada, Iman Soltani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02695">https://arxiv.org/abs/2510.02695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02695">https://arxiv.org/pdf/2510.02695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02695]] RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization(https://arxiv.org/abs/2510.02695)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In safety-critical domains where online data collection is infeasible, offline reinforcement learning (RL) offers an attractive alternative but only if policies deliver high returns without incurring catastrophic lower-tail risk. Prior work on risk-averse offline RL achieves safety at the cost of value conservatism and restricted policy classes, whereas expressive policies are only used in risk-neutral settings. Here, we address this gap by introducing the \textbf{Risk-Aware Multimodal Actor-Critic (RAMAC)} framework, which couples an \emph{expressive generative actor} with a distributional critic. The RAMAC differentiates composite objective combining distributional risk and BC loss through the generative path, achieving risk-sensitive learning in complex multimodal scenarios. We instantiate RAMAC with diffusion and flow-matching actors and observe consistent gains in $\mathrm{CVaR}_{0.1}$ while maintaining strong returns on most Stochastic-D4RL tasks. Code: this https URL</li>
</ul>

<h3>Title: A Statistical Method for Attack-Agnostic Adversarial Attack Detection with Compressive Sensing Comparison</h3>
<ul>
<li><strong>Authors: </strong>Chinthana Wimalasuriya, Spyros Tragoudas</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02707">https://arxiv.org/abs/2510.02707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02707">https://arxiv.org/pdf/2510.02707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02707]] A Statistical Method for Attack-Agnostic Adversarial Attack Detection with Compressive Sensing Comparison(https://arxiv.org/abs/2510.02707)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Adversarial attacks present a significant threat to modern machine learning systems. Yet, existing detection methods often lack the ability to detect unseen attacks or detect different attack types with a high level of accuracy. In this work, we propose a statistical approach that establishes a detection baseline before a neural network's deployment, enabling effective real-time adversarial detection. We generate a metric of adversarial presence by comparing the behavior of a compressed/uncompressed neural network pair. Our method has been tested against state-of-the-art techniques, and it achieves near-perfect detection across a wide range of attack types. Moreover, it significantly reduces false positives, making it both reliable and practical for real-world applications.</li>
</ul>

<h3>Title: A Novel Unified Lightweight Temporal-Spatial Transformer Approach for Intrusion Detection in Drone Networks</h3>
<ul>
<li><strong>Authors: </strong>Tarun Kumar Biswas, Ashrafun Zannat, Waqas Ishtiaq, Md. Alamgir Hossain</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02711">https://arxiv.org/abs/2510.02711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02711">https://arxiv.org/pdf/2510.02711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02711]] A Novel Unified Lightweight Temporal-Spatial Transformer Approach for Intrusion Detection in Drone Networks(https://arxiv.org/abs/2510.02711)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, transformer</a></li>
<li><strong>Abstract: </strong>The growing integration of drones across commercial, industrial, and civilian domains has introduced significant cybersecurity challenges, particularly due to the susceptibility of drone networks to a wide range of cyberattacks. Existing intrusion detection mechanisms often lack the adaptability, efficiency, and generalizability required for the dynamic and resource constrained environments in which drones operate. This paper proposes TSLT-Net, a novel lightweight and unified Temporal Spatial Transformer based intrusion detection system tailored specifically for drone networks. By leveraging self attention mechanisms, TSLT-Net effectively models both temporal patterns and spatial dependencies in network traffic, enabling accurate detection of diverse intrusion types. The framework includes a streamlined preprocessing pipeline and supports both multiclass attack classification and binary anomaly detection within a single architecture. Extensive experiments conducted on the ISOT Drone Anomaly Detection Dataset, consisting of more than 2.3 million labeled records, demonstrate the superior performance of TSLT-Net with 99.99 percent accuracy in multiclass detection and 100 percent in binary anomaly detection, while maintaining a minimal memory footprint of only 0.04 MB and 9722 trainable parameters. These results establish TSLT-Net as an effective and scalable solution for real time drone cybersecurity, particularly suitable for deployment on edge devices in mission critical UAV systems.</li>
</ul>

<h3>Title: Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Yubo Li, Ramayya Krishnan, Rema Padman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02712">https://arxiv.org/abs/2510.02712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02712">https://arxiv.org/pdf/2510.02712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02712]] Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks(https://arxiv.org/abs/2510.02712)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized conversational AI, yet their robustness in extended multi-turn dialogues remains poorly understood. Existing evaluation frameworks focus on static benchmarks and single-turn assessments, failing to capture the temporal dynamics of conversational degradation that characterize real-world interactions. In this work, we present the first comprehensive survival analysis of conversational AI robustness, analyzing 36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a time-to-event process. Our survival modeling framework-employing Cox proportional hazards, Accelerated Failure Time, and Random Survival Forest approaches-reveals extraordinary temporal dynamics. We find that abrupt, prompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing the hazard of conversational failure. In stark contrast, gradual, cumulative drift is highly protective, vastly reducing the failure hazard and enabling significantly longer dialogues. AFT models with interactions demonstrate superior performance, achieving excellent discrimination and exceptional calibration. These findings establish survival analysis as a powerful paradigm for evaluating LLM robustness, offer concrete insights for designing resilient conversational agents, and challenge prevailing assumptions about the necessity of semantic consistency in conversational AI Systems.</li>
</ul>

<h3>Title: CST-AFNet: A dual attention-based deep learning framework for intrusion detection in IoT networks</h3>
<ul>
<li><strong>Authors: </strong>Waqas Ishtiaq, Ashrafun Zannat, A.H.M. Shahariar Parvez, Md. Alamgir Hossain, Muntasir Hasan Kanchan, Muhammad Masud Tarek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02717">https://arxiv.org/abs/2510.02717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02717">https://arxiv.org/pdf/2510.02717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02717]] CST-AFNet: A dual attention-based deep learning framework for intrusion detection in IoT networks(https://arxiv.org/abs/2510.02717)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust, extraction</a></li>
<li><strong>Abstract: </strong>The rapid expansion of the Internet of Things (IoT) has revolutionized modern industries by enabling smart automation and real time connectivity. However, this evolution has also introduced complex cybersecurity challenges due to the heterogeneous, resource constrained, and distributed nature of these environments. To address these challenges, this research presents CST AFNet, a novel dual attention based deep learning framework specifically designed for robust intrusion detection in IoT networks. The model integrates multi scale Convolutional Neural Networks (CNNs) for spatial feature extraction, Bidirectional Gated Recurrent Units (BiGRUs) for capturing temporal dependencies, and a dual attention mechanism, channel and temporal attention, to enhance focus on critical patterns in the data. The proposed method was trained and evaluated on the Edge IIoTset dataset, a comprehensive and realistic benchmark containing more than 2.2 million labeled instances spanning 15 attack types and benign traffic, collected from a seven layer industrial testbed. Our proposed model achieves outstanding accuracy for both 15 attack types and benign traffic. CST AFNet achieves 99.97 percent accuracy. Moreover, this model demonstrates exceptional performance with macro averaged precision, recall, and F1 score all above 99.3 percent. Experimental results show that CST AFNet achieves superior detection accuracy, significantly outperforming traditional deep learning models. The findings confirm that CST AFNet is a powerful and scalable solution for real time cyber threat detection in complex IoT and IIoT environments, paving the way for more secure, intelligent, and adaptive cyber physical systems.</li>
</ul>

<h3>Title: MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context</h3>
<ul>
<li><strong>Authors: </strong>Junyu Shi, Yong Sun, Zhiyuan Zhang, Lijiang Liu, Zhengjie Zhang, Yuxin He, Qiang Nie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02722">https://arxiv.org/abs/2510.02722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02722">https://arxiv.org/pdf/2510.02722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02722]] MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context(https://arxiv.org/abs/2510.02722)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Existing text-driven motion generation methods often treat synthesis as a bidirectional mapping between language and motion, but remain limited in capturing the causal logic of action execution and the human intentions that drive behavior. The absence of visual grounding further restricts precision and personalization, as language alone cannot specify fine-grained spatiotemporal details. We propose MoGIC, a unified framework that integrates intention modeling and visual priors into multimodal motion synthesis. By jointly optimizing multimodal-conditioned motion generation and intention prediction, MoGIC uncovers latent human goals, leverages visual priors to enhance generation, and exhibits versatile multimodal generative capability. We further introduce a mixture-of-attention mechanism with adaptive scope to enable effective local alignment between conditional tokens and motion subsequences. To support this paradigm, we curate Mo440H, a 440-hour benchmark from 21 high-quality motion datasets. Experiments show that after finetuning, MoGIC reduces FID by 38.6\% on HumanML3D and 34.6\% on Mo440H, surpasses LLM-based methods in motion captioning with a lightweight text head, and further enables intention prediction and vision-conditioned generation, advancing controllable motion synthesis and intention understanding. The code is available at this https URL</li>
</ul>

<h3>Title: PGMEL: Policy Gradient-based Generative Adversarial Network for Multimodal Entity Linking</h3>
<ul>
<li><strong>Authors: </strong>KM Pooja, Cheng Long, Aixin Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02726">https://arxiv.org/abs/2510.02726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02726">https://arxiv.org/pdf/2510.02726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02726]] PGMEL: Policy Gradient-based Generative Adversarial Network for Multimodal Entity Linking(https://arxiv.org/abs/2510.02726)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The task of entity linking, which involves associating mentions with their respective entities in a knowledge graph, has received significant attention due to its numerous potential applications. Recently, various multimodal entity linking (MEL) techniques have been proposed, targeted to learn comprehensive embeddings by leveraging both text and vision modalities. The selection of high-quality negative samples can potentially play a crucial role in metric/representation learning. However, to the best of our knowledge, this possibility remains unexplored in existing literature within the framework of MEL. To fill this gap, we address the multimodal entity linking problem in a generative adversarial setting where the generator is responsible for generating high-quality negative samples, and the discriminator is assigned the responsibility for the metric learning tasks. Since the generator is involved in generating samples, which is a discrete process, we optimize it using policy gradient techniques and propose a policy gradient-based generative adversarial network for multimodal entity linking (PGMEL). Experimental results based on Wiki-MEL, Richpedia-MEL and WikiDiverse datasets demonstrate that PGMEL learns meaningful representation by selecting challenging negative samples and outperforms state-of-the-art methods.</li>
</ul>

<h3>Title: Dale meets Langevin: A Multiplicative Denoising Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Nishanth Shetty, Madhava Prasath, Chandra Sekhar Seelamantula</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02730">https://arxiv.org/abs/2510.02730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02730">https://arxiv.org/pdf/2510.02730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02730]] Dale meets Langevin: A Multiplicative Denoising Diffusion Model(https://arxiv.org/abs/2510.02730)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Gradient descent has proven to be a powerful and effective technique for optimization in numerous machine learning applications. Recent advances in computational neuroscience have shown that learning in standard gradient descent optimization formulation is not consistent with learning in biological systems. This has opened up interesting avenues for building biologically inspired learning techniques. One such approach is inspired by Dale's law, which states that inhibitory and excitatory synapses do not swap roles during the course of learning. The resulting exponential gradient descent optimization scheme leads to log-normally distributed synaptic weights. Interestingly, the density that satisfies the Fokker-Planck equation corresponding to the stochastic differential equation (SDE) with geometric Brownian motion (GBM) is the log-normal density. Leveraging this connection, we start with the SDE governing geometric Brownian motion, and show that discretizing the corresponding reverse-time SDE yields a multiplicative update rule, which surprisingly, coincides with the sampling equivalent of the exponential gradient descent update founded on Dale's law. Furthermore, we propose a new formalism for multiplicative denoising score-matching, subsuming the loss function proposed by Hyvaerinen for non-negative data. Indeed, log-normally distributed data is positive and the proposed score-matching formalism turns out to be a natural fit. This allows for training of score-based models for image data and results in a novel multiplicative update scheme for sample generation starting from a log-normal density. Experimental results on MNIST, Fashion MNIST, and Kuzushiji datasets demonstrate generative capability of the new scheme. To the best of our knowledge, this is the first instance of a biologically inspired generative model employing multiplicative updates, founded on geometric Brownian motion.</li>
</ul>

<h3>Title: Hybrid-Collaborative Augmentation and Contrastive Sample Adaptive-Differential Awareness for Robust Attributed Graph Clustering</h3>
<ul>
<li><strong>Authors: </strong>Tianxiang Zhao, Youqing Wang, Jinlu Wang, Jiapu Wang, Mingliang Cui, Junbin Gao, Jipeng Guo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02731">https://arxiv.org/abs/2510.02731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02731">https://arxiv.org/pdf/2510.02731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02731]] Hybrid-Collaborative Augmentation and Contrastive Sample Adaptive-Differential Awareness for Robust Attributed Graph Clustering(https://arxiv.org/abs/2510.02731)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Due to its powerful capability of self-supervised representation learning and clustering, contrastive attributed graph clustering (CAGC) has achieved great success, which mainly depends on effective data augmentation and contrastive objective setting. However, most CAGC methods utilize edges as auxiliary information to obtain node-level embedding representation and only focus on node-level embedding augmentation. This approach overlooks edge-level embedding augmentation and the interactions between node-level and edge-level embedding augmentations across various granularity. Moreover, they often treat all contrastive sample pairs equally, neglecting the significant differences between hard and easy positive-negative sample pairs, which ultimately limits their discriminative capability. To tackle these issues, a novel robust attributed graph clustering (RAGC), incorporating hybrid-collaborative augmentation (HCA) and contrastive sample adaptive-differential awareness (CSADA), is proposed. First, node-level and edge-level embedding representations and augmentations are simultaneously executed to establish a more comprehensive similarity measurement criterion for subsequent contrastive learning. In turn, the discriminative similarity further consciously guides edge augmentation. Second, by leveraging pseudo-label information with high confidence, a CSADA strategy is elaborately designed, which adaptively identifies all contrastive sample pairs and differentially treats them by an innovative weight modulation function. The HCA and CSADA modules mutually reinforce each other in a beneficent cycle, thereby enhancing discriminability in representation learning. Comprehensive graph clustering evaluations over six benchmark datasets demonstrate the effectiveness of the proposed RAGC against several state-of-the-art CAGC methods.</li>
</ul>

<h3>Title: Net2Net: When Un-trained Meets Pre-trained Networks for Robust Real-World Denoising</h3>
<ul>
<li><strong>Authors: </strong>Weimin Yuan, Cai Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02733">https://arxiv.org/abs/2510.02733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02733">https://arxiv.org/pdf/2510.02733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02733]] Net2Net: When Un-trained Meets Pre-trained Networks for Robust Real-World Denoising(https://arxiv.org/abs/2510.02733)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Traditional denoising methods for noise removal have largely relied on handcrafted priors, often perform well in controlled environments but struggle to address the complexity and variability of real noise. In contrast, deep learning-based approaches have gained prominence for learning noise characteristics from large datasets, but these methods frequently require extensive labeled data and may not generalize effectively across diverse noise types and imaging conditions. In this paper, we present an innovative method, termed as Net2Net, that combines the strengths of untrained and pre-trained networks to tackle the challenges of real-world noise removal. The innovation of Net2Net lies in its combination of unsupervised DIP and supervised pre-trained model DRUNet by regularization by denoising (RED). The untrained network adapts to the unique noise characteristics of each input image without requiring labeled data, while the pre-trained network leverages learned representations from large-scale datasets to deliver robust denoising performance. This hybrid framework enhances generalization across varying noise patterns and improves performance, particularly in scenarios with limited training data. Extensive experiments on benchmark datasets demonstrate the superiority of our method for real-world noise removal.</li>
</ul>

<h3>Title: IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using Contrastive Embedding Similarity in the Indian Context</h3>
<ul>
<li><strong>Authors: </strong>Santhosh G S, Akshay Govind S, Gokul S Krishnan, Balaraman Ravindran, Sriraam Natarajan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02742">https://arxiv.org/abs/2510.02742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02742">https://arxiv.org/pdf/2510.02742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02742]] IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using Contrastive Embedding Similarity in the Indian Context(https://arxiv.org/abs/2510.02742)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have gained significant traction across critical domains owing to their impressive contextual understanding and generative capabilities. However, their increasing deployment in high stakes applications necessitates rigorous evaluation of embedded biases, particularly in culturally diverse contexts like India where existing embedding-based bias assessment methods often fall short in capturing nuanced stereotypes. We propose an evaluation framework based on a encoder trained using contrastive learning that captures fine-grained bias through embedding similarity. We also introduce a novel dataset - IndiCASA (IndiBias-based Contextually Aligned Stereotypes and Anti-stereotypes) comprising 2,575 human-validated sentences spanning five demographic axes: caste, gender, religion, disability, and socioeconomic status. Our evaluation of multiple open-weight LLMs reveals that all models exhibit some degree of stereotypical bias, with disability related biases being notably persistent, and religion bias generally lower likely due to global debiasing efforts demonstrating the need for fairer model development.</li>
</ul>

<h3>Title: The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback</h3>
<ul>
<li><strong>Authors: </strong>Hangfan Zhang, Siyuan Xu, Zhimeng Guo, Huaisheng Zhu, Shicheng Liu, Xinrun Wang, Qiaosheng Zhang, Yang Chen, Peng Ye, Lei Bai, Shuyue Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02752">https://arxiv.org/abs/2510.02752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02752">https://arxiv.org/pdf/2510.02752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02752]] The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback(https://arxiv.org/abs/2510.02752)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has demonstrated potential in enhancing the reasoning capabilities of large language models (LLMs), but such training typically demands substantial efforts in creating and annotating data. In this work, we explore improving LLMs through RL with minimal data. Our approach alternates between the LLM proposing a task and then attempting to solve it. To minimize data dependency, we introduce two novel mechanisms grounded in self-awareness: (1) self-aware difficulty prediction, where the model learns to assess task difficulty relative to its own abilities and prioritize challenging yet solvable tasks, and (2) self-aware limit breaking, where the model recognizes when a task is beyond its capability boundary and proactively requests external data to break through that limit. Extensive experiments on nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra data demonstrate the efficacy of self-aware RL and underscore the promise of self-evolving agent training.</li>
</ul>

<h3>Title: Curl Descent: Non-Gradient Learning Dynamics with Sign-Diverse Plasticity</h3>
<ul>
<li><strong>Authors: </strong>Hugo Ninou, Jonathan Kadmon, N. Alex Cayco-Gajic</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02765">https://arxiv.org/abs/2510.02765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02765">https://arxiv.org/pdf/2510.02765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02765]] Curl Descent: Non-Gradient Learning Dynamics with Sign-Diverse Plasticity(https://arxiv.org/abs/2510.02765)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Gradient-based algorithms are a cornerstone of artificial neural network training, yet it remains unclear whether biological neural networks use similar gradient-based strategies during learning. Experiments often discover a diversity of synaptic plasticity rules, but whether these amount to an approximation to gradient descent is unclear. Here we investigate a previously overlooked possibility: that learning dynamics may include fundamentally non-gradient "curl"-like components while still being able to effectively optimize a loss function. Curl terms naturally emerge in networks with inhibitory-excitatory connectivity or Hebbian/anti-Hebbian plasticity, resulting in learning dynamics that cannot be framed as gradient descent on any objective. To investigate the impact of these curl terms, we analyze feedforward networks within an analytically tractable student-teacher framework, systematically introducing non-gradient dynamics through neurons exhibiting rule-flipped plasticity. Small curl terms preserve the stability of the original solution manifold, resulting in learning dynamics similar to gradient descent. Beyond a critical value, strong curl terms destabilize the solution manifold. Depending on the network architecture, this loss of stability can lead to chaotic learning dynamics that destroy performance. In other cases, the curl terms can counterintuitively speed learning compared to gradient descent by allowing the weight dynamics to escape saddles by temporarily ascending the loss. Our results identify specific architectures capable of supporting robust learning via diverse learning rules, providing an important counterpoint to normative theories of gradient-based learning in neural networks.</li>
</ul>

<h3>Title: A Granular Study of Safety Pretraining under Model Abliteration</h3>
<ul>
<li><strong>Authors: </strong>Shashank Agnihotri, Jonas Jakubassa, Priyam Dey, Sachin Goyal, Bernt Schiele, Venkatesh Babu Radhakrishnan, Margret Keuper</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02768">https://arxiv.org/abs/2510.02768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02768">https://arxiv.org/pdf/2510.02768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02768]] A Granular Study of Safety Pretraining under Model Abliteration(https://arxiv.org/abs/2510.02768)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Open-weight LLMs can be modified at inference time with simple activation edits, which raises a practical question for safety: do common safety interventions like refusal training or metatag training survive such edits? We study model abliteration, a lightweight projection technique designed to remove refusal-sensitive directions, and conduct a controlled evaluation across a granular sequence of Safety Pretraining checkpoints for SmolLM2-1.7B, alongside widely used open baselines. For each of 20 systems, original and abliterated, we issue 100 prompts with balanced harmful and harmless cases, classify responses as **Refusal** or **Non-Refusal** using multiple judges, and validate judge fidelity on a small human-labeled subset. We also probe whether models can identify refusal in their own outputs. Our study produces a checkpoint-level characterization of which data-centric safety components remain robust under abliteration, quantifies how judge selection influences evaluation outcomes, and outlines a practical protocol for integrating inference-time edits into safety assessments. Code: this https URL.</li>
</ul>

<h3>Title: AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding</h3>
<ul>
<li><strong>Authors: </strong>Xian Zhang, Zexi Wu, Zinuo Li, Hongming Xu, Luqi Gong, Farid Boussaid, Naoufel Werghi, Mohammed Bennamoun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02778">https://arxiv.org/abs/2510.02778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02778">https://arxiv.org/pdf/2510.02778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02778]] AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding(https://arxiv.org/abs/2510.02778)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding long-form videos remains a significant challenge for vision--language models (VLMs) due to their extensive temporal length and high information density. Most current multimodal large language models (MLLMs) rely on uniform sampling, which often overlooks critical moments, leading to incorrect responses to queries. In parallel, many keyframe selection approaches impose rigid temporal spacing: once a frame is chosen, an exclusion window suppresses adjacent timestamps to reduce redundancy. While effective at limiting overlap, this strategy frequently misses short, fine-grained cues near important events. Other methods instead emphasize visual diversity but neglect query relevance. We propose AdaRD-Key, a training-free keyframe sampling module for query-driven long-form video understanding. AdaRD-Key maximizes a unified Relevance--Diversity Max-Volume (RD-MV) objective, combining a query-conditioned relevance score with a log-determinant diversity component to yield informative yet non-redundant frames. To handle broad queries with weak alignment to the video, AdaRD-Key employs a lightweight relevance-aware gating mechanism; when the relevance distribution indicates weak alignment, the method seamlessly shifts into a diversity-only mode, enhancing coverage without additional supervision. Our pipeline is training-free, computationally efficient (running in real time on a single GPU), and compatible with existing VLMs in a plug-and-play manner. Extensive experiments on LongVideoBench and Video-MME demonstrate state-of-the-art performance, particularly on long-form videos. Code available at this https URL.</li>
</ul>

<h3>Title: Reasoning Riddles: How Explainability Reveals Cognitive Limits in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Prahitha Movva</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02780">https://arxiv.org/abs/2510.02780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02780">https://arxiv.org/pdf/2510.02780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02780]] Reasoning Riddles: How Explainability Reveals Cognitive Limits in Vision-Language Models(https://arxiv.org/abs/2510.02780)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) excel at many multimodal tasks, yet their cognitive processes remain opaque on complex lateral thinking challenges like rebus puzzles. While recent work has demonstrated these models struggle significantly with rebus puzzle solving, the underlying reasoning processes and failure patterns remain largely unexplored. We address this gap through a comprehensive explainability analysis that moves beyond performance metrics to understand how VLMs approach these complex lateral thinking challenges. Our study contributes a systematically annotated dataset of 221 rebus puzzles across six cognitive categories, paired with an evaluation framework that separates reasoning quality from answer correctness. We investigate three prompting strategies designed to elicit different types of explanatory processes and reveal critical insights into VLM cognitive processes. Our findings demonstrate that reasoning quality varies dramatically across puzzle categories, with models showing systematic strengths in visual composition while exhibiting fundamental limitations in absence interpretation and cultural symbolism. We also discover that prompting strategy substantially influences both cognitive approach and problem-solving effectiveness, establishing explainability as an integral component of model performance rather than a post-hoc consideration.</li>
</ul>

<h3>Title: OTR: Synthesizing Overlay Text Dataset for Text Removal</h3>
<ul>
<li><strong>Authors: </strong>Jan Zdenek, Wataru Shimoda, Kota Yamaguchi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02787">https://arxiv.org/abs/2510.02787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02787">https://arxiv.org/pdf/2510.02787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02787]] OTR: Synthesizing Overlay Text Dataset for Text Removal(https://arxiv.org/abs/2510.02787)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Text removal is a crucial task in computer vision with applications such as privacy preservation, image editing, and media reuse. While existing research has primarily focused on scene text removal in natural images, limitations in current datasets hinder out-of-domain generalization or accurate evaluation. In particular, widely used benchmarks such as SCUT-EnsText suffer from ground truth artifacts due to manual editing, overly simplistic text backgrounds, and evaluation metrics that do not capture the quality of generated results. To address these issues, we introduce an approach to synthesizing a text removal benchmark applicable to domains other than scene texts. Our dataset features text rendered on complex backgrounds using object-aware placement and vision-language model-generated content, ensuring clean ground truth and challenging text removal scenarios. The dataset is available at this https URL .</li>
</ul>

<h3>Title: Align Your Query: Representation Alignment for Multimodality Medical Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Ara Seo, Bryan Sangwoo Kim, Hyungjin Chung, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02789">https://arxiv.org/abs/2510.02789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02789">https://arxiv.org/pdf/2510.02789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02789]] Align Your Query: Representation Alignment for Multimodality Medical Object Detection(https://arxiv.org/abs/2510.02789)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Medical object detection suffers when a single detector is trained on mixed medical modalities (e.g., CXR, CT, MRI) due to heterogeneous statistics and disjoint representation spaces. To address this challenge, we turn to representation alignment, an approach that has proven effective for bringing features from different sources into a shared space. Specifically, we target the representations of DETR-style object queries and propose a simple, detector-agnostic framework to align them with modality context. First, we define modality tokens: compact, text-derived embeddings encoding imaging modality that are lightweight and require no extra annotations. We integrate the modality tokens into the detection process via Multimodality Context Attention (MoCA), mixing object-query representations via self-attention to propagate modality context within the query set. This preserves DETR-style architectures and adds negligible latency while injecting modality cues into object queries. We further introduce QueryREPA, a short pretraining stage that aligns query representations to their modality tokens using a task-specific contrastive objective with modality-balanced batches. Together, MoCA and QueryREPA produce modality-aware, class-faithful queries that transfer effectively to downstream training. Across diverse modalities trained altogether, the proposed approach consistently improves AP with minimal overhead and no architectural modifications, offering a practical path toward robust multimodality medical object detection. Project page: this https URL.</li>
</ul>

<h3>Title: VERNIER: an open-source software pushing marker pose estimation down to the micrometer and nanometer scales</h3>
<ul>
<li><strong>Authors: </strong>Patrick Sandoz, Antoine N. André, Guillaume J. Laurent</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02791">https://arxiv.org/abs/2510.02791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02791">https://arxiv.org/pdf/2510.02791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02791]] VERNIER: an open-source software pushing marker pose estimation down to the micrometer and nanometer scales(https://arxiv.org/abs/2510.02791)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Pose estimation is still a challenge at the small scales. Few solutions exist to capture the 6 degrees of freedom of an object with nanometric and microradians resolutions over relatively large ranges. Over the years, we have proposed several fiducial marker and pattern designs to achieve reliable performance for various microscopy applications. Centimeter ranges are possible using pattern encoding methods, while nanometer resolutions can be achieved using phase processing of the periodic frames. This paper presents VERNIER, an open source phase processing software designed to provide fast and reliable pose measurement based on pseudo-periodic patterns. Thanks to a phase-based local thresholding algorithm, the software has proven to be particularly robust to noise, defocus and occlusion. The successive steps of the phase processing are presented, as well as the different types of patterns that address different application needs. The implementation procedure is illustrated with synthetic and experimental images. Finally, guidelines are given for selecting the appropriate pattern design and microscope magnification lenses as a function of the desired performance.</li>
</ul>

<h3>Title: Dissecting Transformers: A CLEAR Perspective towards Green AI</h3>
<ul>
<li><strong>Authors: </strong>Hemang Jain, Shailender Goyal, Divyansh Pandey, Karthik Vaidhyanathan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02810">https://arxiv.org/abs/2510.02810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02810">https://arxiv.org/pdf/2510.02810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02810]] Dissecting Transformers: A CLEAR Perspective towards Green AI(https://arxiv.org/abs/2510.02810)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The rapid adoption of Large Language Models (LLMs) has raised significant environmental concerns. Unlike the one-time cost of training, LLM inference occurs continuously at a global scale and now dominates the AI energy footprint. Yet, most sustainability studies report only coarse, model-level metrics due to the lack of fine-grained measurement methods, treating energy efficiency more as an afterthought than as a primary objective. We present the first fine-grained empirical analysis of inference energy across core components of transformer architecture. We propose a novel methodology, Component-Level Energy Assessment via Repeated sampling (CLEAR), to overcome temporal mismatch between microsecond scale component execution and monitoring of millisecond (ms) scale energy sensors. Using CLEAR, we evaluate 15 models spanning four distinct architecture types and consistently keep component-wise energy variance below 9.5\% while capturing more than 90\% of the model's total energy as individual components. Our empirical analysis reveals that Attention blocks consume significantly more energy per floating-point operation (FLOP), indicating that energy consumption is not proportionally aligned with FLOP counts. This shows that FLOPs alone fail to capture the true energy cost at a component level. Our findings establish detailed component-level energy baselines and provide insight as an initial step to build energy-efficient transformer models through component-level optimizations.</li>
</ul>

<h3>Title: A Computational Framework for Interpretable Text-Based Personality Assessment from Social Media</h3>
<ul>
<li><strong>Authors: </strong>Matej Gjurković</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02811">https://arxiv.org/abs/2510.02811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02811">https://arxiv.org/pdf/2510.02811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02811]] A Computational Framework for Interpretable Text-Based Personality Assessment from Social Media(https://arxiv.org/abs/2510.02811)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Personality refers to individual differences in behavior, thinking, and feeling. With the growing availability of digital footprints, especially from social media, automated methods for personality assessment have become increasingly important. Natural language processing (NLP) enables the analysis of unstructured text data to identify personality indicators. However, two main challenges remain central to this thesis: the scarcity of large, personality-labeled datasets and the disconnect between personality psychology and NLP, which restricts model validity and interpretability. To address these challenges, this thesis presents two datasets -- MBTI9k and PANDORA -- collected from Reddit, a platform known for user anonymity and diverse discussions. The PANDORA dataset contains 17 million comments from over 10,000 users and integrates the MBTI and Big Five personality models with demographic information, overcoming limitations in data size, quality, and label coverage. Experiments on these datasets show that demographic variables influence model validity. In response, the SIMPA (Statement-to-Item Matching Personality Assessment) framework was developed - a computational framework for interpretable personality assessment that matches user-generated statements with validated questionnaire items. By using machine learning and semantic similarity, SIMPA delivers personality assessments comparable to human evaluations while maintaining high interpretability and efficiency. Although focused on personality assessment, SIMPA's versatility extends beyond this domain. Its model-agnostic design, layered cue detection, and scalability make it suitable for various research and practical applications involving complex label taxonomies and variable cue associations with target concepts.</li>
</ul>

<h3>Title: Mitigating Spurious Correlation via Distributionally Robust Learning with Hierarchical Ambiguity Sets</h3>
<ul>
<li><strong>Authors: </strong>Sung Ho Jo, Seonghwi Kim, Minwoo Chae</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02818">https://arxiv.org/abs/2510.02818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02818">https://arxiv.org/pdf/2510.02818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02818]] Mitigating Spurious Correlation via Distributionally Robust Learning with Hierarchical Ambiguity Sets(https://arxiv.org/abs/2510.02818)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Conventional supervised learning methods are often vulnerable to spurious correlations, particularly under distribution shifts in test data. To address this issue, several approaches, most notably Group DRO, have been developed. While these methods are highly robust to subpopulation or group shifts, they remain vulnerable to intra-group distributional shifts, which frequently occur in minority groups with limited samples. We propose a hierarchical extension of Group DRO that addresses both inter-group and intra-group uncertainties, providing robustness to distribution shifts at multiple levels. We also introduce new benchmark settings that simulate realistic minority group distribution shifts-an important yet previously underexplored challenge in spurious correlation research. Our method demonstrates strong robustness under these conditions-where existing robust learning methods consistently fail-while also achieving superior performance on standard benchmarks. These results highlight the importance of broadening the ambiguity set to better capture both inter-group and intra-group distributional uncertainties.</li>
</ul>

<h3>Title: FlexiQ: Adaptive Mixed-Precision Quantization for Latency/Accuracy Trade-Offs in Deep Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Jaemin Kim, Hongjun Um, Sungkyun Kim, Yongjun Park, Jiwon Seo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02822">https://arxiv.org/abs/2510.02822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02822">https://arxiv.org/pdf/2510.02822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02822]] FlexiQ: Adaptive Mixed-Precision Quantization for Latency/Accuracy Trade-Offs in Deep Neural Networks(https://arxiv.org/abs/2510.02822)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Neural networks commonly execute on hardware accelerators such as NPUs and GPUs for their size and computation overhead. These accelerators are costly and it is hard to scale their resources to handle real-time workload fluctuations. We present FlexiQ, an adaptive mixed-precision quantization scheme for computer vision models. FlexiQ selectively applies low-bitwidth computation to feature channels with small value ranges and employs an efficient bit-lowering method to minimize quantization errors while maintaining inference accuracy. Furthermore, FlexiQ adjusts its low-bitwidth channel ratio in real time, enabling quantized models to effectively manage fluctuating inference workload. We implemented FlexiQ prototype, including the mixed-precision inference runtime on our custom NPU and GPUs. Evaluated on eleven convolution- and transformer-based vision models, FlexiQ achieves on average 6.6% higher accuracy for 4-bit models with finetuning and outperforms four state-of-the-art quantization techniques. Moreover, our mixed-precision models achieved an efficient accuracy-latency trade-off, with the 50% 4-bit model incurring only 0.6% accuracy loss while achieving 40% of the speedup of the 100% 4-bit model over 8-bit model. Latency evaluations on our NPU and GPUs confirmed that FlexiQ introduces minimal runtime overhead, demonstrating its hardware efficiency and overall performance benefits.</li>
</ul>

<h3>Title: Multi-scale Autoregressive Models are Laplacian, Discrete, and Latent Diffusion Models in Disguise</h3>
<ul>
<li><strong>Authors: </strong>Steve Hong, Samuel Belkadi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02826">https://arxiv.org/abs/2510.02826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02826">https://arxiv.org/pdf/2510.02826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02826]] Multi-scale Autoregressive Models are Laplacian, Discrete, and Latent Diffusion Models in Disguise(https://arxiv.org/abs/2510.02826)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We revisit Visual Autoregressive (VAR) models through the lens of an iterative-refinement framework. Rather than viewing VAR solely as next-scale autoregression, we formalise it as a deterministic forward process that constructs a Laplacian-style latent pyramid, paired with a learned backward process that reconstructs it in a small number of coarse-to-fine steps. This view connects VAR to denoising diffusion and isolates three design choices that help explain its efficiency and fidelity: refining in a learned latent space, casting prediction as discrete classification over code indices, and partitioning the task by spatial frequency. We run controlled experiments to quantify each factor's contribution to fidelity and speed, and we outline how the same framework extends to permutation-invariant graph generation and to probabilistic, ensemble-style medium-range weather forecasting. The framework also suggests practical interfaces for VAR to leverage tools from the diffusion ecosystem while retaining few-step, scale-parallel generation.</li>
</ul>

<h3>Title: StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Tengjun Ni, Xin Yuan, Shenghong Li, Kai Wu, Ren Ping Liu, Wei Ni, Wenjie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02827">https://arxiv.org/abs/2510.02827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02827">https://arxiv.org/pdf/2510.02827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02827]] StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop Question Answering(https://arxiv.org/abs/2510.02827)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Recent progress in retrieval-augmented generation (RAG) has led to more accurate and interpretable multi-hop question answering (QA). Yet, challenges persist in integrating iterative reasoning steps with external knowledge retrieval. To address this, we introduce StepChain GraphRAG, a framework that unites question decomposition with a Breadth-First Search (BFS) Reasoning Flow for enhanced multi-hop QA. Our approach first builds a global index over the corpus; at inference time, only retrieved passages are parsed on-the-fly into a knowledge graph, and the complex query is split into sub-questions. For each sub-question, a BFS-based traversal dynamically expands along relevant edges, assembling explicit evidence chains without overwhelming the language model with superfluous context. Experiments on MuSiQue, 2WikiMultiHopQA, and HotpotQA show that StepChain GraphRAG achieves state-of-the-art Exact Match and F1 scores. StepChain GraphRAG lifts average EM by 2.57% and F1 by 2.13% over the SOTA method, achieving the largest gain on HotpotQA (+4.70% EM, +3.44% F1). StepChain GraphRAG also fosters enhanced explainability by preserving the chain-of-thought across intermediate retrieval steps. We conclude by discussing how future work can mitigate the computational overhead and address potential hallucinations from large language models to refine efficiency and reliability in multi-hop QA.</li>
</ul>

<h3>Title: Evaluating Large Language Models for IUCN Red List Species Information</h3>
<ul>
<li><strong>Authors: </strong>Shinya Uryu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02830">https://arxiv.org/abs/2510.02830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02830">https://arxiv.org/pdf/2510.02830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02830]] Evaluating Large Language Models for IUCN Red List Species Information(https://arxiv.org/abs/2510.02830)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are rapidly being adopted in conservation to address the biodiversity crisis, yet their reliability for species evaluation is uncertain. This study systematically validates five leading models on 21,955 species across four core IUCN Red List assessment components: taxonomy, conservation status, distribution, and threats. A critical paradox was revealed: models excelled at taxonomic classification (94.9%) but consistently failed at conservation reasoning (27.2% for status assessment). This knowledge-reasoning gap, evident across all models, suggests inherent architectural constraints, not just data limitations. Furthermore, models exhibited systematic biases favoring charismatic vertebrates, potentially amplifying existing conservation inequities. These findings delineate clear boundaries for responsible LLM deployment: they are powerful tools for information retrieval but require human oversight for judgment-based decisions. A hybrid approach is recommended, where LLMs augment expert capacity while human experts retain sole authority over risk assessment and policy.</li>
</ul>

<h3>Title: Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhixin Xie, Xurui Song, Jun Luo</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02833">https://arxiv.org/abs/2510.02833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02833">https://arxiv.org/pdf/2510.02833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02833]] Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs(https://arxiv.org/abs/2510.02833)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Despite substantial efforts in safety alignment, recent research indicates that Large Language Models (LLMs) remain highly susceptible to jailbreak attacks. Among these attacks, finetuning-based ones that compromise LLMs' safety alignment via fine-tuning stand out due to its stable jailbreak performance. In particular, a recent study indicates that fine-tuning with as few as 10 harmful question-answer (QA) pairs can lead to successful jailbreaking across various harmful questions. However, such malicious fine-tuning attacks are readily detectable and hence thwarted by moderation models. In this paper, we demonstrate that LLMs can be jailbroken by fine-tuning with only 10 benign QA pairs; our attack exploits the increased sensitivity of LLMs to fine-tuning data after being overfitted. Specifically, our fine-tuning process starts with overfitting an LLM via fine-tuning with benign QA pairs involving identical refusal answers. Further fine-tuning is then performed with standard benign answers, causing the overfitted LLM to forget the refusal attitude and thus provide compliant answers regardless of the harmfulness of a question. We implement our attack on the ten LLMs and compare it with five existing baselines. Experiments demonstrate that our method achieves significant advantages in both attack effectiveness and attack stealth. Our findings expose previously unreported security vulnerabilities in current LLMs and provide a new perspective on understanding how LLMs' security is compromised, even with benign fine-tuning. Our code is available at this https URL.</li>
</ul>

<h3>Title: Subject-Adaptive Sparse Linear Models for Interpretable Personalized Health Prediction from Multimodal Lifelog Data</h3>
<ul>
<li><strong>Authors: </strong>Dohyun Bu, Jisoo Han, Soohwa Kwon, Yulim So, Jong-Seok Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02835">https://arxiv.org/abs/2510.02835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02835">https://arxiv.org/pdf/2510.02835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02835]] Subject-Adaptive Sparse Linear Models for Interpretable Personalized Health Prediction from Multimodal Lifelog Data(https://arxiv.org/abs/2510.02835)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Improved prediction of personalized health outcomes -- such as sleep quality and stress -- from multimodal lifelog data could have meaningful clinical and practical implications. However, state-of-the-art models, primarily deep neural networks and gradient-boosted ensembles, sacrifice interpretability and fail to adequately address the significant inter-individual variability inherent in lifelog data. To overcome these challenges, we propose the Subject-Adaptive Sparse Linear (SASL) framework, an interpretable modeling approach explicitly designed for personalized health prediction. SASL integrates ordinary least squares regression with subject-specific interactions, systematically distinguishing global from individual-level effects. We employ an iterative backward feature elimination method based on nested $F$-tests to construct a sparse and statistically robust model. Additionally, recognizing that health outcomes often represent discretized versions of continuous processes, we develop a regression-then-thresholding approach specifically designed to maximize macro-averaged F1 scores for ordinal targets. For intrinsically challenging predictions, SASL selectively incorporates outputs from compact LightGBM models through confidence-based gating, enhancing accuracy without compromising interpretability. Evaluations conducted on the CH-2025 dataset -- which comprises roughly 450 daily observations from ten subjects -- demonstrate that the hybrid SASL-LightGBM framework achieves predictive performance comparable to that of sophisticated black-box methods, but with significantly fewer parameters and substantially greater transparency, thus providing clear and actionable insights for clinicians and practitioners.</li>
</ul>

<h3>Title: Knowledge-Aware Modeling with Frequency Adaptive Learning for Battery Health Prognostics</h3>
<ul>
<li><strong>Authors: </strong>Vijay Babu Pamshetti, Wei Zhang, Sumei Sun, Jie Zhang, Yonggang Wen, Qingyu Yan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02839">https://arxiv.org/abs/2510.02839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02839">https://arxiv.org/pdf/2510.02839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02839]] Knowledge-Aware Modeling with Frequency Adaptive Learning for Battery Health Prognostics(https://arxiv.org/abs/2510.02839)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Battery health prognostics are critical for ensuring safety, efficiency, and sustainability in modern energy systems. However, it has been challenging to achieve accurate and robust prognostics due to complex battery degradation behaviors with nonlinearity, noise, capacity regeneration, etc. Existing data-driven models capture temporal degradation features but often lack knowledge guidance, which leads to unreliable long-term health prognostics. To overcome these limitations, we propose Karma, a knowledge-aware model with frequency-adaptive learning for battery capacity estimation and remaining useful life prediction. The model first performs signal decomposition to derive battery signals in different frequency bands. A dual-stream deep learning architecture is developed, where one stream captures long-term low-frequency degradation trends and the other models high-frequency short-term dynamics. Karma regulates the prognostics with knowledge, where battery degradation is modeled as a double exponential function based on empirical studies. Our dual-stream model is used to optimize the parameters of the knowledge with particle filters to ensure physically consistent and reliable prognostics and uncertainty quantification. Experimental study demonstrates Karma's superior performance, achieving average error reductions of 50.6% and 32.6% over state-of-the-art algorithms for battery health prediction on two mainstream datasets, respectively. These results highlight Karma's robustness, generalizability, and potential for safer and more reliable battery management across diverse applications.</li>
</ul>

<h3>Title: Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation</h3>
<ul>
<li><strong>Authors: </strong>Jahidul Arafat, Fariha Tasmin, Sanjaya Poudel, Kamrujjaman, Eftakhar Ahmed Arnob, Ahsan Habib Tareq</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02855">https://arxiv.org/abs/2510.02855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02855">https://arxiv.org/pdf/2510.02855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02855]] Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation(https://arxiv.org/abs/2510.02855)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Wordle presents an algorithmically rich testbed for constraint satisfaction problem (CSP) solving. While existing solvers rely on information-theoretic entropy maximization or frequency-based heuristics without formal constraint treatment, we present the first comprehensive CSP formulation of Wordle with novel constraint-aware solving strategies. We introduce CSP-Aware Entropy, computing information gain after constraint propagation rather than on raw candidate sets, and a Probabilistic CSP framework integrating Bayesian word-frequency priors with logical constraints. Through evaluation on 2,315 English words, CSP-Aware Entropy achieves 3.54 average guesses with 99.9% success rate, a statistically significant 1.7% improvement over Forward Checking (t=-4.82, p<0.001, Cohen's d=0.07) with 46% faster runtime (12.9ms versus 23.7ms per guess). Under 10% noise, CSP-aware approaches maintain 5.3 percentage point advantages (29.0% versus 23.7%, p=0.041), while Probabilistic CSP achieves 100% success across all noise levels (0-20%) through constraint recovery mechanisms. Cross-lexicon validation on 500 Spanish words demonstrates 88% success with zero language-specific tuning, validating that core CSP principles transfer across languages despite an 11.2 percentage point gap from linguistic differences (p<0.001, Fisher's exact test). Our open-source implementation with 34 unit tests achieving 91% code coverage provides reproducible infrastructure for CSP research. The combination of formal CSP treatment, constraint-aware heuristics, probabilistic-logical integration, robustness analysis, and cross-lexicon validation establishes new performance benchmarks demonstrating that principled constraint satisfaction techniques outperform classical information-theoretic and learning-based approaches for structured puzzle-solving domains.</li>
</ul>

<h3>Title: ELMF4EggQ: Ensemble Learning with Multimodal Feature Fusion for Non-Destructive Egg Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Md Zahim Hassan, Md. Osama, Muhammad Ashad Kabir, Md. Saiful Islam, Zannatul Naim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02876">https://arxiv.org/abs/2510.02876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02876">https://arxiv.org/pdf/2510.02876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02876]] ELMF4EggQ: Ensemble Learning with Multimodal Feature Fusion for Non-Destructive Egg Quality Assessment(https://arxiv.org/abs/2510.02876)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Accurate, non-destructive assessment of egg quality is critical for ensuring food safety, maintaining product standards, and operational efficiency in commercial poultry production. This paper introduces ELMF4EggQ, an ensemble learning framework that employs multimodal feature fusion to classify egg grade and freshness using only external attributes - image, shape, and weight. A novel, publicly available dataset of 186 brown-shelled eggs was constructed, with egg grade and freshness levels determined through laboratory-based expert assessments involving internal quality measurements, such as yolk index and Haugh unit. To the best of our knowledge, this is the first study to apply machine learning methods for internal egg quality assessment using only external, non-invasive features, and the first to release a corresponding labeled dataset. The proposed framework integrates deep features extracted from external egg images with structural characteristics such as egg shape and weight, enabling a comprehensive representation of each egg. Image feature extraction is performed using top-performing pre-trained CNN models (ResNet152, DenseNet169, and ResNet152V2), followed by PCA-based dimensionality reduction, SMOTE augmentation, and classification using multiple machine learning algorithms. An ensemble voting mechanism combines predictions from the best-performing classifiers to enhance overall accuracy. Experimental results demonstrate that the multimodal approach significantly outperforms image-only and tabular (shape and weight) only baselines, with the multimodal ensemble approach achieving 86.57% accuracy in grade classification and 70.83% in freshness prediction. All code and data are publicly available at this https URL, promoting transparency, reproducibility, and further research in this domain.</li>
</ul>

<h3>Title: RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Aleksei Arzhantsev, Otmane Sakhi, Flavian Vasile</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02892">https://arxiv.org/abs/2510.02892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02892">https://arxiv.org/pdf/2510.02892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02892]] RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative Reinforcement Learning(https://arxiv.org/abs/2510.02892)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) is central to improving reasoning in large language models (LLMs) but typically requires ground-truth rewards. Test-Time Reinforcement Learning (TTRL) removes this need by using majority-vote rewards, but relies on heavy online RL and incurs substantial computational cost. We propose RoiRL: Reasoning with offline iterative Reinforcement Learning, a family of lightweight offline learning alternatives that can target the same regularized optimal policies. Unlike TTRL, RoiRL eliminates the need to maintain a reference model and instead optimizes weighted log-likelihood objectives, enabling stable training with significantly lower memory and compute requirements. Experimental results show that RoiRL trains to 2.5x faster and consistently outperforms TTRL on reasoning benchmarks, establishing a scalable path to self-improving LLMs without labels.</li>
</ul>

<h3>Title: DMark: Order-Agnostic Watermarking for Diffusion Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Linyu Wu, Linhao Zhong, Wenjie Qu, Yuexin Li, Yue Liu, Shengfang Zhai, Chunhua Shen, Jiaheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02902">https://arxiv.org/abs/2510.02902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02902">https://arxiv.org/pdf/2510.02902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02902]] DMark: Order-Agnostic Watermarking for Diffusion Large Language Models(https://arxiv.org/abs/2510.02902)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark, diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion large language models (dLLMs) offer faster generation than autoregressive models while maintaining comparable quality, but existing watermarking methods fail on them due to their non-sequential decoding. Unlike autoregressive models that generate tokens left-to-right, dLLMs can finalize tokens in arbitrary order, breaking the causal design underlying traditional watermarks. We present DMark, the first watermarking framework designed specifically for dLLMs. DMark introduces three complementary strategies to restore watermark detectability: predictive watermarking uses model-predicted tokens when actual context is unavailable; bidirectional watermarking exploits both forward and backward dependencies unique to diffusion decoding; and predictive-bidirectional watermarking combines both approaches to maximize detection strength. Experiments across multiple dLLMs show that DMark achieves 92.0-99.5% detection rates at 1% false positive rate while maintaining text quality, compared to only 49.6-71.2% for naive adaptations of existing methods. DMark also demonstrates robustness against text manipulations, establishing that effective watermarking is feasible for non-autoregressive language models.</li>
</ul>

<h3>Title: Training-Free Out-Of-Distribution Segmentation With Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Laith Nayal, Hadi Salloum, Ahmad Taha, Yaroslav Kholodov, Alexander Gasnikov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02909">https://arxiv.org/abs/2510.02909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02909">https://arxiv.org/pdf/2510.02909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02909]] Training-Free Out-Of-Distribution Segmentation With Foundation Models(https://arxiv.org/abs/2510.02909)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Detecting unknown objects in semantic segmentation is crucial for safety-critical applications such as autonomous driving. Large vision foundation models, includ- ing DINOv2, InternImage, and CLIP, have advanced visual representation learn- ing by providing rich features that generalize well across diverse tasks. While their strength in closed-set semantic tasks is established, their capability to detect out- of-distribution (OoD) regions in semantic segmentation remains underexplored. In this work, we investigate whether foundation models fine-tuned on segmen- tation datasets can inherently distinguish in-distribution (ID) from OoD regions without any outlier supervision. We propose a simple, training-free approach that utilizes features from the InternImage backbone and applies K-Means clustering alongside confidence thresholding on raw decoder logits to identify OoD clusters. Our method achieves 50.02 Average Precision on the RoadAnomaly benchmark and 48.77 on the benchmark of ADE-OoD with InternImage-L, surpassing several supervised and unsupervised baselines. These results suggest a promising direc- tion for generic OoD segmentation methods that require minimal assumptions or additional data.</li>
</ul>

<h3>Title: Don't Just Chase "Highlighted Tokens" in MLLMs: Revisiting Visual Holistic Context Retention</h3>
<ul>
<li><strong>Authors: </strong>Xin Zou, Di Lu, Yizhou Wang, Yibo Yan, Yuanhuiyi Lyu, Xu Zheng, Linfeng Zhang, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02912">https://arxiv.org/abs/2510.02912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02912">https://arxiv.org/pdf/2510.02912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02912]] Don't Just Chase "Highlighted Tokens" in MLLMs: Revisiting Visual Holistic Context Retention(https://arxiv.org/abs/2510.02912)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite their powerful capabilities, Multimodal Large Language Models (MLLMs) suffer from considerable computational overhead due to their reliance on massive visual tokens. Recent studies have explored token pruning to alleviate this problem, which typically uses text-vision cross-attention or [\texttt{CLS}] attention to assess and discard redundant visual tokens. In this work, we identify a critical limitation of such attention-first pruning approaches, i.e., they tend to preserve semantically similar tokens, resulting in pronounced performance drops under high pruning ratios. To this end, we propose {HoloV}, a simple yet effective, plug-and-play visual token pruning framework for efficient inference. Distinct from previous attention-first schemes, HoloV rethinks token retention from a holistic perspective. By adaptively distributing the pruning budget across different spatial crops, HoloV ensures that the retained tokens capture the global visual context rather than isolated salient features. This strategy minimizes representational collapse and maintains task-relevant information even under aggressive pruning. Experimental results demonstrate that our HoloV achieves superior performance across various tasks, MLLM architectures, and pruning ratios compared to SOTA methods. For instance, LLaVA1.5 equipped with HoloV preserves 95.8\% of the original performance after pruning 88.9\% of visual tokens, achieving superior efficiency-accuracy trade-offs.</li>
</ul>

<h3>Title: Zero-Shot Robustness of Vision Language Models Via Confidence-Aware Weighting</h3>
<ul>
<li><strong>Authors: </strong>Nikoo Naghavian, Mostafa Tavassolipour</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02913">https://arxiv.org/abs/2510.02913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02913">https://arxiv.org/pdf/2510.02913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02913]] Zero-Shot Robustness of Vision Language Models Via Confidence-Aware Weighting(https://arxiv.org/abs/2510.02913)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Vision-language models like CLIP demonstrate impressive zero-shot generalization but remain highly vulnerable to adversarial attacks. In this work, we propose Confidence-Aware Weighting (CAW) to enhance zero-shot robustness in vision-language models. CAW consists of two components: (1) a Confidence-Aware loss that prioritizes uncertain adversarial examples by scaling the KL divergence between clean and adversarial predictions, and (2) a feature alignment regularization that preserves semantic consistency by minimizing the distance between frozen and fine-tuned image encoder features on adversarial inputs. These components work jointly to improve both clean and robust accuracy without sacrificing generalization. Extensive experiments on TinyImageNet and 14 additional datasets show that CAW outperforms recent methods such as PMG-AFT and TGA-ZSR under strong attacks like AutoAttack, while using less memory.</li>
</ul>

<h3>Title: FeDABoost: Fairness Aware Federated Learning with Adaptive Boosting</h3>
<ul>
<li><strong>Authors: </strong>Tharuka Kasthuri Arachchige, Veselka Boeva, Shahrooz Abghari</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02914">https://arxiv.org/abs/2510.02914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02914">https://arxiv.org/pdf/2510.02914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02914]] FeDABoost: Fairness Aware Federated Learning with Adaptive Boosting(https://arxiv.org/abs/2510.02914)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, fair</a></li>
<li><strong>Abstract: </strong>This work focuses on improving the performance and fairness of Federated Learning (FL) in non IID settings by enhancing model aggregation and boosting the training of underperforming clients. We propose FeDABoost, a novel FL framework that integrates a dynamic boosting mechanism and an adaptive gradient aggregation strategy. Inspired by the weighting mechanism of the Multiclass AdaBoost (SAMME) algorithm, our aggregation method assigns higher weights to clients with lower local error rates, thereby promoting more reliable contributions to the global model. In parallel, FeDABoost dynamically boosts underperforming clients by adjusting the focal loss focusing parameter, emphasizing hard to classify examples during local training. We have evaluated FeDABoost on three benchmark datasets MNIST, FEMNIST, and CIFAR10, and compared its performance with those of FedAvg and Ditto. The results show that FeDABoost achieves improved fairness and competitive performance.</li>
</ul>

<h3>Title: Self-Reflective Generation at Test Time</h3>
<ul>
<li><strong>Authors: </strong>Jian Mu, Qixin Zhang, Zhiyong Wang, Menglin Yang, Shuang Qiu, Chengwei Qin, Zhongxiang Dai, Yao Shu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02919">https://arxiv.org/abs/2510.02919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02919">https://arxiv.org/pdf/2510.02919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02919]] Self-Reflective Generation at Test Time(https://arxiv.org/abs/2510.02919)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) increasingly solve complex reasoning tasks via long chain-of-thought, but their forward-only autoregressive generation process is fragile; early token errors can cascade, which creates a clear need for self-reflection mechanisms. However, existing self-reflection either performs revisions over full drafts or learns self-correction via expensive training, both fundamentally reactive and inefficient. To address this, we propose Self-Reflective Generation at Test Time (SRGen), a lightweight test-time framework that reflects before generating at uncertain points. During token generation, SRGen utilizes dynamic entropy thresholding to identify high-uncertainty tokens. For each identified token, it trains a specific corrective vector, which fully exploits the already generated context for a self-reflective generation to correct the token probability distribution. By retrospectively analyzing the partial output, this self-reflection enables more trustworthy decisions, thereby significantly reducing the probability of errors at highly uncertain points. Evaluated on challenging mathematical reasoning benchmarks and a diverse set of LLMs, SRGen can consistently strengthen model reasoning: improvements in single-pass quality also translate into stronger self-consistency voting. Especially, on AIME2024 with DeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on Pass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a plug-and-play method that integrates reflection into the generation process for reliable LLM reasoning, achieving consistent gains with bounded overhead and broad composability with other training-time (e.g., RLHF) and test-time (e.g., SLOT) techniques.</li>
</ul>

<h3>Title: RAxSS: Retrieval-Augmented Sparse Sampling for Explainable Variable-Length Medical Time Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Aydin Javadov, Samir Garibov, Tobias Hoesli, Qiyang Sun, Florian von Wangenheim, Joseph Ollier, Björn W. Schuller</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02936">https://arxiv.org/abs/2510.02936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02936">https://arxiv.org/pdf/2510.02936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02936]] RAxSS: Retrieval-Augmented Sparse Sampling for Explainable Variable-Length Medical Time Series Classification(https://arxiv.org/abs/2510.02936)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>Medical time series analysis is challenging due to data sparsity, noise, and highly variable recording lengths. Prior work has shown that stochastic sparse sampling effectively handles variable-length signals, while retrieval-augmented approaches improve explainability and robustness to noise and weak temporal correlations. In this study, we generalize the stochastic sparse sampling framework for retrieval-informed classification. Specifically, we weight window predictions by within-channel similarity and aggregate them in probability space, yielding convex series-level scores and an explicit evidence trail for explainability. Our method achieves competitive iEEG classification performance and provides practitioners with greater transparency and explainability. We evaluate our method in iEEG recordings collected in four medical centers, demonstrating its potential for reliable and explainable clinical variable-length time series classification.</li>
</ul>

<h3>Title: SoK: Preconfirmations</h3>
<ul>
<li><strong>Authors: </strong>Aikaterini-Panagiota Stouka, Conor McMenamin, Demetris Kyriacou, Lin Oshitani, Quentin Botha</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02947">https://arxiv.org/abs/2510.02947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02947">https://arxiv.org/pdf/2510.02947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02947]] SoK: Preconfirmations(https://arxiv.org/abs/2510.02947)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>In recent years, significant research efforts have focused on improving blockchain throughput and confirmation speeds without compromising security. While decreasing the time it takes for a transaction to be included in the blockchain ledger enhances user experience, a fundamental delay still remains between when a transaction is issued by a user and when its inclusion is confirmed in the blockchain ledger. This delay limits user experience gains through the confirmation uncertainty it brings for users. This inherent delay in conventional blockchain protocols has led to the emergence of preconfirmation protocols -- protocols that provide users with early guarantees of eventual transaction confirmation. This article presents a Systematization of Knowledge (SoK) on preconfirmations. We present the core terms and definitions needed to understand preconfirmations, outline a general framework for preconfirmation protocols, and explore the economics and risks of preconfirmations. Finally, we survey and apply our framework to several implementations of real-world preconfirmation protocols, bridging the gap between theory and practice.</li>
</ul>

<h3>Title: Confidence and Dispersity as Signals: Unsupervised Model Evaluation and Ranking</h3>
<ul>
<li><strong>Authors: </strong>Weijian Deng, Weijie Tu, Ibrahim Radwan, Mohammad Abu Alsheikh, Stephen Gould, Liang Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02956">https://arxiv.org/abs/2510.02956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02956">https://arxiv.org/pdf/2510.02956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02956]] Confidence and Dispersity as Signals: Unsupervised Model Evaluation and Ranking(https://arxiv.org/abs/2510.02956)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Assessing model generalization under distribution shift is essential for real-world deployment, particularly when labeled test data is unavailable. This paper presents a unified and practical framework for unsupervised model evaluation and ranking in two common deployment settings: (1) estimating the accuracy of a fixed model on multiple unlabeled test sets (dataset-centric evaluation), and (2) ranking a set of candidate models on a single unlabeled test set (model-centric evaluation). We demonstrate that two intrinsic properties of model predictions, namely confidence (which reflects prediction certainty) and dispersity (which captures the diversity of predicted classes), together provide strong and complementary signals for generalization. We systematically benchmark a set of confidence-based, dispersity-based, and hybrid metrics across a wide range of model architectures, datasets, and distribution shift types. Our results show that hybrid metrics consistently outperform single-aspect metrics on both dataset-centric and model-centric evaluation settings. In particular, the nuclear norm of the prediction matrix provides robust and accurate performance across tasks, including real-world datasets, and maintains reliability under moderate class imbalance. These findings offer a practical and generalizable basis for unsupervised model assessment in deployment scenarios.</li>
</ul>

<h3>Title: SoK: Kicking CAN Down the Road. Systematizing CAN Security Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Khaled Serag, Zhaozhou Tang, Sungwoo Kim, Vireshwar Kumar, Dave (Jing)Tian, Saman Zonouz, Raheem Beyah, Dongyan Xu, Z. Berkay Celik</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02960">https://arxiv.org/abs/2510.02960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02960">https://arxiv.org/pdf/2510.02960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02960]] SoK: Kicking CAN Down the Road. Systematizing CAN Security Knowledge(https://arxiv.org/abs/2510.02960)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack</a></li>
<li><strong>Abstract: </strong>For decades, the Controller Area Network (CAN) has served as the primary in-vehicle bus (IVB) and extended its use to many non-vehicular systems. Over the past years, CAN security has been intensively scrutinized, yielding extensive research literature. Despite its wealth, the literature lacks structured systematization, complicating efforts to assess attack severity, defense efficacy, identify security gaps, or root causes. This leaves non experts uncertain about the relevancy of specific attacks or defenses to their systems, inadvertently portraying CAN as irredeemably insecure. Further, the introduction of new IVB technologies--CAN evolutions, add-ons, and alternative buses--with heightened security claims risks fostering the misconception that merely adopting these technologies resolves CAN's security challenges. This paper systematizes existing CAN security knowledge, presenting a comprehensive taxonomy and assessment models of attackers, attacks, and defenses. It identifies replicable attacks and defense gaps, investigating their root causes as inherent, accidental, unique, or universal. It then extrapolates these insights to emerging IVB technologies by formally analyzing three emerging IVBs to identify shared root causes with CAN and assess their ability to close security gaps. The findings challenge common perceptions, demonstrating that CAN is more securable than perceived, that most insecurity root causes are shared across IVBs, and that merely adopting newer IVB technology does not solve persistent security issues. The paper concludes by highlighting future research directions to secure IVB communication down the road.</li>
</ul>

<h3>Title: Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking</h3>
<ul>
<li><strong>Authors: </strong>Jingqi Zhang, Ruibo Chen, Yingqing Yang, Peihua Mai, Heng Huang, Yan Pang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02962">https://arxiv.org/abs/2510.02962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02962">https://arxiv.org/pdf/2510.02962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02962]] Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking(https://arxiv.org/abs/2510.02962)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, membership infer, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly fine-tuned on smaller, domain-specific datasets to improve downstream performance. These datasets often contain proprietary or copyrighted material, raising the need for reliable safeguards against unauthorized use. Existing membership inference attacks (MIAs) and dataset-inference methods typically require access to internal signals such as logits, while current black-box approaches often rely on handcrafted prompts or a clean reference dataset for calibration, both of which limit practical applicability. Watermarking is a promising alternative, but prior techniques can degrade text quality or reduce task performance. We propose TRACE, a practical framework for fully black-box detection of copyrighted dataset usage in LLM fine-tuning. \texttt{TRACE} rewrites datasets with distortion-free watermarks guided by a private key, ensuring both text quality and downstream utility. At detection time, we exploit the radioactivity effect of fine-tuning on watermarked data and introduce an entropy-gated procedure that selectively scores high-uncertainty tokens, substantially amplifying detection power. Across diverse datasets and model families, TRACE consistently achieves significant detections (p<0.05), often with extremely strong statistical evidence. Furthermore, it supports multi-dataset attribution and remains robust even after continued pretraining on large non-watermarked corpora. These results establish TRACE as a practical route to reliable black-box verification of copyrighted dataset usage. We will make our code available at: this https URL.</li>
</ul>

<h3>Title: External Data Extraction Attacks against Retrieval-Augmented Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yu He, Yifei Chen, Yiming Li, Shuo Shao, Leyi Qi, Boheng Li, Dacheng Tao, Zhan Qin</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02964">https://arxiv.org/abs/2510.02964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02964">https://arxiv.org/pdf/2510.02964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02964]] External Data Extraction Attacks against Retrieval-Augmented Large Language Models(https://arxiv.org/abs/2510.02964)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>In recent years, RAG has emerged as a key paradigm for enhancing large language models (LLMs). By integrating externally retrieved information, RAG alleviates issues like outdated knowledge and, crucially, insufficient domain expertise. While effective, RAG introduces new risks of external data extraction attacks (EDEAs), where sensitive or copyrighted data in its knowledge base may be extracted verbatim. These risks are particularly acute when RAG is used to customize specialized LLM applications with private knowledge bases. Despite initial studies exploring these risks, they often lack a formalized framework, robust attack performance, and comprehensive evaluation, leaving critical questions about real-world EDEA feasibility unanswered. In this paper, we present the first comprehensive study to formalize EDEAs against retrieval-augmented LLMs. We first formally define EDEAs and propose a unified framework decomposing their design into three components: extraction instruction, jailbreak operator, and retrieval trigger, under which prior attacks can be considered instances within our framework. Guided by this framework, we develop SECRET: a Scalable and EffeCtive exteRnal data Extraction aTtack. Specifically, SECRET incorporates (1) an adaptive optimization process using LLMs as optimizers to generate specialized jailbreak prompts for EDEAs, and (2) cluster-focused triggering, an adaptive strategy that alternates between global exploration and local exploitation to efficiently generate effective retrieval triggers. Extensive evaluations across 4 models reveal that SECRET significantly outperforms previous attacks, and is highly effective against all 16 tested RAG instances. Notably, SECRET successfully extracts 35% of the data from RAG powered by Claude 3.7 Sonnet for the first time, whereas other attacks yield 0% extraction. Our findings call for attention to this emerging threat.</li>
</ul>

<h3>Title: Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines</h3>
<ul>
<li><strong>Authors: </strong>Matthew Lewis, Samuel Thio, Richard JB Dobson, Spiros Denaxas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02967">https://arxiv.org/abs/2510.02967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02967">https://arxiv.org/pdf/2510.02967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02967]] Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines(https://arxiv.org/abs/2510.02967)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents the development and evaluation of a Retrieval-Augmented Generation (RAG) system for querying the United Kingdom's National Institute for Health and Care Excellence (NICE) clinical guidelines using Large Language Models (LLMs). The extensive length and volume of these guidelines can impede their utilisation within a time-constrained healthcare system, a challenge this project addresses through the creation of a system capable of providing users with precisely matched information in response to natural language queries. The system's retrieval architecture, composed of a hybrid embedding mechanism, was evaluated against a database of 10,195 text chunks derived from three hundred guidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR) of 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten retrieved chunks, when evaluated on 7901 queries. The most significant impact of the RAG system was observed during the generation phase. When evaluated on a manually curated dataset of seventy question-answer pairs, RAG-enhanced models showed substantial gains in performance. Faithfulness, the measure of whether an answer is supported by the source text, was increased by 64.7 percentage points to 99.5% for the RAG-enhanced O4-Mini model and significantly outperformed the medical-focused Meditron3-8B LLM, which scored 43%. This, combined with a perfect Context Precision score of 1 for all RAG-enhanced models, confirms the system's ability to prevent information fabrication by grounding its answers in relevant source material. This study thus establishes RAG as an effective, reliable, and scalable approach for applying generative AI in healthcare, enabling cost-effective access to medical guidelines.</li>
</ul>

<h3>Title: Flip Distribution Alignment VAE for Multi-Phase MRI Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyan Kui, Qianmu Xiao, Qqinsong Li, Zexin Ji, JIelin Zhang, Beiji Zou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02970">https://arxiv.org/abs/2510.02970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02970">https://arxiv.org/pdf/2510.02970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02970]] Flip Distribution Alignment VAE for Multi-Phase MRI Synthesis(https://arxiv.org/abs/2510.02970)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Separating shared and independent features is crucial for multi-phase contrast-enhanced (CE) MRI synthesis. However, existing methods use deep autoencoder generators with low parameter efficiency and lack interpretable training strategies. In this paper, we propose Flip Distribution Alignment Variational Autoencoder (FDA-VAE), a lightweight feature-decoupled VAE model for multi-phase CE MRI synthesis. Our method encodes input and target images into two latent distributions that are symmetric concerning a standard normal distribution, effectively separating shared and independent features. The Y-shaped bidirectional training strategy further enhances the interpretability of feature separation. Experimental results show that compared to existing deep autoencoder-based end-to-end synthesis methods, FDA-VAE significantly reduces model parameters and inference time while effectively improving synthesis quality. The source code is publicly available at this https URL.</li>
</ul>

<h3>Title: Towards Scalable and Consistent 3D Editing</h3>
<ul>
<li><strong>Authors: </strong>Ruihao Xia, Yang Tang, Pan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02994">https://arxiv.org/abs/2510.02994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02994">https://arxiv.org/pdf/2510.02994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02994]] Towards Scalable and Consistent 3D Editing(https://arxiv.org/abs/2510.02994)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>3D editing - the task of locally modifying the geometry or appearance of a 3D asset - has wide applications in immersive content creation, digital entertainment, and AR/VR. However, unlike 2D editing, it remains challenging due to the need for cross-view consistency, structural fidelity, and fine-grained controllability. Existing approaches are often slow, prone to geometric distortions, or dependent on manual and accurate 3D masks that are error-prone and impractical. To address these challenges, we advance both the data and model fronts. On the data side, we introduce 3DEditVerse, the largest paired 3D editing benchmark to date, comprising 116,309 high-quality training pairs and 1,500 curated test pairs. Built through complementary pipelines of pose-driven geometric edits and foundation model-guided appearance edits, 3DEditVerse ensures edit locality, multi-view consistency, and semantic alignment. On the model side, we propose 3DEditFormer, a 3D-structure-preserving conditional transformer. By enhancing image-to-3D generation with dual-guidance attention and time-adaptive gating, 3DEditFormer disentangles editable regions from preserved structure, enabling precise and consistent edits without requiring auxiliary 3D masks. Extensive experiments demonstrate that our framework outperforms state-of-the-art baselines both quantitatively and qualitatively, establishing a new standard for practical and scalable 3D editing. Dataset and code will be released. Project: this https URL</li>
</ul>

<h3>Title: Untargeted Jailbreak Attack</h3>
<ul>
<li><strong>Authors: </strong>Xinzhe Huang, Wenjing Hu, Tianhang Zheng, Kedong Xiu, Xiaojun Jia, Di Wang, Zhan Qin, Kui Ren</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02999">https://arxiv.org/abs/2510.02999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02999">https://arxiv.org/pdf/2510.02999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02999]] Untargeted Jailbreak Attack(https://arxiv.org/abs/2510.02999)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Existing gradient-based jailbreak attacks on Large Language Models (LLMs), such as Greedy Coordinate Gradient (GCG) and COLD-Attack, typically optimize adversarial suffixes to align the LLM output with a predefined target response. However, by restricting the optimization objective as inducing a predefined target, these methods inherently constrain the adversarial search space, which limit their overall attack efficacy. Furthermore, existing methods typically require a large number of optimization iterations to fulfill the large gap between the fixed target and the original model response, resulting in low attack efficiency. To overcome the limitations of targeted jailbreak attacks, we propose the first gradient-based untargeted jailbreak attack (UJA), aiming to elicit an unsafe response without enforcing any predefined patterns. Specifically, we formulate an untargeted attack objective to maximize the unsafety probability of the LLM response, which can be quantified using a judge model. Since the objective is non-differentiable, we further decompose it into two differentiable sub-objectives for optimizing an optimal harmful response and the corresponding adversarial prompt, with a theoretical analysis to validate the decomposition. In contrast to targeted jailbreak attacks, UJA's unrestricted objective significantly expands the search space, enabling a more flexible and efficient exploration of LLM this http URL evaluations demonstrate that \textsc{UJA} can achieve over 80\% attack success rates against recent safety-aligned LLMs with only 100 optimization iterations, outperforming the state-of-the-art gradient-based attacks such as I-GCG and COLD-Attack by over 20\%.</li>
</ul>

<h3>Title: BrainIB++: Leveraging Graph Neural Networks and Information Bottleneck for Functional Brain Biomarkers in Schizophrenia</h3>
<ul>
<li><strong>Authors: </strong>Tianzheng Hu, Qiang Li, Shu Liu, Vince D. Calhoun, Guido van Wingen, Shujian Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03004">https://arxiv.org/abs/2510.03004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03004">https://arxiv.org/pdf/2510.03004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03004]] BrainIB++: Leveraging Graph Neural Networks and Information Bottleneck for Functional Brain Biomarkers in Schizophrenia(https://arxiv.org/abs/2510.03004)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The development of diagnostic models is gaining traction in the field of psychiatric disorders. Recently, machine learning classifiers based on resting-state functional magnetic resonance imaging (rs-fMRI) have been developed to identify brain biomarkers that differentiate psychiatric disorders from healthy controls. However, conventional machine learning-based diagnostic models often depend on extensive feature engineering, which introduces bias through manual intervention. While deep learning models are expected to operate without manual involvement, their lack of interpretability poses significant challenges in obtaining explainable and reliable brain biomarkers to support diagnostic decisions, ultimately limiting their clinical applicability. In this study, we introduce an end-to-end innovative graph neural network framework named BrainIB++, which applies the information bottleneck (IB) principle to identify the most informative data-driven brain regions as subgraphs during model training for interpretation. We evaluate the performance of our model against nine established brain network classification methods across three multi-cohort schizophrenia datasets. It consistently demonstrates superior diagnostic accuracy and exhibits generalizability to unseen data. Furthermore, the subgraphs identified by our model also correspond with established clinical biomarkers in schizophrenia, particularly emphasizing abnormalities in the visual, sensorimotor, and higher cognition brain functional network. This alignment enhances the model's interpretability and underscores its relevance for real-world diagnostic applications.</li>
</ul>

<h3>Title: Not every day is a sunny day: Synthetic cloud injection for deep land cover segmentation robustness evaluation across data sources</h3>
<ul>
<li><strong>Authors: </strong>Sara Mobsite, Renaud Hostache, Laure Berti Equille, Emmanuel Roux, Joris Guerin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03006">https://arxiv.org/abs/2510.03006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03006">https://arxiv.org/pdf/2510.03006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03006]] Not every day is a sunny day: Synthetic cloud injection for deep land cover segmentation robustness evaluation across data sources(https://arxiv.org/abs/2510.03006)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Supervised deep learning for land cover semantic segmentation (LCS) relies on labeled satellite data. However, most existing Sentinel-2 datasets are cloud-free, which limits their usefulness in tropical regions where clouds are common. To properly evaluate the extent of this problem, we developed a cloud injection algorithm that simulates realistic cloud cover, allowing us to test how Sentinel-1 radar data can fill in the gaps caused by cloud-obstructed optical imagery. We also tackle the issue of losing spatial and/or spectral details during encoder downsampling in deep networks. To mitigate this loss, we propose a lightweight method that injects Normalized Difference Indices (NDIs) into the final decoding layers, enabling the model to retain key spatial features with minimal additional computation. Injecting NDIs enhanced land cover segmentation performance on the DFC2020 dataset, yielding improvements of 1.99% for U-Net and 2.78% for DeepLabV3 on cloud-free imagery. Under cloud-covered conditions, incorporating Sentinel-1 data led to significant performance gains across all models compared to using optical data alone, highlighting the effectiveness of radar-optical fusion in challenging atmospheric scenarios.</li>
</ul>

<h3>Title: PocketSR: The Super-Resolution Expert in Your Pocket Mobiles</h3>
<ul>
<li><strong>Authors: </strong>Haoze Sun, Linfeng Jiang, Fan Li, Renjing Pei, Zhixin Wang, Yong Guo, Jiaqi Xu, Haoyu Chen, Jin Han, Fenglong Song, Yujiu Yang, Wenbo Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03012">https://arxiv.org/abs/2510.03012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03012">https://arxiv.org/pdf/2510.03012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03012]] PocketSR: The Super-Resolution Expert in Your Pocket Mobiles(https://arxiv.org/abs/2510.03012)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Real-world image super-resolution (RealSR) aims to enhance the visual quality of in-the-wild images, such as those captured by mobile phones. While existing methods leveraging large generative models demonstrate impressive results, the high computational cost and latency make them impractical for edge deployment. In this paper, we introduce PocketSR, an ultra-lightweight, single-step model that brings generative modeling capabilities to RealSR while maintaining high fidelity. To achieve this, we design LiteED, a highly efficient alternative to the original computationally intensive VAE in SD, reducing parameters by 97.5% while preserving high-quality encoding and decoding. Additionally, we propose online annealing pruning for the U-Net, which progressively shifts generative priors from heavy modules to lightweight counterparts, ensuring effective knowledge transfer and further optimizing efficiency. To mitigate the loss of prior knowledge during pruning, we incorporate a multi-layer feature distillation loss. Through an in-depth analysis of each design component, we provide valuable insights for future research. PocketSR, with a model size of 146M parameters, processes 4K images in just 0.8 seconds, achieving a remarkable speedup over previous methods. Notably, it delivers performance on par with state-of-the-art single-step and even multi-step RealSR models, making it a highly practical solution for edge-device applications.</li>
</ul>

<h3>Title: Learning Robust Diffusion Models from Imprecise Supervision</h3>
<ul>
<li><strong>Authors: </strong>Dong-Dong Wu, Jiacheng Cui, Wei Wang, Zhiqiang She, Masashi Sugiyama</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03016">https://arxiv.org/abs/2510.03016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03016">https://arxiv.org/pdf/2510.03016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03016]] Learning Robust Diffusion Models from Imprecise Supervision(https://arxiv.org/abs/2510.03016)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Conditional diffusion models have achieved remarkable success in various generative tasks recently, but their training typically relies on large-scale datasets that inevitably contain imprecise information in conditional inputs. Such supervision, often stemming from noisy, ambiguous, or incomplete labels, will cause condition mismatch and degrade generation quality. To address this challenge, we propose DMIS, a unified framework for training robust Diffusion Models from Imprecise Supervision, which is the first systematic study within diffusion models. Our framework is derived from likelihood maximization and decomposes the objective into generative and classification components: the generative component models imprecise-label distributions, while the classification component leverages a diffusion classifier to infer class-posterior probabilities, with its efficiency further improved by an optimized timestep sampling strategy. Extensive experiments on diverse forms of imprecise supervision, covering tasks of image generation, weakly supervised learning, and noisy dataset condensation demonstrate that DMIS consistently produces high-quality and class-discriminative samples.</li>
</ul>

<h3>Title: Differentially Private Wasserstein Barycenters</h3>
<ul>
<li><strong>Authors: </strong>Anming Gu, Sasidhar Kunapuli, Mark Bun, Edward Chien, Kristjan Greenewald</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03021">https://arxiv.org/abs/2510.03021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03021">https://arxiv.org/pdf/2510.03021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03021]] Differentially Private Wasserstein Barycenters(https://arxiv.org/abs/2510.03021)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The Wasserstein barycenter is defined as the mean of a set of probability measures under the optimal transport metric, and has numerous applications spanning machine learning, statistics, and computer graphics. In practice these input measures are empirical distributions built from sensitive datasets, motivating a differentially private (DP) treatment. We present, to our knowledge, the first algorithms for computing Wasserstein barycenters under differential privacy. Empirically, on synthetic data, MNIST, and large-scale U.S. population datasets, our methods produce high-quality private barycenters with strong accuracy-privacy tradeoffs.</li>
</ul>

<h3>Title: Lightweight Transformer for EEG Classification via Balanced Signed Graph Algorithm Unrolling</h3>
<ul>
<li><strong>Authors: </strong>Junyi Yao, Parham Eftekhar, Gene Cheung, Xujin Chris Liu, Yao Wang, Wei Hu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03027">https://arxiv.org/abs/2510.03027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03027">https://arxiv.org/pdf/2510.03027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03027]] Lightweight Transformer for EEG Classification via Balanced Signed Graph Algorithm Unrolling(https://arxiv.org/abs/2510.03027)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Samples of brain signals collected by EEG sensors have inherent anti-correlations that are well modeled by negative edges in a finite graph. To differentiate epilepsy patients from healthy subjects using collected EEG signals, we build lightweight and interpretable transformer-like neural nets by unrolling a spectral denoising algorithm for signals on a balanced signed graph -- graph with no cycles of odd number of negative edges. A balanced signed graph has well-defined frequencies that map to a corresponding positive graph via similarity transform of the graph Laplacian matrices. We implement an ideal low-pass filter efficiently on the mapped positive graph via Lanczos approximation, where the optimal cutoff frequency is learned from data. Given that two balanced signed graph denoisers learn posterior probabilities of two different signal classes during training, we evaluate their reconstruction errors for binary classification of EEG signals. Experiments show that our method achieves classification performance comparable to representative deep learning schemes, while employing dramatically fewer parameters.</li>
</ul>

<h3>Title: Protecting Persona Biometric Data: The Case of Facial Privacy</h3>
<ul>
<li><strong>Authors: </strong>Lambert Hogenhout, Rinzin Wangmo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03035">https://arxiv.org/abs/2510.03035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03035">https://arxiv.org/pdf/2510.03035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03035]] Protecting Persona Biometric Data: The Case of Facial Privacy(https://arxiv.org/abs/2510.03035)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, biometric</a></li>
<li><strong>Abstract: </strong>The proliferation of digital technologies has led to unprecedented data collection, with facial data emerging as a particularly sensitive commodity. Companies are increasingly leveraging advanced facial recognition technologies, often without the explicit consent or awareness of individuals, to build sophisticated surveillance capabilities. This practice, fueled by weak and fragmented laws in many jurisdictions, has created a regulatory vacuum that allows for the commercialization of personal identity and poses significant threats to individual privacy and autonomy. This article introduces the concept of Facial Privacy. It analyzes the profound challenges posed by unregulated facial recognition by conducting a comprehensive review of existing legal frameworks. It examines and compares regulations such as the GDPR, Brazil's LGPD, Canada's PIPEDA, and privacy acts in China, Singapore, South Korea, and Japan, alongside sector-specific laws in the United States like the Illinois Biometric Information Privacy Act (BIPA). The analysis highlights the societal impacts of this technology, including the potential for discriminatory bias and the long-lasting harm that can result from the theft of immutable biometric data. Ultimately, the paper argues that existing legal loopholes and ambiguities leave individuals vulnerable. It proposes a new policy framework that shifts the paradigm from data as property to a model of inalienable rights, ensuring that fundamental human rights are upheld against unchecked technological expansion.</li>
</ul>

<h3>Title: ZeroShotOpt: Towards Zero-Shot Pretrained Models for Efficient Black-Box Optimization</h3>
<ul>
<li><strong>Authors: </strong>Jamison Meindl, Yunsheng Tian, Tony Cui, Veronika Thost, Zhang-Wei Hong, Johannes Dürholt, Jie Chen, Wojciech Matusik, Mina Konaković Luković</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03051">https://arxiv.org/abs/2510.03051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03051">https://arxiv.org/pdf/2510.03051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03051]] ZeroShotOpt: Towards Zero-Shot Pretrained Models for Efficient Black-Box Optimization(https://arxiv.org/abs/2510.03051)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Global optimization of expensive, derivative-free black-box functions requires extreme sample efficiency. While Bayesian optimization (BO) is the current state-of-the-art, its performance hinges on surrogate and acquisition function hyper-parameters that are often hand-tuned and fail to generalize across problem landscapes. We present ZeroShotOpt, a general-purpose, pretrained model for continuous black-box optimization tasks ranging from 2D to 20D. Our approach leverages offline reinforcement learning on large-scale optimization trajectories collected from 12 BO variants. To scale pretraining, we generate millions of synthetic Gaussian process-based functions with diverse landscapes, enabling the model to learn transferable optimization policies. As a result, ZeroShotOpt achieves robust zero-shot generalization on a wide array of unseen benchmarks, matching or surpassing the sample efficiency of leading global optimizers, including BO, while also offering a reusable foundation for future extensions and improvements. Our open-source code, dataset, and model are available at: this https URL</li>
</ul>

<h3>Title: Comparative Analysis of Parameterized Action Actor-Critic Reinforcement Learning Algorithms for Web Search Match Plan Generation</h3>
<ul>
<li><strong>Authors: </strong>Ubayd Bapoo, Clement N Nyirenda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03064">https://arxiv.org/abs/2510.03064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03064">https://arxiv.org/pdf/2510.03064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03064]] Comparative Analysis of Parameterized Action Actor-Critic Reinforcement Learning Algorithms for Web Search Match Plan Generation(https://arxiv.org/abs/2510.03064)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This study evaluates the performance of Soft Actor Critic (SAC), Greedy Actor Critic (GAC), and Truncated Quantile Critics (TQC) in high-dimensional decision-making tasks using fully observable environments. The focus is on parametrized action (PA) spaces, eliminating the need for recurrent networks, with benchmarks Platform-v0 and Goal-v0 testing discrete actions linked to continuous action-parameter spaces. Hyperparameter optimization was performed with Microsoft NNI, ensuring reproducibility by modifying the codebase for GAC and TQC. Results show that Parameterized Action Greedy Actor-Critic (PAGAC) outperformed other algorithms, achieving the fastest training times and highest returns across benchmarks, completing 5,000 episodes in 41:24 for the Platform game and 24:04 for the Robot Soccer Goal game. Its speed and stability provide clear advantages in complex action spaces. Compared to PASAC and PATQC, PAGAC demonstrated superior efficiency and reliability, making it ideal for tasks requiring rapid convergence and robust performance. Future work could explore hybrid strategies combining entropy-regularization with truncation-based methods to enhance stability and expand investigations into generalizability.</li>
</ul>

<h3>Title: A Unified Deep Reinforcement Learning Approach for Close Enough Traveling Salesman Problem</h3>
<ul>
<li><strong>Authors: </strong>Mingfeng Fan, Jiaqi Cheng, Yaoxin Wu, Yifeng Zhang, Yibin Yang, Guohua Wu, Guillaume Sartoretti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03065">https://arxiv.org/abs/2510.03065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03065">https://arxiv.org/pdf/2510.03065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03065]] A Unified Deep Reinforcement Learning Approach for Close Enough Traveling Salesman Problem(https://arxiv.org/abs/2510.03065)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>In recent years, deep reinforcement learning (DRL) has gained traction for solving the NP-hard traveling salesman problem (TSP). However, limited attention has been given to the close-enough TSP (CETSP), primarily due to the challenge introduced by its neighborhood-based visitation criterion, wherein a node is considered visited if the agent enters a compact neighborhood around it. In this work, we formulate a Markov decision process (MDP) for CETSP using a discretization scheme and propose a novel unified dual-decoder DRL (UD3RL) framework that separates decision-making into node selection and waypoint determination. Specifically, an adapted encoder is employed for effective feature extraction, followed by a node-decoder and a loc-decoder to handle the two sub-tasks, respectively. A k-nearest neighbors subgraph interaction strategy is further introduced to enhance spatial reasoning during location decoding. Furthermore, we customize the REINFORCE algorithm to train UD3RL as a unified model capable of generalizing across different problem sizes and varying neighborhood radius types (i.e., constant and random radii). Experimental results show that UD3RL outperforms conventional methods in both solution quality and runtime, while exhibiting strong generalization across problem scales, spatial distributions, and radius ranges, as well as robustness to dynamic environments.</li>
</ul>

<h3>Title: InsideOut: An EfficientNetV2-S Based Deep Learning Framework for Robust Multi-Class Facial Emotion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Ahsan Farabi, Israt Khandaker, Ibrahim Khalil Shanto, Md Abdul Ahad Minhaz, Tanisha Zaman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03066">https://arxiv.org/abs/2510.03066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03066">https://arxiv.org/pdf/2510.03066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03066]] InsideOut: An EfficientNetV2-S Based Deep Learning Framework for Robust Multi-Class Facial Emotion Recognition(https://arxiv.org/abs/2510.03066)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Facial Emotion Recognition (FER) is a key task in affective computing, enabling applications in human-computer interaction, e-learning, healthcare, and safety systems. Despite advances in deep learning, FER remains challenging due to occlusions, illumination and pose variations, subtle intra-class differences, and dataset imbalance that hinders recognition of minority emotions. We present InsideOut, a reproducible FER framework built on EfficientNetV2-S with transfer learning, strong data augmentation, and imbalance-aware optimization. The approach standardizes FER2013 images, applies stratified splitting and augmentation, and fine-tunes a lightweight classification head with class-weighted loss to address skewed distributions. InsideOut achieves 62.8% accuracy with a macro averaged F1 of 0.590 on FER2013, showing competitive results compared to conventional CNN baselines. The novelty lies in demonstrating that efficient architectures, combined with tailored imbalance handling, can provide practical, transparent, and reproducible FER solutions.</li>
</ul>

<h3>Title: What Drives Compositional Generalization in Visual Generative Models?</h3>
<ul>
<li><strong>Authors: </strong>Karim Farid, Rajat Sahay, Yumna Ali Alnaggar, Simon Schrodi, Volker Fischer, Cordelia Schmid, Thomas Brox</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03075">https://arxiv.org/abs/2510.03075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03075">https://arxiv.org/pdf/2510.03075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03075]] What Drives Compositional Generalization in Visual Generative Models?(https://arxiv.org/abs/2510.03075)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Compositional generalization, the ability to generate novel combinations of known concepts, is a key ingredient for visual generative models. Yet, not all mechanisms that enable or inhibit it are fully understood. In this work, we conduct a systematic study of how various design choices influence compositional generalization in image and video generation in a positive or negative way. Through controlled experiments, we identify two key factors: (i) whether the training objective operates on a discrete or continuous distribution, and (ii) to what extent conditioning provides information about the constituent concepts during training. Building on these insights, we show that relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based objective can improve compositional performance in discrete models like MaskGIT.</li>
</ul>

<h3>Title: Latent Diffusion Unlearning: Protecting Against Unauthorized Personalization Through Trajectory Shifted Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Naresh Kumar Devulapally, Shruti Agarwal, Tejas Gokhale, Vishnu Suresh Lokhande</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03089">https://arxiv.org/abs/2510.03089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03089">https://arxiv.org/pdf/2510.03089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03089]] Latent Diffusion Unlearning: Protecting Against Unauthorized Personalization Through Trajectory Shifted Perturbations(https://arxiv.org/abs/2510.03089)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense, attack, robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have demonstrated remarkable effectiveness in rapid and high-fidelity personalization, even when provided with only a few user images. However, the effectiveness of personalization techniques has lead to concerns regarding data privacy, intellectual property protection, and unauthorized usage. To mitigate such unauthorized usage and model replication, the idea of generating ``unlearnable'' training samples utilizing image poisoning techniques has emerged. Existing methods for this have limited imperceptibility as they operate in the pixel space which results in images with noise and artifacts. In this work, we propose a novel model-based perturbation strategy that operates within the latent space of diffusion models. Our method alternates between denoising and inversion while modifying the starting point of the denoising trajectory: of diffusion models. This trajectory-shifted sampling ensures that the perturbed images maintain high visual fidelity to the original inputs while being resistant to inversion and personalization by downstream generative models. This approach integrates unlearnability into the framework of Latent Diffusion Models (LDMs), enabling a practical and imperceptible defense against unauthorized model adaptation. We validate our approach on four benchmark datasets to demonstrate robustness against state-of-the-art inversion attacks. Results demonstrate that our method achieves significant improvements in imperceptibility ($\sim 8 \% -10\%$ on perceptual metrics including PSNR, SSIM, and FID) and robustness ( $\sim 10\%$ on average across five adversarial settings), highlighting its effectiveness in safeguarding sensitive data.</li>
</ul>

<h3>Title: Distilled Protein Backbone Generation</h3>
<ul>
<li><strong>Authors: </strong>Liyang Xie, Haoran Zhang, Zhendong Wang, Wesley Tansey, Mingyuan Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03095">https://arxiv.org/abs/2510.03095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03095">https://arxiv.org/pdf/2510.03095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03095]] Distilled Protein Backbone Generation(https://arxiv.org/abs/2510.03095)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion- and flow-based generative models have recently demonstrated strong performance in protein backbone generation tasks, offering unprecedented capabilities for de novo protein design. However, while achieving notable performance in generation quality, these models are limited by their generating speed, often requiring hundreds of iterative steps in the reverse-diffusion process. This computational bottleneck limits their practical utility in large-scale protein discovery, where thousands to millions of candidate structures are needed. To address this challenge, we explore the techniques of score distillation, which has shown great success in reducing the number of sampling steps in the vision domain while maintaining high generation quality. However, a straightforward adaptation of these methods results in unacceptably low designability. Through extensive study, we have identified how to appropriately adapt Score identity Distillation (SiD), a state-of-the-art score distillation strategy, to train few-step protein backbone generators which significantly reduce sampling time, while maintaining comparable performance to their pretrained teacher model. In particular, multistep generation combined with inference time noise modulation is key to the success. We demonstrate that our distilled few-step generators achieve more than a 20-fold improvement in sampling speed, while achieving similar levels of designability, diversity, and novelty as the Proteina teacher model. This reduction in inference cost enables large-scale in silico protein design, thereby bringing diffusion-based models closer to real-world protein engineering applications.</li>
</ul>

<h3>Title: Semantic Similarity in Radiology Reports via LLMs and NER</h3>
<ul>
<li><strong>Authors: </strong>Beth Pearson, Ahmed Adnan, Zahraa Abdallah</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03102">https://arxiv.org/abs/2510.03102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03102">https://arxiv.org/pdf/2510.03102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03102]] Semantic Similarity in Radiology Reports via LLMs and NER(https://arxiv.org/abs/2510.03102)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Radiology report evaluation is a crucial part of radiologists' training and plays a key role in ensuring diagnostic accuracy. As part of the standard reporting workflow, a junior radiologist typically prepares a preliminary report, which is then reviewed and edited by a senior radiologist to produce the final report. Identifying semantic differences between preliminary and final reports is essential for junior doctors, both as a training tool and to help uncover gaps in clinical knowledge. While AI in radiology is a rapidly growing field, the application of large language models (LLMs) remains challenging due to the need for specialised domain knowledge. In this paper, we explore the ability of LLMs to provide explainable and accurate comparisons of reports in the radiology domain. We begin by comparing the performance of several LLMs in comparing radiology reports. We then assess a more traditional approach based on Named-Entity-Recognition (NER). However, both approaches exhibit limitations in delivering accurate feedback on semantic similarity. To address this, we propose Llama-EntScore, a semantic similarity scoring method using a combination of Llama 3.1 and NER with tunable weights to emphasise or de-emphasise specific types of differences. Our approach generates a quantitative similarity score for tracking progress and also gives an interpretation of the score that aims to offer valuable guidance in reviewing and refining their reporting. We find our method achieves 67% exact-match accuracy and 93% accuracy within +/- 1 when compared to radiologist-provided ground truth scores - outperforming both LLMs and NER used independently. Code is available at: \href{this https URL}{this http URL\_reports}</li>
</ul>

<h3>Title: GeoComplete: Geometry-Aware Diffusion for Reference-Driven Image Completion</h3>
<ul>
<li><strong>Authors: </strong>Beibei Lin, Tingting Chen, Robby T. Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03110">https://arxiv.org/abs/2510.03110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03110">https://arxiv.org/pdf/2510.03110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03110]] GeoComplete: Geometry-Aware Diffusion for Reference-Driven Image Completion(https://arxiv.org/abs/2510.03110)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Reference-driven image completion, which restores missing regions in a target view using additional images, is particularly challenging when the target view differs significantly from the references. Existing generative methods rely solely on diffusion priors and, without geometric cues such as camera pose or depth, often produce misaligned or implausible content. We propose GeoComplete, a novel framework that incorporates explicit 3D structural guidance to enforce geometric consistency in the completed regions, setting it apart from prior image-only approaches. GeoComplete introduces two key ideas: conditioning the diffusion process on projected point clouds to infuse geometric information, and applying target-aware masking to guide the model toward relevant reference cues. The framework features a dual-branch diffusion architecture. One branch synthesizes the missing regions from the masked target, while the other extracts geometric features from the projected point cloud. Joint self-attention across branches ensures coherent and accurate completion. To address regions visible in references but absent in the target, we project the target view into each reference to detect occluded areas, which are then masked during training. This target-aware masking directs the model to focus on useful cues, enhancing performance in difficult scenarios. By integrating a geometry-aware dual-branch diffusion architecture with a target-aware masking strategy, GeoComplete offers a unified and robust solution for geometry-conditioned image completion. Experiments show that GeoComplete achieves a 17.1 PSNR improvement over state-of-the-art methods, significantly boosting geometric accuracy while maintaining high visual quality.</li>
</ul>

<h3>Title: Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation</h3>
<ul>
<li><strong>Authors: </strong>Jacobo Romero-Díaz, Gerard I. Gállego, Oriol Pareras, Federico Costa, Javier Hernando, Cristina España-Bonet</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03115">https://arxiv.org/abs/2510.03115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03115">https://arxiv.org/pdf/2510.03115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03115]] Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation(https://arxiv.org/abs/2510.03115)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Speech-to-Text Translation (S2TT) systems built from Automatic Speech Recognition (ASR) and Text-to-Text Translation (T2TT) modules face two major limitations: error propagation and the inability to exploit prosodic or other acoustic cues. Chain-of-Thought (CoT) prompting has recently been introduced, with the expectation that jointly accessing speech and transcription will overcome these issues. Analyzing CoT through attribution methods, robustness evaluations with corrupted transcripts, and prosody-awareness, we find that it largely mirrors cascaded behavior, relying mainly on transcripts while barely leveraging speech. Simple training interventions, such as adding Direct S2TT data or noisy transcript injection, enhance robustness and increase speech attribution. These findings challenge the assumed advantages of CoT and highlight the need for architectures that explicitly integrate acoustic information into translation.</li>
</ul>

<h3>Title: Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction</h3>
<ul>
<li><strong>Authors: </strong>Kaisi Guan, Xihua Wang, Zhengfeng Lai, Xin Cheng, Peng Zhang, XiaoJiang Liu, Ruihua Song, Meng Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03117">https://arxiv.org/abs/2510.03117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03117">https://arxiv.org/pdf/2510.03117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03117]] Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction(https://arxiv.org/abs/2510.03117)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>This study focuses on a challenging yet promising task, Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with synchronized audio from text conditions, meanwhile ensuring both modalities are aligned with text. Despite progress in joint audio-video training, two critical challenges still remain unaddressed: (1) a single, shared text caption where the text for video is equal to the text for audio often creates modal interference, confusing the pretrained backbones, and (2) the optimal mechanism for cross-modal feature interaction remains unclear. To address these challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC) framework that generates pairs of disentangled captions, a video caption, and an audio caption, eliminating interference at the conditioning stage. Based on HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer, which employs a Dual CrossAttention (DCA) mechanism that acts as a robust ``bridge" to enable a symmetric, bidirectional exchange of information, achieving both semantic and temporal synchronization. Extensive experiments on three benchmark datasets, supported by human evaluations, demonstrate that our method achieves state-of-the-art results on most metrics. Comprehensive ablation studies further validate the effectiveness of our contributions, offering key insights for the future T2SV task. All the codes and checkpoints will be publicly released.</li>
</ul>

<h3>Title: HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Shiyi Zhang, Dong Liang, Hairong Zheng, Yihang Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03122">https://arxiv.org/abs/2510.03122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03122">https://arxiv.org/pdf/2510.03122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03122]] HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion(https://arxiv.org/abs/2510.03122)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The reconstruction of visual information from brain activity fosters interdisciplinary integration between neuroscience and computer vision. However, existing methods still face challenges in accurately recovering highly complex visual stimuli. This difficulty stems from the characteristics of natural scenes: low-level features exhibit heterogeneity, while high-level features show semantic entanglement due to contextual overlaps. Inspired by the hierarchical representation theory of the visual cortex, we propose the HAVIR model, which separates the visual cortex into two hierarchical regions and extracts distinct features from each. Specifically, the Structural Generator extracts structural information from spatial processing voxels and converts it into latent diffusion priors, while the Semantic Extractor converts semantic processing voxels into CLIP embeddings. These components are integrated via the Versatile Diffusion model to synthesize the final image. Experimental results demonstrate that HAVIR enhances both the structural and semantic quality of reconstructions, even in complex scenes, and outperforms existing models.</li>
</ul>

<h3>Title: Signature-Informed Transformer for Asset Allocation</h3>
<ul>
<li><strong>Authors: </strong>Yoontae Hwang, Stefan Zohren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-fin.PM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03129">https://arxiv.org/abs/2510.03129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03129">https://arxiv.org/pdf/2510.03129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03129]] Signature-Informed Transformer for Asset Allocation(https://arxiv.org/abs/2510.03129)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Robust asset allocation is a key challenge in quantitative finance, where deep-learning forecasters often fail due to objective mismatch and error amplification. We introduce the Signature-Informed Transformer (SIT), a novel framework that learns end-to-end allocation policies by directly optimizing a risk-aware financial objective. SIT's core innovations include path signatures for a rich geometric representation of asset dynamics and a signature-augmented attention mechanism embedding financial inductive biases, like lead-lag effects, into the model. Evaluated on daily S\&P 100 equity data, SIT decisively outperforms traditional and deep-learning baselines, especially when compared to predict-then-optimize models. These results indicate that portfolio-aware objectives and geometry-aware inductive biases are essential for risk-aware capital allocation in machine-learning systems. The code is available at: this https URL</li>
</ul>

<h3>Title: Enhancing XAI Narratives through Multi-Narrative Refinement and Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Flavio Giorgi, Matteo Silvestri, Cesare Campagnano, Fabrizio Silvestri, Gabriele Tolomei</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03134">https://arxiv.org/abs/2510.03134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03134">https://arxiv.org/pdf/2510.03134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03134]] Enhancing XAI Narratives through Multi-Narrative Refinement and Knowledge Distillation(https://arxiv.org/abs/2510.03134)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>Explainable Artificial Intelligence has become a crucial area of research, aiming to demystify the decision-making processes of deep learning models. Among various explainability techniques, counterfactual explanations have been proven particularly promising, as they offer insights into model behavior by highlighting minimal changes that would alter a prediction. Despite their potential, these explanations are often complex and technical, making them difficult for non-experts to interpret. To address this challenge, we propose a novel pipeline that leverages Language Models, large and small, to compose narratives for counterfactual explanations. We employ knowledge distillation techniques along with a refining mechanism to enable Small Language Models to perform comparably to their larger counterparts while maintaining robust reasoning abilities. In addition, we introduce a simple but effective evaluation method to assess natural language narratives, designed to verify whether the models' responses are in line with the factual, counterfactual ground truth. As a result, our proposed pipeline enhances both the reasoning capabilities and practical performance of student models, making them more suitable for real-world use cases.</li>
</ul>

<h3>Title: Beyond the Final Layer: Intermediate Representations for Better Multilingual Calibration in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ej Zhou, Caiqi Zhang, Tiancheng Hu, Chengzu Li, Nigel Collier, Ivan Vulić, Anna Korhonen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03136">https://arxiv.org/abs/2510.03136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03136">https://arxiv.org/pdf/2510.03136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03136]] Beyond the Final Layer: Intermediate Representations for Better Multilingual Calibration in Large Language Models(https://arxiv.org/abs/2510.03136)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Confidence calibration, the alignment of a model's predicted confidence with its actual accuracy, is crucial for the reliable deployment of Large Language Models (LLMs). However, this critical property remains largely under-explored in multilingual contexts. In this work, we conduct the first large-scale, systematic studies of multilingual calibration across six model families and over 100 languages, revealing that non-English languages suffer from systematically worse calibration. To diagnose this, we investigate the model's internal representations and find that the final layer, biased by English-centric training, provides a poor signal for multilingual confidence. In contrast, our layer-wise analysis uncovers a key insight that late-intermediate layers consistently offer a more reliable and better-calibrated signal. Building on this, we introduce a suite of training-free methods, including Language-Aware Confidence Ensemble (LACE), which adaptively selects an optimal ensemble of layers for each specific language. Our study highlights the hidden costs of English-centric alignment and offer a new path toward building more globally equitable and trustworthy LLMs by looking beyond the final layer.</li>
</ul>

<h3>Title: Taming Imperfect Process Verifiers: A Sampling Perspective on Backtracking</h3>
<ul>
<li><strong>Authors: </strong>Dhruv Rohatgi, Abhishek Shetty, Donya Saless, Yuchen Li, Ankur Moitra, Andrej Risteski, Dylan J. Foster</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03149">https://arxiv.org/abs/2510.03149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03149">https://arxiv.org/pdf/2510.03149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03149]] Taming Imperfect Process Verifiers: A Sampling Perspective on Backtracking(https://arxiv.org/abs/2510.03149)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Test-time algorithms that combine the generative power of language models with process verifiers that assess the quality of partial generations offer a promising lever for eliciting new reasoning capabilities, but the algorithmic design space and computational scaling properties of such approaches are still opaque, and their benefits are far from apparent when one accounts for the cost of learning a high-quality verifier. Our starting point is the observation that seemingly benign errors in a learned verifier can lead to catastrophic failures for standard decoding techniques due to error amplification during the course of generation. We then ask: can this be improved with more sophisticated decoding strategies? We introduce a new process-guided test-time sampling algorithm, VGB, which uses theoretically grounded backtracking to achieve provably better robustness to verifier errors. VGB interprets autoregressive generation as a random walk on a tree of partial generations, with transition probabilities guided by the process verifier and base model; crucially, backtracking occurs probabilistically. This process generalizes the seminal Sinclair-Jerrum random walk (Sinclair & Jerrum, 1989) from the literature on approximate counting and sampling in theoretical computer science, and a conceptual contribution of our work is to highlight parallels with this literature. Empirically, we demonstrate on both synthetic and real language modeling tasks that VGB outperforms baselines on a variety of metrics.</li>
</ul>

<h3>Title: Mixture of Many Zero-Compute Experts: A High-Rate Quantization Theory Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yehuda Dar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03151">https://arxiv.org/abs/2510.03151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03151">https://arxiv.org/pdf/2510.03151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03151]] Mixture of Many Zero-Compute Experts: A High-Rate Quantization Theory Perspective(https://arxiv.org/abs/2510.03151)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper uses classical high-rate quantization theory to provide new insights into mixture-of-experts (MoE) models for regression tasks. Our MoE is defined by a segmentation of the input space to regions, each with a single-parameter expert that acts as a constant predictor with zero-compute at inference. Motivated by high-rate quantization theory assumptions, we assume that the number of experts is sufficiently large to make their input-space regions very small. This lets us to study the approximation error of our MoE model class: (i) for one-dimensional inputs, we formulate the test error and its minimizing segmentation and experts; (ii) for multidimensional inputs, we formulate an upper bound for the test error and study its minimization. Moreover, we consider the learning of the expert parameters from a training dataset, given an input-space segmentation, and formulate their statistical learning properties. This leads us to theoretically and empirically show how the tradeoff between approximation and estimation errors in MoE learning depends on the number of experts.</li>
</ul>

<h3>Title: EditLens: Quantifying the Extent of AI Editing in Text</h3>
<ul>
<li><strong>Authors: </strong>Katherine Thai, Bradley Emi, Elyas Masrour, Mohit Iyyer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03154">https://arxiv.org/abs/2510.03154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03154">https://arxiv.org/pdf/2510.03154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03154]] EditLens: Quantifying the Extent of AI Editing in Text(https://arxiv.org/abs/2510.03154)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A significant proportion of queries to large language models ask them to edit user-provided text, rather than generate new text from scratch. While previous work focuses on detecting fully AI-generated text, we demonstrate that AI-edited text is distinguishable from human-written and AI-generated text. First, we propose using lightweight similarity metrics to quantify the magnitude of AI editing present in a text given the original human-written text and validate these metrics with human annotators. Using these similarity metrics as intermediate supervision, we then train EditLens, a regression model that predicts the amount of AI editing present within a text. Our model achieves state-of-the-art performance on both binary (F1=94.7%) and ternary (F1=90.4%) classification tasks in distinguishing human, AI, and mixed writing. Not only do we show that AI-edited text can be detected, but also that the degree of change made by AI to human writing can be detected, which has implications for authorship attribution, education, and policy. Finally, as a case study, we use our model to analyze the effects of AI-edits applied by Grammarly, a popular writing assistance tool. To encourage further research, we commit to publicly releasing our models and dataset.</li>
</ul>

<h3>Title: Neural Correlates of Language Models Are Specific to Human Language</h3>
<ul>
<li><strong>Authors: </strong>Iñigo Parra</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03156">https://arxiv.org/abs/2510.03156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03156">https://arxiv.org/pdf/2510.03156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03156]] Neural Correlates of Language Models Are Specific to Human Language(https://arxiv.org/abs/2510.03156)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Previous work has shown correlations between the hidden states of large language models and fMRI brain responses, on language tasks. These correlations have been taken as evidence of the representational similarity of these models and brain states. This study tests whether these previous results are robust to several possible concerns. Specifically this study shows: (i) that the previous results are still found after dimensionality reduction, and thus are not attributable to the curse of dimensionality; (ii) that previous results are confirmed when using new measures of similarity; (iii) that correlations between brain representations and those from models are specific to models trained on human language; and (iv) that the results are dependent on the presence of positional encoding in the models. These results confirm and strengthen the results of previous research and contribute to the debate on the biological plausibility and interpretability of state-of-the-art large language models.</li>
</ul>

<h3>Title: UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization</h3>
<ul>
<li><strong>Authors: </strong>Qing Huang, Zhipei Xu, Xuanyu Zhang, Jian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03161">https://arxiv.org/abs/2510.03161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03161">https://arxiv.org/pdf/2510.03161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03161]] UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization(https://arxiv.org/abs/2510.03161)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>With the rapid advancements in image generation, synthetic images have become increasingly realistic, posing significant societal risks, such as misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus emerges as essential for maintaining information integrity and societal security. Despite impressive performances by existing domain-specific detection methods, their practical applicability remains limited, primarily due to their narrow specialization, poor cross-domain generalization, and the absence of an integrated adaptive framework. To address these issues, we propose UniShield, the novel multi-agent-based unified system capable of detecting and localizing image forgeries across diverse domains, including image manipulation, document manipulation, DeepFake, and AI-generated images. UniShield innovatively integrates a perception agent with a detection agent. The perception agent intelligently analyzes image features to dynamically select suitable detection models, while the detection agent consolidates various expert detectors into a unified framework and generates interpretable reports. Extensive experiments show that UniShield achieves state-of-the-art results, surpassing both existing unified approaches and domain-specific detectors, highlighting its superior practicality, adaptiveness, and scalability.</li>
</ul>

<h3>Title: ROGR: Relightable 3D Objects using Generative Relighting</h3>
<ul>
<li><strong>Authors: </strong>Jiapeng Tang, Matthew Lavine, Dor Verbin, Stephan J. Garbin, Matthias Nießner, Ricardo Martin Brualla, Pratul P. Srinivasan, Philipp Henzler</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03163">https://arxiv.org/abs/2510.03163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03163">https://arxiv.org/pdf/2510.03163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03163]] ROGR: Relightable 3D Objects using Generative Relighting(https://arxiv.org/abs/2510.03163)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce ROGR, a novel approach that reconstructs a relightable 3D model of an object captured from multiple views, driven by a generative relighting model that simulates the effects of placing the object under novel environment illuminations. Our method samples the appearance of the object under multiple lighting environments, creating a dataset that is used to train a lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's appearance under any input environmental lighting. The lighting-conditioned NeRF uses a novel dual-branch architecture to encode the general lighting effects and specularities separately. The optimized lighting-conditioned NeRF enables efficient feed-forward relighting under arbitrary environment maps without requiring per-illumination optimization or light transport simulation. We evaluate our approach on the established TensoIR and Stanford-ORB datasets, where it improves upon the state-of-the-art on most metrics, and showcase our approach on real-world object captures.</li>
</ul>

<h3>Title: FTTE: Federated Learning on Resource-Constrained Devices</h3>
<ul>
<li><strong>Authors: </strong>Irene Tenison, Anna Murphy, Charles Beauville, Lalana Kagal</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03165">https://arxiv.org/abs/2510.03165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03165">https://arxiv.org/pdf/2510.03165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03165]] FTTE: Federated Learning on Resource-Constrained Devices(https://arxiv.org/abs/2510.03165)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy, but deployment on resource-constrained edge nodes remains challenging due to limited memory, energy, and communication bandwidth. Traditional synchronous and asynchronous FL approaches further suffer from straggler induced delays and slow convergence in heterogeneous, large scale networks. We present FTTE (Federated Tiny Training Engine),a novel semi-asynchronous FL framework that uniquely employs sparse parameter updates and a staleness-weighted aggregation based on both age and variance of client updates. Extensive experiments across diverse models and data distributions - including up to 500 clients and 90% stragglers - demonstrate that FTTE not only achieves 81% faster convergence, 80% lower on-device memory usage, and 69% communication payload reduction than synchronous FL (this http URL), but also consistently reaches comparable or higher target accuracy than semi-asynchronous (this http URL) in challenging regimes. These results establish FTTE as the first practical and scalable solution for real-world FL deployments on heterogeneous and predominantly resource-constrained edge devices.</li>
</ul>

<h3>Title: Topic Modeling as Long-Form Generation: Can Long-Context LLMs revolutionize NTM via Zero-Shot Prompting?</h3>
<ul>
<li><strong>Authors: </strong>Xuan Xu, Haolun Li, Zhongliang Yang, Beilin Chu, Jia Song, Moxuan Xu, Linna Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03174">https://arxiv.org/abs/2510.03174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03174">https://arxiv.org/pdf/2510.03174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03174]] Topic Modeling as Long-Form Generation: Can Long-Context LLMs revolutionize NTM via Zero-Shot Prompting?(https://arxiv.org/abs/2510.03174)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Traditional topic models such as neural topic models rely on inference and generation networks to learn latent topic distributions. This paper explores a new paradigm for topic modeling in the era of large language models, framing TM as a long-form generation task whose definition is updated in this paradigm. We propose a simple but practical approach to implement LLM-based topic model tasks out of the box (sample a data subset, generate topics and representative text with our prompt, text assignment with keyword match). We then investigate whether the long-form generation paradigm can beat NTMs via zero-shot prompting. We conduct a systematic comparison between NTMs and LLMs in terms of topic quality and empirically examine the claim that "a majority of NTMs are outdated."</li>
</ul>

<h3>Title: Dynamic Prompt Generation for Interactive 3D Medical Image Segmentation Training</h3>
<ul>
<li><strong>Authors: </strong>Tidiane Camaret Ndir, Alexander Pfefferle, Robin Tibor Schirrmeister</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03189">https://arxiv.org/abs/2510.03189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03189">https://arxiv.org/pdf/2510.03189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03189]] Dynamic Prompt Generation for Interactive 3D Medical Image Segmentation Training(https://arxiv.org/abs/2510.03189)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Interactive 3D biomedical image segmentation requires efficient models that can iteratively refine predictions based on user prompts. Current foundation models either lack volumetric awareness or suffer from limited interactive capabilities. We propose a training strategy that combines dynamic volumetric prompt generation with content-aware adaptive cropping to optimize the use of the image encoder. Our method simulates realistic user interaction patterns during training while addressing the computational challenges of learning from sequential refinement feedback on a single GPU. For efficient training, we initialize our network using the publicly available weights from the nnInteractive segmentation model. Evaluation on the \textbf{Foundation Models for Interactive 3D Biomedical Image Segmentation} competition demonstrates strong performance with an average final Dice score of 0.6385, normalized surface distance of 0.6614, and area-under-the-curve metrics of 2.4799 (Dice) and 2.5671 (NSD).</li>
</ul>

<h3>Title: Product-Quantised Image Representation for High-Quality Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Denis Zavadski, Nikita Philip Tatsch, Carsten Rother</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03191">https://arxiv.org/abs/2510.03191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03191">https://arxiv.org/pdf/2510.03191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03191]] Product-Quantised Image Representation for High-Quality Image Synthesis(https://arxiv.org/abs/2510.03191)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Product quantisation (PQ) is a classical method for scalable vector encoding, yet it has seen limited usage for latent representations in high-fidelity image generation. In this work, we introduce PQGAN, a quantised image autoencoder that integrates PQ into the well-known vector quantisation (VQ) framework of VQGAN. PQGAN achieves a noticeable improvement over state-of-the-art methods in terms of reconstruction performance, including both quantisation methods and their continuous counterparts. We achieve a PSNR score of 37dB, where prior work achieves 27dB, and are able to reduce the FID, LPIPS, and CMMD score by up to 96%. Our key to success is a thorough analysis of the interaction between codebook size, embedding dimensionality, and subspace factorisation, with vector and scalar quantisation as special cases. We obtain novel findings, such that the performance of VQ and PQ behaves in opposite ways when scaling the embedding dimension. Furthermore, our analysis shows performance trends for PQ that help guide optimal hyperparameter selection. Finally, we demonstrate that PQGAN can be seamlessly integrated into pre-trained diffusion models. This enables either a significantly faster and more compute-efficient generation, or a doubling of the output resolution at no additional cost, positioning PQ as a strong extension for discrete latent representation in image synthesis.</li>
</ul>

<h3>Title: Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft</h3>
<ul>
<li><strong>Authors: </strong>Junchao Huang, Xinting Hu, Boyao Han, Shaoshuai Shi, Zhuotao Tian, Tianyu He, Li Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03198">https://arxiv.org/abs/2510.03198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03198">https://arxiv.org/pdf/2510.03198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03198]] Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft(https://arxiv.org/abs/2510.03198)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Autoregressive video diffusion models have proved effective for world modeling and interactive scene generation, with Minecraft gameplay as a representative application. To faithfully simulate play, a model must generate natural content while exploring new scenes and preserve spatial consistency when revisiting explored areas. Under limited computation budgets, it must compress and exploit historical cues within a finite context window, which exposes a trade-off: Temporal-only memory lacks long-term spatial consistency, whereas adding spatial memory strengthens consistency but may degrade new scene generation quality when the model over-relies on insufficient spatial context. We present Memory Forcing, a learning framework that pairs training protocols with a geometry-indexed spatial memory. Hybrid Training exposes distinct gameplay regimes, guiding the model to rely on temporal memory during exploration and incorporate spatial memory for revisits. Chained Forward Training extends autoregressive training with model rollouts, where chained predictions create larger pose variations and encourage reliance on spatial memory for maintaining consistency. Point-to-Frame Retrieval efficiently retrieves history by mapping currently visible points to their source frames, while Incremental 3D Reconstruction maintains and updates an explicit 3D cache. Extensive experiments demonstrate that Memory Forcing achieves superior long-term spatial consistency and generative quality across diverse environments, while maintaining computational efficiency for extended sequences.</li>
</ul>

<h3>Title: MonSTeR: a Unified Model for Motion, Scene, Text Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Luca Collorone, Matteo Gioia, Massimiliano Pappa, Paolo Leoni, Giovanni Ficarra, Or Litany, Indro Spinelli, Fabio Galasso</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03200">https://arxiv.org/abs/2510.03200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03200">https://arxiv.org/pdf/2510.03200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03200]] MonSTeR: a Unified Model for Motion, Scene, Text Retrieval(https://arxiv.org/abs/2510.03200)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Intention drives human movement in complex environments, but such movement can only happen if the surrounding context supports it. Despite the intuitive nature of this mechanism, existing research has not yet provided tools to evaluate the alignment between skeletal movement (motion), intention (text), and the surrounding context (scene). In this work, we introduce MonSTeR, the first MOtioN-Scene-TExt Retrieval model. Inspired by the modeling of higher-order relations, MonSTeR constructs a unified latent space by leveraging unimodal and cross-modal representations. This allows MonSTeR to capture the intricate dependencies between modalities, enabling flexible but robust retrieval across various tasks. Our results show that MonSTeR outperforms trimodal models that rely solely on unimodal representations. Furthermore, we validate the alignment of our retrieval scores with human preferences through a dedicated user study. We demonstrate the versatility of MonSTeR's latent space on zero-shot in-Scene Object Placement and Motion Captioning. Code and pre-trained models are available at this http URL.</li>
</ul>

<h3>Title: FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents</h3>
<ul>
<li><strong>Authors: </strong>Imene Kerboua, Sahar Omidi Shayegan, Megh Thakkar, Xing Han Lù, Léo Boisvert, Massimo Caccia, Jérémy Espinas, Alexandre Aussem, Véronique Eglin, Alexandre Lacoste</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03204">https://arxiv.org/abs/2510.03204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03204">https://arxiv.org/pdf/2510.03204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03204]] FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents(https://arxiv.org/abs/2510.03204)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Web agents powered by large language models (LLMs) must process lengthy web page observations to complete user goals; these pages often exceed tens of thousands of tokens. This saturates context limits and increases computational cost processing; moreover, processing full pages exposes agents to security risks such as prompt injection. Existing pruning strategies either discard relevant content or retain irrelevant context, leading to suboptimal action prediction. We introduce FocusAgent, a simple yet effective approach that leverages a lightweight LLM retriever to extract the most relevant lines from accessibility tree (AxTree) observations, guided by task goals. By pruning noisy and irrelevant content, FocusAgent enables efficient reasoning while reducing vulnerability to injection attacks. Experiments on WorkArena and WebArena benchmarks show that FocusAgent matches the performance of strong baselines, while reducing observation size by over 50%. Furthermore, a variant of FocusAgent significantly reduces the success rate of prompt-injection attacks, including banner and pop-up attacks, while maintaining task success performance in attack-free settings. Our results highlight that targeted LLM-based retrieval is a practical and robust strategy for building web agents that are efficient, effective, and secure.</li>
</ul>

<h3>Title: Cache-to-Cache: Direct Semantic Communication Between Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Fu, Zihan Min, Hanling Zhang, Jichao Yan, Guohao Dai, Wanli Ouyang, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03215">https://arxiv.org/abs/2510.03215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03215">https://arxiv.org/pdf/2510.03215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03215]] Cache-to-Cache: Direct Semantic Communication Between Large Language Models(https://arxiv.org/abs/2510.03215)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-LLM systems harness the complementary strengths of diverse Large Language Models, achieving performance and efficiency gains unattainable by a single model. In existing designs, LLMs communicate through text, forcing internal representations to be transformed into output token sequences. This process both loses rich semantic information and incurs token-by-token generation latency. Motivated by these limitations, we ask: Can LLMs communicate beyond text? Oracle experiments show that enriching the KV-Cache semantics can improve response quality without increasing cache size, supporting KV-Cache as an effective medium for inter-model communication. Thus, we propose Cache-to-Cache (C2C), a new paradigm for direct semantic communication between LLMs. C2C uses a neural network to project and fuse the source model's KV-cache with that of the target model to enable direct semantic transfer. A learnable gating mechanism selects the target layers that benefit from cache communication. Compared with text communication, C2C utilizes the deep, specialized semantics from both models, while avoiding explicit intermediate text generation. Experiments show that C2C achieves 8.5-10.5% higher average accuracy than individual models. It further outperforms the text communication paradigm by approximately 3.0-5.0%, while delivering an average 2.0x speedup in latency. Our code is available at this https URL.</li>
</ul>

<h3>Title: TPM-Based Continuous Remote Attestation and Integrity Verification for 5G VNFs on Kubernetes</h3>
<ul>
<li><strong>Authors: </strong>Al Nahian Bin Emran, Rajendra Upadhyay, Rajendra Paudyal, Lisa Donnan, Duminda Wijesekera</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03219">https://arxiv.org/abs/2510.03219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03219">https://arxiv.org/pdf/2510.03219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03219]] TPM-Based Continuous Remote Attestation and Integrity Verification for 5G VNFs on Kubernetes(https://arxiv.org/abs/2510.03219)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving landscape of 5G technology, the adoption of cloud-based infrastructure for the deployment of 5G services has become increasingly common. Using a service-based architecture, critical 5G components, such as the Access and Mobility Management Function (AMF), Session Management Function (SMF), and User Plane Function (UPF), now run as containerized pods on Kubernetes clusters. Although this approach improves scalability, flexibility, and resilience, it also introduces new security challenges, particularly to ensure the integrity and trustworthiness of these components. Current 5G security specifications (for example, 3GPP TS 33.501) focus on communication security and assume that network functions remain trustworthy after authentication, consequently lacking mechanisms to continuously validate the integrity of NVFs at runtime. To close this gap, and to align with Zero Trust principles of 'never trust, always verify', we present a TPM 2.0-based continuous remote attestation solution for core 5G components deployed on Kubernetes. Our approach uses the Linux Integrity Measurement Architecture (IMA) and a Trusted Platform Module (TPM) to provide hardware-based runtime validation. We integrate the open-source Keylime framework with a custom IMA template that isolates pod-level measurements, allowing per-pod integrity verification. A prototype on a k3s cluster (consisting of 1 master, 2 worker nodes) was implemented to attest to core functions, including AMF, SMF and UPF. The experimental results show that the system detects unauthorized modifications in real time, labels each pod's trust state, and generates detailed audit logs. This work provides hardware-based continuous attestation for cloud native and edge deployments, strengthening the resilience of 5G as critical infrastructure in multi-vendor and mission-critical scenarios of 5G.</li>
</ul>

<h3>Title: Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward</h3>
<ul>
<li><strong>Authors: </strong>Guanhua Huang, Tingqiang Xu, Mingze Wang, Qi Yi, Xue Gong, Siheng Li, Ruibin Xiong, Kejiao Li, Yuhao Jiang, Bo Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03222">https://arxiv.org/abs/2510.03222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03222">https://arxiv.org/pdf/2510.03222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03222]] Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward(https://arxiv.org/abs/2510.03222)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large Language Models in complex reasoning, yet its scalability is often hindered by a training bottleneck where performance plateaus as policy entropy collapses, signaling a loss of exploration. Previous methods typically address this by maintaining high policy entropy, yet the precise mechanisms that govern meaningful exploration have remained underexplored. Our analysis suggests that an unselective focus on entropy risks amplifying irrelevant tokens and destabilizing training. This paper investigates the exploration dynamics within RLVR and identifies a key issue: the gradual elimination of valuable low-probability exploratory tokens, which we term \textbf{\textit{reasoning sparks}}. We find that while abundant in pre-trained models, these sparks are systematically extinguished during RLVR due to over-penalization, leading to a degeneracy in exploration. To address this, we introduce Low-probability Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a heuristic proxy distribution. This proxy is constructed by filtering out presumed noise tokens and re-normalizing the distribution over the remaining candidates. The result is a less-noisy proxy where the probability of \textit{reasoning sparks} is amplified, which then serves as a soft regularization target to shield these valuable tokens from elimination via KL divergence. Experiments show that Lp-Reg enables stable on-policy training for around 1,000 steps, a regime where baseline entropy-control methods collapse. This sustained exploration leads to state-of-the-art performance, achieving a $60.17\%$ average accuracy on five math benchmarks, an improvement of $2.66\%$ over prior methods. Code is available at this https URL.</li>
</ul>

<h3>Title: Self-Anchor: Large Language Model Reasoning via Step-by-step Attention Alignment</h3>
<ul>
<li><strong>Authors: </strong>Hongxiang Zhang, Yuan Tian, Tianyi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03223">https://arxiv.org/abs/2510.03223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03223">https://arxiv.org/pdf/2510.03223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03223]] Self-Anchor: Large Language Model Reasoning via Step-by-step Attention Alignment(https://arxiv.org/abs/2510.03223)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To solve complex reasoning tasks for Large Language Models (LLMs), prompting-based methods offer a lightweight alternative to fine-tuning and reinforcement learning. However, as reasoning chains extend, critical intermediate steps and the original prompt will be buried in the context, receiving insufficient attention and leading to errors. In this paper, we propose Self-Anchor, a novel pipeline that leverages the inherent structure of reasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories into structured plans and automatically aligns the model's attention to the most relevant inference steps, allowing the model to maintain focus throughout generation. Our experiment shows that Self-Anchor outperforms SOTA prompting methods across six benchmarks. Notably, Self-Anchor significantly reduces the performance gap between ``non-reasoning'' models and specialized reasoning models, with the potential to enable most LLMs to tackle complex reasoning tasks without retraining.</li>
</ul>

<h3>Title: Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Dong Lao, Yuxiang Zhang, Haniyeh Ehsani Oskouie, Yangchao Wu, Alex Wong, Stefano Soatto</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03224">https://arxiv.org/abs/2510.03224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03224">https://arxiv.org/pdf/2510.03224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03224]] Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles(https://arxiv.org/abs/2510.03224)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>We propose a test-time defense mechanism against adversarial attacks: imperceptible image perturbations that significantly alter the predictions of a model. Unlike existing methods that rely on feature filtering or smoothing, which can lead to information loss, we propose to "combat noise with noise" by leveraging stochastic resonance to enhance robustness while minimizing information loss. Our approach introduces small translational perturbations to the input image, aligns the transformed feature embeddings, and aggregates them before mapping back to the original reference image. This can be expressed in a closed-form formula, which can be deployed on diverse existing network architectures without introducing additional network modules or fine-tuning for specific attack types. The resulting method is entirely training-free, architecture-agnostic, and attack-agnostic. Empirical results show state-of-the-art robustness on image classification and, for the first time, establish a generic test-time defense for dense prediction tasks, including stereo matching and optical flow, highlighting the method's versatility and practicality. Specifically, relative to clean (unperturbed) performance, our method recovers up to 68.1% of the accuracy loss on image classification, 71.9% on stereo matching, and 29.2% on optical flow under various types of adversarial attacks.</li>
</ul>

<h3>Title: Reward Models are Metrics in a Trench Coat</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Gehrmann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03231">https://arxiv.org/abs/2510.03231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03231">https://arxiv.org/pdf/2510.03231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03231]] Reward Models are Metrics in a Trench Coat(https://arxiv.org/abs/2510.03231)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The emergence of reinforcement learning in post-training of large language models has sparked significant interest in reward models. Reward models assess the quality of sampled model outputs to generate training signals. This task is also performed by evaluation metrics that monitor the performance of an AI model. We find that the two research areas are mostly separate, leading to redundant terminology and repeated pitfalls. Common challenges include susceptibility to spurious correlations, impact on downstream reward hacking, methods to improve data quality, and approaches to meta-evaluation. Our position paper argues that a closer collaboration between the fields can help overcome these issues. To that end, we show how metrics outperform reward models on specific tasks and provide an extensive survey of the two areas. Grounded in this survey, we point to multiple research topics in which closer alignment can improve reward models and metrics in areas such as preference elicitation methods, avoidance of spurious correlations and reward hacking, and calibration-aware meta-evaluation.</li>
</ul>

<h3>Title: LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ci-Siang Lin, Min-Hung Chen, Yu-Yang Sheng, Yu-Chiang Frank Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03232">https://arxiv.org/abs/2510.03232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03232">https://arxiv.org/pdf/2510.03232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03232]] LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models(https://arxiv.org/abs/2510.03232)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have achieved strong performance on general visual benchmarks but struggle with out-of-distribution (OOD) tasks in specialized domains such as medical imaging, where labeled data is limited and expensive. We introduce LEAML, a label-efficient adaptation framework that leverages both scarce labeled VQA samples and abundant unlabeled images. Our approach generates domain-relevant pseudo question-answer pairs for unlabeled data using a QA generator regularized by caption distillation. Importantly, we selectively update only those neurons most relevant to question-answering, enabling the QA Generator to efficiently acquire domain-specific knowledge during distillation. Experiments on gastrointestinal endoscopy and sports VQA demonstrate that LEAML consistently outperforms standard fine-tuning under minimal supervision, highlighting the effectiveness of our proposed LEAML framework.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
