<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Revisiting Fully Homomorphic Encryption Schemes. (arXiv:2305.05904v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05904">http://arxiv.org/abs/2305.05904</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05904] Revisiting Fully Homomorphic Encryption Schemes](http://arxiv.org/abs/2305.05904) #secure</code></li>
<li>Summary: <p>Homomorphic encryption is a sophisticated encryption technique that allows
computations on encrypted data to be done without the requirement for
decryption. This trait makes homomorphic encryption appropriate for safe
computation in sensitive data scenarios, such as cloud computing, medical data
exchange, and financial transactions. The data is encrypted using a public key
in homomorphic encryption, and the calculation is conducted on the encrypted
data using an algorithm that retains the encryption. The computed result is
then decrypted with a private key to acquire the final output. This abstract
notion protects data while allowing complicated computations to be done on the
encrypted data, resulting in a secure and efficient approach to analysing
sensitive information. This article is intended to give a clear idea about the
various fully Homomorphic Encryption Schemes present in the literature and
analyse and compare the results of each of these schemes. Further, we also
provide applications and open-source tools of homomorphic encryption schemes.
</p></li>
</ul>

<h3>Title: FedSOV: Federated Model Secure Ownership Verification with Unforgeable Signature. (arXiv:2305.06085v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06085">http://arxiv.org/abs/2305.06085</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06085] FedSOV: Federated Model Secure Ownership Verification with Unforgeable Signature](http://arxiv.org/abs/2305.06085) #secure</code></li>
<li>Summary: <p>Federated learning allows multiple parties to collaborate in learning a
global model without revealing private data. The high cost of training and the
significant value of the global model necessitates the need for ownership
verification of federated learning. However, the existing ownership
verification schemes in federated learning suffer from several limitations,
such as inadequate support for a large number of clients and vulnerability to
ambiguity attacks. To address these limitations, we propose a cryptographic
signature-based federated learning model ownership verification scheme named
FedSOV. FedSOV allows numerous clients to embed their ownership credentials and
verify ownership using unforgeable digital signatures. The scheme provides
theoretical resistance to ambiguity attacks with the unforgeability of the
signature. Experimental results on computer vision and natural language
processing tasks demonstrate that FedSOV is an effective federated model
ownership verification scheme enhanced with provable cryptographic security.
</p></li>
</ul>

<h3>Title: SafeLLVM: LLVM Without The ROP Gadgets!. (arXiv:2305.06092v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06092">http://arxiv.org/abs/2305.06092</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06092] SafeLLVM: LLVM Without The ROP Gadgets!](http://arxiv.org/abs/2305.06092) #secure</code></li>
<li>Summary: <p>Memory safety is a cornerstone of secure and robust software systems, as it
prevents a wide range of vulnerabilities and exploitation techniques. Among
these, we focus on Return-Oriented Programming (ROP). ROP works as such: the
attacker takes control of the program's execution flow via a memory corruption
attack, then takes advantages of code snippets already in the program's memory,
dubbed "gadgets," to achieve the attacker's desired effect.
</p></li>
</ul>

<p>In this paper, we introduce SafeLLVM, an approach to minimize the number of
gadgets in x86-64 binaries compiled with the LLVM infrastructure. Building upon
the techniques outlined in previous works, we implement a series of passes
within the LLVM compiler's backend to minimize the number of gadgets present
and thus prevent ROP attacks. We evaluated our approach by compiling a number
of real-world applications, including cJSON, zlib, curl, and mimalloc. The
results show our solution is able to prevent any form of ROP on the binaries
compiled with SafeLLVM while maintaining the same functionality as the original
binaries.
</p>

<h3>Title: Patchwork Learning: A Paradigm Towards Integrative Analysis across Diverse Biomedical Data Sources. (arXiv:2305.06217v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06217">http://arxiv.org/abs/2305.06217</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06217] Patchwork Learning: A Paradigm Towards Integrative Analysis across Diverse Biomedical Data Sources](http://arxiv.org/abs/2305.06217) #secure</code></li>
<li>Summary: <p>Machine learning (ML) in healthcare presents numerous opportunities for
enhancing patient care, population health, and healthcare providers' workflows.
However, the real-world clinical and cost benefits remain limited due to
challenges in data privacy, heterogeneous data sources, and the inability to
fully leverage multiple data modalities. In this perspective paper, we
introduce "patchwork learning" (PL), a novel paradigm that addresses these
limitations by integrating information from disparate datasets composed of
different data modalities (e.g., clinical free-text, medical images, omics) and
distributed across separate and secure sites. PL allows the simultaneous
utilization of complementary data sources while preserving data privacy,
enabling the development of more holistic and generalizable ML models. We
present the concept of patchwork learning and its current implementations in
healthcare, exploring the potential opportunities and applicable data sources
for addressing various healthcare challenges. PL leverages bridging modalities
or overlapping feature spaces across sites to facilitate information sharing
and impute missing data, thereby addressing related prediction tasks. We
discuss the challenges associated with PL, many of which are shared by
federated and multimodal learning, and provide recommendations for future
research in this field. By offering a more comprehensive approach to healthcare
data integration, patchwork learning has the potential to revolutionize the
clinical applicability of ML models. This paradigm promises to strike a balance
between personalization and generalizability, ultimately enhancing patient
experiences, improving population health, and optimizing healthcare providers'
workflows.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Analysis of Adversarial Image Manipulations. (arXiv:2305.06307v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06307">http://arxiv.org/abs/2305.06307</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06307] Analysis of Adversarial Image Manipulations](http://arxiv.org/abs/2305.06307) #security</code></li>
<li>Summary: <p>As virtual and physical identity grow increasingly intertwined, the
importance of privacy and security in the online sphere becomes paramount. In
recent years, multiple news stories have emerged of private companies scraping
web content and doing research with or selling the data. Images uploaded online
can be scraped without users' consent or knowledge. Users of social media
platforms whose images are scraped may be at risk of being identified in other
uploaded images or in real-world identification situations. This paper
investigates how simple, accessible image manipulation techniques affect the
accuracy of facial recognition software in identifying an individual's various
face images based on one unique image.
</p></li>
</ul>

<h3>Title: A semi-automatic method for document classification in the shipping industry. (arXiv:2305.06148v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06148">http://arxiv.org/abs/2305.06148</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06148] A semi-automatic method for document classification in the shipping industry](http://arxiv.org/abs/2305.06148) #security</code></li>
<li>Summary: <p>In the shipping industry, document classification plays a crucial role in
ensuring that the necessary documents are properly identified and processed for
customs clearance. OCR technology is being used to automate the process of
document classification, which involves identifying important documents such as
Commercial Invoices, Packing Lists, Export/Import Customs Declarations, Bills
of Lading, Sea Waybills, Certificates, Air or Rail Waybills, Arrival Notices,
Certificate of Origin, Importer Security Filings, and Letters of Credit. By
using OCR technology, the shipping industry can improve accuracy and efficiency
in document classification and streamline the customs clearance process. The
aim of this study is to build a robust document classification system based on
keyword frequencies. The research is carried out by analyzing Contract-Breach
law documents available with IN-D. The documents were collected by scraping the
Singapore Government Judiciary website. The database developed has 250
Contract-Breach documents. These documents are splitted to generate 200
training documents and 50 test documents. A semi-automatic approach is used to
select keyword vectors for document classification. The accuracy of the
reported model is 92.00 %.
</p></li>
</ul>

<h3>Title: Unraveling the MEV Enigma: ABI-Free Detection Model using Graph Neural Networks. (arXiv:2305.05952v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05952">http://arxiv.org/abs/2305.05952</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05952] Unraveling the MEV Enigma: ABI-Free Detection Model using Graph Neural Networks](http://arxiv.org/abs/2305.05952) #security</code></li>
<li>Summary: <p>The detection of Maximal Extractable Value (MEV) in blockchain is crucial for
enhancing blockchain security, as it enables the evaluation of potential
consensus layer risks, the effectiveness of anti-centralization solutions, and
the assessment of user exploitation. However, existing MEV detection methods
face limitations due to their low recall rate, reliance on pre-registered
Application Binary Interfaces (ABIs) and the need for continuous monitoring of
new DeFi services.
</p></li>
</ul>

<p>In this paper, we propose ArbiNet, a novel GNN-based detection model that
offers a low-overhead and accurate solution for MEV detection without requiring
knowledge of smart contract code or ABIs. We collected an extensive MEV
dataset, surpassing currently available public datasets, to train ArbiNet. Our
implemented model and open dataset enhance the understanding of the MEV
landscape, serving as a foundation for MEV quantification and improved
blockchain security.
</p>

<h3>Title: Conflict Analysis and Resolution of Safety and Security Boundary Conditions for Industrial Control Systems. (arXiv:2305.06185v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06185">http://arxiv.org/abs/2305.06185</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06185] Conflict Analysis and Resolution of Safety and Security Boundary Conditions for Industrial Control Systems](http://arxiv.org/abs/2305.06185) #security</code></li>
<li>Summary: <p>Safety and security are the two most important properties of industrial
control systems (ICS), and their integration is necessary to ensure that safety
goals do not undermine security goals and vice versa. Sometimes, safety and
security co-engineering leads to conflicting requirements or violations capable
of impacting the normal behavior of the system. Identification, analysis, and
resolution of conflicts arising from safety and security co-engineering is a
major challenge, an under-researched area in safety-critical systems(ICS). This
paper presents an STPA-SafeSec-CDCL approach that addresses the challenge. Our
proposed methodology combines the STPA-SafeSec approach for safety and security
analysis and the Conflict-Driven Clause Learning (CDCL) approach for the
identification, analysis, and resolution of conflicts where conflicting
constraints are encoded in satisfiability (SAT) problems. We apply our
framework to the Tennessee Eastman Plant process model, a chemical process
model developed specifically for the study of industrial control processes, to
demonstrate how to use the proposed method. Our methodology goes beyond the
requirement analysis phase and can be applied to the early stages of system
design and development to increase system reliability, robustness, and
resilience.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: DPMLBench: Holistic Evaluation of Differentially Private Machine Learning. (arXiv:2305.05900v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05900">http://arxiv.org/abs/2305.05900</a></li>
<li>Code URL: <a href="https://github.com/dmskinson/dpmlbench">https://github.com/dmskinson/dpmlbench</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05900] DPMLBench: Holistic Evaluation of Differentially Private Machine Learning](http://arxiv.org/abs/2305.05900) #privacy</code></li>
<li>Summary: <p>Differential privacy (DP), as a rigorous mathematical definition quantifying
privacy leakage, has become a well-accepted standard for privacy protection.
Combined with powerful machine learning techniques, differentially private
machine learning (DPML) is increasingly important. As the most classic DPML
algorithm, DP-SGD incurs a significant loss of utility, which hinders DPML's
deployment in practice. Many studies have recently proposed improved algorithms
based on DP-SGD to mitigate utility loss. However, these studies are isolated
and cannot comprehensively measure the performance of improvements proposed in
algorithms. More importantly, there is a lack of comprehensive research to
compare improvements in these DPML algorithms across utility, defensive
capabilities, and generalizability.
</p></li>
</ul>

<p>We fill this gap by performing a holistic measurement of improved DPML
algorithms on utility and defense capability against membership inference
attacks (MIAs) on image classification tasks. We first present a taxonomy of
where improvements are located in the machine learning life cycle. Based on our
taxonomy, we jointly perform an extensive measurement study of the improved
DPML algorithms. We also cover state-of-the-art label differential privacy
(Label DP) algorithms in the evaluation. According to our empirical results, DP
can effectively defend against MIAs, and sensitivity-bounding techniques such
as per-sample gradient clipping play an important role in defense. We also
explore some improvements that can maintain model utility and defend against
MIAs more effectively. Experiments show that Label DP algorithms achieve less
utility loss but are fragile to MIAs. To support our evaluation, we implement a
modular re-usable software, DPMLBench, which enables sensitive data owners to
deploy DPML algorithms and serves as a benchmark tool for researchers and
practitioners.
</p>

<h3>Title: Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models. (arXiv:2305.05973v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05973">http://arxiv.org/abs/2305.05973</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05973] Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models](http://arxiv.org/abs/2305.05973) #privacy</code></li>
<li>Summary: <p>We propose a novel approach for developing privacy-preserving large-scale
recommender systems using differentially private (DP) large language models
(LLMs) which overcomes certain challenges and limitations in DP training these
complex systems. Our method is particularly well suited for the emerging area
of LLM-based recommender systems, but can be readily employed for any
recommender systems that process representations of natural language inputs.
Our approach involves using DP training methods to fine-tune a publicly
pre-trained LLM on a query generation task. The resulting model can generate
private synthetic queries representative of the original queries which can be
freely shared for any downstream non-private recommendation training procedures
without incurring any additional privacy cost. We evaluate our method on its
ability to securely train effective deep retrieval models, and we observe
significant improvements in their retrieval quality without compromising
query-level privacy guarantees compared to methods where the retrieval models
are directly DP trained.
</p></li>
</ul>

<h3>Title: Privacy-Preserving Prompt Tuning for Large Language Model Services. (arXiv:2305.06212v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06212">http://arxiv.org/abs/2305.06212</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06212] Privacy-Preserving Prompt Tuning for Large Language Model Services](http://arxiv.org/abs/2305.06212) #privacy</code></li>
<li>Summary: <p>Prompt tuning provides an efficient way for users to customize Large Language
Models (LLMs) with their private data in the emerging LLM service scenario.
However, the sensitive nature of private data brings the need for privacy
preservation in LLM service customization. Based on prompt tuning, we propose
Privacy-Preserving Prompt Tuning (RAPT), a framework that provides privacy
guarantees for LLM services. \textsc{rapt} adopts a local privacy setting,
allowing users to privatize their data locally with local differential privacy.
As prompt tuning performs poorly when directly trained on privatized data, we
introduce a novel privatized token reconstruction task that is trained jointly
with the downstream task, allowing LLMs to learn better task-dependent
representations. Despite the simplicity of our framework, experiments show that
RAPT achieves competitive performance across tasks while providing privacy
guarantees against adversaries.
</p></li>
</ul>

<h3>Title: DOCTOR: A Multi-Disease Detection Continual Learning Framework Based on Wearable Medical Sensors. (arXiv:2305.05738v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05738">http://arxiv.org/abs/2305.05738</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05738] DOCTOR: A Multi-Disease Detection Continual Learning Framework Based on Wearable Medical Sensors](http://arxiv.org/abs/2305.05738) #privacy</code></li>
<li>Summary: <p>Modern advances in machine learning (ML) and wearable medical sensors (WMSs)
in edge devices have enabled ML-driven disease detection for smart healthcare.
Conventional ML-driven disease detection methods rely on customizing individual
models for each disease and its corresponding WMS data. However, such methods
lack adaptability to distribution shifts and new task classification classes.
Also, they need to be rearchitected and retrained from scratch for each new
disease. Moreover, installing multiple ML models in an edge device consumes
excessive memory, drains the battery faster, and complicates the detection
process. To address these challenges, we propose DOCTOR, a multi-disease
detection continual learning (CL) framework based on WMSs. It employs a
multi-headed deep neural network (DNN) and an exemplar-replay-style CL
algorithm. The CL algorithm enables the framework to continually learn new
missions where different data distributions, classification classes, and
disease detection tasks are introduced sequentially. It counteracts
catastrophic forgetting with a data preservation method and a synthetic data
generation module. The data preservation method efficiently preserves the most
informative subset of training data from previous missions based on the average
training loss of each data instance. The synthetic data generation module
models the probability distribution of the real training data and then
generates as much synthetic data as needed for replays while maintaining data
privacy. The multi-headed DNN enables DOCTOR to detect multiple diseases
simultaneously based on user WMS data. We demonstrate DOCTOR's efficacy in
maintaining high multi-disease classification accuracy with a single DNN model
in various CL experiments. DOCTOR achieves very competitive performance across
all CL scenarios relative to the ideal joint-training framework while
maintaining a small model size.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: ChatGPT as a Text Simplification Tool to Remove Bias. (arXiv:2305.06166v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06166">http://arxiv.org/abs/2305.06166</a></li>
<li>Code URL: <a href="https://github.com/charmainebarker/chatgpt-as-a-text-simplification-tool-to-remove-bias">https://github.com/charmainebarker/chatgpt-as-a-text-simplification-tool-to-remove-bias</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06166] ChatGPT as a Text Simplification Tool to Remove Bias](http://arxiv.org/abs/2305.06166) #protect</code></li>
<li>Summary: <p>The presence of specific linguistic signals particular to a certain sub-group
of people can be picked up by language models during training. This may lead to
discrimination if the model has learnt to pick up on a certain group's
language. If the model begins to associate specific language with a distinct
group, any decisions made based upon this language would hold a strong
correlation to a decision based on their protected characteristic. We explore a
possible technique for bias mitigation in the form of simplification of text.
The driving force of this idea is that simplifying text should standardise
language to one way of speaking while keeping the same meaning. The experiment
shows promising results as the classifier accuracy for predicting the sensitive
attribute drops by up to 17% for the simplified data.
</p></li>
</ul>

<h3>Title: Spectrum Breathing: Protecting Over-the-Air Federated Learning Against Interference. (arXiv:2305.05933v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05933">http://arxiv.org/abs/2305.05933</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05933] Spectrum Breathing: Protecting Over-the-Air Federated Learning Against Interference](http://arxiv.org/abs/2305.05933) #protect</code></li>
<li>Summary: <p>Federated Learning (FL) is a widely embraced paradigm for distilling
artificial intelligence from distributed mobile data. However, the deployment
of FL in mobile networks can be compromised by exposure to interference from
neighboring cells or jammers. Existing interference mitigation techniques
require multi-cell cooperation or at least interference channel state
information, which is expensive in practice. On the other hand, power control
that treats interference as noise may not be effective due to limited power
budgets, and also that this mechanism can trigger countermeasures by
interference sources. As a practical approach for protecting FL against
interference, we propose Spectrum Breathing, which cascades stochastic-gradient
pruning and spread spectrum to suppress interference without bandwidth
expansion. The cost is higher learning latency by exploiting the graceful
degradation of learning speed due to pruning. We synchronize the two operations
such that their levels are controlled by the same parameter, Breathing Depth.
To optimally control the parameter, we develop a martingale-based approach to
convergence analysis of Over-the-Air FL with spectrum breathing, termed
AirBreathing FL. We show a performance tradeoff between gradient-pruning and
interference-induced error as regulated by the breathing depth. Given receive
SIR and model size, the optimization of the tradeoff yields two schemes for
controlling the breathing depth that can be either fixed or adaptive to
channels and the learning process. As shown by experiments, in scenarios where
traditional Over-the-Air FL fails to converge in the presence of strong
interference, AirBreahing FL with either fixed or adaptive breathing depth can
ensure convergence where the adaptive scheme achieves close-to-ideal
performance.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Similarity-Based Logic Locking Against Machine Learning Attacks. (arXiv:2305.05870v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05870">http://arxiv.org/abs/2305.05870</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05870] Similarity-Based Logic Locking Against Machine Learning Attacks](http://arxiv.org/abs/2305.05870) #attack</code></li>
<li>Summary: <p>Logic locking is a promising technique for protecting integrated circuit
designs while outsourcing their fabrication. Recently, graph neural network
(GNN)-based link prediction attacks have been developed which can successfully
break all the multiplexer-based locking techniques that were expected to be
learning-resilient. We present SimLL, a novel similarity-based locking
technique which locks a design using multiplexers and shows robustness against
the existing structure-exploiting oracle-less learning-based attacks. Aiming to
confuse the machine learning (ML) models, SimLL introduces key-controlled
multiplexers between logic gates or wires that exhibit high levels of
topological and functional similarity. Empirical results show that SimLL can
degrade the accuracy of existing ML-based attacks to approximately 50%,
resulting in a negligible advantage over random guessing.
</p></li>
</ul>

<h3>Title: Quantization Aware Attack: Enhancing the Transferability of Adversarial Attacks across Target Models with Different Quantization Bitwidths. (arXiv:2305.05875v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05875">http://arxiv.org/abs/2305.05875</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05875] Quantization Aware Attack: Enhancing the Transferability of Adversarial Attacks across Target Models with Different Quantization Bitwidths](http://arxiv.org/abs/2305.05875) #attack</code></li>
<li>Summary: <p>Quantized Neural Networks (QNNs) receive increasing attention in
resource-constrained scenarios because of their excellent generalization
abilities, but their robustness under realistic black-box adversarial attacks
has not been deeply studied, in which the adversary requires to improve the
attack capability across target models with unknown quantization bitwidths. One
major challenge is that adversarial examples transfer poorly against QNNs with
unknown bitwidths because of the quantization shift and gradient misalignment
issues. This paper proposes the Quantization Aware Attack to enhance the attack
transferability by making the substitute model ``aware of'' the target of
attacking models with multiple bitwidths. Specifically, we design a training
objective with multiple bitwidths to align the gradient of the substitute model
with the target model with different bitwidths and thus mitigate the negative
effect of the above two issues. We conduct comprehensive evaluations by
performing multiple transfer-based attacks on standard models and defense
models with different architectures and quantization bitwidths. Experimental
results show that QAA significantly improves the adversarial transferability of
the state-of-the-art attacks by 3.4%-20.9% against normally trained models and
3.7%-13.4% against adversarially trained models on average.
</p></li>
</ul>

<h3>Title: RNNS: Representation Nearest Neighbor Search Black-Box Attack on Code Models. (arXiv:2305.05896v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05896">http://arxiv.org/abs/2305.05896</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05896] RNNS: Representation Nearest Neighbor Search Black-Box Attack on Code Models](http://arxiv.org/abs/2305.05896) #attack</code></li>
<li>Summary: <p>Pre-trained code models are mainly evaluated using the in-distribution test
data. The robustness of models, i.e., the ability to handle hard unseen data,
still lacks evaluation. In this paper, we propose a novel search-based
black-box adversarial attack guided by model behaviours for pre-trained
programming language models, named Representation Nearest Neighbor
Search(RNNS), to evaluate the robustness of Pre-trained PL models. Unlike other
black-box adversarial attacks, RNNS uses the model-change signal to guide the
search in the space of the variable names collected from real-world projects.
Specifically, RNNS contains two main steps, 1) indicate which variable (attack
position location) we should attack based on model uncertainty, and 2) search
which adversarial tokens we should use for variable renaming according to the
model behaviour observations. We evaluate RNNS on 6 code tasks (e.g., clone
detection), 3 programming languages (Java, Python, and C), and 3 pre-trained
code models: CodeBERT, GraphCodeBERT, and CodeT5. The results demonstrate that
RNNS outperforms the state-of-the-art black-box attacking methods (MHM and
ALERT) in terms of attack success rate (ASR) and query times (QT). The
perturbation of generated adversarial examples from RNNS is smaller than the
baselines with respect to the number of replaced variables and the variable
length change. Our experiments also show that RNNS is efficient in attacking
the defended models and is useful for adversarial training.
</p></li>
</ul>

<h3>Title: XMI-ICU: Explainable Machine Learning Model for Pseudo-Dynamic Prediction of Mortality in the ICU for Heart Attack Patients. (arXiv:2305.06109v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06109">http://arxiv.org/abs/2305.06109</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06109] XMI-ICU: Explainable Machine Learning Model for Pseudo-Dynamic Prediction of Mortality in the ICU for Heart Attack Patients](http://arxiv.org/abs/2305.06109) #attack</code></li>
<li>Summary: <p>Heart attack remain one of the greatest contributors to mortality in the
United States and globally. Patients admitted to the intensive care unit (ICU)
with diagnosed heart attack (myocardial infarction or MI) are at higher risk of
death. In this study, we use two retrospective cohorts extracted from the eICU
and MIMIC-IV databases, to develop a novel pseudo-dynamic machine learning
framework for mortality prediction in the ICU with interpretability and
clinical risk analysis. The method provides accurate prediction for ICU
patients up to 24 hours before the event and provide time-resolved
interpretability results. The performance of the framework relying on extreme
gradient boosting was evaluated on a held-out test set from eICU, and
externally validated on the MIMIC-IV cohort using the most important features
identified by time-resolved Shapley values achieving AUCs of 91.0 (balanced
accuracy of 82.3) for 6-hour prediction of mortality respectively. We show that
our framework successfully leverages time-series physiological measurements by
translating them into stacked static prediction problems to be robustly
predictive through time in the ICU stay and can offer clinical insight from
time-resolved interpretability
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Visual Place Recognition with Low-Resolution Images. (arXiv:2305.05776v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05776">http://arxiv.org/abs/2305.05776</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05776] Visual Place Recognition with Low-Resolution Images](http://arxiv.org/abs/2305.05776) #robust</code></li>
<li>Summary: <p>Images incorporate a wealth of information from a robot's surroundings. With
the widespread availability of compact cameras, visual information has become
increasingly popular for addressing the localisation problem, which is then
termed as Visual Place Recognition (VPR). While many applications use
high-resolution cameras and high-end systems to achieve optimal place-matching
performance, low-end commercial systems face limitations due to resource
constraints and relatively low-resolution and low-quality cameras. In this
paper, we analyse the effects of image resolution on the accuracy and
robustness of well-established handcrafted VPR pipelines. Handcrafted designs
have low computational demands and can adapt to flexible image resolutions,
making them a suitable approach to scale to any image source and to operate
under resource limitations. This paper aims to help academic researchers and
companies in the hardware and software industry co-design VPR solutions and
expand the use of VPR algorithms in commercial products.
</p></li>
</ul>

<h3>Title: Even Small Correlation and Diversity Shifts Pose Dataset-Bias Issues. (arXiv:2305.05807v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05807">http://arxiv.org/abs/2305.05807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05807] Even Small Correlation and Diversity Shifts Pose Dataset-Bias Issues](http://arxiv.org/abs/2305.05807) #robust</code></li>
<li>Summary: <p>Distribution shifts are common in real-world datasets and can affect the
performance and reliability of deep learning models. In this paper, we study
two types of distribution shifts: diversity shifts, which occur when test
samples exhibit patterns unseen during training, and correlation shifts, which
occur when test data present a different correlation between seen invariant and
spurious features. We propose an integrated protocol to analyze both types of
shifts using datasets where they co-exist in a controllable manner. Finally, we
apply our approach to a real-world classification problem of skin cancer
analysis, using out-of-distribution datasets and specialized bias annotations.
Our protocol reveals three findings: 1) Models learn and propagate correlation
shifts even with low-bias training; this poses a risk of accumulating and
combining unaccountable weak biases; 2) Models learn robust features in high-
and low-bias scenarios but use spurious ones if test samples have them; this
suggests that spurious correlations do not impair the learning of robust
features; 3) Diversity shift can reduce the reliance on spurious correlations;
this is counter intuitive since we expect biased models to depend more on
biases when invariant features are missing. Our work has implications for
distribution shift research and practice, providing new insights into how
models learn and rely on spurious correlations under different types of shifts.
</p></li>
</ul>

<h3>Title: Low-Light Image Enhancement via Structure Modeling and Guidance. (arXiv:2305.05839v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05839">http://arxiv.org/abs/2305.05839</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05839] Low-Light Image Enhancement via Structure Modeling and Guidance](http://arxiv.org/abs/2305.05839) #robust</code></li>
<li>Summary: <p>This paper proposes a new framework for low-light image enhancement by
simultaneously conducting the appearance as well as structure modeling. It
employs the structural feature to guide the appearance enhancement, leading to
sharp and realistic results. The structure modeling in our framework is
implemented as the edge detection in low-light images. It is achieved with a
modified generative model via designing a structure-aware feature extractor and
generator. The detected edge maps can accurately emphasize the essential
structural information, and the edge prediction is robust towards the noises in
dark areas. Moreover, to improve the appearance modeling, which is implemented
with a simple U-Net, a novel structure-guided enhancement module is proposed
with structure-guided feature synthesis layers. The appearance modeling, edge
detector, and enhancement module can be trained end-to-end. The experiments are
conducted on representative datasets (sRGB and RAW domains), showing that our
model consistently achieves SOTA performance on all datasets with the same
architecture.
</p></li>
</ul>

<h3>Title: Sketching the Future (STF): Applying Conditional Control Techniques to Text-to-Video Models. (arXiv:2305.05845v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05845">http://arxiv.org/abs/2305.05845</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05845] Sketching the Future (STF): Applying Conditional Control Techniques to Text-to-Video Models](http://arxiv.org/abs/2305.05845) #robust</code></li>
<li>Summary: <p>The proliferation of video content demands efficient and flexible neural
network based approaches for generating new video content. In this paper, we
propose a novel approach that combines zero-shot text-to-video generation with
ControlNet to improve the output of these models. Our method takes multiple
sketched frames as input and generates video output that matches the flow of
these frames, building upon the Text-to-Video Zero architecture and
incorporating ControlNet to enable additional input conditions. By first
interpolating frames between the inputted sketches and then running
Text-to-Video Zero using the new interpolated frames video as the control
technique, we leverage the benefits of both zero-shot text-to-video generation
and the robust control provided by ControlNet. Experiments demonstrate that our
method excels at producing high-quality and remarkably consistent video content
that more accurately aligns with the user's intended motion for the subject
within the video. We provide a comprehensive resource package, including a demo
video, project website, open-source GitHub repository, and a Colab playground
to foster further research and application of our proposed method.
</p></li>
</ul>

<h3>Title: Level-line Guided Edge Drawing for Robust Line Segment Detection. (arXiv:2305.05883v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05883">http://arxiv.org/abs/2305.05883</a></li>
<li>Code URL: <a href="https://github.com/roylin1229/gedrlsd">https://github.com/roylin1229/gedrlsd</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05883] Level-line Guided Edge Drawing for Robust Line Segment Detection](http://arxiv.org/abs/2305.05883) #robust</code></li>
<li>Summary: <p>Line segment detection plays a cornerstone role in computer vision tasks.
Among numerous detection methods that have been recently proposed, the ones
based on edge drawing attract increasing attention owing to their excellent
detection efficiency. However, the existing methods are not robust enough due
to the inadequate usage of image gradients for edge drawing and line segment
fitting. Based on the observation that the line segments should locate on the
edge points with both consistent coordinates and level-line information, i.e.,
the unit vector perpendicular to the gradient orientation, this paper proposes
a level-line guided edge drawing for robust line segment detection (GEDRLSD).
The level-line information provides potential directions for edge tracking,
which could be served as a guideline for accurate edge drawing. Additionally,
the level-line information is fused in line segment fitting to improve the
robustness. Numerical experiments show the superiority of the proposed GEDRLSD
algorithm compared with state-of-the-art methods.
</p></li>
</ul>

<h3>Title: DMNR: Unsupervised De-noising of Point Clouds Corrupted by Airborne Particles. (arXiv:2305.05991v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05991">http://arxiv.org/abs/2305.05991</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05991] DMNR: Unsupervised De-noising of Point Clouds Corrupted by Airborne Particles](http://arxiv.org/abs/2305.05991) #robust</code></li>
<li>Summary: <p>LiDAR sensors are critical for autonomous driving and robotics applications
due to their ability to provide accurate range measurements and their
robustness to lighting conditions. However, airborne particles, such as fog,
rain, snow, and dust, will degrade its performance and it is inevitable to
encounter these inclement environmental conditions outdoors. It would be a
straightforward approach to remove them by supervised semantic segmentation.
But annotating these particles point wisely is too laborious. To address this
problem and enhance the perception under inclement conditions, we develop two
dynamic filtering methods called Dynamic Multi-threshold Noise Removal (DMNR)
and DMNR-H by accurate analysis of the position distribution and intensity
characteristics of noisy points and clean points on publicly available WADS and
DENSE datasets. Both DMNR and DMNR-H outperform state-of-the-art unsupervised
methods by a significant margin on the two datasets and are slightly better
than supervised deep learning-based methods. Furthermore, our methods are more
robust to different LiDAR sensors and airborne particles, such as snow and fog.
</p></li>
</ul>

<h3>Title: The Robustness of Computer Vision Models against Common Corruptions: a Survey. (arXiv:2305.06024v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06024">http://arxiv.org/abs/2305.06024</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06024] The Robustness of Computer Vision Models against Common Corruptions: a Survey](http://arxiv.org/abs/2305.06024) #robust</code></li>
<li>Summary: <p>The performance of computer vision models is susceptible to unexpected
changes in input images when deployed in real scenarios. These changes are
referred to as common corruptions. While they can hinder the applicability of
computer vision models in real-world scenarios, they are not always considered
as a testbed for model generalization and robustness. In this survey, we
present a comprehensive and systematic overview of methods that improve
corruption robustness of computer vision models. Unlike existing surveys that
focus on adversarial attacks and label noise, we cover extensively the study of
robustness to common corruptions that can occur when deploying computer vision
models to work in practical applications. We describe different types of image
corruption and provide the definition of corruption robustness. We then
introduce relevant evaluation metrics and benchmark datasets. We categorize
methods into four groups. We also cover indirect methods that show improvements
in generalization and may improve corruption robustness as a byproduct. We
report benchmark results collected from the literature and find that they are
not evaluated in a unified manner, making it difficult to compare and analyze.
We thus built a unified benchmark framework to obtain directly comparable
results on benchmark datasets. Furthermore, we evaluate relevant backbone
networks pre-trained on ImageNet using our framework, providing an overview of
the base corruption robustness of existing models to help choose appropriate
backbones for computer vision tasks. We identify that developing methods to
handle a wide range of corruptions and efficiently learn with limited data and
computational resources is crucial for future development. Additionally, we
highlight the need for further investigation into the relationship among
corruption robustness, OOD generalization, and shortcut learning.
</p></li>
</ul>

<h3>Title: Ranking &amp; Reweighting Improves Group Distributional Robustness. (arXiv:2305.05759v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05759">http://arxiv.org/abs/2305.05759</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05759] Ranking &amp; Reweighting Improves Group Distributional Robustness](http://arxiv.org/abs/2305.05759) #robust</code></li>
<li>Summary: <p>Recent work has shown that standard training via empirical risk minimization
(ERM) can produce models that achieve high accuracy on average but low accuracy
on underrepresented groups due to the prevalence of spurious features. A
predominant approach to tackle this group robustness problem minimizes the
worst group error (akin to a minimax strategy) on the training data, hoping it
will generalize well on the testing data. However, this is often suboptimal,
especially when the out-of-distribution (OOD) test data contains previously
unseen groups. Inspired by ideas from the information retrieval and
learning-to-rank literature, this paper first proposes to use Discounted
Cumulative Gain (DCG) as a metric of model quality for facilitating better
hyperparameter tuning and model selection. Being a ranking-based metric, DCG
weights multiple poorly-performing groups (instead of considering just the
group with the worst performance). As a natural next step, we build on our
results to propose a ranking-based training method called Discounted Rank
Upweighting (DRU), which differentially reweights a ranked list of
poorly-performing groups in the training data to learn models that exhibit
strong OOD performance on the test data. Results on several synthetic and
real-world datasets highlight the superior generalization ability of our
group-ranking-based (akin to soft-minimax) approach in selecting and learning
models that are robust to group distributional shifts.
</p></li>
</ul>

<h3>Title: Multi-hop Commonsense Knowledge Injection Framework for Zero-Shot Commonsense Question Answering. (arXiv:2305.05936v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05936">http://arxiv.org/abs/2305.05936</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05936] Multi-hop Commonsense Knowledge Injection Framework for Zero-Shot Commonsense Question Answering](http://arxiv.org/abs/2305.05936) #robust</code></li>
<li>Summary: <p>Commonsense question answering (QA) research requires machines to answer
questions based on commonsense knowledge. However, this research requires
expensive labor costs to annotate data as the basis of research, and models
that rely on fine-tuning paradigms only apply to specific tasks, rather than
learn a general commonsense reasoning ability. As a more robust method,
zero-shot commonsense question answering shows a good prospect. The current
zero-shot framework tries to convert triples in commonsense knowledge graphs
(KGs) into QA-form samples as the pre-trained data source to incorporate
commonsense knowledge into the model. However, this method ignores the
multi-hop relationship in the KG, which is also an important central problem in
commonsense reasoning. In this paper, we propose a novel multi-hop commonsense
knowledge injection framework. Specifically, it explores multi-hop reasoning
paradigm in KGs that conform to linguistic logic, and we further propose two
multi-hop QA generation methods based on KGs. Then, we utilize contrastive
learning to pre-train the model with the synthetic QA dataset to inject
multi-hop commonsense knowledge. Extensive experiments on five commonsense
question answering benchmarks demonstrate that our framework achieves
state-of-art performance.
</p></li>
</ul>

<h3>Title: Learning Robust Self-attention Features for Speech Emotion Recognition with Label-adaptive Mixup. (arXiv:2305.06273v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06273">http://arxiv.org/abs/2305.06273</a></li>
<li>Code URL: <a href="https://github.com/leitro/labeladaptivemixup-ser">https://github.com/leitro/labeladaptivemixup-ser</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06273] Learning Robust Self-attention Features for Speech Emotion Recognition with Label-adaptive Mixup](http://arxiv.org/abs/2305.06273) #robust</code></li>
<li>Summary: <p>Speech Emotion Recognition (SER) is to recognize human emotions in a natural
verbal interaction scenario with machines, which is considered as a challenging
problem due to the ambiguous human emotions. Despite the recent progress in
SER, state-of-the-art models struggle to achieve a satisfactory performance. We
propose a self-attention based method with combined use of label-adaptive mixup
and center loss. By adapting label probabilities in mixup and fitting center
loss to the mixup training scheme, our proposed method achieves a superior
performance to the state-of-the-art methods.
</p></li>
</ul>

<h3>Title: RECKONING: Reasoning through Dynamic Knowledge Encoding. (arXiv:2305.06349v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06349">http://arxiv.org/abs/2305.06349</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06349] RECKONING: Reasoning through Dynamic Knowledge Encoding](http://arxiv.org/abs/2305.06349) #robust</code></li>
<li>Summary: <p>Recent studies on transformer-based language models show that they can answer
questions by reasoning over knowledge provided as part of the context (i.e.,
in-context reasoning). However, since the available knowledge is often not
filtered for a particular question, in-context reasoning can be sensitive to
distractor facts, additional content that is irrelevant to a question but that
may be relevant for a different question (i.e., not necessarily random noise).
In these situations, the model fails to distinguish the knowledge that is
necessary to answer the question, leading to spurious reasoning and degraded
performance. This reasoning failure contrasts with the model's apparent ability
to distinguish its contextual knowledge from all the knowledge it has memorized
during pre-training. Following this observation, we propose teaching the model
to reason more robustly by folding the provided contextual knowledge into the
model's parameters before presenting it with a question. Our method, RECKONING,
is a bi-level learning algorithm that teaches language models to reason by
updating their parametric knowledge through back-propagation, allowing them to
then answer questions using the updated parameters. During training, the inner
loop rapidly adapts a copy of the model weights to encode contextual knowledge
into its parameters. In the outer loop, the model learns to uses the updated
weights to reproduce and answer reasoning questions about the memorized
knowledge. Our experiments on two multi-hop reasoning datasets show that
RECKONING's performance improves over the in-context reasoning baseline (by up
to 4.5%). We also find that compared to in-context reasoning, RECKONING
generalizes better to longer reasoning chains unseen during training, is more
robust to distractors in the context, and is more computationally efficient
when multiple questions are asked about the same knowledge.
</p></li>
</ul>

<h3>Title: Neurosymbolic Artificial Intelligence (NSAI) based Algorithm for predicting the Impact Strength of Additive Manufactured Polylactic Acid (PLA) Specimens. (arXiv:2305.05668v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05668">http://arxiv.org/abs/2305.05668</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05668] Neurosymbolic Artificial Intelligence (NSAI) based Algorithm for predicting the Impact Strength of Additive Manufactured Polylactic Acid (PLA) Specimens](http://arxiv.org/abs/2305.05668) #robust</code></li>
<li>Summary: <p>In this study, we introduce application of Neurosymbolic Artificial
Intelligence (NSAI) for predicting the impact strength of additive manufactured
polylactic acid (PLA) components, representing the first-ever use of NSAI in
the domain of additive manufacturing. The NSAI model amalgamates the advantages
of neural networks and symbolic AI, offering a more robust and accurate
prediction than traditional machine learning techniques. Experimental data was
collected and synthetically augmented to 1000 data points, enhancing the
model's precision. The Neurosymbolic model was developed using a neural network
architecture comprising input, two hidden layers, and an output layer, followed
by a decision tree regressor representing the symbolic component. The model's
performance was benchmarked against a Simple Artificial Neural Network (ANN)
model by assessing mean squared error (MSE) and R-squared (R2) values for both
training and validation datasets. The results reveal that the Neurosymbolic
model surpasses the Simple ANN model, attaining lower MSE and higher R2 values
for both training and validation sets. This innovative application of the
Neurosymbolic approach in estimating the impact strength of additive
manufactured PLA components underscores its potential for optimizing the
additive manufacturing process. Future research could investigate further
refinements to the Neurosymbolic model, extend its application to other
materials and additive manufacturing processes, and incorporate real-time
monitoring and control for enhanced process optimization.
</p></li>
</ul>

<h3>Title: Causal Information Splitting: Engineering Proxy Features for Robustness to Distribution Shifts. (arXiv:2305.05832v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05832">http://arxiv.org/abs/2305.05832</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05832] Causal Information Splitting: Engineering Proxy Features for Robustness to Distribution Shifts](http://arxiv.org/abs/2305.05832) #robust</code></li>
<li>Summary: <p>Statistical prediction models are often trained on data that is drawn from
different probability distributions than their eventual use cases. One approach
to proactively prepare for these shifts harnesses the intuition that causal
mechanisms should remain invariant between environments. Here we focus on a
challenging setting in which the causal and anticausal variables of the target
are unobserved. Leaning on information theory, we develop feature selection and
engineering techniques for the observed downstream variables that act as
proxies. We identify proxies that help to build stable models and moreover
utilize auxiliary training tasks to extract stability-enhancing information
from proxies. We demonstrate the effectiveness of our techniques on synthetic
and real data.
</p></li>
</ul>

<h3>Title: Deep Partial Multi-Label Learning with Graph Disambiguation. (arXiv:2305.05882v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05882">http://arxiv.org/abs/2305.05882</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05882] Deep Partial Multi-Label Learning with Graph Disambiguation](http://arxiv.org/abs/2305.05882) #robust</code></li>
<li>Summary: <p>In partial multi-label learning (PML), each data example is equipped with a
candidate label set, which consists of multiple ground-truth labels and other
false-positive labels. Recently, graph-based methods, which demonstrate a good
ability to estimate accurate confidence scores from candidate labels, have been
prevalent to deal with PML problems. However, we observe that existing
graph-based PML methods typically adopt linear multi-label classifiers and thus
fail to achieve superior performance. In this work, we attempt to remove
several obstacles for extending them to deep models and propose a novel deep
Partial multi-Label model with grAph-disambIguatioN (PLAIN). Specifically, we
introduce the instance-level and label-level similarities to recover label
confidences as well as exploit label dependencies. At each training epoch,
labels are propagated on the instance and label graphs to produce relatively
accurate pseudo-labels; then, we train the deep model to fit the numerical
labels. Moreover, we provide a careful analysis of the risk functions to
guarantee the robustness of the proposed model. Extensive experiments on
various synthetic datasets and three real-world PML datasets demonstrate that
PLAIN achieves significantly superior results to state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Extracting Diagnosis Pathways from Electronic Health Records Using Deep Reinforcement Learning. (arXiv:2305.06295v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06295">http://arxiv.org/abs/2305.06295</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06295] Extracting Diagnosis Pathways from Electronic Health Records Using Deep Reinforcement Learning](http://arxiv.org/abs/2305.06295) #robust</code></li>
<li>Summary: <p>Clinical diagnosis guidelines aim at specifying the steps that may lead to a
diagnosis. Guidelines enable rationalizing and normalizing clinical decisions
but suffer drawbacks as they are built to cover the majority of the population
and may fail in guiding to the right diagnosis for patients with uncommon
conditions or multiple pathologies. Moreover, their updates are long and
expensive, making them unsuitable to emerging practices. Inspired by
guidelines, we formulate the task of diagnosis as a sequential decision-making
problem and study the use of Deep Reinforcement Learning (DRL) algorithms
trained on Electronic Health Records (EHRs) to learn the optimal sequence of
observations to perform in order to obtain a correct diagnosis. Because of the
variety of DRL algorithms and of their sensitivity to the context, we
considered several approaches and settings that we compared to each other, and
to classical classifiers. We experimented on a synthetic but realistic dataset
to differentially diagnose anemia and its subtypes and particularly evaluated
the robustness of various approaches to noise and missing data as those are
frequent in EHRs. Within the DRL algorithms, Dueling DQN with Prioritized
Experience Replay, and Dueling Double DQN with Prioritized Experience Replay
show the best and most stable performances. In the presence of imperfect data,
the DRL algorithms show competitive, but less stable performances when compared
to the classifiers (Random Forest and XGBoost); although they enable the
progressive generation of a pathway to the suggested diagnosis, which can both
guide or explain the decision process.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Change Detection Methods for Remote Sensing in the Last Decade: A Comprehensive Review. (arXiv:2305.05813v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05813">http://arxiv.org/abs/2305.05813</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05813] Change Detection Methods for Remote Sensing in the Last Decade: A Comprehensive Review](http://arxiv.org/abs/2305.05813) #extraction</code></li>
<li>Summary: <p>Change detection is an essential and widely utilized task in remote sensing
that aims to detect and analyze changes occurring in the same geographical area
over time, which has broad applications in urban development, agricultural
surveys, and land cover monitoring. Detecting changes in remote sensing images
is a complex challenge due to various factors, including variations in image
quality, noise, registration errors, illumination changes, complex landscapes,
and spatial heterogeneity. In recent years, deep learning has emerged as a
powerful tool for feature extraction and addressing these challenges. Its
versatility has resulted in its widespread adoption for numerous
image-processing tasks. This paper presents a comprehensive survey of
significant advancements in change detection for remote sensing images over the
past decade. We first introduce some preliminary knowledge for the change
detection task, such as problem definition, datasets, evaluation metrics, and
transformer basics, as well as provide a detailed taxonomy of existing
algorithms from three different perspectives: algorithm granularity,
supervision modes, and learning frameworks in the methodology section. This
survey enables readers to gain systematic knowledge of change detection tasks
from various angles. We then summarize the state-of-the-art performance on
several dominant change detection datasets, providing insights into the
strengths and limitations of existing algorithms. Based on our survey, some
future research directions for change detection in remote sensing are well
identified. This survey paper will shed some light on the community and inspire
further research efforts in the change detection task.
</p></li>
</ul>

<h3>Title: Weakly-supervised ROI extraction method based on contrastive learning for remote sensing images. (arXiv:2305.05887v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05887">http://arxiv.org/abs/2305.05887</a></li>
<li>Code URL: <a href="https://github.com/he-lingfeng/roi-extraction">https://github.com/he-lingfeng/roi-extraction</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05887] Weakly-supervised ROI extraction method based on contrastive learning for remote sensing images](http://arxiv.org/abs/2305.05887) #extraction</code></li>
<li>Summary: <p>ROI extraction is an active but challenging task in remote sensing because of
the complicated landform, the complex boundaries and the requirement of
annotations. Weakly supervised learning (WSL) aims at learning a mapping from
input image to pixel-wise prediction under image-wise labels, which can
dramatically decrease the labor cost. However, due to the imprecision of
labels, the accuracy and time consumption of WSL methods are relatively
unsatisfactory. In this paper, we propose a two-step ROI extraction based on
contractive learning. Firstly, we present to integrate multiscale Grad-CAM to
obtain pseudo pixelwise annotations with well boundaries. Then, to reduce the
compact of misjudgments in pseudo annotations, we construct a contrastive
learning strategy to encourage the features inside ROI as close as possible and
separate background features from foreground features. Comprehensive
experiments demonstrate the superiority of our proposal. Code is available at
https://github.com/HE-Lingfeng/ROI-Extraction
</p></li>
</ul>

<h3>Title: VTPNet for 3D deep learning on point cloud. (arXiv:2305.06115v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06115">http://arxiv.org/abs/2305.06115</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06115] VTPNet for 3D deep learning on point cloud](http://arxiv.org/abs/2305.06115) #extraction</code></li>
<li>Summary: <p>Recently, Transformer-based methods for point cloud learning have achieved
good results on various point cloud learning benchmarks. However, since the
attention mechanism needs to generate three feature vectors of query, key, and
value to calculate attention features, most of the existing Transformer-based
point cloud learning methods usually consume a large amount of computational
time and memory resources when calculating global attention. To address this
problem, we propose a Voxel-Transformer-Point (VTP) Block for extracting local
and global features of point clouds. VTP combines the advantages of
voxel-based, point-based and Transformer-based methods, which consists of
Voxel-Based Branch (V branch), Point-Based Transformer Branch (PT branch) and
Point-Based Branch (P branch). The V branch extracts the coarse-grained
features of the point cloud through low voxel resolution; the PT branch obtains
the fine-grained features of the point cloud by calculating the self-attention
in the local neighborhood and the inter-neighborhood cross-attention; the P
branch uses a simplified MLP network to generate the global location
information of the point cloud. In addition, to enrich the local features of
point clouds at different scales, we set the voxel scale in the V branch and
the neighborhood sphere scale in the PT branch to one large and one small
(large voxel scale \&amp; small neighborhood sphere scale or small voxel scale \&amp;
large neighborhood sphere scale). Finally, we use VTP as the feature extraction
network to construct a VTPNet for point cloud learning, and performs shape
classification, part segmentation, and semantic segmentation tasks on the
ModelNet40, ShapeNet Part, and S3DIS datasets. The experimental results
indicate that VTPNet has good performance in 3D point cloud learning.
</p></li>
</ul>

<h3>Title: Clothes-Invariant Feature Learning by Causal Intervention for Clothes-Changing Person Re-identification. (arXiv:2305.06145v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06145">http://arxiv.org/abs/2305.06145</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06145] Clothes-Invariant Feature Learning by Causal Intervention for Clothes-Changing Person Re-identification](http://arxiv.org/abs/2305.06145) #extraction</code></li>
<li>Summary: <p>Clothes-invariant feature extraction is critical to the clothes-changing
person re-identification (CC-ReID). It can provide discriminative identity
features and eliminate the negative effects caused by the confounder--clothing
changes. But we argue that there exists a strong spurious correlation between
clothes and human identity, that restricts the common likelihood-based ReID
method P(Y|X) to extract clothes-irrelevant features. In this paper, we propose
a new Causal Clothes-Invariant Learning (CCIL) method to achieve
clothes-invariant feature learning by modeling causal intervention P(Y|do(X)).
This new causality-based model is inherently invariant to the confounder in the
causal view, which can achieve the clothes-invariant features and avoid the
barrier faced by the likelihood-based methods. Extensive experiments on three
CC-ReID benchmarks, including PRCC, LTCC, and VC-Clothes, demonstrate the
effectiveness of our approach, which achieves a new state of the art.
</p></li>
</ul>

<h3>Title: CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors. (arXiv:2305.05711v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05711">http://arxiv.org/abs/2305.05711</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05711] CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors](http://arxiv.org/abs/2305.05711) #extraction</code></li>
<li>Summary: <p>Large language models (LLMs) pre-trained on massive corpora have demonstrated
impressive few-shot learning ability on many NLP tasks. A common practice is to
recast the task into a text-to-text format such that generative LLMs of natural
language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is
nontrivial to perform information extraction (IE) tasks with NL-LLMs since the
output of the IE task is usually structured and therefore is hard to be
converted into plain text. In this paper, we propose to recast the structured
output in the form of code instead of natural language and utilize generative
LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular,
named entity recognition and relation extraction. In contrast to NL-LLMs, we
show that Code-LLMs can be well-aligned with these IE tasks by designing
code-style prompts and formulating these IE tasks as code generation tasks.
Experiment results on seven benchmarks show that our method consistently
outperforms fine-tuning moderate-size pre-trained models specially designed for
IE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further
conduct a series of in-depth analyses to demonstrate the merits of leveraging
Code-LLMs for IE tasks.
</p></li>
</ul>

<h2>membership infer</h2>
<h3>Title: Finding Meaningful Distributions of ML Black-boxes under Forensic Investigation. (arXiv:2305.05869v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05869">http://arxiv.org/abs/2305.05869</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05869] Finding Meaningful Distributions of ML Black-boxes under Forensic Investigation](http://arxiv.org/abs/2305.05869) #membership infer</code></li>
<li>Summary: <p>Given a poorly documented neural network model, we take the perspective of a
forensic investigator who wants to find out the model's data domain (e.g.
whether on face images or traffic signs). Although existing methods such as
membership inference and model inversion can be used to uncover some
information about an unknown model, they still require knowledge of the data
domain to start with. In this paper, we propose solving this problem by
leveraging on comprehensive corpus such as ImageNet to select a meaningful
distribution that is close to the original training distribution and leads to
high performance in follow-up investigations. The corpus comprises two
components, a large dataset of samples and meta information such as
hierarchical structure and textual information on the samples. Our goal is to
select a set of samples from the corpus for the given model. The core of our
method is an objective function that considers two criteria on the selected
samples: the model functional properties (derived from the dataset), and
semantics (derived from the metadata). We also give an algorithm to efficiently
search the large space of all possible subsets w.r.t. the objective function.
Experimentation results show that the proposed method is effective. For
example, cloning a given model (originally trained with CIFAR-10) by using
Caltech 101 can achieve 45.5% accuracy. By using datasets selected by our
method, the accuracy is improved to 72.0%.
</p></li>
</ul>

<h2>federate</h2>
<h3>Title: FedDWA: Personalized Federated Learning with Online Weight Adjustment. (arXiv:2305.06124v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06124">http://arxiv.org/abs/2305.06124</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06124] FedDWA: Personalized Federated Learning with Online Weight Adjustment](http://arxiv.org/abs/2305.06124) #federate</code></li>
<li>Summary: <p>Different from conventional federated learning, personalized federated
learning (PFL) is able to train a customized model for each individual client
according to its unique requirement. The mainstream approach is to adopt a kind
of weighted aggregation method to generate personalized models, in which
weights are determined by the loss value or model parameters among different
clients. However, such kinds of methods require clients to download others'
models. It not only sheer increases communication traffic but also potentially
infringes data privacy. In this paper, we propose a new PFL algorithm called
\emph{FedDWA (Federated Learning with Dynamic Weight Adjustment)} to address
the above problem, which leverages the parameter server (PS) to compute
personalized aggregation weights based on collected models from clients. In
this way, FedDWA can capture similarities between clients with much less
communication overhead. More specifically, we formulate the PFL problem as an
optimization problem by minimizing the distance between personalized models and
guidance models, so as to customize aggregation weights for each client.
Guidance models are obtained by the local one-step ahead adaptation on
individual clients. Finally, we conduct extensive experiments using five real
datasets and the results demonstrate that FedDWA can significantly reduce the
communication traffic and achieve much higher model accuracy than the
state-of-the-art approaches.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: An Evaluation and Ranking of Different Voting Schemes for Improved Visual Place Recognition. (arXiv:2305.05705v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05705">http://arxiv.org/abs/2305.05705</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05705] An Evaluation and Ranking of Different Voting Schemes for Improved Visual Place Recognition](http://arxiv.org/abs/2305.05705) #fair</code></li>
<li>Summary: <p>Visual Place Recognition has recently seen a surge of endeavours utilizing
different ensemble approaches to improve VPR performance. Ideas like
multi-process fusion or switching involve combining different VPR techniques
together, utilizing different strategies. One major aspect often common to many
of these strategies is voting. Voting is widely used in many ensemble methods,
so it is potentially a relevant subject to explore in terms of its application
and significance for improving VPR performance. This paper attempts to looks
into detail and analyze a variety of voting schemes to evaluate which voting
technique is optimal for an ensemble VPR set up. We take inspiration from a
variety of voting schemes that exist and are widely employed in other research
fields such as politics and sociology. The idea is inspired by an observation
that different voting methods result in different outcomes for the same type of
data and each voting scheme is utilized for specific cases in different
academic fields. Some of these voting schemes include Condorcet voting, Broda
Count and Plurality voting. Voting employed in any aspect requires that a fair
system be established, that outputs the best and most favourable results which
in our case would involve improving VPR performance. We evaluate some of these
voting techniques in a standardized testing of different VPR techniques, using
a variety of VPR data sets. We aim to determine whether a single optimal voting
scheme exists or, much like in other fields of research, the selection of a
voting technique is relative to its application and environment. We also aim to
propose a ranking of these different voting methods from best to worst
according to our results as this will allow for better selection of voting
schemes.
</p></li>
</ul>

<h3>Title: Effects of data time lag in a decision-making system using machine learning for pork price prediction. (arXiv:2305.05677v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05677">http://arxiv.org/abs/2305.05677</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05677] Effects of data time lag in a decision-making system using machine learning for pork price prediction](http://arxiv.org/abs/2305.05677) #fair</code></li>
<li>Summary: <p>Spain is the third-largest producer of pork meat in the world, and many farms
in several regions depend on the evolution of this market. However, the current
pricing system is unfair, as some actors have better market information than
others. In this context, historical pricing is an easy-to-find and affordable
data source that can help all agents to be better informed. However, the time
lag in data acquisition can affect their pricing decisions. In this paper, we
study the effect that data acquisition delay has on a price prediction system
using multiple prediction algorithms. We describe the integration of the best
proposal into a decision support system prototype and test it in a real-case
scenario. Specifically, we use public data from the most important regional
pork meat markets in Spain published by the Ministry of Agriculture with a
two-week delay and subscription-based data of the same markets obtained on the
same day. The results show that the error difference between the best public
and data subscription models is 0.6 Euro cents in favor of the data without
delay. The market dimension makes these differences significant in the supply
chain, giving pricing agents a better tool to negotiate market prices.
</p></li>
</ul>

<h3>Title: Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments. (arXiv:2305.06026v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06026">http://arxiv.org/abs/2305.06026</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06026] Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments](http://arxiv.org/abs/2305.06026) #fair</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) are a pertinent tool for any machine learning
task due to their ability to learn functions over graph structures, a powerful
and expressive data representation. The detection of communities, an
unsupervised task has increasingly been performed with GNNs. Clustering nodes
in a graph using the multi-dimensionality of node features with the
connectivity of the graph has many applications to real world tasks from social
networks to genomics. Unfortunately, there is currently a gap in the literature
with no established sufficient benchmarking environment for fairly and
rigorously evaluating GNN based community detection, thereby potentially
impeding progress in this nascent field. We observe the particular difficulties
in this setting is the ambiguous hyperparameter tuning environments combined
with conflicting metrics of performance and evaluation datasets. In this work,
we propose and evaluate frameworks for the consistent comparisons of community
detection algorithms using GNNs. With this, we show the strong dependence of
the performance to the experimental settings, exacerbated by factors such as
the use of GNNs and the unsupervised nature of the task, providing clear
motivation for the use of a framework to facilitate congruent research in the
field.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: FusionDepth: Complement Self-Supervised Monocular Depth Estimation with Cost Volume. (arXiv:2305.06036v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06036">http://arxiv.org/abs/2305.06036</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06036] FusionDepth: Complement Self-Supervised Monocular Depth Estimation with Cost Volume](http://arxiv.org/abs/2305.06036) #interpretability</code></li>
<li>Summary: <p>Multi-view stereo depth estimation based on cost volume usually works better
than self-supervised monocular depth estimation except for moving objects and
low-textured surfaces. So in this paper, we propose a multi-frame depth
estimation framework which monocular depth can be refined continuously by
multi-frame sequential constraints, leveraging a Bayesian fusion layer within
several iterations. Both monocular and multi-view networks can be trained with
no depth supervision. Our method also enhances the interpretability when
combining monocular estimation with multi-view cost volume. Detailed
experiments show that our method surpasses state-of-the-art unsupervised
methods utilizing single or multiple frames at test time on KITTI benchmark.
</p></li>
</ul>

<h3>Title: Interpretable multimodal sentiment analysis based on textual modality descriptions by using large-scale language models. (arXiv:2305.06162v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06162">http://arxiv.org/abs/2305.06162</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06162] Interpretable multimodal sentiment analysis based on textual modality descriptions by using large-scale language models](http://arxiv.org/abs/2305.06162) #interpretability</code></li>
<li>Summary: <p>Multimodal sentiment analysis is an important area for understanding the
user's internal states. Deep learning methods were effective, but the problem
of poor interpretability has gradually gained attention. Previous works have
attempted to use attention weights or vector distributions to provide
interpretability. However, their explanations were not intuitive and can be
influenced by different trained models. This study proposed a novel approach to
provide interpretability by converting nonverbal modalities into text
descriptions and by using large-scale language models for sentiment
predictions. This provides an intuitive approach to directly interpret what
models depend on with respect to making decisions from input texts, thus
significantly improving interpretability. Specifically, we convert descriptions
based on two feature patterns for the audio modality and discrete action units
for the facial modality. Experimental results on two sentiment analysis tasks
demonstrated that the proposed approach maintained, or even improved
effectiveness for sentiment analysis compared to baselines using conventional
features, with the highest improvement of 2.49% on the F1 score. The results
also showed that multimodal descriptions have similar characteristics on fusing
modalities as those of conventional fusion methods. The results demonstrated
that the proposed approach is interpretable and effective for multimodal
sentiment analysis.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h3>Title: SepMark: Deep Separable Watermarking for Unified Source Tracing and Deepfake Detection. (arXiv:2305.06321v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06321">http://arxiv.org/abs/2305.06321</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06321] SepMark: Deep Separable Watermarking for Unified Source Tracing and Deepfake Detection](http://arxiv.org/abs/2305.06321) #watermark</code></li>
<li>Summary: <p>Malicious Deepfakes have led to a sharp conflict over distinguishing between
genuine and forged faces. Although many countermeasures have been developed to
detect Deepfakes ex-post, undoubtedly, passive forensics has not considered any
preventive measures for the pristine face before foreseeable manipulations. To
complete this forensics ecosystem, we thus put forward the proactive solution
dubbed SepMark, which provides a unified framework for source tracing and
Deepfake detection. SepMark originates from encoder-decoder-based deep
watermarking but with two separable decoders. For the first time the deep
separable watermarking, SepMark brings a new paradigm to the established study
of deep watermarking, where a single encoder embeds one watermark elegantly,
while two decoders can extract the watermark separately at different levels of
robustness. The robust decoder termed Tracer that resists various distortions
may have an overly high level of robustness, allowing the watermark to survive
both before and after Deepfake. The semi-robust one termed Detector is
selectively sensitive to malicious distortions, making the watermark disappear
after Deepfake. Only SepMark comprising of Tracer and Detector can reliably
trace the trusted source of the marked face and detect whether it has been
altered since being marked; neither of the two alone can achieve this.
Extensive experiments demonstrate the effectiveness of the proposed SepMark on
typical Deepfakes, including face swapping, expression reenactment, and
attribute editing.
</p></li>
</ul>

<h2>diffusion</h2>
<h3>Title: DifFIQA: Face Image Quality Assessment Using Denoising Diffusion Probabilistic Models. (arXiv:2305.05768v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05768">http://arxiv.org/abs/2305.05768</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05768] DifFIQA: Face Image Quality Assessment Using Denoising Diffusion Probabilistic Models](http://arxiv.org/abs/2305.05768) #diffusion</code></li>
<li>Summary: <p>Modern face recognition (FR) models excel in constrained scenarios, but often
suffer from decreased performance when deployed in unconstrained (real-world)
environments due to uncertainties surrounding the quality of the captured
facial data. Face image quality assessment (FIQA) techniques aim to mitigate
these performance degradations by providing FR models with sample-quality
predictions that can be used to reject low-quality samples and reduce false
match errors. However, despite steady improvements, ensuring reliable quality
estimates across facial images with diverse characteristics remains
challenging. In this paper, we present a powerful new FIQA approach, named
DifFIQA, which relies on denoising diffusion probabilistic models (DDPM) and
ensures highly competitive results. The main idea behind the approach is to
utilize the forward and backward processes of DDPMs to perturb facial images
and quantify the impact of these perturbations on the corresponding image
embeddings for quality prediction. Because the diffusion-based perturbations
are computationally expensive, we also distill the knowledge encoded in DifFIQA
into a regression-based quality predictor, called DifFIQA(R), that balances
performance and execution time. We evaluate both models in comprehensive
experiments on 7 datasets, with 4 target FR models and against 10
state-of-the-art FIQA techniques with highly encouraging results. The source
code will be made publicly available.
</p></li>
</ul>

<h3>Title: Comprehensive Dataset of Synthetic and Manipulated Overhead Imagery for Development and Evaluation of Forensic Tools. (arXiv:2305.05784v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05784">http://arxiv.org/abs/2305.05784</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05784] Comprehensive Dataset of Synthetic and Manipulated Overhead Imagery for Development and Evaluation of Forensic Tools](http://arxiv.org/abs/2305.05784) #diffusion</code></li>
<li>Summary: <p>We present a first of its kind dataset of overhead imagery for development
and evaluation of forensic tools. Our dataset consists of real, fully synthetic
and partially manipulated overhead imagery generated from a custom diffusion
model trained on two sets of different zoom levels and on two sources of
pristine data. We developed our model to support controllable generation of
multiple manipulation categories including fully synthetic imagery conditioned
on real and generated base maps, and location. We also support partial
in-painted imagery with same conditioning options and with several types of
manipulated content. The data consist of raw images and ground truth
annotations describing the manipulation parameters. We also report benchmark
performance on several tasks supported by our dataset including detection of
fully and partially manipulated imagery, manipulation localization and
classification.
</p></li>
</ul>

<h3>Title: Text-guided High-definition Consistency Texture Model. (arXiv:2305.05901v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05901">http://arxiv.org/abs/2305.05901</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05901] Text-guided High-definition Consistency Texture Model](http://arxiv.org/abs/2305.05901) #diffusion</code></li>
<li>Summary: <p>With the advent of depth-to-image diffusion models, text-guided generation,
editing, and transfer of realistic textures are no longer difficult. However,
due to the limitations of pre-trained diffusion models, they can only create
low-resolution, inconsistent textures. To address this issue, we present the
High-definition Consistency Texture Model (HCTM), a novel method that can
generate high-definition and consistent textures for 3D meshes according to the
text prompts. We achieve this by leveraging a pre-trained depth-to-image
diffusion model to generate single viewpoint results based on the text prompt
and a depth map. We fine-tune the diffusion model with Parameter-Efficient
Fine-Tuning to quickly learn the style of the generated result, and apply the
multi-diffusion strategy to produce high-resolution and consistent results from
different viewpoints. Furthermore, we propose a strategy that prevents the
appearance of noise on the textures caused by backpropagation. Our proposed
approach has demonstrated promising results in generating high-definition and
consistent textures for 3D meshes, as demonstrated through a series of
experiments.
</p></li>
</ul>

<h3>Title: iEdit: Localised Text-guided Image Editing with Weak Supervision. (arXiv:2305.05947v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05947">http://arxiv.org/abs/2305.05947</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05947] iEdit: Localised Text-guided Image Editing with Weak Supervision](http://arxiv.org/abs/2305.05947) #diffusion</code></li>
<li>Summary: <p>Diffusion models (DMs) can generate realistic images with text guidance using
large-scale datasets. However, they demonstrate limited controllability in the
output space of the generated images. We propose a novel learning method for
text-guided image editing, namely \texttt{iEdit}, that generates images
conditioned on a source image and a textual edit prompt. As a fully-annotated
dataset with target images does not exist, previous approaches perform
subject-specific fine-tuning at test time or adopt contrastive learning without
a target image, leading to issues on preserving the fidelity of the source
image. We propose to automatically construct a dataset derived from LAION-5B,
containing pseudo-target images with their descriptive edit prompts given input
image-caption pairs. This dataset gives us the flexibility of introducing a
weakly-supervised loss function to generate the pseudo-target image from the
latent noise of the source image conditioned on the edit prompt. To encourage
localised editing and preserve or modify spatial structures in the image, we
propose a loss function that uses segmentation masks to guide the editing
during training and optionally at inference. Our model is trained on the
constructed dataset with 200K samples and constrained GPU resources. It shows
favourable results against its counterparts in terms of image fidelity, CLIP
alignment score and qualitatively for editing both generated and real images.
</p></li>
</ul>

<h3>Title: Relightify: Relightable 3D Faces from a Single Image via Diffusion Models. (arXiv:2305.06077v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06077">http://arxiv.org/abs/2305.06077</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06077] Relightify: Relightable 3D Faces from a Single Image via Diffusion Models](http://arxiv.org/abs/2305.06077) #diffusion</code></li>
<li>Summary: <p>Following the remarkable success of diffusion models on image generation,
recent works have also demonstrated their impressive ability to address a
number of inverse problems in an unsupervised way, by properly constraining the
sampling process based on a conditioning input. Motivated by this, in this
paper, we present the first approach to use diffusion models as a prior for
highly accurate 3D facial BRDF reconstruction from a single image. We start by
leveraging a high-quality UV dataset of facial reflectance (diffuse and
specular albedo and normals), which we render under varying illumination
settings to simulate natural RGB textures and, then, train an unconditional
diffusion model on concatenated pairs of rendered textures and reflectance
components. At test time, we fit a 3D morphable model to the given image and
unwrap the face in a partial UV texture. By sampling from the diffusion model,
while retaining the observed texture part intact, the model inpaints not only
the self-occluded areas but also the unknown reflectance components, in a
single sequence of denoising steps. In contrast to existing methods, we
directly acquire the observed texture from the input image, thus, resulting in
more faithful and consistent reflectance estimation. Through a series of
qualitative and quantitative comparisons, we demonstrate superior performance
in both texture completion as well as reflectance reconstruction tasks.
</p></li>
</ul>

<h2>noise learning</h2>
<h3>Title: Rethinking the Value of Labels for Instance-Dependent Label Noise Learning. (arXiv:2305.06247v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06247">http://arxiv.org/abs/2305.06247</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06247] Rethinking the Value of Labels for Instance-Dependent Label Noise Learning](http://arxiv.org/abs/2305.06247) #noise learning</code></li>
<li>Summary: <p>Label noise widely exists in large-scale datasets and significantly
degenerates the performances of deep learning algorithms. Due to the
non-identifiability of the instance-dependent noise transition matrix, most
existing algorithms address the problem by assuming the noisy label generation
process to be independent of the instance features. Unfortunately, noisy labels
in real-world applications often depend on both the true label and the
features. In this work, we tackle instance-dependent label noise with a novel
deep generative model that avoids explicitly modeling the noise transition
matrix. Our algorithm leverages casual representation learning and
simultaneously identifies the high-level content and style latent factors from
the data. By exploiting the supervision information of noisy labels with
structural causal models, our empirical evaluations on a wide range of
synthetic and real-world instance-dependent label noise datasets demonstrate
that the proposed algorithm significantly outperforms the state-of-the-art
counterparts.
</p></li>
</ul>

<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: MMoT: Mixture-of-Modality-Tokens Transformer for Composed Multimodal Conditional Image Synthesis. (arXiv:2305.05992v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05992">http://arxiv.org/abs/2305.05992</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05992] MMoT: Mixture-of-Modality-Tokens Transformer for Composed Multimodal Conditional Image Synthesis](http://arxiv.org/abs/2305.05992) #transformer</code></li>
<li>Summary: <p>Existing multimodal conditional image synthesis (MCIS) methods generate
images conditioned on any combinations of various modalities that require all
of them must be exactly conformed, hindering the synthesis controllability and
leaving the potential of cross-modality under-exploited. To this end, we
propose to generate images conditioned on the compositions of multimodal
control signals, where modalities are imperfectly complementary, i.e., composed
multimodal conditional image synthesis (CMCIS). Specifically, we observe two
challenging issues of the proposed CMCIS task, i.e., the modality coordination
problem and the modality imbalance problem. To tackle these issues, we
introduce a Mixture-of-Modality-Tokens Transformer (MMoT) that adaptively fuses
fine-grained multimodal control signals, a multimodal balanced training loss to
stabilize the optimization of each modality, and a multimodal sampling guidance
to balance the strength of each modality control signal. Comprehensive
experimental results demonstrate that MMoT achieves superior performance on
both unimodal conditional image synthesis (UCIS) and MCIS tasks with
high-quality and faithful image synthesis on complex multimodal conditions. The
project website is available at https://jabir-zheng.github.io/MMoT.
</p></li>
</ul>

<h3>Title: Brain Tumor Detection using Swin Transformers. (arXiv:2305.06025v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06025">http://arxiv.org/abs/2305.06025</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06025] Brain Tumor Detection using Swin Transformers](http://arxiv.org/abs/2305.06025) #transformer</code></li>
<li>Summary: <p>The first MRI scan was done in the year 1978 by researchers at EML
Laboratories. As per an estimate, approximately 251,329 people died due to
primary cancerous brain and CNS (Central Nervous System) Tumors in the year</li>
<li>It has been recommended by various medical professionals that brain tumor
detection at an early stage would help in saving many lives. Whenever
radiologists deal with a brain MRI they try to diagnose it with the
histological subtype which is quite subjective and here comes the major issue.
Upon that, in developing countries like India, where there is 1 doctor for
every 1151 people, the need for efficient diagnosis to help radiologists and
doctors come into picture. In our approach, we aim to solve the problem using
swin transformers and deep learning to detect, classify, locate and provide the
size of the tumor in the particular MRI scan which would assist the doctors and
radiologists in increasing their efficiency. At the end, the medics would be
able to download the predictions and measures in a PDF (Portable Document
Format). Keywords: brain tumor, transformers, classification, medical, deep
learning, detection
</p></li>
</ul>

<h3>Title: Transformer-based model for monocular visual odometry: a video understanding approach. (arXiv:2305.06121v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06121">http://arxiv.org/abs/2305.06121</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06121] Transformer-based model for monocular visual odometry: a video understanding approach](http://arxiv.org/abs/2305.06121) #transformer</code></li>
<li>Summary: <p>Estimating the camera pose given images of a single camera is a traditional
task in mobile robots and autonomous vehicles. This problem is called monocular
visual odometry and it often relies on geometric approaches that require
engineering effort for a specific scenario. Deep learning methods have shown to
be generalizable after proper training and a considerable amount of available
data. Transformer-based architectures have dominated the state-of-the-art in
natural language processing and computer vision tasks, such as image and video
understanding. In this work, we deal with the monocular visual odometry as a
video understanding task to estimate the 6-DoF camera's pose. We contribute by
presenting the TSformer-VO model based on spatio-temporal self-attention
mechanisms to extract features from clips and estimate the motions in an
end-to-end manner. Our approach achieved competitive state-of-the-art
performance compared with geometry-based and deep learning-based methods on the
KITTI visual odometry dataset, outperforming the DeepVO implementation highly
accepted in the visual odometry community.
</p></li>
</ul>

<h3>Title: Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving. (arXiv:2305.06242v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06242">http://arxiv.org/abs/2305.06242</a></li>
<li>Code URL: <a href="https://github.com/opendrivelab/thinktwice">https://github.com/opendrivelab/thinktwice</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06242] Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving](http://arxiv.org/abs/2305.06242) #transformer</code></li>
<li>Summary: <p>End-to-end autonomous driving has made impressive progress in recent years.
Existing methods usually adopt the decoupled encoder-decoder paradigm, where
the encoder extracts hidden features from raw sensor data, and the decoder
outputs the ego-vehicle's future trajectories or actions. Under such a
paradigm, the encoder does not have access to the intended behavior of the ego
agent, leaving the burden of finding out safety-critical regions from the
massive receptive field and inferring about future situations to the decoder.
Even worse, the decoder is usually composed of several simple multi-layer
perceptrons (MLP) or GRUs while the encoder is delicately designed (e.g., a
combination of heavy ResNets or Transformer). Such an imbalanced resource-task
division hampers the learning process.
</p></li>
</ul>

<p>In this work, we aim to alleviate the aforementioned problem by two
principles: (1) fully utilizing the capacity of the encoder; (2) increasing the
capacity of the decoder. Concretely, we first predict a coarse-grained future
position and action based on the encoder features. Then, conditioned on the
position and action, the future scene is imagined to check the ramification if
we drive accordingly. We also retrieve the encoder features around the
predicted coordinate to obtain fine-grained information about the
safety-critical region. Finally, based on the predicted future and the
retrieved salient feature, we refine the coarse-grained position and action by
predicting its offset from ground-truth. The above refinement module could be
stacked in a cascaded fashion, which extends the capacity of the decoder with
spatial-temporal prior knowledge about the conditioned future. We conduct
experiments on the CARLA simulator and achieve state-of-the-art performance in
closed-loop benchmarks. Extensive ablation studies demonstrate the
effectiveness of each proposed module.
</p>

<h3>Title: SoGAR: Self-supervised Spatiotemporal Attention-based Social Group Activity Recognition. (arXiv:2305.06310v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06310">http://arxiv.org/abs/2305.06310</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06310] SoGAR: Self-supervised Spatiotemporal Attention-based Social Group Activity Recognition](http://arxiv.org/abs/2305.06310) #transformer</code></li>
<li>Summary: <p>This paper introduces a novel approach to Social Group Activity Recognition
(SoGAR) using Self-supervised Transformers network that can effectively utilize
unlabeled video data. To extract spatio-temporal information, we create local
and global views with varying frame rates. Our self-supervised objective
ensures that features extracted from contrasting views of the same video are
consistent across spatio-temporal domains. Our proposed approach is efficient
in using transformer-based encoders for alleviating the weakly supervised
setting of group activity recognition. By leveraging the benefits of
transformer models, our approach can model long-term relationships along
spatio-temporal dimensions. Our proposed SoGAR method achieves state-of-the-art
results on three group activity recognition benchmarks, namely JRDB-PAR, NBA,
and Volleyball datasets, surpassing the current state-of-the-art in terms of
F1-score, MCA, and MPCA metrics.
</p></li>
</ul>

<h3>Title: Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception. (arXiv:2305.06324v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06324">http://arxiv.org/abs/2305.06324</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06324] Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception](http://arxiv.org/abs/2305.06324) #transformer</code></li>
<li>Summary: <p>We present Integrated Multimodal Perception (IMP), a simple and scalable
multimodal multi-task training and modeling approach. IMP integrates multimodal
inputs including image, video, text, and audio into a single Transformer
encoder with minimal modality-specific components. IMP makes use of a novel
design that combines Alternating Gradient Descent (AGD) and Mixture-of-Experts
(MoE) for efficient model \&amp; task scaling. We conduct extensive empirical
studies about IMP and reveal the following key insights: 1) performing gradient
descent updates by alternating on diverse heterogeneous modalities, loss
functions, and tasks, while also varying input resolutions, efficiently
improves multimodal understanding. 2) model sparsification with MoE on a single
modality-agnostic encoder substantially improves the performance, outperforming
dense models that use modality-specific encoders or additional fusion layers
and greatly mitigating the conflicts between modalities. IMP achieves
competitive performance on a wide range of downstream tasks including image
classification, video classification, image-text, and video-text retrieval.
Most notably, we train a sparse IMP-MoE-L focusing on video tasks that achieves
new state-of-the-art in zero-shot video classification. Our model achieves
77.0% on Kinetics-400, 76.8% on Kinetics-600, and 76.8% on Kinetics-700
zero-shot classification accuracy, improving the previous state-of-the-art by
+5%, +6.7%, and +5.8%, respectively, while using only 15% of their total
training computational cost.
</p></li>
</ul>

<h3>Title: Multi-Path Transformer is Better: A Case Study on Neural Machine Translation. (arXiv:2305.05948v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05948">http://arxiv.org/abs/2305.05948</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05948] Multi-Path Transformer is Better: A Case Study on Neural Machine Translation](http://arxiv.org/abs/2305.05948) #transformer</code></li>
<li>Summary: <p>For years the model performance in machine learning obeyed a power-law
relationship with the model size. For the consideration of parameter
efficiency, recent studies focus on increasing model depth rather than width to
achieve better performance. In this paper, we study how model width affects the
Transformer model through a parameter-efficient multi-path structure. To better
fuse features extracted from different paths, we add three additional
operations to each sublayer: a normalization at the end of each path, a cheap
operation to produce more features, and a learnable weighted mechanism to fuse
all features flexibly. Extensive experiments on 12 WMT machine translation
tasks show that, with the same number of parameters, the shallower multi-path
model can achieve similar or even better performance than the deeper model. It
reveals that we should pay more attention to the multi-path structure, and
there should be a balance between the model depth and width to train a better
large-scale Transformer.
</p></li>
</ul>

<h3>Title: PAI at SemEval-2023 Task 2: A Universal System for Named Entity Recognition with External Entity Information. (arXiv:2305.06099v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06099">http://arxiv.org/abs/2305.06099</a></li>
<li>Code URL: <a href="https://github.com/diqiuzhuanzhuan/semeval-2023">https://github.com/diqiuzhuanzhuan/semeval-2023</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06099] PAI at SemEval-2023 Task 2: A Universal System for Named Entity Recognition with External Entity Information](http://arxiv.org/abs/2305.06099) #transformer</code></li>
<li>Summary: <p>The MultiCoNER II task aims to detect complex, ambiguous, and fine-grained
named entities in low-context situations and noisy scenarios like the presence
of spelling mistakes and typos for multiple languages. The task poses
significant challenges due to the scarcity of contextual information, the high
granularity of the entities(up to 33 classes), and the interference of noisy
data. To address these issues, our team {\bf PAI} proposes a universal Named
Entity Recognition (NER) system that integrates external entity information to
improve performance. Specifically, our system retrieves entities with
properties from the knowledge base (i.e. Wikipedia) for a given text, then
concatenates entity information with the input sentence and feeds it into
Transformer-based models. Finally, our system wins 2 first places, 4 second
places, and 1 third place out of 13 tracks. The code is publicly available at
\url{https://github.com/diqiuzhuanzhuan/semeval-2023}.
</p></li>
</ul>

<h3>Title: Multi-Task End-to-End Training Improves Conversational Recommendation. (arXiv:2305.06218v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06218">http://arxiv.org/abs/2305.06218</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06218] Multi-Task End-to-End Training Improves Conversational Recommendation](http://arxiv.org/abs/2305.06218) #transformer</code></li>
<li>Summary: <p>In this paper, we analyze the performance of a multitask end-to-end
transformer model on the task of conversational recommendations, which aim to
provide recommendations based on a user's explicit preferences expressed in
dialogue. While previous works in this area adopt complex multi-component
approaches where the dialogue management and entity recommendation tasks are
handled by separate components, we show that a unified transformer model, based
on the T5 text-to-text transformer model, can perform competitively in both
recommending relevant items and generating conversation dialogue. We fine-tune
our model on the ReDIAL conversational movie recommendation dataset, and create
additional training tasks derived from MovieLens (such as the prediction of
movie attributes and related movies based on an input movie), in a multitask
learning setting. Using a series of probe studies, we demonstrate that the
learned knowledge in the additional tasks is transferred to the conversational
setting, where each task leads to a 9%-52% increase in its related probe score.
</p></li>
</ul>

<h3>Title: Inclusive FinTech Lending via Contrastive Learning and Domain Adaptation. (arXiv:2305.05827v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05827">http://arxiv.org/abs/2305.05827</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05827] Inclusive FinTech Lending via Contrastive Learning and Domain Adaptation](http://arxiv.org/abs/2305.05827) #transformer</code></li>
<li>Summary: <p>FinTech lending (e.g., micro-lending) has played a significant role in
facilitating financial inclusion. It has reduced processing times and costs,
enhanced the user experience, and made it possible for people to obtain loans
who may not have qualified for credit from traditional lenders. However, there
are concerns about the potentially biased algorithmic decision-making during
loan screening. Machine learning algorithms used to evaluate credit quality can
be influenced by representation bias in the training data, as we only have
access to the default outcome labels of approved loan applications, for which
the borrowers' socioeconomic characteristics are better than those of rejected
ones. In this case, the model trained on the labeled data performs well on the
historically approved population, but does not generalize well to borrowers of
low socioeconomic background. In this paper, we investigate the problem of
representation bias in loan screening for a real-world FinTech lending
platform. We propose a new Transformer-based sequential loan screening model
with self-supervised contrastive learning and domain adaptation to tackle this
challenging issue. We use contrastive learning to train our feature extractor
on unapproved (unlabeled) loan applications and use domain adaptation to
generalize the performance of our label predictor. We demonstrate the
effectiveness of our model through extensive experimentation in the real-world
micro-lending setting. Our results show that our model significantly promotes
the inclusiveness of funding decisions, while also improving loan screening
accuracy and profit by 7.10% and 8.95%, respectively. We also show that
incorporating the test data into contrastive learning and domain adaptation and
labeling a small ratio of test data can further boost model performance.
</p></li>
</ul>

<h3>Title: XTab: Cross-table Pretraining for Tabular Transformers. (arXiv:2305.06090v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06090">http://arxiv.org/abs/2305.06090</a></li>
<li>Code URL: <a href="https://github.com/bingzhaozhu/xtab">https://github.com/bingzhaozhu/xtab</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06090] XTab: Cross-table Pretraining for Tabular Transformers](http://arxiv.org/abs/2305.06090) #transformer</code></li>
<li>Summary: <p>The success of self-supervised learning in computer vision and natural
language processing has motivated pretraining methods on tabular data. However,
most existing tabular self-supervised learning models fail to leverage
information across multiple data tables and cannot generalize to new tables. In
this work, we introduce XTab, a framework for cross-table pretraining of
tabular transformers on datasets from various domains. We address the challenge
of inconsistent column types and quantities among tables by utilizing
independent featurizers and using federated learning to pretrain the shared
component. Tested on 84 tabular prediction tasks from the OpenML-AutoML
Benchmark (AMLB), we show that (1) XTab consistently boosts the
generalizability, learning speed, and performance of multiple tabular
transformers, (2) by pretraining FT-Transformer via XTab, we achieve superior
performance than other state-of-the-art tabular deep learning models on various
tasks such as regression, binary, and multiclass classification.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Generative Steganographic Flow. (arXiv:2305.05838v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05838">http://arxiv.org/abs/2305.05838</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05838] Generative Steganographic Flow](http://arxiv.org/abs/2305.05838) #generative</code></li>
<li>Summary: <p>Generative steganography (GS) is a new data hiding manner, featuring direct
generation of stego media from secret data. Existing GS methods are generally
criticized for their poor performances. In this paper, we propose a novel flow
based GS approach -- Generative Steganographic Flow (GSF), which provides
direct generation of stego images without cover image. We take the stego image
generation and secret data recovery process as an invertible transformation,
and build a reversible bijective mapping between input secret data and
generated stego images. In the forward mapping, secret data is hidden in the
input latent of Glow model to generate stego images. By reversing the mapping,
hidden data can be extracted exactly from generated stego images. Furthermore,
we propose a novel latent optimization strategy to improve the fidelity of
stego images. Experimental results show our proposed GSF has far better
performances than SOTA works.
</p></li>
</ul>

<h3>Title: A Hybrid of Generative and Discriminative Models Based on the Gaussian-coupled Softmax Layer. (arXiv:2305.05912v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05912">http://arxiv.org/abs/2305.05912</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05912] A Hybrid of Generative and Discriminative Models Based on the Gaussian-coupled Softmax Layer](http://arxiv.org/abs/2305.05912) #generative</code></li>
<li>Summary: <p>Generative models have advantageous characteristics for classification tasks
such as the availability of unsupervised data and calibrated confidence,
whereas discriminative models have advantages in terms of the simplicity of
their model structures and learning algorithms and their ability to outperform
their generative counterparts. In this paper, we propose a method to train a
hybrid of discriminative and generative models in a single neural network (NN),
which exhibits the characteristics of both models. The key idea is the
Gaussian-coupled softmax layer, which is a fully connected layer with a softmax
activation function coupled with Gaussian distributions. This layer can be
embedded into an NN-based classifier and allows the classifier to estimate both
the class posterior distribution and the class-conditional data distribution.
We demonstrate that the proposed hybrid model can be applied to semi-supervised
learning and confidence calibration.
</p></li>
</ul>

<h3>Title: Post-training Model Quantization Using GANs for Synthetic Data Generation. (arXiv:2305.06052v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06052">http://arxiv.org/abs/2305.06052</a></li>
<li>Code URL: <a href="https://github.com/thanosm97/gsoc2022-openvino">https://github.com/thanosm97/gsoc2022-openvino</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06052] Post-training Model Quantization Using GANs for Synthetic Data Generation](http://arxiv.org/abs/2305.06052) #generative</code></li>
<li>Summary: <p>Quantization is a widely adopted technique for deep neural networks to reduce
the memory and computational resources required. However, when quantized, most
models would need a suitable calibration process to keep their performance
intact, which requires data from the target domain, such as a fraction of the
dataset used in model training and model validation (i.e. calibration dataset).
</p></li>
</ul>

<p>In this study, we investigate the use of synthetic data as a substitute for
the calibration with real data for the quantization method. We propose a data
generation method based on Generative Adversarial Networks that are trained
prior to the model quantization step. We compare the performance of models
quantized using data generated by StyleGAN2-ADA and our pre-trained DiStyleGAN,
with quantization using real data and an alternative data generation method
based on fractal images. Overall, the results of our experiments demonstrate
the potential of leveraging synthetic data for calibration during the
quantization process. In our experiments, the percentage of accuracy
degradation of the selected models was less than 0.6%, with our best
performance achieved on MobileNetV2 (0.05%). The code is available at:
https://github.com/ThanosM97/gsoc2022-openvino
</p>

<h3>Title: Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era. (arXiv:2305.06131v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06131">http://arxiv.org/abs/2305.06131</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06131] Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era](http://arxiv.org/abs/2305.06131) #generative</code></li>
<li>Summary: <p>Generative AI (AIGC, a.k.a. AI generated content) has made remarkable
progress in the past few years, among which text-guided content generation is
the most practical one since it enables the interaction between human
instruction and AIGC. Due to the development in text-to-image as well 3D
modeling technologies (like NeRF), text-to-3D has become a newly emerging yet
highly active research field. Our work conducts the first yet comprehensive
survey on text-to-3D to help readers interested in this direction quickly catch
up with its fast development. First, we introduce 3D data representations,
including both Euclidean data and non-Euclidean data. On top of that, we
introduce various foundation technologies as well as summarize how recent works
combine those foundation technologies to realize satisfactory text-to-3D.
Moreover, we summarize how text-to-3D technology is used in various
applications, including avatar generation, texture generation, shape
transformation, and scene generation.
</p></li>
</ul>

<h3>Title: DaGAN++: Depth-Aware Generative Adversarial Network for Talking Head Video Generation. (arXiv:2305.06225v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06225">http://arxiv.org/abs/2305.06225</a></li>
<li>Code URL: <a href="https://github.com/harlanhong/cvpr2022-dagan">https://github.com/harlanhong/cvpr2022-dagan</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06225] DaGAN++: Depth-Aware Generative Adversarial Network for Talking Head Video Generation](http://arxiv.org/abs/2305.06225) #generative</code></li>
<li>Summary: <p>Predominant techniques on talking head generation largely depend on 2D
information, including facial appearances and motions from input face images.
Nevertheless, dense 3D facial geometry, such as pixel-wise depth, plays a
critical role in constructing accurate 3D facial structures and suppressing
complex background noises for generation. However, dense 3D annotations for
facial videos is prohibitively costly to obtain. In this work, firstly, we
present a novel self-supervised method for learning dense 3D facial geometry
(ie, depth) from face videos, without requiring camera parameters and 3D
geometry annotations in training. We further propose a strategy to learn
pixel-level uncertainties to perceive more reliable rigid-motion pixels for
geometry learning. Secondly, we design an effective geometry-guided facial
keypoint estimation module, providing accurate keypoints for generating motion
fields. Lastly, we develop a 3D-aware cross-modal (ie, appearance and depth)
attention mechanism, which can be applied to each generation layer, to capture
facial geometries in a coarse-to-fine manner. Extensive experiments are
conducted on three challenging benchmarks (ie, VoxCeleb1, VoxCeleb2, and HDTF).
The results demonstrate that our proposed framework can generate highly
realistic-looking reenacted talking videos, with new state-of-the-art
performances established on these benchmarks. The codes and trained models are
publicly available on the GitHub project page at
https://github.com/harlanhong/CVPR2022-DaGAN
</p></li>
</ul>

<h3>Title: Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks. (arXiv:2305.05862v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05862">http://arxiv.org/abs/2305.05862</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05862] Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks](http://arxiv.org/abs/2305.05862) #generative</code></li>
<li>Summary: <p>The most recent large language models such as ChatGPT and GPT-4 have garnered
significant attention, as they are capable of generating high-quality responses
to human input. Despite the extensive testing of ChatGPT and GPT-4 on generic
text corpora, showcasing their impressive capabilities, a study focusing on
financial corpora has not been conducted. In this study, we aim to bridge this
gap by examining the potential of ChatGPT and GPT-4 as a solver for typical
financial text analytic problems in the zero-shot or few-shot setting.
Specifically, we assess their capabilities on four representative tasks over
five distinct financial textual datasets. The preliminary study shows that
ChatGPT and GPT-4 struggle on tasks such as financial named entity recognition
(NER) and sentiment analysis, where domain-specific knowledge is required,
while they excel in numerical reasoning tasks. We report both the strengths and
limitations of the current versions of ChatGPT and GPT-4, comparing them to the
state-of-the-art finetuned models as well as pretrained domain-specific
generative models. Our experiments provide qualitative studies, through which
we hope to help understand the capability of the existing models and facilitate
further improvements.
</p></li>
</ul>

<h3>Title: CQSumDP: A ChatGPT-Annotated Resource for Query-Focused Abstractive Summarization Based on Debatepedia. (arXiv:2305.06147v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06147">http://arxiv.org/abs/2305.06147</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06147] CQSumDP: A ChatGPT-Annotated Resource for Query-Focused Abstractive Summarization Based on Debatepedia](http://arxiv.org/abs/2305.06147) #generative</code></li>
<li>Summary: <p>Debatepedia is a publicly available dataset consisting of arguments and
counter-arguments on controversial topics that has been widely used for the
single-document query-focused abstractive summarization task in recent years.
However, it has been recently found that this dataset is limited by noise and
even most queries in this dataset do not have any relevance to the respective
document. In this paper, we present a methodology for cleaning the Debatepedia
dataset by leveraging the generative power of large language models to make it
suitable for query-focused abstractive summarization. More specifically, we
harness the language generation capabilities of ChatGPT to regenerate its
queries. We evaluate the effectiveness of the proposed ChatGPT annotated
version of the Debatepedia dataset using several benchmark summarization models
and demonstrate that the newly annotated version of Debatepedia outperforms the
original dataset in terms of both query relevance as well as summary generation
quality. We will make this annotated and cleaned version of the dataset
publicly available.
</p></li>
</ul>

<h3>Title: Fine-tuning Language Models with Generative Adversarial Feedback. (arXiv:2305.06176v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06176">http://arxiv.org/abs/2305.06176</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06176] Fine-tuning Language Models with Generative Adversarial Feedback](http://arxiv.org/abs/2305.06176) #generative</code></li>
<li>Summary: <p>Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to
significantly enhance the performance of large language models (LLMs) by
aligning their outputs with desired human values. However, RLHF is constrained
by the expertise and productivity limitations of human evaluators. In this
study, we investigate an alternative approach: Reinforcement Learning with
Generative Adversarial Feedback (RLGAF) to RLHF. Our preliminary findings
indicate that RLGAF can help align LLMs outputs while not suffering from the
inherent restrictions of RLHF, suggesting promising avenues for further
research on automating AI alignment.
</p></li>
</ul>

<h3>Title: Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files. (arXiv:2305.05708v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05708">http://arxiv.org/abs/2305.05708</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05708] Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files](http://arxiv.org/abs/2305.05708) #generative</code></li>
<li>Summary: <p>Language models are powerful tools for molecular design. Currently, the
dominant paradigm is to parse molecular graphs into linear string
representations that can easily be trained on. This approach has been very
successful, however, it is limited to chemical structures that can be
completely represented by a graph -- like organic molecules -- while materials
and biomolecular structures like protein binding sites require a more complete
representation that includes the relative positioning of their atoms in space.
In this work, we show how language models, without any architecture
modifications, trained using next-token prediction -- can generate novel and
valid structures in three dimensions from various substantially different
distributions of chemical structures. In particular, we demonstrate that
language models trained directly on sequences derived directly from chemical
file formats like XYZ files, Crystallographic Information files (CIFs), or
Protein Data Bank files (PDBs) can directly generate molecules, crystals, and
protein binding sites in three dimensions. Furthermore, despite being trained
on chemical file sequences -- language models still achieve performance
comparable to state-of-the-art models that use graph and graph-derived string
representations, as well as other domain-specific 3D generative models. In
doing so, we demonstrate that it is not necessary to use simplified molecular
representations to train chemical language models -- that they are powerful
generative models capable of directly exploring chemical space in three
dimensions for very different structures.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Vision-Language Models in Remote Sensing: Current Progress and Future Trends. (arXiv:2305.05726v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05726">http://arxiv.org/abs/2305.05726</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05726] Vision-Language Models in Remote Sensing: Current Progress and Future Trends](http://arxiv.org/abs/2305.05726) #large language model</code></li>
<li>Summary: <p>The remarkable achievements of ChatGPT and GPT-4 have sparked a wave of
interest and research in the field of large language models for Artificial
General Intelligence (AGI). These models provide us with intelligent solutions
that are more similar to human thinking, enabling us to use general artificial
intelligence to solve problems in various applications. However, in the field
of remote sensing, the scientific literature on the implementation of AGI
remains relatively scant. Existing AI-related research primarily focuses on
visual understanding tasks while neglecting the semantic understanding of the
objects and their relationships. This is where vision-language models excel, as
they enable reasoning about images and their associated textual descriptions,
allowing for a deeper understanding of the underlying semantics.
Vision-language models can go beyond recognizing the objects in an image and
can infer the relationships between them, as well as generate natural language
descriptions of the image. This makes them better suited for tasks that require
both visual and textual understanding, such as image captioning, text-based
image retrieval, and visual question answering. This paper provides a
comprehensive review of the research on vision-language models in remote
sensing, summarizing the latest progress, highlighting the current challenges,
and identifying potential research opportunities. Specifically, we review the
application of vision-language models in several mainstream remote sensing
tasks, including image captioning, text-based image generation, text-based
image retrieval, visual question answering, scene classification, semantic
segmentation, and object detection. For each task, we briefly describe the task
background and review some representative works. Finally, we summarize the
limitations of existing work and provide some possible directions for future
development.
</p></li>
</ul>

<h3>Title: VideoChat: Chat-Centric Video Understanding. (arXiv:2305.06355v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06355">http://arxiv.org/abs/2305.06355</a></li>
<li>Code URL: <a href="https://github.com/opengvlab/ask-anything">https://github.com/opengvlab/ask-anything</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06355] VideoChat: Chat-Centric Video Understanding](http://arxiv.org/abs/2305.06355) #large language model</code></li>
<li>Summary: <p>In this study, we initiate an exploration into video understanding by
introducing VideoChat, an end-to-end chat-centric video understanding system.
It integrates video foundation models and large language models via a learnable
neural interface, excelling in spatiotemporal reasoning, event localization,
and causal relationship inference. To instructively tune this system, we
propose a video-centric instruction dataset, composed of thousands of videos
matched with detailed descriptions and conversations. This dataset emphasizes
spatiotemporal reasoning and causal relationships, providing a valuable asset
for training chat-centric video understanding systems. Preliminary qualitative
experiments reveal our system's potential across a broad spectrum of video
applications and set the standard for future research. Access our code and data
at https://github.com/OpenGVLab/Ask-Anything
</p></li>
</ul>

<h3>Title: Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment. (arXiv:2305.05940v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05940">http://arxiv.org/abs/2305.05940</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05940] Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment](http://arxiv.org/abs/2305.05940) #large language model</code></li>
<li>Summary: <p>In-context learning (ICL) unfolds as large language models become capable of
inferring test labels conditioned on a few labeled samples without any gradient
update. ICL-enabled large language models provide a promising step forward
toward bypassing recurrent annotation costs in a low-resource setting. Yet,
only a handful of past studies have explored ICL in a cross-lingual setting, in
which the need for transferring label-knowledge from a high-resource language
to a low-resource one is immensely crucial. To bridge the gap, we provide the
first in-depth analysis of ICL for cross-lingual text classification. We find
that the prevalent mode of selecting random input-label pairs to construct the
prompt-context is severely limited in the case of cross-lingual ICL, primarily
due to the lack of alignment in the input as well as the output spaces. To
mitigate this, we propose a novel prompt construction strategy -- Cross-lingual
In-context Source-Target Alignment (X-InSTA). With an injected coherence in the
semantics of the input examples and a task-based alignment across the source
and target languages, X-InSTA is able to outperform random prompt selection by
a large margin across three different tasks using 44 different cross-lingual
pairs.
</p></li>
</ul>

<h3>Title: Adapter-TST: A Parameter Efficient Method for Multiple-Attribute Text Style Transfer. (arXiv:2305.05945v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05945">http://arxiv.org/abs/2305.05945</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05945] Adapter-TST: A Parameter Efficient Method for Multiple-Attribute Text Style Transfer](http://arxiv.org/abs/2305.05945) #large language model</code></li>
<li>Summary: <p>Adapting a large language model for multiple-attribute text style transfer
via fine-tuning can be challenging due to the significant amount of
computational resources and labeled data required for the specific task. In
this paper, we address this challenge by introducing AdapterTST, a framework
that freezes the pre-trained model's original parameters and enables the
development of a multiple-attribute text style transfer model. Using BART as
the backbone model, Adapter-TST utilizes different neural adapters to capture
different attribute information, like a plug-in connected to BART. Our method
allows control over multiple attributes, like sentiment, tense, voice, etc.,
and configures the adapters' architecture to generate multiple outputs
respected to attributes or compositional editing on the same sentence. We
evaluate the proposed model on both traditional sentiment transfer and
multiple-attribute transfer tasks. The experiment results demonstrate that
Adapter-TST outperforms all the state-of-the-art baselines with significantly
lesser computational resources. We have also empirically shown that each
adapter is able to capture specific stylistic attributes effectively and can be
configured to perform compositional editing.
</p></li>
</ul>

<h3>Title: Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge. (arXiv:2305.05976v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05976">http://arxiv.org/abs/2305.05976</a></li>
<li>Code URL: <a href="https://github.com/jiangjiechen/uncommongen">https://github.com/jiangjiechen/uncommongen</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05976] Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge](http://arxiv.org/abs/2305.05976) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have been widely studied for their ability to
store and utilize positive knowledge. However, negative knowledge, such as
"lions don't live in the ocean", is also ubiquitous in the world but rarely
mentioned explicitly in the text. What do LLMs know about negative knowledge?
This work examines the ability of LLMs to negative commonsense knowledge. We
design a constrained keywords-to-sentence generation task (CG) and a Boolean
question-answering task (QA) to probe LLMs. Our experiments reveal that LLMs
frequently fail to generate valid sentences grounded in negative commonsense
knowledge, yet they can correctly answer polar yes-or-no questions. We term
this phenomenon the belief conflict of LLMs. Our further analysis shows that
statistical shortcuts and negation reporting bias from language modeling
pre-training cause this conflict.
</p></li>
</ul>

<h3>Title: Generating medically-accurate summaries of patient-provider dialogue: A multi-stage approach using large language models. (arXiv:2305.05982v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05982">http://arxiv.org/abs/2305.05982</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05982] Generating medically-accurate summaries of patient-provider dialogue: A multi-stage approach using large language models](http://arxiv.org/abs/2305.05982) #large language model</code></li>
<li>Summary: <p>A medical provider's summary of a patient visit serves several critical
purposes, including clinical decision-making, facilitating hand-offs between
providers, and as a reference for the patient. An effective summary is required
to be coherent and accurately capture all the medically relevant information in
the dialogue, despite the complexity of patient-generated language. Even minor
inaccuracies in visit summaries (for example, summarizing "patient does not
have a fever" when a fever is present) can be detrimental to the outcome of
care for the patient.
</p></li>
</ul>

<p>This paper tackles the problem of medical conversation summarization by
discretizing the task into several smaller dialogue-understanding tasks that
are sequentially built upon. First, we identify medical entities and their
affirmations within the conversation to serve as building blocks. We study
dynamically constructing few-shot prompts for tasks by conditioning on relevant
patient information and use GPT-3 as the backbone for our experiments. We also
develop GPT-derived summarization metrics to measure performance against
reference summaries quantitatively. Both our human evaluation study and metrics
for medical correctness show that summaries generated using this approach are
clinically accurate and outperform the baseline approach of summarizing the
dialog in a zero-shot, single-prompt setting.
</p>

<h3>Title: The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation. (arXiv:2305.06156v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06156">http://arxiv.org/abs/2305.06156</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06156] The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation](http://arxiv.org/abs/2305.06156) #large language model</code></li>
<li>Summary: <p>We present The Vault, an open-source, large-scale code-text dataset designed
to enhance the training of code-focused large language models (LLMs). Existing
open-source datasets for training code-based LLMs often face challenges in
terms of size, quality (due to noisy signals), and format (only containing code
function and text explanation pairings). The Vault overcomes these limitations
by providing 40 million code-text pairs across 10 popular programming
languages, thorough cleaning for 10+ prevalent issues, and various levels of
code-text pairings, including class, function, and line levels. Researchers and
practitioners can utilize The Vault for training diverse code-focused LLMs or
incorporate the provided data cleaning methods and scripts to improve their
datasets. By employing The Vault as the training dataset for code-centric LLMs,
we anticipate significant advancements in code understanding and generation
tasks, fostering progress in both artificial intelligence research and software
development practices.
</p></li>
</ul>

<h3>Title: StarCoder: may the source be with you!. (arXiv:2305.06161v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06161">http://arxiv.org/abs/2305.06161</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06161] StarCoder: may the source be with you!](http://arxiv.org/abs/2305.06161) #large language model</code></li>
<li>Summary: <p>The BigCode community, an open-scientific collaboration working on the
responsible development of Large Language Models for Code (Code LLMs),
introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context
length, infilling capabilities and fast large-batch inference enabled by
multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced
from The Stack, a large collection of permissively licensed GitHub repositories
with inspection tools and an opt-out process. We fine-tuned StarCoderBase on
35B Python tokens, resulting in the creation of StarCoder. We perform the most
comprehensive evaluation of Code LLMs to date and show that StarCoderBase
outperforms every open Code LLM that supports multiple programming languages
and matches or outperforms the OpenAI code-cushman-001 model. Furthermore,
StarCoder outperforms every model that is fine-tuned on Python, can be prompted
to achieve 40\% pass@1 on HumanEval, and still retains its performance on other
programming languages. We take several important steps towards a safe
open-access model release, including an improved PII redaction pipeline and a
novel attribution tracing tool, and make the StarCoder models publicly
available under a more commercially viable version of the Open Responsible AI
Model license.
</p></li>
</ul>

<h3>Title: Algebra Error Classification with Large Language Models. (arXiv:2305.06163v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06163">http://arxiv.org/abs/2305.06163</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06163] Algebra Error Classification with Large Language Models](http://arxiv.org/abs/2305.06163) #large language model</code></li>
<li>Summary: <p>Automated feedback as students answer open-ended math questions has
significant potential in improving learning outcomes at large scale. A key part
of automated feedback systems is an error classification component, which
identifies student errors and enables appropriate, predefined feedback to be
deployed. Most existing approaches to error classification use a rule-based
method, which has limited capacity to generalize. Existing data-driven methods
avoid these limitations but specifically require mathematical expressions in
student responses to be parsed into syntax trees. This requirement is itself a
limitation, since student responses are not always syntactically valid and
cannot be converted into trees. In this work, we introduce a flexible method
for error classification using pre-trained large language models. We
demonstrate that our method can outperform existing methods in algebra error
classification, and is able to classify a larger set of student responses.
Additionally, we analyze common classification errors made by our method and
discuss limitations of automated error classification.
</p></li>
</ul>

<h3>Title: Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success). (arXiv:2305.06299v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06299">http://arxiv.org/abs/2305.06299</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06299] Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)](http://arxiv.org/abs/2305.06299) #large language model</code></li>
<li>Summary: <p>Large language models, particularly GPT-3, are able to produce high quality
summaries of general domain news articles in few- and zero-shot settings.
However, it is unclear if such models are similarly capable in more
specialized, high-stakes domains such as biomedicine. In this paper, we enlist
domain experts (individuals with medical training) to evaluate summaries of
biomedical articles generated by GPT-3, given zero supervision. We consider
both single- and multi-document settings. In the former, GPT-3 is tasked with
generating regular and plain-language summaries of articles describing
randomized controlled trials; in the latter, we assess the degree to which
GPT-3 is able to \emph{synthesize} evidence reported across a collection of
articles. We design an annotation scheme for evaluating model outputs, with an
emphasis on assessing the factual accuracy of generated summaries. We find that
while GPT-3 is able to summarize and simplify single biomedical articles
faithfully, it struggles to provide accurate aggregations of findings over
multiple documents. We release all data and annotations used in this work.
</p></li>
</ul>

<h3>Title: Automatic Evaluation of Attribution by Large Language Models. (arXiv:2305.06311v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06311">http://arxiv.org/abs/2305.06311</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06311] Automatic Evaluation of Attribution by Large Language Models](http://arxiv.org/abs/2305.06311) #large language model</code></li>
<li>Summary: <p>A recent focus of large language model (LLM) development, as exemplified by
generative search engines, is to incorporate external references to generate
and support their claims. However, evaluating the attribution, i.e., verifying
whether the generated statement is indeed fully supported by the cited
reference, remains an open problem. Although human evaluation is common
practice, it is costly and time-consuming. In this paper, we investigate the
automatic evaluation of attribution by LLMs. We begin by providing a definition
of attribution and then explore two approaches for automatic evaluation:
prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed
from related tasks, such as question answering, fact-checking, natural language
inference, and summarization. To facilitate the evaluation, we manually curate
a set of test examples covering 12 domains from a generative search engine, New
Bing. Our results on the curated test set and simulated test examples from
existing benchmark questions highlight both promising signals as well as
remaining challenges for the automatic evaluation of attribution. We hope our
testbed, modeling methodology, and insights will help lay the foundation for
future studies on this important problem.
</p></li>
</ul>

<h3>Title: Fast Distributed Inference Serving for Large Language Models. (arXiv:2305.05920v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05920">http://arxiv.org/abs/2305.05920</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05920] Fast Distributed Inference Serving for Large Language Models](http://arxiv.org/abs/2305.05920) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) power a new generation of interactive AI
applications exemplified by ChatGPT. The interactive nature of these
applications demand low job completion time (JCT) for model inference. Existing
LLM serving systems use run-to-completion processing for inference jobs, which
suffers from head-of-line blocking and long JCT. We present FastServe, a
distributed inference serving system for LLMs. FastServe exploits the
autoregressive pattern of LLM inference to enable preemption at the granularity
of each output token. FastServe uses preemptive scheduling to minimize JCT with
a novel skip-join Multi-Level Feedback Queue scheduler. Based on the new semi
information-agnostic setting of LLM inference, the scheduler leverages the
input length information to assign an appropriate initial queue for each
arrival job to join. The higher priority queues than the joined queue are
skipped to reduce demotions. We design an efficient GPU memory management
mechanism that proactively offloads and uploads intermediate states between GPU
memory and host memory for LLM inference. We build a system prototype of
FastServe based on NVIDIA FasterTransformer. Experimental results show that
compared to the state-of-the-art solution Orca, FastServe improves the average
and tail JCT by up to 5.1$\times$ and 6.4$\times$, respectively.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Unsupervised Domain Adaptation for Semantic Segmentation via Feature-space Density Matching. (arXiv:2305.05789v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05789">http://arxiv.org/abs/2305.05789</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05789] Unsupervised Domain Adaptation for Semantic Segmentation via Feature-space Density Matching](http://arxiv.org/abs/2305.05789) #segmentation</code></li>
<li>Summary: <p>Semantic segmentation is a critical step in automated image interpretation
and analysis where pixels are classified into one or more predefined
semantically meaningful classes. Deep learning approaches for semantic
segmentation rely on harnessing the power of annotated images to learn features
indicative of these semantic classes. Nonetheless, they often fail to
generalize when there is a significant domain (i.e., distributional) shift
between the training (i.e., source) data and the dataset(s) encountered when
deployed (i.e., target), necessitating manual annotations for the target data
to achieve acceptable performance. This is especially important in medical
imaging because different image modalities have significant intra- and
inter-site variations due to protocol and vendor variability. Current
techniques are sensitive to hyperparameter tuning and target dataset size. This
paper presents an unsupervised domain adaptation approach for semantic
segmentation that alleviates the need for annotating target data. Using kernel
density estimation, we match the target data distribution to the source data in
the feature space. We demonstrate that our results are comparable or superior
on multiple-site prostate MRI and histopathology images, which mitigates the
need for annotating target data.
</p></li>
</ul>

<h3>Title: Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation. (arXiv:2305.05803v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05803">http://arxiv.org/abs/2305.05803</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05803] Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation](http://arxiv.org/abs/2305.05803) #segmentation</code></li>
<li>Summary: <p>Weakly Supervised Semantic Segmentation (WSSS) with only image-level
supervision has garnered increasing attention due to its low annotation cost
compared to pixel-level annotation. Most existing methods rely on Class
Activation Maps (CAM) to generate pixel-level pseudo labels for supervised
training. However, it is well known that CAM often suffers from partial
activation -- activating the most discriminative part instead of the entire
object area, and false activation -- unnecessarily activating the background
around the object. In this study, we introduce a simple yet effective approach
to address these limitations by harnessing the recently released Segment
Anything Model (SAM) to generate higher-quality pseudo labels with CAM. SAM is
a segmentation foundation model that demonstrates strong zero-shot ability in
partitioning images into segments but lacks semantic labels for these regions.
To circumvent this, we employ pseudo labels for a specific class as the signal
to select the most relevant masks and label them to generate the refined pseudo
labels for this class. The segments generated by SAM are highly precise,
leading to substantial improvements in partial and false activation. Moreover,
existing post-processing modules for producing pseudo labels, such as
AffinityNet, are often computationally heavy, with a significantly long
training time. Surprisingly, we discovered that using the initial CAM with SAM
can achieve on-par performance as the post-processed pseudo label generated
from these modules with much less computational cost. Our approach is highly
versatile and capable of seamless integration into existing WSSS models without
modification to base networks or pipelines. Despite its simplicity, our
approach improves the mean Intersection over Union (mIoU) of pseudo labels from
five state-of-the-art WSSS methods by 6.2\% on average on the PASCAL VOC 2012
dataset.
</p></li>
</ul>

<h3>Title: A Self-Training Framework Based on Multi-Scale Attention Fusion for Weakly Supervised Semantic Segmentation. (arXiv:2305.05841v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05841">http://arxiv.org/abs/2305.05841</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05841] A Self-Training Framework Based on Multi-Scale Attention Fusion for Weakly Supervised Semantic Segmentation](http://arxiv.org/abs/2305.05841) #segmentation</code></li>
<li>Summary: <p>Weakly supervised semantic segmentation (WSSS) based on image-level labels is
challenging since it is hard to obtain complete semantic regions. To address
this issue, we propose a self-training method that utilizes fused multi-scale
class-aware attention maps. Our observation is that attention maps of different
scales contain rich complementary information, especially for large and small
objects. Therefore, we collect information from attention maps of different
scales and obtain multi-scale attention maps. We then apply denoising and
reactivation strategies to enhance the potential regions and reduce noisy
areas. Finally, we use the refined attention maps to retrain the network.
Experiments showthat our method enables the model to extract rich semantic
information from multi-scale images and achieves 72.4% mIou scores on both the
PASCAL VOC 2012 validation and test sets. The code is available at
https://bupt-ai-cz.github.io/SMAF.
</p></li>
</ul>

<h3>Title: Medical supervised masked autoencoders: Crafting a better masking strategy and efficient fine-tuning schedule for medical image classification. (arXiv:2305.05871v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.05871">http://arxiv.org/abs/2305.05871</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.05871] Medical supervised masked autoencoders: Crafting a better masking strategy and efficient fine-tuning schedule for medical image classification](http://arxiv.org/abs/2305.05871) #segmentation</code></li>
<li>Summary: <p>Masked autoencoders (MAEs) have displayed significant potential in the
classification and semantic segmentation of medical images in the last year.
Due to the high similarity of human tissues, even slight changes in medical
images may represent diseased tissues, necessitating fine-grained inspection to
pinpoint diseased tissues. The random masking strategy of MAEs is likely to
result in areas of lesions being overlooked by the model. At the same time,
inconsistencies between the pre-training and fine-tuning phases impede the
performance and efficiency of MAE in medical image classification. To address
these issues, we propose a medical supervised masked autoencoder (MSMAE) in
this paper. In the pre-training phase, MSMAE precisely masks medical images via
the attention maps obtained from supervised training, contributing to the
representation learning of human tissue in the lesion area. During the
fine-tuning phase, MSMAE is also driven by attention to the accurate masking of
medical images. This improves the computational efficiency of the MSMAE while
increasing the difficulty of fine-tuning, which indirectly improves the quality
of MSMAE medical diagnosis. Extensive experiments demonstrate that MSMAE
achieves state-of-the-art performance in case with three official medical
datasets for various diseases. Meanwhile, transfer learning for MSMAE also
demonstrates the great potential of our approach for medical semantic
segmentation tasks. Moreover, the MSMAE accelerates the inference time in the
fine-tuning phase by 11.2% and reduces the number of floating-point operations
(FLOPs) by 74.08% compared to a traditional MAE.
</p></li>
</ul>

<h3>Title: Radious: Unveiling the Enigma of Dental Radiology with BEIT Adaptor and Mask2Former in Semantic Segmentation. (arXiv:2305.06236v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06236">http://arxiv.org/abs/2305.06236</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06236] Radious: Unveiling the Enigma of Dental Radiology with BEIT Adaptor and Mask2Former in Semantic Segmentation](http://arxiv.org/abs/2305.06236) #segmentation</code></li>
<li>Summary: <p>X-ray images are the first steps for diagnosing and further treating dental
problems. So, early diagnosis prevents the development and increase of oral and
dental diseases. In this paper, we developed a semantic segmentation algorithm
based on BEIT adaptor and Mask2Former to detect and identify teeth, roots, and
multiple dental diseases and abnormalities such as pulp chamber, restoration,
endodontics, crown, decay, pin, composite, bridge, pulpitis, orthodontics,
radicular cyst, periapical cyst, cyst, implant, and bone graft material in
panoramic, periapical, and bitewing X-ray images. We compared the result of our
algorithm to two state-of-the-art algorithms in image segmentation named:
Deeplabv3 and Segformer on our own data set. We discovered that Radious
outperformed those algorithms by increasing the mIoU scores by 9% and 33% in
Deeplabv3+ and Segformer, respectively.
</p></li>
</ul>

<h3>Title: Self-Supervised Instance Segmentation by Grasping. (arXiv:2305.06305v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06305">http://arxiv.org/abs/2305.06305</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06305] Self-Supervised Instance Segmentation by Grasping](http://arxiv.org/abs/2305.06305) #segmentation</code></li>
<li>Summary: <p>Instance segmentation is a fundamental skill for many robotic applications.
We propose a self-supervised method that uses grasp interactions to collect
segmentation supervision for an instance segmentation model. When a robot
grasps an item, the mask of that grasped item can be inferred from the images
of the scene before and after the grasp. Leveraging this insight, we learn a
grasp segmentation model to segment the grasped object from before and after
grasp images. Such a model can segment grasped objects from thousands of grasp
interactions without costly human annotation. Using the segmented grasped
objects, we can "cut" objects from their original scenes and "paste" them into
new scenes to generate instance supervision. We show that our grasp
segmentation model provides a 5x error reduction when segmenting grasped
objects compared with traditional image subtraction approaches. Combined with
our "cut-and-paste" generation method, instance segmentation models trained
with our method achieve better performance than a model trained with 10x the
amount of labeled data. On a real robotic grasping system, our instance
segmentation model reduces the rate of grasp errors by over 3x compared to an
image subtraction baseline.
</p></li>
</ul>

<h3>Title: Scan2LoD3: Reconstructing semantic 3D building models at LoD3 using ray casting and Bayesian networks. (arXiv:2305.06314v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06314">http://arxiv.org/abs/2305.06314</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06314] Scan2LoD3: Reconstructing semantic 3D building models at LoD3 using ray casting and Bayesian networks](http://arxiv.org/abs/2305.06314) #segmentation</code></li>
<li>Summary: <p>Reconstructing semantic 3D building models at the level of detail (LoD) 3 is
a long-standing challenge. Unlike mesh-based models, they require watertight
geometry and object-wise semantics at the fa\c{c}ade level. The principal
challenge of such demanding semantic 3D reconstruction is reliable
fa\c{c}ade-level semantic segmentation of 3D input data. We present a novel
method, called Scan2LoD3, that accurately reconstructs semantic LoD3 building
models by improving fa\c{c}ade-level semantic 3D segmentation. To this end, we
leverage laser physics and 3D building model priors to probabilistically
identify model conflicts. These probabilistic physical conflicts propose
locations of model openings: Their final semantics and shapes are inferred in a
Bayesian network fusing multimodal probabilistic maps of conflicts, 3D point
clouds, and 2D images. To fulfill demanding LoD3 requirements, we use the
estimated shapes to cut openings in 3D building priors and fit semantic 3D
objects from a library of fa\c{c}ade objects. Extensive experiments on the TUM
city campus datasets demonstrate the superior performance of the proposed
Scan2LoD3 over the state-of-the-art methods in fa\c{c}ade-level detection,
semantic segmentation, and LoD3 building model reconstruction. We believe our
method can foster the development of probability-driven semantic 3D
reconstruction at LoD3 since not only the high-definition reconstruction but
also reconstruction confidence becomes pivotal for various applications such as
autonomous driving and urban simulations.
</p></li>
</ul>

<h3>Title: Korean Named Entity Recognition Based on Language-Specific Features. (arXiv:2305.06330v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2305.06330">http://arxiv.org/abs/2305.06330</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2305.06330] Korean Named Entity Recognition Based on Language-Specific Features](http://arxiv.org/abs/2305.06330) #segmentation</code></li>
<li>Summary: <p>In the paper, we propose a novel way of improving named entity recognition in
the Korean language using its language-specific features. While the field of
named entity recognition has been studied extensively in recent years, the
mechanism of efficiently recognizing named entities in Korean has hardly been
explored. This is because the Korean language has distinct linguistic
properties that prevent models from achieving their best performances.
Therefore, an annotation scheme for {Korean corpora} by adopting the CoNLL-U
format, which decomposes Korean words into morphemes and reduces the ambiguity
of named entities in the original segmentation that may contain functional
morphemes such as postpositions and particles, is proposed herein. We
investigate how the named entity tags are best represented in this
morpheme-based scheme and implement an algorithm to convert word-based {and
syllable-based Korean corpora} with named entities into the proposed
morpheme-based format. Analyses of the results of {statistical and neural}
models reveal that the proposed morpheme-based format is feasible, and the
{varied} performances of the models under the influence of various additional
language-specific features are demonstrated. Extrinsic conditions were also
considered to observe the variance of the performances of the proposed models,
given different types of data, including the original segmentation and
different types of tagging formats.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
