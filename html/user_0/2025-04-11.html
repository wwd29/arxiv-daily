<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-04-11</h1>
<h3>Title: EnDive: A Cross-Dialect Benchmark for Fairness and Performance in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Abhay Gupta, Jacob Cheung, Philip Meng, Shayan Sayyed, Austen Liao, Kevin Zhu, Sean O'Brien</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07100">https://arxiv.org/abs/2504.07100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07100">https://arxiv.org/pdf/2504.07100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07100]] EnDive: A Cross-Dialect Benchmark for Fairness and Performance in Large Language Models(https://arxiv.org/abs/2504.07100)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The diversity of human language, shaped by social, cultural, and regional influences, presents significant challenges for natural language processing (NLP) systems. Existing benchmarks often overlook intra-language variations, leaving speakers of non-standard dialects underserved. To address this gap, we introduce EnDive (English Diversity), a benchmark that evaluates five widely-used large language models (LLMs) across tasks in language understanding, algorithmic reasoning, mathematics, and logic. Our framework translates Standard American English datasets into five underrepresented dialects using few-shot prompting with verified examples from native speakers, and compare these translations against rule-based methods via fluency assessments, preference tests, and semantic similarity metrics. Human evaluations confirm high translation quality, with average scores of at least 6.02/7 for faithfulness, fluency, and formality. By filtering out near-identical translations, we create a challenging dataset that reveals significant performance disparities - models consistently underperform on dialectal inputs compared to Standard American English. EnDive thus advances dialect-aware NLP by uncovering model biases and promoting more equitable language technologies.</li>
</ul>

<h3>Title: How Robust Are Router-LLMs? Analysis of the Fragility of LLM Routing Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Aly M. Kassem, Bernhard Sch√∂lkopf, Zhijing Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07113">https://arxiv.org/abs/2504.07113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07113">https://arxiv.org/pdf/2504.07113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07113]] How Robust Are Router-LLMs? Analysis of the Fragility of LLM Routing Capabilities(https://arxiv.org/abs/2504.07113)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) routing has emerged as a crucial strategy for balancing computational costs with performance by dynamically assigning queries to the most appropriate model based on query complexity. Despite recent advances showing that preference-data-based routers can outperform traditional methods, current evaluation benchmarks remain limited. They largely focus on general model capabilities while overlooking task-specific behaviors and critical concerns such as privacy, safety, and potential backdoor vulnerabilities introduced through preference data. In response, we propose the DSC benchmark: Diverse, Simple, and Categorized, an evaluation framework that categorizes router performance across a broad spectrum of query types, including coding, translation, mathematics, human instructions, general knowledge, and LLM jailbreaking. Additionally, it integrates privacy and safety assessments to reveal hidden risks. Our experiments on three preference-based routers and two commercial counterparts demonstrate that while these systems improve efficiency, they often make suboptimal, category-driven decisions. For instance, a BERT-based router directs all coding and mathematics queries to the most powerful LLM even when simpler models would suffice, while routing jailbreaking attempts to weaker models, thereby elevating safety risks.</li>
</ul>

<h3>Title: EqualizeIR: Mitigating Linguistic Biases in Retrieval Models</h3>
<ul>
<li><strong>Authors: </strong>Jiali Cheng, Hadi Amiri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07115">https://arxiv.org/abs/2504.07115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07115">https://arxiv.org/pdf/2504.07115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07115]] EqualizeIR: Mitigating Linguistic Biases in Retrieval Models(https://arxiv.org/abs/2504.07115)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This study finds that existing information retrieval (IR) models show significant biases based on the linguistic complexity of input queries, performing well on linguistically simpler (or more complex) queries while underperforming on linguistically more complex (or simpler) queries. To address this issue, we propose EqualizeIR, a framework to mitigate linguistic biases in IR models. EqualizeIR uses a linguistically biased weak learner to capture linguistic biases in IR datasets and then trains a robust model by regularizing and refining its predictions using the biased weak learner. This approach effectively prevents the robust model from overfitting to specific linguistic patterns in data. We propose four approaches for developing linguistically-biased models. Extensive experiments on several datasets show that our method reduces performance disparities across linguistically simple and complex queries, while improving overall retrieval performance.</li>
</ul>

<h3>Title: SINCon: Mitigate LLM-Generated Malicious Message Injection Attack for Rumor Detection</h3>
<ul>
<li><strong>Authors: </strong>Mingqing Zhang, Qiang Liu, Xiang Tao, Shu Wu, Liang Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07135">https://arxiv.org/abs/2504.07135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07135">https://arxiv.org/pdf/2504.07135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07135]] SINCon: Mitigate LLM-Generated Malicious Message Injection Attack for Rumor Detection(https://arxiv.org/abs/2504.07135)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>In the era of rapidly evolving large language models (LLMs), state-of-the-art rumor detection systems, particularly those based on Message Propagation Trees (MPTs), which represent a conversation tree with the post as its root and the replies as its descendants, are facing increasing threats from adversarial attacks that leverage LLMs to generate and inject malicious messages. Existing methods are based on the assumption that different nodes exhibit varying degrees of influence on predictions. They define nodes with high predictive influence as important nodes and target them for attacks. If the model treats nodes' predictive influence more uniformly, attackers will find it harder to target high predictive influence nodes. In this paper, we propose Similarizing the predictive Influence of Nodes with Contrastive Learning (SINCon), a defense mechanism that encourages the model to learn graph representations where nodes with varying importance have a more uniform influence on predictions. Extensive experiments on the Twitter and Weibo datasets demonstrate that SINCon not only preserves high classification accuracy on clean data but also significantly enhances resistance against LLM-driven message injection attacks.</li>
</ul>

<h3>Title: Large Language Model (LLM) for Software Security: Code Analysis, Malware Analysis, Reverse Engineering</h3>
<ul>
<li><strong>Authors: </strong>Hamed Jelodar, Samita Bai, Parisa Hamedi, Hesamodin Mohammadian, Roozbeh Razavi-Far, Ali Ghorbani</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07137">https://arxiv.org/abs/2504.07137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07137">https://arxiv.org/pdf/2504.07137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07137]] Large Language Model (LLM) for Software Security: Code Analysis, Malware Analysis, Reverse Engineering(https://arxiv.org/abs/2504.07137)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have recently emerged as powerful tools in cybersecurity, offering advanced capabilities in malware detection, generation, and real-time monitoring. Numerous studies have explored their application in cybersecurity, demonstrating their effectiveness in identifying novel malware variants, analyzing malicious code structures, and enhancing automated threat analysis. Several transformer-based architectures and LLM-driven models have been proposed to improve malware analysis, leveraging semantic and structural insights to recognize malicious intent more accurately. This study presents a comprehensive review of LLM-based approaches in malware code analysis, summarizing recent advancements, trends, and methodologies. We examine notable scholarly works to map the research landscape, identify key challenges, and highlight emerging innovations in LLM-driven cybersecurity. Additionally, we emphasize the role of static analysis in malware detection, introduce notable datasets and specialized LLM models, and discuss essential datasets supporting automated malware research. This study serves as a valuable resource for researchers and cybersecurity professionals, offering insights into LLM-powered malware detection and defence strategies while outlining future directions for strengthening cybersecurity resilience.</li>
</ul>

<h3>Title: Secure Text Mail Encryption with Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Alexej Schelle</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07140">https://arxiv.org/abs/2504.07140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07140">https://arxiv.org/pdf/2504.07140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07140]] Secure Text Mail Encryption with Generative Adversarial Networks(https://arxiv.org/abs/2504.07140)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, generative</a></li>
<li><strong>Abstract: </strong>This work presents an encryption model based on Generative Adversarial Networks (GANs). Encryption of RTF-8 data is realized by dynamically generating decimal numbers that lead to the encryption and decryption of alphabetic strings in integer representation by simple addition rules, the modulus of the dimension of the considered alphabet. The binary numbers for the private dynamical keys correlate with the binary numbers of public reference keys from a mapping defined by the specific GAN configuration. For reversible encryption with bijective mapping between dynamic and reference keys as defined by the GAN encryptor with random combinations of NOT logical gates between bitwise subcomponents of the transmitted text signal, secure text encryption can be realized by transferring a GAN-encrypted public key with encrypted text from a sender to a receiver. Using the technique described above, secure text mail transfer can be realized from component-wise encryption of text mail strings with total key sizes of up to $10^{8}$ bits that define random decimal numbers obtained from the GAN. From the present model, we assert that encrypted texts can be transmitted more efficiently and securely than from RSA encryption, as long as users of the specific configuration of the GAN encryption model are unaware of the GAN encryptor circuit.</li>
</ul>

<h3>Title: Holistic Capability Preservation: Towards Compact Yet Comprehensive Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Ling Team: Caizhi Tang, Chilin Fu, Chunwei Wu, Jia Guo, Jianwen Wang, Jingyu Hu, Liang Jiang, Meng Li, Peng Jiao, Pingping Liu, Shaomian Zheng, Shiwei Liang, Shuaicheng Li, Yalin Zhang, Yingting Wu, Yongkang Liu, Zhenyu Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07158">https://arxiv.org/abs/2504.07158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07158">https://arxiv.org/pdf/2504.07158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07158]] Holistic Capability Preservation: Towards Compact Yet Comprehensive Reasoning Models(https://arxiv.org/abs/2504.07158)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This technical report presents Ring-Lite-Distill, a lightweight reasoning model derived from our open-source Mixture-of-Experts (MoE) Large Language Models (LLMs) Ling-Lite. This study demonstrates that through meticulous high-quality data curation and ingenious training paradigms, the compact MoE model Ling-Lite can be further trained to achieve exceptional reasoning capabilities, while maintaining its parameter-efficient architecture with only 2.75 billion activated parameters, establishing an efficient lightweight reasoning architecture. In particular, in constructing this model, we have not merely focused on enhancing advanced reasoning capabilities, exemplified by high-difficulty mathematical problem solving, but rather aimed to develop a reasoning model with more comprehensive competency coverage. Our approach ensures coverage across reasoning tasks of varying difficulty levels while preserving generic capabilities, such as instruction following, tool use, and knowledge retention. We show that, Ring-Lite-Distill's reasoning ability reaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general capabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The models are accessible at this https URL</li>
</ul>

<h3>Title: Perception in Reflection</h3>
<ul>
<li><strong>Authors: </strong>Yana Wei, Liang Zhao, Kangheng Lin, En Yu, Yuang Peng, Runpei Dong, Jianjian Sun, Haoran Wei, Zheng Ge, Xiangyu Zhang, Vishal M. Patel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07165">https://arxiv.org/abs/2504.07165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07165">https://arxiv.org/pdf/2504.07165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07165]] Perception in Reflection(https://arxiv.org/abs/2504.07165)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a perception in reflection paradigm designed to transcend the limitations of current large vision-language models (LVLMs), which are expected yet often fail to achieve perfect perception initially. Specifically, we propose Reflective Perception (RePer), a dual-model reflection mechanism that systematically alternates between policy and critic models, enables iterative refinement of visual perception. This framework is powered by Reflective Perceptual Learning (RPL), which reinforces intrinsic reflective capabilities through a methodically constructed visual reflection dataset and reflective unlikelihood training. Comprehensive experimental evaluation demonstrates RePer's quantifiable improvements in image understanding, captioning precision, and hallucination reduction. Notably, RePer achieves strong alignment between model attention patterns and human visual focus, while RPL optimizes fine-grained and free-form preference alignment. These advancements establish perception in reflection as a robust paradigm for future multimodal agents, particularly in tasks requiring complex reasoning and multi-step manipulation.</li>
</ul>

<h3>Title: Trustworthy AI Must Account for Intersectionality</h3>
<ul>
<li><strong>Authors: </strong>Jesse C. Cresswell</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07170">https://arxiv.org/abs/2504.07170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07170">https://arxiv.org/pdf/2504.07170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07170]] Trustworthy AI Must Account for Intersectionality(https://arxiv.org/abs/2504.07170)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, fair, explainability</a></li>
<li><strong>Abstract: </strong>Trustworthy AI encompasses many aspirational aspects for aligning AI systems with human values, including fairness, privacy, robustness, explainability, and uncertainty quantification. However, efforts to enhance one aspect often introduce unintended trade-offs that negatively impact others, making it challenging to improve all aspects simultaneously. In this position paper, we review notable approaches to these five aspects and systematically consider every pair, detailing the negative interactions that can arise. For example, applying differential privacy to model training can amplify biases in the data, undermining fairness. Drawing on these findings, we take the position that addressing trustworthiness along each axis in isolation is insufficient. Instead, research on Trustworthy AI must account for intersectionality between aspects and adopt a holistic view across all relevant axes at once. To illustrate our perspective, we provide guidance on how researchers can work towards integrated trustworthiness, a case study on how intersectionality applies to the financial industry, and alternative views to our position.</li>
</ul>

<h3>Title: HypoEval: Hypothesis-Guided Evaluation for Natural Language Generation</h3>
<ul>
<li><strong>Authors: </strong>Mingxuan Li, Hanchen Li, Chenhao Tan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07174">https://arxiv.org/abs/2504.07174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07174">https://arxiv.org/pdf/2504.07174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07174]] HypoEval: Hypothesis-Guided Evaluation for Natural Language Generation(https://arxiv.org/abs/2504.07174)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated great potential for automating the evaluation of natural language generation. Previous frameworks of LLM-as-a-judge fall short in two ways: they either use zero-shot setting without consulting any human input, which leads to low alignment, or fine-tune LLMs on labeled data, which requires a non-trivial number of samples. Moreover, previous methods often provide little reasoning behind automated evaluations. In this paper, we propose HypoEval, Hypothesis-guided Evaluation framework, which first uses a small corpus of human evaluations to generate more detailed rubrics for human judgments and then incorporates a checklist-like approach to combine LLM's assigned scores on each decomposed dimension to acquire overall scores. With only 30 human evaluations, HypoEval achieves state-of-the-art performance in alignment with both human rankings (Spearman correlation) and human scores (Pearson correlation), on average outperforming G-Eval by 11.86% and fine-tuned Llama-3.1-8B-Instruct with at least 3 times more human evaluations by 11.95%. Furthermore, we conduct systematic studies to assess the robustness of HypoEval, highlighting its effectiveness as a reliable and interpretable automated evaluation framework.</li>
</ul>

<h3>Title: Face-LLaVA: Facial Expression and Attribute Understanding through Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Ashutosh Chaubey, Xulang Guan, Mohammad Soleymani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07198">https://arxiv.org/abs/2504.07198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07198">https://arxiv.org/pdf/2504.07198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07198]] Face-LLaVA: Facial Expression and Attribute Understanding through Instruction Tuning(https://arxiv.org/abs/2504.07198)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The human face plays a central role in social communication, necessitating the use of performant computer vision tools for human-centered applications. We propose Face-LLaVA, a multimodal large language model for face-centered, in-context learning, including facial expression and attribute recognition. Additionally, Face-LLaVA is able to generate natural language descriptions that can be used for reasoning. Leveraging existing visual databases, we first developed FaceInstruct-1M, a face-centered database for instruction tuning MLLMs for face processing. We then developed a novel face-specific visual encoder powered by Face-Region Guided Cross-Attention that integrates face geometry with local visual features. We evaluated the proposed method across nine different datasets and five different face processing tasks, including facial expression recognition, action unit detection, facial attribute detection, age estimation and deepfake detection. Face-LLaVA achieves superior results compared to existing open-source MLLMs and competitive performance compared to commercial solutions. Our model output also receives a higher reasoning rating by GPT under a zero-shot setting across all the tasks. Both our dataset and model wil be released at this https URL to support future advancements in social AI and foundational vision-language research.</li>
</ul>

<h3>Title: Leveraging Machine Learning Techniques in Intrusion Detection Systems for Internet of Things</h3>
<ul>
<li><strong>Authors: </strong>Saeid Jamshidi, Amin Nikanjam, Nafi Kawser Wazed, Foutse Khomh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07220">https://arxiv.org/abs/2504.07220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07220">https://arxiv.org/pdf/2504.07220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07220]] Leveraging Machine Learning Techniques in Intrusion Detection Systems for Internet of Things(https://arxiv.org/abs/2504.07220)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, generative, large language model</a></li>
<li><strong>Abstract: </strong>As the Internet of Things (IoT) continues to expand, ensuring the security of connected devices has become increasingly critical. Traditional Intrusion Detection Systems (IDS) often fall short in managing the dynamic and large-scale nature of IoT networks. This paper explores how Machine Learning (ML) and Deep Learning (DL) techniques can significantly enhance IDS performance in IoT environments. We provide a thorough overview of various IDS deployment strategies and categorize the types of intrusions common in IoT systems. A range of ML methods -- including Support Vector Machines, Naive Bayes, K-Nearest Neighbors, Decision Trees, and Random Forests -- are examined alongside advanced DL models such as LSTM, CNN, Autoencoders, RNNs, and Deep Belief Networks. Each technique is evaluated based on its accuracy, efficiency, and suitability for real-world IoT applications. We also address major challenges such as high false positive rates, data imbalance, encrypted traffic analysis, and the resource constraints of IoT devices. In addition, we highlight the emerging role of Generative AI and Large Language Models (LLMs) in improving threat detection, automating responses, and generating intelligent security policies. Finally, we discuss ethical and privacy concerns, underscoring the need for responsible and transparent implementation. This paper aims to provide a comprehensive framework for developing adaptive, intelligent, and secure IDS solutions tailored for the evolving landscape of IoT.</li>
</ul>

<h3>Title: ECDSA Cracking Methods</h3>
<ul>
<li><strong>Authors: </strong>William J. Buchanan, Jamie Gilchrist, Keir Finlow-Bates</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07265">https://arxiv.org/abs/2504.07265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07265">https://arxiv.org/pdf/2504.07265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07265]] ECDSA Cracking Methods(https://arxiv.org/abs/2504.07265)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>The ECDSA (Elliptic Curve Digital Signature Algorithm) is used in many blockchain networks for digital signatures. This includes the Bitcoin and the Ethereum blockchains. While it has good performance levels and as strong current security, it should be handled with care. This care typically relates to the usage of the nonce value which is used to create the signature. This paper outlines the methods that can be used to break ECDSA signatures, including revealed nonces, weak nonce choice, nonce reuse, two keys and shared nonces, and fault attack.</li>
</ul>

<h3>Title: Language Modeling for the Future of Finance: A Quantitative Survey into Metrics, Tasks, and Data Opportunities</h3>
<ul>
<li><strong>Authors: </strong>Nikita Tatarinov, Siddhant Sukhani, Agam Shah, Sudheer Chava</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07274">https://arxiv.org/abs/2504.07274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07274">https://arxiv.org/pdf/2504.07274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07274]] Language Modeling for the Future of Finance: A Quantitative Survey into Metrics, Tasks, and Data Opportunities(https://arxiv.org/abs/2504.07274)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, extraction, explainability</a></li>
<li><strong>Abstract: </strong>Recent advances in language modeling have led to growing interest in applying Natural Language Processing (NLP) techniques to financial problems, enabling new approaches to analysis and decision-making. To systematically examine this trend, we review 374 NLP research papers published between 2017 and 2024 across 38 conferences and workshops, with a focused analysis of 221 papers that directly address finance-related tasks. We evaluate these papers across 11 qualitative and quantitative dimensions, identifying key trends such as the increasing use of general-purpose language models, steady progress in sentiment analysis and information extraction, and emerging efforts around explainability and privacy-preserving methods. We also discuss the use of evaluation metrics, highlighting the importance of domain-specific ones to complement standard machine learning metrics. Our findings emphasize the need for more accessible, adaptive datasets and highlight the significance of incorporating financial crisis periods to strengthen model robustness under real-world conditions. This survey provides a structured overview of NLP research applied to finance and offers practical insights for researchers and practitioners working at this intersection.</li>
</ul>

<h3>Title: A Multi-Phase Analysis of Blood Culture Stewardship: Machine Learning Prediction, Expert Recommendation Assessment, and LLM Automation</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Amrollahi, Nicholas Marshall, Fateme Nateghi Haredasht, Kameron C Black, Aydin Zahedivash, Manoj V Maddali, Stephen P. Ma, Amy Chang, MD Phar Stanley C Deresinski, Mary Kane Goldstein, Steven M. Asch, Niaz Banaei, Jonathan H Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07278">https://arxiv.org/abs/2504.07278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07278">https://arxiv.org/pdf/2504.07278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07278]] A Multi-Phase Analysis of Blood Culture Stewardship: Machine Learning Prediction, Expert Recommendation Assessment, and LLM Automation(https://arxiv.org/abs/2504.07278)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Blood cultures are often over ordered without clear justification, straining healthcare resources and contributing to inappropriate antibiotic use pressures worsened by the global shortage. In study of 135483 emergency department (ED) blood culture orders, we developed machine learning (ML) models to predict the risk of bacteremia using structured electronic health record (EHR) data and provider notes via a large language model (LLM). The structured models AUC improved from 0.76 to 0.79 with note embeddings and reached 0.81 with added diagnosis codes. Compared to an expert recommendation framework applied by human reviewers and an LLM-based pipeline, our ML approach offered higher specificity without compromising sensitivity. The recommendation framework achieved sensitivity 86%, specificity 57%, while the LLM maintained high sensitivity (96%) but over classified negatives, reducing specificity (16%). These findings demonstrate that ML models integrating structured and unstructured data can outperform consensus recommendations, enhancing diagnostic stewardship beyond existing standards of care.</li>
</ul>

<h3>Title: Conthereum: Concurrent Ethereum Optimized Transaction Scheduling for Multi-Core Execution</h3>
<ul>
<li><strong>Authors: </strong>Atefeh Zareh Chahoki, Maurice Herlihy, Marco Roveri</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07280">https://arxiv.org/abs/2504.07280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07280">https://arxiv.org/pdf/2504.07280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07280]] Conthereum: Concurrent Ethereum Optimized Transaction Scheduling for Multi-Core Execution(https://arxiv.org/abs/2504.07280)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Blockchain technology has revolutionized decentralized computation, providing high security through transparent cryptographic protocols and immutable data. However, the Blockchain Trilemma-an inherent trade-off between security, scalability, and performance-limits computational efficiency, resulting in low transactions-per-second (TPS) compared to conventional systems like Visa or PayPal. To address this, we introduce Conthereum, a novel concurrent blockchain solution that enhances multi-core usage in transaction processing through a deterministic scheduling scheme. It reformulates smart contract execution as a variant of the Flexible Job Shop Scheduling Problem (FJSS), optimizing both time and power consumption. Conthereum offers the most efficient open-source implementation compared to existing solutions. Empirical evaluations based on Ethereum, the most widely used blockchain platform, show near-linear throughput increases with available computational power. Additionally, an integrated energy consumption model allows participant to optimize power usage by intelligently distributing workloads across cores. This solution not only boosts network TPS and energy efficiency, offering a scalable and sustainable framework for blockchain transaction processing. The proposed approach also opens new avenues for further optimizations in Ethereum and is adaptable for broader applications in other blockchain infrastructures.</li>
</ul>

<h3>Title: RAISE: Reinforenced Adaptive Instruction Selection For Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lv Qingsong, Yangning Li, Zihua Lan, Zishan Xu, Jiwei Tang, Yinghui Li, Wenhao Jiang, Hai-Tao Zheng, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07282">https://arxiv.org/abs/2504.07282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07282">https://arxiv.org/pdf/2504.07282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07282]] RAISE: Reinforenced Adaptive Instruction Selection For Large Language Models(https://arxiv.org/abs/2504.07282)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the instruction fine-tuning of large language models (LLMs), it has become a consensus that a few high-quality instructions are superior to a large number of low-quality instructions. At present, many instruction selection methods have been proposed, but most of these methods select instruction based on heuristic quality metrics, and only consider data selection before training. These designs lead to insufficient optimization of instruction fine-tuning, and fixed heuristic indicators are often difficult to optimize for specific tasks. So we designed a dynamic, task-objective-driven instruction selection framework RAISE(Reinforenced Adaptive Instruction SElection), which incorporates the entire instruction fine-tuning process into optimization, selecting instruction at each step based on the expected impact of instruction on model performance improvement. Our approach is well interpretable and has strong task-specific optimization capabilities. By modeling dynamic instruction selection as a sequential decision-making process, we use RL to train our selection strategy. Extensive experiments and result analysis prove the superiority of our method compared with other instruction selection methods. Notably, RAISE achieves superior performance by updating only 1\% of the training steps compared to full-data training, demonstrating its efficiency and effectiveness.</li>
</ul>

<h3>Title: MDIT: A Model-free Data Interpolation Method for Diverse Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yangning Li, Zihua Lan, Lv Qingsong, Yinghui Li, Hai-Tao Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07288">https://arxiv.org/abs/2504.07288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07288">https://arxiv.org/pdf/2504.07288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07288]] MDIT: A Model-free Data Interpolation Method for Diverse Instruction Tuning(https://arxiv.org/abs/2504.07288)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) are increasingly applied across various tasks, instruction tuning has emerged as a critical method for enhancing model performance. However, current data management strategies face substantial challenges in generating diverse and comprehensive data, restricting further improvements in model performance. To address this gap, we propose MDIT, a novel model-free data interpolation method for diverse instruction tuning, which generates varied and high-quality instruction data by performing task interpolation. Moreover, it contains diversity-based clustering strategies to ensure the diversity of the training data. Extensive experiments show that our method achieves superior performance in multiple benchmark tasks. The LLMs finetuned with MDIT show significant improvements in numerous tasks such as general question answering, math reasoning, and code generation. MDIT offers an efficient and automatic data synthetic method, generating diverse instruction data without depending on external resources while expanding the application potential of LLMs in complex environments.</li>
</ul>

<h3>Title: PAYADOR: A Minimalist Approach to Grounding Language Models on Structured Data for Interactive Storytelling and Role-playing Games</h3>
<ul>
<li><strong>Authors: </strong>Santiago G√≥ngora, Luis Chiruzzo, Gonzalo M√©ndez, Pablo Gerv√°s</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07304">https://arxiv.org/abs/2504.07304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07304">https://arxiv.org/pdf/2504.07304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07304]] PAYADOR: A Minimalist Approach to Grounding Language Models on Structured Data for Interactive Storytelling and Role-playing Games(https://arxiv.org/abs/2504.07304)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Every time an Interactive Storytelling (IS) system gets a player input, it is facing the world-update problem. Classical approaches to this problem consist in mapping that input to known preprogrammed actions, what can severely constrain the free will of the player. When the expected experience has a strong focus on improvisation, like in Role-playing Games (RPGs), this problem is critical. In this paper we present PAYADOR, a different approach that focuses on predicting the outcomes of the actions instead of representing the actions themselves. To implement this approach, we ground a Large Language Model to a minimal representation of the fictional world, obtaining promising results. We make this contribution open-source, so it can be adapted and used for other related research on unleashing the co-creativity power of RPGs.</li>
</ul>

<h3>Title: Alice: Proactive Learning with Teacher's Demonstrations for Weak-to-Strong Generalization</h3>
<ul>
<li><strong>Authors: </strong>Shujin Wu, Cheng Qian, Yi R. (May)Fung, Paul Pu Liang, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07316">https://arxiv.org/abs/2504.07316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07316">https://arxiv.org/pdf/2504.07316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07316]] Alice: Proactive Learning with Teacher's Demonstrations for Weak-to-Strong Generalization(https://arxiv.org/abs/2504.07316)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The growing capabilities of large language models (LLMs) present a key challenge of maintaining effective human oversight. Weak-to-strong generalization (W2SG) offers a promising framework for supervising increasingly capable LLMs using weaker ones. Traditional W2SG methods rely on passive learning, where a weak teacher provides noisy demonstrations to train a strong student. This hinders students from employing their knowledge during training and reaching their full potential. In this work, we introduce Alice (pro{A}ctive {l}earning w{i}th tea{c}her's D{e}monstrations), a framework that leverages complementary knowledge between teacher and student to enhance the learning this http URL probe the knowledge base of the teacher model by eliciting their uncertainty, and then use these insights together with teachers' responses as demonstrations to guide student models in self-generating improved responses for supervision. In addition, for situations with significant capability gaps between teacher and student models, we introduce cascade Alice, which employs a hierarchical training approach where weak teachers initially supervise intermediate models, who then guide stronger models in sequence. Experimental results demonstrate that our method significantly enhances the W2SG performance, yielding substantial improvements in three key tasks compared to the original W2SG: knowledge-based reasoning (+4.0%), mathematical reasoning (+22.62%), and logical reasoning (+12.11%). This highlights the effectiveness of our new W2SG paradigm that enables more robust knowledge transfer and supervision outcome.</li>
</ul>

<h3>Title: Prekey Pogo: Investigating Security and Privacy Issues in WhatsApp's Handshake Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Gabriel K. Gegenhuber, Philipp √â. Frenzel, Maximilian G√ºnther, Aljosha Judmayer</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07323">https://arxiv.org/abs/2504.07323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07323">https://arxiv.org/pdf/2504.07323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07323]] Prekey Pogo: Investigating Security and Privacy Issues in WhatsApp's Handshake Mechanism(https://arxiv.org/abs/2504.07323)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack</a></li>
<li><strong>Abstract: </strong>WhatsApp, the world's largest messaging application, uses a version of the Signal protocol to provide end-to-end encryption (E2EE) with strong security guarantees, including Perfect Forward Secrecy (PFS). To ensure PFS right from the start of a new conversation -- even when the recipient is offline -- a stash of ephemeral (one-time) prekeys must be stored on a server. While the critical role of these one-time prekeys in achieving PFS has been outlined in the Signal specification, we are the first to demonstrate a targeted depletion attack against them on individual WhatsApp user devices. Our findings not only reveal an attack that can degrade PFS for certain messages, but also expose inherent privacy risks and serious availability implications arising from the refilling and distribution procedure essential for this security mechanism.</li>
</ul>

<h3>Title: Objaverse++: Curated 3D Object Dataset with Quality Annotations</h3>
<ul>
<li><strong>Authors: </strong>Chendi Lin, Heshan Liu, Qunshu Lin, Zachary Bright, Shitao Tang, Yihui He, Minghao Liu, Ling Zhu, Cindy Le</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07334">https://arxiv.org/abs/2504.07334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07334">https://arxiv.org/pdf/2504.07334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07334]] Objaverse++: Curated 3D Object Dataset with Quality Annotations(https://arxiv.org/abs/2504.07334)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents Objaverse++, a curated subset of Objaverse enhanced with detailed attribute annotations by human experts. Recent advances in 3D content generation have been driven by large-scale datasets such as Objaverse, which contains over 800,000 3D objects collected from the Internet. Although Objaverse represents the largest available 3D asset collection, its utility is limited by the predominance of low-quality models. To address this limitation, we manually annotate 10,000 3D objects with detailed attributes, including aesthetic quality scores, texture color classifications, multi-object composition flags, transparency characteristics, etc. Then, we trained a neural network capable of annotating the tags for the rest of the Objaverse dataset. Through experiments and a user study on generation results, we demonstrate that models pre-trained on our quality-focused subset achieve better performance than those trained on the larger dataset of Objaverse in image-to-3D generation tasks. In addition, by comparing multiple subsets of training data filtered by our tags, our results show that the higher the data quality, the faster the training loss converges. These findings suggest that careful curation and rich annotation can compensate for the raw dataset size, potentially offering a more efficient path to develop 3D generative models. We release our enhanced dataset of approximately 500,000 curated 3D models to facilitate further research on various downstream tasks in 3D computer vision. In the near future, we aim to extend our annotations to cover the entire Objaverse dataset.</li>
</ul>

<h3>Title: DLTPose: 6DoF Pose Estimation From Accurate Dense Surface Point Estimates</h3>
<ul>
<li><strong>Authors: </strong>Akash Jadhav, Michael Greenspan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07335">https://arxiv.org/abs/2504.07335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07335">https://arxiv.org/pdf/2504.07335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07335]] DLTPose: 6DoF Pose Estimation From Accurate Dense Surface Point Estimates(https://arxiv.org/abs/2504.07335)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose DLTPose, a novel method for 6DoF object pose estimation from RGB-D images that combines the accuracy of sparse keypoint methods with the robustness of dense pixel-wise predictions. DLTPose predicts per-pixel radial distances to a set of minimally four keypoints, which are then fed into our novel Direct Linear Transform (DLT) formulation to produce accurate 3D object frame surface estimates, leading to better 6DoF pose estimation. Additionally, we introduce a novel symmetry-aware keypoint ordering approach, designed to handle object symmetries that otherwise cause inconsistencies in keypoint assignments. Previous keypoint-based methods relied on fixed keypoint orderings, which failed to account for the multiple valid configurations exhibited by symmetric objects, which our ordering approach exploits to enhance the model's ability to learn stable keypoint representations. Extensive experiments on the benchmark LINEMOD, Occlusion LINEMOD and YCB-Video datasets show that DLTPose outperforms existing methods, especially for symmetric and occluded objects, demonstrating superior Mean Average Recall values of 86.5% (LM), 79.7% (LM-O) and 89.5% (YCB-V). The code is available at this https URL .</li>
</ul>

<h3>Title: Zeus: Zero-shot LLM Instruction for Union Segmentation in Multimodal Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Dai, Kai Ye, Guodong Liu, Haoteng Tang, Liang Zhan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07336">https://arxiv.org/abs/2504.07336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07336">https://arxiv.org/pdf/2504.07336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07336]] Zeus: Zero-shot LLM Instruction for Union Segmentation in Multimodal Medical Imaging(https://arxiv.org/abs/2504.07336)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation has achieved remarkable success through the continuous advancement of UNet-based and Transformer-based foundation backbones. However, clinical diagnosis in the real world often requires integrating domain knowledge, especially textual information. Conducting multimodal learning involves visual and text modalities shown as a solution, but collecting paired vision-language datasets is expensive and time-consuming, posing significant challenges. Inspired by the superior ability in numerous cross-modal tasks for Large Language Models (LLMs), we proposed a novel Vision-LLM union framework to address the issues. Specifically, we introduce frozen LLMs for zero-shot instruction generation based on corresponding medical images, imitating the radiology scanning and report generation process. {To better approximate real-world diagnostic processes}, we generate more precise text instruction from multimodal radiology images (e.g., T1-w or T2-w MRI and CT). Based on the impressive ability of semantic understanding and rich knowledge of LLMs. This process emphasizes extracting special features from different modalities and reunion the information for the ultimate clinical diagnostic. With generated text instruction, our proposed union segmentation framework can handle multimodal segmentation without prior collected vision-language datasets. To evaluate our proposed method, we conduct comprehensive experiments with influential baselines, the statistical results and the visualized case study demonstrate the superiority of our novel method.}</li>
</ul>

<h3>Title: Leveraging deep learning for plant disease identification: a bibliometric analysis in SCOPUS from 2018 to 2024</h3>
<ul>
<li><strong>Authors: </strong>Enow Takang Achuo Albert, Ngalle Hermine Bille, Ngonkeu Mangaptche Eddy Leonard</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07342">https://arxiv.org/abs/2504.07342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07342">https://arxiv.org/pdf/2504.07342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07342]] Leveraging deep learning for plant disease identification: a bibliometric analysis in SCOPUS from 2018 to 2024(https://arxiv.org/abs/2504.07342)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This work aimed to present a bibliometric analysis of deep learning research for plant disease identification, with a special focus on generative modeling. A thorough analysis of SCOPUS-sourced bibliometric data from 253 documents was performed. Key performance metrics such as accuracy, precision, recall, and F1-score were analyzed for generative modeling. The findings highlighted significant contributions from some authors Too and Arnal Barbedo, whose works had notable citation counts, suggesting their influence on the academic community. Co-authorship networks revealed strong collaborative clusters, while keyword analysis identified emerging research gaps. This study highlights the role of collaboration and citation metrics in shaping research directions and enhancing the impact of scholarly work in applications of deep learning to plant disease identification. Future research should explore the methodologies of highly cited studies to inform best practices and policy-making.</li>
</ul>

<h3>Title: Revisiting Prompt Optimization with Large Reasoning Models-A Case Study on Event Extraction</h3>
<ul>
<li><strong>Authors: </strong>Saurabh Srivastava, Ziyu Yao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07357">https://arxiv.org/abs/2504.07357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07357">https://arxiv.org/pdf/2504.07357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07357]] Revisiting Prompt Optimization with Large Reasoning Models-A Case Study on Event Extraction(https://arxiv.org/abs/2504.07357)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large Reasoning Models (LRMs) such as DeepSeek-R1 and OpenAI o1 have demonstrated remarkable capabilities in various reasoning tasks. Their strong capability to generate and reason over intermediate thoughts has also led to arguments that they may no longer require extensive prompt engineering or optimization to interpret human instructions and produce accurate outputs. In this work, we aim to systematically study this open question, using the structured task of event extraction for a case study. We experimented with two LRMs (DeepSeek-R1 and o1) and two general-purpose Large Language Models (LLMs) (GPT-4o and GPT-4.5), when they were used as task models or prompt optimizers. Our results show that on tasks as complicated as event extraction, LRMs as task models still benefit from prompt optimization, and that using LRMs as prompt optimizers yields more effective prompts. Finally, we provide an error analysis of common errors made by LRMs and highlight the stability and consistency of LRMs in refining task instructions and event guidelines.</li>
</ul>

<h3>Title: Electronic Warfare Cyberattacks, Countermeasures and Modern Defensive Strategies of UAV Avionics: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Aaron Yu, Iuliia Kolotylo, Hashim A. Hashim, A. E.E. Eltoukhy</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07358">https://arxiv.org/abs/2504.07358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07358">https://arxiv.org/pdf/2504.07358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07358]] Electronic Warfare Cyberattacks, Countermeasures and Modern Defensive Strategies of UAV Avionics: A Survey(https://arxiv.org/abs/2504.07358)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Unmanned Aerial Vehicles (UAVs) play a pivotal role in modern autonomous air mobility, and the reliability of UAV avionics systems is critical to ensuring mission success, sustainability practices, and public safety. The success of UAV missions depends on effectively mitigating various aspects of electronic warfare, including non-destructive and destructive cyberattacks, transponder vulnerabilities, and jamming threats, while rigorously implementing countermeasures and defensive aids. This paper provides a comprehensive review of UAV cyberattacks, countermeasures, and defensive strategies. It explores UAV-to-UAV coordination attacks and their associated features, such as dispatch system attacks, Automatic Dependent Surveillance-Broadcast (ADS-B) attacks, Traffic Alert and Collision Avoidance System (TCAS)-induced collisions, and TCAS attacks. Additionally, the paper examines UAV-to-command center coordination attacks, as well as UAV functionality attacks. The review also covers various countermeasures and defensive aids designed for UAVs. Lastly, a comparison of common cyberattacks and countermeasure approaches is conducted, along with a discussion of future trends in the field. Keywords: Electronic warfare, UAVs, Avionics Systems, cyberattacks, coordination attacks, functionality attacks, countermeasure, defensive-aids.</li>
</ul>

<h3>Title: Enhancing Time Series Forecasting via Multi-Level Text Alignment with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Taibiao Zhao, Xiaobing Chen, Mingxuan Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07360">https://arxiv.org/abs/2504.07360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07360">https://arxiv.org/pdf/2504.07360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07360]] Enhancing Time Series Forecasting via Multi-Level Text Alignment with LLMs(https://arxiv.org/abs/2504.07360)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The adaptation of large language models (LLMs) to time series forecasting poses unique challenges, as time series data is continuous in nature, while LLMs operate on discrete tokens. Despite the success of LLMs in natural language processing (NLP) and other structured domains, aligning time series data with language-based representations while maintaining both predictive accuracy and interpretability remains a significant hurdle. Existing methods have attempted to reprogram time series data into text-based forms, but these often fall short in delivering meaningful, interpretable results. In this paper, we propose a multi-level text alignment framework for time series forecasting using LLMs that not only improves prediction accuracy but also enhances the interpretability of time series representations. Our method decomposes time series into trend, seasonal, and residual components, which are then reprogrammed into component-specific text representations. We introduce a multi-level alignment mechanism, where component-specific embeddings are aligned with pre-trained word tokens, enabling more interpretable forecasts. Experiments on multiple datasets demonstrate that our method outperforms state-of-the-art models in accuracy while providing good interpretability.</li>
</ul>

<h3>Title: Augmented Shuffle Protocols for Accurate and Robust Frequency Estimation under Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Takao Murakami, Yuichi Sei, Reo Eriguchi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07362">https://arxiv.org/abs/2504.07362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07362">https://arxiv.org/pdf/2504.07362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07362]] Augmented Shuffle Protocols for Accurate and Robust Frequency Estimation under Differential Privacy(https://arxiv.org/abs/2504.07362)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust</a></li>
<li><strong>Abstract: </strong>The shuffle model of DP (Differential Privacy) provides high utility by introducing a shuffler that randomly shuffles noisy data sent from users. However, recent studies show that existing shuffle protocols suffer from the following two major drawbacks. First, they are vulnerable to local data poisoning attacks, which manipulate the statistics about input data by sending crafted data, especially when the privacy budget epsilon is small. Second, the actual value of epsilon is increased by collusion attacks by the data collector and users. In this paper, we address these two issues by thoroughly exploring the potential of the augmented shuffle model, which allows the shuffler to perform additional operations, such as random sampling and dummy data addition. Specifically, we propose a generalized framework for local-noise-free protocols in which users send (encrypted) input data to the shuffler without adding noise. We show that this generalized protocol provides DP and is robust to the above two attacks if a simpler mechanism that performs the same process on binary input data provides DP. Based on this framework, we propose three concrete protocols providing DP and robustness against the two attacks. Our first protocol generates the number of dummy values for each item from a binomial distribution and provides higher utility than several state-of-the-art existing shuffle protocols. Our second protocol significantly improves the utility of our first protocol by introducing a novel dummy-count distribution: asymmetric two-sided geometric distribution. Our third protocol is a special case of our second protocol and provides pure epsilon-DP. We show the effectiveness of our protocols through theoretical analysis and comprehensive experiments.</li>
</ul>

<h3>Title: View-Dependent Uncertainty Estimation of 3D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Han, Corentin Dumery</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07370">https://arxiv.org/abs/2504.07370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07370">https://arxiv.org/pdf/2504.07370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07370]] View-Dependent Uncertainty Estimation of 3D Gaussian Splatting(https://arxiv.org/abs/2504.07370)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting (3DGS) has become increasingly popular in 3D scene reconstruction for its high visual accuracy. However, uncertainty estimation of 3DGS scenes remains underexplored and is crucial to downstream tasks such as asset extraction and scene completion. Since the appearance of 3D gaussians is view-dependent, the color of a gaussian can thus be certain from an angle and uncertain from another. We thus propose to model uncertainty in 3DGS as an additional view-dependent per-gaussian feature that can be modeled with spherical harmonics. This simple yet effective modeling is easily interpretable and can be integrated into the traditional 3DGS pipeline. It is also significantly faster than ensemble methods while maintaining high accuracy, as demonstrated in our experiments.</li>
</ul>

<h3>Title: ChronoFormer: Time-Aware Transformer Architectures for Structured Clinical Event Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yuanyun Zhang, Shi Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07373">https://arxiv.org/abs/2504.07373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07373">https://arxiv.org/pdf/2504.07373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07373]] ChronoFormer: Time-Aware Transformer Architectures for Structured Clinical Event Modeling(https://arxiv.org/abs/2504.07373)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The temporal complexity of electronic health record (EHR) data presents significant challenges for predicting clinical outcomes using machine learning. This paper proposes ChronoFormer, an innovative transformer based architecture specifically designed to encode and leverage temporal dependencies in longitudinal patient data. ChronoFormer integrates temporal embeddings, hierarchical attention mechanisms, and domain specific masking techniques. Extensive experiments conducted on three benchmark tasks mortality prediction, readmission prediction, and long term comorbidity onset demonstrate substantial improvements over current state of the art methods. Furthermore, detailed analyses of attention patterns underscore ChronoFormer's capability to capture clinically meaningful long range temporal relationships.</li>
</ul>

<h3>Title: Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction</h3>
<ul>
<li><strong>Authors: </strong>Junyi Ma, Wentao Bao, Jingyi Xu, Guanzhong Sun, Xieyuanli Chen, Hesheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07375">https://arxiv.org/abs/2504.07375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07375">https://arxiv.org/pdf/2504.07375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07375]] Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction(https://arxiv.org/abs/2504.07375)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Predicting hand motion is critical for understanding human intentions and bridging the action space between human movements and robot manipulations. Existing hand trajectory prediction (HTP) methods forecast the future hand waypoints in 3D space conditioned on past egocentric observations. However, such models are only designed to accommodate 2D egocentric video inputs. There is a lack of awareness of multimodal environmental information from both 2D and 3D observations, hindering the further improvement of 3D HTP performance. In addition, these models overlook the synergy between hand movements and headset camera egomotion, either predicting hand trajectories in isolation or encoding egomotion only from past frames. To address these limitations, we propose novel diffusion models (MMTwin) for multimodal 3D hand trajectory prediction. MMTwin is designed to absorb multimodal information as input encompassing 2D RGB images, 3D point clouds, past hand waypoints, and text prompt. Besides, two latent diffusion models, the egomotion diffusion and the HTP diffusion as twins, are integrated into MMTwin to predict camera egomotion and future hand trajectories concurrently. We propose a novel hybrid Mamba-Transformer module as the denoising model of the HTP diffusion to better fuse multimodal features. The experimental results on three publicly available datasets and our self-recorded data demonstrate that our proposed MMTwin can predict plausible future 3D hand trajectories compared to the state-of-the-art baselines, and generalizes well to unseen environments. The code and pretrained models will be released at this https URL.</li>
</ul>

<h3>Title: BRepFormer: Transformer-Based B-rep Geometric Feature Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yongkang Dai, Xiaoshui Huang, Yunpeng Bai, Hao Guo, Hongping Gan, Ling Yang, Yilei Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07378">https://arxiv.org/abs/2504.07378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07378">https://arxiv.org/pdf/2504.07378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07378]] BRepFormer: Transformer-Based B-rep Geometric Feature Recognition(https://arxiv.org/abs/2504.07378)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recognizing geometric features on B-rep models is a cornerstone technique for multimedia content-based retrieval and has been widely applied in intelligent manufacturing. However, previous research often merely focused on Machining Feature Recognition (MFR), falling short in effectively capturing the intricate topological and geometric characteristics of complex geometry features. In this paper, we propose BRepFormer, a novel transformer-based model to recognize both machining feature and complex CAD models' features. BRepFormer encodes and fuses the geometric and topological features of the models. Afterwards, BRepFormer utilizes a transformer architecture for feature propagation and a recognition head to identify geometry features. During each iteration of the transformer, we incorporate a bias that combines edge features and topology features to reinforce geometric constraints on each face. In addition, we also proposed a dataset named Complex B-rep Feature Dataset (CBF), comprising 20,000 B-rep models. By covering more complex B-rep models, it is better aligned with industrial applications. The experimental results demonstrate that BRepFormer achieves state-of-the-art accuracy on the MFInstSeg, MFTRCAD, and our CBF datasets.</li>
</ul>

<h3>Title: Model Discrepancy Learning: Synthetic Faces Detection Based on Multi-Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Qingchao Jiang, Zhishuo Xu, Zhiying Zhu, Ning Chen, Haoyue Wang, Zhongjie Ba</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07382">https://arxiv.org/abs/2504.07382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07382">https://arxiv.org/pdf/2504.07382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07382]] Model Discrepancy Learning: Synthetic Faces Detection Based on Multi-Reconstruction(https://arxiv.org/abs/2504.07382)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Advances in image generation enable hyper-realistic synthetic faces but also pose risks, thus making synthetic face detection crucial. Previous research focuses on the general differences between generated images and real images, often overlooking the discrepancies among various generative techniques. In this paper, we explore the intrinsic relationship between synthetic images and their corresponding generation technologies. We find that specific images exhibit significant reconstruction discrepancies across different generative methods and that matching generation techniques provide more accurate reconstructions. Based on this insight, we propose a Multi-Reconstruction-based detector. By reversing and reconstructing images using multiple generative models, we analyze the reconstruction differences among real, GAN-generated, and DM-generated images to facilitate effective differentiation. Additionally, we introduce the Asian Synthetic Face Dataset (ASFD), containing synthetic Asian faces generated with various GANs and DMs. This dataset complements existing synthetic face datasets. Experimental results demonstrate that our detector achieves exceptional performance, with strong generalization and robustness.</li>
</ul>

<h3>Title: TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sher Badshah, Ali Emami, Hassan Sajjad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07385">https://arxiv.org/abs/2504.07385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07385">https://arxiv.org/pdf/2504.07385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07385]] TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large Language Models(https://arxiv.org/abs/2504.07385)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) become increasingly integrated into real-world, autonomous applications, relying on static, pre-annotated references for evaluation poses significant challenges in cost, scalability, and completeness. We propose Tool-Augmented LLM Evaluation (TALE), a framework to assess LLM outputs without predetermined ground-truth answers. Unlike conventional metrics that compare to fixed references or depend solely on LLM-as-a-judge knowledge, TALE employs an agent with tool-access capabilities that actively retrieves and synthesizes external evidence. It iteratively generates web queries, collects information, summarizes findings, and refines subsequent searches through reflection. By shifting away from static references, TALE aligns with free-form question-answering tasks common in real-world scenarios. Experimental results on multiple free-form QA benchmarks show that TALE not only outperforms standard reference-based metrics for measuring response accuracy but also achieves substantial to near-perfect agreement with human evaluations. TALE enhances the reliability of LLM evaluations in real-world, dynamic scenarios without relying on static references.</li>
</ul>

<h3>Title: Task-Circuit Quantization: Leveraging Knowledge Localization and Interpretability for Compression</h3>
<ul>
<li><strong>Authors: </strong>Hanqi Xiao, Yi-Lin Sung, Elias Stengel-Eskin, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07389">https://arxiv.org/abs/2504.07389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07389">https://arxiv.org/pdf/2504.07389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07389]] Task-Circuit Quantization: Leveraging Knowledge Localization and Interpretability for Compression(https://arxiv.org/abs/2504.07389)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Post-training quantization (PTQ) reduces a model's memory footprint by mapping full precision weights into low bit weights without costly retraining, but can degrade its downstream performance especially in low 2- to 3-bit settings. We develop a new mixed-precision PTQ approach, Task-Circuit Quantization (TaCQ), that draws parallels to automated circuit discovery, directly conditioning the quantization process on specific weight circuits -- which we define as sets of weights associated with downstream task performance. These weights are kept as 16-bit weights, while others are quantized, maintaining performance while only adding a marginal memory cost. Specifically, TaCQ contrasts unquantized model weights with a uniformly-quantized model to estimate the expected change in weights due to quantization and uses gradient information to predict the resulting impact on task performance, allowing us to preserve task-specific weights. We compare TaCQ-based quantization to existing mixed-precision quantization methods when conditioning both on general-purpose and task-specific data. Across QA, math reasoning, and text-to-SQL tasks for both Llama-3 and Qwen2.5, we find that TaCQ outperforms baselines using the same calibration data and a lower weight budget, achieving major improvements in the 2 and 3-bit regime. With only 3.1 bits we are able to recover 96% of Llama-3-8B-Instruct's unquantized 16-bit MMLU performance, obtaining a 5.25% absolute improvement over SPQR. We also observe consistently large gains over existing methods in the 2-bit regime, with an average gain of 14.74% over the strongest baseline, SliM-LLM. Moreover, we observe a 7.20% gain without conditioning on specific tasks, showing TaCQ's ability to identify important weights is not limited to task-conditioned settings.</li>
</ul>

<h3>Title: ID-Booth: Identity-consistent Face Generation with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Darian Toma≈°eviƒá, Fadi Boutros, Chenhao Lin, Naser Damer, Vitomir ≈†truc, Peter Peer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07392">https://arxiv.org/abs/2504.07392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07392">https://arxiv.org/pdf/2504.07392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07392]] ID-Booth: Identity-consistent Face Generation with Diffusion Models(https://arxiv.org/abs/2504.07392)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative modeling have enabled the generation of high-quality synthetic data that is applicable in a variety of domains, including face recognition. Here, state-of-the-art generative models typically rely on conditioning and fine-tuning of powerful pretrained diffusion models to facilitate the synthesis of realistic images of a desired identity. Yet, these models often do not consider the identity of subjects during training, leading to poor consistency between generated and intended identities. In contrast, methods that employ identity-based training objectives tend to overfit on various aspects of the identity, and in turn, lower the diversity of images that can be generated. To address these issues, we present in this paper a novel generative diffusion-based framework, called ID-Booth. ID-Booth consists of a denoising network responsible for data generation, a variational auto-encoder for mapping images to and from a lower-dimensional latent space and a text encoder that allows for prompt-based control over the generation procedure. The framework utilizes a novel triplet identity training objective and enables identity-consistent image generation while retaining the synthesis capabilities of pretrained diffusion models. Experiments with a state-of-the-art latent diffusion model and diverse prompts reveal that our method facilitates better intra-identity consistency and inter-identity separability than competing methods, while achieving higher image diversity. In turn, the produced data allows for effective augmentation of small-scale datasets and training of better-performing recognition models in a privacy-preserving manner. The source code for the ID-Booth framework is publicly available at this https URL.</li>
</ul>

<h3>Title: ClimateBench-M: A Multi-Modal Climate Data Benchmark with a Simple Generative Method</h3>
<ul>
<li><strong>Authors: </strong>Dongqi Fu, Yada Zhu, Zhining Liu, Lecheng Zheng, Xiao Lin, Zihao Li, Liri Fang, Katherine Tieu, Onkar Bhardwaj, Kommy Weldemariam, Hanghang Tong, Hendrik Hamann, Jingrui He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07394">https://arxiv.org/abs/2504.07394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07394">https://arxiv.org/pdf/2504.07394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07394]] ClimateBench-M: A Multi-Modal Climate Data Benchmark with a Simple Generative Method(https://arxiv.org/abs/2504.07394)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Climate science studies the structure and dynamics of Earth's climate system and seeks to understand how climate changes over time, where the data is usually stored in the format of time series, recording the climate features, geolocation, time attributes, etc. Recently, much research attention has been paid to the climate benchmarks. In addition to the most common task of weather forecasting, several pioneering benchmark works are proposed for extending the modality, such as domain-specific applications like tropical cyclone intensity prediction and flash flood damage estimation, or climate statement and confidence level in the format of natural language. To further motivate the artificial general intelligence development for climate science, in this paper, we first contribute a multi-modal climate benchmark, i.e., ClimateBench-M, which aligns (1) the time series climate data from ERA5, (2) extreme weather events data from NOAA, and (3) satellite image data from NASA HLS based on a unified spatial-temporal granularity. Second, under each data modality, we also propose a simple but strong generative method that could produce competitive performance in weather forecasting, thunderstorm alerts, and crop segmentation tasks in the proposed ClimateBench-M. The data and code of ClimateBench-M are publicly available at this https URL.</li>
</ul>

<h3>Title: FAIR-SIGHT: Fairness Assurance in Image Recognition via Simultaneous Conformal Thresholding and Dynamic Output Repair</h3>
<ul>
<li><strong>Authors: </strong>Arya Fayyazi, Mehdi Kamal, Massoud Pedram</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07395">https://arxiv.org/abs/2504.07395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07395">https://arxiv.org/pdf/2504.07395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07395]] FAIR-SIGHT: Fairness Assurance in Image Recognition via Simultaneous Conformal Thresholding and Dynamic Output Repair(https://arxiv.org/abs/2504.07395)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>We introduce FAIR-SIGHT, an innovative post-hoc framework designed to ensure fairness in computer vision systems by combining conformal prediction with a dynamic output repair mechanism. Our approach calculates a fairness-aware non-conformity score that simultaneously assesses prediction errors and fairness violations. Using conformal prediction, we establish an adaptive threshold that provides rigorous finite-sample, distribution-free guarantees. When the non-conformity score for a new image exceeds the calibrated threshold, FAIR-SIGHT implements targeted corrective adjustments, such as logit shifts for classification and confidence recalibration for detection, to reduce both group and individual fairness disparities, all without the need for retraining or having access to internal model parameters. Comprehensive theoretical analysis validates our method's error control and convergence properties. At the same time, extensive empirical evaluations on benchmark datasets show that FAIR-SIGHT significantly reduces fairness disparities while preserving high predictive performance.</li>
</ul>

<h3>Title: LauraTSE: Target Speaker Extraction using Auto-Regressive Decoder-Only Language Models</h3>
<ul>
<li><strong>Authors: </strong>Beilong Tang, Bang Zeng, Ming Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07402">https://arxiv.org/abs/2504.07402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07402">https://arxiv.org/pdf/2504.07402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07402]] LauraTSE: Target Speaker Extraction using Auto-Regressive Decoder-Only Language Models(https://arxiv.org/abs/2504.07402)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>We propose LauraTSE, an Auto-Regressive Decoder-Only Language Model for Target Speaker Extraction (TSE) based on the LauraGPT backbone. It employs a small-scale auto-regressive decoder-only language model which takes the continuous representations for both the mixture and the reference speeches and produces the first few layers of the target speech's discrete codec representations. In addition, a one-step encoder-only language model reconstructs the sum of the predicted codec embeddings using both the mixture and the reference information. Our approach achieves superior or comparable performance to existing generative and discriminative TSE models. To the best of our knowledge, LauraTSE is the first single-task TSE model to leverage an auto-regressive decoder-only language model as the backbone.</li>
</ul>

<h3>Title: Multi-Selection for Recommendation Systems</h3>
<ul>
<li><strong>Authors: </strong>Sahasrajit Sarmasarkar, Zhihao Jiang, Ashish Goel, Aleksandra Korolova, Kamesh Munagala</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07403">https://arxiv.org/abs/2504.07403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07403">https://arxiv.org/pdf/2504.07403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07403]] Multi-Selection for Recommendation Systems(https://arxiv.org/abs/2504.07403)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>We present the construction of a multi-selection model to answer differentially private queries in the context of recommendation systems. The server sends back multiple recommendations and a ``local model'' to the user, which the user can run locally on its device to select the item that best fits its private features. We study a setup where the server uses a deep neural network (trained on the Movielens 25M dataset as the ground truth for movie recommendation. In the multi-selection paradigm, the average recommendation utility is approximately 97\% of the optimal utility (as determined by the ground truth neural network) while maintaining a local differential privacy guarantee with $\epsilon$ ranging around 1 with respect to feature vectors of neighboring users. This is in comparison to an average recommendation utility of 91\% in the non-multi-selection regime under the same constraints.</li>
</ul>

<h3>Title: FlexIP: Dynamic Control of Preservation and Personality for Customized Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Linyan Huang, Haonan Lin, Yanning Zhou, Kaiwen Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07405">https://arxiv.org/abs/2504.07405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07405">https://arxiv.org/pdf/2504.07405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07405]] FlexIP: Dynamic Control of Preservation and Personality for Customized Image Generation(https://arxiv.org/abs/2504.07405)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of 2D generative models, preserving subject identity while enabling diverse editing has emerged as a critical research focus. Existing methods typically face inherent trade-offs between identity preservation and personalized manipulation. We introduce FlexIP, a novel framework that decouples these objectives through two dedicated components: a Personalization Adapter for stylistic manipulation and a Preservation Adapter for identity maintenance. By explicitly injecting both control mechanisms into the generative model, our framework enables flexible parameterized control during inference through dynamic tuning of the weight adapter. Experimental results demonstrate that our approach breaks through the performance limitations of conventional methods, achieving superior identity preservation while supporting more diverse personalized generation capabilities (Project Page: this https URL).</li>
</ul>

<h3>Title: AI Coding with Few-Shot Prompting for Thematic Analysis</h3>
<ul>
<li><strong>Authors: </strong>Samuel Flanders, Melati Nungsari, Mark Cheong Wing Loong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07408">https://arxiv.org/abs/2504.07408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07408">https://arxiv.org/pdf/2504.07408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07408]] AI Coding with Few-Shot Prompting for Thematic Analysis(https://arxiv.org/abs/2504.07408)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper explores the use of large language models (LLMs), here represented by GPT 3.5-Turbo to perform coding for a thematic analysis. Coding is highly labor intensive, making it infeasible for most researchers to conduct exhaustive thematic analyses of large corpora. We utilize few-shot prompting with higher quality codes generated on semantically similar passages to enhance the quality of the codes while utilizing a cheap, more easily scalable model.</li>
</ul>

<h3>Title: Decomposition-Based Optimal Bounds for Privacy Amplification via Shuffling</h3>
<ul>
<li><strong>Authors: </strong>Pengcheng Su, Haibo Cheng, Ping Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07414">https://arxiv.org/abs/2504.07414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07414">https://arxiv.org/pdf/2504.07414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07414]] Decomposition-Based Optimal Bounds for Privacy Amplification via Shuffling(https://arxiv.org/abs/2504.07414)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Shuffling has been shown to amplify differential privacy guarantees, offering a stronger privacy-utility trade-off. To characterize and compute this amplification, two fundamental analytical frameworks have been proposed: the privacy blanket by Balle et al. (CRYPTO 2019) and the clone paradigm (including both the standard clone and stronger clone) by Feldman et al. (FOCS 2021, SODA 2023). All these methods rely on decomposing local randomizers. In this work, we introduce a unified analysis framework--the general clone paradigm--which encompasses all possible decompositions. We identify the optimal decomposition within the general clone paradigm. Moreover, we develop a simple and efficient algorithm to compute the exact value of the optimal privacy amplification bounds via Fast Fourier Transform. Experimental results demonstrate that the computed upper bounds for privacy amplification closely approximate the lower bounds, highlighting the tightness of our approach. Finally, using our algorithm, we conduct the first systematic analysis of the joint composition of LDP protocols in the shuffle model.</li>
</ul>

<h3>Title: Leveraging LLMs for Multimodal Retrieval-Augmented Radiology Report Generation via Key Phrase Extraction</h3>
<ul>
<li><strong>Authors: </strong>Kyoyun Choi, Byungmu Yoon, Soobum Kim, Jonggwon Park</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07415">https://arxiv.org/abs/2504.07415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07415">https://arxiv.org/pdf/2504.07415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07415]] Leveraging LLMs for Multimodal Retrieval-Augmented Radiology Report Generation via Key Phrase Extraction(https://arxiv.org/abs/2504.07415)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Automated radiology report generation (RRG) holds potential to reduce radiologists' workload, especially as recent advancements in large language models (LLMs) enable the development of multimodal models for chest X-ray (CXR) report generation. However, multimodal LLMs (MLLMs) are resource-intensive, requiring vast datasets and substantial computational cost for training. To address these challenges, we propose a retrieval-augmented generation approach that leverages multimodal retrieval and LLMs to generate radiology reports while mitigating hallucinations and reducing computational demands. Our method uses LLMs to extract key phrases from radiology reports, effectively focusing on essential diagnostic information. Through exploring effective training strategies, including image encoder structure search, adding noise to text embeddings, and additional training objectives, we combine complementary pre-trained image encoders and adopt contrastive learning between text and semantic image embeddings. We evaluate our approach on MIMIC-CXR dataset, achieving state-of-the-art results on CheXbert metrics and competitive RadGraph F1 metric alongside MLLMs, without requiring LLM fine-tuning. Our method demonstrates robust generalization for multi-view RRG, making it suitable for comprehensive clinical applications.</li>
</ul>

<h3>Title: RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability</h3>
<ul>
<li><strong>Authors: </strong>Jonggwon Park, Soobum Kim, Byungmu Yoon, Kyoyun Choi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07416">https://arxiv.org/abs/2504.07416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07416">https://arxiv.org/pdf/2504.07416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07416]] RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability(https://arxiv.org/abs/2504.07416)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability, transformer, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Recent advancements in multi-modal models have significantly improved vision-language alignment in radiology. However, existing approaches struggle to effectively utilize complex radiology reports for learning, rely on low-resolution images, and offer limited interpretability in attention mechanisms. To address these challenges, we introduce RadZero, a novel similarity-based cross-attention framework for vision-language alignment in radiology with zero-shot multi-task capability. RadZero leverages large language models to extract minimal semantic sentences from radiology reports and employs a multi-positive contrastive learning strategy to effectively capture relationships between images and multiple relevant textual descriptions. It also utilizes a pre-trained vision encoder with additional trainable Transformer layers, allowing efficient high-resolution image processing. By computing similarity between text embeddings and local image patch features, RadZero enables zero-shot inference with similarity probability for classification and pixel-level cross-modal similarity maps for grounding and segmentation. Experimental results on public chest radiograph benchmarks show that RadZero outperforms state-of-the-art methods in zero-shot classification, grounding, and segmentation. Furthermore, cross-modal similarity map analysis highlights its potential for improving explainability in vision-language alignment. Additionally, qualitative evaluation demonstrates RadZero's capability for open-vocabulary semantic segmentation, further validating its effectiveness in medical imaging.</li>
</ul>

<h3>Title: ThermoStereoRT: Thermal Stereo Matching in Real Time via Knowledge Distillation and Attention-based Refinement</h3>
<ul>
<li><strong>Authors: </strong>Anning Hu, Ang Li, Xirui Jin, Danping Zou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07418">https://arxiv.org/abs/2504.07418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07418">https://arxiv.org/pdf/2504.07418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07418]] ThermoStereoRT: Thermal Stereo Matching in Real Time via Knowledge Distillation and Attention-based Refinement(https://arxiv.org/abs/2504.07418)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce ThermoStereoRT, a real-time thermal stereo matching method designed for all-weather conditions that recovers disparity from two rectified thermal stereo images, envisioning applications such as night-time drone surveillance or under-bed cleaning robots. Leveraging a lightweight yet powerful backbone, ThermoStereoRT constructs a 3D cost volume from thermal images and employs multi-scale attention mechanisms to produce an initial disparity map. To refine this map, we design a novel channel and spatial attention module. Addressing the challenge of sparse ground truth data in thermal imagery, we utilize knowledge distillation to boost performance without increasing computational demands. Comprehensive evaluations on multiple datasets demonstrate that ThermoStereoRT delivers both real-time capacity and robust accuracy, making it a promising solution for real-world deployment in various challenging environments. Our code will be released on this https URL</li>
</ul>

<h3>Title: Exploring Vulnerabilities and Concerns in Solana Smart Contracts</h3>
<ul>
<li><strong>Authors: </strong>Xiangfan Wu, Ju Xing, Xiaoqi Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07419">https://arxiv.org/abs/2504.07419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07419">https://arxiv.org/pdf/2504.07419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07419]] Exploring Vulnerabilities and Concerns in Solana Smart Contracts(https://arxiv.org/abs/2504.07419)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The Solana blockchain was created by Anatoly Yakovenko of Solana Labs and was introduced in 2017, employing a novel transaction verification method. However, at the same time, the innovation process introduced some new security issues. The frequent security incidents in smart contracts have not only caused enormous economic losses, but also undermined the credit system based on the blockchain. The security and reliability of smart contracts have become a new focus of research both domestically and abroad. This paper studies the current status of security analysis of Solana by researching Solana smart contract security analysis tools. This paper systematically sorts out the vulnerabilities existing in Solana smart contracts and gives examples of some vulnerabilities, summarizes the principles of security analysis tools, and comprehensively summarizes and details the security analysis tools in Solana smart contracts. The data of Solana smart contract security analysis tools are collected and compared with Ethereum, and the differences are analyzed and some tools are selected for practical testing.</li>
</ul>

<h3>Title: AgentAda: Skill-Adaptive Data Analytics for Tailored Insight Discovery</h3>
<ul>
<li><strong>Authors: </strong>Amirhossein Abaskohi, Amrutha Varshini Ramesh, Shailesh Nanisetty, Chirag Goel, David Vazquez, Christopher Pal, Spandana Gella, Giuseppe Carenini, Issam H. Laradji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07421">https://arxiv.org/abs/2504.07421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07421">https://arxiv.org/pdf/2504.07421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07421]] AgentAda: Skill-Adaptive Data Analytics for Tailored Insight Discovery(https://arxiv.org/abs/2504.07421)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>We introduce AgentAda, the first LLM-powered analytics agent that can learn and use new analytics skills to extract more specialized insights. Unlike existing methods that require users to manually decide which data analytics method to apply, AgentAda automatically identifies the skill needed from a library of analytical skills to perform the analysis. This also allows AgentAda to use skills that existing LLMs cannot perform out of the box. The library covers a range of methods, including clustering, predictive modeling, and NLP techniques like BERT, which allow AgentAda to handle complex analytics tasks based on what the user needs. AgentAda's dataset-to-insight extraction strategy consists of three key steps: (I) a question generator to generate queries relevant to the user's goal and persona, (II) a hybrid Retrieval-Augmented Generation (RAG)-based skill matcher to choose the best data analytics skill from the skill library, and (III) a code generator that produces executable code based on the retrieved skill's documentation to extract key patterns. We also introduce KaggleBench, a benchmark of curated notebooks across diverse domains, to evaluate AgentAda's performance. We conducted a human evaluation demonstrating that AgentAda provides more insightful analytics than existing tools, with 48.78% of evaluators preferring its analyses, compared to 27.67% for the unskilled agent. We also propose a novel LLM-as-a-judge approach that we show is aligned with human evaluation as a way to automate insight quality evaluation at larger scale.</li>
</ul>

<h3>Title: From Token to Line: Enhancing Code Generation with a Long-Term Perspective</h3>
<ul>
<li><strong>Authors: </strong>Tingwei Lu, Yangning Li, Liyuan Wang, Binghuai Lin, Jiwei Tang, Wanshi Xu, Hai-Tao Zheng, Yinghui Li, Bingxu An, Zhao Wei, Yong Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07433">https://arxiv.org/abs/2504.07433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07433">https://arxiv.org/pdf/2504.07433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07433]] From Token to Line: Enhancing Code Generation with a Long-Term Perspective(https://arxiv.org/abs/2504.07433)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The emergence of large language models (LLMs) has significantly promoted the development of code generation task, sparking a surge in pertinent literature. Current research is hindered by redundant generation results and a tendency to overfit local patterns in the short term. Although existing studies attempt to alleviate the issue by adopting a multi-token prediction strategy, there remains limited focus on choosing the appropriate processing length for generations. By analyzing the attention between tokens during the generation process of LLMs, it can be observed that the high spikes of the attention scores typically appear at the end of lines. This insight suggests that it is reasonable to treat each line of code as a fundamental processing unit and generate them sequentially. Inspired by this, we propose the \textbf{LSR-MCTS} algorithm, which leverages MCTS to determine the code line-by-line and select the optimal path. Further, we integrate a self-refine mechanism at each node to enhance diversity and generate higher-quality programs through error correction. Extensive experiments and comprehensive analyses on three public coding benchmarks demonstrate that our method outperforms the state-of-the-art performance approaches.</li>
</ul>

<h3>Title: Unifying and extending Diffusion Models through PDEs for solving Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Agnimitra Dasgupta, Alexsander Marciano da Cunha, Ali Fardisi, Mehrnegar Aminy, Brianna Binder, Bryan Shaddy, Assad A Oberai</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.CO, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07437">https://arxiv.org/abs/2504.07437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07437">https://arxiv.org/pdf/2504.07437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07437]] Unifying and extending Diffusion Models through PDEs for solving Inverse Problems(https://arxiv.org/abs/2504.07437)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as powerful generative tools with applications in computer vision and scientific machine learning (SciML), where they have been used to solve large-scale probabilistic inverse problems. Traditionally, these models have been derived using principles of variational inference, denoising, statistical signal processing, and stochastic differential equations. In contrast to the conventional presentation, in this study we derive diffusion models using ideas from linear partial differential equations and demonstrate that this approach has several benefits that include a constructive derivation of the forward and reverse processes, a unified derivation of multiple formulations and sampling strategies, and the discovery of a new class of models. We also apply the conditional version of these models to solving canonical conditional density estimation problems and challenging inverse problems. These problems help establish benchmarks for systematically quantifying the performance of different formulations and sampling strategies in this study, and for future studies. Finally, we identify and implement a mechanism through which a single diffusion model can be applied to measurements obtained from multiple measurement operators. Taken together, the contents of this manuscript provide a new understanding and several new directions in the application of diffusion models to solving physics-based inverse problems.</li>
</ul>

<h3>Title: Revisiting LLM Evaluation through Mechanism Interpretability: a New Metric and Model Utility Law</h3>
<ul>
<li><strong>Authors: </strong>Yixin Cao, Jiahao Ying, Yaoning Wang, Xipeng Qiu, Xuanjing Huang, Yugang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07440">https://arxiv.org/abs/2504.07440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07440">https://arxiv.org/pdf/2504.07440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07440]] Revisiting LLM Evaluation through Mechanism Interpretability: a New Metric and Model Utility Law(https://arxiv.org/abs/2504.07440)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become indispensable across academia, industry, and daily applications, yet current evaluation methods struggle to keep pace with their rapid development. In this paper, we analyze the core limitations of traditional evaluation pipelines and propose a novel metric, the Model Utilization Index (MUI), which introduces mechanism interpretability techniques to complement traditional performance metrics. MUI quantifies the extent to which a model leverages its capabilities to complete tasks. The core idea is that to assess an LLM's overall ability, we must evaluate not only its task performance but also the effort expended to achieve the outcome. Our extensive experiments reveal an inverse relationship between MUI and performance, from which we deduce a common trend observed in popular LLMs, which we term the Utility Law. Based on this, we derive four corollaries that address key challenges, including training judgement, the issue of data contamination, fairness in model comparison, and data diversity. We hope that our survey, novel metric, and utility law will foster mutual advancement in both evaluation and mechanism interpretability. Our code can be found at this https URL.</li>
</ul>

<h3>Title: WS-DETR: Robust Water Surface Object Detection through Vision-Radar Fusion with Detection Transformer</h3>
<ul>
<li><strong>Authors: </strong>Huilin Yin, Pengyu Wang, Senmao Li, Jun Yan, Daniel Watzenig</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07441">https://arxiv.org/abs/2504.07441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07441">https://arxiv.org/pdf/2504.07441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07441]] WS-DETR: Robust Water Surface Object Detection through Vision-Radar Fusion with Detection Transformer(https://arxiv.org/abs/2504.07441)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Robust object detection for Unmanned Surface Vehicles (USVs) in complex water environments is essential for reliable navigation and operation. Specifically, water surface object detection faces challenges from blurred edges and diverse object scales. Although vision-radar fusion offers a feasible solution, existing approaches suffer from cross-modal feature conflicts, which negatively affect model robustness. To address this problem, we propose a robust vision-radar fusion model WS-DETR. In particular, we first introduce a Multi-Scale Edge Information Integration (MSEII) module to enhance edge perception and a Hierarchical Feature Aggregator (HiFA) to boost multi-scale object detection in the encoder. Then, we adopt self-moving point representations for continuous convolution and residual connection to efficiently extract irregular features under the scenarios of irregular point cloud data. To further mitigate cross-modal conflicts, an Adaptive Feature Interactive Fusion (AFIF) module is introduced to integrate visual and radar features through geometric alignment and semantic fusion. Extensive experiments on the WaterScenes dataset demonstrate that WS-DETR achieves state-of-the-art (SOTA) performance, maintaining its superiority even under adverse weather and lighting conditions.</li>
</ul>

<h3>Title: LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Juzheng Zhang, Jiacheng You, Ashwinee Panda, Tom Goldstein</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07448">https://arxiv.org/abs/2504.07448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07448">https://arxiv.org/pdf/2504.07448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07448]] LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation(https://arxiv.org/abs/2504.07448)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices $A$ as random projections and sparsifies the matrices $B$ using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference. Code is available at: this https URL</li>
</ul>

<h3>Title: How Can Objects Help Video-Language Understanding?</h3>
<ul>
<li><strong>Authors: </strong>Zitian Tang, Shijie Wang, Junho Cho, Jaewook Yoo, Chen Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07454">https://arxiv.org/abs/2504.07454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07454">https://arxiv.org/pdf/2504.07454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07454]] How Can Objects Help Video-Language Understanding?(https://arxiv.org/abs/2504.07454)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>How multimodal large language models (MLLMs) perceive the visual world remains a mystery. To one extreme, object and relation modeling may be implicitly implemented with inductive biases, for example by treating objects as tokens. To the other extreme, empirical results reveal the surprising finding that simply performing visual captioning, which tends to ignore spatial configuration of the objects, serves as a strong baseline for video understanding. We aim to answer the question: how can objects help video-language understanding in MLLMs? We tackle the question from the object representation and adaptation perspectives. Specifically, we investigate the trade-off between representation expressiveness (e.g., distributed versus symbolic) and integration difficulty (e.g., data-efficiency when learning the adapters). Through extensive evaluations on five video question answering datasets, we confirm that explicit integration of object-centric representation remains necessary, and the symbolic objects can be most easily integrated while being performant for question answering. We hope our findings can encourage the community to explore the explicit integration of perception modules into MLLM design. Our code and models will be publicly released.</li>
</ul>

<h3>Title: CyberAlly: Leveraging LLMs and Knowledge Graphs to Empower Cyber Defenders</h3>
<ul>
<li><strong>Authors: </strong>Minjune Kim, Jeff Wang, Kristen Moore, Diksha Goel, Derui Wang, Ahmad Mohsin, Ahmed Ibrahim, Robin Doss, Seyit Camtepe, Helge Janicke</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07457">https://arxiv.org/abs/2504.07457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07457">https://arxiv.org/pdf/2504.07457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07457]] CyberAlly: Leveraging LLMs and Knowledge Graphs to Empower Cyber Defenders(https://arxiv.org/abs/2504.07457)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack</a></li>
<li><strong>Abstract: </strong>The increasing frequency and sophistication of cyberattacks demand innovative approaches to strengthen defense capabilities. Training on live infrastructure poses significant risks to organizations, making secure, isolated cyber ranges an essential tool for conducting Red vs. Blue Team training events. These events enable security teams to refine their skills without impacting operational environments. While such training provides a strong foundation, the ever-evolving nature of cyber threats necessitates additional support for effective defense. To address this challenge, we introduce CyberAlly, a knowledge graph-enhanced AI assistant designed to enhance the efficiency and effectiveness of Blue Teams during incident response. Integrated into our cyber range alongside an open-source SIEM platform, CyberAlly monitors alerts, tracks Blue Team actions, and suggests tailored mitigation recommendations based on insights from prior Red vs. Blue Team exercises. This demonstration highlights the feasibility and impact of CyberAlly in augmenting incident response and equipping defenders to tackle evolving threats with greater precision and confidence.</li>
</ul>

<h3>Title: Beyond LLMs: A Linguistic Approach to Causal Graph Generation from Narrative Texts</h3>
<ul>
<li><strong>Authors: </strong>Zehan Li, Ruhua Pan, Xinyu Pi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07459">https://arxiv.org/abs/2504.07459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07459">https://arxiv.org/pdf/2504.07459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07459]] Beyond LLMs: A Linguistic Approach to Causal Graph Generation from Narrative Texts(https://arxiv.org/abs/2504.07459)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose a novel framework for generating causal graphs from narrative texts, bridging high-level causality and detailed event-specific relationships. Our method first extracts concise, agent-centered vertices using large language model (LLM)-based summarization. We introduce an "Expert Index," comprising seven linguistically informed features, integrated into a Situation-Task-Action-Consequence (STAC) classification model. This hybrid system, combining RoBERTa embeddings with the Expert Index, achieves superior precision in causal link identification compared to pure LLM-based approaches. Finally, a structured five-iteration prompting process refines and constructs connected causal graphs. Experiments on 100 narrative chapters and short stories demonstrate that our approach consistently outperforms GPT-4o and Claude 3.5 in causal graph quality, while maintaining readability. The open-source tool provides an interpretable, efficient solution for capturing nuanced causal chains in narratives.</li>
</ul>

<h3>Title: Learning Universal Features for Generalizable Image Forgery Localization</h3>
<ul>
<li><strong>Authors: </strong>Hengrun Zhao, Yunzhi Zhuge, Yifan Wang, Lijun Wang, Huchuan Lu, Yu Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07462">https://arxiv.org/abs/2504.07462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07462">https://arxiv.org/pdf/2504.07462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07462]] Learning Universal Features for Generalizable Image Forgery Localization(https://arxiv.org/abs/2504.07462)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In recent years, advanced image editing and generation methods have rapidly evolved, making detecting and locating forged image content increasingly challenging. Most existing image forgery detection methods rely on identifying the edited traces left in the image. However, because the traces of different forgeries are distinct, these methods can identify familiar forgeries included in the training data but struggle to handle unseen ones. In response, we present an approach for Generalizable Image Forgery Localization (GIFL). Once trained, our model can detect both seen and unseen forgeries, providing a more practical and efficient solution to counter false information in the era of generative AI. Our method focuses on learning general features from the pristine content rather than traces of specific forgeries, which are relatively consistent across different types of forgeries and therefore can be used as universal features to locate unseen forgeries. Additionally, as existing image forgery datasets are still dominated by traditional hand-crafted forgeries, we construct a new dataset consisting of images edited by various popular deep generative image editing methods to further encourage research in detecting images manipulated by deep generative models. Extensive experimental results show that the proposed approach outperforms state-of-the-art methods in the detection of unseen forgeries and also demonstrates competitive results for seen forgeries. The code and dataset are available at this https URL.</li>
</ul>

<h3>Title: Multi-Modal Data Fusion for Moisture Content Prediction in Apple Drying</h3>
<ul>
<li><strong>Authors: </strong>Shichen Li, Chenhui Shao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07465">https://arxiv.org/abs/2504.07465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07465">https://arxiv.org/pdf/2504.07465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07465]] Multi-Modal Data Fusion for Moisture Content Prediction in Apple Drying(https://arxiv.org/abs/2504.07465)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Fruit drying is widely used in food manufacturing to reduce product moisture, ensure product safety, and extend product shelf life. Accurately predicting final moisture content (MC) is critically needed for quality control of drying processes. State-of-the-art methods can build deterministic relationships between process parameters and MC, but cannot adequately account for inherent process variabilities that are ubiquitous in fruit drying. To address this gap, this paper presents a novel multi-modal data fusion framework to effectively fuse two modalities of data: tabular data (process parameters) and high-dimensional image data (images of dried apple slices) to enable accurate MC prediction. The proposed modeling architecture permits flexible adjustment of information portion from tabular and image data modalities. Experimental validation shows that the multi-modal approach improves predictive accuracy substantially compared to state-of-the-art methods. The proposed method reduces root-mean-squared errors by 19.3%, 24.2%, and 15.2% over tabular-only, image-only, and standard tabular-image fusion models, respectively. Furthermore, it is demonstrated that our method is robust in varied tabular-image ratios and capable of effectively capturing inherent small-scale process variabilities. The proposed framework is extensible to a variety of other drying technologies.</li>
</ul>

<h3>Title: Defense against Prompt Injection Attacks via Mixture of Encodings</h3>
<ul>
<li><strong>Authors: </strong>Ruiyi Zhang, David Sullivan, Kyle Jackson, Pengtao Xie, Mei Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07467">https://arxiv.org/abs/2504.07467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07467">https://arxiv.org/pdf/2504.07467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07467]] Defense against Prompt Injection Attacks via Mixture of Encodings(https://arxiv.org/abs/2504.07467)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have emerged as a dominant approach for a wide range of NLP tasks, with their access to external information further enhancing their capabilities. However, this introduces new vulnerabilities, known as prompt injection attacks, where external content embeds malicious instructions that manipulate the LLM's output. Recently, the Base64 defense has been recognized as one of the most effective methods for reducing success rate of prompt injection attacks. Despite its efficacy, this method can degrade LLM performance on certain NLP tasks. To address this challenge, we propose a novel defense mechanism: mixture of encodings, which utilizes multiple character encodings, including Base64. Extensive experimental results show that our method achieves one of the lowest attack success rates under prompt injection attacks, while maintaining high performance across all NLP tasks, outperforming existing character encoding-based defense methods. This underscores the effectiveness of our mixture of encodings strategy for both safety and task performance metrics.</li>
</ul>

<h3>Title: Transformer-Based Temporal Information Extraction and Application: A Review</h3>
<ul>
<li><strong>Authors: </strong>Xin Su, Phillip Howard, Steven Bethard</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07470">https://arxiv.org/abs/2504.07470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07470">https://arxiv.org/pdf/2504.07470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07470]] Transformer-Based Temporal Information Extraction and Application: A Review(https://arxiv.org/abs/2504.07470)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Temporal information extraction (IE) aims to extract structured temporal information from unstructured text, thereby uncovering the implicit timelines within. This technique is applied across domains such as healthcare, newswire, and intelligence analysis, aiding models in these areas to perform temporal reasoning and enabling human users to grasp the temporal structure of text. Transformer-based pre-trained language models have produced revolutionary advancements in natural language processing, demonstrating exceptional performance across a multitude of tasks. Despite the achievements garnered by Transformer-based approaches in temporal IE, there is a lack of comprehensive reviews on these endeavors. In this paper, we aim to bridge this gap by systematically summarizing and analyzing the body of work on temporal IE using Transformers while highlighting potential future research directions.</li>
</ul>

<h3>Title: Traversal Learning Coordination For Lossless And Efficient Distributed Learning</h3>
<ul>
<li><strong>Authors: </strong>Erdenebileg Batbaatar, Jeonggeol Kim, Yongcheol Kim, Young Yoon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07471">https://arxiv.org/abs/2504.07471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07471">https://arxiv.org/pdf/2504.07471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07471]] Traversal Learning Coordination For Lossless And Efficient Distributed Learning(https://arxiv.org/abs/2504.07471)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce Traversal Learning (TL), a novel approach designed to address the problem of decreased quality encountered in popular distributed learning (DL) paradigms such as Federated Learning (FL), Split Learning (SL), and SplitFed Learning (SFL). Traditional FL experiences from an accuracy drop during aggregation due to its averaging function, while SL and SFL face increased loss due to the independent gradient updates on each split network. TL adopts a unique strategy where the model traverses the nodes during forward propagation (FP) and performs backward propagation (BP) on the orchestrator, effectively implementing centralized learning (CL) principles within a distributed environment. The orchestrator is tasked with generating virtual batches and planning the sequential node visits of the model during FP, aligning them with the ordered index of the data within these batches. We conducted experiments on six datasets representing diverse characteristics across various domains. Our evaluation demonstrates that TL is on par with classic CL approaches in terms of accurate inference, thereby offering a viable and robust solution for DL tasks. TL outperformed other DL methods and improved accuracy by 7.85% for independent and identically distributed (IID) datasets, macro F1-score by 1.06% for non-IID datasets, accuracy by 2.60% for text classification, and AUC by 3.88% and 4.54% for medical and financial datasets, respectively. By effectively preserving data privacy while maintaining performance, TL represents a significant advancement in DL methodologies.</li>
</ul>

<h3>Title: CMEdataset Advancing China Map Detection and Standardization with Digital Image Resources</h3>
<ul>
<li><strong>Authors: </strong>Yan Xu, Zhenqiang Zhang, Zhiwei Zhou, Liting Geng, Yue Li, Jintao Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07476">https://arxiv.org/abs/2504.07476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07476">https://arxiv.org/pdf/2504.07476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07476]] CMEdataset Advancing China Map Detection and Standardization with Digital Image Resources(https://arxiv.org/abs/2504.07476)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Digital images of Chinas maps play a crucial role in map detection, particularly in ensuring national sovereignty, territorial integrity, and map compliance. However, there is currently no publicly available dataset specifically dedicated to problematic maps the CME dataset. Existing datasets primarily focus on general map data and are insufficient for effectively identifying complex issues such as national boundary misrepresentations, missing elements, and blurred boundaries. Therefore, this study creates a Problematic Map dataset that covers five key problem areas, aiming to provide diverse samples for problematic map detection technologies, support high-precision map compliance detection, and enhance map data quality and timeliness. This dataset not only provides essential resources for map compliance, national security monitoring, and map updates, but also fosters innovation and application of related technologies.</li>
</ul>

<h3>Title: Intelligent DoS and DDoS Detection: A Hybrid GRU-NTM Approach to Network Security</h3>
<ul>
<li><strong>Authors: </strong>Caroline Panggabean, Chandrasekar Venkatachalam, Priyanka Shah, Sincy John, Renuka Devi P, Shanmugavalli Venkatachalam</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07478">https://arxiv.org/abs/2504.07478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07478">https://arxiv.org/pdf/2504.07478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07478]] Intelligent DoS and DDoS Detection: A Hybrid GRU-NTM Approach to Network Security(https://arxiv.org/abs/2504.07478)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Detecting Denial of Service (DoS) and Distributed Denial of Service (DDoS) attacks remains a critical challenge in cybersecurity. This research introduces a hybrid deep learning model combining Gated Recurrent Units (GRUs) and a Neural Turing Machine (NTM) for enhanced intrusion detection. Trained on the UNSW-NB15 and BoT-IoT datasets, the model employs GRU layers for sequential data processing and an NTM for long-term pattern recognition. The proposed approach achieves 99% accuracy in distinguishing between normal, DoS, and DDoS traffic. These findings offer promising advancements in real-time threat detection and contribute to improved network security across various domains.</li>
</ul>

<h3>Title: Geological Inference from Textual Data using Word Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Nanmanas Linphrachaya, Irving G√≥mez-M√©ndez, Adil Siripatana</a></li>
<li><strong>Subjects: </strong>cs.CL, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07490">https://arxiv.org/abs/2504.07490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07490">https://arxiv.org/pdf/2504.07490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07490]] Geological Inference from Textual Data using Word Embeddings(https://arxiv.org/abs/2504.07490)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This research explores the use of Natural Language Processing (NLP) techniques to locate geological resources, with a specific focus on industrial minerals. By using word embeddings trained with the GloVe model, we extract semantic relationships between target keywords and a corpus of geological texts. The text is filtered to retain only words with geographical significance, such as city names, which are then ranked by their cosine similarity to the target keyword. Dimensional reduction techniques, including Principal Component Analysis (PCA), Autoencoder, Variational Autoencoder (VAE), and VAE with Long Short-Term Memory (VAE-LSTM), are applied to enhance feature extraction and improve the accuracy of semantic relations. For benchmarking, we calculate the proximity between the ten cities most semantically related to the target keyword and identified mine locations using the haversine equation. The results demonstrate that combining NLP with dimensional reduction techniques provides meaningful insights into the spatial distribution of natural resources. Although the result shows to be in the same region as the supposed location, the accuracy has room for improvement.</li>
</ul>

<h3>Title: Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving</h3>
<ul>
<li><strong>Authors: </strong>Shihong Gao, Xin Zhang, Yanyan Shen, Lei Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07494">https://arxiv.org/abs/2504.07494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07494">https://arxiv.org/pdf/2504.07494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07494]] Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving(https://arxiv.org/abs/2504.07494)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) inference serving systems are essential to various LLM-based applications. As demand for LLM services continues to grow, scaling these systems to handle high request rates while meeting latency Service-Level Objectives (SLOs), referred to as effective throughput, becomes critical. However, existing systems often struggle to improve effective throughput, primarily due to a significant decline in Time To First Token (TTFT) SLO attainment. We identify two major causes of this bottleneck: (1) memory-intensive KV cache that limits batch size expansion under GPU memory constraints, and (2) rigid batch composition enforced by the default First-Come-First-Serve scheduling policy. In this paper, we introduce Apt-Serve, a scalable framework designed to enhance effective throughput in LLM inference serving. Apt-Serve features a new hybrid cache scheme that combines KV cache with a memory-efficient hidden cache for reusable input hidden state vectors, allowing large batch sizes and improving request concurrency. Based on the hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism that dynamically optimizes batch composition. We formally define the adaptive scheduling optimization problem and propose an efficient algorithm with theoretical guarantees. Extensive evaluations on three real-world datasets and LLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up to 8.8x improvement in effective throughput compared to the state-of-the-art inference serving systems.</li>
</ul>

<h3>Title: Event Signal Filtering via Probability Flux Estimation</h3>
<ul>
<li><strong>Authors: </strong>Jinze Chen, Wei Zhai, Yang Cao, Bin Li, Zheng-Jun Zha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07503">https://arxiv.org/abs/2504.07503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07503">https://arxiv.org/pdf/2504.07503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07503]] Event Signal Filtering via Probability Flux Estimation(https://arxiv.org/abs/2504.07503)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Events offer a novel paradigm for capturing scene dynamics via asynchronous sensing, but their inherent randomness often leads to degraded signal quality. Event signal filtering is thus essential for enhancing fidelity by reducing this internal randomness and ensuring consistent outputs across diverse acquisition conditions. Unlike traditional time series that rely on fixed temporal sampling to capture steady-state behaviors, events encode transient dynamics through polarity and event intervals, making signal modeling significantly more complex. To address this, the theoretical foundation of event generation is revisited through the lens of diffusion processes. The state and process information within events is modeled as continuous probability flux at threshold boundaries of the underlying irradiance diffusion. Building on this insight, a generative, online filtering framework called Event Density Flow Filter (EDFilter) is introduced. EDFilter estimates event correlation by reconstructing the continuous probability flux from discrete events using nonparametric kernel smoothing, and then resamples filtered events from this flux. To optimize fidelity over time, spatial and temporal kernels are employed in a time-varying optimization framework. A fast recursive solver with O(1) complexity is proposed, leveraging state-space models and lookup tables for efficient likelihood computation. Furthermore, a new real-world benchmark Rotary Event Dataset (RED) is released, offering microsecond-level ground truth irradiance for full-reference event filtering evaluation. Extensive experiments validate EDFilter's performance across tasks like event filtering, super-resolution, and direct event-based blob tracking. Significant gains in downstream applications such as SLAM and video reconstruction underscore its robustness and effectiveness.</li>
</ul>

<h3>Title: GPT Carry-On: Training Foundation Model for Customization Could Be Simple, Scalable and Affordable</h3>
<ul>
<li><strong>Authors: </strong>Jianqiao Wangni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07513">https://arxiv.org/abs/2504.07513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07513">https://arxiv.org/pdf/2504.07513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07513]] GPT Carry-On: Training Foundation Model for Customization Could Be Simple, Scalable and Affordable(https://arxiv.org/abs/2504.07513)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Modern large language foundation models (LLM) have now entered the daily lives of millions of users. We ask a natural question whether it is possible to customize LLM for every user or every task. From system and industrial economy consideration, general continue-training or fine-tuning still require substantial computation and memory of training GPU nodes, whereas most inference nodes under deployment, possibly with lower-end GPUs, are configured to make forward pass fastest possible. We propose a framework to take full advantages of existing LLMs and systems of online service. We train an additional branch of transformer blocks on the final-layer embedding of pretrained LLMs, which is the base, then a carry-on module merge the base models to compose a customized LLM. We can mix multiple layers, or multiple LLMs specialized in different domains such as chat, coding, math, to form a new mixture of LLM that best fit a new task. As the base model don't need to update parameters, we are able to outsource most computation of the training job on inference nodes, and only train a lightweight carry-on on training nodes, where we consume less than 1GB GPU memory to train a 100M carry-on layer on 30B LLM. We tested Qwen and DeepSeek opensourced models for continue-pretraining and got faster loss convergence. We use it to improve solving math questions with extremely small computation and model size, with 1000 data samples of chain-of-thoughts, and as small as 1 MB parameters of two layer layer carry-on, and the results are promising.</li>
</ul>

<h3>Title: VideoExpert: Augmented LLM for Temporal-Sensitive Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Henghao Zhao, Ge-Peng Ji, Rui Yan, Huan Xiong, Zechao Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07519">https://arxiv.org/abs/2504.07519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07519">https://arxiv.org/pdf/2504.07519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07519]] VideoExpert: Augmented LLM for Temporal-Sensitive Video Understanding(https://arxiv.org/abs/2504.07519)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The core challenge in video understanding lies in perceiving dynamic content changes over time. However, multimodal large language models struggle with temporal-sensitive video tasks, which requires generating timestamps to mark the occurrence of specific events. Existing strategies require MLLMs to generate absolute or relative timestamps directly. We have observed that those MLLMs tend to rely more on language patterns than visual cues when generating timestamps, affecting their performance. To address this problem, we propose VideoExpert, a general-purpose MLLM suitable for several temporal-sensitive video tasks. Inspired by the expert concept, VideoExpert integrates two parallel modules: the Temporal Expert and the Spatial Expert. The Temporal Expert is responsible for modeling time sequences and performing temporal grounding. It processes high-frame-rate yet compressed tokens to capture dynamic variations in videos and includes a lightweight prediction head for precise event localization. The Spatial Expert focuses on content detail analysis and instruction following. It handles specially designed spatial tokens and language input, aiming to generate content-related responses. These two experts collaborate seamlessly via a special token, ensuring coordinated temporal grounding and content generation. Notably, the Temporal and Spatial Experts maintain independent parameter sets. By offloading temporal grounding from content generation, VideoExpert prevents text pattern biases in timestamp predictions. Moreover, we introduce a Spatial Compress module to obtain spatial tokens. This module filters and compresses patch tokens while preserving key information, delivering compact yet detail-rich input for the Spatial Expert. Extensive experiments demonstrate the effectiveness and versatility of the VideoExpert.</li>
</ul>

<h3>Title: Adversarial Subspace Generation for Outlier Detection in High-Dimensional Data</h3>
<ul>
<li><strong>Authors: </strong>Jose Cribeiro-Ramallo, Federico Matteucci, Paul Enciu, Alexander Jenke, Vadim Arzamasov, Thorsten Strufe, Klemens B√∂hm</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07522">https://arxiv.org/abs/2504.07522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07522">https://arxiv.org/pdf/2504.07522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07522]] Adversarial Subspace Generation for Outlier Detection in High-Dimensional Data(https://arxiv.org/abs/2504.07522)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Outlier detection in high-dimensional tabular data is challenging since data is often distributed across multiple lower-dimensional subspaces -- a phenomenon known as the Multiple Views effect (MV). This effect led to a large body of research focused on mining such subspaces, known as subspace selection. However, as the precise nature of the MV effect was not well understood, traditional methods had to rely on heuristic-driven search schemes that struggle to accurately capture the true structure of the data. Properly identifying these subspaces is critical for unsupervised tasks such as outlier detection or clustering, where misrepresenting the underlying data structure can hinder the performance. We introduce Myopic Subspace Theory (MST), a new theoretical framework that mathematically formulates the Multiple Views effect and writes subspace selection as a stochastic optimization problem. Based on MST, we introduce V-GAN, a generative method trained to solve such an optimization problem. This approach avoids any exhaustive search over the feature space while ensuring that the intrinsic data structure is preserved. Experiments on 42 real-world datasets show that using V-GAN subspaces to build ensemble methods leads to a significant increase in one-class classification performance -- compared to existing subspace selection, feature selection, and embedding methods. Further experiments on synthetic data show that V-GAN identifies subspaces more accurately while scaling better than other relevant subspace selection methods. These results confirm the theoretical guarantees of our approach and also highlight its practical viability in high-dimensional settings.</li>
</ul>

<h3>Title: Supervised Optimism Correction: Be Confident When LLMs Are Sure</h3>
<ul>
<li><strong>Authors: </strong>Junjie Zhang, Rushuai Yang, Shunyu Liu, Ting-En Lin, Fei Huang, Yi Chen, Yongbin Li, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07527">https://arxiv.org/abs/2504.07527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07527">https://arxiv.org/pdf/2504.07527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07527]] Supervised Optimism Correction: Be Confident When LLMs Are Sure(https://arxiv.org/abs/2504.07527)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this work, we establish a novel theoretical connection between supervised fine-tuning and offline reinforcement learning under the token-level Markov decision process, revealing that large language models indeed learn an implicit $Q$-function for inference. Through this theoretical lens, we demonstrate that the widely used beam search method suffers from unacceptable over-optimism, where inference errors are inevitably amplified due to inflated $Q$-value estimations of suboptimal steps. To address this limitation, we propose Supervised Optimism Correction(SOC), which introduces a simple yet effective auxiliary loss for token-level $Q$-value estimations during supervised fine-tuning. Specifically, the auxiliary loss employs implicit value regularization to boost model confidence in expert-demonstrated responses, thereby suppressing over-optimism toward insufficiently supervised responses. Extensive experiments on mathematical reasoning benchmarks, including GSM8K, MATH, and GAOKAO, showcase the superiority of the proposed SOC with beam search across a series of open-source models.</li>
</ul>

<h3>Title: SydneyScapes: Image Segmentation for Australian Environments</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Lyu, Julie Stephany Berrio, Mao Shan, Stewart Worrall</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07542">https://arxiv.org/abs/2504.07542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07542">https://arxiv.org/pdf/2504.07542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07542]] SydneyScapes: Image Segmentation for Australian Environments(https://arxiv.org/abs/2504.07542)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Autonomous Vehicles (AVs) are being partially deployed and tested across various global locations, including China, the USA, Germany, France, Japan, Korea, and the UK, but with limited demonstrations in Australia. The integration of machine learning (ML) into AV perception systems highlights the need for locally labelled datasets to develop and test algorithms in specific environments. To address this, we introduce SydneyScapes - a dataset tailored for computer vision tasks of image semantic, instance, and panoptic segmentation. This dataset, collected from Sydney and surrounding cities in New South Wales (NSW), Australia, consists of 756 images with high-quality pixel-level annotations. It is designed to assist AV industry and researchers by providing annotated data and tools for algorithm development, testing, and deployment in the Australian context. Additionally, we offer benchmarking results using state-of-the-art algorithms to establish reference points for future research and development. The dataset is publicly available at this https URL.</li>
</ul>

<h3>Title: MUFFLER: Secure Tor Traffic Obfuscation with Dynamic Connection Shuffling and Splitting</h3>
<ul>
<li><strong>Authors: </strong>Minjae Seo, Myoungsung You, Jaehan Kim, Taejune Park, Seungwon Shin, Jinwoo Kim</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07543">https://arxiv.org/abs/2504.07543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07543">https://arxiv.org/pdf/2504.07543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07543]] MUFFLER: Secure Tor Traffic Obfuscation with Dynamic Connection Shuffling and Splitting(https://arxiv.org/abs/2504.07543)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, defense, attack</a></li>
<li><strong>Abstract: </strong>Tor, a widely utilized privacy network, enables anonymous communication but is vulnerable to flow correlation attacks that deanonymize users by correlating traffic patterns from Tor's ingress and egress segments. Various defenses have been developed to mitigate these attacks; however, they have two critical limitations: (i) significant network overhead during obfuscation and (ii) a lack of dynamic obfuscation for egress segments, exposing traffic patterns to adversaries. In response, we introduce MUFFLER, a novel connection-level traffic obfuscation system designed to secure Tor egress traffic. It dynamically maps real connections to a distinct set of virtual connections between the final Tor nodes and targeted services, either public or hidden. This approach creates egress traffic patterns fundamentally different from those at ingress segments without adding intentional padding bytes or timing delays. The mapping of real and virtual connections is adjusted in real-time based on ongoing network conditions, thwarting adversaries' efforts to detect egress traffic patterns. Extensive evaluations show that MUFFLER mitigates powerful correlation attacks with a TPR of 1% at an FPR of 10^-2 while imposing only a 2.17% bandwidth overhead. Moreover, it achieves up to 27x lower latency overhead than existing solutions and seamlessly integrates with the current Tor architecture.</li>
</ul>

<h3>Title: STeP: A General and Scalable Framework for Solving Video Inverse Problems with Spatiotemporal Diffusion Priors</h3>
<ul>
<li><strong>Authors: </strong>Bingliang Zhang, Zihui Wu, Berthy T. Feng, Yang Song, Yisong Yue, Katherine L. Bouman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07549">https://arxiv.org/abs/2504.07549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07549">https://arxiv.org/pdf/2504.07549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07549]] STeP: A General and Scalable Framework for Solving Video Inverse Problems with Spatiotemporal Diffusion Priors(https://arxiv.org/abs/2504.07549)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We study how to solve general Bayesian inverse problems involving videos using diffusion model priors. While it is desirable to use a video diffusion prior to effectively capture complex temporal relationships, due to the computational and data requirements of training such a model, prior work has instead relied on image diffusion priors on single frames combined with heuristics to enforce temporal consistency. However, these approaches struggle with faithfully recovering the underlying temporal relationships, particularly for tasks with high temporal uncertainty. In this paper, we demonstrate the feasibility of practical and accessible spatiotemporal diffusion priors by fine-tuning latent video diffusion models from pretrained image diffusion models using limited videos in specific domains. Leveraging this plug-and-play spatiotemporal diffusion prior, we introduce a general and scalable framework for solving video inverse problems. We then apply our framework to two challenging scientific video inverse problems--black hole imaging and dynamic MRI. Our framework enables the generation of diverse, high-fidelity video reconstructions that not only fit observations but also recover multi-modal solutions. By incorporating a spatiotemporal diffusion prior, we significantly improve our ability to capture complex temporal relationships in the data while also enhancing spatial fidelity.</li>
</ul>

<h3>Title: Using LLMs for Analyzing AIS Data</h3>
<ul>
<li><strong>Authors: </strong>Gaspard Mertends, Gilles Dejaegere, Mahmoud Sakr</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07557">https://arxiv.org/abs/2504.07557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07557">https://arxiv.org/pdf/2504.07557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07557]] Using LLMs for Analyzing AIS Data(https://arxiv.org/abs/2504.07557)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent research in Large Language Models (LLMs), has had a profound impact across various fields, including mobility data science. This paper explores the and experiment with different approaches to using LLMs for analyzing AIS data. We propose a set of carefully designed queries to assess the reasoning capabilities of LLMs in this kind of tasks. Further, we experiment with four different methods: (1) using LLMs as a natural language interface to a spatial database, (2) reasoning on raw data, (3) reasoning on compressed trajectories, and (4) reasoning on semantic trajectories. We investigate the strengths and weaknesses for the four methods, and discuss the findings. The goal is to provide valuable insights for both researchers and practitioners on selecting the most appropriate LLM-based method depending on their specific data analysis objectives.</li>
</ul>

<h3>Title: Diffusion Transformers for Tabular Data Time Series Generation</h3>
<ul>
<li><strong>Authors: </strong>Fabrizio Garuti, Enver Sangineto, Simone Luetto, Lorenzo Forni, Rita Cucchiara</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07566">https://arxiv.org/abs/2504.07566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07566">https://arxiv.org/pdf/2504.07566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07566]] Diffusion Transformers for Tabular Data Time Series Generation(https://arxiv.org/abs/2504.07566)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Tabular data generation has recently attracted a growing interest due to its different application scenarios. However, generating time series of tabular data, where each element of the series depends on the others, remains a largely unexplored domain. This gap is probably due to the difficulty of jointly solving different problems, the main of which are the heterogeneity of tabular data (a problem common to non-time-dependent approaches) and the variable length of a time series. In this paper, we propose a Diffusion Transformers (DiTs) based approach for tabular data series generation. Inspired by the recent success of DiTs in image and video generation, we extend this framework to deal with heterogeneous data and variable-length sequences. Using extensive experiments on six datasets, we show that the proposed approach outperforms previous work by a large margin.</li>
</ul>

<h3>Title: Benchmarking Image Embeddings for E-Commerce: Evaluating Off-the Shelf Foundation Models, Fine-Tuning Strategies and Practical Trade-offs</h3>
<ul>
<li><strong>Authors: </strong>Urszula Czerwinska, Cenk Bircanoglu, Jeremy Chamoux</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CE, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07567">https://arxiv.org/abs/2504.07567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07567">https://arxiv.org/pdf/2504.07567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07567]] Benchmarking Image Embeddings for E-Commerce: Evaluating Off-the Shelf Foundation Models, Fine-Tuning Strategies and Practical Trade-offs(https://arxiv.org/abs/2504.07567)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We benchmark foundation models image embeddings for classification and retrieval in e-Commerce, evaluating their suitability for real-world applications. Our study spans embeddings from pre-trained convolutional and transformer models trained via supervised, self-supervised, and text-image contrastive learning. We assess full fine-tuning and transfer learning (top-tuning) on six diverse e-Commerce datasets: fashion, consumer goods, cars, food, and retail. Results show full fine-tuning consistently performs well, while text-image and self-supervised embeddings can match its performance with less training. While supervised embeddings remain stable across architectures, SSL and contrastive embeddings vary significantly, often benefiting from top-tuning. Top-tuning emerges as an efficient alternative to full fine-tuning, reducing computational costs. We also explore cross-tuning, noting its impact depends on dataset characteristics. Our findings offer practical guidelines for embedding selection and fine-tuning strategies, balancing efficiency and performance.</li>
</ul>

<h3>Title: Privacy-Preserving Vertical K-Means Clustering</h3>
<ul>
<li><strong>Authors: </strong>Federico Mazzone, Trevor Brown, Florian Kerschbaum, Kevin H. Wilson, Maarten Everts, Florian Hahn, Andreas Peter</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07578">https://arxiv.org/abs/2504.07578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07578">https://arxiv.org/pdf/2504.07578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07578]] Privacy-Preserving Vertical K-Means Clustering(https://arxiv.org/abs/2504.07578)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>Clustering is a fundamental data processing task used for grouping records based on one or more features. In the vertically partitioned setting, data is distributed among entities, with each holding only a subset of those features. A key challenge in this scenario is that computing distances between records requires access to all distributed features, which may be privacy-sensitive and cannot be directly shared with other parties. The goal is to compute the joint clusters while preserving the privacy of each entity's dataset. Existing solutions using secret sharing or garbled circuits implement privacy-preserving variants of Lloyd's algorithm but incur high communication costs, scaling as O(nkt), where n is the number of data points, k the number of clusters, and t the number of rounds. These methods become impractical for large datasets or several parties, limiting their use to LAN settings only. On the other hand, a different line of solutions rely on differential privacy (DP) to outsource the local features of the parties to a central server. However, they often significantly degrade the utility of the clustering outcome due to excessive noise. In this work, we propose a novel solution based on homomorphic encryption and DP, reducing communication complexity to O(n+kt). In our method, parties securely outsource their features once, allowing a computing party to perform clustering operations under encryption. DP is applied only to the clusters' centroids, ensuring privacy with minimal impact on utility. Our solution clusters 100,000 two-dimensional points into five clusters using only 73MB of communication, compared to 101GB for existing works, and completes in just under 3 minutes on a 100Mbps network, whereas existing works take over 1 day. This makes our solution practical even for WAN deployments, all while maintaining accuracy comparable to plaintext k-means algorithms.</li>
</ul>

<h3>Title: Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Patrick Fernandes, Sweta Agrawal, Emmanouil Zaranis, Andr√© F.T. Martins, Graham Neubig</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07583">https://arxiv.org/abs/2504.07583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07583">https://arxiv.org/pdf/2504.07583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07583]] Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with Question Answering(https://arxiv.org/abs/2504.07583)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Despite the steady progress in machine translation evaluation, existing automatic metrics struggle to capture how well meaning is preserved beyond sentence boundaries. We posit that reliance on a single intrinsic quality score, trained to mimic human judgments, might be insufficient for evaluating translations of long, complex passages, and a more ``pragmatic'' approach that assesses how accurately key information is conveyed by a translation in context is needed. We introduce TREQA (Translation Evaluation via Question-Answering), a framework that extrinsically evaluates translation quality by assessing how accurately candidate translations answer reading comprehension questions that target key information in the original source or reference texts. In challenging domains that require long-range understanding, such as literary texts, we show that TREQA is competitive with and, in some cases, outperforms state-of-the-art neural and LLM-based metrics in ranking alternative paragraph-level translations, despite never being explicitly optimized to correlate with human judgments. Furthermore, the generated questions and answers offer interpretability: empirical analysis shows that they effectively target translation errors identified by experts in evaluated datasets. Our code is available at this https URL</li>
</ul>

<h3>Title: DWFS-Obfuscation: Dynamic Weighted Feature Selection for Robust Malware Familial Classification under Obfuscation</h3>
<ul>
<li><strong>Authors: </strong>Xingyuan Wei, Zijun Cheng, Ning Li, Qiujian Lv, Ziyang Yu, Degang Sun</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07590">https://arxiv.org/abs/2504.07590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07590">https://arxiv.org/pdf/2504.07590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07590]] DWFS-Obfuscation: Dynamic Weighted Feature Selection for Robust Malware Familial Classification under Obfuscation(https://arxiv.org/abs/2504.07590)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Due to its open-source nature, the Android operating system has consistently been a primary target for attackers. Learning-based methods have made significant progress in the field of Android malware detection. However, traditional detection methods based on static features struggle to identify obfuscated malicious code, while methods relying on dynamic analysis suffer from low efficiency. To address this, we propose a dynamic weighted feature selection method that analyzes the importance and stability of features, calculates scores to filter out the most robust features, and combines these selected features with the program's structural information. We then utilize graph neural networks for classification, thereby improving the robustness and accuracy of the detection system. We analyzed 8,664 malware samples from eight malware families and tested a total of 44,940 malware variants generated using seven obfuscation strategies. Experiments demonstrate that our proposed method achieves an F1-score of 95.56% on the unobfuscated dataset and 92.28% on the obfuscated dataset, indicating that the model can effectively detect obfuscated malware.</li>
</ul>

<h3>Title: On Model and Data Scaling for Skeleton-based Self-Supervised Gait Recognition</h3>
<ul>
<li><strong>Authors: </strong>Adrian Cosma, Andy C«étrun«é, Emilian R«édoi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07598">https://arxiv.org/abs/2504.07598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07598">https://arxiv.org/pdf/2504.07598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07598]] On Model and Data Scaling for Skeleton-based Self-Supervised Gait Recognition(https://arxiv.org/abs/2504.07598)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, biometric, transformer</a></li>
<li><strong>Abstract: </strong>Gait recognition from video streams is a challenging problem in computer vision biometrics due to the subtle differences between gaits and numerous confounding factors. Recent advancements in self-supervised pretraining have led to the development of robust gait recognition models that are invariant to walking covariates. While neural scaling laws have transformed model development in other domains by linking performance to data, model size, and compute, their applicability to gait remains unexplored. In this work, we conduct the first empirical study scaling on skeleton-based self-supervised gait recognition to quantify the effect of data quantity, model size and compute on downstream gait recognition performance. We pretrain multiple variants of GaitPT - a transformer-based architecture - on a dataset of 2.7 million walking sequences collected in the wild. We evaluate zero-shot performance across four benchmark datasets to derive scaling laws for data, model size, and compute. Our findings demonstrate predictable power-law improvements in performance with increased scale and confirm that data and compute scaling significantly influence downstream accuracy. We further isolate architectural contributions by comparing GaitPT with GaitFormer under controlled compute budgets. These results provide practical insights into resource allocation and performance estimation for real-world gait recognition systems.</li>
</ul>

<h3>Title: RASMD: RGB And SWIR Multispectral Driving Dataset for Robust Perception in Adverse Conditions</h3>
<ul>
<li><strong>Authors: </strong>Youngwan Jin, Michal Kovac, Yagiz Nalcakan, Hyeongjin Ju, Hanbin Song, Sanghyeop Yeo, Shiho Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07603">https://arxiv.org/abs/2504.07603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07603">https://arxiv.org/pdf/2504.07603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07603]] RASMD: RGB And SWIR Multispectral Driving Dataset for Robust Perception in Adverse Conditions(https://arxiv.org/abs/2504.07603)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Current autonomous driving algorithms heavily rely on the visible spectrum, which is prone to performance degradation in adverse conditions like fog, rain, snow, glare, and high contrast. Although other spectral bands like near-infrared (NIR) and long-wave infrared (LWIR) can enhance vision perception in such situations, they have limitations and lack large-scale datasets and benchmarks. Short-wave infrared (SWIR) imaging offers several advantages over NIR and LWIR. However, no publicly available large-scale datasets currently incorporate SWIR data for autonomous driving. To address this gap, we introduce the RGB and SWIR Multispectral Driving (RASMD) dataset, which comprises 100,000 synchronized and spatially aligned RGB-SWIR image pairs collected across diverse locations, lighting, and weather conditions. In addition, we provide a subset for RGB-SWIR translation and object detection annotations for a subset of challenging traffic scenarios to demonstrate the utility of SWIR imaging through experiments on both object detection and RGB-to-SWIR image translation. Our experiments show that combining RGB and SWIR data in an ensemble framework significantly improves detection accuracy compared to RGB-only approaches, particularly in conditions where visible-spectrum sensors struggle. We anticipate that the RASMD dataset will advance research in multispectral imaging for autonomous driving and robust perception systems.</li>
</ul>

<h3>Title: Conditional Conformal Risk Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Rui Luo, Zhixin Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07611">https://arxiv.org/abs/2504.07611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07611">https://arxiv.org/pdf/2504.07611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07611]] Conditional Conformal Risk Adaptation(https://arxiv.org/abs/2504.07611)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Uncertainty quantification is becoming increasingly important in image segmentation, especially for high-stakes applications like medical imaging. While conformal risk control generalizes conformal prediction beyond standard miscoverage to handle various loss functions such as false negative rate, its application to segmentation often yields inadequate conditional risk control: some images experience very high false negative rates while others have negligibly small ones. We develop Conformal Risk Adaptation (CRA), which introduces a new score function for creating adaptive prediction sets that significantly improve conditional risk control for segmentation tasks. We establish a novel theoretical framework that demonstrates a fundamental connection between conformal risk control and conformal prediction through a weighted quantile approach, applicable to any score function. To address the challenge of poorly calibrated probabilities in segmentation models, we introduce a specialized probability calibration framework that enhances the reliability of pixel-wise inclusion estimates. Using these calibrated probabilities, we propose Calibrated Conformal Risk Adaptation (CCRA) and a stratified variant (CCRA-S) that partitions images based on their characteristics and applies group-specific thresholds to further enhance conditional risk control. Our experiments on polyp segmentation demonstrate that all three methods (CRA, CCRA, and CCRA-S) provide valid marginal risk control and deliver more consistent conditional risk control across diverse images compared to standard approaches, offering a principled approach to uncertainty quantification that is particularly valuable for high-stakes and personalized segmentation applications.</li>
</ul>

<h3>Title: VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model</h3>
<ul>
<li><strong>Authors: </strong>Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, Tiancheng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07615">https://arxiv.org/abs/2504.07615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07615">https://arxiv.org/pdf/2504.07615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07615]] VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model(https://arxiv.org/abs/2504.07615)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the "OD aha moment", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at this https URL</li>
</ul>

<h3>Title: ConceptFormer: Towards Efficient Use of Knowledge-Graph Embeddings in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Joel Barmettler, Abraham Bernstein, Luca Rossetto</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07624">https://arxiv.org/abs/2504.07624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07624">https://arxiv.org/pdf/2504.07624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07624]] ConceptFormer: Towards Efficient Use of Knowledge-Graph Embeddings in Large Language Models(https://arxiv.org/abs/2504.07624)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval Augmented Generation (RAG) has enjoyed increased attention in the recent past and recent advancements in Large Language Models (LLMs) have highlighted the importance of integrating world knowledge into these systems. Current RAG methodologies often modify the internal architecture of pre-trained language models (PLMs) or rely on textifying knowledge graphs (KGs), which is inefficient in terms of token usage. This paper introduces ConceptFormer, a new approach to augment LLMs with structured knowledge from KGs, such as Wikidata, without altering their internal structure or relying on textual input of KGs. ConceptFormer operates in the LLM embedding vector space, creating and injecting \emph{concept vectors} that encapsulate the information of the KG nodes directly. Trained in conjunction with a frozen LLM, ConceptFormer generates a comprehensive lookup table that maps KG nodes to their respective concept vectors. The approach aims to enhance the factual recall capabilities of LLMs by enabling them to process these concept vectors natively, thus enriching them with structured world knowledge in an efficient and scalable manner. Our experiments demonstrate that the addition of concept vectors to GPT-2 0.1B substantially increases its factual recall ability (Hit@10) by up to 272\% when tested on sentences from Wikipedia and up to 348\% on synthetically generated sentences. Even injecting only a single concept vector into the prompt increases factual recall ability (Hit@10) by up to 213\% on Wikipedia sentences, significantly outperforming RAG with graph textification while consuming 130x fewer input tokens.</li>
</ul>

<h3>Title: Deep Learning Meets Teleconnections: Improving S2S Predictions for European Winter Weather</h3>
<ul>
<li><strong>Authors: </strong>Philine L. Bommer, Marlene Kretschmer, Fiona R. Spuler, Kirill Bykov, Marina M.-C. H√∂hne</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07625">https://arxiv.org/abs/2504.07625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07625">https://arxiv.org/pdf/2504.07625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07625]] Deep Learning Meets Teleconnections: Improving S2S Predictions for European Winter Weather(https://arxiv.org/abs/2504.07625)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Predictions on subseasonal-to-seasonal (S2S) timescales--ranging from two weeks to two month--are crucial for early warning systems but remain challenging owing to chaos in the climate system. Teleconnections, such as the stratospheric polar vortex (SPV) and Madden-Julian Oscillation (MJO), offer windows of enhanced predictability, however, their complex interactions remain underutilized in operational forecasting. Here, we developed and evaluated deep learning architectures to predict North Atlantic-European (NAE) weather regimes, systematically assessing the role of remote drivers in improving S2S forecast skill of deep learning models. We implemented (1) a Long Short-term Memory (LSTM) network predicting the NAE regimes of the next six weeks based on previous regimes, (2) an Index-LSTM incorporating SPV and MJO indices, and (3) a ViT-LSTM using a Vision Transformer to directly encode stratospheric wind and tropical outgoing longwave radiation fields. These models are compared with operational hindcasts as well as other AI models. Our results show that leveraging teleconnection information enhances skill at longer lead times. Notably, the ViT-LSTM outperforms ECMWF's subseasonal hindcasts beyond week 4 by improving Scandinavian Blocking (SB) and Atlantic Ridge (AR) predictions. Analysis of high-confidence predictions reveals that NAO-, SB, and AR opportunity forecasts can be associated with SPV variability and MJO phase patterns aligning with established pathways, also indicating new patterns. Overall, our work demonstrates that encoding physically meaningful climate fields can enhance S2S prediction skill, advancing AI-driven subseasonal forecast. Moreover, the experiments highlight the potential of deep learning methods as investigative tools, providing new insights into atmospheric dynamics and predictability.</li>
</ul>

<h3>Title: Kernel Logistic Regression Learning for High-Capacity Hopfield Networks</h3>
<ul>
<li><strong>Authors: </strong>Akira Tamamori</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07633">https://arxiv.org/abs/2504.07633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07633">https://arxiv.org/pdf/2504.07633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07633]] Kernel Logistic Regression Learning for High-Capacity Hopfield Networks(https://arxiv.org/abs/2504.07633)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Hebbian learning limits Hopfield network storage capacity (pattern-to-neuron ratio around 0.14). We propose Kernel Logistic Regression (KLR) learning. Unlike linear methods, KLR uses kernels to implicitly map patterns to high-dimensional feature space, enhancing separability. By learning dual variables, KLR dramatically improves storage capacity, achieving perfect recall even when pattern numbers exceed neuron numbers (up to ratio 1.5 shown), and enhances noise robustness. KLR demonstrably outperforms Hebbian and linear logistic regression approaches.</li>
</ul>

<h3>Title: On the Temporal Question-Answering Capabilities of Large Language Models Over Anonymized Data</h3>
<ul>
<li><strong>Authors: </strong>Alfredo Garrach√≥n Ruiz, Tom√°s de la Rosa, Daniel Borrajo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07646">https://arxiv.org/abs/2504.07646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07646">https://arxiv.org/pdf/2504.07646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07646]] On the Temporal Question-Answering Capabilities of Large Language Models Over Anonymized Data(https://arxiv.org/abs/2504.07646)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The applicability of Large Language Models (LLMs) in temporal reasoning tasks over data that is not present during training is still a field that remains to be explored. In this paper we work on this topic, focusing on structured and semi-structured anonymized data. We not only develop a direct LLM pipeline, but also compare various methodologies and conduct an in-depth analysis. We identified and examined seventeen common temporal reasoning tasks in natural language, focusing on their algorithmic components. To assess LLM performance, we created the \textit{Reasoning and Answering Temporal Ability} dataset (RATA), featuring semi-structured anonymized data to ensure reliance on reasoning rather than on prior knowledge. We compared several methodologies, involving SoTA techniques such as Tree-of-Thought, self-reflexion and code execution, tuned specifically for this scenario. Our results suggest that achieving scalable and reliable solutions requires more than just standalone LLMs, highlighting the need for integrated approaches.</li>
</ul>

<h3>Title: ms-Mamba: Multi-scale Mamba for Time-Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Yusuf Meric Karadag, Sinan Kalkan, Ipek Gursel Dino</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07654">https://arxiv.org/abs/2504.07654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07654">https://arxiv.org/pdf/2504.07654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07654]] ms-Mamba: Multi-scale Mamba for Time-Series Forecasting(https://arxiv.org/abs/2504.07654)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The problem of Time-series Forecasting is generally addressed by recurrent, Transformer-based and the recently proposed Mamba-based architectures. However, existing architectures generally process their input at a single temporal scale, which may be sub-optimal for many tasks where information changes over multiple time scales. In this paper, we introduce a novel architecture called Multi-scale Mamba (ms-Mamba) to address this gap. ms-Mamba incorporates multiple temporal scales by using multiple Mamba blocks with different sampling rates ($\Delta$s). Our experiments on many benchmarks demonstrate that ms-Mamba outperforms state-of-the-art approaches, including the recently proposed Transformer-based and Mamba-based models.</li>
</ul>

<h3>Title: End-to-End Facial Expression Detection in Long Videos</h3>
<ul>
<li><strong>Authors: </strong>Yini Fang, Alec Diallo, Yiqi Shi, Frederic Jumelle, Bertram Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07660">https://arxiv.org/abs/2504.07660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07660">https://arxiv.org/pdf/2504.07660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07660]] End-to-End Facial Expression Detection in Long Videos(https://arxiv.org/abs/2504.07660)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Facial expression detection involves two interrelated tasks: spotting, which identifies the onset and offset of expressions, and recognition, which classifies them into emotional categories. Most existing methods treat these tasks separately using a two-step training pipelines. A spotting model first detects expression intervals. A recognition model then classifies the detected segments. However, this sequential approach leads to error propagation, inefficient feature learning, and suboptimal performance due to the lack of joint optimization of the two tasks. We propose FEDN, an end-to-end Facial Expression Detection Network that jointly optimizes spotting and recognition. Our model introduces a novel attention-based feature extraction module, incorporating segment attention and sliding window attention to improve facial feature learning. By unifying two tasks within a single network, we greatly reduce error propagation and enhance overall performance. Experiments on CASME}^2 and CASME^3 demonstrate state-of-the-art accuracy for both spotting and detection, underscoring the benefits of joint optimization for robust facial expression detection in long videos.</li>
</ul>

<h3>Title: Unveiling the Impact of Multimodal Features on Chinese Spelling Correction: From Analysis to Design</h3>
<ul>
<li><strong>Authors: </strong>Xiaowu Zhang, Hongfei Zhao, Jingyi Hou, Zhijie Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07661">https://arxiv.org/abs/2504.07661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07661">https://arxiv.org/pdf/2504.07661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07661]] Unveiling the Impact of Multimodal Features on Chinese Spelling Correction: From Analysis to Design(https://arxiv.org/abs/2504.07661)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Chinese Spelling Correction (CSC) task focuses on detecting and correcting spelling errors in sentences. Current research primarily explores two approaches: traditional multimodal pre-trained models and large language models (LLMs). However, LLMs face limitations in CSC, particularly over-correction, making them suboptimal for this task. While existing studies have investigated the use of phonetic and graphemic information in multimodal CSC models, effectively leveraging these features to enhance correction performance remains a challenge. To address this, we propose the Multimodal Analysis for Character Usage (\textbf{MACU}) experiment, identifying potential improvements for multimodal correctison. Based on empirical findings, we introduce \textbf{NamBert}, a novel multimodal model for Chinese spelling correction. Experiments on benchmark datasets demonstrate NamBert's superiority over SOTA methods. We also conduct a comprehensive comparison between NamBert and LLMs, systematically evaluating their strengths and limitations in CSC. Our code and model are available at this https URL.</li>
</ul>

<h3>Title: Synthetic Fluency: Hallucinations, Confabulations, and the Creation of Irish Words in LLM-Generated Translations</h3>
<ul>
<li><strong>Authors: </strong>Sheila Castilho, Zoe Fitzsimmons, Claire Holton, Aoife Mc Donagh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07680">https://arxiv.org/abs/2504.07680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07680">https://arxiv.org/pdf/2504.07680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07680]] Synthetic Fluency: Hallucinations, Confabulations, and the Creation of Irish Words in LLM-Generated Translations(https://arxiv.org/abs/2504.07680)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study examines hallucinations in Large Language Model (LLM) translations into Irish, specifically focusing on instances where the models generate novel, non-existent words. We classify these hallucinations within verb and noun categories, identifying six distinct patterns among the latter. Additionally, we analyse whether these hallucinations adhere to Irish morphological rules and what linguistic tendencies they exhibit. Our findings show that while both GPT-4.o and GPT-4.o Mini produce similar types of hallucinations, the Mini model generates them at a significantly higher frequency. Beyond classification, the discussion raises speculative questions about the implications of these hallucinations for the Irish language. Rather than seeking definitive answers, we offer food for thought regarding the increasing use of LLMs and their potential role in shaping Irish vocabulary and linguistic evolution. We aim to prompt discussion on how such technologies might influence language over time, particularly in the context of low-resource, morphologically rich languages.</li>
</ul>

<h3>Title: FMNV: A Dataset of Media-Published News Videos for Fake News Detection</h3>
<ul>
<li><strong>Authors: </strong>Yihao Wang, Zhong Qian, Peifeng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07687">https://arxiv.org/abs/2504.07687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07687">https://arxiv.org/pdf/2504.07687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07687]] FMNV: A Dataset of Media-Published News Videos for Fake News Detection(https://arxiv.org/abs/2504.07687)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>News media, particularly video-based platforms, have become deeply embedded in daily life, concurrently amplifying risks of misinformation dissemination. Consequently, multimodal fake news detection has garnered significant research attention. However, existing datasets predominantly comprise user-generated videos characterized by crude editing and limited public engagement, whereas professionally crafted fake news videos disseminated by media outlets often politically or virally motivated pose substantially greater societal harm. To address this gap, we construct FMNV, a novel dataset exclusively composed of news videos published by media organizations. Through empirical analysis of existing datasets and our curated collection, we categorize fake news videos into four distinct types. Building upon this taxonomy, we employ Large Language Models (LLMs) to automatically generate deceptive content by manipulating authentic media-published news videos. Furthermore, we propose FMNVD, a baseline model featuring a dual-stream architecture integrating CLIP and Faster R-CNN for video feature extraction, enhanced by co-attention mechanisms for feature refinement and multimodal aggregation. Comparative experiments demonstrate both the generalization capability of FMNV across multiple baselines and the superior detection efficacy of FMNVD. This work establishes critical benchmarks for detecting high-impact fake news in media ecosystems while advancing methodologies for cross-modal inconsistency analysis.</li>
</ul>

<h3>Title: Distilling Knowledge from Heterogeneous Architectures for Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yanglin Huang, Kai Hu, Yuan Zhang, Zhineng Chen, Xieping Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07691">https://arxiv.org/abs/2504.07691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07691">https://arxiv.org/pdf/2504.07691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07691]] Distilling Knowledge from Heterogeneous Architectures for Semantic Segmentation(https://arxiv.org/abs/2504.07691)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Current knowledge distillation (KD) methods for semantic segmentation focus on guiding the student to imitate the teacher's knowledge within homogeneous architectures. However, these methods overlook the diverse knowledge contained in architectures with different inductive biases, which is crucial for enabling the student to acquire a more precise and comprehensive understanding of the data during distillation. To this end, we propose for the first time a generic knowledge distillation method for semantic segmentation from a heterogeneous perspective, named HeteroAKD. Due to the substantial disparities between heterogeneous architectures, such as CNN and Transformer, directly transferring cross-architecture knowledge presents significant challenges. To eliminate the influence of architecture-specific information, the intermediate features of both the teacher and student are skillfully projected into an aligned logits space. Furthermore, to utilize diverse knowledge from heterogeneous architectures and deliver customized knowledge required by the student, a teacher-student knowledge mixing mechanism (KMM) and a teacher-student knowledge evaluation mechanism (KEM) are introduced. These mechanisms are performed by assessing the reliability and its discrepancy between heterogeneous teacher-student knowledge. Extensive experiments conducted on three main-stream benchmarks using various teacher-student pairs demonstrate that our HeteroAKD outperforms state-of-the-art KD methods in facilitating distillation between heterogeneous architectures.</li>
</ul>

<h3>Title: Proactive User Information Acquisition via Chats on User-Favored Topics</h3>
<ul>
<li><strong>Authors: </strong>Shiki Sato, Jun Baba, Asahi Hentona, Shinji Iwata, Akifumi Yoshimoto, Koichiro Yoshino</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07698">https://arxiv.org/abs/2504.07698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07698">https://arxiv.org/pdf/2504.07698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07698]] Proactive User Information Acquisition via Chats on User-Favored Topics(https://arxiv.org/abs/2504.07698)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chat-oriented dialogue systems designed to provide tangible benefits, such as sharing the latest news or preventing frailty in senior citizens, often require Proactive acquisition of specific user Information via chats on user-faVOred Topics (PIVOT). This study proposes the PIVOT task, designed to advance the technical foundation for these systems. In this task, a system needs to acquire the answers of a user to predefined questions without making the user feel abrupt while engaging in a chat on a predefined topic. We found that even recent large language models (LLMs) show a low success rate in the PIVOT task. We constructed a dataset suitable for the analysis to develop more effective systems. Finally, we developed a simple but effective system for this task by incorporating insights obtained through the analysis of this dataset.</li>
</ul>

<h3>Title: PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented Generation in Large Language Models via Bilevel Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yang Jiao, Xiaodong Wang, Kai Yang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07717">https://arxiv.org/abs/2504.07717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07717">https://arxiv.org/pdf/2504.07717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07717]] PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented Generation in Large Language Models via Bilevel Optimization(https://arxiv.org/abs/2504.07717)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of applications, e.g., medical question-answering, mathematical sciences, and code generation. However, they also exhibit inherent limitations, such as outdated knowledge and susceptibility to hallucinations. Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to address these issues, but it also introduces new vulnerabilities. Recent efforts have focused on the security of RAG-based LLMs, yet existing attack methods face three critical challenges: (1) their effectiveness declines sharply when only a limited number of poisoned texts can be injected into the knowledge database, (2) they lack sufficient stealth, as the attacks are often detectable by anomaly detection systems, which compromises their effectiveness, and (3) they rely on heuristic approaches to generate poisoned texts, lacking formal optimization frameworks and theoretic guarantees, which limits their effectiveness and applicability. To address these issues, we propose coordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack that introduces a small number of poisoned texts into the knowledge database while embedding a backdoor trigger within the prompt. When activated, the trigger causes the LLM to generate pre-designed responses to targeted queries, while maintaining normal behavior in other contexts. This ensures both high effectiveness and stealth. We formulate the attack generation process as a bilevel optimization problem leveraging a principled optimization framework to develop optimal poisoned texts and triggers. Extensive experiments across diverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving a high attack success rate even with a limited number of poisoned texts and significantly improved stealth compared to existing methods.</li>
</ul>

<h3>Title: Multi-modal Reference Learning for Fine-grained Text-to-Image Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Zehong Ma, Hao Chen, Wei Zeng, Limin Su, Shiliang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07718">https://arxiv.org/abs/2504.07718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07718">https://arxiv.org/pdf/2504.07718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07718]] Multi-modal Reference Learning for Fine-grained Text-to-Image Retrieval(https://arxiv.org/abs/2504.07718)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Fine-grained text-to-image retrieval aims to retrieve a fine-grained target image with a given text query. Existing methods typically assume that each training image is accurately depicted by its textual descriptions. However, textual descriptions can be ambiguous and fail to depict discriminative visual details in images, leading to inaccurate representation learning. To alleviate the effects of text ambiguity, we propose a Multi-Modal Reference learning framework to learn robust representations. We first propose a multi-modal reference construction module to aggregate all visual and textual details of the same object into a comprehensive multi-modal reference. The multi-modal reference hence facilitates the subsequent representation learning and retrieval similarity computation. Specifically, a reference-guided representation learning module is proposed to use multi-modal references to learn more accurate visual and textual representations. Additionally, we introduce a reference-based refinement method that employs the object references to compute a reference-based similarity that refines the initial retrieval results. Extensive experiments are conducted on five fine-grained text-to-image retrieval datasets for different text-to-image retrieval tasks. The proposed method has achieved superior performance over state-of-the-art methods. For instance, on the text-to-person image retrieval dataset RSTPReid, our method achieves the Rank1 accuracy of 56.2\%, surpassing the recent CFine by 5.6\%.</li>
</ul>

<h3>Title: Counting Hours, Counting Losses: The Toll of Unpredictable Work Schedules on Financial Security</h3>
<ul>
<li><strong>Authors: </strong>Pegah Nokhiz, Aravinda Kanchana Ruwanpathirana, Aditya Bhaskara, Suresh Venkatasubramanian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07719">https://arxiv.org/abs/2504.07719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07719">https://arxiv.org/pdf/2504.07719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07719]] Counting Hours, Counting Losses: The Toll of Unpredictable Work Schedules on Financial Security(https://arxiv.org/abs/2504.07719)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Financial instability has become a significant issue in today's society. While research typically focuses on financial aspects, there is a tendency to overlook time-related aspects of unstable work schedules. The inability to rely on consistent work schedules leads to burnout, work-family conflicts, and financial shocks that directly impact workers' income and assets. Unforeseen fluctuations in earnings pose challenges in financial planning, affecting decisions on savings and spending and ultimately undermining individuals' long-term financial stability and well-being. This issue is particularly evident in sectors where workers experience frequently changing schedules without sufficient notice, including those in the food service and retail sectors, part-time and hourly workers, and individuals with lower incomes. These groups are already more financially vulnerable, and the unpredictable nature of their schedules exacerbates their financial fragility. Our objective is to understand how unforeseen fluctuations in earnings exacerbate financial fragility by investigating the extent to which individuals' financial management depends on their ability to anticipate and plan for the future. To address this question, we develop a simulation framework that models how individuals optimize utility amidst financial uncertainty and the imperative to avoid financial ruin. We employ online learning techniques, specifically adapting workers' consumption policies based on evolving information about their work schedules. With this framework, we show both theoretically and empirically how a worker's capacity to anticipate schedule changes enhances their long-term utility. Conversely, the inability to predict future events can worsen workers' instability. Moreover, our framework enables us to explore interventions to mitigate the problem of schedule uncertainty and evaluate their effectiveness.</li>
</ul>

<h3>Title: MRD-RAG: Enhancing Medical Diagnosis with Multi-Round Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Yixiang Chen, Penglei Sun, Xiang Li, Xiaowen Chu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07724">https://arxiv.org/abs/2504.07724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07724">https://arxiv.org/pdf/2504.07724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07724]] MRD-RAG: Enhancing Medical Diagnosis with Multi-Round Retrieval-Augmented Generation(https://arxiv.org/abs/2504.07724)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>In recent years, accurately and quickly deploying medical large language models (LLMs) has become a significant trend. Among these, retrieval-augmented generation (RAG) has garnered significant attention due to its features of rapid deployment and privacy protection. However, existing medical RAG frameworks still have shortcomings. Most existing medical RAG frameworks are designed for single-round question answering tasks and are not suitable for multi-round diagnostic dialogue. On the other hand, existing medical multi-round RAG frameworks do not consider the interconnections between potential diseases to inquire precisely like a doctor. To address these issues, we propose a Multi-Round Diagnostic RAG (MRD-RAG) framework that mimics the doctor's diagnostic process. This RAG framework can analyze diagnosis information of potential diseases and accurately conduct multi-round diagnosis like a doctor. To evaluate the effectiveness of our proposed frameworks, we conduct experiments on two modern medical datasets and two traditional Chinese medicine datasets, with evaluations by GPT and human doctors on different methods. The results indicate that our RAG framework can significantly enhance the diagnostic performance of LLMs, highlighting the potential of our approach in medical diagnosis. The code and data can be found in our project website this https URL.</li>
</ul>

<h3>Title: Benchmarking Multi-Organ Segmentation Tools for Multi-Parametric T1-weighted Abdominal MRI</h3>
<ul>
<li><strong>Authors: </strong>Nicole Tran, Anisa Prasad, Yan Zhuang, Tejas Sudharshan Mathai, Boah Kim, Sydney Lewis, Pritam Mukherjee, Jianfei Liu, Ronald M. Summers</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07729">https://arxiv.org/abs/2504.07729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07729">https://arxiv.org/pdf/2504.07729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07729]] Benchmarking Multi-Organ Segmentation Tools for Multi-Parametric T1-weighted Abdominal MRI(https://arxiv.org/abs/2504.07729)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The segmentation of multiple organs in multi-parametric MRI studies is critical for many applications in radiology, such as correlating imaging biomarkers with disease status (e.g., cirrhosis, diabetes). Recently, three publicly available tools, such as MRSegmentator (MRSeg), TotalSegmentator MRI (TS), and TotalVibeSegmentator (VIBE), have been proposed for multi-organ segmentation in MRI. However, the performance of these tools on specific MRI sequence types has not yet been quantified. In this work, a subset of 40 volumes from the public Duke Liver Dataset was curated. The curated dataset contained 10 volumes each from the pre-contrast fat saturated T1, arterial T1w, venous T1w, and delayed T1w phases, respectively. Ten abdominal structures were manually annotated in these volumes. Next, the performance of the three public tools was benchmarked on this curated dataset. The results indicated that MRSeg obtained a Dice score of 80.7 $\pm$ 18.6 and Hausdorff Distance (HD) error of 8.9 $\pm$ 10.4 mm. It fared the best ($p < .05$) across the different sequence types in contrast to TS and VIBE.</li>
</ul>

<h3>Title: DeepGreen: Effective LLM-Driven Green-washing Monitoring System Designed for Empirical Testing -- Evidence from China</h3>
<ul>
<li><strong>Authors: </strong>Congluo Xu, Yu Miao, Yiling Xiao, Chengmengjia Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07733">https://arxiv.org/abs/2504.07733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07733">https://arxiv.org/pdf/2504.07733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07733]] DeepGreen: Effective LLM-Driven Green-washing Monitoring System Designed for Empirical Testing -- Evidence from China(https://arxiv.org/abs/2504.07733)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper proposes DeepGreen, an Large Language Model Driven (LLM-Driven) system for detecting corporate green-washing behaviour. Utilizing dual-layer LLM analysis, DeepGreen preliminarily identifies potential green keywords in financial statements and then assesses their implementation degree via iterative semantic analysis of LLM. A core variable GreenImplement is derived from the ratio from the two layers' output. We extract 204 financial statements of 68 companies from A-share market over three years, comprising 89,893 words, and analyse them through DeepGreen. Our analysis, supported by violin plots and K-means clustering, reveals insights and validates the variable against the Huazheng ESG rating. It offers a novel perspective for regulatory agencies and investors, serving as a proactive monitoring tool that complements traditional this http URL tests show that green implementation can significantly boost the asset return rate of companies, but there is heterogeneity in scale. Small and medium-sized companies have limited contribution to asset return via green implementation, so there is a stronger motivation for green-washing.</li>
</ul>

<h3>Title: Automated Construction of a Knowledge Graph of Nuclear Fusion Energy for Effective Elicitation and Retrieval of Information</h3>
<ul>
<li><strong>Authors: </strong>A. Loreti, K. Chen, R. George, R. Firth, A. Agnello, S. Tanaka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07738">https://arxiv.org/abs/2504.07738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07738">https://arxiv.org/pdf/2504.07738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07738]] Automated Construction of a Knowledge Graph of Nuclear Fusion Energy for Effective Elicitation and Retrieval of Information(https://arxiv.org/abs/2504.07738)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this document, we discuss a multi-step approach to automated construction of a knowledge graph, for structuring and representing domain-specific knowledge from large document corpora. We apply our method to build the first knowledge graph of nuclear fusion energy, a highly specialized field characterized by vast scope and heterogeneity. This is an ideal benchmark to test the key features of our pipeline, including automatic named entity recognition and entity resolution. We show how pre-trained large language models can be used to address these challenges and we evaluate their performance against Zipf's law, which characterizes human-generated natural language. Additionally, we develop a knowledge-graph retrieval-augmented generation system that combines large language models with a multi-prompt approach. This system provides contextually relevant answers to natural-language queries, including complex multi-hop questions that require reasoning across interconnected entities.</li>
</ul>

<h3>Title: MMLA: Multi-Environment, Multi-Species, Low-Altitude Aerial Footage Dataset</h3>
<ul>
<li><strong>Authors: </strong>Jenna Kline, Samuel Stevens, Guy Maalouf, Camille Rondeau Saint-Jean, Dat Nguyen Ngoc, Majid Mirmehdi, David Guerin, Tilo Burghardt, Elzbieta Pastucha, Blair Costelloe, Matthew Watson, Thomas Richardson, Ulrik Pagh Schultz Lundquist</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07744">https://arxiv.org/abs/2504.07744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07744">https://arxiv.org/pdf/2504.07744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07744]] MMLA: Multi-Environment, Multi-Species, Low-Altitude Aerial Footage Dataset(https://arxiv.org/abs/2504.07744)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Real-time wildlife detection in drone imagery is critical for numerous applications, including animal ecology, conservation, and biodiversity monitoring. Low-altitude drone missions are effective for collecting fine-grained animal movement and behavior data, particularly if missions are automated for increased speed and consistency. However, little work exists on evaluating computer vision models on low-altitude aerial imagery and generalizability across different species and settings. To fill this gap, we present a novel multi-environment, multi-species, low-altitude aerial footage (MMLA) dataset. MMLA consists of drone footage collected across three diverse environments: Ol Pejeta Conservancy and Mpala Research Centre in Kenya, and The Wilds Conservation Center in Ohio, which includes five species: Plains zebras, Grevy's zebras, giraffes, onagers, and African Painted Dogs. We comprehensively evaluate three YOLO models (YOLOv5m, YOLOv8m, and YOLOv11m) for detecting animals. Results demonstrate significant performance disparities across locations and species-specific detection variations. Our work highlights the importance of evaluating detection algorithms across different environments for robust wildlife monitoring applications using drones.</li>
</ul>

<h3>Title: SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained Understanding</h3>
<ul>
<li><strong>Authors: </strong>Yangliu Hu, Zikai Song, Na Feng, Yawei Luo, Junqing Yu, Yi-Ping Phoebe Chen, Wei Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07745">https://arxiv.org/abs/2504.07745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07745">https://arxiv.org/pdf/2504.07745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07745]] SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained Understanding(https://arxiv.org/abs/2504.07745)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video-based Large Language Models (Video-LLMs) have witnessed substantial advancements in recent years, propelled by the advancement in multi-modal LLMs. Although these models have demonstrated proficiency in providing the overall description of videos, they struggle with fine-grained understanding, particularly in aspects such as visual dynamics and video details inquiries. To tackle these shortcomings, we find that fine-tuning Video-LLMs on self-supervised fragment tasks, greatly improve their fine-grained video understanding abilities. Hence we propose two key contributions:(1) Self-Supervised Fragment Fine-Tuning (SF$^2$T), a novel effortless fine-tuning method, employs the rich inherent characteristics of videos for training, while unlocking more fine-grained understanding ability of Video-LLMs. Moreover, it relieves researchers from labor-intensive annotations and smartly circumvents the limitations of natural language, which often fails to capture the complex spatiotemporal variations in videos; (2) A novel benchmark dataset, namely FineVidBench, for rigorously assessing Video-LLMs' performance at both the scene and fragment levels, offering a comprehensive evaluation of their capabilities. We assessed multiple models and validated the effectiveness of SF$^2$T on them. Experimental results reveal that our approach improves their ability to capture and interpret spatiotemporal details.</li>
</ul>

<h3>Title: NorEval: A Norwegian Language Understanding and Generation Evaluation Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Vladislav Mikhailov, Tita Enstad, David Samuel, Hans Christian Farseth√•s, Andrey Kutuzov, Erik Velldal, Lilja √òvrelid</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07749">https://arxiv.org/abs/2504.07749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07749">https://arxiv.org/pdf/2504.07749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07749]] NorEval: A Norwegian Language Understanding and Generation Evaluation Benchmark(https://arxiv.org/abs/2504.07749)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces NorEval, a new and comprehensive evaluation suite for large-scale standardized benchmarking of Norwegian generative language models (LMs). NorEval consists of 24 high-quality human-created datasets -- of which five are created from scratch. In contrast to existing benchmarks for Norwegian, NorEval covers a broad spectrum of task categories targeting Norwegian language understanding and generation, establishes human baselines, and focuses on both of the official written standards of the Norwegian language: Bokm√•l and Nynorsk. All our datasets and a collection of over 100 human-written prompts are integrated into LM Evaluation Harness, ensuring flexible and reproducible evaluation. We describe the NorEval design and present the results of benchmarking 19 open-source pre-trained and instruction-tuned LMs for Norwegian in various scenarios. Our benchmark, evaluation framework, and annotation materials are publicly available.</li>
</ul>

<h3>Title: Efficient Tuning of Large Language Models for Knowledge-Grounded Dialogue Generation</h3>
<ul>
<li><strong>Authors: </strong>Bo Zhang, Hui Ma, Dailin Li, Jian Ding, Jian Wang, Bo Xu, HongFei Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07754">https://arxiv.org/abs/2504.07754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07754">https://arxiv.org/pdf/2504.07754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07754]] Efficient Tuning of Large Language Models for Knowledge-Grounded Dialogue Generation(https://arxiv.org/abs/2504.07754)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate remarkable text comprehension and generation capabilities but often lack the ability to utilize up-to-date or domain-specific knowledge not included in their training data. To address this gap, we introduce KEDiT, an efficient method for fine-tuning LLMs for knowledge-grounded dialogue generation. KEDiT operates in two main phases: first, it employs an information bottleneck to compress retrieved knowledge into learnable parameters, retaining essential information while minimizing computational overhead. Second, a lightweight knowledge-aware adapter integrates these compressed knowledge vectors into the LLM during fine-tuning, updating less than 2\% of the model parameters. The experimental results on the Wizard of Wikipedia and a newly constructed PubMed-Dialog dataset demonstrate that KEDiT excels in generating contextually relevant and informative responses, outperforming competitive baselines in automatic, LLM-based, and human evaluations. This approach effectively combines the strengths of pretrained LLMs with the adaptability needed for incorporating dynamic knowledge, presenting a scalable solution for fields such as medicine.</li>
</ul>

<h3>Title: PIDSR:ComplementaryPolarizedImageDemosaicingandSuper-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Shuangfan Zhou, Chu Zhou, Youwei Lyu, Heng Guo, Zhanyu Ma, Boxin Shi, Imari Sato</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07758">https://arxiv.org/abs/2504.07758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07758">https://arxiv.org/pdf/2504.07758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07758]] PIDSR:ComplementaryPolarizedImageDemosaicingandSuper-Resolution(https://arxiv.org/abs/2504.07758)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Polarization cameras can capture multiple polarized images with different polarizer angles in a single shot, bringing convenience to polarization-based downstream tasks. However, their direct outputs are color-polarization filter array (CPFA) raw images, requiring demosaicing to reconstruct full-resolution, full-color polarized images; unfortunately, this necessary step introduces artifacts that make polarization-related parameters such as the degree of polarization (DoP) and angle of polarization (AoP) prone to error. Besides, limited by the hardware design, the resolution of a polarization camera is often much lower than that of a conventional RGB camera. Existing polarized image demosaicing (PID) methods are limited in that they cannot enhance resolution, while polarized image super-resolution (PISR) methods, though designed to obtain high-resolution (HR) polarized images from the demosaicing results, tend to retain or even amplify errors in the DoP and AoP introduced by demosaicing artifacts. In this paper, we propose PIDSR, a joint framework that performs complementary Polarized Image Demosaicing and Super-Resolution, showing the ability to robustly obtain high-quality HR polarized images with more accurate DoP and AoP from a CPFA raw image in a direct manner. Experiments show our PIDSR not only achieves state-of-the-art performance on both synthetic and real data, but also facilitates downstream tasks.</li>
</ul>

<h3>Title: Exploring a Patch-Wise Approach for Privacy-Preserving Fake ID Detection</h3>
<ul>
<li><strong>Authors: </strong>Javier Mu√±oz-Haro, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07761">https://arxiv.org/abs/2504.07761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07761">https://arxiv.org/pdf/2504.07761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07761]] Exploring a Patch-Wise Approach for Privacy-Preserving Fake ID Detection(https://arxiv.org/abs/2504.07761)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, transformer</a></li>
<li><strong>Abstract: </strong>In an increasingly digitalized world, verifying the authenticity of ID documents has become a critical challenge for real-life applications such as digital banking, crypto-exchanges, renting, etc. This study focuses on the topic of fake ID detection, covering several limitations in the field. In particular, no publicly available data from real ID documents exists, and most studies rely on proprietary in-house databases that are not available due to privacy reasons. In order to shed some light on this critical challenge that makes difficult to advance in the field, we explore a trade-off between privacy (i.e., amount of sensitive data available) and performance, proposing a novel patch-wise approach for privacy-preserving fake ID detection. Our proposed approach explores how privacy can be enhanced through: i) two levels of anonymization for an ID document (i.e., fully- and pseudo-anonymized), and ii) different patch size configurations, varying the amount of sensitive data visible in the patch image. Also, state-of-the-art methods such as Vision Transformers and Foundation Models are considered in the analysis. The experimental framework shows that, on an unseen database (DLC-2021), our proposal achieves 13.91% and 0% EERs at patch and ID document level, showing a good generalization to other databases. In addition to this exploration, another key contribution of our study is the release of the first publicly available database that contains 48,400 patches from both real and fake ID documents, along with the experimental framework and models, which will be available in our GitHub.</li>
</ul>

<h3>Title: Realigning Incentives to Build Better Software: a Holistic Approach to Vendor Accountability</h3>
<ul>
<li><strong>Authors: </strong>Gergely Bicz√≥k, Sasha Romanosky, Mingyan Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE, econ.TH</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07766">https://arxiv.org/abs/2504.07766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07766">https://arxiv.org/pdf/2504.07766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07766]] Realigning Incentives to Build Better Software: a Holistic Approach to Vendor Accountability(https://arxiv.org/abs/2504.07766)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust</a></li>
<li><strong>Abstract: </strong>In this paper, we ask the question of why the quality of commercial software, in terms of security and safety, does not measure up to that of other (durable) consumer goods we have come to expect. We examine this question through the lens of incentives. We argue that the challenge around better quality software is due in no small part to a sequence of misaligned incentives, the most critical of which being that the harm caused by software problems is by and large shouldered by consumers, not developers. This lack of liability means software vendors have every incentive to rush low-quality software onto the market and no incentive to enhance quality control. Within this context, this paper outlines a holistic technical and policy framework we believe is needed to incentivize better and more secure software development. At the heart of the incentive realignment is the concept of software liability. This framework touches on various components, including legal, technical, and financial, that are needed for software liability to work in practice; some currently exist, some will need to be re-imagined or established. This is primarily a market-driven approach that emphasizes voluntary participation but highlights the role appropriate regulation can play. We connect and contrast this with the EU legal environment and discuss what this framework means for open-source software (OSS) development and emerging AI risks. Moreover, we present a CrowdStrike case study complete with a what-if analysis had our proposed framework been in effect. Our intention is very much to stimulate a robust conversation among both researchers and practitioners.</li>
</ul>

<h3>Title: Breaking the Barriers: Video Vision Transformers for Word-Level Sign Language Recognition</h3>
<ul>
<li><strong>Authors: </strong>Alexander Brettmann, Jakob Gr√§vinghoff, Marlene R√ºschoff, Marie Westhues</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07792">https://arxiv.org/abs/2504.07792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07792">https://arxiv.org/pdf/2504.07792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07792]] Breaking the Barriers: Video Vision Transformers for Word-Level Sign Language Recognition(https://arxiv.org/abs/2504.07792)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Sign language is a fundamental means of communication for the deaf and hard-of-hearing (DHH) community, enabling nuanced expression through gestures, facial expressions, and body movements. Despite its critical role in facilitating interaction within the DHH population, significant barriers persist due to the limited fluency in sign language among the hearing population. Overcoming this communication gap through automatic sign language recognition (SLR) remains a challenge, particularly at a dynamic word-level, where temporal and spatial dependencies must be effectively recognized. While Convolutional Neural Networks have shown potential in SLR, they are computationally intensive and have difficulties in capturing global temporal dependencies between video sequences. To address these limitations, we propose a Video Vision Transformer (ViViT) model for word-level American Sign Language (ASL) recognition. Transformer models make use of self-attention mechanisms to effectively capture global relationships across spatial and temporal dimensions, which makes them suitable for complex gesture recognition tasks. The VideoMAE model achieves a Top-1 accuracy of 75.58% on the WLASL100 dataset, highlighting its strong performance compared to traditional CNNs with 65.89%. Our study demonstrates that transformer-based architectures have great potential to advance SLR, overcome communication barriers and promote the inclusion of DHH individuals.</li>
</ul>

<h3>Title: Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling Representations</h3>
<ul>
<li><strong>Authors: </strong>Yifan Ding, Arturas Aleksandrauskas, Amirhossein Ahmadian, Jonas Unger, Fredrik Lindsten, Gabriel Eilertsen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07793">https://arxiv.org/abs/2504.07793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07793">https://arxiv.org/pdf/2504.07793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07793]] Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling Representations(https://arxiv.org/abs/2504.07793)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning systems, particularly in safety-critical applications. Likelihood-based deep generative models have historically faced criticism for their unsatisfactory performance in OOD detection, often assigning higher likelihood to OOD data than in-distribution samples when applied to image data. In this work, we demonstrate that likelihood is not inherently flawed. Rather, several properties in the images space prohibit likelihood as a valid detection score. Given a sufficiently good likelihood estimator, specifically using the probability flow formulation of a diffusion model, we show that likelihood-based methods can still perform on par with state-of-the-art methods when applied in the representation space of pre-trained encoders. The code of our work can be found at $\href{this https URL}{\texttt{this https URL}}$.</li>
</ul>

<h3>Title: Plan-and-Refine: Diverse and Comprehensive Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Alireza Salemi, Chris Samarinas, Hamed Zamani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07794">https://arxiv.org/abs/2504.07794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07794">https://arxiv.org/pdf/2504.07794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07794]] Plan-and-Refine: Diverse and Comprehensive Retrieval-Augmented Generation(https://arxiv.org/abs/2504.07794)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper studies the limitations of (retrieval-augmented) large language models (LLMs) in generating diverse and comprehensive responses, and introduces the Plan-and-Refine (P&R) framework based on a two phase system design. In the global exploration phase, P&R generates a diverse set of plans for the given input, where each plan consists of a list of diverse query aspects with corresponding additional descriptions. This phase is followed by a local exploitation phase that generates a response proposal for the input query conditioned on each plan and iteratively refines the proposal for improving the proposal quality. Finally, a reward model is employed to select the proposal with the highest factuality and coverage. We conduct our experiments based on the ICAT evaluation methodology--a recent approach for answer factuality and comprehensiveness evaluation. Experiments on the two diverse information seeking benchmarks adopted from non-factoid question answering and TREC search result diversification tasks demonstrate that P&R significantly outperforms baselines, achieving up to a 13.1% improvement on the ANTIQUE dataset and a 15.41% improvement on the TREC dataset. Furthermore, a smaller scale user study confirms the substantial efficacy of the P&R framework.</li>
</ul>

<h3>Title: A System for Comprehensive Assessment of RAG Frameworks</h3>
<ul>
<li><strong>Authors: </strong>Mattia Rengo, Senad Beadini, Domenico Alfano, Roberto Abbruzzese</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07803">https://arxiv.org/abs/2504.07803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07803">https://arxiv.org/pdf/2504.07803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07803]] A System for Comprehensive Assessment of RAG Frameworks(https://arxiv.org/abs/2504.07803)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval Augmented Generation (RAG) has emerged as a standard paradigm for enhancing the factual accuracy and contextual relevance of Large Language Models (LLMs) by integrating retrieval mechanisms. However, existing evaluation frameworks fail to provide a holistic black-box approach to assessing RAG systems, especially in real-world deployment scenarios. To address this gap, we introduce SCARF (System for Comprehensive Assessment of RAG Frameworks), a modular and flexible evaluation framework designed to benchmark deployed RAG applications systematically. SCARF provides an end-to-end, black-box evaluation methodology, enabling a limited-effort comparison across diverse RAG frameworks. Our framework supports multiple deployment configurations and facilitates automated testing across vector databases and LLM serving strategies, producing a detailed performance report. Moreover, SCARF integrates practical considerations such as response coherence, providing a scalable and adaptable solution for researchers and industry professionals evaluating RAG applications. Using the REST APIs interface, we demonstrate how SCARF can be applied to real-world scenarios, showcasing its flexibility in assessing different RAG frameworks and configurations. SCARF is available at GitHub repository.</li>
</ul>

<h3>Title: Cluster-Driven Expert Pruning for Mixture-of-Experts Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongcheng Guo, Juntao Yao, Boyang Wang, Junjia Du, Shaosheng Cao, Donglin Di, Shun Zhang, Zhoujun Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07807">https://arxiv.org/abs/2504.07807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07807">https://arxiv.org/pdf/2504.07807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07807]] Cluster-Driven Expert Pruning for Mixture-of-Experts Large Language Models(https://arxiv.org/abs/2504.07807)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) architectures have emerged as a promising paradigm for scaling large language models (LLMs) with sparse activation of task-specific experts. Despite their computational efficiency during inference, the massive overall parameter footprint of MoE models (e.g., GPT-4) introduces critical challenges for practical deployment. Current pruning approaches often fail to address two inherent characteristics of MoE systems: 1).intra-layer expert homogeneity where experts within the same MoE layer exhibit functional redundancy, and 2). inter-layer similarity patterns where deeper layers tend to contain progressively more homogeneous experts. To tackle these issues, we propose Cluster-driven Expert Pruning (C-Prune), a novel two-stage framework for adaptive task-specific compression of MoE LLMs. C-Prune operates through layer-wise expert clustering, which groups functionally similar experts within each MoE layer using parameter similarity metrics, followed by global cluster pruning, which eliminates redundant clusters across all layers through a unified importance scoring mechanism that accounts for cross-layer homogeneity. We validate C-Prune through extensive experiments on multiple MoE models and benchmarks. The results demonstrate that C-Prune effectively reduces model size while outperforming existing MoE pruning methods.</li>
</ul>

<h3>Title: Nonlocal Retinex-Based Variational Model and its Deep Unfolding Twin for Low-Light Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Daniel Torres, Joan Duran, Julia Navarro, Catalina Sbert</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07810">https://arxiv.org/abs/2504.07810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07810">https://arxiv.org/pdf/2504.07810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07810]] Nonlocal Retinex-Based Variational Model and its Deep Unfolding Twin for Low-Light Image Enhancement(https://arxiv.org/abs/2504.07810)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Images captured under low-light conditions present significant limitations in many applications, as poor lighting can obscure details, reduce contrast, and hide noise. Removing the illumination effects and enhancing the quality of such images is crucial for many tasks, such as image segmentation and object detection. In this paper, we propose a variational method for low-light image enhancement based on the Retinex decomposition into illumination, reflectance, and noise components. A color correction pre-processing step is applied to the low-light image, which is then used as the observed input in the decomposition. Moreover, our model integrates a novel nonlocal gradient-type fidelity term designed to preserve structural details. Additionally, we propose an automatic gamma correction module. Building on the proposed variational approach, we extend the model by introducing its deep unfolding counterpart, in which the proximal operators are replaced with learnable networks. We propose cross-attention mechanisms to capture long-range dependencies in both the nonlocal prior of the reflectance and the nonlocal gradient-based constraint. Experimental results demonstrate that both methods compare favorably with several recent and state-of-the-art techniques across different datasets. In particular, despite not relying on learning strategies, the variational model outperforms most deep learning approaches both visually and in terms of quality metrics.</li>
</ul>

<h3>Title: P2Object: Single Point Supervised Object Detection and Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Pengfei Chen, Xuehui Yu, Xumeng Han, Kuiran Wang, Guorong Li, Lingxi Xie, Zhenjun Han, Jianbin Jiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07813">https://arxiv.org/abs/2504.07813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07813">https://arxiv.org/pdf/2504.07813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07813]] P2Object: Single Point Supervised Object Detection and Instance Segmentation(https://arxiv.org/abs/2504.07813)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Object recognition using single-point supervision has attracted increasing attention recently. However, the performance gap compared with fully-supervised algorithms remains large. Previous works generated class-agnostic \textbf{\textit{proposals in an image}} offline and then treated mixed candidates as a single bag, putting a huge burden on multiple instance learning (MIL). In this paper, we introduce Point-to-Box Network (P2BNet), which constructs balanced \textbf{\textit{instance-level proposal bags}} by generating proposals in an anchor-like way and refining the proposals in a coarse-to-fine paradigm. Through further research, we find that the bag of proposals, either at the image level or the instance level, is established on discrete box sampling. This leads the pseudo box estimation into a sub-optimal solution, resulting in the truncation of object boundaries or the excessive inclusion of background. Hence, we conduct a series exploration of discrete-to-continuous optimization, yielding P2BNet++ and Point-to-Mask Network (P2MNet). P2BNet++ conducts an approximately continuous proposal sampling strategy by better utilizing spatial clues. P2MNet further introduces low-level image information to assist in pixel prediction, and a boundary self-prediction is designed to relieve the limitation of the estimated boxes. Benefiting from the continuous object-aware \textbf{\textit{pixel-level perception}}, P2MNet can generate more precise bounding boxes and generalize to segmentation tasks. Our method largely surpasses the previous methods in terms of the mean average precision on COCO, VOC, SBD, and Cityscapes, demonstrating great potential to bridge the performance gap compared with fully supervised tasks.</li>
</ul>

<h3>Title: DG-STMTL: A Novel Graph Convolutional Network for Multi-Task Spatio-Temporal Traffic Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Wanna Cui, Peizheng Wang, Faliang Yin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07822">https://arxiv.org/abs/2504.07822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07822">https://arxiv.org/pdf/2504.07822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07822]] DG-STMTL: A Novel Graph Convolutional Network for Multi-Task Spatio-Temporal Traffic Forecasting(https://arxiv.org/abs/2504.07822)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Spatio-temporal traffic prediction is crucial in intelligent transportation systems. The key challenge of accurate prediction is how to model the complex spatio-temporal dependencies and adapt to the inherent dynamics in data. Traditional Graph Convolutional Networks (GCNs) often struggle with static adjacency matrices that introduce domain bias or learnable matrices that may be overfitting to specific patterns. This challenge becomes more complex when considering Multi-Task Learning (MTL). While MTL has the potential to enhance prediction accuracy through task synergies, it can also face significant hurdles due to task interference. To overcome these challenges, this study introduces a novel MTL framework, Dynamic Group-wise Spatio-Temporal Multi-Task Learning (DG-STMTL). DG-STMTL proposes a hybrid adjacency matrix generation module that combines static matrices with dynamic ones through a task-specific gating mechanism. We also introduce a group-wise GCN module to enhance the modelling capability of spatio-temporal dependencies. We conduct extensive experiments on two real-world datasets to evaluate our method. Results show that our method outperforms other state-of-the-arts, indicating its effectiveness and robustness.</li>
</ul>

<h3>Title: What the HellaSwag? On the Validity of Common-Sense Reasoning Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Pavel Chizhov, Mattia Nee, Pierre-Carl Langlais, Ivan P. Yamshchikov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07825">https://arxiv.org/abs/2504.07825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07825">https://arxiv.org/pdf/2504.07825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07825]] What the HellaSwag? On the Validity of Common-Sense Reasoning Benchmarks(https://arxiv.org/abs/2504.07825)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Common-sense reasoning is a key language model capability because it encapsulates not just specific factual knowledge but rather general language and world understanding. Measuring common-sense reasoning, therefore, is crucial for language models of different sizes and applications. One of the most widely used benchmarks for evaluating such capabilities is HellaSwag; however, in this paper, we show that it has severe construct validity issues. These issues range from basic ungrammaticality and numerous typos to misleading prompts or equally correct options. Furthermore, we show that if models are evaluated only on answer texts, or with "Lorem ipsum dolor..." instead of the question, more than 65% of model predictions remain the same, and this cannot be attributed merely to contamination. Since benchmark scores are an essential part of model selection in both research and commercial applications, these validity issues can have severe consequences. In particular, knowing that taking benchmark scores at face value is ubiquitous, inadequate evaluation leads to ill-informed decisions about models. In this paper, we thoroughly investigate critical validity issues posed by HellaSwag and illustrate them with various evaluations using generative language models of different sizes. We argue that this benchmark does not accurately measure common-sense reasoning and, therefore, should not be used for evaluation in its current state. Based on the results of our study, we propose requirements that should be met by future common-sense reasoning benchmarks. In addition, we release GoldenSwag, a corrected subset of HellaSwag, which, to our belief, facilitates acceptable common-sense reasoning evaluation.</li>
</ul>

<h3>Title: MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations</h3>
<ul>
<li><strong>Authors: </strong>Genglin Liu, Salman Rahman, Elisa Kreiss, Marzyeh Ghassemi, Saadia Gabriel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07830">https://arxiv.org/abs/2504.07830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07830">https://arxiv.org/pdf/2504.07830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07830]] MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations(https://arxiv.org/abs/2504.07830)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences.</li>
</ul>

<h3>Title: Deep Learning-based Intrusion Detection Systems: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Xu, Yujuan Wu, Shiheng Wang, Jiabao Gao, Tian Qiu, Ziqi Wang, Hai Wan, Xibin Zhao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07839">https://arxiv.org/abs/2504.07839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07839">https://arxiv.org/pdf/2504.07839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07839]] Deep Learning-based Intrusion Detection Systems: A Survey(https://arxiv.org/abs/2504.07839)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Intrusion Detection Systems (IDS) have long been a hot topic in the cybersecurity community. In recent years, with the introduction of deep learning (DL) techniques, IDS have made great progress due to their increasing generalizability. The rationale behind this is that by learning the underlying patterns of known system behaviors, IDS detection can be generalized to intrusions that exploit zero-day vulnerabilities. In this survey, we refer to this type of IDS as DL-based IDS (DL-IDS). From the perspective of DL, this survey systematically reviews all the stages of DL-IDS, including data collection, log storage, log parsing, graph summarization, attack detection, and attack investigation. To accommodate current researchers, a section describing the publicly available benchmark datasets is included. This survey further discusses current challenges and potential future research directions, aiming to help researchers understand the basic ideas and visions of DL-IDS research, as well as to motivate their research interests.</li>
</ul>

<h3>Title: The KL3M Data Project: Copyright-Clean Training Resources for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Michael J Bommarito II, Jillian Bommarito, Daniel Martin Katz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07854">https://arxiv.org/abs/2504.07854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07854">https://arxiv.org/pdf/2504.07854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07854]] The KL3M Data Project: Copyright-Clean Training Resources for Large Language Models(https://arxiv.org/abs/2504.07854)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Practically all large language models have been pre-trained on data that is subject to global uncertainty related to copyright infringement and breach of contract. This creates potential risk for users and developers due to this uncertain legal status. The KL3M Data Project directly confronts this critical issue by introducing the largest comprehensive training data pipeline that minimizes risks related to copyright or breach of contract. The foundation of this project is a corpus of over 132 million documents and trillions of tokens spanning 16 different sources that have been verified to meet the strict copyright and licensing protocol detailed herein. We are releasing the entire pipeline, including 1) the source code to acquire and process these documents, 2) the original document formats with associated provenance and metadata, 3) extracted content in a standardized format, 4) pre-tokenized representations of the documents, and 5) various mid- and post-train resources such as question-answer, summarization, conversion, drafting, classification, prediction, and conversational data. All of these resources are freely available to the public on S3, Hugging Face, and GitHub under CC-BY terms. We are committed to continuing this project in furtherance of a more ethical, legal, and sustainable approach to the development and use of AI models.</li>
</ul>

<h3>Title: Robust Hallucination Detection in LLMs via Adaptive Token Selection</h3>
<ul>
<li><strong>Authors: </strong>Mengjia Niu, Hamed Haddadi, Guansong Pang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07863">https://arxiv.org/abs/2504.07863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07863">https://arxiv.org/pdf/2504.07863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07863]] Robust Hallucination Detection in LLMs via Adaptive Token Selection(https://arxiv.org/abs/2504.07863)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Hallucinations in large language models (LLMs) pose significant safety concerns that impede their broader deployment. Recent research in hallucination detection has demonstrated that LLMs' internal representations contain truthfulness hints, which can be harnessed for detector training. However, the performance of these detectors is heavily dependent on the internal representations of predetermined tokens, fluctuating considerably when working on free-form generations with varying lengths and sparse distributions of hallucinated entities. To address this, we propose HaMI, a novel approach that enables robust detection of hallucinations through adaptive selection and learning of critical tokens that are most indicative of hallucinations. We achieve this robustness by an innovative formulation of the Hallucination detection task as Multiple Instance (HaMI) learning over token-level representations within a sequence, thereby facilitating a joint optimisation of token selection and hallucination detection on generation sequences of diverse forms. Comprehensive experimental results on four hallucination benchmarks show that HaMI significantly outperforms existing state-of-the-art approaches.</li>
</ul>

<h3>Title: Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs</h3>
<ul>
<li><strong>Authors: </strong>Yichun Yin, Wenyong Huang, Kaikai Song, Yehui Tang, Xueyu Wu, Wei Guo, Peng Guo, Yaoyuan Wang, Xiaojun Meng, Yasheng Wang, Dong Li, Can Chen, Dandan Tu, Yin Li, Fisher Yu, Ruiming Tang, Yunhe Wang, Baojun Wang, Bin Wang, Bo Wang, Boxiao Liu, Changzheng Zhang, Duyu Tang, Fei Mi, Hui Jin, Jiansheng Wei, Jiarui Qin, Jinpeng Li, Jun Zhao, Liqun Deng, Lin Li, Minghui Xu, Naifu Zhang, Nianzu Zheng, Qiang Li, Rongju Ruan, Shengjun Cheng, Tianyu Guo, Wei He, Wei Li, Weiwen Liu, Wulong Liu, Xinyi Dai, Yonghan Dong, Yu Pan, Yue Li, Yufei Wang, Yujun Li, Yunsheng Ni, Zhe Liu, Zhenhe Zhang, Zhicheng Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07866">https://arxiv.org/abs/2504.07866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07866">https://arxiv.org/pdf/2504.07866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07866]] Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs(https://arxiv.org/abs/2504.07866)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.</li>
</ul>

<h3>Title: SAFARI: a Scalable Air-gapped Framework for Automated Ransomware Investigation</h3>
<ul>
<li><strong>Authors: </strong>Tommaso Compagnucci, Franco Callegati, Saverio Giallorenzo, Andrea Melis, Simone Melloni, Alessandro Vannini</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07868">https://arxiv.org/abs/2504.07868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07868">https://arxiv.org/pdf/2504.07868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07868]] SAFARI: a Scalable Air-gapped Framework for Automated Ransomware Investigation(https://arxiv.org/abs/2504.07868)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Ransomware poses a significant threat to individuals and organisations, compelling tools to investigate its behaviour and the effectiveness of mitigations. To answer this need, we present SAFARI, an open-source framework designed for safe and efficient ransomware analysis. SAFARI's design emphasises scalability, air-gapped security, and automation, democratising access to safe ransomware investigation tools and fostering collaborative efforts. SAFARI leverages virtualisation, Infrastructure-as-Code, and OS-agnostic task automation to create isolated environments for controlled ransomware execution and analysis. The framework enables researchers to profile ransomware behaviour and evaluate mitigation strategies through automated, reproducible experiments. We demonstrate SAFARI's capabilities by building a proof-of-concept implementation and using it to run two case studies. The first analyses five renowned ransomware strains (including WannaCry and LockBit) to identify their encryption patterns and file targeting strategies. The second evaluates Ranflood, a contrast tool which we use against three dangerous strains. Our results provide insights into ransomware behaviour and the effectiveness of countermeasures, showcasing SAFARI's potential to advance ransomware research and defence development.</li>
</ul>

<h3>Title: Token Level Routing Inference System for Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Jianshu She, Wenhao Zheng, Zhengzhong Liu, Hongyi Wang, Eric Xing, Huaxiu Yao, Qirong Ho</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07878">https://arxiv.org/abs/2504.07878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07878">https://arxiv.org/pdf/2504.07878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07878]] Token Level Routing Inference System for Edge Devices(https://arxiv.org/abs/2504.07878)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The computational complexity of large language model (LLM) inference significantly constrains their deployment efficiency on edge devices. In contrast, small language models offer faster decoding and lower resource consumption but often suffer from degraded response quality and heightened susceptibility to hallucinations. To address this trade-off, collaborative decoding, in which a large model assists in generating critical tokens, has emerged as a promising solution. This paradigm leverages the strengths of both model types by enabling high-quality inference through selective intervention of the large model, while maintaining the speed and efficiency of the smaller model. In this work, we present a novel collaborative decoding inference system that allows small models to perform on-device inference while selectively consulting a cloud-based large model for critical token generation. Remarkably, the system achieves a 60% performance gain on CommonsenseQA using only a 0.5B model on an M1 MacBook, with under 7% of tokens generation uploaded to the large model in the cloud.</li>
</ul>

<h3>Title: Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge</h3>
<ul>
<li><strong>Authors: </strong>Riccardo Cantini, Alessio Orsino, Massimo Ruggiero, Domenico Talia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07887">https://arxiv.org/abs/2504.07887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07887">https://arxiv.org/pdf/2504.07887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07887]] Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge(https://arxiv.org/abs/2504.07887)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized artificial intelligence, driving advancements in machine translation, summarization, and conversational agents. However, their increasing integration into critical societal domains has raised concerns about embedded biases, which can perpetuate stereotypes and compromise fairness. These biases stem from various sources, including historical inequalities in training data, linguistic imbalances, and adversarial manipulation. Despite mitigation efforts, recent studies indicate that LLMs remain vulnerable to adversarial attacks designed to elicit biased responses. This work proposes a scalable benchmarking framework to evaluate LLM robustness against adversarial bias elicitation. Our methodology involves (i) systematically probing models with a multi-task approach targeting biases across various sociocultural dimensions, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach for automated assessment of model responses, and (iii) employing jailbreak techniques to investigate vulnerabilities in safety mechanisms. Our analysis examines prevalent biases in both small and large state-of-the-art models and their impact on model safety. Additionally, we assess the safety of domain-specific models fine-tuned for critical fields, such as medicine. Finally, we release a curated dataset of bias-related prompts, CLEAR-Bias, to facilitate systematic vulnerability benchmarking. Our findings reveal critical trade-offs between model size and safety, aiding the development of fairer and more robust future language models.</li>
</ul>

<h3>Title: DiverseFlow: Sample-Efficient Diverse Mode Coverage in Flows</h3>
<ul>
<li><strong>Authors: </strong>Mashrur M. Morshed, Vishnu Boddeti</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07894">https://arxiv.org/abs/2504.07894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07894">https://arxiv.org/pdf/2504.07894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07894]] DiverseFlow: Sample-Efficient Diverse Mode Coverage in Flows(https://arxiv.org/abs/2504.07894)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Many real-world applications of flow-based generative models desire a diverse set of samples that cover multiple modes of the target distribution. However, the predominant approach for obtaining diverse sets is not sample-efficient, as it involves independently obtaining many samples from the source distribution and mapping them through the flow until the desired mode coverage is achieved. As an alternative to repeated sampling, we introduce DiverseFlow: a training-free approach to improve the diversity of flow models. Our key idea is to employ a determinantal point process to induce a coupling between the samples that drives diversity under a fixed sampling budget. In essence, DiverseFlow allows exploration of more variations in a learned flow model with fewer samples. We demonstrate the efficacy of our method for tasks where sample-efficient diversity is desirable, such as text-guided image generation with polysemous words, inverse problems like large-hole inpainting, and class-conditional image synthesis.</li>
</ul>

<h3>Title: Redefining Machine Translation on Social Network Services with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongcheng Guo, Fei Zhao, Shaosheng Cao, Xinze Lyu, Ziyan Liu, Yue Wang, Boyang Wang, Zhoujun Li, Chonggang Lu, Zhe Xu, Yao Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07901">https://arxiv.org/abs/2504.07901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07901">https://arxiv.org/pdf/2504.07901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07901]] Redefining Machine Translation on Social Network Services with Large Language Models(https://arxiv.org/abs/2504.07901)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The globalization of social interactions has heightened the need for machine translation (MT) on Social Network Services (SNS), yet traditional models struggle with culturally nuanced content like memes, slang, and pop culture references. While large language models (LLMs) have advanced general-purpose translation, their performance on SNS-specific content remains limited due to insufficient specialized training data and evaluation benchmarks. This paper introduces RedTrans, a 72B LLM tailored for SNS translation, trained on a novel dataset developed through three innovations: (1) Supervised Finetuning with Dual-LLM Back-Translation Sampling, an unsupervised sampling method using LLM-based back-translation to select diverse data for large-scale finetuning; (2) Rewritten Preference Optimization (RePO), an algorithm that identifies and corrects erroneous preference pairs through expert annotation, building reliable preference corpora; and (3) RedTrans-Bench, the first benchmark for SNS translation, evaluating phenomena like humor localization, emoji semantics, and meme adaptation. Experiments show RedTrans outperforms state-of-the-art LLMs. Besides, RedTrans has already been deployed in a real-world production environment, demonstrating that domain-specific adaptation, effectively bridges the gap between generic and culturally grounded translation systems.</li>
</ul>

<h3>Title: Hodge Laplacians and Hodge Diffusion Maps</h3>
<ul>
<li><strong>Authors: </strong>Alvaro Almeida Gomez, Jorge Duque Franco</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07910">https://arxiv.org/abs/2504.07910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07910">https://arxiv.org/pdf/2504.07910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07910]] Hodge Laplacians and Hodge Diffusion Maps(https://arxiv.org/abs/2504.07910)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Hodge Diffusion Maps, a novel manifold learning algorithm designed to analyze and extract topological information from high-dimensional data-sets. This method approximates the exterior derivative acting on differential forms, thereby providing an approximation of the Hodge Laplacian operator. Hodge Diffusion Maps extend existing non-linear dimensionality reduction techniques, including vector diffusion maps, as well as the theories behind diffusion maps and Laplacian Eigenmaps. Our approach captures higher-order topological features of the data-set by projecting it into lower-dimensional Euclidean spaces using the Hodge Laplacian. We develop a theoretical framework to estimate the approximation error of the exterior derivative, based on sample points distributed over a real manifold. Numerical experiments support and validate the proposed methodology.</li>
</ul>

<h3>Title: SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement</h3>
<ul>
<li><strong>Authors: </strong>Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, Lijuan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07934">https://arxiv.org/abs/2504.07934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07934">https://arxiv.org/pdf/2504.07934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07934]] SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement(https://arxiv.org/abs/2504.07934)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at this https URL.</li>
</ul>

<h3>Title: Development of a Quantum-Resistant File Transfer System with Blockchain Audit Trail</h3>
<ul>
<li><strong>Authors: </strong>Ernesto Sola-Thomas, Masudul H Imtiaz</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07938">https://arxiv.org/abs/2504.07938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07938">https://arxiv.org/pdf/2504.07938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07938]] Development of a Quantum-Resistant File Transfer System with Blockchain Audit Trail(https://arxiv.org/abs/2504.07938)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust</a></li>
<li><strong>Abstract: </strong>This paper presents a condensed system architecture for a file transfer solution that leverages post quantum cryptography and blockchain to secure data against quantum threats. The architecture integrates NIST standardized algorithms CRYSTALS Kyber for encryption and CRYSTALS Dilithium for digital signatures with an immutable blockchain ledger to provide an auditable, decentralized storage mechanism. The system is modular, comprising a Sender module for secure encryption and signing, a central User Storage module for decryption, reencryption, and blockchain logging, and a Requestor module for authenticated data access. We include detailed pseudocode, analyze security risks, and offer performance insights to demonstrate the system's robustness, scalability, and transparency.</li>
</ul>

<h3>Title: MARS: a Multimodal Alignment and Ranking System for Few-Shot Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Nico Catalano, Stefano Samele, Paolo Pertino, Matteo Matteucci</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07942">https://arxiv.org/abs/2504.07942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07942">https://arxiv.org/pdf/2504.07942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07942]] MARS: a Multimodal Alignment and Ranking System for Few-Shot Segmentation(https://arxiv.org/abs/2504.07942)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Current Few Shot Segmentation literature lacks a mask selection method that goes beyond visual similarity between the query and example images, leading to suboptimal predictions. We present MARS, a plug-and-play ranking system that leverages multimodal cues to filter and merge mask proposals robustly. Starting from a set of mask predictions for a single query image, we score, filter, and merge them to improve results. Proposals are evaluated using multimodal scores computed at local and global levels. Extensive experiments on COCO-20i, Pascal-5i, LVIS-92i, and FSS-1000 demonstrate that integrating all four scoring components is crucial for robust ranking, validating our contribution. As MARS can be effortlessly integrated with various mask proposal systems, we deploy it across a wide range of top-performer methods and achieve new state-of-the-art results on multiple existing benchmarks. Code will be available upon acceptance.</li>
</ul>

<h3>Title: HoloPart: Generative 3D Part Amodal Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yunhan Yang, Yuan-Chen Guo, Yukun Huang, Zi-Xin Zou, Zhipeng Yu, Yangguang Li, Yan-Pei Cao, Xihui Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07943">https://arxiv.org/abs/2504.07943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07943">https://arxiv.org/pdf/2504.07943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07943]] HoloPart: Generative 3D Part Amodal Segmentation(https://arxiv.org/abs/2504.07943)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>3D part amodal segmentation--decomposing a 3D shape into complete, semantically meaningful parts, even when occluded--is a challenging but crucial task for 3D content creation and understanding. Existing 3D part segmentation methods only identify visible surface patches, limiting their utility. Inspired by 2D amodal segmentation, we introduce this novel task to the 3D domain and propose a practical, two-stage approach, addressing the key challenges of inferring occluded 3D geometry, maintaining global shape consistency, and handling diverse shapes with limited training data. First, we leverage existing 3D part segmentation to obtain initial, incomplete part segments. Second, we introduce HoloPart, a novel diffusion-based model, to complete these segments into full 3D parts. HoloPart utilizes a specialized architecture with local attention to capture fine-grained part geometry and global shape context attention to ensure overall shape consistency. We introduce new benchmarks based on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart significantly outperforms state-of-the-art shape completion methods. By incorporating HoloPart with existing segmentation techniques, we achieve promising results on 3D part amodal segmentation, opening new avenues for applications in geometry editing, animation, and material assignment.</li>
</ul>

<h3>Title: GenEAva: Generating Cartoon Avatars with Fine-Grained Facial Expressions from Realistic Diffusion-based Faces</h3>
<ul>
<li><strong>Authors: </strong>Hao Yu, Rupayan Mallick, Margrit Betke, Sarah Adel Bargal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07945">https://arxiv.org/abs/2504.07945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07945">https://arxiv.org/pdf/2504.07945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07945]] GenEAva: Generating Cartoon Avatars with Fine-Grained Facial Expressions from Realistic Diffusion-based Faces(https://arxiv.org/abs/2504.07945)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion</a></li>
<li><strong>Abstract: </strong>Cartoon avatars have been widely used in various applications, including social media, online tutoring, and gaming. However, existing cartoon avatar datasets and generation methods struggle to present highly expressive avatars with fine-grained facial expressions and are often inspired from real-world identities, raising privacy concerns. To address these challenges, we propose a novel framework, GenEAva, for generating high-quality cartoon avatars with fine-grained facial expressions. Our approach fine-tunes a state-of-the-art text-to-image diffusion model to synthesize highly detailed and expressive facial expressions. We then incorporate a stylization model that transforms these realistic faces into cartoon avatars while preserving both identity and expression. Leveraging this framework, we introduce the first expressive cartoon avatar dataset, GenEAva 1.0, specifically designed to capture 135 fine-grained facial expressions, featuring 13,230 expressive cartoon avatars with a balanced distribution across genders, racial groups, and age ranges. We demonstrate that our fine-tuned model generates more expressive faces than the state-of-the-art text-to-image diffusion model SDXL. We also verify that the cartoon avatars generated by our framework do not include memorized identities from fine-tuning data. The proposed framework and dataset provide a diverse and expressive benchmark for future research in cartoon avatar generation.</li>
</ul>

<h3>Title: VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yukun Qi, Yiming Zhao, Yu Zeng, Xikun Bao, Wenxuan Huang, Lin Chen, Zehui Chen, Jie Zhao, Zhongang Qi, Feng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07956">https://arxiv.org/abs/2504.07956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07956">https://arxiv.org/pdf/2504.07956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07956]] VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning(https://arxiv.org/abs/2504.07956)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task.</li>
</ul>

<h3>Title: MM-IFEngine: Towards Multimodal Instruction Following</h3>
<ul>
<li><strong>Authors: </strong>Shengyuan Ding, Shenxi Wu, Xiangyu Zhao, Yuhang Zang, Haodong Duan, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Dahua Lin, Jiaqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07957">https://arxiv.org/abs/2504.07957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07957">https://arxiv.org/pdf/2504.07957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07957]] MM-IFEngine: Towards Multimodal Instruction Following(https://arxiv.org/abs/2504.07957)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right. Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions, and the evaluation strategies are imprecise for tasks demanding exact output constraints. To address this, we present MM-IFEngine, an effective pipeline to generate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for Direct Preference Optimization (DPO). We further introduce MM-IFEval, a challenging and diverse multi-modal instruction-following benchmark that includes (1) both compose-level constraints for output responses and perception-level constraints tied to the input images, and (2) a comprehensive evaluation pipeline incorporating both rule-based assessment and judge model. We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on MM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF benchmarks, such as MM-IFEval (+10.2$\%$), MIA (+7.6$\%$), and IFEval (+12.3$\%$). The full data and evaluation code will be released on this https URL.</li>
</ul>

<h3>Title: VisualCloze: A Universal Image Generation Framework via Visual In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhong-Yu Li, Ruoyi Du, Juncheng Yan, Le Zhuo, Zhen Li, Peng Gao, Zhanyu Ma, Ming-Ming Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07960">https://arxiv.org/abs/2504.07960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07960">https://arxiv.org/pdf/2504.07960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07960]] VisualCloze: A Universal Image Generation Framework via Visual In-Context Learning(https://arxiv.org/abs/2504.07960)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to address this limitation, they face critical challenges, including generalizable task instruction, appropriate task distributions, and unified architectural design. To tackle these challenges, we propose VisualCloze, a universal image generation framework, which supports a wide range of in-domain tasks, generalization to unseen ones, unseen unification of multiple tasks, and reverse generation. Unlike existing methods that rely on language-based task instruction, leading to task ambiguity and weak generalization, we integrate visual in-context learning, allowing models to identify tasks from visual demonstrations. Meanwhile, the inherent sparsity of visual task distributions hampers the learning of transferable knowledge across tasks. To this end, we introduce Graph200K, a graph-structured dataset that establishes various interrelated tasks, enhancing task density and transferable knowledge. Furthermore, we uncover that our unified image generation formulation shared a consistent objective with image infilling, enabling us to leverage the strong generative priors of pre-trained infilling models without modifying the architectures.</li>
</ul>

<h3>Title: Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, Andrea Vedaldi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07961">https://arxiv.org/abs/2504.07961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07961">https://arxiv.org/pdf/2504.07961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07961]] Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction(https://arxiv.org/abs/2504.07961)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Geo4D, a method to repurpose video diffusion models for monocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic prior captured by such video models, Geo4D can be trained using only synthetic data while generalizing well to real data in a zero-shot manner. Geo4D predicts several complementary geometric modalities, namely point, depth, and ray maps. It uses a new multi-modal alignment algorithm to align and fuse these modalities, as well as multiple sliding windows, at inference time, thus obtaining robust and accurate 4D reconstruction of long videos. Extensive experiments across multiple benchmarks show that Geo4D significantly surpasses state-of-the-art video depth estimation methods, including recent methods such as MonST3R, which are also designed to handle dynamic scenes.</li>
</ul>

<h3>Title: GLUS: Global-Local Reasoning Unified into A Single Large Language Model for Video Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Lang Lin, Xueyang Yu, Ziqi Pang, Yu-Xiong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07962">https://arxiv.org/abs/2504.07962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07962">https://arxiv.org/pdf/2504.07962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07962]] GLUS: Global-Local Reasoning Unified into A Single Large Language Model for Video Segmentation(https://arxiv.org/abs/2504.07962)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>This paper proposes a novel framework utilizing multi-modal large language models (MLLMs) for referring video object segmentation (RefVOS). Previous MLLM-based methods commonly struggle with the dilemma between "Ref" and "VOS": they either specialize in understanding a few key frames (global reasoning) or tracking objects on continuous frames (local reasoning), and rely on external VOS or frame selectors to mitigate the other end of the challenge. However, our framework GLUS shows that global and local consistency can be unified into a single video segmentation MLLM: a set of sparse "context frames" provides global information, while a stream of continuous "query frames" conducts local object tracking. This is further supported by jointly training the MLLM with a pre-trained VOS memory bank to simultaneously digest short-range and long-range temporal information. To improve the information efficiency within the limited context window of MLLMs, we introduce object contrastive learning to distinguish hard false-positive objects and a self-refined framework to identify crucial frames and perform propagation. By collectively integrating these insights, our GLUS delivers a simple yet effective baseline, achieving new state-of-the-art for MLLMs on the MeViS and Ref-Youtube-VOS benchmark. Our project page is at this https URL.</li>
</ul>

<h3>Title: PixelFlow: Pixel-Space Generative Models with Flow</h3>
<ul>
<li><strong>Authors: </strong>Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, Ping Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07963">https://arxiv.org/abs/2504.07963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07963">https://arxiv.org/pdf/2504.07963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07963]] PixelFlow: Pixel-Space Generative Models with Flow(https://arxiv.org/abs/2504.07963)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present PixelFlow, a family of image generation models that operate directly in the raw pixel space, in contrast to the predominant latent-space models. This approach simplifies the image generation process by eliminating the need for a pre-trained Variational Autoencoder (VAE) and enabling the whole model end-to-end trainable. Through efficient cascade flow modeling, PixelFlow achieves affordable computation cost in pixel space. It achieves an FID of 1.98 on 256$\times$256 ImageNet class-conditional image generation benchmark. The qualitative text-to-image results demonstrate that PixelFlow excels in image quality, artistry, and semantic control. We hope this new paradigm will inspire and open up new opportunities for next-generation visual generation models. Code and models are available at this https URL.</li>
</ul>

<h3>Title: C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization for Test-Time Expert Re-Mixing</h3>
<ul>
<li><strong>Authors: </strong>Zhongyang Li, Ziyue Li, Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07964">https://arxiv.org/abs/2504.07964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07964">https://arxiv.org/pdf/2504.07964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07964]] C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization for Test-Time Expert Re-Mixing(https://arxiv.org/abs/2504.07964)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely sub-optimal expert pathways-our study reveals that naive expert selection learned from pretraining leaves a surprising 10-20% accuracy gap for improvement. Motivated by this observation, we develop a novel class of test-time optimization methods to re-weight or "re-mixing" the experts in different layers jointly for each test sample. Since the test sample's ground truth is unknown, we propose to optimize a surrogate objective defined by the sample's "successful neighbors" from a reference set of samples. We introduce three surrogates and algorithms based on mode-finding, kernel regression, and the average loss of similar reference samples/tasks. To reduce the cost of optimizing whole pathways, we apply our algorithms merely to the core experts' mixing weights in critical layers, which enjoy similar performance but save significant computation. This leads to "Critical-Layer, Core-Expert, Collaborative Pathway Optimization (C3PO)". We apply C3PO to two recent MoE LLMs and examine it on six widely-used benchmarks. It consistently improves the base model by 7-15% in accuracy and outperforms widely used test-time learning baselines, e.g., in-context learning and prompt/prefix tuning, by a large margin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to outperform LLMs of 7-9B parameters, hence improving MoE's advantages on efficiency. Our thorough ablation study further sheds novel insights on achieving test-time improvement on MoE.</li>
</ul>

<h3>Title: Cat, Rat, Meow: On the Alignment of Language Model and Human Term-Similarity Judgments</h3>
<ul>
<li><strong>Authors: </strong>Lorenz Linhardt, Tom Neuh√§user, Lenka Tƒõtkov√°, Oliver Eberle</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07965">https://arxiv.org/abs/2504.07965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07965">https://arxiv.org/pdf/2504.07965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07965]] Cat, Rat, Meow: On the Alignment of Language Model and Human Term-Similarity Judgments(https://arxiv.org/abs/2504.07965)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Small and mid-sized generative language models have gained increasing attention. Their size and availability make them amenable to being analyzed at a behavioral as well as a representational level, allowing investigations of how these levels interact. We evaluate 32 publicly available language models for their representational and behavioral alignment with human similarity judgments on a word triplet task. This provides a novel evaluation setting to probe semantic associations in language beyond common pairwise comparisons. We find that (1) even the representations of small language models can achieve human-level alignment, (2) instruction-tuned model variants can exhibit substantially increased agreement, (3) the pattern of alignment across layers is highly model dependent, and (4) alignment based on models' behavioral responses is highly dependent on model size, matching their representational alignment only for the largest evaluated models.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
