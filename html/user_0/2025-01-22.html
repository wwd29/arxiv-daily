<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-01-22</h1>
<h3>Title: One-Time Signature Based on Pseudorandom Number Generator</h3>
<ul>
<li><strong>Authors: </strong>Abel C. H. Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10393">https://arxiv.org/abs/2501.10393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10393">https://arxiv.org/pdf/2501.10393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10393]] One-Time Signature Based on Pseudorandom Number Generator(https://arxiv.org/abs/2501.10393)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>With the advancement of quantum computing technologies, recent years have seen increasing efforts to identify cryptographic methods resistant to quantum attacks and to establish post-quantum cryptography (PQC) approaches. Among these, hash-based digital signature algorithms (DSAs) are a notable category of PQC. Hash functions are not only utilized in digital signatures but are also widely applied in pseudorandom number generators (PRNGs). Building on the foundation of hash-based DSAs, this study proposes a modified approach that introduces a DSA based on PRNGs, suitable for one-time signature (OTS) applications. The study explores the security of the proposed PRNG-based OTS algorithm and validates its feasibility through experiments comparing various parameter configurations. These experiments examine key length, signature length, key generation time, signature generation time, and signature verification time under different parameter settings.</li>
</ul>

<h3>Title: The Continuous Logarithm in the Complex Circle for Post-Quantum Cryptographic Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Jaafar Gaber</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10394">https://arxiv.org/abs/2501.10394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10394">https://arxiv.org/pdf/2501.10394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10394]] The Continuous Logarithm in the Complex Circle for Post-Quantum Cryptographic Algorithms(https://arxiv.org/abs/2501.10394)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel cryptographic approach based on the continuous logarithm in the complex circle, designed to address the challenges posed by quantum computing. By leveraging its multi-valued and spectral properties, this framework enables the reintroduction of classical algorithms (DH, ECDSA, ElGamal, EC) and elliptic curve variants into the post-quantum landscape. Transitioning from classical or elliptic algebraic structures to the geometric and spectral properties of the complex circle, we propose a robust and adaptable foundation for post-quantum cryptography.</li>
</ul>

<h3>Title: Towards General Purpose Robots at Scale: Lifelong Learning and Learning to Use Memory</h3>
<ul>
<li><strong>Authors: </strong>William Yue</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10395">https://arxiv.org/abs/2501.10395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10395">https://arxiv.org/pdf/2501.10395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10395]] Towards General Purpose Robots at Scale: Lifelong Learning and Learning to Use Memory(https://arxiv.org/abs/2501.10395)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The widespread success of artificial intelligence in fields like natural language processing and computer vision has not yet fully transferred to robotics, where progress is hindered by the lack of large-scale training data and the complexity of real-world tasks. To address this, many robot learning researchers are pushing to get robots deployed at scale in everyday unstructured environments like our homes to initiate a data flywheel. While current robot learning systems are effective for certain short-horizon tasks, they are not designed to autonomously operate over long time horizons in unstructured environments. This thesis focuses on addressing two key challenges for robots operating over long time horizons: memory and lifelong learning. We propose two novel methods to advance these capabilities. First, we introduce t-DGR, a trajectory-based deep generative replay method that achieves state-of-the-art performance on Continual World benchmarks, advancing lifelong learning. Second, we develop a framework that leverages human demonstrations to teach agents effective memory utilization, improving learning efficiency and success rates on Memory Gym tasks. Finally, we discuss future directions for achieving the lifelong learning and memory capabilities necessary for robots to function at scale in real-world settings.</li>
</ul>

<h3>Title: Using hypervisors to create a cyber polygon</h3>
<ul>
<li><strong>Authors: </strong>Dmytro Tymoshchuk, Vasyl Yatskiv</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10403">https://arxiv.org/abs/2501.10403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10403">https://arxiv.org/pdf/2501.10403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10403]] Using hypervisors to create a cyber polygon(https://arxiv.org/abs/2501.10403)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Cyber polygon used to train cybersecurity professionals, test new security technologies and simulate attacks play an important role in ensuring cybersecurity. The creation of such training grounds is based on the use of hypervisors, which allow efficient management of virtual machines, isolating operating systems and resources of a physical computer from virtual machines, ensuring a high level of security and stability. The paper analyses various aspects of using hypervisors in cyber polygons, including types of hypervisors, their main functions, and the specifics of their use in modelling cyber threats. The article shows the ability of hypervisors to increase the efficiency of hardware resources, create complex virtual environments for detailed modelling of network structures and simulation of real situations in cyberspace.</li>
</ul>

<h3>Title: A Protocol for Compliant, Obliviously Managed Electronic Transfers</h3>
<ul>
<li><strong>Authors: </strong>Geoffrey Goodell</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10419">https://arxiv.org/abs/2501.10419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10419">https://arxiv.org/pdf/2501.10419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10419]] A Protocol for Compliant, Obliviously Managed Electronic Transfers(https://arxiv.org/abs/2501.10419)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>We describe a protocol for creating, updating, and transferring digital assets securely, with strong privacy and self-custody features for the initial owner based upon the earlier work of Goodell, Toliver, and Nakib. The architecture comprises three components: a mechanism to unlink counterparties in the transaction channel, a mechanism for oblivious transactions, and a mechanism to prevent service providers from equivocating. We present an approach for the implementation of these components.</li>
</ul>

<h3>Title: Robust Hybrid Classical-Quantum Transfer Learning Model for Text Classification Using GPT-Neo 125M with LoRA & SMOTE Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Santanam Wishal</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10435">https://arxiv.org/abs/2501.10435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10435">https://arxiv.org/pdf/2501.10435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10435]] Robust Hybrid Classical-Quantum Transfer Learning Model for Text Classification Using GPT-Neo 125M with LoRA & SMOTE Enhancement(https://arxiv.org/abs/2501.10435)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This research introduces a hybrid classical-quantum framework for text classification, integrating GPT-Neo 125M with Low-Rank Adaptation (LoRA) and Synthetic Minority Over-sampling Technique (SMOTE) using quantum computing backends. While the GPT-Neo 125M baseline remains the best-performing model, the implementation of LoRA and SMOTE enhances the hybrid model, resulting in improved accuracy, faster convergence, and better generalization. Experiments on IBM's 127-qubit quantum backend and Pennylane's 32-qubit simulation demonstrate the viability of combining classical neural networks with quantum circuits. This framework underscores the potential of hybrid architectures for advancing natural language processing applications.</li>
</ul>

<h3>Title: A Review of Detection, Evolution, and Data Reconstruction Strategies for False Data Injection Attacks in Power Cyber-Physical Systems</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyong Bo</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10441">https://arxiv.org/abs/2501.10441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10441">https://arxiv.org/pdf/2501.10441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10441]] A Review of Detection, Evolution, and Data Reconstruction Strategies for False Data Injection Attacks in Power Cyber-Physical Systems(https://arxiv.org/abs/2501.10441)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack, steal, interpretability</a></li>
<li><strong>Abstract: </strong>The integration of information and physical systems in modern power grids has heightened vulnerabilities to False Data Injection Attacks (FDIAs), threatening the secure operation of power cyber-physical systems (CPS). This paper reviews FDIA detection, evolution, and data reconstruction strategies, highlighting cross-domain coordination, multi-temporal evolution, and stealth characteristics. Challenges in existing detection methods, including poor interpretability and data imbalance, are discussed, alongside advanced state-aware and action-control data reconstruction techniques. Key issues, such as modeling FDIA evolution and distinguishing malicious data from regular faults, are identified. Future directions to enhance system resilience and detection accuracy are proposed, contributing to the secure operation of power CPS.</li>
</ul>

<h3>Title: Towards Lightweight Time Series Forecasting: a Patch-wise Transformer with Weak Data Enriching</h3>
<ul>
<li><strong>Authors: </strong>Meng Wang, Jintao Yang, Bin Yang, Hui Li, Tongxin Gong, Bo Yang, Jiangtao Cui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10448">https://arxiv.org/abs/2501.10448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10448">https://arxiv.org/pdf/2501.10448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10448]] Towards Lightweight Time Series Forecasting: a Patch-wise Transformer with Weak Data Enriching(https://arxiv.org/abs/2501.10448)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Patch-wise Transformer based time series forecasting achieves superior accuracy. However, this superiority relies heavily on intricate model design with massive parameters, rendering both training and inference expensive, thus preventing their deployments on edge devices with limited resources and low latency requirements. In addition, existing methods often work in an autoregressive manner, which take into account only historical values, but ignore valuable, easy-to-obtain context information, such as weather forecasts, date and time of day. To contend with the two limitations, we propose LiPFormer, a novel Lightweight Patch-wise Transformer with weak data enriching. First, to simplify the Transformer backbone, LiPFormer employs a novel lightweight cross-patch attention and a linear transformation-based attention to eliminate Layer Normalization and Feed Forward Network, two heavy components in existing Transformers. Second, we propose a lightweight, weak data enriching module to provide additional, valuable weak supervision to the training. It enhances forecasting accuracy without significantly increasing model complexity as it does not involve expensive, human-labeling but using easily accessible context information. This facilitates the weak data enriching to plug-and-play on existing models. Extensive experiments on nine benchmark time series datasets demonstrate that LiPFormer outperforms state-of-the-art methods in accuracy, while significantly reducing parameter scale, training duration, and GPU memory usage. Deployment on an edge device reveals that LiPFormer takes only 1/3 inference time compared to classic Transformers. In addition, we demonstrate that the weak data enriching can integrate seamlessly into various Transformer based models to enhance their accuracy, suggesting its generality.</li>
</ul>

<h3>Title: Automating Credit Card Limit Adjustments Using Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Diego Pestana</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10451">https://arxiv.org/abs/2501.10451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10451">https://arxiv.org/pdf/2501.10451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10451]] Automating Credit Card Limit Adjustments Using Machine Learning(https://arxiv.org/abs/2501.10451)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Venezuelan banks have historically made credit card limit adjustment decisions manually through committees. However, since the number of credit card holders in Venezuela is expected to increase in the upcoming months due to economic improvements, manual decisions are starting to become unfeasible. In this project, a machine learning model that uses cost-sensitive learning is proposed to automate the task of handing out credit card limit increases. To accomplish this, several neural network and XGBoost models are trained and compared, leveraging Venezolano de Credito's data and using grid search with 10-fold cross-validation. The proposed model is ultimately chosen due to its superior balance of accuracy, cost-effectiveness, and interpretability. The model's performance is evaluated against the committee's decisions using Cohen's kappa coefficient, showing an almost perfect agreement.</li>
</ul>

<h3>Title: Uncovering Bias in Foundation Models: Impact, Testing, Harm, and Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Shuzhou Sun (1 and 2), Li Liu (3), Yongxiang Liu (3), Zhen Liu (3), Shuanghui Zhang (3), Janne Heikkilä (2), Xiang Li (3) ((1) The College of Computer Science, Nankai University, Tianjin, China, (2) The Center for Machine Vision and Signal Analysis, University of Oulu, Finland, (3) The College of Electronic Science, National University of Defense Technology, China)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10453">https://arxiv.org/abs/2501.10453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10453">https://arxiv.org/pdf/2501.10453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10453]] Uncovering Bias in Foundation Models: Impact, Testing, Harm, and Mitigation(https://arxiv.org/abs/2501.10453)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Bias in Foundation Models (FMs) - trained on vast datasets spanning societal and historical knowledge - poses significant challenges for fairness and equity across fields such as healthcare, education, and finance. These biases, rooted in the overrepresentation of stereotypes and societal inequalities in training data, exacerbate real-world discrimination, reinforce harmful stereotypes, and erode trust in AI systems. To address this, we introduce Trident Probe Testing (TriProTesting), a systematic testing method that detects explicit and implicit biases using semantically designed probes. Here we show that FMs, including CLIP, ALIGN, BridgeTower, and OWLv2, demonstrate pervasive biases across single and mixed social attributes (gender, race, age, and occupation). Notably, we uncover mixed biases when social attributes are combined, such as gender x race, gender x age, and gender x occupation, revealing deeper layers of discrimination. We further propose Adaptive Logit Adjustment (AdaLogAdjustment), a post-processing technique that dynamically redistributes probability power to mitigate these biases effectively, achieving significant improvements in fairness without retraining models. These findings highlight the urgent need for ethical AI practices and interdisciplinary solutions to address biases not only at the model level but also in societal structures. Our work provides a scalable and interpretable solution that advances fairness in AI systems while offering practical insights for future research on fair AI technologies.</li>
</ul>

<h3>Title: Spatio-Temporal Graph Convolutional Networks: Optimised Temporal Architecture</h3>
<ul>
<li><strong>Authors: </strong>Edward Turner</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10454">https://arxiv.org/abs/2501.10454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10454">https://arxiv.org/pdf/2501.10454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10454]] Spatio-Temporal Graph Convolutional Networks: Optimised Temporal Architecture(https://arxiv.org/abs/2501.10454)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Spatio-Temporal graph convolutional networks were originally introduced with CNNs as temporal blocks for feature extraction. Since then LSTM temporal blocks have been proposed and shown to have promising results. We propose a novel architecture combining both CNN and LSTM temporal blocks and then provide an empirical comparison between our new and the pre-existing models. We provide theoretical arguments for the different temporal blocks and use a multitude of tests across different datasets to assess our hypotheses.</li>
</ul>

<h3>Title: BloomScene: Lightweight Structured 3D Gaussian Splatting for Crossmodal Scene Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaolu Hou, Mingcheng Li, Dingkang Yang, Jiawei Chen, Ziyun Qian, Xiao Zhao, Yue Jiang, Jinjie Wei, Qingyao Xu, Lihua Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10462">https://arxiv.org/abs/2501.10462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10462">https://arxiv.org/pdf/2501.10462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10462]] BloomScene: Lightweight Structured 3D Gaussian Splatting for Crossmodal Scene Generation(https://arxiv.org/abs/2501.10462)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With the widespread use of virtual reality applications, 3D scene generation has become a new challenging research frontier. 3D scenes have highly complex structures and need to ensure that the output is dense, coherent, and contains all necessary structures. Many current 3D scene generation methods rely on pre-trained text-to-image diffusion models and monocular depth estimators. However, the generated scenes occupy large amounts of storage space and often lack effective regularisation methods, leading to geometric distortions. To this end, we propose BloomScene, a lightweight structured 3D Gaussian splatting for crossmodal scene generation, which creates diverse and high-quality 3D scenes from text or image inputs. Specifically, a crossmodal progressive scene generation framework is proposed to generate coherent scenes utilizing incremental point cloud reconstruction and 3D Gaussian splatting. Additionally, we propose a hierarchical depth prior-based regularization mechanism that utilizes multi-level constraints on depth accuracy and smoothness to enhance the realism and continuity of the generated scenes. Ultimately, we propose a structured context-guided compression mechanism that exploits structured hash grids to model the context of unorganized anchor attributes, which significantly eliminates structural redundancy and reduces storage overhead. Comprehensive experiments across multiple scenes demonstrate the significant potential and advantages of our framework compared with several baselines.</li>
</ul>

<h3>Title: GLow -- A Novel, Flower-Based Simulated Gossip Learning Strategy</h3>
<ul>
<li><strong>Authors: </strong>Aitor Belenguer, Jose A. Pascual, Javier Navaridas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10463">https://arxiv.org/abs/2501.10463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10463">https://arxiv.org/pdf/2501.10463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10463]] GLow -- A Novel, Flower-Based Simulated Gossip Learning Strategy(https://arxiv.org/abs/2501.10463)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Fully decentralized learning algorithms are still in an early stage of development. Creating modular Gossip Learning strategies is not trivial due to convergence challenges and Byzantine faults intrinsic in systems of decentralized nature. Our contribution provides a novel means to simulate custom Gossip Learning systems by leveraging the state-of-the-art Flower Framework. Specifically, we introduce GLow, which will allow researchers to train and assess scalability and convergence of devices, across custom network topologies, before making a physical deployment. The Flower Framework is selected for being a simulation featured library with a very active community on Federated Learning research. However, Flower exclusively includes vanilla Federated Learning strategies and, thus, is not originally designed to perform simulations without a centralized authority. GLow is presented to fill this gap and make simulation of Gossip Learning systems possible. Results achieved by GLow in the MNIST and CIFAR10 datasets, show accuracies over 0.98 and 0.75 respectively. More importantly, GLow performs similarly in terms of accuracy and convergence to its analogous Centralized and Federated approaches in all designed experiments.</li>
</ul>

<h3>Title: Improving the Efficiency of Self-Supervised Adversarial Training through Latent Clustering-Based Selection</h3>
<ul>
<li><strong>Authors: </strong>Somrita Ghosh, Yuelin Xu, Xiao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10466">https://arxiv.org/abs/2501.10466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10466">https://arxiv.org/pdf/2501.10466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10466]] Improving the Efficiency of Self-Supervised Adversarial Training through Latent Clustering-Based Selection(https://arxiv.org/abs/2501.10466)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Compared with standard learning, adversarially robust learning is widely recognized to demand significantly more training examples. Recent works propose the use of self-supervised adversarial training (SSAT) with external or synthetically generated unlabeled data to enhance model robustness. However, SSAT requires a substantial amount of extra unlabeled data, significantly increasing memory usage and model training times. To address these challenges, we propose novel methods to strategically select a small subset of unlabeled data essential for SSAT and robustness improvement. Our selection prioritizes data points near the model's decision boundary based on latent clustering-based techniques, efficiently identifying a critical subset of unlabeled data with a higher concentration of boundary-adjacent points. While focusing on near-boundary data, our methods are designed to maintain a balanced ratio between boundary and non-boundary data points to avoid overfitting. Our experiments on image benchmarks show that integrating our selection strategies into self-supervised adversarial training can largely reduce memory and computational requirements while achieving high model robustness. In particular, our latent clustering-based selection method with k-means is the most effective, achieving nearly identical test-time robust accuracies with 5 to 10 times less external or generated unlabeled data when applied to image benchmarks. Additionally, we validate the generalizability of our approach across various application scenarios, including a real-world medical dataset for COVID-19 chest X-ray classification.</li>
</ul>

<h3>Title: Securing the AI Frontier: Urgent Ethical and Regulatory Imperatives for AI-Driven Cybersecurity</h3>
<ul>
<li><strong>Authors: </strong>Vikram Kulothungan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10467">https://arxiv.org/abs/2501.10467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10467">https://arxiv.org/pdf/2501.10467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10467]] Securing the AI Frontier: Urgent Ethical and Regulatory Imperatives for AI-Driven Cybersecurity(https://arxiv.org/abs/2501.10467)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>This paper critically examines the evolving ethical and regulatory challenges posed by the integration of artificial intelligence (AI) in cybersecurity. We trace the historical development of AI regulation, highlighting major milestones from theoretical discussions in the 1940s to the implementation of recent global frameworks such as the European Union AI Act. The current regulatory landscape is analyzed, emphasizing risk-based approaches, sector-specific regulations, and the tension between fostering innovation and mitigating risks. Ethical concerns such as bias, transparency, accountability, privacy, and human oversight are explored in depth, along with their implications for AI-driven cybersecurity systems. Furthermore, we propose strategies for promoting AI literacy and public engagement, essential for shaping a future regulatory framework. Our findings underscore the need for a unified, globally harmonized regulatory approach that addresses the unique risks of AI in cybersecurity. We conclude by identifying future research opportunities and recommending pathways for collaboration between policymakers, industry leaders, and researchers to ensure the responsible deployment of AI technologies in cybersecurity.</li>
</ul>

<h3>Title: Using Domain Knowledge with Deep Learning to Solve Applied Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Qinyi Tian, Winston Lindqwister, Manolis Veveakis, Laura E. Dalton</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10481">https://arxiv.org/abs/2501.10481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10481">https://arxiv.org/pdf/2501.10481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10481]] Using Domain Knowledge with Deep Learning to Solve Applied Inverse Problems(https://arxiv.org/abs/2501.10481)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Advancements in deep learning have improved the ability to model complex, nonlinear relationships, such as those encountered in complex material inverse problems. However, the effectiveness of these methods often depends on large datasets, which are not always available. In this study, the incorporation of domain-specific knowledge of mechanical behavior is investigated to evaluate the impact on the predictive performance of the models in data-scarce scenarios. To demonstrate this, stress-strain curves were used to predict key microstructural features of porous materials, and the performance of models trained with and without domain knowledge was compared using five deep learning models: Convolutional Neural Networks, Extreme Gradient Boosting, K-Nearest Neighbors, Long Short-Term Memory, and Random Forest. The results of the models with domain-specific characteristics consistently achieved higher $R^2$ values and improved learning efficiency compared to models without prior knowledge. When the models did not include domain knowledge, the model results revealed meaningful patterns were not recognized, while those enhanced with mechanical insights showed superior feature extraction and predictions. These findings underscore the critical role of domain knowledge in guiding deep learning models, highlighting the need to combine domain expertise with data-driven approaches to achieve reliable and accurate outcomes in materials science and related fields.</li>
</ul>

<h3>Title: Tabular-TX: Theme-Explanation Structure-based Table Summarization via In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>TaeYoon Kwack, Jisoo Kim, Ki Yong Jung, DongGeon Lee, Heesun Park</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10487">https://arxiv.org/abs/2501.10487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10487">https://arxiv.org/pdf/2501.10487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10487]] Tabular-TX: Theme-Explanation Structure-based Table Summarization via In-Context Learning(https://arxiv.org/abs/2501.10487)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper proposes a Theme-Explanation Structure-based Table Summarization (Tabular-TX) pipeline designed to efficiently process table data. Tabular-TX preprocesses table data by focusing on highlighted cells and then generates summary sentences structured with a Theme Part in the form of adverbial phrases followed by an Explanation Part in the form of clauses. In this process, customized analysis is performed by considering the structural characteristics and comparability of the table. Additionally, by utilizing In-Context Learning, Tabular-TX optimizes the analytical capabilities of large language models (LLMs) without the need for fine-tuning, effectively handling the structural complexity of table data. Results from applying the proposed Tabular-TX to generate table-based summaries demonstrated superior performance compared to existing fine-tuning-based methods, despite limitations in dataset size. Experimental results confirmed that Tabular-TX can process complex table data more effectively and established it as a new alternative for table-based question answering and summarization tasks, particularly in resource-constrained environments.</li>
</ul>

<h3>Title: 4bit-Quantization in Vector-Embedding for RAG</h3>
<ul>
<li><strong>Authors: </strong>Taehee Jeong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10534">https://arxiv.org/abs/2501.10534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10534">https://arxiv.org/pdf/2501.10534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10534]] 4bit-Quantization in Vector-Embedding for RAG(https://arxiv.org/abs/2501.10534)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) is a promising technique that has shown great potential in addressing some of the limitations of large language models (LLMs). LLMs have two major limitations: they can contain outdated information due to their training data, and they can generate factually inaccurate responses, a phenomenon known as hallucinations. RAG aims to mitigate these issues by leveraging a database of relevant documents, which are stored as embedding vectors in a high-dimensional space. However, one of the challenges of using high-dimensional embeddings is that they require a significant amount of memory to store. This can be a major issue, especially when dealing with large databases of documents. To alleviate this problem, we propose the use of 4-bit quantization to store the embedding vectors. This involves reducing the precision of the vectors from 32-bit floating-point numbers to 4-bit integers, which can significantly reduce the memory requirements. Our approach has several benefits. Firstly, it significantly reduces the memory storage requirements of the high-dimensional vector database, making it more feasible to deploy RAG systems in resource-constrained environments. Secondly, it speeds up the searching process, as the reduced precision of the vectors allows for faster computation. Our code is available at this https URL</li>
</ul>

<h3>Title: FORLAPS: An Innovative Data-Driven Reinforcement Learning Approach for Prescriptive Process Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Mostafa Abbasi, Maziyar Khadivi, Maryam Ahang, Patricia Lasserre, Yves Lucet, Homayoun Najjaran</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10543">https://arxiv.org/abs/2501.10543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10543">https://arxiv.org/pdf/2501.10543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10543]] FORLAPS: An Innovative Data-Driven Reinforcement Learning Approach for Prescriptive Process Monitoring(https://arxiv.org/abs/2501.10543)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a novel 5-step framework called Fine-Tuned Offline Reinforcement Learning Augmented Process Sequence Optimization (FORLAPS), which aims to identify optimal execution paths in business processes using reinforcement learning. We implemented this approach on real-life event logs from our case study an energy regulator in Canada and other real-life event logs, demonstrating the feasibility of the proposed method. Additionally, to compare FORLAPS with the existing models (Permutation Feature Importance and multi-task LSTM-Based model), we experimented to evaluate its effectiveness in terms of resource savings and process time span reduction. The experimental results on real-life event log validate that FORLAPS achieves 31% savings in resource time spent and a 23% reduction in process time span. Using this innovative data augmentation technique, we propose a fine-tuned reinforcement learning approach that aims to automatically fine-tune the model by selectively increasing the average estimated Q-value in the sampled batches. The results show that we obtained a 44% performance improvement compared to the pre-trained model. This study introduces an innovative evaluation model, benchmarking its performance against earlier works using nine publicly available datasets. Robustness is ensured through experiments utilizing the Damerau-Levenshtein distance as the primary metric. In addition, we discussed the suitability of datasets, taking into account their inherent properties, to evaluate the performance of different models. The proposed model, FORLAPS, demonstrated exceptional performance, outperforming existing state-of-the-art approaches in suggesting the most optimal policies or predicting the best next activities within a process trace.</li>
</ul>

<h3>Title: Towards Data-Centric AI: A Comprehensive Survey of Traditional, Reinforcement, and Generative Approaches for Tabular Data Transformation</h3>
<ul>
<li><strong>Authors: </strong>Dongjie Wang, Yanyong Huang, Wangyang Ying, Haoyue Bai, Nanxu Gong, Xinyuan Wang, Sixun Dong, Tao Zhe, Kunpeng Liu, Meng Xiao, Pengfei Wang, Pengyang Wang, Hui Xiong, Yanjie Fu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10555">https://arxiv.org/abs/2501.10555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10555">https://arxiv.org/pdf/2501.10555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10555]] Towards Data-Centric AI: A Comprehensive Survey of Traditional, Reinforcement, and Generative Approaches for Tabular Data Transformation(https://arxiv.org/abs/2501.10555)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Tabular data is one of the most widely used formats across industries, driving critical applications in areas such as finance, healthcare, and marketing. In the era of data-centric AI, improving data quality and representation has become essential for enhancing model performance, particularly in applications centered around tabular data. This survey examines the key aspects of tabular data-centric AI, emphasizing feature selection and feature generation as essential techniques for data space refinement. We provide a systematic review of feature selection methods, which identify and retain the most relevant data attributes, and feature generation approaches, which create new features to simplify the capture of complex data patterns. This survey offers a comprehensive overview of current methodologies through an analysis of recent advancements, practical applications, and the strengths and limitations of these techniques. Finally, we outline open challenges and suggest future perspectives to inspire continued innovation in this field.</li>
</ul>

<h3>Title: Picachv: Formally Verified Data Use Policy Enforcement for Secure Data Analytics</h3>
<ul>
<li><strong>Authors: </strong>Haobin Hiroki Chen, Hongbo Chen, Mingshen Sun, Chenghong Wang, XiaoFeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DB, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10560">https://arxiv.org/abs/2501.10560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10560">https://arxiv.org/pdf/2501.10560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10560]] Picachv: Formally Verified Data Use Policy Enforcement for Secure Data Analytics(https://arxiv.org/abs/2501.10560)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy</a></li>
<li><strong>Abstract: </strong>Ensuring the proper use of sensitive data in analytics under complex privacy policies is an increasingly critical challenge. Many existing approaches lack portability, verifiability, and scalability across diverse data processing frameworks. We introduce Picachv, a novel security monitor that automatically enforces data use policies. It works on relational algebra as an abstraction for program semantics, enabling policy enforcement on query plans generated by programs during execution. This approach simplifies analysis across diverse analytical operations and supports various front-end query languages. By formalizing both data use policies and relational algebra semantics in Coq, we prove that Picachv correctly enforces policies. Picachv also leverages Trusted Execution Environments (TEEs) to enhance trust in runtime, providing provable policy compliance to stakeholders that the analytical tasks comply with their data use policies. We integrated Picachv into Polars, a state-of-the-art data analytics framework, and evaluate its performance using the TPC-H benchmark. We also apply our approach to real-world use cases. Our work demonstrates the practical application of formal methods in securing data analytics, addressing key challenges.</li>
</ul>

<h3>Title: On the Benefits of Instance Decomposition in Video Prediction Models</h3>
<ul>
<li><strong>Authors: </strong>Eliyas Suleyman, Paul Henderson, Nicolas Pugeault</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10562">https://arxiv.org/abs/2501.10562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10562">https://arxiv.org/pdf/2501.10562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10562]] On the Benefits of Instance Decomposition in Video Prediction Models(https://arxiv.org/abs/2501.10562)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Video prediction is a crucial task for intelligent agents such as robots and autonomous vehicles, since it enables them to anticipate and act early on time-critical incidents. State-of-the-art video prediction methods typically model the dynamics of a scene jointly and implicitly, without any explicit decomposition into separate objects. This is challenging and potentially sub-optimal, as every object in a dynamic scene has their own pattern of movement, typically somewhat independent of others. In this paper, we investigate the benefit of explicitly modeling the objects in a dynamic scene separately within the context of latent-transformer video prediction models. We conduct detailed and carefully-controlled experiments on both synthetic and real-world datasets; our results show that decomposing a dynamic scene leads to higher quality predictions compared with models of a similar capacity that lack such decomposition.</li>
</ul>

<h3>Title: The Geometry of Tokens in Internal Representations of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Karthik Viswanathan, Yuri Gardinazzi, Giada Panerai, Alberto Cazzaniga, Matteo Biagetti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10573">https://arxiv.org/abs/2501.10573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10573">https://arxiv.org/pdf/2501.10573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10573]] The Geometry of Tokens in Internal Representations of Large Language Models(https://arxiv.org/abs/2501.10573)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>We investigate the relationship between the geometry of token embeddings and their role in the next token prediction within transformer models. An important aspect of this connection uses the notion of empirical measure, which encodes the distribution of token point clouds across transformer layers and drives the evolution of token representations in the mean-field interacting picture. We use metrics such as intrinsic dimension, neighborhood overlap, and cosine similarity to observationally probe these empirical measures across layers. To validate our approach, we compare these metrics to a dataset where the tokens are shuffled, which disrupts the syntactic and semantic structure. Our findings reveal a correlation between the geometric properties of token embeddings and the cross-entropy loss of next token predictions, implying that prompts with higher loss values have tokens represented in higher-dimensional spaces.</li>
</ul>

<h3>Title: Adapting Large Language Models for Character-based Augmentative and Alternative Communication</h3>
<ul>
<li><strong>Authors: </strong>Dylan Gaines, Keith Vertanen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10582">https://arxiv.org/abs/2501.10582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10582">https://arxiv.org/pdf/2501.10582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10582]] Adapting Large Language Models for Character-based Augmentative and Alternative Communication(https://arxiv.org/abs/2501.10582)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Users of Augmentative and Alternative Communication (AAC) may write letter-by-letter via an interface that uses a character language model. However, most state-of-the-art large pretrained language models predict subword tokens of variable length. We investigate how to practically use such models to make accurate and efficient character predictions. We fine-tune models using a large dataset of sentences we curated in which each sentence is rated according to how useful it might be for spoken or written AAC communication. We find that using an algorithm to produce character predictions from a subword large language model provides more accurate predictions than adding a classification layer or using a byte-level model. We also find that our domain adaptation curriculum is effective at improving model performance on simple, conversational text.</li>
</ul>

<h3>Title: When language and vision meet road safety: leveraging multimodal large language models for video-based traffic accident analysis</h3>
<ul>
<li><strong>Authors: </strong>Ruixuan Zhang, Beichen Wang, Juexiao Zhang, Zilin Bian, Chen Feng, Kaan Ozbay</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10604">https://arxiv.org/abs/2501.10604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10604">https://arxiv.org/pdf/2501.10604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10604]] When language and vision meet road safety: leveraging multimodal large language models for video-based traffic accident analysis(https://arxiv.org/abs/2501.10604)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>The increasing availability of traffic videos functioning on a 24/7/365 time scale has the great potential of increasing the spatio-temporal coverage of traffic accidents, which will help improve traffic safety. However, analyzing footage from hundreds, if not thousands, of traffic cameras in a 24/7/365 working protocol remains an extremely challenging task, as current vision-based approaches primarily focus on extracting raw information, such as vehicle trajectories or individual object detection, but require laborious post-processing to derive actionable insights. We propose SeeUnsafe, a new framework that integrates Multimodal Large Language Model (MLLM) agents to transform video-based traffic accident analysis from a traditional extraction-then-explanation workflow to a more interactive, conversational approach. This shift significantly enhances processing throughput by automating complex tasks like video classification and visual grounding, while improving adaptability by enabling seamless adjustments to diverse traffic scenarios and user-defined queries. Our framework employs a severity-based aggregation strategy to handle videos of various lengths and a novel multimodal prompt to generate structured responses for review and evaluation and enable fine-grained visual grounding. We introduce IMS (Information Matching Score), a new MLLM-based metric for aligning structured responses with ground truth. We conduct extensive experiments on the Toyota Woven Traffic Safety dataset, demonstrating that SeeUnsafe effectively performs accident-aware video classification and visual grounding by leveraging off-the-shelf MLLMs. Source code will be available at \url{this https URL}.</li>
</ul>

<h3>Title: Differentiable Adversarial Attacks for Marked Temporal Point Processes</h3>
<ul>
<li><strong>Authors: </strong>Pritish Chakraborty, Vinayak Gupta, Rahul R, Srikanta J. Bedathur, Abir De</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10606">https://arxiv.org/abs/2501.10606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10606">https://arxiv.org/pdf/2501.10606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10606]] Differentiable Adversarial Attacks for Marked Temporal Point Processes(https://arxiv.org/abs/2501.10606)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Marked temporal point processes (MTPPs) have been shown to be extremely effective in modeling continuous time event sequences (CTESs). In this work, we present adversarial attacks designed specifically for MTPP models. A key criterion for a good adversarial attack is its imperceptibility. For objects such as images or text, this is often achieved by bounding perturbation in some fixed $L_p$ norm-ball. However, similarly minimizing distance norms between two CTESs in the context of MTPPs is challenging due to their sequential nature and varying time-scales and lengths. We address this challenge by first permuting the events and then incorporating the additive noise to the arrival timestamps. However, the worst case optimization of such adversarial attacks is a hard combinatorial problem, requiring exploration across a permutation space that is factorially large in the length of the input sequence. As a result, we propose a novel differentiable scheme PERMTPP using which we can perform adversarial attacks by learning to minimize the likelihood, while minimizing the distance between two CTESs. Our experiments on four real-world datasets demonstrate the offensive and defensive capabilities, and lower inference times of PERMTPP.</li>
</ul>

<h3>Title: Hierarchical LoG Bayesian Neural Network for Enhanced Aorta Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Delin An, Pan Du, Pengfei Gu, Jian-Xun Wang, Chaoli Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10615">https://arxiv.org/abs/2501.10615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10615">https://arxiv.org/pdf/2501.10615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10615]] Hierarchical LoG Bayesian Neural Network for Enhanced Aorta Segmentation(https://arxiv.org/abs/2501.10615)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of the aorta and its associated arch branches is crucial for diagnosing aortic diseases. While deep learning techniques have significantly improved aorta segmentation, they remain challenging due to the intricate multiscale structure and the complexity of the surrounding tissues. This paper presents a novel approach for enhancing aorta segmentation using a Bayesian neural network-based hierarchical Laplacian of Gaussian (LoG) model. Our model consists of a 3D U-Net stream and a hierarchical LoG stream: the former provides an initial aorta segmentation, and the latter enhances blood vessel detection across varying scales by learning suitable LoG kernels, enabling self-adaptive handling of different parts of the aorta vessels with significant scale differences. We employ a Bayesian method to parameterize the LoG stream and provide confidence intervals for the segmentation results, ensuring robustness and reliability of the prediction for vascular medical image analysts. Experimental results show that our model can accurately segment main and supra-aortic vessels, yielding at least a 3% gain in the Dice coefficient over state-of-the-art methods across multiple volumes drawn from two aorta datasets, and can provide reliable confidence intervals for different parts of the aorta. The code is available at this https URL.</li>
</ul>

<h3>Title: Mutual Regression Distance</h3>
<ul>
<li><strong>Authors: </strong>Dong Qiao, Jicong Fan</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10617">https://arxiv.org/abs/2501.10617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10617">https://arxiv.org/pdf/2501.10617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10617]] Mutual Regression Distance(https://arxiv.org/abs/2501.10617)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>The maximum mean discrepancy and Wasserstein distance are popular distance measures between distributions and play important roles in many machine learning problems such as metric learning, generative modeling, domain adaption, and clustering. However, since they are functions of pair-wise distances between data points in two distributions, they do not exploit the potential manifold properties of data such as smoothness and hence are not effective in measuring the dissimilarity between the two distributions in the form of manifolds. In this paper, different from existing measures, we propose a novel distance called Mutual Regression Distance (MRD) induced by a constrained mutual regression problem, which can exploit the manifold property of data. We prove that MRD is a pseudometric that satisfies almost all the axioms of a metric. Since the optimization of the original MRD is costly, we provide a tight MRD and a simplified MRD, based on which a heuristic algorithm is established. We also provide kernel variants of MRDs that are more effective in handling nonlinear data. Our MRDs especially the simplified MRDs have much lower computational complexity than the Wasserstein distance. We provide theoretical guarantees, such as robustness, for MRDs. Finally, we apply MRDs to distribution clustering, generative models, and domain adaptation. The numerical results demonstrate the effectiveness and superiority of MRDs compared to the baselines.</li>
</ul>

<h3>Title: AI/ML Based Detection and Categorization of Covert Communication in IPv6 Network</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Wali Ur Rahman, Yu-Zheng Lin, Carter Weeks, David Ruddell, Jeff Gabriellini, Bill Hayes, Salim Hariri, Edward V. Ziegler Jr</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10627">https://arxiv.org/abs/2501.10627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10627">https://arxiv.org/pdf/2501.10627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10627]] AI/ML Based Detection and Categorization of Covert Communication in IPv6 Network(https://arxiv.org/abs/2501.10627)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, generative</a></li>
<li><strong>Abstract: </strong>The flexibility and complexity of IPv6 extension headers allow attackers to create covert channels or bypass security mechanisms, leading to potential data breaches or system compromises. The mature development of machine learning has become the primary detection technology option used to mitigate covert communication threats. However, the complexity of detecting covert communication, evolving injection techniques, and scarcity of data make building machine-learning models challenging. In previous related research, machine learning has shown good performance in detecting covert communications, but oversimplified attack scenario assumptions cannot represent the complexity of modern covert technologies and make it easier for machine learning models to detect covert communications. To bridge this gap, in this study, we analyzed the packet structure and network traffic behavior of IPv6, used encryption algorithms, and performed covert communication injection without changing network packet behavior to get closer to real attack scenarios. In addition to analyzing and injecting methods for covert communications, this study also uses comprehensive machine learning techniques to train the model proposed in this study to detect threats, including traditional decision trees such as random forests and gradient boosting, as well as complex neural network architectures such as CNNs and LSTMs, to achieve detection accuracy of over 90\%. This study details the methods used for dataset augmentation and the comparative performance of the applied models, reinforcing insights into the adaptability and resilience of the machine learning application in IPv6 covert communication. In addition, we also proposed a Generative AI-assisted interpretation concept based on prompt engineering as a preliminary study of the role of Generative AI agents in covert communication.</li>
</ul>

<h3>Title: Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks</h3>
<ul>
<li><strong>Authors: </strong>Xin Yi, Yue Li, Linlin Wang, Xiaoling Wang, Liang He</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10639">https://arxiv.org/abs/2501.10639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10639">https://arxiv.org/pdf/2501.10639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10639]] Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks(https://arxiv.org/abs/2501.10639)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Ensuring safety alignment has become a critical requirement for large language models (LLMs), particularly given their widespread deployment in real-world applications. However, LLMs remain susceptible to jailbreak attacks, which exploit system vulnerabilities to bypass safety measures and generate harmful outputs. Although numerous defense mechanisms based on adversarial training have been proposed, a persistent challenge lies in the exacerbation of over-refusal behaviors, which compromise the overall utility of the model. To address these challenges, we propose a Latent-space Adversarial Training with Post-aware Calibration (LATPC) framework. During the adversarial training phase, LATPC compares harmful and harmless instructions in the latent space and extracts safety-critical dimensions to construct refusal features attack, precisely simulating agnostic jailbreak attack types requiring adversarial mitigation. At the inference stage, an embedding-level calibration mechanism is employed to alleviate over-refusal behaviors with minimal computational overhead. Experimental results demonstrate that, compared to various defense methods across five types of jailbreak attacks, LATPC framework achieves a superior balance between safety and utility. Moreover, our analysis underscores the effectiveness of extracting safety-critical dimensions from the latent space for constructing robust refusal feature attacks.</li>
</ul>

<h3>Title: ClusterViG: Efficient Globally Aware Vision GNNs via Image Partitioning</h3>
<ul>
<li><strong>Authors: </strong>Dhruv Parikh, Jacob Fein-Ashley, Tian Ye, Rajgopal Kannan, Viktor Prasanna</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10640">https://arxiv.org/abs/2501.10640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10640">https://arxiv.org/pdf/2501.10640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10640]] ClusterViG: Efficient Globally Aware Vision GNNs via Image Partitioning(https://arxiv.org/abs/2501.10640)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Convolutional Neural Networks (CNN) and Vision Transformers (ViT) have dominated the field of Computer Vision (CV). Graph Neural Networks (GNN) have performed remarkably well across diverse domains because they can represent complex relationships via unstructured graphs. However, the applicability of GNNs for visual tasks was unexplored till the introduction of Vision GNNs (ViG). Despite the success of ViGs, their performance is severely bottlenecked due to the expensive $k$-Nearest Neighbors ($k$-NN) based graph construction. Recent works addressing this bottleneck impose constraints on the flexibility of GNNs to build unstructured graphs, undermining their core advantage while introducing additional inefficiencies. To address these issues, in this paper, we propose a novel method called Dynamic Efficient Graph Convolution (DEGC) for designing efficient and globally aware ViGs. DEGC partitions the input image and constructs graphs in parallel for each partition, improving graph construction efficiency. Further, DEGC integrates local intra-graph and global inter-graph feature learning, enabling enhanced global context awareness. Using DEGC as a building block, we propose a novel CNN-GNN architecture, ClusterViG, for CV tasks. Extensive experiments indicate that ClusterViG reduces end-to-end inference latency for vision tasks by up to $5\times$ when compared against a suite of models such as ViG, ViHGNN, PVG, and GreedyViG, with a similar model parameter count. Additionally, ClusterViG reaches state-of-the-art performance on image classification, object detection, and instance segmentation tasks, demonstrating the effectiveness of the proposed globally aware learning strategy. Finally, input partitioning performed by DEGC enables ClusterViG to be trained efficiently on higher-resolution images, underscoring the scalability of our approach.</li>
</ul>

<h3>Title: Iterative Tree Analysis for Medical Critics</h3>
<ul>
<li><strong>Authors: </strong>Zenan Huang, Mingwei Li, Zheng Zhou, Youxin Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10642">https://arxiv.org/abs/2501.10642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10642">https://arxiv.org/pdf/2501.10642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10642]] Iterative Tree Analysis for Medical Critics(https://arxiv.org/abs/2501.10642)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have been widely adopted across various domains, yet their application in the medical field poses unique challenges, particularly concerning the generation of hallucinations. Hallucinations in open-ended long medical text manifest as misleading critical claims, which are difficult to verify due to two reasons. First, critical claims are often deeply entangled within the text and cannot be extracted based solely on surface-level presentation. Second, verifying these claims is challenging because surface-level token-based retrieval often lacks precise or specific evidence, leaving the claims unverifiable without deeper mechanism-based analysis. In this paper, we introduce a novel method termed Iterative Tree Analysis (ITA) for medical critics. ITA is designed to extract implicit claims from long medical texts and verify each claim through an iterative and adaptive tree-like reasoning process. This process involves a combination of top-down task decomposition and bottom-up evidence consolidation, enabling precise verification of complex medical claims through detailed mechanism-level reasoning. Our extensive experiments demonstrate that ITA significantly outperforms previous methods in detecting factual inaccuracies in complex medical text verification tasks by 10%. Additionally, we will release a comprehensive test set to the public, aiming to foster further advancements in research within this domain.</li>
</ul>

<h3>Title: UAV-Assisted Multi-Task Federated Learning with Task Knowledge Sharing</h3>
<ul>
<li><strong>Authors: </strong>Yubo Yang, Tao Yang, Xiaofeng Wu, Bo Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10644">https://arxiv.org/abs/2501.10644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10644">https://arxiv.org/pdf/2501.10644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10644]] UAV-Assisted Multi-Task Federated Learning with Task Knowledge Sharing(https://arxiv.org/abs/2501.10644)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>The rapid development of Unmanned aerial vehicles (UAVs) technology has spawned a wide variety of applications, such as emergency communications, regional surveillance, and disaster relief. Due to their limited battery capacity and processing power, multiple UAVs are often required for complex tasks. In such cases, a control center is crucial for coordinating their activities, which fits well with the federated learning (FL) framework. However, conventional FL approaches often focus on a single task, ignoring the potential of training multiple related tasks simultaneously. In this paper, we propose a UAV-assisted multi-task federated learning scheme, in which data collected by multiple UAVs can be used to train multiple related tasks concurrently. The scheme facilitates the training process by sharing feature extractors across related tasks and introduces a task attention mechanism to balance task performance and encourage knowledge sharing. To provide an analytical description of training performance, the convergence analysis of the proposed scheme is performed. Additionally, the optimal bandwidth allocation for UAVs under limited bandwidth conditions is derived to minimize communication time. Meanwhile, a UAV-EV association strategy based on coalition formation game is proposed. Simulation results validate the effectiveness of the proposed scheme in enhancing multi-task performance and training speed.</li>
</ul>

<h3>Title: Can Multimodal LLMs do Visual Temporal Understanding and Reasoning? The answer is No!</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Fazli Imam, Chenyang Lyu, Alham Fikri Aji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10674">https://arxiv.org/abs/2501.10674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10674">https://arxiv.org/pdf/2501.10674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10674]] Can Multimodal LLMs do Visual Temporal Understanding and Reasoning? The answer is No!(https://arxiv.org/abs/2501.10674)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have achieved significant advancements in tasks like Visual Question Answering (VQA) by leveraging foundational Large Language Models (LLMs). However, their abilities in specific areas such as temporal understanding, which is crucial for comprehending real-world dynamics, remain underexplored. To address this, we propose a challenging evaluation benchmark named TemporalVQA, consisting of two parts: (1) Temporal Order Understanding and (2) Time-lapse Estimation. The first part requires MLLMs to determine the sequence of events by analyzing temporally consecutive video frames. The second part presents image pairs with varying time differences, framed as multiple-choice questions, asking MLLMs to estimate the time-lapse between images with options ranging from seconds to years. Our evaluations of advanced MLLMs, including models like GPT-4o and Gemini-1.5-Pro, reveal significant challenges: GPT-4o achieved only 43.8% average consistent accuracy in temporal order tasks and 70% in time-lapse estimation, with open-source models performing even less effectively. These findings underscore the limitations of current MLLMs in visual temporal understanding and reasoning, highlighting the need for further improvements in their temporal capabilities. Our dataset can be found at this https URL.</li>
</ul>

<h3>Title: Class-Imbalanced-Aware Adaptive Dataset Distillation for Scalable Pretrained Model on Credit Scoring</h3>
<ul>
<li><strong>Authors: </strong>Xia Li, Hanghang Zheng, Xiao Chen, Hong Liu, Mao Mao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-fin.RM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10677">https://arxiv.org/abs/2501.10677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10677">https://arxiv.org/pdf/2501.10677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10677]] Class-Imbalanced-Aware Adaptive Dataset Distillation for Scalable Pretrained Model on Credit Scoring(https://arxiv.org/abs/2501.10677)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The advent of artificial intelligence has significantly enhanced credit scoring technologies. Despite the remarkable efficacy of advanced deep learning models, mainstream adoption continues to favor tree-structured models due to their robust predictive performance on tabular data. Although pretrained models have seen considerable development, their application within the financial realm predominantly revolves around question-answering tasks and the use of such models for tabular-structured credit scoring datasets remains largely unexplored. Tabular-oriented large models, such as TabPFN, has made the application of large models in credit scoring feasible, albeit can only processing with limited sample sizes. This paper provides a novel framework to combine tabular-tailored dataset distillation technique with the pretrained model, empowers the scalability for TabPFN. Furthermore, though class imbalance distribution is the common nature in financial datasets, its influence during dataset distillation has not been explored. We thus integrate the imbalance-aware techniques during dataset distillation, resulting in improved performance in financial datasets (e.g., a 2.5% enhancement in AUC). This study presents a novel framework for scaling up the application of large pretrained models on financial tabular datasets and offers a comparative analysis of the influence of class imbalance on the dataset distillation process. We believe this approach can broaden the applications and downstream tasks of large models in the financial domain.</li>
</ul>

<h3>Title: Deep Operator Networks for Bayesian Parameter Estimation in PDEs</h3>
<ul>
<li><strong>Authors: </strong>Amogh Raj, Carol Eunice Gudumotou, Sakol Bun, Keerthana Srinivasa, Arash Sarshar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10684">https://arxiv.org/abs/2501.10684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10684">https://arxiv.org/pdf/2501.10684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10684]] Deep Operator Networks for Bayesian Parameter Estimation in PDEs(https://arxiv.org/abs/2501.10684)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>We present a novel framework combining Deep Operator Networks (DeepONets) with Physics-Informed Neural Networks (PINNs) to solve partial differential equations (PDEs) and estimate their unknown parameters. By integrating data-driven learning with physical constraints, our method achieves robust and accurate solutions across diverse scenarios. Bayesian training is implemented through variational inference, allowing for comprehensive uncertainty quantification for both aleatoric and epistemic uncertainties. This ensures reliable predictions and parameter estimates even in noisy conditions or when some of the physical equations governing the problem are missing. The framework demonstrates its efficacy in solving forward and inverse problems, including the 1D unsteady heat equation and 2D reaction-diffusion equations, as well as regression tasks with sparse, noisy observations. This approach provides a computationally efficient and generalizable method for addressing uncertainty quantification in PDE surrogate modeling.</li>
</ul>

<h3>Title: Harnessing the Potential of Large Language Models in Modern Marketing Management: Applications, Future Directions, and Strategic Recommendations</h3>
<ul>
<li><strong>Authors: </strong>Raha Aghaei, Ali A. Kiaei, Mahnaz Boush, Javad Vahidi, Mohammad Zavvar, Zeynab Barzegar, Mahan Rofoosheh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10685">https://arxiv.org/abs/2501.10685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10685">https://arxiv.org/pdf/2501.10685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10685]] Harnessing the Potential of Large Language Models in Modern Marketing Management: Applications, Future Directions, and Strategic Recommendations(https://arxiv.org/abs/2501.10685)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized the process of customer engagement, campaign optimization, and content generation, in marketing management. In this paper, we explore the transformative potential of LLMs along with the current applications, future directions, and strategic recommendations for marketers. In particular, we focus on LLMs major business drivers such as personalization, real-time-interactive customer insights, and content automation, and how they enable customers and business outcomes. For instance, the ethical aspects of AI with respect to data privacy, transparency, and mitigation of bias are also covered, with the goal of promoting responsible use of the technology through best practices and the use of new technologies businesses can tap into the LLM potential, which help growth and stay one step ahead in the turmoil of digital marketing. This article is designed to give marketers the necessary guidance by using best industry practices to integrate these powerful LLMs into their marketing strategy and innovation without compromising on the ethos of their brand.</li>
</ul>

<h3>Title: EMO2: End-Effector Guided Audio-Driven Avatar Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Linrui Tian, Siqi Hu, Qi Wang, Bang Zhang, Liefeng Bo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10687">https://arxiv.org/abs/2501.10687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10687">https://arxiv.org/pdf/2501.10687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10687]] EMO2: End-Effector Guided Audio-Driven Avatar Video Generation(https://arxiv.org/abs/2501.10687)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a novel audio-driven talking head method capable of simultaneously generating highly expressive facial expressions and hand gestures. Unlike existing methods that focus on generating full-body or half-body poses, we investigate the challenges of co-speech gesture generation and identify the weak correspondence between audio features and full-body gestures as a key limitation. To address this, we redefine the task as a two-stage process. In the first stage, we generate hand poses directly from audio input, leveraging the strong correlation between audio signals and hand movements. In the second stage, we employ a diffusion model to synthesize video frames, incorporating the hand poses generated in the first stage to produce realistic facial expressions and body movements. Our experimental results demonstrate that the proposed method outperforms state-of-the-art approaches, such as CyberHost and Vlogger, in terms of both visual quality and synchronization accuracy. This work provides a new perspective on audio-driven gesture generation and a robust framework for creating expressive and natural talking head animations.</li>
</ul>

<h3>Title: Simulation of Hypergraph Algorithms with Looped Transformers</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Li, Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song, Zhen Zhuang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CC, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10688">https://arxiv.org/abs/2501.10688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10688">https://arxiv.org/pdf/2501.10688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10688]] Simulation of Hypergraph Algorithms with Looped Transformers(https://arxiv.org/abs/2501.10688)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Looped Transformers have shown exceptional capability in simulating traditional graph algorithms, but their application to more complex structures like hypergraphs remains underexplored. Hypergraphs generalize graphs by modeling higher-order relationships among multiple entities, enabling richer representations but introducing significant computational challenges. In this work, we extend the Loop Transformer architecture to simulate hypergraph algorithms efficiently, addressing the gap between neural networks and combinatorial optimization over hypergraphs. In this paper, we extend the Loop Transformer architecture to simulate hypergraph algorithms efficiently, addressing the gap between neural networks and combinatorial optimization over hypergraphs. Specifically, we propose a novel degradation mechanism for reducing hypergraphs to graph representations, enabling the simulation of graph-based algorithms, such as Dijkstra's shortest path. Furthermore, we introduce a hyperedge-aware encoding scheme to simulate hypergraph-specific algorithms, exemplified by Helly's algorithm. The paper establishes theoretical guarantees for these simulations, demonstrating the feasibility of processing high-dimensional and combinatorial data using Loop Transformers. This work highlights the potential of Transformers as general-purpose algorithmic solvers for structured data.</li>
</ul>

<h3>Title: VENENA: A Deceptive Visual Encryption Framework for Wireless Semantic Secrecy</h3>
<ul>
<li><strong>Authors: </strong>Bin Han, Ye Yuan, Hans D. Schotten</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10699">https://arxiv.org/abs/2501.10699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10699">https://arxiv.org/pdf/2501.10699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10699]] VENENA: A Deceptive Visual Encryption Framework for Wireless Semantic Secrecy(https://arxiv.org/abs/2501.10699)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, transformer</a></li>
<li><strong>Abstract: </strong>Eavesdropping has been a long-standing threat to the security and privacy of wireless communications, since it is difficult to detect and costly to prevent. As networks evolve towards Sixth Generation (6G) and semantic communication becomes increasingly central to next-generation wireless systems, securing semantic information transmission emerges as a critical challenge. While classical physical layer security (PLS) focuses on passive security, the recently proposed concept of physical layer deception (PLD) offers a semantic encryption measure to actively deceive eavesdroppers. Yet the existing studies of PLD have been dominantly information-theoretical and link-level oriented, lacking considerations of system-level design and practical implementation. In this work we propose a novel artificial intelligence (AI)-enabled framework called Visual ENcryption for Eavesdropping NegAtion (VENENA), which combines the techniques of PLD, visual encryption, and image poisoning, into a comprehensive mechanism for deceptive secure semantic transmission in future wireless networks. By leveraging advanced vision transformers and semantic codecs, VENENA demonstrates how semantic security can be enhanced through the synergy of physical layer techniques and artificial intelligence, paving the way for secure semantic communication in 6G networks.</li>
</ul>

<h3>Title: FSMoE: A Flexible and Scalable Training System for Sparse Mixture-of-Experts Models</h3>
<ul>
<li><strong>Authors: </strong>Xinglin Pan, Wenxiang Lin, Lin Zhang, Shaohuai Shi, Zhenheng Tang, Rui Wang, Bo Li, Xiaowen Chu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10714">https://arxiv.org/abs/2501.10714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10714">https://arxiv.org/pdf/2501.10714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10714]] FSMoE: A Flexible and Scalable Training System for Sparse Mixture-of-Experts Models(https://arxiv.org/abs/2501.10714)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent large language models (LLMs) have tended to leverage sparsity to reduce computations, employing the sparsely activated mixture-of-experts (MoE) technique. MoE introduces four modules, including token routing, token communication, expert computation, and expert parallelism, that impact model quality and training efficiency. To enable versatile usage of MoE models, we introduce FSMoE, a flexible training system optimizing task scheduling with three novel techniques: 1) Unified abstraction and online profiling of MoE modules for task scheduling across various MoE implementations. 2) Co-scheduling intra-node and inter-node communications with computations to minimize communication overheads. 3) To support near-optimal task scheduling, we design an adaptive gradient partitioning method for gradient aggregation and a schedule to adaptively pipeline communications and computations. We conduct extensive experiments with configured MoE layers and real-world MoE models on two GPU clusters. Experimental results show that 1) our FSMoE supports four popular types of MoE routing functions and is more efficient than existing implementations (with up to a 1.42$\times$ speedup), and 2) FSMoE outperforms the state-of-the-art MoE training systems (DeepSpeed-MoE and Tutel) by 1.18$\times$-1.22$\times$ on 1458 MoE layers and 1.19$\times$-3.01$\times$ on real-world MoE models based on GPT-2 and Mixtral using a popular routing function.</li>
</ul>

<h3>Title: A CNN-Transformer for Classification of Longitudinal 3D MRI Images -- A Case Study on Hepatocellular Carcinoma Prediction</h3>
<ul>
<li><strong>Authors: </strong>Jakob Nolte, Maureen M. J. Guichelaar, Donald E. Bouman, Stephanie M. van den Berg, Maryam Amir Haeri</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10733">https://arxiv.org/abs/2501.10733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10733">https://arxiv.org/pdf/2501.10733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10733]] A CNN-Transformer for Classification of Longitudinal 3D MRI Images -- A Case Study on Hepatocellular Carcinoma Prediction(https://arxiv.org/abs/2501.10733)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Longitudinal MRI analysis is crucial for predicting disease outcomes, particularly in chronic conditions like hepatocellular carcinoma (HCC), where early detection can significantly influence treatment strategies and patient prognosis. Yet, due to challenges like limited data availability, subtle parenchymal changes, and the irregular timing of medical screenings, current approaches have so far focused on cross-sectional imaging data. To address this, we propose HCCNet, a novel model architecture that integrates a 3D adaptation of the ConvNeXt CNN architecture with a Transformer encoder, capturing both the intricate spatial features of 3D MRIs and the complex temporal dependencies across different time points. HCCNet utilizes a two-stage pre-training process tailored for longitudinal MRI data. The CNN backbone is pre-trained using a self-supervised learning framework adapted for 3D MRIs, while the Transformer encoder is pre-trained with a sequence-order-prediction task to enhance its understanding of disease progression over time. We demonstrate the effectiveness of HCCNet by applying it to a cohort of liver cirrhosis patients undergoing regular MRI screenings for HCC surveillance. Our results show that HCCNet significantly improves predictive accuracy and reliability over baseline models, providing a robust tool for personalized HCC surveillance. The methodological approach presented in this paper is versatile and can be adapted to various longitudinal MRI screening applications. Its ability to handle varying patient record lengths and irregular screening intervals establishes it as an invaluable framework for monitoring chronic diseases, where timely and accurate disease prognosis is critical for effective treatment planning.</li>
</ul>

<h3>Title: Semi-supervised Semantic Segmentation for Remote Sensing Images via Multi-scale Uncertainty Consistency and Cross-Teacher-Student Attention</h3>
<ul>
<li><strong>Authors: </strong>Shanwen Wang, Changrui Chen, Xin Sun, Danfeng Hong, Jungong Han</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10736">https://arxiv.org/abs/2501.10736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10736">https://arxiv.org/pdf/2501.10736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10736]] Semi-supervised Semantic Segmentation for Remote Sensing Images via Multi-scale Uncertainty Consistency and Cross-Teacher-Student Attention(https://arxiv.org/abs/2501.10736)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semi-supervised learning offers an appealing solution for remote sensing (RS) image segmentation to relieve the burden of labor-intensive pixel-level labeling. However, RS images pose unique challenges, including rich multi-scale features and high inter-class similarity. To address these problems, this paper proposes a novel semi-supervised Multi-Scale Uncertainty and Cross-Teacher-Student Attention (MUCA) model for RS image semantic segmentation tasks. Specifically, MUCA constrains the consistency among feature maps at different layers of the network by introducing a multi-scale uncertainty consistency regularization. It improves the multi-scale learning capability of semi-supervised algorithms on unlabeled data. Additionally, MUCA utilizes a Cross-Teacher-Student attention mechanism to guide the student network, guiding the student network to construct more discriminative feature representations through complementary features from the teacher network. This design effectively integrates weak and strong augmentations (WA and SA) to further boost segmentation performance. To verify the effectiveness of our model, we conduct extensive experiments on ISPRS-Potsdam and LoveDA datasets. The experimental results show the superiority of our method over state-of-the-art semi-supervised methods. Notably, our model excels in distinguishing highly similar objects, showcasing its potential for advancing semi-supervised RS image segmentation tasks.</li>
</ul>

<h3>Title: Computational Discovery of Chiasmus in Ancient Religious Text</h3>
<ul>
<li><strong>Authors: </strong>Hope McGovern, Hale Sirin, Tom Lippincott</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10739">https://arxiv.org/abs/2501.10739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10739">https://arxiv.org/pdf/2501.10739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10739]] Computational Discovery of Chiasmus in Ancient Religious Text(https://arxiv.org/abs/2501.10739)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Chiasmus, a debated literary device in Biblical texts, has captivated mystics while sparking ongoing scholarly discussion. In this paper, we introduce the first computational approach to systematically detect chiasmus within Biblical passages. Our method leverages neural embeddings to capture lexical and semantic patterns associated with chiasmus, applied at multiple levels of textual granularity (half-verses, verses). We also involve expert annotators to review a subset of the detected patterns. Despite its computational efficiency, our method achieves robust results, with high inter-annotator agreement and system precision@k of 0.80 at the verse level and 0.60 at the half-verse level. We further provide a qualitative analysis of the distribution of detected chiasmi, along with selected examples that highlight the effectiveness of our approach.</li>
</ul>

<h3>Title: Development of Application-Specific Large Language Models to Facilitate Research Ethics Review</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Porsdam Mann, Joel Seah Jiehao, Stephen R. Latham, Julian Savulescu, Mateo Aboy, Brian D. Earp</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10741">https://arxiv.org/abs/2501.10741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10741">https://arxiv.org/pdf/2501.10741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10741]] Development of Application-Specific Large Language Models to Facilitate Research Ethics Review(https://arxiv.org/abs/2501.10741)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Institutional review boards (IRBs) play a crucial role in ensuring the ethical conduct of human subjects research, but face challenges including inconsistency, delays, and inefficiencies. We propose the development and implementation of application-specific large language models (LLMs) to facilitate IRB review processes. These IRB-specific LLMs would be fine-tuned on IRB-specific literature and institutional datasets, and equipped with retrieval capabilities to access up-to-date, context-relevant information. We outline potential applications, including pre-review screening, preliminary analysis, consistency checking, and decision support. While addressing concerns about accuracy, context sensitivity, and human oversight, we acknowledge remaining challenges such as over-reliance on AI and the need for transparency. By enhancing the efficiency and quality of ethical review while maintaining human judgment in critical decisions, IRB-specific LLMs offer a promising tool to improve research oversight. We call for pilot studies to evaluate the feasibility and impact of this approach.</li>
</ul>

<h3>Title: MedFILIP: Medical Fine-grained Language-Image Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Xinjie Liang, Xiangyu Li, Fanding Li, Jie Jiang, Qing Dong, Wei Wang, Kuanquan Wang, Suyu Dong, Gongning Luo, Shuo Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10775">https://arxiv.org/abs/2501.10775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10775">https://arxiv.org/pdf/2501.10775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10775]] MedFILIP: Medical Fine-grained Language-Image Pre-training(https://arxiv.org/abs/2501.10775)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Medical vision-language pretraining (VLP) that leverages naturally-paired medical image-report data is crucial for medical image analysis. However, existing methods struggle to accurately characterize associations between images and diseases, leading to inaccurate or incomplete diagnostic results. In this work, we propose MedFILIP, a fine-grained VLP model, introduces medical image-specific knowledge through contrastive learning, specifically: 1) An information extractor based on a large language model is proposed to decouple comprehensive disease details from reports, which excels in extracting disease deals through flexible prompt engineering, thereby effectively reducing text complexity while retaining rich information at a tiny cost. 2) A knowledge injector is proposed to construct relationships between categories and visual attributes, which help the model to make judgments based on image features, and fosters knowledge extrapolation to unfamiliar disease categories. 3) A semantic similarity matrix based on fine-grained annotations is proposed, providing smoother, information-richer labels, thus allowing fine-grained image-text alignment. 4) We validate MedFILIP on numerous datasets, e.g., RSNA-Pneumonia, NIH ChestX-ray14, VinBigData, and COVID-19. For single-label, multi-label, and fine-grained classification, our model achieves state-of-the-art performance, the classification accuracy has increased by a maximum of 6.69\%. The code is available in this https URL.</li>
</ul>

<h3>Title: Measuring Fairness in Financial Transaction Machine Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Carlos Mougan, Deniz Sezin Ayvaz, Lorenzo Belenguer, Hankun He, Deborah Dormah Kanubala, Mingxu Li, Soung Low, Faithful Chiagoziem Onwuegbuche, Yulu Pi, Natalia Sikora, Dan Tran, Shresth Verma, Hanzhi Wang, Skyler Xie, Adeline Pelletier</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10784">https://arxiv.org/abs/2501.10784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10784">https://arxiv.org/pdf/2501.10784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10784]] Measuring Fairness in Financial Transaction Machine Learning Models(https://arxiv.org/abs/2501.10784)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Mastercard, a global leader in financial services, develops and deploys machine learning models aimed at optimizing card usage and preventing attrition through advanced predictive models. These models use aggregated and anonymized card usage patterns, including cross-border transactions and industry-specific spending, to tailor bank offerings and maximize revenue opportunities. Mastercard has established an AI Governance program, based on its Data and Tech Responsibility Principles, to evaluate any built and bought AI for efficacy, fairness, and transparency. As part of this effort, Mastercard has sought expertise from the Turing Institute through a Data Study Group to better assess fairness in more complex AI/ML models. The Data Study Group challenge lies in defining, measuring, and mitigating fairness in these predictions, which can be complex due to the various interpretations of fairness, gaps in the research literature, and ML-operations challenges.</li>
</ul>

<h3>Title: LD-DETR: Loop Decoder DEtection TRansformer for Video Moment Retrieval and Highlight Detection</h3>
<ul>
<li><strong>Authors: </strong>Pengcheng Zhao, Zhixian He, Fuwei Zhang, Shujin Lin, Fan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10787">https://arxiv.org/abs/2501.10787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10787">https://arxiv.org/pdf/2501.10787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10787]] LD-DETR: Loop Decoder DEtection TRansformer for Video Moment Retrieval and Highlight Detection(https://arxiv.org/abs/2501.10787)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Video Moment Retrieval and Highlight Detection aim to find corresponding content in the video based on a text query. Existing models usually first use contrastive learning methods to align video and text features, then fuse and extract multimodal information, and finally use a Transformer Decoder to decode multimodal information. However, existing methods face several issues: (1) Overlapping semantic information between different samples in the dataset hinders the model's multimodal aligning performance; (2) Existing models are not able to efficiently extract local features of the video; (3) The Transformer Decoder used by the existing model cannot adequately decode multimodal features. To address the above issues, we proposed the LD-DETR model for Video Moment Retrieval and Highlight Detection tasks. Specifically, we first distilled the similarity matrix into the identity matrix to mitigate the impact of overlapping semantic information. Then, we designed a method that enables convolutional layers to extract multimodal local features more efficiently. Finally, we fed the output of the Transformer Decoder back into itself to adequately decode multimodal information. We evaluated LD-DETR on four public benchmarks and conducted extensive experiments to demonstrate the superiority and effectiveness of our approach. Our model outperforms the State-Of-The-Art models on QVHighlight, Charades-STA and TACoS datasets. Our code is available at this https URL.</li>
</ul>

<h3>Title: Dynamic Trend Fusion Module for Traffic Flow Prediction</h3>
<ul>
<li><strong>Authors: </strong>Jing Chen, Haocheng Ye, Zhian Ying, Yuntao Sun, Wenqiang Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10796">https://arxiv.org/abs/2501.10796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10796">https://arxiv.org/pdf/2501.10796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10796]] Dynamic Trend Fusion Module for Traffic Flow Prediction(https://arxiv.org/abs/2501.10796)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate traffic flow prediction is essential for applications like transport logistics but remains challenging due to complex spatio-temporal correlations and non-linear traffic patterns. Existing methods often model spatial and temporal dependencies separately, failing to effectively fuse them. To overcome this limitation, the Dynamic Spatial-Temporal Trend Transformer DST2former is proposed to capture spatio-temporal correlations through adaptive embedding and to fuse dynamic and static information for learning multi-view dynamic features of traffic networks. The approach employs the Dynamic Trend Representation Transformer (DTRformer) to generate dynamic trends using encoders for both temporal and spatial dimensions, fused via Cross Spatial-Temporal Attention. Predefined graphs are compressed into a representation graph to extract static attributes and reduce redundancy. Experiments on four real-world traffic datasets demonstrate that our framework achieves state-of-the-art performance.</li>
</ul>

<h3>Title: Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback</h3>
<ul>
<li><strong>Authors: </strong>Yen-Ting Lin, Di Jin, Tengyu Xu, Tianhao Wu, Sainbayar Sukhbaatar, Chen Zhu, Yun He, Yun-Nung Chen, Jason Weston, Yuandong Tian, Arash Rahnama, Sinong Wang, Hao Ma, Han Fang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10799">https://arxiv.org/abs/2501.10799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10799">https://arxiv.org/pdf/2501.10799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10799]] Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback(https://arxiv.org/abs/2501.10799)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently demonstrated remarkable success in mathematical reasoning. Despite progress in methods like chain-of-thought prompting and self-consistency sampling, these advances often focus on final correctness without ensuring that the underlying reasoning process is coherent and reliable. This paper introduces Step-KTO, a training framework that combines process-level and outcome-level binary feedback to guide LLMs toward more trustworthy reasoning trajectories. By providing binary evaluations for both the intermediate reasoning steps and the final answer, Step-KTO encourages the model to adhere to logical progressions rather than relying on superficial shortcuts. Our experiments on challenging mathematical benchmarks show that Step-KTO significantly improves both final answer accuracy and the quality of intermediate reasoning steps. For example, on the MATH-500 dataset, Step-KTO achieves a notable improvement in Pass@1 accuracy over strong baselines. These results highlight the promise of integrating stepwise process feedback into LLM training, paving the way toward more interpretable and dependable reasoning capabilities.</li>
</ul>

<h3>Title: Jailbreaking Large Language Models in Infinitely Many Ways</h3>
<ul>
<li><strong>Authors: </strong>Oliver Goldstein, Emanuele La Malfa, Felix Drinkall, Samuele Marro, Michael Wooldridge</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10800">https://arxiv.org/abs/2501.10800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10800">https://arxiv.org/pdf/2501.10800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10800]] Jailbreaking Large Language Models in Infinitely Many Ways(https://arxiv.org/abs/2501.10800)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, large language model</a></li>
<li><strong>Abstract: </strong>We discuss the "Infinitely Many Meanings" attacks (IMM), a category of jailbreaks that leverages the increasing capabilities of a model to handle paraphrases and encoded communications to bypass their defensive mechanisms. IMMs' viability pairs and grows with a model's capabilities to handle and bind the semantics of simple mappings between tokens and work extremely well in practice, posing a concrete threat to the users of the most powerful LLMs in commerce. We show how one can bypass the safeguards of the most powerful open- and closed-source LLMs and generate content that explicitly violates their safety policies. One can protect against IMMs by improving the guardrails and making them scale with the LLMs' capabilities. For two categories of attacks that are straightforward to implement, i.e., bijection and encoding, we discuss two defensive strategies, one in token and the other in embedding space. We conclude with some research questions we believe should be prioritised to enhance the defensive mechanisms of LLMs and our understanding of their safety.</li>
</ul>

<h3>Title: An Interpretable Measure for Quantifying Predictive Dependence between Continuous Random Variables -- Extended Version</h3>
<ul>
<li><strong>Authors: </strong>Renato Assunção, Flávio Figueiredo, Francisco N. Tinoco Júnior, Léo M. de Sá-Freire, Fábio Silva</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10815">https://arxiv.org/abs/2501.10815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10815">https://arxiv.org/pdf/2501.10815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10815]] An Interpretable Measure for Quantifying Predictive Dependence between Continuous Random Variables -- Extended Version(https://arxiv.org/abs/2501.10815)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>A fundamental task in statistical learning is quantifying the joint dependence or association between two continuous random variables. We introduce a novel, fully non-parametric measure that assesses the degree of association between continuous variables $X$ and $Y$, capable of capturing a wide range of relationships, including non-functional ones. A key advantage of this measure is its interpretability: it quantifies the expected relative loss in predictive accuracy when the distribution of $X$ is ignored in predicting $Y$. This measure is bounded within the interval [0,1] and is equal to zero if and only if $X$ and $Y$ are independent. We evaluate the performance of our measure on over 90,000 real and synthetic datasets, benchmarking it against leading alternatives. Our results demonstrate that the proposed measure provides valuable insights into underlying relationships, particularly in cases where existing methods fail to capture important dependencies.</li>
</ul>

<h3>Title: A comprehensive survey on RPL routing-based attacks, defences and future directions in Internet of Things</h3>
<ul>
<li><strong>Authors: </strong>Anil K Prajapati, Emmanuel S Pilli, Ramesh B Battula, Vijay Varadharajan, Abhishek Verma, R C Joshi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10817">https://arxiv.org/abs/2501.10817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10817">https://arxiv.org/pdf/2501.10817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10817]] A comprehensive survey on RPL routing-based attacks, defences and future directions in Internet of Things(https://arxiv.org/abs/2501.10817)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>The Internet of Things (IoT) is a network of digital devices like sensors, processors, embedded and communication devices that can connect to and exchange data with other devices and systems over the internet. IoT devices have limitations on power, memory, and computational resources. Researchers have developed the IPv6 Over Low-power Wireless Personal Area Network (6LoWPAN) protocols to provide wireless connectivity among these devices while overcoming the constraints on resources. 6LoWPAN has been approved subsequently by the Internet Engineering Task Force (IETF). The IETF Routing Over Low-power and Lossy Networks (ROLL) standardized the Routing Protocol for LLNs known as RPL (IETF RFC 6550), which is part of the 6LoWPAN stack. However, IoT devices are vulnerable to various attacks on RPL-based routing. This survey provides an in depth study of existing RPL-based attacks and defense published from year 2011 to 2024 from highly reputed journals and conferences. By thematic analysis of existing routing attacks on RPL, we developed a novel attack taxonomy which focuses on the nature of routing attacks and classifies them into 12 major categories. Subsequently, the impact of each attack on the network is analyzed and discussed real life scenarios of these attacks. Another contribution of this survey proposed a novel taxonomy for classification of defense mechanisms into 8 major categories against routing attacks based on type of defense strategy. The detailed analysis of each defense mechanism with real life applicability is explained. Furthermore, evaluation tools such as testbeds and simulators for RPL-based attack and defense are discussed and critically analyzed in terms of real world applicability. Finally, open research challenges are presented on the basis of research gaps of existing literature along with research directions for practitioners and researchers.</li>
</ul>

<h3>Title: GAUDA: Generative Adaptive Uncertainty-guided Diffusion-based Augmentation for Surgical Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yannik Frisch, Christina Bornberg, Moritz Fuchs, Anirban Mukhopadhyay</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10819">https://arxiv.org/abs/2501.10819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10819">https://arxiv.org/pdf/2501.10819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10819]] GAUDA: Generative Adaptive Uncertainty-guided Diffusion-based Augmentation for Surgical Segmentation(https://arxiv.org/abs/2501.10819)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Augmentation by generative modelling yields a promising alternative to the accumulation of surgical data, where ethical, organisational and regulatory aspects must be considered. Yet, the joint synthesis of (image, mask) pairs for segmentation, a major application in surgery, is rather unexplored. We propose to learn semantically comprehensive yet compact latent representations of the (image, mask) space, which we jointly model with a Latent Diffusion Model. We show that our approach can effectively synthesise unseen high-quality paired segmentation data of remarkable semantic coherence. Generative augmentation is typically applied pre-training by synthesising a fixed number of additional training samples to improve downstream task models. To enhance this approach, we further propose Generative Adaptive Uncertainty-guided Diffusion-based Augmentation (GAUDA), leveraging the epistemic uncertainty of a Bayesian downstream model for targeted online synthesis. We condition the generative model on classes with high estimated uncertainty during training to produce additional unseen samples for these classes. By adaptively utilising the generative model online, we can minimise the number of additional training samples and centre them around the currently most uncertain parts of the data distribution. GAUDA effectively improves downstream segmentation results over comparable methods by an average absolute IoU of 1.6% on CaDISv2 and 1.5% on CholecSeg8k, two prominent surgical datasets for semantic segmentation.</li>
</ul>

<h3>Title: Addressing Multilabel Imbalance with an Efficiency-Focused Approach Using Diffusion Model-Generated Synthetic Samples</h3>
<ul>
<li><strong>Authors: </strong>Francisco Charte, Miguel Ángel Dávila, María Dolores Pérez-Godoy, María José del Jesus</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10822">https://arxiv.org/abs/2501.10822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10822">https://arxiv.org/pdf/2501.10822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10822]] Addressing Multilabel Imbalance with an Efficiency-Focused Approach Using Diffusion Model-Generated Synthetic Samples(https://arxiv.org/abs/2501.10822)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Predictive models trained on imbalanced data tend to produce biased results. This problem is exacerbated when there is not just one output label, but a set of them. This is the case for multilabel learning (MLL) algorithms used to classify patterns, rank labels, or learn the distribution of outputs. Many solutions have been proposed in the literature. The one that can be applied universally, independent of the algorithm used to build the model, is data resampling. The generation of new instances associated with minority labels, so that empty areas of the feature space are filled, helps to improve the obtained models. The quality of these new instances depends on the algorithm used to generate them. In this paper, a diffusion model tailored to produce new instances for MLL data, called MLDM (\textit{MultiLabel Diffusion Model}), is proposed. Diffusion models have been mainly used to generate artificial images and videos. Our proposed MLDM is based on this type of models. The experiments conducted compare MLDM with several other MLL resampling algorithms. The results show that MLDM is competitive while it improves efficiency.</li>
</ul>

<h3>Title: Visual RAG: Expanding MLLM visual knowledge without fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Mirco Bonomo, Simone Bianco</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10834">https://arxiv.org/abs/2501.10834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10834">https://arxiv.org/pdf/2501.10834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10834]] Visual RAG: Expanding MLLM visual knowledge without fine-tuning(https://arxiv.org/abs/2501.10834)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have achieved notable performance in computer vision tasks that require reasoning across visual and textual modalities, yet their capabilities are limited to their pre-trained data, requiring extensive fine-tuning for updates. Recent researches have explored the use of In-Context Learning (ICL) to overcome these challenges by providing a set of demonstrating examples as context to augment MLLMs performance in several tasks, showing that many-shot ICL leads to substantial improvements compared to few-shot ICL. However, the reliance on numerous demonstrating examples and the limited MLLMs context windows presents significant obstacles. This paper aims to address these challenges by introducing a novel approach, Visual RAG, that synergically combines the MLLMs capability to learn from the context, with a retrieval mechanism. The crux of this approach is to ensure to augment the MLLM knowledge by selecting only the most relevant demonstrating examples for the query, pushing it to learn by analogy. In this way, relying on the new information provided dynamically during inference time, the resulting system is not limited to the knowledge extracted from the training data, but can be updated rapidly and easily without fine-tuning. Furthermore, this greatly reduces the computational costs for improving the model image classification performance, and augments the model knowledge to new visual domains and tasks it was not trained for. Extensive experiments on eight different datasets in the state of the art spanning several domains and image classification tasks show that the proposed Visual RAG, compared to the most recent state of the art (i.e., many-shot ICL), is able to obtain an accuracy that is very close or even higher (approx. +2% improvement on average) while using a much smaller set of demonstrating examples (approx. only 23% on average).</li>
</ul>

<h3>Title: BAP v2: An Enhanced Task Framework for Instruction Following in Minecraft Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Prashant Jayannavar, Liliang Ren, Marisa Hudspeth, Charlotte Lambert, Ariel Cordes, Elizabeth Kaplan, Anjali Narayan-Chen, Julia Hockenmaier</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10836">https://arxiv.org/abs/2501.10836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10836">https://arxiv.org/pdf/2501.10836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10836]] BAP v2: An Enhanced Task Framework for Instruction Following in Minecraft Dialogues(https://arxiv.org/abs/2501.10836)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, transformer</a></li>
<li><strong>Abstract: </strong>Interactive agents capable of understanding and executing instructions in the physical world have long been a central goal in AI research. The Minecraft Collaborative Building Task (MCBT) provides one such setting to work towards this goal (Narayan-Chen, Jayannavar, and Hockenmaier 2019). It is a two-player game in which an Architect (A) instructs a Builder (B) to construct a target structure in a simulated Blocks World Environment. We focus on the challenging Builder Action Prediction (BAP) subtask of predicting correct action sequences in a given multimodal game context with limited training data (Jayannavar, Narayan-Chen, and Hockenmaier 2020). We take a closer look at evaluation and data for the BAP task, discovering key challenges and making significant improvements on both fronts to propose BAP v2, an upgraded version of the task. This will allow future work to make more efficient and meaningful progress on it. It comprises of: (1) an enhanced evaluation benchmark that includes a cleaner test set and fairer, more insightful metrics, and (2) additional synthetic training data generated from novel Minecraft dialogue and target structure simulators emulating the MCBT. We show that the synthetic data can be used to train more performant and robust neural models even with relatively simple training methods. Looking ahead, such data could also be crucial for training more sophisticated, data-hungry deep transformer models and training/fine-tuning increasingly large LLMs. Although modeling is not the primary focus of this work, we also illustrate the impact of our data and training methodologies on a simple LLM- and transformer-based model, thus validating the robustness of our approach, and setting the stage for more advanced architectures and LLMs going forward.</li>
</ul>

<h3>Title: Practical and Ready-to-Use Methodology to Assess the re-identification Risk in Anonymized Datasets</h3>
<ul>
<li><strong>Authors: </strong>Louis-Philippe Sondeck, Maryline Laurent</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10841">https://arxiv.org/abs/2501.10841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10841">https://arxiv.org/pdf/2501.10841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10841]] Practical and Ready-to-Use Methodology to Assess the re-identification Risk in Anonymized Datasets(https://arxiv.org/abs/2501.10841)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack</a></li>
<li><strong>Abstract: </strong>To prove that a dataset is sufficiently anonymized, many privacy policies suggest that a re-identification risk assessment be performed, but do not provide a precise methodology for doing so, leaving the industry alone with the problem. This paper proposes a practical and ready-to-use methodology for re-identification risk assessment, the originality of which is manifold: (1) it is the first to follow well-known risk analysis methods (e.g. EBIOS) that have been used in the cybersecurity field for years, which consider not only the ability to perform an attack, but also the impact such an attack can have on an individual; (2) it is the first to qualify attributes and values of attributes with e.g. degree of exposure, as known real-world attacks mainly target certain types of attributes and not others.</li>
</ul>

<h3>Title: Zero-shot and Few-shot Learning with Instruction-following LLMs for Claim Matching in Automated Fact-checking</h3>
<ul>
<li><strong>Authors: </strong>Dina Pisarevskaya, Arkaitz Zubiaga</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10860">https://arxiv.org/abs/2501.10860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10860">https://arxiv.org/pdf/2501.10860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10860]] Zero-shot and Few-shot Learning with Instruction-following LLMs for Claim Matching in Automated Fact-checking(https://arxiv.org/abs/2501.10860)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The claim matching (CM) task can benefit an automated fact-checking pipeline by putting together claims that can be resolved with the same fact-check. In this work, we are the first to explore zero-shot and few-shot learning approaches to the task. We consider CM as a binary classification task and experiment with a set of instruction-following large language models (GPT-3.5-turbo, Gemini-1.5-flash, Mistral-7B-Instruct, and Llama-3-8B-Instruct), investigating prompt templates. We introduce a new CM dataset, ClaimMatch, which will be released upon acceptance. We put LLMs to the test in the CM task and find that it can be tackled by leveraging more mature yet similar tasks such as natural language inference or paraphrase detection. We also propose a pipeline for CM, which we evaluate on texts of different lengths.</li>
</ul>

<h3>Title: Diffusion-Based Imitation Learning for Social Pose Generation</h3>
<ul>
<li><strong>Authors: </strong>Antonio Lech Martin-Ozimek, Isuru Jayarathne, Su Larb Mon, Jouh Yeong Chew</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10869">https://arxiv.org/abs/2501.10869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10869">https://arxiv.org/pdf/2501.10869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10869]] Diffusion-Based Imitation Learning for Social Pose Generation(https://arxiv.org/abs/2501.10869)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Intelligent agents, such as robots and virtual agents, must understand the dynamics of complex social interactions to interact with humans. Effectively representing social dynamics is challenging because we require multi-modal, synchronized observations to understand a scene. We explore how using a single modality, the pose behavior, of multiple individuals in a social interaction can be used to generate nonverbal social cues for the facilitator of that interaction. The facilitator acts to make a social interaction proceed smoothly and is an essential role for intelligent agents to replicate in human-robot interactions. In this paper, we adapt an existing diffusion behavior cloning model to learn and replicate facilitator behaviors. Furthermore, we evaluate two representations of pose observations from a scene, one representation has pre-processing applied and one does not. The purpose of this paper is to introduce a new use for diffusion behavior cloning for pose generation in social interactions. The second is to understand the relationship between performance and computational load for generating social pose behavior using two different techniques for collecting scene observations. As such, we are essentially testing the effectiveness of two different types of conditioning for a diffusion model. We then evaluate the resulting generated behavior from each technique using quantitative measures such as mean per-joint position error (MPJPE), training time, and inference time. Additionally, we plot training and inference time against MPJPE to examine the trade-offs between efficiency and performance. Our results suggest that the further pre-processed data can successfully condition diffusion models to generate realistic social behavior, with reasonable trade-offs in accuracy and processing time.</li>
</ul>

<h3>Title: Distributed Quasi-Newton Method for Fair and Fast Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Shayan Mohajer Hamidi, Linfeng Ye</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10877">https://arxiv.org/abs/2501.10877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10877">https://arxiv.org/pdf/2501.10877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10877]] Distributed Quasi-Newton Method for Fair and Fast Federated Learning(https://arxiv.org/abs/2501.10877)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, fair</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a promising technology that enables edge devices/clients to collaboratively and iteratively train a machine learning model under the coordination of a central server. The most common approach to FL is first-order methods, where clients send their local gradients to the server in each iteration. However, these methods often suffer from slow convergence rates. As a remedy, second-order methods, such as quasi-Newton, can be employed in FL to accelerate its convergence. Unfortunately, similarly to the first-order FL methods, the application of second-order methods in FL can lead to unfair models, achieving high average accuracy while performing poorly on certain clients' local datasets. To tackle this issue, in this paper we introduce a novel second-order FL framework, dubbed \textbf{d}istributed \textbf{q}uasi-\textbf{N}ewton \textbf{fed}erated learning (DQN-Fed). This approach seeks to ensure fairness while leveraging the fast convergence properties of quasi-Newton methods in the FL context. Specifically, DQN-Fed helps the server update the global model in such a way that (i) all local loss functions decrease to promote fairness, and (ii) the rate of change in local loss functions aligns with that of the quasi-Newton method. We prove the convergence of DQN-Fed and demonstrate its \textit{linear-quadratic} convergence rate. Moreover, we validate the efficacy of DQN-Fed across a range of federated datasets, showing that it surpasses state-of-the-art fair FL methods in fairness, average accuracy and convergence speed.</li>
</ul>

<h3>Title: Addressing Network Packet-based Cheats in Multiplayer Games: A Secret Sharing Approach</h3>
<ul>
<li><strong>Authors: </strong>Yaqi Cai, Konstantinos Markantonakis, Carlton Shepherd</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10881">https://arxiv.org/abs/2501.10881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10881">https://arxiv.org/pdf/2501.10881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10881]] Addressing Network Packet-based Cheats in Multiplayer Games: A Secret Sharing Approach(https://arxiv.org/abs/2501.10881)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Multiplayer online gaming has witnessed an explosion in popularity over the past two decades. However, security issues continue to give rise to in-game cheating, deterring honest gameplay, detracting from user experience, and ultimately bringing financial harm to game developers. In this paper, we present a new approach for detecting network packet-based cheats, such as forgery and timing cheats, within the context of multiplayer games using an application of secret sharing. Our developed protocols are subjected to formal verification using AVISPA, and we present simulation results using a Python-based implementation. We show that our proposal is practical in addressing some widely used attacks in online gaming.</li>
</ul>

<h3>Title: Automated Selfish Mining Analysis for DAG-based PoW Consensus Protocols</h3>
<ul>
<li><strong>Authors: </strong>Patrik Keller</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10888">https://arxiv.org/abs/2501.10888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10888">https://arxiv.org/pdf/2501.10888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10888]] Automated Selfish Mining Analysis for DAG-based PoW Consensus Protocols(https://arxiv.org/abs/2501.10888)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Selfish mining is strategic rule-breaking to maximize rewards in proof-of-work protocols. Markov Decision Processes (MDPs) are the preferred tool for finding optimal strategies in Bitcoin and similar linear chain protocols. Protocols increasingly adopt DAG-based chain structures, for which MDP analysis is more involved. To date, researchers have tailored specific MDPs for each protocol. Protocol design suffers long feedback loops, as each protocol change implies manual work on the MDP. To overcome this, we propose a generic attack model that covers a wide range of protocols, including Ethereum Proof-of-Work, GhostDAG, and Parallel Proof-of-Work. Our approach is modular: we specify each protocol as a concise program, and our tooling then derives and solves the selfish mining MDP automatically.</li>
</ul>

<h3>Title: Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments</h3>
<ul>
<li><strong>Authors: </strong>Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, Sercan Ö. Arık</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10893">https://arxiv.org/abs/2501.10893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10893">https://arxiv.org/pdf/2501.10893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10893]] Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments(https://arxiv.org/abs/2501.10893)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Autonomous agents powered by large language models (LLMs) have the potential to enhance human capabilities, assisting with digital tasks from sending emails to performing data analysis. The abilities of existing LLMs at such tasks are often hindered by the lack of high-quality agent data from the corresponding environments they interact with. We propose Learn-by-interact, a data-centric framework to adapt LLM agents to any given environments without human annotations. Learn-by-interact synthesizes trajectories of agent-environment interactions based on documentations, and constructs instructions by summarizing or abstracting the interaction histories, a process called backward construction. We assess the quality of our synthetic data by using them in both training-based scenarios and training-free in-context learning (ICL), where we craft innovative retrieval approaches optimized for agents. Extensive experiments on SWE-bench, WebArena, OSWorld and Spider2-V spanning across realistic coding, web, and desktop environments show the effectiveness of Learn-by-interact in various downstream agentic tasks -- baseline results are improved by up to 12.2\% for ICL with Claude-3.5 and 19.5\% for training with Codestral-22B. We further demonstrate the critical role of backward construction, which provides up to 14.0\% improvement for training. Our ablation studies demonstrate the efficiency provided by our synthesized data in ICL and the superiority of our retrieval pipeline over alternative approaches like conventional retrieval-augmented generation (RAG). We expect that Learn-by-interact will serve as a foundation for agent data synthesis as LLMs are increasingly deployed at real-world environments.</li>
</ul>

<h3>Title: A Remote Sensing Image Change Detection Method Integrating Layer Exchange and Channel-Spatial Differences</h3>
<ul>
<li><strong>Authors: </strong>Sijun Dong, Fangcheng Zuo, Geng Chen, Siming Fu, Xiaoliang Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10905">https://arxiv.org/abs/2501.10905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10905">https://arxiv.org/pdf/2501.10905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10905]] A Remote Sensing Image Change Detection Method Integrating Layer Exchange and Channel-Spatial Differences(https://arxiv.org/abs/2501.10905)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Change detection in remote sensing imagery is a critical technique for Earth observation, primarily focusing on pixel-level segmentation of change regions between bi-temporal images. The essence of pixel-level change detection lies in determining whether corresponding pixels in bi-temporal images have changed. In deep learning, the spatial and channel dimensions of feature maps represent different information from the original images. In this study, we found that in change detection tasks, difference information can be computed not only from the spatial dimension of bi-temporal features but also from the channel dimension. Therefore, we designed the Channel-Spatial Difference Weighting (CSDW) module as an aggregation-distribution mechanism for bi-temporal features in change detection. This module enhances the sensitivity of the change detection model to difference features. Additionally, bi-temporal images share the same geographic location and exhibit strong inter-image correlations. To construct the correlation between bi-temporal images, we designed a decoding structure based on the Layer-Exchange (LE) method to enhance the interaction of bi-temporal features. Comprehensive experiments on the CLCD, PX-CLCD, LEVIR-CD, and S2Looking datasets demonstrate that the proposed LENet model significantly improves change detection performance. The code and pre-trained models will be available at: this https URL.</li>
</ul>

<h3>Title: Explainable Adversarial Attacks on Coarse-to-Fine Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Akram Heidarizadeh, Connor Hatfield, Lorenzo Lazzarotto, HanQin Cai, George Atia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10906">https://arxiv.org/abs/2501.10906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10906">https://arxiv.org/pdf/2501.10906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10906]] Explainable Adversarial Attacks on Coarse-to-Fine Classifiers(https://arxiv.org/abs/2501.10906)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Traditional adversarial attacks typically aim to alter the predicted labels of input images by generating perturbations that are imperceptible to the human eye. However, these approaches often lack explainability. Moreover, most existing work on adversarial attacks focuses on single-stage classifiers, but multi-stage classifiers are largely unexplored. In this paper, we introduce instance-based adversarial attacks for multi-stage classifiers, leveraging Layer-wise Relevance Propagation (LRP), which assigns relevance scores to pixels based on their influence on classification outcomes. Our approach generates explainable adversarial perturbations by utilizing LRP to identify and target key features critical for both coarse and fine-grained classifications. Unlike conventional attacks, our method not only induces misclassification but also enhances the interpretability of the model's behavior across classification stages, as demonstrated by experimental results.</li>
</ul>

<h3>Title: Know "No" Better: A Data-Driven Approach for Enhancing Negation Awareness in CLIP</h3>
<ul>
<li><strong>Authors: </strong>Junsung Park, Jungbeom Lee, Jongyoon Song, Sangwon Yu, Dahuin Jung, Sungroh Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10913">https://arxiv.org/abs/2501.10913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10913">https://arxiv.org/pdf/2501.10913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10913]] Know "No" Better: A Data-Driven Approach for Enhancing Negation Awareness in CLIP(https://arxiv.org/abs/2501.10913)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>While CLIP has significantly advanced multimodal understanding by bridging vision and language, the inability to grasp negation - such as failing to differentiate concepts like "parking" from "no parking" - poses substantial challenges. By analyzing the data used in the public CLIP model's pre-training, we posit this limitation stems from a lack of negation-inclusive data. To address this, we introduce data generation pipelines that employ a large language model (LLM) and a multimodal LLM to produce negation-inclusive captions. Fine-tuning CLIP with data generated from our pipelines, we develop NegationCLIP, which enhances negation awareness while preserving the generality. Moreover, to enable a comprehensive evaluation of negation understanding, we propose NegRefCOCOg-a benchmark tailored to test VLMs' ability to interpret negation across diverse expressions and positions within a sentence. Experiments on various CLIP architectures validate the effectiveness of our data generation pipelines in enhancing CLIP's ability to perceive negation accurately. Additionally, NegationCLIP's enhanced negation awareness has practical applications across various multimodal tasks, demonstrated by performance gains in text-to-image generation and referring image segmentation.</li>
</ul>

<h3>Title: LegalGuardian: A Privacy-Preserving Framework for Secure Integration of Large Language Models in Legal Practice</h3>
<ul>
<li><strong>Authors: </strong>M. Mikail Demir, Hakan T. Otal, M. Abdullah Canbaz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10915">https://arxiv.org/abs/2501.10915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10915">https://arxiv.org/pdf/2501.10915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10915]] LegalGuardian: A Privacy-Preserving Framework for Secure Integration of Large Language Models in Legal Practice(https://arxiv.org/abs/2501.10915)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) hold promise for advancing legal practice by automating complex tasks and improving access to justice. However, their adoption is limited by concerns over client confidentiality, especially when lawyers include sensitive Personally Identifiable Information (PII) in prompts, risking unauthorized data exposure. To mitigate this, we introduce LegalGuardian, a lightweight, privacy-preserving framework tailored for lawyers using LLM-based tools. LegalGuardian employs Named Entity Recognition (NER) techniques and local LLMs to mask and unmask confidential PII within prompts, safeguarding sensitive data before any external interaction. We detail its development and assess its effectiveness using a synthetic prompt library in immigration law scenarios. Comparing traditional NER models with one-shot prompted local LLM, we find that LegalGuardian achieves a F1-score of 93% with GLiNER and 97% with Qwen2.5-14B in PII detection. Semantic similarity analysis confirms that the framework maintains high fidelity in outputs, ensuring robust utility of LLM-based tools. Our findings indicate that legal professionals can harness advanced AI technologies without compromising client confidentiality or the quality of legal documents.</li>
</ul>

<h3>Title: Decomposing and Fusing Intra- and Inter-Sensor Spatio-Temporal Signal for Multi-Sensor Wearable Human Activity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Xie, Haoxuan Li, Chunyuan Zheng, Haonan Yuan, Guorui Liao, Jun Liao, Li Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10917">https://arxiv.org/abs/2501.10917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10917">https://arxiv.org/pdf/2501.10917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10917]] Decomposing and Fusing Intra- and Inter-Sensor Spatio-Temporal Signal for Multi-Sensor Wearable Human Activity Recognition(https://arxiv.org/abs/2501.10917)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Wearable Human Activity Recognition (WHAR) is a prominent research area within ubiquitous computing. Multi-sensor synchronous measurement has proven to be more effective for WHAR than using a single sensor. However, existing WHAR methods use shared convolutional kernels for indiscriminate temporal feature extraction across each sensor variable, which fails to effectively capture spatio-temporal relationships of intra-sensor and inter-sensor variables. We propose the DecomposeWHAR model consisting of a decomposition phase and a fusion phase to better model the relationships between modality variables. The decomposition creates high-dimensional representations of each intra-sensor variable through the improved Depth Separable Convolution to capture local temporal features while preserving their unique characteristics. The fusion phase begins by capturing relationships between intra-sensor variables and fusing their features at both the channel and variable levels. Long-range temporal dependencies are modeled using the State Space Model (SSM), and later cross-sensor interactions are dynamically captured through a self-attention mechanism, highlighting inter-sensor spatial correlations. Our model demonstrates superior performance on three widely used WHAR datasets, significantly outperforming state-of-the-art models while maintaining acceptable computational efficiency. Our codes and supplementary materials are available at this https URL.</li>
</ul>

<h3>Title: Data Enrichment Opportunities for Distribution Grid Cable Networks using Variational Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Konrad Sundsgaard, Kutay Bölat, Guangya Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10920">https://arxiv.org/abs/2501.10920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10920">https://arxiv.org/pdf/2501.10920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10920]] Data Enrichment Opportunities for Distribution Grid Cable Networks using Variational Autoencoders(https://arxiv.org/abs/2501.10920)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Electricity distribution cable networks suffer from incomplete and unbalanced data, hindering the effectiveness of machine learning models for predictive maintenance and reliability evaluation. Features such as the installation date of the cables are frequently missing. To address data scarcity, this study investigates the application of Variational Autoencoders (VAEs) for data enrichment, synthetic data generation, imbalanced data handling, and outlier detection. Based on a proof-of-concept case study for Denmark, targeting the imputation of missing age information in cable network asset registers, the analysis underlines the potential of generative models to support data-driven maintenance. However, the study also highlights several areas for improvement, including enhanced feature importance analysis, incorporating network characteristics and external features, and handling biases in missing data. Future initiatives should expand the application of VAEs by incorporating semi-supervised learning, advanced sampling techniques, and additional distribution grid elements, including low-voltage networks, into the analysis.</li>
</ul>

<h3>Title: Generative Physical AI in Vision: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Daochang Liu, Junyu Zhang, Anh-Dung Dinh, Eunbyung Park, Shichao Zhang, Chang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10928">https://arxiv.org/abs/2501.10928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10928">https://arxiv.org/pdf/2501.10928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10928]] Generative Physical AI in Vision: A Survey(https://arxiv.org/abs/2501.10928)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Artificial Intelligence (AI) has rapidly advanced the field of computer vision by enabling machines to create and interpret visual data with unprecedented sophistication. This transformation builds upon a foundation of generative models to produce realistic images, videos, and 3D or 4D content. Traditionally, generative models primarily focus on visual fidelity while often neglecting the physical plausibility of generated content. This gap limits their effectiveness in applications requiring adherence to real-world physical laws, such as robotics, autonomous systems, and scientific simulations. As generative AI evolves to increasingly integrate physical realism and dynamic simulation, its potential to function as a "world simulator" expands-enabling the modeling of interactions governed by physics and bridging the divide between virtual and physical realities. This survey systematically reviews this emerging field of physics-aware generative AI in computer vision, categorizing methods based on how they incorporate physical knowledge-either through explicit simulation or implicit learning. We analyze key paradigms, discuss evaluation protocols, and identify future research directions. By offering a comprehensive overview, this survey aims to help future developments in physically grounded generation for vision. The reviewed papers are summarized at this https URL.</li>
</ul>

<h3>Title: TSVC:Tripartite Learning with Semantic Variation Consistency for Robust Image-Text Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Shuai Lyu, Zijing Tian, Zhonghong Ou, Yifan Zhu, Xiao Zhang, Qiankun Ha, Haoran Luo, Meina Song</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10935">https://arxiv.org/abs/2501.10935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10935">https://arxiv.org/pdf/2501.10935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10935]] TSVC:Tripartite Learning with Semantic Variation Consistency for Robust Image-Text Retrieval(https://arxiv.org/abs/2501.10935)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Cross-modal retrieval maps data under different modality via semantic relevance. Existing approaches implicitly assume that data pairs are well-aligned and ignore the widely existing annotation noise, i.e., noisy correspondence (NC). Consequently, it inevitably causes performance degradation. Despite attempts that employ the co-teaching paradigm with identical architectures to provide distinct data perspectives, the differences between these architectures are primarily stemmed from random initialization. Thus, the model becomes increasingly homogeneous along with the training process. Consequently, the additional information brought by this paradigm is severely limited. In order to resolve this problem, we introduce a Tripartite learning with Semantic Variation Consistency (TSVC) for robust image-text retrieval. We design a tripartite cooperative learning mechanism comprising a Coordinator, a Master, and an Assistant model. The Coordinator distributes data, and the Assistant model supports the Master model's noisy label prediction with diverse data. Moreover, we introduce a soft label estimation method based on mutual information variation, which quantifies the noise in new samples and assigns corresponding soft labels. We also present a new loss function to enhance robustness and optimize training effectiveness. Extensive experiments on three widely used datasets demonstrate that, even at increasing noise ratios, TSVC exhibits significant advantages in retrieval accuracy and maintains stable training performance.</li>
</ul>

<h3>Title: Leveraging Chain of Thought towards Empathetic Spoken Dialogue without Corresponding Question-Answering Data</h3>
<ul>
<li><strong>Authors: </strong>Jingran Xie, Shun Lei, Yue Yu, Yang Xiang, Hui Wang, Xixin Wu, Zhiyong Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10937">https://arxiv.org/abs/2501.10937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10937">https://arxiv.org/pdf/2501.10937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10937]] Leveraging Chain of Thought towards Empathetic Spoken Dialogue without Corresponding Question-Answering Data(https://arxiv.org/abs/2501.10937)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Empathetic dialogue is crucial for natural human-computer interaction, allowing the dialogue system to respond in a more personalized and emotionally aware manner, improving user satisfaction and engagement. The emergence of large language models (LLMs) has revolutionized dialogue generation by harnessing their powerful capabilities and shown its potential in multimodal domains. Many studies have integrated speech with text-based LLMs to take speech question as input and output text response. However, the lack of spoken question-answering datasets that include speech style information to supervised fine-tuning (SFT) limits the performance of these systems. As a result, while these systems excel at understanding speech content, they often struggle to generate empathetic responses. In response, we propose a novel approach that circumvents the need for question-answering data, called Listen, Perceive, and Express (LPE). Our method employs a two-stage training process, initially guiding the LLM to listen the content and perceive the emotional aspects of speech. Subsequently, we utilize Chain-of-Thought (CoT) prompting to unlock the model's potential for expressing empathetic responses based on listened spoken content and perceived emotional cues. We employ experiments to prove the effectiveness of proposed method. To our knowledge, this is the first attempt to leverage CoT for speech-based dialogue.</li>
</ul>

<h3>Title: Blockchain-assisted Demonstration Cloning for Multi-Agent Deep Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Alagha, Jamal Bentahar, Hadi Otrok, Shakti Singh, Rabeb Mizouni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10938">https://arxiv.org/abs/2501.10938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10938">https://arxiv.org/pdf/2501.10938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10938]] Blockchain-assisted Demonstration Cloning for Multi-Agent Deep Reinforcement Learning(https://arxiv.org/abs/2501.10938)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Multi-Agent Deep Reinforcement Learning (MDRL) is a promising research area in which agents learn complex behaviors in cooperative or competitive environments. However, MDRL comes with several challenges that hinder its usability, including sample efficiency, curse of dimensionality, and environment exploration. Recent works proposing Federated Reinforcement Learning (FRL) to tackle these issues suffer from problems related to model restrictions and maliciousness. Other proposals using reward shaping require considerable engineering and could lead to local optima. In this paper, we propose a novel Blockchain-assisted Multi-Expert Demonstration Cloning (MEDC) framework for MDRL. The proposed method utilizes expert demonstrations in guiding the learning of new MDRL agents, by suggesting exploration actions in the environment. A model sharing framework on Blockchain is designed to allow users to share their trained models, which can be allocated as expert models to requesting users to aid in training MDRL systems. A Consortium Blockchain is adopted to enable traceable and autonomous execution without the need for a single trusted entity. Smart Contracts are designed to manage users and models allocation, which are shared using IPFS. The proposed framework is tested on several applications, and is benchmarked against existing methods in FRL, Reward Shaping, and Imitation Learning-assisted RL. The results show the outperformance of the proposed framework in terms of learning speed and resiliency to faulty and malicious models.</li>
</ul>

<h3>Title: InsQABench: Benchmarking Chinese Insurance Domain Question Answering with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jing Ding, Kai Feng, Binbin Lin, Jiarui Cai, Qiushi Wang, Yu Xie, Xiaojin Zhang, Zhongyu Wei, Wei Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10943">https://arxiv.org/abs/2501.10943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10943">https://arxiv.org/pdf/2501.10943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10943]] InsQABench: Benchmarking Chinese Insurance Domain Question Answering with Large Language Models(https://arxiv.org/abs/2501.10943)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The application of large language models (LLMs) has achieved remarkable success in various fields, but their effectiveness in specialized domains like the Chinese insurance industry remains underexplored. The complexity of insurance knowledge, encompassing specialized terminology and diverse data types, poses significant challenges for both models and users. To address this, we introduce InsQABench, a benchmark dataset for the Chinese insurance sector, structured into three categories: Insurance Commonsense Knowledge, Insurance Structured Database, and Insurance Unstructured Documents, reflecting real-world insurance question-answering this http URL also propose two methods, SQL-ReAct and RAG-ReAct, to tackle challenges in structured and unstructured data tasks. Evaluations show that while LLMs struggle with domain-specific terminology and nuanced clause texts, fine-tuning on InsQABench significantly improves performance. Our benchmark establishes a solid foundation for advancing LLM applications in the insurance domain, with data and code available at this https URL.</li>
</ul>

<h3>Title: Gradient-Based Multi-Objective Deep Learning: Algorithms, Theories, Applications, and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Weiyu Chen, Xiaoyuan Zhang, Baijiong Lin, Xi Lin, Han Zhao, Qingfu Zhang, James T. Kwok</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10945">https://arxiv.org/abs/2501.10945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10945">https://arxiv.org/pdf/2501.10945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10945]] Gradient-Based Multi-Objective Deep Learning: Algorithms, Theories, Applications, and Beyond(https://arxiv.org/abs/2501.10945)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-objective optimization (MOO) in deep learning aims to simultaneously optimize multiple conflicting objectives, a challenge frequently encountered in areas like multi-task learning and multi-criteria learning. Recent advancements in gradient-based MOO methods have enabled the discovery of diverse types of solutions, ranging from a single balanced solution to finite or even infinite Pareto sets, tailored to user needs. These developments have broad applications across domains such as reinforcement learning, computer vision, recommendation systems, and large language models. This survey provides the first comprehensive review of gradient-based MOO in deep learning, covering algorithms, theories, and practical applications. By unifying various approaches and identifying critical challenges, it serves as a foundational resource for driving innovation in this evolving field. A comprehensive list of MOO algorithms in deep learning is available at \url{this https URL}.</li>
</ul>

<h3>Title: MARIO: A Mixed Annotation Framework For Polyp Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Haoyang Li, Yiwen Hu, Jun Wei, Zhen Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10957">https://arxiv.org/abs/2501.10957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10957">https://arxiv.org/pdf/2501.10957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10957]] MARIO: A Mixed Annotation Framework For Polyp Segmentation(https://arxiv.org/abs/2501.10957)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Existing polyp segmentation models are limited by high labeling costs and the small size of datasets. Additionally, vast polyp datasets remain underutilized because these models typically rely on a single type of annotation. To address this dilemma, we introduce MARIO, a mixed supervision model designed to accommodate various annotation types, significantly expanding the range of usable data. MARIO learns from underutilized datasets by incorporating five forms of supervision: pixel-level, box-level, polygon-level, scribblelevel, and point-level. Each form of supervision is associated with a tailored loss that effectively leverages the supervision labels while minimizing the noise. This allows MARIO to move beyond the constraints of relying on a single annotation type. Furthermore, MARIO primarily utilizes dataset with weak and cheap annotations, reducing the dependence on large-scale, fully annotated ones. Experimental results across five benchmark datasets demonstrate that MARIO consistently outperforms existing methods, highlighting its efficacy in balancing trade-offs between different forms of supervision and maximizing polyp segmentation performance</li>
</ul>

<h3>Title: Rethinking Early-Fusion Strategies for Improved Multimodal Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zhengwen Shen, Yulian Li, Han Zhang, Yuchen Weng, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10958">https://arxiv.org/abs/2501.10958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10958">https://arxiv.org/pdf/2501.10958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10958]] Rethinking Early-Fusion Strategies for Improved Multimodal Image Segmentation(https://arxiv.org/abs/2501.10958)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>RGB and thermal image fusion have great potential to exhibit improved semantic segmentation in low-illumination conditions. Existing methods typically employ a two-branch encoder framework for multimodal feature extraction and design complicated feature fusion strategies to achieve feature extraction and fusion for multimodal semantic segmentation. However, these methods require massive parameter updates and computational effort during the feature extraction and fusion. To address this issue, we propose a novel multimodal fusion network (EFNet) based on an early fusion strategy and a simple but effective feature clustering for training efficient RGB-T semantic segmentation. In addition, we also propose a lightweight and efficient multi-scale feature aggregation decoder based on Euclidean distance. We validate the effectiveness of our method on different datasets and outperform previous state-of-the-art methods with lower parameters and computation.</li>
</ul>

<h3>Title: The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Nitay Calderon, Roi Reichart, Rotem Dror</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10970">https://arxiv.org/abs/2501.10970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10970">https://arxiv.org/pdf/2501.10970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10970]] The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs(https://arxiv.org/abs/2501.10970)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The "LLM-as-a-judge" paradigm employs Large Language Models (LLMs) as annotators and evaluators in tasks traditionally performed by humans. LLM annotations are widely used, not only in NLP research but also in fields like medicine, psychology, and social science. Despite their role in shaping study results and insights, there is no standard or rigorous procedure to determine whether LLMs can replace human annotators. In this paper, we propose a novel statistical procedure -- the Alternative Annotator Test (alt-test) -- that requires only a modest subset of annotated examples to justify using LLM annotations. Additionally, we introduce a versatile and interpretable measure for comparing LLM judges. To demonstrate our procedure, we curated a diverse collection of ten datasets, consisting of language and vision-language tasks, and conducted experiments with six LLMs and four prompting techniques. Our results show that LLMs can sometimes replace humans with closed-source LLMs (such as GPT-4o), outperforming open-source LLMs, and that prompting techniques yield judges of varying quality. We hope this study encourages more rigorous and reliable practices.</li>
</ul>

<h3>Title: Control LLM: Controlled Evolution for Intelligence Retention in LLM</h3>
<ul>
<li><strong>Authors: </strong>Haichao Wei, Yunxiang Ren, Zhoutong Fu, Aman Lunia, Yi-Lin Chen, Alice Leung, Ya Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10979">https://arxiv.org/abs/2501.10979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10979">https://arxiv.org/pdf/2501.10979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10979]] Control LLM: Controlled Evolution for Intelligence Retention in LLM(https://arxiv.org/abs/2501.10979)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demand significant computational resources, making it essential to enhance their capabilities without retraining from scratch. A key challenge in this domain is \textit{catastrophic forgetting} (CF), which hampers performance during Continuous Pre-training (CPT) and Continuous Supervised Fine-Tuning (CSFT). We propose \textbf{Control LLM}, a novel approach that leverages parallel pre-trained and expanded transformer blocks, aligning their hidden-states through interpolation strategies This method effectively preserves performance on existing tasks while seamlessly integrating new knowledge. Extensive experiments demonstrate the effectiveness of Control LLM in both CPT and CSFT. On Llama3.1-8B-Instruct, it achieves significant improvements in mathematical reasoning ($+14.4\%$ on Math-Hard) and coding performance ($+10\%$ on MBPP-PLUS). On Llama3.1-8B, it enhances multilingual capabilities ($+10.6\%$ on C-Eval, $+6.8\%$ on CMMLU, and $+30.2\%$ on CMMLU-0shot-CoT). It surpasses existing methods and achieves SOTA among open-source models tuned from the same base model, using substantially less data and compute. Crucially, these gains are realized while preserving strong original capabilities, with minimal degradation ($<4.3\% \text{on MMLU}$) compared to $>35\%$ in open-source Math and Coding models. This approach has been successfully deployed in LinkedIn's GenAI-powered job seeker and Ads unit products. To support further research, we release the training and evaluation code (\url{this https URL}) along with models trained on public datasets (\url{ this https URL}) to the community.</li>
</ul>

<h3>Title: CIBPU: A Conflict-Invisible Secure Branch Prediction Unit</h3>
<ul>
<li><strong>Authors: </strong>Zhe Zhou, Fei Tong, Hongyu Wang, Xiaoyu Cheng, Fang Jiang, Zhikun Zhang, Yuxing Mao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10983">https://arxiv.org/abs/2501.10983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10983">https://arxiv.org/pdf/2501.10983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10983]] CIBPU: A Conflict-Invisible Secure Branch Prediction Unit(https://arxiv.org/abs/2501.10983)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Previous schemes for designing secure branch prediction unit (SBPU) based on physical isolation can only offer limited security and significantly affect BPU's prediction capability, leading to prominent performance degradation. Moreover, encryption-based SBPU schemes based on periodic key re-randomization have the risk of being compromised by advanced attack algorithms, and the performance overhead is also considerable. To this end, this paper proposes a conflict-invisible SBPU (CIBPU). CIBPU employs redundant storage design, load-aware indexing, and replacement design, as well as an encryption mechanism without requiring periodic key updates, to prevent attackers' perception of branch conflicts. We provide a thorough security analysis, which shows that CIBPU achieves strong security throughout the BPU's lifecycle. We implement CIBPU in a RISC-V core model in gem5. The experimental results show that CIBPU causes an average performance overhead of only 1.12%-2.20% with acceptable hardware storage overhead, which is the lowest among the state-of-the-art SBPU schemes. CIBPU has also been implemented in the open-source RISC-V core, SonicBOOM, which is then burned onto an FPGA board. The evaluation based on the board shows an average performance degradation of 2.01%, which is approximately consistent with the result obtained in gem5.</li>
</ul>

<h3>Title: GRID: Protecting Training Graph from Link Stealing Attacks on GNN Models</h3>
<ul>
<li><strong>Authors: </strong>Jiadong Lou, Xu Yuan, Rui Zhang, Xingliang Yuan, Neil Gong, Nian-Feng Tzeng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10985">https://arxiv.org/abs/2501.10985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10985">https://arxiv.org/pdf/2501.10985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10985]] GRID: Protecting Training Graph from Link Stealing Attacks on GNN Models(https://arxiv.org/abs/2501.10985)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, attack, steal</a></li>
<li><strong>Abstract: </strong>Graph neural networks (GNNs) have exhibited superior performance in various classification tasks on graph-structured data. However, they encounter the potential vulnerability from the link stealing attacks, which can infer the presence of a link between two nodes via measuring the similarity of its incident nodes' prediction vectors produced by a GNN model. Such attacks pose severe security and privacy threats to the training graph used in GNN models. In this work, we propose a novel solution, called Graph Link Disguise (GRID), to defend against link stealing attacks with the formal guarantee of GNN model utility for retaining prediction accuracy. The key idea of GRID is to add carefully crafted noises to the nodes' prediction vectors for disguising adjacent nodes as n-hop indirect neighboring nodes. We take into account the graph topology and select only a subset of nodes (called core nodes) covering all links for adding noises, which can avert the noises offset and have the further advantages of reducing both the distortion loss and the computation cost. Our crafted noises can ensure 1) the noisy prediction vectors of any two adjacent nodes have their similarity level like that of two non-adjacent nodes and 2) the model prediction is unchanged to ensure zero utility loss. Extensive experiments on five datasets are conducted to show the effectiveness of our proposed GRID solution against different representative link-stealing attacks under transductive settings and inductive settings respectively, as well as two influence-based attacks. Meanwhile, it achieves a much better privacy-utility trade-off than existing methods when extended to GNNs.</li>
</ul>

<h3>Title: Effectiveness of Adversarial Benign and Malware Examples in Evasion and Poisoning Attacks</h3>
<ul>
<li><strong>Authors: </strong>Matouš Kozák, Martin Jureček</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.10996">https://arxiv.org/abs/2501.10996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.10996">https://arxiv.org/pdf/2501.10996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.10996]] Effectiveness of Adversarial Benign and Malware Examples in Evasion and Poisoning Attacks(https://arxiv.org/abs/2501.10996)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>Adversarial attacks present significant challenges for malware detection systems. This research investigates the effectiveness of benign and malicious adversarial examples (AEs) in evasion and poisoning attacks on the Portable Executable file domain. A novel focus of this study is on benign AEs, which, although not directly harmful, can increase false positives and undermine trust in antivirus solutions. We propose modifying existing adversarial malware generators to produce benign AEs and show they are as successful as malware AEs in evasion attacks. Furthermore, our data show that benign AEs have a more decisive influence in poisoning attacks than standard malware AEs, demonstrating their superior ability to decrease the model's performance. Our findings introduce new opportunities for adversaries and further increase the attack surface that needs to be protected by security researchers.</li>
</ul>

<h3>Title: pMixFed: Efficient Personalized Federated Learning through Adaptive Layer-Wise Mixup</h3>
<ul>
<li><strong>Authors: </strong>Yasaman Saadati, Mohammad Rostami, M. Hadi Amini</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11002">https://arxiv.org/abs/2501.11002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11002">https://arxiv.org/pdf/2501.11002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11002]] pMixFed: Efficient Personalized Federated Learning through Adaptive Layer-Wise Mixup(https://arxiv.org/abs/2501.11002)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Traditional Federated Learning (FL) methods encounter significant challenges when dealing with heterogeneous data and providing personalized solutions for non-IID scenarios. Personalized Federated Learning (PFL) approaches aim to address these issues by balancing generalization and personalization, often through parameter decoupling or partial models that freeze some neural network layers for personalization while aggregating other layers globally. However, existing methods still face challenges of global-local model discrepancy, client drift, and catastrophic forgetting, which degrade model accuracy. To overcome these limitations, we propose pMixFed, a dynamic, layer-wise PFL approach that integrates mixup between shared global and personalized local models. Our method introduces an adaptive strategy for partitioning between personalized and shared layers, a gradual transition of personalization degree to enhance local client adaptation, improved generalization across clients, and a novel aggregation mechanism to mitigate catastrophic forgetting. Extensive experiments demonstrate that pMixFed outperforms state-of-the-art PFL methods, showing faster model training, increased robustness, and improved handling of data heterogeneity under different heterogeneous settings.</li>
</ul>

<h3>Title: LF-Steering: Latent Feature Activation Steering for Enhancing Semantic Consistency in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jingyuan Yang, Rongjun Li, Weixuan Wang, Ziyu Zhou, Zhiyong Feng, Wei Peng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11036">https://arxiv.org/abs/2501.11036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11036">https://arxiv.org/pdf/2501.11036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11036]] LF-Steering: Latent Feature Activation Steering for Enhancing Semantic Consistency in Large Language Models(https://arxiv.org/abs/2501.11036)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often generate inconsistent responses when prompted with semantically equivalent paraphrased inputs. Recently, activation steering, a technique that modulates LLM behavior by adjusting their latent representations during inference time, has been explored to improve the semantic consistency of LLMs. However, these methods typically operate at the model component level, such as layer hidden states or attention heads. They face a challenge due to the ``polysemanticity issue'', where the model components of LLMs typically encode multiple entangled features, making precise steering difficult. To address this challenge, we drill down to feature-level representations and propose LF-Steering, a novel activation steering approach to precisely identify latent feature representations responsible for semantic inconsistency. More specifically, our method maps the hidden states of relevant transformer layer into a sparsely activated, high-dimensional feature space based on a sparse autoencoder (SAE), ensuring model steering based on decoupled feature representations with minimal interference. Comprehensive experiments on both NLU and NLG datasets demonstrate the effectiveness of our method in enhancing semantic consistency, resulting in significant performance gains for various NLU and NLG tasks.</li>
</ul>

<h3>Title: Beyond Any-Shot Adaptation: Predicting Optimization Outcome for Robustness Gains without Extra Pay</h3>
<ul>
<li><strong>Authors: </strong>Qi Cheems Wang, Zehao Xiao, Yixiu Mao, Yun Qu, Jiayi Shen, Yiqin Lv, Xiangyang Ji</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11039">https://arxiv.org/abs/2501.11039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11039">https://arxiv.org/pdf/2501.11039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11039]] Beyond Any-Shot Adaptation: Predicting Optimization Outcome for Robustness Gains without Extra Pay(https://arxiv.org/abs/2501.11039)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust, generative</a></li>
<li><strong>Abstract: </strong>The foundation model enables fast problem-solving without learning from scratch, and such a desirable adaptation property benefits from its adopted cross-task generalization paradigms, e.g., pretraining, meta-training, or finetuning. Recent trends have focused on the curation of task datasets during optimization, which includes task selection as an indispensable consideration for either adaptation robustness or sampling efficiency purposes. Despite some progress, selecting crucial task batches to optimize over iteration mostly exhausts massive task queries and requires intensive evaluation and computations to secure robust adaptation. This work underscores the criticality of both robustness and learning efficiency, especially in scenarios where tasks are risky to collect or costly to evaluate. To this end, we present Model Predictive Task Sampling (MPTS), a novel active task sampling framework to establish connections between the task space and adaptation risk landscape achieve robust adaptation. Technically, MPTS characterizes the task episodic information with a generative model and predicts optimization outcome after adaptation from posterior inference, i.e., forecasting task-specific adaptation risk values. The resulting risk learner amortizes expensive annotation, evaluation, or computation operations in task robust adaptation learning paradigms. Extensive experimental results show that MPTS can be seamlessly integrated into zero-shot, few-shot, and many-shot learning paradigms, increases adaptation robustness, and retains learning efficiency without affording extra cost. The code will be available at the project site this https URL.</li>
</ul>

<h3>Title: Enhancing Semantic Consistency of Large Language Models through Model Editing: An Interpretability-Oriented Approach</h3>
<ul>
<li><strong>Authors: </strong>Jingyuan Yang, Dapeng Chen, Yajing Sun, Rongjun Li, Zhiyong Feng, Wei Peng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11041">https://arxiv.org/abs/2501.11041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11041">https://arxiv.org/pdf/2501.11041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11041]] Enhancing Semantic Consistency of Large Language Models through Model Editing: An Interpretability-Oriented Approach(https://arxiv.org/abs/2501.11041)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>A Large Language Model (LLM) tends to generate inconsistent and sometimes contradictory outputs when presented with a prompt that has equivalent semantics but is expressed differently from the original prompt. To achieve semantic consistency of an LLM, one of the key approaches is to finetune the model with prompt-output pairs with semantically equivalent meanings. Despite its effectiveness, a data-driven finetuning method incurs substantial computation costs in data preparation and model optimization. In this regime, an LLM is treated as a ``black box'', restricting our ability to gain deeper insights into its internal mechanism. In this paper, we are motivated to enhance the semantic consistency of LLMs through a more interpretable method (i.e., model editing) to this end. We first identify the model components (i.e., attention heads) that have a key impact on the semantic consistency of an LLM. We subsequently inject biases into the output of these model components along the semantic-consistency activation direction. It is noteworthy that these modifications are cost-effective, without reliance on mass manipulations of the original model parameters. Through comprehensive experiments on the constructed NLU and open-source NLG datasets, our method demonstrates significant improvements in the semantic consistency and task performance of LLMs. Additionally, our method exhibits promising generalization capabilities by performing well on tasks beyond the primary tasks.</li>
</ul>

<h3>Title: Bridging the Security Gap: Lessons from 5G and What 6G Should Do Better</h3>
<ul>
<li><strong>Authors: </strong>Isabella D. Lutz, Matthew C. Valenti</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11045">https://arxiv.org/abs/2501.11045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11045">https://arxiv.org/pdf/2501.11045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11045]] Bridging the Security Gap: Lessons from 5G and What 6G Should Do Better(https://arxiv.org/abs/2501.11045)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, robust</a></li>
<li><strong>Abstract: </strong>The security requirements for future 6G mobile networks are anticipated to be significantly more complex and demanding than those of 5G. This increase stems from several factors: the proliferation of massive machine-type communications will dramatically increase the density of devices competing for network access; secure ultra-reliable low-latency communication will impose stringent requirements on security, latency, and reliability; and the widespread deployment of small cells and non-terrestrial networks, including satellite mega-constellations, will result in more frequent handovers. This paper provides a set of security recommendations for 6G networks, with a particular focus on access and handover procedures, which often lack encryption and integrity protection, making them more vulnerable to exploitation. Since 6G is expected to be a backward-compatible extension of 5G, and given that secure systems cannot be effectively designed without a clear understanding of their goals, it is imperative to first evaluate the limitations of the current generation. To this end, the paper begins by reviewing existing 5G access and authentication mechanisms, highlighting several critical vulnerabilities in these procedures. It then examines potential 6G challenges and concludes with actionable recommendations to enhance the security, resilience, and robustness of 6G access and handover mechanisms.</li>
</ul>

<h3>Title: SLVC-DIDA: Signature-less Verifiable Credential-based Issuer-hiding and Multi-party Authentication for Decentralized Identity</h3>
<ul>
<li><strong>Authors: </strong>Tianxiu Xie, Keke Gai, Jing Yu, Chennan Guo, Liehuang Zhu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11052">https://arxiv.org/abs/2501.11052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11052">https://arxiv.org/pdf/2501.11052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11052]] SLVC-DIDA: Signature-less Verifiable Credential-based Issuer-hiding and Multi-party Authentication for Decentralized Identity(https://arxiv.org/abs/2501.11052)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack</a></li>
<li><strong>Abstract: </strong>As an emerging paradigm in digital identity, Decentralized Identity (DID) appears advantages over traditional identity management methods in a variety of aspects, e.g., enhancing user-centric online services and ensuring complete user autonomy and control. Verifiable Credential (VC) techniques are used to facilitate decentralized DID-based access control across multiple entities. However, existing DID schemes generally rely on a distributed public key infrastructure that also causes challenges, such as context information deduction, key exposure, and issuer data leakage. To address the issues above, this paper proposes a Permanent Issuer-Hiding (PIH)-based DID multi-party authentication framework with a signature-less VC model, named SLVC-DIDA, for the first time. Our proposed scheme avoids the dependence on signing keys by employing hashing and issuer membership proofs, which supports universal zero-knowledge multi-party DID authentications, eliminating additional technical integrations. We adopt a zero-knowledge RSA accumulator to maintain the anonymity of the issuer set, thereby enabling public verification while safeguarding the privacy of identity attributes via a Merkle tree-based VC list. By eliminating reliance on a Public Key Infrastructure (PKI), SLVC-DIDA enables fully decentralized issuance and verification of DIDs. Furthermore, our scheme ensures PIH through the implementation of the zero-knowledge Issuer set and VC list, so that the risks of key leakage and contextual inference attacks are effectively mitigated. Our experiments further evaluate the effectiveness and practicality of SLVC-DIDA.</li>
</ul>

<h3>Title: Learning with Open-world Noisy Data via Class-independent Margin in Dual Representation Space</h3>
<ul>
<li><strong>Authors: </strong>Linchao Pan, Can Gao, Jie Zhou, Jinbao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11053">https://arxiv.org/abs/2501.11053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11053">https://arxiv.org/pdf/2501.11053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11053]] Learning with Open-world Noisy Data via Class-independent Margin in Dual Representation Space(https://arxiv.org/abs/2501.11053)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Learning with Noisy Labels (LNL) aims to improve the model generalization when facing data with noisy labels, and existing methods generally assume that noisy labels come from known classes, called closed-set noise. However, in real-world scenarios, noisy labels from similar unknown classes, i.e., open-set noise, may occur during the training and inference stage. Such open-world noisy labels may significantly impact the performance of LNL methods. In this study, we propose a novel dual-space joint learning method to robustly handle the open-world noise. To mitigate model overfitting on closed-set and open-set noises, a dual representation space is constructed by two networks. One is a projection network that learns shared representations in the prototype space, while the other is a One-Vs-All (OVA) network that makes predictions using unique semantic representations in the class-independent space. Then, bi-level contrastive learning and consistency regularization are introduced in two spaces to enhance the detection capability for data with unknown classes. To benefit from the memorization effects across different types of samples, class-independent margin criteria are designed for sample identification, which selects clean samples, weights closed-set noise, and filters open-set noise effectively. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods and achieves an average accuracy improvement of 4.55\% and an AUROC improvement of 6.17\% on CIFAR80N.</li>
</ul>

<h3>Title: Temporal Analysis of Adversarial Attacks in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Rohit Mapakshi, Sayma Akther, Mark Stamp</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11054">https://arxiv.org/abs/2501.11054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11054">https://arxiv.org/pdf/2501.11054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11054]] Temporal Analysis of Adversarial Attacks in Federated Learning(https://arxiv.org/abs/2501.11054)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>In this paper, we experimentally analyze the robustness of selected Federated Learning (FL) systems in the presence of adversarial clients. We find that temporal attacks significantly affect model performance in the FL models tested, especially when the adversaries are active throughout or during the later rounds. We consider a variety of classic learning models, including Multinominal Logistic Regression (MLR), Random Forest, XGBoost, Support Vector Classifier (SVC), as well as various Neural Network models including Multilayer Perceptron (MLP), Convolution Neural Network (CNN), Recurrent Neural Network (RNN), and Long Short-Term Memory (LSTM). Our results highlight the effectiveness of temporal attacks and the need to develop strategies to make the FL process more robust against such attacks. We also briefly consider the effectiveness of defense mechanisms, including outlier detection in the aggregation algorithm.</li>
</ul>

<h3>Title: Enhancing Sample Utilization in Noise-Robust Deep Metric Learning With Subgroup-Based Positive-Pair Selection</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng Yu, Qianqian Xu, Yangbangyan Jiang, Yingfei Sun, Qingming Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11063">https://arxiv.org/abs/2501.11063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11063">https://arxiv.org/pdf/2501.11063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11063]] Enhancing Sample Utilization in Noise-Robust Deep Metric Learning With Subgroup-Based Positive-Pair Selection(https://arxiv.org/abs/2501.11063)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The existence of noisy labels in real-world data negatively impacts the performance of deep learning models. Although much research effort has been devoted to improving the robustness towards noisy labels in classification tasks, the problem of noisy labels in deep metric learning (DML) remains under-explored. Existing noisy label learning methods designed for DML mainly discard suspicious noisy samples, resulting in a waste of the training data. To address this issue, we propose a noise-robust DML framework with SubGroup-based Positive-pair Selection (SGPS), which constructs reliable positive pairs for noisy samples to enhance the sample utilization. Specifically, SGPS first effectively identifies clean and noisy samples by a probability-based clean sample selectionstrategy. To further utilize the remaining noisy samples, we discover their potential similar samples based on the subgroup information given by a subgroup generation module and then aggregate them into informative positive prototypes for each noisy sample via a positive prototype generation module. Afterward, a new contrastive loss is tailored for the noisy samples with their selected positive pairs. SGPS can be easily integrated into the training process of existing pair-wise DML tasks, like image retrieval and face recognition. Extensive experiments on multiple synthetic and real-world large-scale label noise datasets demonstrate the effectiveness of our proposed method. Without any bells and whistles, our SGPS framework outperforms the state-of-the-art noisy label DML methods. Code is available at \url{this https URL}.</li>
</ul>

<h3>Title: IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems</h3>
<ul>
<li><strong>Authors: </strong>Elad Levi, Ilan Kadar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11067">https://arxiv.org/abs/2501.11067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11067">https://arxiv.org/pdf/2501.11067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11067]] IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems(https://arxiv.org/abs/2501.11067)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are transforming artificial intelligence, evolving into task-oriented systems capable of autonomous planning and execution. One of the primary applications of LLMs is conversational AI systems, which must navigate multi-turn dialogues, integrate domain-specific APIs, and adhere to strict policy constraints. However, evaluating these agents remains a significant challenge, as traditional methods fail to capture the complexity and variability of real-world interactions. We introduce IntellAgent, a scalable, open-source multi-agent framework designed to evaluate conversational AI systems comprehensively. IntellAgent automates the creation of diverse, synthetic benchmarks by combining policy-driven graph modeling, realistic event generation, and interactive user-agent simulations. This innovative approach provides fine-grained diagnostics, addressing the limitations of static and manually curated benchmarks with coarse-grained metrics. IntellAgent represents a paradigm shift in evaluating conversational AI. By simulating realistic, multi-policy scenarios across varying levels of complexity, IntellAgent captures the nuanced interplay of agent capabilities and policy constraints. Unlike traditional methods, it employs a graph-based policy model to represent relationships, likelihoods, and complexities of policy interactions, enabling highly detailed diagnostics. IntellAgent also identifies critical performance gaps, offering actionable insights for targeted optimization. Its modular, open-source design supports seamless integration of new domains, policies, and APIs, fostering reproducibility and community collaboration. Our findings demonstrate that IntellAgent serves as an effective framework for advancing conversational AI by addressing challenges in bridging research and deployment. The framework is available at this https URL</li>
</ul>

<h3>Title: Generative AI-driven Cross-layer Covert Communication: Fundamentals, Framework and Case Study</h3>
<ul>
<li><strong>Authors: </strong>Tianhao Liu, Jiqiang Liu, Tao Zhang, Jian Wang, Jiacheng Wang, Jiawen Kang, Dusit Niyato, Shiwen Mao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11068">https://arxiv.org/abs/2501.11068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11068">https://arxiv.org/pdf/2501.11068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11068]] Generative AI-driven Cross-layer Covert Communication: Fundamentals, Framework and Case Study(https://arxiv.org/abs/2501.11068)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Ensuring end-to-end cross-layer communication security in military networks by selecting covert schemes between nodes is a key solution for military communication security. With the development of communication technology, covert communication has expanded from the physical layer to the network and application layers, utilizing methods such as artificial noise, private networks, and semantic coding to transmit secret messages. However, as adversaries continuously eavesdrop on specific communication channels, the accumulation of sufficient data may reveal underlying patterns that influence concealment, and establishing a cross-layer covert communication mechanism emerges as an effective strategy to mitigate these regulatory challenges. In this article, we first survey the communication security solution based on covert communication, specifically targeting three typical scenarios: device-to-device, private network communication, and public network communication, and analyze their application scopes. Furthermore, we propose an end-to-end cross-layer covert communication scheme driven by Generative Artificial Intelligence (GenAI), highlighting challenges and their solutions. Additionally, a case study is conducted using diffusion reinforcement learning to sovle cloud edge internet of things cross-layer secure communication.</li>
</ul>

<h3>Title: Achieving Network Resilience through Graph Neural Network-enabled Deep Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Xuzeng Li, Tao Zhang, Jian Wang, Zhen Han, Jiqiang Liu, Jiawen Kang, Dusit Niyato, Abbas Jamalipour</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11074">https://arxiv.org/abs/2501.11074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11074">https://arxiv.org/pdf/2501.11074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11074]] Achieving Network Resilience through Graph Neural Network-enabled Deep Reinforcement Learning(https://arxiv.org/abs/2501.11074)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep reinforcement learning (DRL) has been widely used in many important tasks of communication networks. In order to improve the perception ability of DRL on the network, some studies have combined graph neural networks (GNNs) with DRL, which use the GNNs to extract unstructured features of the network. However, as networks continue to evolve and become increasingly complex, existing GNN-DRL methods still face challenges in terms of scalability and robustness. Moreover, these methods are inadequate for addressing network security issues. From the perspective of security and robustness, this paper explores the solution of combining GNNs with DRL to build a resilient network. This article starts with a brief tutorial of GNNs and DRL, and introduces their existing applications in networks. Furthermore, we introduce the network security methods that can be strengthened by GNN-DRL approaches. Then, we designed a framework based on GNN-DRL to defend against attacks and enhance network resilience. Additionally, we conduct a case study using an encrypted traffic dataset collected from real IoT environments, and the results demonstrated the effectiveness and superiority of our framework. Finally, we highlight key open challenges and opportunities for enhancing network resilience with GNN-DRL.</li>
</ul>

<h3>Title: Federated Deep Reinforcement Learning for Energy Efficient Multi-Functional RIS-Assisted Low-Earth Orbit Networks</h3>
<ul>
<li><strong>Authors: </strong>Li-Hsiang Shen, Jyun-Jhe Huang, Kai-Ten Feng, Lie-Liang Yang, Jen-Ming Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11079">https://arxiv.org/abs/2501.11079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11079">https://arxiv.org/pdf/2501.11079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11079]] Federated Deep Reinforcement Learning for Energy Efficient Multi-Functional RIS-Assisted Low-Earth Orbit Networks(https://arxiv.org/abs/2501.11079)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>In this paper, a novel network architecture that deploys the multi-functional reconfigurable intelligent surface (MF-RIS) in low-Earth orbit (LEO) is proposed. Unlike traditional RIS with only signal reflection capability, the MF-RIS can reflect, refract, and amplify signals, as well as harvest energy from wireless signals. Given the high energy demands in shadow regions where solar energy is unavailable, MF-RIS is deployed in LEO to enhance signal coverage and improve energy efficiency (EE). To address this, we formulate a long-term EE optimization problem by determining the optimal parameters for MF-RIS configurations, including amplification and phase-shifts, energy harvesting ratios, and LEO transmit beamforming. To address the complex non-convex and non-linear problem, a federated learning enhanced multi-agent deep deterministic policy gradient (FEMAD) scheme is designed. Multi-agent DDPG of each agent can provide the optimal action policy from its interaction to environments, whereas federated learning enables the hidden information exchange among multi-agents. In numerical results, we can observe significant EE improvements compared to the other benchmarks, including centralized deep reinforcement learning as well as distributed multi-agent deep deterministic policy gradient (DDPG). Additionally, the proposed LEO-MF-RIS architecture has demonstrated its effectiveness, achieving the highest EE performance compared to the scenarios of fixed/no energy harvesting in MF-RIS, traditional reflection-only RIS, and deployment without RISs/MF-RISs.</li>
</ul>

<h3>Title: Leveraging counterfactual concepts for debugging and improving CNN model performance</h3>
<ul>
<li><strong>Authors: </strong>Syed Ali Tariq, Tehseen Zia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11087">https://arxiv.org/abs/2501.11087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11087">https://arxiv.org/pdf/2501.11087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11087]] Leveraging counterfactual concepts for debugging and improving CNN model performance(https://arxiv.org/abs/2501.11087)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Counterfactual explanation methods have recently received significant attention for explaining CNN-based image classifiers due to their ability to provide easily understandable explanations that align more closely with human reasoning. However, limited attention has been given to utilizing explainability methods to improve model performance. In this paper, we propose to leverage counterfactual concepts aiming to enhance the performance of CNN models in image classification tasks. Our proposed approach utilizes counterfactual reasoning to identify crucial filters used in the decision-making process. Following this, we perform model retraining through the design of a novel methodology and loss functions that encourage the activation of class-relevant important filters and discourage the activation of irrelevant filters for each class. This process effectively minimizes the deviation of activation patterns of local predictions and the global activation patterns of their respective inferred classes. By incorporating counterfactual explanations, we validate unseen model predictions and identify misclassifications. The proposed methodology provides insights into potential weaknesses and biases in the model's learning process, enabling targeted improvements and enhanced performance. Experimental results on publicly available datasets have demonstrated an improvement of 1-2\%, validating the effectiveness of the approach.</li>
</ul>

<h3>Title: Enhanced Suicidal Ideation Detection from Social Media Using a CNN-BiLSTM Hybrid Model</h3>
<ul>
<li><strong>Authors: </strong>Mohaiminul Islam Bhuiyan, Nur Shazwani Kamarudin, Nur Hafieza Ismail</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11094">https://arxiv.org/abs/2501.11094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11094">https://arxiv.org/pdf/2501.11094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11094]] Enhanced Suicidal Ideation Detection from Social Media Using a CNN-BiLSTM Hybrid Model(https://arxiv.org/abs/2501.11094)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Suicidal ideation detection is crucial for preventing suicides, a leading cause of death worldwide. Many individuals express suicidal thoughts on social media, offering a vital opportunity for early detection through advanced machine learning techniques. The identification of suicidal ideation in social media text is improved by utilising a hybrid framework that integrates Convolutional Neural Networks (CNN) and Bidirectional Long Short-Term Memory (BiLSTM), enhanced with an attention mechanism. To enhance the interpretability of the model's predictions, Explainable AI (XAI) methods are applied, with a particular focus on SHapley Additive exPlanations (SHAP), are incorporated. At first, the model managed to reach an accuracy of 92.81%. By applying fine-tuning and early stopping techniques, the accuracy improved to 94.29%. The SHAP analysis revealed key features influencing the model's predictions, such as terms related to mental health struggles. This level of transparency boosts the model's credibility while helping mental health professionals understand and trust the predictions. This work highlights the potential for improving the accuracy and interpretability of detecting suicidal tendencies, making a valuable contribution to the progress of mental health monitoring systems. It emphasizes the significance of blending powerful machine learning methods with explainability to develop reliable and impactful mental health solutions.</li>
</ul>

<h3>Title: Reproducibility review of "Why Not Other Classes": Towards Class-Contrastive Back-Propagation Explanations</h3>
<ul>
<li><strong>Authors: </strong>Arvid Eriksson (1), Anton Israelsson (1), Mattias Kallhauge (1) ((1) KTH Royal Institute of Technology)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11096">https://arxiv.org/abs/2501.11096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11096">https://arxiv.org/pdf/2501.11096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11096]] Reproducibility review of "Why Not Other Classes": Towards Class-Contrastive Back-Propagation Explanations(https://arxiv.org/abs/2501.11096)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>"Why Not Other Classes?": Towards Class-Contrastive Back-Propagation Explanations (Wang & Wang, 2022) provides a method for contrastively explaining why a certain class in a neural network image classifier is chosen above others. This method consists of using back-propagation-based explanation methods from after the softmax layer rather than before. Our work consists of reproducing the work in the original paper. We also provide extensions to the paper by evaluating the method on XGradCAM, FullGrad, and Vision Transformers to evaluate its generalization capabilities. The reproductions show similar results as the original paper, with the only difference being the visualization of heatmaps which could not be reproduced to look similar. The generalization seems to be generally good, with implementations working for Vision Transformers and alternative back-propagation methods. We also show that the original paper suffers from issues such as a lack of detail in the method and an erroneous equation which makes reproducibility difficult. To remedy this we provide an open-source repository containing all code used for this project.</li>
</ul>

<h3>Title: Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yiyao Yu, Yuxiang Zhang, Dongdong Zhang, Xiao Liang, Hengyuan Zhang, Xingxing Zhang, Ziyi Yang, Mahmoud Khademi, Hany Awadalla, Junjie Wang, Yujiu Yang, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11110">https://arxiv.org/abs/2501.11110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11110">https://arxiv.org/pdf/2501.11110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11110]] Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective(https://arxiv.org/abs/2501.11110)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have made notable progress in mathematical reasoning, yet they often rely on single-paradigm reasoning that limits their effectiveness across diverse tasks. In this paper, we introduce Chain-of-Reasoning (CoR), a novel unified framework that integrates multiple reasoning paradigms--Natural Language Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning (SR)--to enable synergistic collaboration. CoR generates multiple potential answers using different reasoning paradigms and synthesizes them into a coherent final solution. We propose a Progressive Paradigm Training (PPT) strategy that allows models to progressively master these paradigms, culminating in the development of CoR-Math-7B. Experimental results demonstrate that CoR-Math-7B significantly outperforms current SOTA models, achieving up to a 41.0% absolute improvement over GPT-4 in theorem proving tasks and a 7.9% improvement over RL-based methods in arithmetic tasks. These results showcase the enhanced mathematical comprehensive ability of our model, achieving significant performance gains on specific tasks and enabling zero-shot generalization across tasks.</li>
</ul>

<h3>Title: A Novel Pearson Correlation-Based Merging Algorithm for Robust Distributed Machine Learning with Heterogeneous Data</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Ghabel Rahmat, Majid Khalilian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11112">https://arxiv.org/abs/2501.11112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11112">https://arxiv.org/pdf/2501.11112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11112]] A Novel Pearson Correlation-Based Merging Algorithm for Robust Distributed Machine Learning with Heterogeneous Data(https://arxiv.org/abs/2501.11112)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning faces significant challenges in scenarios with heterogeneous data distributions and adverse network conditions, such as delays, packet loss, and data poisoning attacks. This paper proposes a novel method based on the SCAFFOLD algorithm to improve the quality of local updates and enhance the robustness of the global model. The key idea is to form intermediary nodes by merging local models with high similarity, using the Pearson correlation coefficient as a similarity measure. The proposed merging algorithm reduces the number of local nodes while maintaining the accuracy of the global model, effectively addressing communication overhead and bandwidth consumption. Experimental results on the MNIST dataset under simulated federated learning scenarios demonstrate the method's effectiveness. After 10 rounds of training using a CNN model, the proposed approach achieved accuracies of 0.82, 0.73, and 0.66 under normal conditions, packet loss and data poisoning attacks, respectively, outperforming the baseline SCAFFOLD algorithm. These results highlight the potential of the proposed method to improve efficiency and resilience in federated learning systems.</li>
</ul>

<h3>Title: Clinical trial cohort selection using Large Language Models on n2c2 Challenges</h3>
<ul>
<li><strong>Authors: </strong>Chi-en Amy Tai, Xavier Tannier</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11114">https://arxiv.org/abs/2501.11114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11114">https://arxiv.org/pdf/2501.11114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11114]] Clinical trial cohort selection using Large Language Models on n2c2 Challenges(https://arxiv.org/abs/2501.11114)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Clinical trials are a critical process in the medical field for introducing new treatments and innovations. However, cohort selection for clinical trials is a time-consuming process that often requires manual review of patient text records for specific keywords. Though there have been studies on standardizing the information across the various platforms, Natural Language Processing (NLP) tools remain crucial for spotting eligibility criteria in textual reports. Recently, pre-trained large language models (LLMs) have gained popularity for various NLP tasks due to their ability to acquire a nuanced understanding of text. In this paper, we study the performance of large language models on clinical trial cohort selection and leverage the n2c2 challenges to benchmark their performance. Our results are promising with regard to the incorporation of LLMs for simple cohort selection tasks, but also highlight the difficulties encountered by these models as soon as fine-grained knowledge and reasoning are required.</li>
</ul>

<h3>Title: Tell me about yourself: LLMs are aware of their learned behaviors</h3>
<ul>
<li><strong>Authors: </strong>Jan Betley, Xuchan Bao, Martín Soto, Anna Sztyber-Betley, James Chua, Owain Evans</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11120">https://arxiv.org/abs/2501.11120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11120">https://arxiv.org/pdf/2501.11120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11120]] Tell me about yourself: LLMs are aware of their learned behaviors(https://arxiv.org/abs/2501.11120)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>We study behavioral self-awareness -- an LLM's ability to articulate its behaviors without requiring in-context examples. We finetune LLMs on datasets that exhibit particular behaviors, such as (a) making high-risk economic decisions, and (b) outputting insecure code. Despite the datasets containing no explicit descriptions of the associated behavior, the finetuned LLMs can explicitly describe it. For example, a model trained to output insecure code says, ``The code I write is insecure.'' Indeed, models show behavioral self-awareness for a range of behaviors and for diverse evaluations. Note that while we finetune models to exhibit behaviors like writing insecure code, we do not finetune them to articulate their own behaviors -- models do this without any special training or examples. Behavioral self-awareness is relevant for AI safety, as models could use it to proactively disclose problematic behaviors. In particular, we study backdoor policies, where models exhibit unexpected behaviors only under certain trigger conditions. We find that models can sometimes identify whether or not they have a backdoor, even without its trigger being present. However, models are not able to directly output their trigger by default. Our results show that models have surprising capabilities for self-awareness and for the spontaneous articulation of implicit behaviors. Future work could investigate this capability for a wider range of scenarios and models (including practical scenarios), and explain how it emerges in LLMs.</li>
</ul>

<h3>Title: CLOFAI: A Dataset of Real And Fake Image Classification Tasks for Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>William Doherty, Anton Lee, Heitor Murilo Gomes</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11140">https://arxiv.org/abs/2501.11140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11140">https://arxiv.org/pdf/2501.11140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11140]] CLOFAI: A Dataset of Real And Fake Image Classification Tasks for Continual Learning(https://arxiv.org/abs/2501.11140)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of generative AI models capable of creating realistic media has led to a need for classifiers that can accurately distinguish between genuine and artificially-generated images. A significant challenge for these classifiers emerges when they encounter images from generative models that are not represented in their training data, usually resulting in diminished performance. A typical approach is to periodically update the classifier's training data with images from the new generative models then retrain the classifier on the updated dataset. However, in some real-life scenarios, storage, computational, or privacy constraints render this approach impractical. Additionally, models used in security applications may be required to rapidly adapt. In these circumstances, continual learning provides a promising alternative, as the classifier can be updated without retraining on the entire dataset. In this paper, we introduce a new dataset called CLOFAI (Continual Learning On Fake and Authentic Images), which takes the form of a domain-incremental image classification problem. Moreover, we showcase the applicability of this dataset as a benchmark for evaluating continual learning methodologies. In doing this, we set a baseline on our novel dataset using three foundational continual learning methods -- EWC, GEM, and Experience Replay -- and find that EWC performs poorly, while GEM and Experience Replay show promise, performing significantly better than a Naive baseline. The dataset and code to run the experiments can be accessed from the following GitHub repository: this https URL.</li>
</ul>

<h3>Title: Efficient Frame Extraction: A Novel Approach Through Frame Similarity and Surgical Tool Tracking for Video Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Huu Phong Nguyen, Shekhar Madhav Khairnar, Sofia Garces Palacios, Amr Al-Abbas, Francisco Antunes, Bernardete Ribeiro, Melissa E. Hogg, Amer H. Zureikat, Patricio M. Polanco, Herbert Zeh III, Ganesh Sankaranarayanan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11153">https://arxiv.org/abs/2501.11153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11153">https://arxiv.org/pdf/2501.11153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11153]] Efficient Frame Extraction: A Novel Approach Through Frame Similarity and Surgical Tool Tracking for Video Segmentation(https://arxiv.org/abs/2501.11153)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>The interest in leveraging Artificial Intelligence (AI) for surgical procedures to automate analysis has witnessed a significant surge in recent years. One of the primary tools for recording surgical procedures and conducting subsequent analyses, such as performance assessment, is through videos. However, these operative videos tend to be notably lengthy compared to other fields, spanning from thirty minutes to several hours, which poses a challenge for AI models to effectively learn from them. Despite this challenge, the foreseeable increase in the volume of such videos in the near future necessitates the development and implementation of innovative techniques to tackle this issue effectively. In this article, we propose a novel technique called Kinematics Adaptive Frame Recognition (KAFR) that can efficiently eliminate redundant frames to reduce dataset size and computation time while retaining useful frames to improve accuracy. Specifically, we compute the similarity between consecutive frames by tracking the movement of surgical tools. Our approach follows these steps: i) Tracking phase: a YOLOv8 model is utilized to detect tools presented in the scene, ii) Similarity phase: Similarities between consecutive frames are computed by estimating variation in the spatial positions and velocities of the tools, iii) Classification phase: A X3D CNN is trained to classify segmentation. We evaluate the effectiveness of our approach by analyzing datasets obtained through retrospective reviews of cases at two referral centers. The Gastrojejunostomy (GJ) dataset covers procedures performed between 2017 to 2021, while the Pancreaticojejunostomy (PJ) dataset spans from 2011 to 2022 at the same centers. By adaptively selecting relevant frames, we achieve a tenfold reduction in the number of frames while improving accuracy by 4.32% (from 0.749 to 0.7814).</li>
</ul>

<h3>Title: Federated Testing (FedTest): A New Scheme to Enhance Convergence and Mitigate Adversarial Attacks in Federating Learning</h3>
<ul>
<li><strong>Authors: </strong>Mustafa Ghaleb, Mohanad Obeed, Muhamad Felemban, Anas Chaaban, Halim Yanikomeroglu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11167">https://arxiv.org/abs/2501.11167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11167">https://arxiv.org/pdf/2501.11167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11167]] Federated Testing (FedTest): A New Scheme to Enhance Convergence and Mitigate Adversarial Attacks in Federating Learning(https://arxiv.org/abs/2501.11167)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has emerged as a significant paradigm for training machine learning models. This is due to its data-privacy-preserving property and its efficient exploitation of distributed computational resources. This is achieved by conducting the training process in parallel at distributed users. However, traditional FL strategies grapple with difficulties in evaluating the quality of received models, handling unbalanced models, and reducing the impact of detrimental models. To resolve these problems, we introduce a novel federated learning framework, which we call federated testing for federated learning (FedTest). In the FedTest method, the local data of a specific user is used to train the model of that user and test the models of the other users. This approach enables users to test each other's models and determine an accurate score for each. This score can then be used to aggregate the models efficiently and identify any malicious ones. Our numerical results reveal that the proposed method not only accelerates convergence rates but also diminishes the potential influence of malicious users. This significantly enhances the overall efficiency and robustness of FL systems.</li>
</ul>

<h3>Title: DeepEyeNet: Adaptive Genetic Bayesian Algorithm Based Hybrid ConvNeXtTiny Framework For Multi-Feature Glaucoma Eye Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Angshuman Roy, Anuvab Sen, Soumyajit Gupta, Soham Haldar, Subhrajit Deb, Taraka Nithin Vankala, Arkapravo Das</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11168">https://arxiv.org/abs/2501.11168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11168">https://arxiv.org/pdf/2501.11168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11168]] DeepEyeNet: Adaptive Genetic Bayesian Algorithm Based Hybrid ConvNeXtTiny Framework For Multi-Feature Glaucoma Eye Diagnosis(https://arxiv.org/abs/2501.11168)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Glaucoma is a leading cause of irreversible blindness worldwide, emphasizing the critical need for early detection and intervention. In this paper, we present DeepEyeNet, a novel and comprehensive framework for automated glaucoma detection using retinal fundus images. Our approach integrates advanced image standardization through dynamic thresholding, precise optic disc and cup segmentation via a U-Net model, and comprehensive feature extraction encompassing anatomical and texture-based features. We employ a customized ConvNeXtTiny based Convolutional Neural Network (CNN) classifier, optimized using our Adaptive Genetic Bayesian Optimization (AGBO) algorithm. This proposed AGBO algorithm balances exploration and exploitation in hyperparameter tuning, leading to significant performance improvements. Experimental results on the EyePACS-AIROGS-light-V2 dataset demonstrate that DeepEyeNet achieves a high classification accuracy of 95.84%, which was possible due to the effective optimization provided by the novel AGBO algorithm, outperforming existing methods. The integration of sophisticated image processing techniques, deep learning, and optimized hyperparameter tuning through our proposed AGBO algorithm positions DeepEyeNet as a promising tool for early glaucoma detection in clinical settings.</li>
</ul>

<h3>Title: AIMA at SemEval-2024 Task 3: Simple Yet Powerful Emotion Cause Pair Analysis</h3>
<ul>
<li><strong>Authors: </strong>Alireza Ghahramani Kure, Mahshid Dehghani, Mohammad Mahdi Abootorabi, Nona Ghazizadeh, Seyed Arshan Dalili, Ehsaneddin Asgari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11170">https://arxiv.org/abs/2501.11170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11170">https://arxiv.org/pdf/2501.11170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11170]] AIMA at SemEval-2024 Task 3: Simple Yet Powerful Emotion Cause Pair Analysis(https://arxiv.org/abs/2501.11170)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The SemEval-2024 Task 3 presents two subtasks focusing on emotion-cause pair extraction within conversational contexts. Subtask 1 revolves around the extraction of textual emotion-cause pairs, where causes are defined and annotated as textual spans within the conversation. Conversely, Subtask 2 extends the analysis to encompass multimodal cues, including language, audio, and vision, acknowledging instances where causes may not be exclusively represented in the textual data. Our proposed model for emotion-cause analysis is meticulously structured into three core segments: (i) embedding extraction, (ii) cause-pair extraction & emotion classification, and (iii) cause extraction using QA after finding pairs. Leveraging state-of-the-art techniques and fine-tuning on task-specific datasets, our model effectively unravels the intricate web of conversational dynamics and extracts subtle cues signifying causality in emotional expressions. Our team, AIMA, demonstrated strong performance in the SemEval-2024 Task 3 competition. We ranked as the 10th in subtask 1 and the 6th in subtask 2 out of 23 teams.</li>
</ul>

<h3>Title: Counteracting temporal attacks in Video Copy Detection</h3>
<ul>
<li><strong>Authors: </strong>Katarzyna Fojcik, Piotr Syga</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IR, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11171">https://arxiv.org/abs/2501.11171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11171">https://arxiv.org/pdf/2501.11171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11171]] Counteracting temporal attacks in Video Copy Detection(https://arxiv.org/abs/2501.11171)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust</a></li>
<li><strong>Abstract: </strong>Video Copy Detection (VCD) plays a crucial role in copyright protection and content verification by identifying duplicates and near-duplicates in large-scale video databases. The META AI Challenge on video copy detection provided a benchmark for evaluating state-of-the-art methods, with the Dual-level detection approach emerging as a winning solution. This method integrates Video Editing Detection and Frame Scene Detection to handle adversarial transformations and large datasets efficiently. However, our analysis reveals significant limitations in the VED component, particularly in its ability to handle exact copies. Moreover, Dual-level detection shows vulnerability to temporal attacks. To address it, we propose an improved frame selection strategy based on local maxima of interframe differences, which enhances robustness against adversarial temporal modifications while significantly reducing computational overhead. Our method achieves an increase of 1.4 to 5.8 times in efficiency over the standard 1 FPS approach. Compared to Dual-level detection method, our approach maintains comparable micro-average precision ($\mu$AP) while also demonstrating improved robustness against temporal attacks. Given 56\% reduced representation size and the inference time of more than 2 times faster, our approach is more suitable to real-world resource restriction.</li>
</ul>

<h3>Title: Can Safety Fine-Tuning Be More Principled? Lessons Learned from Cybersecurity</h3>
<ul>
<li><strong>Authors: </strong>David Williams-King, Linh Le, Adam Oberman, Yoshua Bengio</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11183">https://arxiv.org/abs/2501.11183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11183">https://arxiv.org/pdf/2501.11183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11183]] Can Safety Fine-Tuning Be More Principled? Lessons Learned from Cybersecurity(https://arxiv.org/abs/2501.11183)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>As LLMs develop increasingly advanced capabilities, there is an increased need to minimize the harm that could be caused to society by certain model outputs; hence, most LLMs have safety guardrails added, for example via fine-tuning. In this paper, we argue the position that current safety fine-tuning is very similar to a traditional cat-and-mouse game (or arms race) between attackers and defenders in cybersecurity. Model jailbreaks and attacks are patched with bandaids to target the specific attack mechanism, but many similar attack vectors might remain. When defenders are not proactively coming up with principled mechanisms, it becomes very easy for attackers to sidestep any new defenses. We show how current defenses are insufficient to prevent new adversarial jailbreak attacks, reward hacking, and loss of control problems. In order to learn from past mistakes in cybersecurity, we draw analogies with historical examples and develop lessons learned that can be applied to LLM safety. These arguments support the need for new and more principled approaches to designing safe models, which are architected for security from the beginning. We describe several such approaches from the AI literature.</li>
</ul>

<h3>Title: Embedding-Driven Diversity Sampling to Improve Few-Shot Synthetic Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Ivan Lopez, Fateme Nateghi Haredasht, Kaitlin Caoili, Jonathan H Chen, Akshay Chaudhari</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11199">https://arxiv.org/abs/2501.11199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11199">https://arxiv.org/pdf/2501.11199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11199]] Embedding-Driven Diversity Sampling to Improve Few-Shot Synthetic Data Generation(https://arxiv.org/abs/2501.11199)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Accurate classification of clinical text often requires fine-tuning pre-trained language models, a process that is costly and time-consuming due to the need for high-quality data and expert annotators. Synthetic data generation offers an alternative, though pre-trained models may not capture the syntactic diversity of clinical notes. We propose an embedding-driven approach that uses diversity sampling from a small set of real clinical notes to guide large language models in few-shot prompting, generating synthetic text that better reflects clinical syntax. We evaluated this method using the CheXpert dataset on a classification task, comparing it to random few-shot and zero-shot approaches. Using cosine similarity and a Turing test, our approach produced synthetic notes that more closely align with real clinical text. Our pipeline reduced the data needed to reach the 0.85 AUC cutoff by 40% for AUROC and 30% for AUPRC, while augmenting models with synthetic data improved AUROC by 57% and AUPRC by 68%. Additionally, our synthetic data was 0.9 times as effective as real data, a 60% improvement in value.</li>
</ul>

<h3>Title: Advancing Oyster Phenotype Segmentation with Multi-Network Ensemble and Multi-Scale mechanism</h3>
<ul>
<li><strong>Authors: </strong>Wenli Yang, Yanyu Chen, Andrew Trotter, Byeong Kang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11203">https://arxiv.org/abs/2501.11203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11203">https://arxiv.org/pdf/2501.11203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11203]] Advancing Oyster Phenotype Segmentation with Multi-Network Ensemble and Multi-Scale mechanism(https://arxiv.org/abs/2501.11203)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Phenotype segmentation is pivotal in analysing visual features of living organisms, enhancing our understanding of their characteristics. In the context of oysters, meat quality assessment is paramount, focusing on shell, meat, gonad, and muscle components. Traditional manual inspection methods are time-consuming and subjective, prompting the adoption of machine vision technology for efficient and objective evaluation. We explore machine vision's capacity for segmenting oyster components, leading to the development of a multi-network ensemble approach with a global-local hierarchical attention mechanism. This approach integrates predictions from diverse models and addresses challenges posed by varying scales, ensuring robust instance segmentation across components. Finally, we provide a comprehensive evaluation of the proposed method's performance using different real-world datasets, highlighting its efficacy and robustness in enhancing oyster phenotype segmentation.</li>
</ul>

<h3>Title: ENOLA: Efficient Control-Flow Attestation for Embedded Systems</h3>
<ul>
<li><strong>Authors: </strong>Md Armanuzzaman, Engin Kirda, Ziming Zhao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11207">https://arxiv.org/abs/2501.11207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11207">https://arxiv.org/pdf/2501.11207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11207]] ENOLA: Efficient Control-Flow Attestation for Embedded Systems(https://arxiv.org/abs/2501.11207)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Microcontroller-based embedded systems are vital in daily life, but are especially vulnerable to control-flow hijacking attacks due to hardware and software constraints. Control-Flow Attestation (CFA) aims to precisely attest the execution path of a program to a remote verifier. However, existing CFA solutions face challenges with large measurement and/or trace data, limiting these solutions to small programs. In addition, slow software-based measurement calculations limit their feasibility for microcontroller systems. In this paper, we present ENOLA, an efficient control-flow attestation solution for low-end embedded systems. ENOLA introduces a novel authenticator that achieves linear space complexity. Moreover, ENOLA capitalizes on the latest hardware-assisted message authentication code computation capabilities found in commercially-available devices for measurement computation. ENOLA employs a trusted execution environment, and allocates general-purpose registers to thwart memory corruption attacks. We have developed the ENOLA compiler through LLVM passes and attestation engine on the ARMv8.1-M architecture. Our evaluations demonstrate ENOLA's effectiveness in minimizing data transmission, while achieving lower or comparable performance to the existing works.</li>
</ul>

<h3>Title: Risk Analysis of Flowlines in the Oil and Gas Sector: A GIS and Machine Learning Approach</h3>
<ul>
<li><strong>Authors: </strong>I. Chittumuri, N. Alshehab, R. J. Voss, L. L. Douglass, S. Kamrava, Y. Fan, J. Miskimins, W. Fleckenstein, S. Bandyopadhyay</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11213">https://arxiv.org/abs/2501.11213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11213">https://arxiv.org/pdf/2501.11213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11213]] Risk Analysis of Flowlines in the Oil and Gas Sector: A GIS and Machine Learning Approach(https://arxiv.org/abs/2501.11213)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>This paper presents a risk analysis of flowlines in the oil and gas sector using Geographic Information Systems (GIS) and machine learning (ML). Flowlines, vital conduits transporting oil, gas, and water from wellheads to surface facilities, often face under-assessment compared to transmission pipelines. This study addresses this gap using advanced tools to predict and mitigate failures, improving environmental safety and reducing human exposure. Extensive datasets from the Colorado Energy and Carbon Management Commission (ECMC) were processed through spatial matching, feature engineering, and geometric extraction to build robust predictive models. Various ML algorithms, including logistic regression, support vector machines, gradient boosting decision trees, and K-Means clustering, were used to assess and classify risks, with ensemble classifiers showing superior accuracy, especially when paired with Principal Component Analysis (PCA) for dimensionality reduction. Finally, a thorough data analysis highlighted spatial and operational factors influencing risks, identifying high-risk zones for focused monitoring. Overall, the study demonstrates the transformative potential of integrating GIS and ML in flowline risk management, proposing a data-driven approach that emphasizes the need for accurate data and refined models to improve safety in petroleum extraction.</li>
</ul>

<h3>Title: Mitigating Spatial Disparity in Urban Prediction Using Residual-Aware Spatiotemporal Graph Neural Networks: A Chicago Case Study</h3>
<ul>
<li><strong>Authors: </strong>Dingyi Zhuang, Hanyong Xu, Xiaotong Guo, Yunhan Zheng, Shenhao Wang, Jinhua Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11214">https://arxiv.org/abs/2501.11214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11214">https://arxiv.org/pdf/2501.11214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11214]] Mitigating Spatial Disparity in Urban Prediction Using Residual-Aware Spatiotemporal Graph Neural Networks: A Chicago Case Study(https://arxiv.org/abs/2501.11214)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Urban prediction tasks, such as forecasting traffic flow, temperature, and crime rates, are crucial for efficient urban planning and management. However, existing Spatiotemporal Graph Neural Networks (ST-GNNs) often rely solely on accuracy, overlooking spatial and demographic disparities in their predictions. This oversight can lead to imbalanced resource allocation and exacerbate existing inequities in urban areas. This study introduces a Residual-Aware Attention (RAA) Block and an equality-enhancing loss function to address these disparities. By adapting the adjacency matrix during training and incorporating spatial disparity metrics, our approach aims to reduce local segregation of residuals and errors. We applied our methodology to urban prediction tasks in Chicago, utilizing a travel demand dataset as an example. Our model achieved a 48% significant improvement in fairness metrics with only a 9% increase in error metrics. Spatial analysis of residual distributions revealed that models with RAA Blocks produced more equitable prediction results, particularly by reducing errors clustered in central regions. Attention maps demonstrated the model's ability to dynamically adjust focus, leading to more balanced predictions. Case studies of various community areas in Chicago further illustrated the effectiveness of our approach in addressing spatial and demographic disparities, supporting more balanced and equitable urban planning and policy-making.</li>
</ul>

<h3>Title: Leveraging GANs For Active Appearance Models Optimized Model Fitting</h3>
<ul>
<li><strong>Authors: </strong>Anurag Awasthi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11218">https://arxiv.org/abs/2501.11218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11218">https://arxiv.org/pdf/2501.11218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11218]] Leveraging GANs For Active Appearance Models Optimized Model Fitting(https://arxiv.org/abs/2501.11218)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Generative Adversarial Networks (GANs) have gained prominence in refining model fitting tasks in computer vision, particularly in domains involving deformable models like Active Appearance Models (AAMs). This paper explores the integration of GANs to enhance the AAM fitting process, addressing challenges in optimizing nonlinear parameters associated with appearance and shape variations. By leveraging GANs' adversarial training framework, the aim is to minimize fitting errors and improve convergence rates. Achieving robust performance even in cases with high appearance variability and occlusions. Our approach demonstrates significant improvements in accuracy and computational efficiency compared to traditional optimization techniques, thus establishing GANs as a potent tool for advanced image model fitting.</li>
</ul>

<h3>Title: An Imbalanced Learning-based Sampling Method for Physics-informed Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Luo, Yahong Yang, Yuan Yuan, Shixin Xu, Wenrui Hao</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11222">https://arxiv.org/abs/2501.11222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11222">https://arxiv.org/pdf/2501.11222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11222]] An Imbalanced Learning-based Sampling Method for Physics-informed Neural Networks(https://arxiv.org/abs/2501.11222)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper introduces Residual-based Smote (RSmote), an innovative local adaptive sampling technique tailored to improve the performance of Physics-Informed Neural Networks (PINNs) through imbalanced learning strategies. Traditional residual-based adaptive sampling methods, while effective in enhancing PINN accuracy, often struggle with efficiency and high memory consumption, particularly in high-dimensional problems. RSmote addresses these challenges by targeting regions with high residuals and employing oversampling techniques from imbalanced learning to refine the sampling process. Our approach is underpinned by a rigorous theoretical analysis that supports the effectiveness of RSmote in managing computational resources more efficiently. Through extensive evaluations, we benchmark RSmote against the state-of-the-art Residual-based Adaptive Distribution (RAD) method across a variety of dimensions and differential equations. The results demonstrate that RSmote not only achieves or exceeds the accuracy of RAD but also significantly reduces memory usage, making it particularly advantageous in high-dimensional scenarios. These contributions position RSmote as a robust and resource-efficient solution for solving complex partial differential equations, especially when computational constraints are a critical consideration.</li>
</ul>

<h3>Title: Successive Interference Cancellation-aided Diffusion Models for Joint Channel Estimation and Data Detection in Low Rank Channel Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Sagnik Bhattacharya, Muhammad Ahmed Mohsin, Kamyar Rajabalifardi, John M. Cioffi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IT, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11229">https://arxiv.org/abs/2501.11229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11229">https://arxiv.org/pdf/2501.11229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11229]] Successive Interference Cancellation-aided Diffusion Models for Joint Channel Estimation and Data Detection in Low Rank Channel Scenarios(https://arxiv.org/abs/2501.11229)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper proposes a novel joint channel-estimation and source-detection algorithm using successive interference cancellation (SIC)-aided generative score-based diffusion models. Prior work in this area focuses on massive MIMO scenarios, which are typically characterized by full-rank channels, and fail in low-rank channel scenarios. The proposed algorithm outperforms existing methods in joint source-channel estimation, especially in low-rank scenarios where the number of users exceeds the number of antennas at the access point (AP). The proposed score-based iterative diffusion process estimates the gradient of the prior distribution on partial channels, and recursively updates the estimated channel parts as well as the source. Extensive simulation results show that the proposed method outperforms the baseline methods in terms of normalized mean squared error (NMSE) and symbol error rate (SER) in both full-rank and low-rank channel scenarios, while having a more dominant effect in the latter, at various signal-to-noise ratios (SNR).</li>
</ul>

<h3>Title: KPL: Training-Free Medical Knowledge Mining of Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaxiang Liu, Tianxiang Hu, Jiawei Du, Ruiyuan Zhang, Joey Tianyi Zhou, Zuozhu Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11231">https://arxiv.org/abs/2501.11231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11231">https://arxiv.org/pdf/2501.11231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11231]] KPL: Training-Free Medical Knowledge Mining of Vision-Language Models(https://arxiv.org/abs/2501.11231)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Visual Language Models such as CLIP excel in image recognition due to extensive image-text pre-training. However, applying the CLIP inference in zero-shot classification, particularly for medical image diagnosis, faces challenges due to: 1) the inadequacy of representing image classes solely with single category names; 2) the modal gap between the visual and text spaces generated by CLIP encoders. Despite attempts to enrich disease descriptions with large language models, the lack of class-specific knowledge often leads to poor performance. In addition, empirical evidence suggests that existing proxy learning methods for zero-shot image classification on natural image datasets exhibit instability when applied to medical datasets. To tackle these challenges, we introduce the Knowledge Proxy Learning (KPL) to mine knowledge from CLIP. KPL is designed to leverage CLIP's multimodal understandings for medical image classification through Text Proxy Optimization and Multimodal Proxy Learning. Specifically, KPL retrieves image-relevant knowledge descriptions from the constructed knowledge-enhanced base to enrich semantic text proxies. It then harnesses input images and these descriptions, encoded via CLIP, to stably generate multimodal proxies that boost the zero-shot classification performance. Extensive experiments conducted on both medical and natural image datasets demonstrate that KPL enables effective zero-shot image classification, outperforming all baselines. These findings highlight the great potential in this paradigm of mining knowledge from CLIP for medical image classification and broader areas.</li>
</ul>

<h3>Title: Arbitrary-Threshold Fully Homomorphic Encryption with Lower Complexity</h3>
<ul>
<li><strong>Authors: </strong>Yijia Chang, Songze Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11235">https://arxiv.org/abs/2501.11235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11235">https://arxiv.org/pdf/2501.11235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11235]] Arbitrary-Threshold Fully Homomorphic Encryption with Lower Complexity(https://arxiv.org/abs/2501.11235)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, robust</a></li>
<li><strong>Abstract: </strong>Threshold fully homomorphic encryption (ThFHE) enables multiple parties to compute functions over their sensitive data without leaking data privacy. Most of existing ThFHE schemes are restricted to full threshold and require the participation of \textit{all} parties to output computing results. Compared with these full-threshold schemes, arbitrary threshold (ATh)-FHE schemes are robust to non-participants and can be a promising solution to many real-world applications. However, existing AThFHE schemes are either inefficient to be applied with a large number of parties $N$ and a large data size $K$, or insufficient to tolerate all types of non-participants. In this paper, we propose an AThFHE scheme to handle all types of non-participants with lower complexity over existing schemes. At the core of our scheme is the reduction from AThFHE construction to the design of a new primitive called \textit{approximate secret sharing} (ApproxSS). Particularly, we formulate ApproxSS and prove the correctness and security of AThFHE on top of arbitrary-threshold (ATh)-ApproxSS's properties. Such a reduction reveals that existing AThFHE schemes implicitly design ATh-ApproxSS following a similar idea called ``noisy share''. Nonetheless, their ATh-ApproxSS design has high complexity and become the performance bottleneck. By developing ATASSES, an ATh-ApproxSS scheme based on a novel ``encrypted share'' idea, we reduce the computation (resp. communication) complexity from $\mathcal{O}(N^2K)$ to $\mathcal{O}(N^2+K)$ (resp. from $\mathcal{O}(NK)$ to $\mathcal{O}(N+K)$). We not only theoretically prove the (approximate) correctness and security of ATASSES, but also empirically evaluate its efficiency against existing baselines. Particularly, when applying to a system with one thousand parties, ATASSES achieves a speedup of $3.83\times$ -- $15.4\times$ over baselines.</li>
</ul>

<h3>Title: A New Formulation of Lipschitz Constrained With Functional Gradient Learning for GANs</h3>
<ul>
<li><strong>Authors: </strong>Chang Wan, Ke Fan, Xinwei Sun, Yanwei Fu, Minglu Li, Yunliang Jiang, Zhonglong Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11236">https://arxiv.org/abs/2501.11236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11236">https://arxiv.org/pdf/2501.11236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11236]] A New Formulation of Lipschitz Constrained With Functional Gradient Learning for GANs(https://arxiv.org/abs/2501.11236)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a promising alternative method for training Generative Adversarial Networks (GANs) on large-scale datasets with clear theoretical guarantees. GANs are typically learned through a minimax game between a generator and a discriminator, which is known to be empirically unstable. Previous learning paradigms have encountered mode collapse issues without a theoretical solution. To address these challenges, we propose a novel Lipschitz-constrained Functional Gradient GANs learning (Li-CFG) method to stabilize the training of GAN and provide a theoretical foundation for effectively increasing the diversity of synthetic samples by reducing the neighborhood size of the latent vector. Specifically, we demonstrate that the neighborhood size of the latent vector can be reduced by increasing the norm of the discriminator gradient, resulting in enhanced diversity of synthetic samples. To efficiently enlarge the norm of the discriminator gradient, we introduce a novel {\epsilon}-centered gradient penalty that amplifies the norm of the discriminator gradient using the hyper-parameter {\epsilon}. In comparison to other constraints, our method enlarging the discriminator norm, thus obtaining the smallest neighborhood size of the latent vector. Extensive experiments on benchmark datasets for image generation demonstrate the efficacy of the Li-CFG method and the {\epsilon}-centered gradient penalty. The results showcase improved stability and increased diversity of synthetic samples.</li>
</ul>

<h3>Title: Fast instance-specific algorithm configuration with graph neural network</h3>
<ul>
<li><strong>Authors: </strong>Shingo Aihara, Matthieu Parizy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11240">https://arxiv.org/abs/2501.11240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11240">https://arxiv.org/pdf/2501.11240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11240]] Fast instance-specific algorithm configuration with graph neural network(https://arxiv.org/abs/2501.11240)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Combinatorial optimization (CO) problems are pivotal across various industrial applications, where the speed of solving these problems is crucial. Improving the performance of CO solvers across diverse input instances requires fine-tuning solver parameters for each instance. However, this tuning process is time-consuming, and the time required increases with the number of instances. To address this, a method called instance-specific algorithm configuration (ISAC) has been devised. This approach involves two main steps: training and execution. During the training step, features are extracted from various instances and then grouped into clusters. For each cluster, parameters are fine-tuned. This cluster-specific tuning process results in a set of generalized parameters for instances belonging to each class. In the execution step, features are extracted from an unknown instance to determine its cluster, and the corresponding pre-tuned parameters are applied. Generally, the running time of a solver is evaluated by the time to solution ($TTS$). However, methods like ISAC require preprocessing. Therefore, the total execution time is $T_{tot}=TTS+T_{tune}$, where $T_{tune}$ represents the tuning time. While the goal is to minimize $T_{tot}$, it is important to note that extracting features in the ISAC method requires a certain amount of computational time. The extracting features include summary statistics of the solver execution logs, which takes several 10 seconds. This research presents a method to significantly reduce the time of the ISAC execution step by streamlining feature extraction and class determination with a graph neural network. Experimental results show that $T_{tune}$ in the execution step, which take several 10 seconds in the original ISAC manner, could be reduced to sub-seconds.</li>
</ul>

<h3>Title: Irony in Emojis: A Comparative Study of Human and LLM Interpretation</h3>
<ul>
<li><strong>Authors: </strong>Yawen Zheng, Hanjia Lyu, Jiebo Luo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11241">https://arxiv.org/abs/2501.11241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11241">https://arxiv.org/pdf/2501.11241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11241]] Irony in Emojis: A Comparative Study of Human and LLM Interpretation(https://arxiv.org/abs/2501.11241)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Emojis have become a universal language in online communication, often carrying nuanced and context-dependent meanings. Among these, irony poses a significant challenge for Large Language Models (LLMs) due to its inherent incongruity between appearance and intent. This study examines the ability of GPT-4o to interpret irony in emojis. By prompting GPT-4o to evaluate the likelihood of specific emojis being used to express irony on social media and comparing its interpretations with human perceptions, we aim to bridge the gap between machine and human understanding. Our findings reveal nuanced insights into GPT-4o's interpretive capabilities, highlighting areas of alignment with and divergence from human behavior. Additionally, this research underscores the importance of demographic factors, such as age and gender, in shaping emoji interpretation and evaluates how these factors influence GPT-4o's performance.</li>
</ul>

<h3>Title: Multivariate Wireless Link Quality Prediction Based on Pre-trained Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhuangzhuang Yan, Xinyu Gu, Shilong Fan, Zhenyu Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11247">https://arxiv.org/abs/2501.11247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11247">https://arxiv.org/pdf/2501.11247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11247]] Multivariate Wireless Link Quality Prediction Based on Pre-trained Large Language Models(https://arxiv.org/abs/2501.11247)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Accurate and reliable link quality prediction (LQP) is crucial for optimizing network performance, ensuring communication stability, and enhancing user experience in wireless communications. However, LQP faces significant challenges due to the dynamic and lossy nature of wireless links, which are influenced by interference, multipath effects, fading, and blockage. In this paper, we propose GAT-LLM, a novel multivariate wireless link quality prediction model that combines Large Language Models (LLMs) with Graph Attention Networks (GAT) to enable accurate and reliable multivariate LQP of wireless communications. By framing LQP as a time series prediction task and appropriately preprocessing the input data, we leverage LLMs to improve the accuracy of link quality prediction. To address the limitations of LLMs in multivariate prediction due to typically handling one-dimensional data, we integrate GAT to model interdependencies among multiple variables across different protocol layers, enhancing the model's ability to handle complex dependencies. Experimental results demonstrate that GAT-LLM significantly improves the accuracy and robustness of link quality prediction, particularly in multi-step prediction scenarios.</li>
</ul>

<h3>Title: Cybersecurity and Frequent Cyber Attacks on IoT Devices in Healthcare: Issues and Solutions</h3>
<ul>
<li><strong>Authors: </strong>Zag ElSayed, Ahmed Abdelgawad, Nelly Elsayed</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11250">https://arxiv.org/abs/2501.11250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11250">https://arxiv.org/pdf/2501.11250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11250]] Cybersecurity and Frequent Cyber Attacks on IoT Devices in Healthcare: Issues and Solutions(https://arxiv.org/abs/2501.11250)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Integrating Internet of Things (IoT) devices in healthcare has revolutionized patient care, offering improved monitoring, diagnostics, and treatment. However, the proliferation of these devices has also introduced significant cybersecurity challenges. This paper reviews the current landscape of cybersecurity threats targeting IoT devices in healthcare, discusses the underlying issues contributing to these vulnerabilities, and explores potential solutions. Additionally, this study offers solutions and suggestions for researchers, agencies, and security specialists to overcome these IoT in healthcare cybersecurity vulnerabilities. A comprehensive literature survey highlights the nature and frequency of cyber attacks, their impact on healthcare systems, and emerging strategies to mitigate these risks.</li>
</ul>

<h3>Title: Enhancing Uncertainty Estimation in Semantic Segmentation via Monte-Carlo Frequency Dropout</h3>
<ul>
<li><strong>Authors: </strong>Tal Zeevi, Lawrence H. Staib, John A. Onofrey</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11258">https://arxiv.org/abs/2501.11258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11258">https://arxiv.org/pdf/2501.11258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11258]] Enhancing Uncertainty Estimation in Semantic Segmentation via Monte-Carlo Frequency Dropout(https://arxiv.org/abs/2501.11258)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Monte-Carlo (MC) Dropout provides a practical solution for estimating predictive distributions in deterministic neural networks. Traditional dropout, applied within the signal space, may fail to account for frequency-related noise common in medical imaging, leading to biased predictive estimates. A novel approach extends Dropout to the frequency domain, allowing stochastic attenuation of signal frequencies during inference. This creates diverse global textural variations in feature maps while preserving structural integrity -- a factor we hypothesize and empirically show is contributing to accurately estimating uncertainties in semantic segmentation. We evaluated traditional MC-Dropout and the MC-frequency Dropout in three segmentation tasks involving different imaging modalities: (i) prostate zones in biparametric MRI, (ii) liver tumors in contrast-enhanced CT, and (iii) lungs in chest X-ray scans. Our results show that MC-Frequency Dropout improves calibration, convergence, and semantic uncertainty, thereby improving prediction scrutiny, boundary delineation, and has the potential to enhance medical decision-making.</li>
</ul>

<h3>Title: Towards Loss-Resilient Image Coding for Unstable Satellite Networks</h3>
<ul>
<li><strong>Authors: </strong>Hongwei Sha, Muchen Dong, Quanyou Luo, Ming Lu, Hao Chen, Zhan Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11263">https://arxiv.org/abs/2501.11263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11263">https://arxiv.org/pdf/2501.11263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11263]] Towards Loss-Resilient Image Coding for Unstable Satellite Networks(https://arxiv.org/abs/2501.11263)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Geostationary Earth Orbit (GEO) satellite communication demonstrates significant advantages in emergency short burst data services. However, unstable satellite networks, particularly those with frequent packet loss, present a severe challenge to accurate image transmission. To address it, we propose a loss-resilient image coding approach that leverages end-to-end optimization in learned image compression (LIC). Our method builds on the channel-wise progressive coding framework, incorporating Spatial-Channel Rearrangement (SCR) on the encoder side and Mask Conditional Aggregation (MCA) on the decoder side to improve reconstruction quality with unpredictable errors. By integrating the Gilbert-Elliot model into the training process, we enhance the model's ability to generalize in real-world network conditions. Extensive evaluations show that our approach outperforms traditional and deep learning-based methods in terms of compression performance and stability under diverse packet loss, offering robust and efficient progressive transmission even in challenging environments. Code is available at this https URL.</li>
</ul>

<h3>Title: Sparse L0-norm based Kernel-free Quadratic Surface Support Vector Machines</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Mousavi, Ramin Zandvakili</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11268">https://arxiv.org/abs/2501.11268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11268">https://arxiv.org/pdf/2501.11268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11268]] Sparse L0-norm based Kernel-free Quadratic Surface Support Vector Machines(https://arxiv.org/abs/2501.11268)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Kernel-free quadratic surface support vector machine (SVM) models have gained significant attention in machine learning. However, introducing a quadratic classifier increases the model's complexity by quadratically expanding the number of parameters relative to the dimensionality of the data, exacerbating overfitting. To address this, we propose sparse $\ell_0$-norm based Kernel-free quadratic surface SVMs, designed to mitigate overfitting and enhance interpretability. Given the intractable nature of these models, we present a penalty decomposition algorithm to efficiently obtain first-order optimality points. Our analysis shows that the subproblems in this framework either admit closed-form solutions or can leverage duality theory to improve computational efficiency. Through empirical evaluations on real-world datasets, we demonstrate the efficacy and robustness of our approach, showcasing its potential to advance Kernel-free quadratic surface SVMs in practical applications while addressing overfitting concerns. All the implemented models and experiment codes are available at \url{this https URL}.</li>
</ul>

<h3>Title: Can xLLMs Understand the Structure of Dialog? Exploring Multilingual Response Generation in Complex Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Zhongtian Hu, Yiwen Cui, Ronghan Li, Meng Zhao, Lifang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11269">https://arxiv.org/abs/2501.11269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11269">https://arxiv.org/pdf/2501.11269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11269]] Can xLLMs Understand the Structure of Dialog? Exploring Multilingual Response Generation in Complex Scenarios(https://arxiv.org/abs/2501.11269)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multilingual research has garnered increasing attention, especially in the domain of dialogue systems. The rapid advancements in large language models (LLMs) have fueled the demand for high-performing multilingual models. However, two major challenges persist: the scarcity of high-quality multilingual datasets and the limited complexity of existing datasets in capturing realistic dialogue scenarios. To address these gaps, we introduce XMP, a high-quality parallel Multilingual dataset sourced from Multi-party Podcast dialogues. Each sample in the dataset features at least three participants discussing a wide range of topics, including society, culture, politics, and this http URL extensive experiments, we uncover significant limitations in previously recognized multilingual capabilities of LLMs when applied to such complex dialogue scenarios. For instance, the widely accepted multilingual complementary ability of LLMs is notably impacted. By conducting further experiments, we explore the mechanisms of LLMs in multilingual environments from multiple perspectives, shedding new light on their performance in real-world, diverse conversational contexts.</li>
</ul>

<h3>Title: Spatiotemporal Air Quality Mapping in Urban Areas Using Sparse Sensor Data, Satellite Imagery, Meteorological Factors, and Spatial Features</h3>
<ul>
<li><strong>Authors: </strong>Osama Ahmad, Zubair Khalid, Muhammad Tahir, Momin Uppal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11270">https://arxiv.org/abs/2501.11270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11270">https://arxiv.org/pdf/2501.11270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11270]] Spatiotemporal Air Quality Mapping in Urban Areas Using Sparse Sensor Data, Satellite Imagery, Meteorological Factors, and Spatial Features(https://arxiv.org/abs/2501.11270)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Monitoring air pollution is crucial for protecting human health from exposure to harmful substances. Traditional methods of air quality monitoring, such as ground-based sensors and satellite-based remote sensing, face limitations due to high deployment costs, sparse sensor coverage, and environmental interferences. To address these challenges, this paper proposes a framework for high-resolution spatiotemporal Air Quality Index (AQI) mapping using sparse sensor data, satellite imagery, and various spatiotemporal factors. By leveraging Graph Neural Networks (GNNs), we estimate AQI values at unmonitored locations based on both spatial and temporal dependencies. The framework incorporates a wide range of environmental features, including meteorological data, road networks, points of interest (PoIs), population density, and urban green spaces, which enhance prediction accuracy. We illustrate the use of our approach through a case study in Lahore, Pakistan, where multi-resolution data is used to generate the air quality index map at a fine spatiotemporal scale.</li>
</ul>

<h3>Title: Multi-round, Chain-of-thought Post-editing for Unfaithful Summaries</h3>
<ul>
<li><strong>Authors: </strong>Yi-Hui Lee, Xiangci Li, Jessica Ouyang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11273">https://arxiv.org/abs/2501.11273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11273">https://arxiv.org/pdf/2501.11273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11273]] Multi-round, Chain-of-thought Post-editing for Unfaithful Summaries(https://arxiv.org/abs/2501.11273)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent large language models (LLMs) have demonstrated a remarkable ability to perform natural language understanding and generation tasks. In this work, we investigate the use of LLMs for evaluating faithfulness in news summarization, finding that it achieves a strong correlation with human judgments. We further investigate LLMs' capabilities as a faithfulness post-editor, experimenting with different chain-of-thought prompts for locating and correcting factual inconsistencies between a generated summary and the source news document and are able to achieve a higher editing success rate than was reported in prior work. We perform both automated and human evaluations of the post-edited summaries, finding that prompting LLMs using chain-of-thought reasoning about factual error types is an effective faithfulness post-editing strategy, performing comparably to fine-tuned post-editing models. We also demonstrate that multiple rounds of post-editing, which has not previously been explored, can be used to gradually improve the faithfulness of summaries whose errors cannot be fully corrected in a single round.</li>
</ul>

<h3>Title: PD-SORT: Occlusion-Robust Multi-Object Tracking Using Pseudo-Depth Cues</h3>
<ul>
<li><strong>Authors: </strong>Yanchao Wang, Dawei Zhang, Run Li, Zhonglong Zheng, Minglu Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11288">https://arxiv.org/abs/2501.11288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11288">https://arxiv.org/pdf/2501.11288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11288]] PD-SORT: Occlusion-Robust Multi-Object Tracking Using Pseudo-Depth Cues(https://arxiv.org/abs/2501.11288)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-object tracking (MOT) is a rising topic in video processing technologies and has important application value in consumer electronics. Currently, tracking-by-detection (TBD) is the dominant paradigm for MOT, which performs target detection and association frame by frame. However, the association performance of TBD methods degrades in complex scenes with heavy occlusions, which hinders the application of such methods in real-world this http URL this end, we incorporate pseudo-depth cues to enhance the association performance and propose Pseudo-Depth SORT (PD-SORT). First, we extend the Kalman filter state vector with pseudo-depth states. Second, we introduce a novel depth volume IoU (DVIoU) by combining the conventional 2D IoU with pseudo-depth. Furthermore, we develop a quantized pseudo-depth measurement (QPDM) strategy for more robust data association. Besides, we also integrate camera motion compensation (CMC) to handle dynamic camera situations. With the above designs, PD-SORT significantly alleviates the occlusion-induced ambiguous associations and achieves leading performances on DanceTrack, MOT17, and MOT20. Note that the improvement is especially obvious on DanceTrack, where objects show complex motions, similar appearances, and frequent occlusions. The code is available at this https URL.</li>
</ul>

<h3>Title: MIFNet: Learning Modality-Invariant Features for Generalizable Multimodal Image Matching</h3>
<ul>
<li><strong>Authors: </strong>Yepeng Liu, Zhichao Sun, Baosheng Yu, Yitian Zhao, Bo Du, Yongchao Xu, Jun Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11299">https://arxiv.org/abs/2501.11299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11299">https://arxiv.org/pdf/2501.11299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11299]] MIFNet: Learning Modality-Invariant Features for Generalizable Multimodal Image Matching(https://arxiv.org/abs/2501.11299)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Many keypoint detection and description methods have been proposed for image matching or registration. While these methods demonstrate promising performance for single-modality image matching, they often struggle with multimodal data because the descriptors trained on single-modality data tend to lack robustness against the non-linear variations present in multimodal data. Extending such methods to multimodal image matching often requires well-aligned multimodal data to learn modality-invariant descriptors. However, acquiring such data is often costly and impractical in many real-world scenarios. To address this challenge, we propose a modality-invariant feature learning network (MIFNet) to compute modality-invariant features for keypoint descriptions in multimodal image matching using only single-modality training data. Specifically, we propose a novel latent feature aggregation module and a cumulative hybrid aggregation module to enhance the base keypoint descriptors trained on single-modality data by leveraging pre-trained features from Stable Diffusion models. We validate our method with recent keypoint detection and description methods in three multimodal retinal image datasets (CF-FA, CF-OCT, EMA-OCTA) and two remote sensing datasets (Optical-SAR and Optical-NIR). Extensive experiments demonstrate that the proposed MIFNet is able to learn modality-invariant feature for multimodal image matching without accessing the targeted modality and has good zero-shot generalization ability. The source code will be made publicly available.</li>
</ul>

<h3>Title: Nested Annealed Training Scheme for Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Chang Wan, Ming-Hsuan Yang, Minglu Li, Yunliang Jiang, Zhonglong Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11318">https://arxiv.org/abs/2501.11318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11318">https://arxiv.org/pdf/2501.11318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11318]] Nested Annealed Training Scheme for Generative Adversarial Networks(https://arxiv.org/abs/2501.11318)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recently, researchers have proposed many deep generative models, including generative adversarial networks(GANs) and denoising diffusion models. Although significant breakthroughs have been made and empirical success has been achieved with the GAN, its mathematical underpinnings remain relatively unknown. This paper focuses on a rigorous mathematical theoretical framework: the composite-functional-gradient GAN (CFG)[1]. Specifically, we reveal the theoretical connection between the CFG model and score-based models. We find that the training objective of the CFG discriminator is equivalent to finding an optimal D(x). The optimal gradient of D(x) differentiates the integral of the differences between the score functions of real and synthesized samples. Conversely, training the CFG generator involves finding an optimal G(x) that minimizes this difference. In this paper, we aim to derive an annealed weight preceding the weight of the CFG discriminator. This new explicit theoretical explanation model is called the annealed CFG method. To overcome the limitation of the annealed CFG method, as the method is not readily applicable to the SOTA GAN model, we propose a nested annealed training scheme (NATS). This scheme keeps the annealed weight from the CFG method and can be seamlessly adapted to various GAN models, no matter their structural, loss, or regularization differences. We conduct thorough experimental evaluations on various benchmark datasets for image generation. The results show that our annealed CFG and NATS methods significantly improve the quality and diversity of the synthesized samples. This improvement is clear when comparing the CFG method and the SOTA GAN models.</li>
</ul>

<h3>Title: StyleSSP: Sampling StartPoint Enhancement for Training-free Diffusion-based Method for Style Transfer</h3>
<ul>
<li><strong>Authors: </strong>Ruojun Xu, Weijie Xi, Xiaodi Wang, Yongbo Mao, Zach Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11319">https://arxiv.org/abs/2501.11319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11319">https://arxiv.org/pdf/2501.11319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11319]] StyleSSP: Sampling StartPoint Enhancement for Training-free Diffusion-based Method for Style Transfer(https://arxiv.org/abs/2501.11319)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion</a></li>
<li><strong>Abstract: </strong>Training-free diffusion-based methods have achieved remarkable success in style transfer, eliminating the need for extensive training or fine-tuning. However, due to the lack of targeted training for style information extraction and constraints on the content image layout, training-free methods often suffer from layout changes of original content and content leakage from style images. Through a series of experiments, we discovered that an effective startpoint in the sampling stage significantly enhances the style transfer process. Based on this discovery, we propose StyleSSP, which focuses on obtaining a better startpoint to address layout changes of original content and content leakage from style image. StyleSSP comprises two key components: (1) Frequency Manipulation: To improve content preservation, we reduce the low-frequency components of the DDIM latent, allowing the sampling stage to pay more attention to the layout of content images; and (2) Negative Guidance via Inversion: To mitigate the content leakage from style image, we employ negative guidance in the inversion stage to ensure that the startpoint of the sampling stage is distanced from the content of style image. Experiments show that StyleSSP surpasses previous training-free style transfer baselines, particularly in preserving original content and minimizing the content leakage from style image.</li>
</ul>

<h3>Title: CatV2TON: Taming Diffusion Transformers for Vision-Based Virtual Try-On with Temporal Concatenation</h3>
<ul>
<li><strong>Authors: </strong>Zheng Chong, Wenqing Zhang, Shiyue Zhang, Jun Zheng, Xiao Dong, Haoxiang Li, Yiling Wu, Dongmei Jiang, Xiaodan Liang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11325">https://arxiv.org/abs/2501.11325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11325">https://arxiv.org/pdf/2501.11325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11325]] CatV2TON: Taming Diffusion Transformers for Vision-Based Virtual Try-On with Temporal Concatenation(https://arxiv.org/abs/2501.11325)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Virtual try-on (VTON) technology has gained attention due to its potential to transform online retail by enabling realistic clothing visualization of images and videos. However, most existing methods struggle to achieve high-quality results across image and video try-on tasks, especially in long video scenarios. In this work, we introduce CatV2TON, a simple and effective vision-based virtual try-on (V2TON) method that supports both image and video try-on tasks with a single diffusion transformer model. By temporally concatenating garment and person inputs and training on a mix of image and video datasets, CatV2TON achieves robust try-on performance across static and dynamic settings. For efficient long-video generation, we propose an overlapping clip-based inference strategy that uses sequential frame guidance and Adaptive Clip Normalization (AdaCN) to maintain temporal consistency with reduced resource demands. We also present ViViD-S, a refined video try-on dataset, achieved by filtering back-facing frames and applying 3D mask smoothing for enhanced temporal consistency. Comprehensive experiments demonstrate that CatV2TON outperforms existing methods in both image and video try-on tasks, offering a versatile and reliable solution for realistic virtual try-ons across diverse scenarios.</li>
</ul>

<h3>Title: Few-shot Policy (de)composition in Conversational Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Kyle Erwin, Guy Axelrod, Maria Chang, Achille Fokoue, Maxwell Crouse, Soham Dan, Tian Gao, Rosario Uceda-Sosa, Ndivhuwo Makondo, Naweed Khan, Alexander Gray</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11335">https://arxiv.org/abs/2501.11335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11335">https://arxiv.org/pdf/2501.11335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11335]] Few-shot Policy (de)composition in Conversational Question Answering(https://arxiv.org/abs/2501.11335)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>The task of policy compliance detection (PCD) is to determine if a scenario is in compliance with respect to a set of written policies. In a conversational setting, the results of PCD can indicate if clarifying questions must be asked to determine compliance status. Existing approaches usually claim to have reasoning capabilities that are latent or require a large amount of annotated data. In this work, we propose logical decomposition for policy compliance (LDPC): a neuro-symbolic framework to detect policy compliance using large language models (LLMs) in a few-shot setting. By selecting only a few exemplars alongside recently developed prompting techniques, we demonstrate that our approach soundly reasons about policy compliance conversations by extracting sub-questions to be answered, assigning truth values from contextual information, and explicitly producing a set of logic statements from the given policies. The formulation of explicit logic graphs can in turn help answer PCDrelated questions with increased transparency and explainability. We apply this approach to the popular PCD and conversational machine reading benchmark, ShARC, and show competitive performance with no task-specific finetuning. We also leverage the inherently interpretable architecture of LDPC to understand where errors occur, revealing ambiguities in the ShARC dataset and highlighting the challenges involved with reasoning for conversational question answering.</li>
</ul>

<h3>Title: GenVidBench: A Challenging Benchmark for Detecting AI-Generated Video</h3>
<ul>
<li><strong>Authors: </strong>Zhenliang Ni, Qiangyu Yan, Mouxiao Huang, Tianning Yuan, Yehui Tang, Hailin Hu, Xinghao Chen, Yunhe Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11340">https://arxiv.org/abs/2501.11340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11340">https://arxiv.org/pdf/2501.11340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11340]] GenVidBench: A Challenging Benchmark for Detecting AI-Generated Video(https://arxiv.org/abs/2501.11340)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of video generation models has made it increasingly challenging to distinguish AI-generated videos from real ones. This issue underscores the urgent need for effective AI-generated video detectors to prevent the dissemination of false information through such videos. However, the development of high-performance generative video detectors is currently impeded by the lack of large-scale, high-quality datasets specifically designed for generative video detection. To this end, we introduce GenVidBench, a challenging AI-generated video detection dataset with several key advantages: 1) Cross Source and Cross Generator: The cross-generation source mitigates the interference of video content on the detection. The cross-generator ensures diversity in video attributes between the training and test sets, preventing them from being overly similar. 2) State-of-the-Art Video Generators: The dataset includes videos from 8 state-of-the-art AI video generators, ensuring that it covers the latest advancements in the field of video generation. 3) Rich Semantics: The videos in GenVidBench are analyzed from multiple dimensions and classified into various semantic categories based on their content. This classification ensures that the dataset is not only large but also diverse, aiding in the development of more generalized and effective detection models. We conduct a comprehensive evaluation of different advanced video generators and present a challenging setting. Additionally, we present rich experimental results including advanced video classification models as baselines. With the GenVidBench, researchers can efficiently develop and evaluate AI-generated video detection models. Datasets and code are available at this https URL.</li>
</ul>

<h3>Title: EndoChat: Grounded Multimodal Large Language Model for Endoscopic Surgery</h3>
<ul>
<li><strong>Authors: </strong>Guankun Wang, Long Bai, Junyi Wang, Kun Yuan, Zhen Li, Tianxu Jiang, Xiting He, Jinlin Wu, Zhen Chen, Zhen Lei, Hongbin Liu, Jiazheng Wang, Fan Zhang, Nicolas Padoy, Nassir Navab, Hongliang Ren</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11347">https://arxiv.org/abs/2501.11347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11347">https://arxiv.org/pdf/2501.11347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11347]] EndoChat: Grounded Multimodal Large Language Model for Endoscopic Surgery(https://arxiv.org/abs/2501.11347)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, Multimodal Large Language Models (MLLMs) have demonstrated their immense potential in computer-aided diagnosis and decision-making. In the context of robotic-assisted surgery, MLLMs can serve as effective tools for surgical training and guidance. However, there is still a lack of MLLMs specialized for surgical scene understanding in clinical applications. In this work, we introduce EndoChat to address various dialogue paradigms and subtasks in surgical scene understanding that surgeons encounter. To train our EndoChat, we construct the Surg-396K dataset through a novel pipeline that systematically extracts surgical information and generates structured annotations based on collected large-scale endoscopic surgery datasets. Furthermore, we introduce a multi-scale visual token interaction mechanism and a visual contrast-based reasoning mechanism to enhance the model's representation learning and reasoning capabilities. Our model achieves state-of-the-art performance across five dialogue paradigms and eight surgical scene understanding tasks. Additionally, we conduct evaluations with professional surgeons, most of whom provide positive feedback on collaborating with EndoChat. Overall, these results demonstrate that our EndoChat has great potential to significantly advance training and automation in robotic-assisted surgery.</li>
</ul>

<h3>Title: Adaptive parameters identification for nonlinear dynamics using deep permutation invariant networks</h3>
<ul>
<li><strong>Authors: </strong>Mouad Elaarabi, Domenico Borzacchiello, Yves Le Guennec, Philippe Le Bot, Sebastien Comas-Cardona</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11350">https://arxiv.org/abs/2501.11350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11350">https://arxiv.org/pdf/2501.11350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11350]] Adaptive parameters identification for nonlinear dynamics using deep permutation invariant networks(https://arxiv.org/abs/2501.11350)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>The promising outcomes of dynamical system identification techniques, such as SINDy [Brunton et al. 2016], highlight their advantages in providing qualitative interpretability and extrapolation compared to non-interpretable deep neural networks [Rudin 2019]. These techniques suffer from parameter updating in real-time use cases, especially when the system parameters are likely to change during or between processes. Recently, the OASIS [Bhadriraju et al. 2020] framework introduced a data-driven technique to address the limitations of real-time dynamical system parameters updating, yielding interesting results. Nevertheless, we show in this work that superior performance can be achieved using more advanced model architectures. We present an innovative encoding approach, based mainly on the use of Set Encoding methods of sequence data, which give accurate adaptive model identification for complex dynamic systems, with variable input time series length. Two Set Encoding methods are used, the first is Deep Set [Zaheer et al. 2017], and the second is Set Transformer [Lee et al. 2019]. Comparing Set Transformer to OASIS framework on Lotka Volterra for real-time local dynamical system identification and time series forecasting, we find that the Set Transformer architecture is well adapted to learning relationships within data sets. We then compare the two Set Encoding methods based on the Lorenz system for online global dynamical system identification. Finally, we trained a Deep Set model to perform identification and characterization of abnormalities for 1D heat-transfer problem.</li>
</ul>

<h3>Title: Automatic Labelling & Semantic Segmentation with 4D Radar Tensors</h3>
<ul>
<li><strong>Authors: </strong>Botao Sun, Ignacio Roldan, Francesco Fioranelli</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11351">https://arxiv.org/abs/2501.11351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11351">https://arxiv.org/pdf/2501.11351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11351]] Automatic Labelling & Semantic Segmentation with 4D Radar Tensors(https://arxiv.org/abs/2501.11351)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, an automatic labelling process is presented for automotive datasets, leveraging on complementary information from LiDAR and camera. The generated labels are then used as ground truth with the corresponding 4D radar data as inputs to a proposed semantic segmentation network, to associate a class label to each spatial voxel. Promising results are shown by applying both approaches to the publicly shared RaDelft dataset, with the proposed network achieving over 65% of the LiDAR detection performance, improving 13.2% in vehicle detection probability, and reducing 0.54 m in terms of Chamfer distance, compared to variants inspired from the literature.</li>
</ul>

<h3>Title: Federated Learning with Sample-level Client Drift Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Haoran Xu, Jiaze Li, Wanyi Wu, Hao Ren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11360">https://arxiv.org/abs/2501.11360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11360">https://arxiv.org/pdf/2501.11360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11360]] Federated Learning with Sample-level Client Drift Mitigation(https://arxiv.org/abs/2501.11360)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) suffers from severe performance degradation due to the data heterogeneity among clients. Existing works reveal that the fundamental reason is that data heterogeneity can cause client drift where the local model update deviates from the global one, and thus they usually tackle this problem from the perspective of calibrating the obtained local update. Despite effectiveness, existing methods substantially lack a deep understanding of how heterogeneous data samples contribute to the formation of client drift. In this paper, we bridge this gap by identifying that the drift can be viewed as a cumulative manifestation of biases present in all local samples and the bias between samples is different. Besides, the bias dynamically changes as the FL training progresses. Motivated by this, we propose FedBSS that first mitigates the heterogeneity issue in a sample-level manner, orthogonal to existing methods. Specifically, the core idea of our method is to adopt a bias-aware sample selection scheme that dynamically selects the samples from small biases to large epoch by epoch to train progressively the local model in each round. In order to ensure the stability of training, we set the diversified knowledge acquisition stage as the warm-up stage to avoid the local optimality caused by knowledge deviation in the early stage of the model. Evaluation results show that FedBSS outperforms state-of-the-art baselines. In addition, we also achieved effective results on feature distribution skew and noise label dataset setting, which proves that FedBSS can not only reduce heterogeneity, but also has scalability and robustness.</li>
</ul>

<h3>Title: Block Flow: Learning Straight Flow on Data Blocks</h3>
<ul>
<li><strong>Authors: </strong>Zibin Wang, Zhiyuan Ouyang, Xiangyun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11361">https://arxiv.org/abs/2501.11361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11361">https://arxiv.org/pdf/2501.11361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11361]] Block Flow: Learning Straight Flow on Data Blocks(https://arxiv.org/abs/2501.11361)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Flow-matching models provide a powerful framework for various applications, offering efficient sampling and flexible probability path modeling. These models are characterized by flows with low curvature in learned generative trajectories, which results in reduced truncation error at each sampling step. To further reduce curvature, we propose block matching. This novel approach leverages label information to partition the data distribution into blocks and match them with a prior distribution parameterized using the same label information, thereby learning straighter flows. We demonstrate that the variance of the prior distribution can control the curvature upper bound of forward trajectories in flow-matching models. By designing flexible regularization strategies to adjust this variance, we achieve optimal generation performance, effectively balancing the trade-off between maintaining diversity in generated samples and minimizing numerical solver errors. Our results demonstrate competitive performance with models of the same parameter this http URL is available at \url{this https URL}.</li>
</ul>

<h3>Title: UniTrans: A Unified Vertical Federated Knowledge Transfer Framework for Enhancing Cross-Hospital Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Chung-ju Huang, Yuanpeng He, Xiao Han, Wenpin Jiao, Zhi Jin, Leye Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11388">https://arxiv.org/abs/2501.11388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11388">https://arxiv.org/pdf/2501.11388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11388]] UniTrans: A Unified Vertical Federated Knowledge Transfer Framework for Enhancing Cross-Hospital Collaboration(https://arxiv.org/abs/2501.11388)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Cross-hospital collaboration has the potential to address disparities in medical resources across different regions. However, strict privacy regulations prohibit the direct sharing of sensitive patient information between hospitals. Vertical federated learning (VFL) offers a novel privacy-preserving machine learning paradigm that maximizes data utility across multiple hospitals. Traditional VFL methods, however, primarily benefit patients with overlapping data, leaving vulnerable non-overlapping patients without guaranteed improvements in medical prediction services. While some knowledge transfer techniques can enhance the prediction performance for non-overlapping patients, they fall short in addressing scenarios where overlapping and non-overlapping patients belong to different domains, resulting in challenges such as feature heterogeneity and label heterogeneity. To address these issues, we propose a novel unified vertical federated knowledge transfer framework (Unitrans). Our framework consists of three key steps. First, we extract the federated representation of overlapping patients by employing an effective vertical federated representation learning method to model multi-party joint features online. Next, each hospital learns a local knowledge transfer module offline, enabling the transfer of knowledge from the federated representation of overlapping patients to the enriched representation of local non-overlapping patients in a domain-adaptive manner. Finally, hospitals utilize these enriched local representations to enhance performance across various downstream medical prediction tasks. Experiments on real-world medical datasets validate the framework's dual effectiveness in both intra-domain and cross-domain knowledge transfer. The code of \method is available at \url{this https URL}.</li>
</ul>

<h3>Title: Voltage Profile-Driven Physical Layer Authentication for RIS-aided Backscattering Tag-to-Tag Networks</h3>
<ul>
<li><strong>Authors: </strong>Masoud Kaveh, Farshad Rostami Ghadi, Yifan Zhang, Zheng Yan, Riku Jäntti</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11405">https://arxiv.org/abs/2501.11405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11405">https://arxiv.org/pdf/2501.11405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11405]] Voltage Profile-Driven Physical Layer Authentication for RIS-aided Backscattering Tag-to-Tag Networks(https://arxiv.org/abs/2501.11405)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust</a></li>
<li><strong>Abstract: </strong>Backscattering tag-to-tag networks (BTTNs) are emerging passive radio frequency identification (RFID) systems that facilitate direct communication between tags using an external RF field and play a pivotal role in ubiquitous Internet of Things (IoT) applications. Despite their potential, BTTNs face significant security vulnerabilities, which remain their primary concern to enable reliable communication. Existing authentication schemes in backscatter communication (BC) systems, which mainly focus on tag-to-reader or reader-to-tag scenarios, are unsuitable for BTTNs due to the ultra-low power constraints and limited computational capabilities of the tags, leaving the challenge of secure tag-to-tag authentication largely unexplored. To bridge this gap, this paper proposes a physical layer authentication (PLA) scheme, where a Talker tag (TT) and a Listener tag (LT) can authenticate each other in the presence of an adversary, only leveraging the unique output voltage profile of the energy harvesting and the envelope detector circuits embedded in their power and demodulation units. This allows for efficient authentication of BTTN tags without additional computational overhead. In addition, since the low spectral efficiency and limited coverage range in BTTNs hinder PLA performance, we propose integrating an indoor reconfigurable intelligent surface (RIS) into the system to enhance authentication accuracy and enable successful authentication over longer distances. Security analysis and simulation results indicate that our scheme is robust against various attack vectors and achieves acceptable performance across various experimental settings. Additionally, the results indicate that using RIS significantly enhances PLA performance in terms of accuracy and robustness, especially at longer distances compared to traditional BTTN scenarios without RIS.</li>
</ul>

<h3>Title: Neural Contextual Reinforcement Framework for Logical Structure Language Generation</h3>
<ul>
<li><strong>Authors: </strong>Marcus Irvin, William Cooper, Edward Hughes, Jessica Morgan, Christopher Hamilton</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11417">https://arxiv.org/abs/2501.11417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11417">https://arxiv.org/pdf/2501.11417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11417]] Neural Contextual Reinforcement Framework for Logical Structure Language Generation(https://arxiv.org/abs/2501.11417)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The Neural Contextual Reinforcement Framework introduces an innovative approach to enhancing the logical coherence and structural consistency of text generated by large language models. Leveraging reinforcement learning principles, the framework integrates custom reward functions and dynamic context alignment mechanisms to address challenges inherent in maintaining long-range dependencies across extended sequences. The architecture incorporates multi-head attention layers and hierarchical encoding modules, enabling the model to produce outputs that align closely with human expectations of logical structure and semantic flow. Quantitative evaluations across diverse datasets demonstrate substantial improvements in coherence metrics, perplexity reduction, and semantic alignment, showcasing the framework's ability to outperform baseline models in both general and domain-specific tasks. Qualitative analyses further highlight the framework's capacity to generate text with improved narrative clarity and reduced redundancy, reflecting its effectiveness in balancing fluency with structural precision. In addition to its performance gains, the framework exhibits robustness in handling noisy input data and scalability across varying model sizes, reinforcing its versatility in practical applications. Experimental results reveal that optimal context window sizes significantly influence coherence outcomes, showing the importance of architectural flexibility in adapting to diverse linguistic structures. Cross-lingual performance evaluations affirm the framework's adaptability to multiple languages, extending its utility beyond monolingual contexts. Resource efficiency analyses indicate a reduction in computational overhead compared to traditional approaches, emphasizing the practicality of the framework for large-scale deployment.</li>
</ul>

<h3>Title: Enhancing Coronary Artery Calcium Scoring via Multi-Organ Segmentation on Non-Contrast Cardiac Computed Tomography</h3>
<ul>
<li><strong>Authors: </strong>Jakub Nalepa, Tomasz Bartczak, Mariusz Bujny, Jarosław Gośliński, Katarzyna Jesionek, Wojciech Malara, Filip Malawski, Karol Miszalski-Jamka, Patrycja Rewa, Marcin Kostur</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11428">https://arxiv.org/abs/2501.11428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11428">https://arxiv.org/pdf/2501.11428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11428]] Enhancing Coronary Artery Calcium Scoring via Multi-Organ Segmentation on Non-Contrast Cardiac Computed Tomography(https://arxiv.org/abs/2501.11428)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>Despite coronary artery calcium scoring being considered a largely solved problem within the realm of medical artificial intelligence, this paper argues that significant improvements can still be made. By shifting the focus from pathology detection to a deeper understanding of anatomy, the novel algorithm proposed in the paper both achieves high accuracy in coronary artery calcium scoring and offers enhanced interpretability of the results. This approach not only aids in the precise quantification of calcifications in coronary arteries, but also provides valuable insights into the underlying anatomical structures. Through this anatomically-informed methodology, the paper shows how a nuanced understanding of the heart's anatomy can lead to more accurate and interpretable results in the field of cardiovascular health. We demonstrate the superior accuracy of the proposed method by evaluating it on an open-source multi-vendor dataset, where we obtain results at the inter-observer level, surpassing the current state of the art. Finally, the qualitative analyses show the practical value of the algorithm in such tasks as labeling coronary artery calcifications, identifying aortic calcifications, and filtering out false positive detections due to noise.</li>
</ul>

<h3>Title: A Survey on Diffusion Models for Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Jing Liu, Zhenchao Ma, Zepu Wang, Yang Liu, Zehua Wang, Peng Sun, Liang Song, Bo Hu, Azzedine Boukerche, Victor C.M. Leung</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11430">https://arxiv.org/abs/2501.11430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11430">https://arxiv.org/pdf/2501.11430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11430]] A Survey on Diffusion Models for Anomaly Detection(https://arxiv.org/abs/2501.11430)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, interpretability, diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) have emerged as a powerful class of generative AI models, showing remarkable potential in anomaly detection (AD) tasks across various domains, such as cybersecurity, fraud detection, healthcare, and manufacturing. The intersection of these two fields, termed diffusion models for anomaly detection (DMAD), offers promising solutions for identifying deviations in increasingly complex and high-dimensional data. In this survey, we systematically review recent advances in DMAD research and investigate their capabilities. We begin by presenting the fundamental concepts of AD and DMs, followed by a comprehensive analysis of classic DM architectures including DDPMs, DDIMs, and Score SDEs. We further categorize existing DMAD methods into reconstruction-based, density-based, and hybrid approaches, providing detailed examinations of their methodological innovations. We also explore the diverse tasks across different data modalities, encompassing image, time series, video, and multimodal data analysis. Furthermore, we discuss critical challenges and emerging research directions, including computational efficiency, model interpretability, robustness enhancement, edge-cloud collaboration, and integration with large language models. The collection of DMAD research papers and resources is available at this https URL.</li>
</ul>

<h3>Title: RACCOON: A Retrieval-Augmented Generation Approach for Location Coordinate Capture from News Articles</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Lin, Aditya Joshi, Hye-young Paik, Tri Dung Doung, Deepti Gurdasani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11440">https://arxiv.org/abs/2501.11440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11440">https://arxiv.org/pdf/2501.11440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11440]] RACCOON: A Retrieval-Augmented Generation Approach for Location Coordinate Capture from News Articles(https://arxiv.org/abs/2501.11440)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Geocoding involves automatic extraction of location coordinates of incidents reported in news articles, and can be used for epidemic intelligence or disaster management. This paper introduces Retrieval-Augmented Coordinate Capture Of Online News articles (RACCOON), an open-source geocoding approach that extracts geolocations from news articles. RACCOON uses a retrieval-augmented generation (RAG) approach where candidate locations and associated information are retrieved in the form of context from a location database, and a prompt containing the retrieved context, location mentions and news articles is fed to an LLM to generate the location coordinates. Our evaluation on three datasets, two underlying LLMs, three baselines and several ablation tests based on the components of RACCOON demonstrate the utility of RACCOON. To the best of our knowledge, RACCOON is the first RAG-based approach for geocoding using pre-trained LLMs.</li>
</ul>

<h3>Title: On the Adversarial Vulnerabilities of Transfer Learning in Remote Sensing</h3>
<ul>
<li><strong>Authors: </strong>Tao Bai, Xingjian Tian, Yonghao Xu, Bihan Wen</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11462">https://arxiv.org/abs/2501.11462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11462">https://arxiv.org/pdf/2501.11462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11462]] On the Adversarial Vulnerabilities of Transfer Learning in Remote Sensing(https://arxiv.org/abs/2501.11462)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>The use of pretrained models from general computer vision tasks is widespread in remote sensing, significantly reducing training costs and improving performance. However, this practice also introduces vulnerabilities to downstream tasks, where publicly available pretrained models can be used as a proxy to compromise downstream models. This paper presents a novel Adversarial Neuron Manipulation method, which generates transferable perturbations by selectively manipulating single or multiple neurons in pretrained models. Unlike existing attacks, this method eliminates the need for domain-specific information, making it more broadly applicable and efficient. By targeting multiple fragile neurons, the perturbations achieve superior attack performance, revealing critical vulnerabilities in deep learning models. Experiments on diverse models and remote sensing datasets validate the effectiveness of the proposed method. This low-access adversarial neuron manipulation technique highlights a significant security risk in transfer learning models, emphasizing the urgent need for more robust defenses in their design when addressing the safety-critical remote sensing tasks.</li>
</ul>

<h3>Title: Curiosity-Driven Reinforcement Learning from Human Feedback</h3>
<ul>
<li><strong>Authors: </strong>Haoran Sun, Yekun Chai, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11463">https://arxiv.org/abs/2501.11463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11463">https://arxiv.org/pdf/2501.11463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11463]] Curiosity-Driven Reinforcement Learning from Human Feedback(https://arxiv.org/abs/2501.11463)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but often at the cost of reduced output diversity. This trade-off between diversity and alignment quality remains a significant challenge. Drawing inspiration from curiosity-driven exploration in reinforcement learning, we introduce curiosity-driven RLHF (CD-RLHF), a framework that incorporates intrinsic rewards for novel states, alongside traditional sparse extrinsic rewards, to optimize both output diversity and alignment quality. We demonstrate the effectiveness of CD-RLHF through extensive experiments on a range of tasks, including text summarization and instruction following. Our approach achieves significant gains in diversity on multiple diversity-oriented metrics while maintaining alignment with human preferences comparable to standard RLHF. We make our code publicly available at this https URL.</li>
</ul>

<h3>Title: Graph-defined Language Learning with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Huachi Zhou, Jiahe Du, Chuang Zhou, Chang Yang, Yilin Xiao, Yuxuan Xie, Xiao Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11478">https://arxiv.org/abs/2501.11478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11478">https://arxiv.org/pdf/2501.11478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11478]] Graph-defined Language Learning with LLMs(https://arxiv.org/abs/2501.11478)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent efforts leverage Large Language Models (LLMs) for modeling text-attributed graph structures in node classification tasks. These approaches describe graph structures for LLMs to understand or aggregate LLM-generated textual attribute embeddings through graph structure. However, these approaches face two main limitations in modeling graph structures with LLMs. (i) Graph descriptions become verbose in describing high-order graph structure. (ii) Textual attributes alone do not contain adequate graph structure information. It is challenging to model graph structure concisely and adequately with LLMs. LLMs lack built-in mechanisms to model graph structures directly. They also struggle with complex long-range dependencies between high-order nodes and target nodes. Inspired by the observation that LLMs pre-trained on one language can achieve exceptional performance on another with minimal additional training, we propose \textbf{G}raph-\textbf{D}efined \textbf{L}anguage for \textbf{L}arge \textbf{L}anguage \textbf{M}odel (GDL4LLM). This novel framework enables LLMs to transfer their powerful language understanding capabilities to graph-structured data. GDL4LLM translates graphs into a graph language corpus instead of graph descriptions and pre-trains LLMs on this corpus to adequately understand graph structures. During fine-tuning, this corpus describes the structural information of target nodes concisely with only a few tokens. By treating graphs as a new language, GDL4LLM enables LLMs to model graph structures adequately and concisely for node classification tasks. Extensive experiments on three real-world datasets demonstrate that GDL4LLM outperforms description-based and textual attribute embeddings-based baselines by efficiently modeling different orders of graph structure with LLMs.</li>
</ul>

<h3>Title: SimLabel: Consistency-Guided OOD Detection with Pretrained Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shu Zou, Xinyu Tian, Qinyu Zhao, Zhaoyuan Yang, Jing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11485">https://arxiv.org/abs/2501.11485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11485">https://arxiv.org/pdf/2501.11485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11485]] SimLabel: Consistency-Guided OOD Detection with Pretrained Vision-Language Models(https://arxiv.org/abs/2501.11485)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Detecting out-of-distribution (OOD) data is crucial in real-world machine learning applications, particularly in safety-critical domains. Existing methods often leverage language information from vision-language models (VLMs) to enhance OOD detection by improving confidence estimation through rich class-wise text information. However, when building OOD detection score upon on in-distribution (ID) text-image affinity, existing works either focus on each ID class or whole ID label sets, overlooking inherent ID classes' connection. We find that the semantic information across different ID classes is beneficial for effective OOD detection. We thus investigate the ability of image-text comprehension among different semantic-related ID labels in VLMs and propose a novel post-hoc strategy called SimLabel. SimLabel enhances the separability between ID and OOD samples by establishing a more robust image-class similarity metric that considers consistency over a set of similar class labels. Extensive experiments demonstrate the superior performance of SimLabel on various zero-shot OOD detection benchmarks. The proposed model is also extended to various VLM-backbones, demonstrating its good generalization ability. Our demonstration and implementation codes are available at: this https URL.</li>
</ul>

<h3>Title: Communication-Efficient Federated Learning Based on Explanation-Guided Pruning for Remote Sensing Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Jonas Klotz, Barış Büyüktaş, Begüm Demir</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11493">https://arxiv.org/abs/2501.11493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11493">https://arxiv.org/pdf/2501.11493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11493]] Communication-Efficient Federated Learning Based on Explanation-Guided Pruning for Remote Sensing Image Classification(https://arxiv.org/abs/2501.11493)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a decentralized machine learning paradigm, where multiple clients collaboratively train a global model by exchanging only model updates with the central server without sharing the local data of clients. Due to the large volume of model updates required to be transmitted between clients and the central server, most FL systems are associated with high transfer costs (i.e., communication overhead). This issue is more critical for operational applications in remote sensing (RS), especially when large-scale RS data is processed and analyzed through FL systems with restricted communication bandwidth. To address this issue, we introduce an explanation-guided pruning strategy for communication-efficient FL in the context of RS image classification. Our pruning strategy is defined based on the layerwise relevance propagation (LRP) driven explanations to: 1) efficiently and effectively identify the most relevant and informative model parameters (to be exchanged between clients and the central server); and 2) eliminate the non-informative ones to minimize the volume of model updates. The experimental results on the BigEarthNet-S2 dataset demonstrate that our strategy effectively reduces the number of shared model updates, while increasing the generalization ability of the global model. The code of this work will be publicly available at this https URL</li>
</ul>

<h3>Title: Generative AI and Large Language Models in Language Preservation: Opportunities and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Vincent Koc</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11496">https://arxiv.org/abs/2501.11496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11496">https://arxiv.org/pdf/2501.11496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11496]] Generative AI and Large Language Models in Language Preservation: Opportunities and Challenges(https://arxiv.org/abs/2501.11496)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative AI and large-scale language models (LLM) have emerged as powerful tools in language preservation, particularly for near-native and endangered languages. With the increasing reliance on technology for communication, education, and cultural documentation, new opportunities have emerged to mitigate the dramatic decline of linguistic diversity worldwide. This paper examines the role of generative AIs and LLMs in preserving endangered languages, highlighting the risks and challenges associated with their use. We analyze the underlying technologies driving these models, including natural language processing (NLP) and deep learning, and explore several cases where these technologies have been applied to low-resource languages. Additionally, we discuss ethical considerations, data scarcity issues, and technical challenges while proposing solutions to enhance AI-driven language preservation.</li>
</ul>

<h3>Title: UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Chen, Yujin Wang, Xin Cai, Zhiyuan You, Zheming Lu, Fan Zhang, Shi Guo, Tianfan Xue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11515">https://arxiv.org/abs/2501.11515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11515">https://arxiv.org/pdf/2501.11515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11515]] UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion(https://arxiv.org/abs/2501.11515)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>Capturing high dynamic range (HDR) scenes is one of the most important issues in camera design. Majority of cameras use exposure fusion technique, which fuses images captured by different exposure levels, to increase dynamic range. However, this approach can only handle images with limited exposure difference, normally 3-4 stops. When applying to very high dynamic scenes where a large exposure difference is required, this approach often fails due to incorrect alignment or inconsistent lighting between inputs, or tone mapping artifacts. In this work, we propose UltraFusion, the first exposure fusion technique that can merge input with 9 stops differences. The key idea is that we model the exposure fusion as a guided inpainting problem, where the under-exposed image is used as a guidance to fill the missing information of over-exposed highlight in the over-exposed region. Using under-exposed image as a soft guidance, instead of a hard constrain, our model is robust to potential alignment issue or lighting variations. Moreover, utilizing the image prior of the generative model, our model also generates natural tone mapping, even for very high-dynamic range scene. Our approach outperforms HDR-Transformer on latest HDR benchmarks. Moreover, to test its performance in ultra high dynamic range scene, we capture a new real-world exposure fusion benchmark, UltraFusion Dataset, with exposure difference up to 9 stops, and experiments show that \model~can generate beautiful and high-quality fusion results under various scenarios. An online demo is provided at this https URL.</li>
</ul>

<h3>Title: Technical Report for the Forgotten-by-Design Project: Targeted Obfuscation for Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Rickard Brännvall, Laurynas Adomaitis, Olof Görnerup, Anass Sedrati</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11525">https://arxiv.org/abs/2501.11525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11525">https://arxiv.org/pdf/2501.11525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11525]] Technical Report for the Forgotten-by-Design Project: Targeted Obfuscation for Machine Learning(https://arxiv.org/abs/2501.11525)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, robust, membership infer</a></li>
<li><strong>Abstract: </strong>The right to privacy, enshrined in various human rights declarations, faces new challenges in the age of artificial intelligence (AI). This paper explores the concept of the Right to be Forgotten (RTBF) within AI systems, contrasting it with traditional data erasure methods. We introduce Forgotten by Design, a proactive approach to privacy preservation that integrates instance-specific obfuscation techniques during the AI model training process. Unlike machine unlearning, which modifies models post-training, our method prevents sensitive data from being embedded in the first place. Using the LIRA membership inference attack, we identify vulnerable data points and propose defenses that combine additive gradient noise and weighting schemes. Our experiments on the CIFAR-10 dataset demonstrate that our techniques reduce privacy risks by at least an order of magnitude while maintaining model accuracy (at 95% significance). Additionally, we present visualization methods for the privacy-utility trade-off, providing a clear framework for balancing privacy risk and model accuracy. This work contributes to the development of privacy-preserving AI systems that align with human cognitive processes of motivated forgetting, offering a robust framework for safeguarding sensitive information and ensuring compliance with privacy regulations.</li>
</ul>

<h3>Title: DenoMAE: A Multimodal Autoencoder for Denoising Modulation Signals</h3>
<ul>
<li><strong>Authors: </strong>Atik Faysal, Taha Boushine, Mohammad Rostami, Reihaneh Gh. Roshan, Huaxia Wang, Nikhil Muralidhar, Avimanyu Sahoo, Yu-Dong Yao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11538">https://arxiv.org/abs/2501.11538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11538">https://arxiv.org/pdf/2501.11538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11538]] DenoMAE: A Multimodal Autoencoder for Denoising Modulation Signals(https://arxiv.org/abs/2501.11538)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose Denoising Masked Autoencoder (Deno-MAE), a novel multimodal autoencoder framework for denoising modulation signals during pretraining. DenoMAE extends the concept of masked autoencoders by incorporating multiple input modalities, including noise as an explicit modality, to enhance cross-modal learning and improve denoising performance. The network is pre-trained using unlabeled noisy modulation signals and constellation diagrams, effectively learning to reconstruct their equivalent noiseless signals and diagrams. Deno-MAE achieves state-of-the-art accuracy in automatic modulation classification tasks with significantly fewer training samples, demonstrating a 10% reduction in unlabeled pretraining data and a 3% reduction in labeled fine-tuning data compared to existing approaches. Moreover, our model exhibits robust performance across varying signal-to-noise ratios (SNRs) and supports extrapolation on unseen lower SNRs. The results indicate that DenoMAE is an efficient, flexible, and data-efficient solution for denoising and classifying modulation signals in challenging noise-intensive environments.</li>
</ul>

<h3>Title: PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinyu Wang, Jingjing Fu, Lei Song, Jiang Bian</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11551">https://arxiv.org/abs/2501.11551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11551">https://arxiv.org/pdf/2501.11551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11551]] PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation(https://arxiv.org/abs/2501.11551)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Despite notable advancements in Retrieval-Augmented Generation (RAG) systems that expand large language model (LLM) capabilities through external retrieval, these systems often struggle to meet the complex and diverse needs of real-world industrial applications. The reliance on retrieval alone proves insufficient for extracting deep, domain-specific knowledge performing in logical reasoning from specialized corpora. To address this, we introduce sPecIalized KnowledgE and Rationale Augmentation Generation (PIKE-RAG), focusing on extracting, understanding, and applying specialized knowledge, while constructing coherent rationale to incrementally steer LLMs toward accurate responses. Recognizing the diverse challenges of industrial tasks, we introduce a new paradigm that classifies tasks based on their complexity in knowledge extraction and application, allowing for a systematic evaluation of RAG systems' problem-solving capabilities. This strategic approach offers a roadmap for the phased development and enhancement of RAG systems, tailored to meet the evolving demands of industrial applications. Furthermore, we propose knowledge atomizing and knowledge-aware task decomposition to effectively extract multifaceted knowledge from the data chunks and iteratively construct the rationale based on original query and the accumulated knowledge, respectively, showcasing exceptional performance across various benchmarks.</li>
</ul>

<h3>Title: Secure Resource Allocation via Constrained Deep Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Jianfei Sun, Qiang Gao, Cong Wu, Yuxian Li, Jiacheng Wang, Dusit Niyato</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11557">https://arxiv.org/abs/2501.11557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11557">https://arxiv.org/pdf/2501.11557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11557]] Secure Resource Allocation via Constrained Deep Reinforcement Learning(https://arxiv.org/abs/2501.11557)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust</a></li>
<li><strong>Abstract: </strong>The proliferation of Internet of Things (IoT) devices and the advent of 6G technologies have introduced computationally intensive tasks that often surpass the processing capabilities of user devices. Efficient and secure resource allocation in serverless multi-cloud edge computing environments is essential for supporting these demands and advancing distributed computing. However, existing solutions frequently struggle with the complexity of multi-cloud infrastructures, robust security integration, and effective application of traditional deep reinforcement learning (DRL) techniques under system constraints. To address these challenges, we present SARMTO, a novel framework that integrates an action-constrained DRL model. SARMTO dynamically balances resource allocation, task offloading, security, and performance by utilizing a Markov decision process formulation, an adaptive security mechanism, and sophisticated optimization techniques. Extensive simulations across varying scenarios, including different task loads, data sizes, and MEC capacities, show that SARMTO consistently outperforms five baseline approaches, achieving up to a 40% reduction in system costs and a 41.5% improvement in energy efficiency over state-of-the-art methods. These enhancements highlight SARMTO's potential to revolutionize resource management in intricate distributed computing environments, opening the door to more efficient and secure IoT and edge computing applications.</li>
</ul>

<h3>Title: A performance analysis of VM-based Trusted Execution Environments for Confidential Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Bruno Casella</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11558">https://arxiv.org/abs/2501.11558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11558">https://arxiv.org/pdf/2501.11558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11558]] A performance analysis of VM-based Trusted Execution Environments for Confidential Federated Learning(https://arxiv.org/abs/2501.11558)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, attack, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a distributed machine learning approach that has emerged as an effective way to address recent privacy concerns. However, FL introduces the need for additional security measures as FL alone is still subject to vulnerabilities such as model and data poisoning and inference attacks. Confidential Computing (CC) is a paradigm that, by leveraging hardware-based trusted execution environments (TEEs), protects the confidentiality and integrity of ML models and data, thus resulting in a powerful ally of FL applications. Typical TEEs offer an application-isolation level but suffer many drawbacks, such as limited available memory and debugging and coding difficulties. The new generation of TEEs offers a virtual machine (VM)-based isolation level, thus reducing the porting effort for existing applications. In this work, we compare the performance of VM-based and application-isolation level TEEs for confidential FL (CFL) applications. In particular, we evaluate the impact of TEEs and additional security mechanisms such as TLS (for securing the communication channel). The results, obtained across three datasets and two deep learning models, demonstrate that the new VM-based TEEs introduce a limited overhead (at most 1.5x), thus paving the way to leverage public and untrusted computing environments, such as HPC facilities or public cloud, without detriment to performance.</li>
</ul>

<h3>Title: Explainable Lane Change Prediction for Near-Crash Scenarios Using Knowledge Graph Embeddings and Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>M. Manzour, A. Ballardini, R. Izquierdo, M. Á. Sotelo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11560">https://arxiv.org/abs/2501.11560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11560">https://arxiv.org/pdf/2501.11560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11560]] Explainable Lane Change Prediction for Near-Crash Scenarios Using Knowledge Graph Embeddings and Retrieval Augmented Generation(https://arxiv.org/abs/2501.11560)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Lane-changing maneuvers, particularly those executed abruptly or in risky situations, are a significant cause of road traffic accidents. However, current research mainly focuses on predicting safe lane changes. Furthermore, existing accident datasets are often based on images only and lack comprehensive sensory data. In this work, we focus on predicting risky lane changes using the CRASH dataset (our own collected dataset specifically for risky lane changes), and safe lane changes (using the HighD dataset). Then, we leverage KG and Bayesian inference to predict these maneuvers using linguistic contextual information, enhancing the model's interpretability and transparency. The model achieved a 91.5% f1-score with anticipation time extending to four seconds for risky lane changes, and a 90.0% f1-score for predicting safe lane changes with the same anticipation time. We validate our model by integrating it into a vehicle within the CARLA simulator in scenarios that involve risky lane changes. The model managed to anticipate sudden lane changes, thus providing automated vehicles with further time to plan and execute appropriate safe reactions. Finally, to enhance the explainability of our model, we utilize RAG to provide clear and natural language explanations for the given prediction.</li>
</ul>

<h3>Title: Teaching Large Language Models to Regress Accurate Image Quality Scores using Score Distribution</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan You, Xin Cai, Jinjin Gu, Tianfan Xue, Chao Dong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11561">https://arxiv.org/abs/2501.11561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11561">https://arxiv.org/pdf/2501.11561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11561]] Teaching Large Language Models to Regress Accurate Image Quality Scores using Score Distribution(https://arxiv.org/abs/2501.11561)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of Multi-modal Large Language Models (MLLMs), MLLM-based Image Quality Assessment (IQA) methods have shown promising performance in linguistic quality description. However, current methods still fall short in accurately scoring image quality. In this work, we aim to leverage MLLMs to regress accurate quality scores. A key challenge is that the quality score is inherently continuous, typically modeled as a Gaussian distribution, whereas MLLMs generate discrete token outputs. This mismatch necessitates score discretization. Previous approaches discretize the mean score into a one-hot label, resulting in information loss and failing to capture inter-image relationships. We propose a distribution-based approach that discretizes the score distribution into a soft label. This method preserves the characteristics of the score distribution, achieving high accuracy and maintaining inter-image relationships. Moreover, to address dataset variation, where different IQA datasets exhibit various distributions, we introduce a fidelity loss based on Thurstone's model. This loss captures intra-dataset relationships, facilitating co-training across multiple IQA datasets. With these designs, we develop the distribution-based Depicted image Quality Assessment model for Score regression (DeQA-Score). Experiments across multiple benchmarks show that DeQA-Score stably outperforms baselines in score regression. Also, DeQA-Score can predict the score distribution that closely aligns with human annotations. Codes and model weights have been released in this https URL.</li>
</ul>

<h3>Title: Graph Defense Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Xin He, Wenqi Fan, Yili Wang, Chengyi Liu, Rui Miao, Xin Juan, Xin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11568">https://arxiv.org/abs/2501.11568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11568">https://arxiv.org/pdf/2501.11568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11568]] Graph Defense Diffusion Model(https://arxiv.org/abs/2501.11568)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) demonstrate significant potential in various applications but remain highly vulnerable to adversarial attacks, which can greatly degrade their performance. Existing graph purification methods attempt to address this issue by filtering attacked graphs; however, they struggle to effectively defend against multiple types of adversarial attacks simultaneously due to their limited flexibility, and they lack comprehensive modeling of graph data due to their heavy reliance on heuristic prior knowledge. To overcome these challenges, we propose a more versatile approach for defending against adversarial attacks on graphs. In this work, we introduce the Graph Defense Diffusion Model (GDDM), a flexible purification method that leverages the denoising and modeling capabilities of diffusion models. The iterative nature of diffusion models aligns well with the stepwise process of adversarial attacks, making them particularly suitable for defense. By iteratively adding and removing noise, GDDM effectively purifies attacked graphs, restoring their original structure and features. Our GDDM consists of two key components: (1) Graph Structure-Driven Refiner, which preserves the basic fidelity of the graph during the denoising process, and ensures that the generated graph remains consistent with the original scope; and (2) Node Feature-Constrained Regularizer, which removes residual impurities from the denoised graph, further enhances the purification effect. Additionally, we design tailored denoising strategies to handle different types of adversarial attacks, improving the model's adaptability to various attack scenarios. Extensive experiments conducted on three real-world datasets demonstrate that GDDM outperforms state-of-the-art methods in defending against a wide range of adversarial attacks, showcasing its robustness and effectiveness.</li>
</ul>

<h3>Title: Rethinking Membership Inference Attacks Against Transfer Learning</h3>
<ul>
<li><strong>Authors: </strong>Cong Wu, Jing Chen, Qianru Fang, Kun He, Ziming Zhao, Hao Ren, Guowen Xu, Yang Liu, Yang Xiang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11577">https://arxiv.org/abs/2501.11577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11577">https://arxiv.org/pdf/2501.11577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11577]] Rethinking Membership Inference Attacks Against Transfer Learning(https://arxiv.org/abs/2501.11577)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Transfer learning, successful in knowledge translation across related tasks, faces a substantial privacy threat from membership inference attacks (MIAs). These attacks, despite posing significant risk to ML model's training data, remain limited-explored in transfer learning. The interaction between teacher and student models in transfer learning has not been thoroughly explored in MIAs, potentially resulting in an under-examined aspect of privacy vulnerabilities within transfer learning. In this paper, we propose a new MIA vector against transfer learning, to determine whether a specific data point was used to train the teacher model while only accessing the student model in a white-box setting. Our method delves into the intricate relationship between teacher and student models, analyzing the discrepancies in hidden layer representations between the student model and its shadow counterpart. These identified differences are then adeptly utilized to refine the shadow model's training process and to inform membership inference decisions effectively. Our method, evaluated across four datasets in diverse transfer learning tasks, reveals that even when an attacker only has access to the student model, the teacher model's training data remains susceptible to MIAs. We believe our work unveils the unexplored risk of membership inference in transfer learning.</li>
</ul>

<h3>Title: GCSAM: Gradient Centralized Sharpness Aware Minimization</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Hassan, Aleksandar Vakanski, Boyu Zhang, Min Xian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11584">https://arxiv.org/abs/2501.11584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11584">https://arxiv.org/pdf/2501.11584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11584]] GCSAM: Gradient Centralized Sharpness Aware Minimization(https://arxiv.org/abs/2501.11584)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The generalization performance of deep neural networks (DNNs) is a critical factor in achieving robust model behavior on unseen data. Recent studies have highlighted the importance of sharpness-based measures in promoting generalization by encouraging convergence to flatter minima. Among these approaches, Sharpness-Aware Minimization (SAM) has emerged as an effective optimization technique for reducing the sharpness of the loss landscape, thereby improving generalization. However, SAM's computational overhead and sensitivity to noisy gradients limit its scalability and efficiency. To address these challenges, we propose Gradient-Centralized Sharpness-Aware Minimization (GCSAM), which incorporates Gradient Centralization (GC) to stabilize gradients and accelerate convergence. GCSAM normalizes gradients before the ascent step, reducing noise and variance, and improving stability during training. Our evaluations indicate that GCSAM consistently outperforms SAM and the Adam optimizer in terms of generalization and computational efficiency. These findings demonstrate GCSAM's effectiveness across diverse domains, including general and medical imaging tasks.</li>
</ul>

<h3>Title: Recurrent Diffusion for Large-Scale Parameter Generation</h3>
<ul>
<li><strong>Authors: </strong>Kai Wang, Dongwen Tang, Wangbo Zhao, Yang You</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11587">https://arxiv.org/abs/2501.11587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11587">https://arxiv.org/pdf/2501.11587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11587]] Recurrent Diffusion for Large-Scale Parameter Generation(https://arxiv.org/abs/2501.11587)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Parameter generation has struggled to scale up for a long time, significantly limiting its range of applications. In this study, we introduce \textbf{R}ecurrent diffusion for large-scale \textbf{P}arameter \textbf{G}eneration, called \textbf{RPG}. We first divide the trained parameters into non-overlapping parts, after which a recurrent model is proposed to learn their relationships. The recurrent model's outputs, as conditions, are then fed into a diffusion model to generate the neural network parameters. Using only a single GPU, recurrent diffusion enables us to generate popular vision and language models such as ConvNeXt-L and LoRA parameters of LLaMA-7B. Meanwhile, across various architectures and tasks, the generated parameters consistently perform comparable results over trained networks. Notably, our approach also shows the potential to generate models for handling unseen tasks, which largely increases the practicality of parameter generation. Our code is available \href{this https URL}{here}.</li>
</ul>

<h3>Title: Training-free Ultra Small Model for Universal Sparse Reconstruction in Compressed Sensing</h3>
<ul>
<li><strong>Authors: </strong>Chaoqing Tang, Huanze Zhuang, Guiyun Tian, Zhenli Zeng, Yi Ding, Wenzhong Liu, Xiang Bai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11592">https://arxiv.org/abs/2501.11592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11592">https://arxiv.org/pdf/2501.11592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11592]] Training-free Ultra Small Model for Universal Sparse Reconstruction in Compressed Sensing(https://arxiv.org/abs/2501.11592)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Pre-trained large models attract widespread attention in recent years, but they face challenges in applications that require high interpretability or have limited resources, such as physical sensing, medical imaging, and bioinformatics. Compressed Sensing (CS) is a well-proved theory that drives many recent breakthroughs in these applications. However, as a typical under-determined linear system, CS suffers from excessively long sparse reconstruction times when using traditional iterative methods, particularly with large-scale data. Current AI methods like deep unfolding fail to substitute them because pre-trained models exhibit poor generality beyond their training conditions and dataset distributions, or lack interpretability. Instead of following the big model fervor, this paper proposes ultra-small artificial neural models called coefficients learning (CL), enabling training-free and rapid sparse reconstruction while perfectly inheriting the generality and interpretability of traditional iterative methods, bringing new feature of incorporating prior knowledges. In CL, a signal of length $n$ only needs a minimal of $n$ trainable parameters. A case study model called CLOMP is implemented for evaluation. Experiments are conducted on both synthetic and real one-dimensional and two-dimensional signals, demonstrating significant improvements in efficiency and accuracy. Compared to representative iterative methods, CLOMP improves efficiency by 100 to 1000 folds for large-scale data. Test results on eight diverse image datasets indicate that CLOMP improves structural similarity index by 292%, 98%, 45% for sampling rates of 0.1, 0.3, 0.5, respectively. We believe this method can truly usher CS reconstruction into the AI era, benefiting countless under-determined linear systems that rely on sparse solution.</li>
</ul>

<h3>Title: Conversation Routines: A Prompt Engineering Framework for Task-Oriented Dialog Systems</h3>
<ul>
<li><strong>Authors: </strong>Giorgio Robino</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.ET, cs.HC, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11613">https://arxiv.org/abs/2501.11613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11613">https://arxiv.org/pdf/2501.11613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11613]] Conversation Routines: A Prompt Engineering Framework for Task-Oriented Dialog Systems(https://arxiv.org/abs/2501.11613)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This study introduces Conversation Routines (CR), a structured prompt engineering framework for developing task-oriented dialog systems using Large Language Models (LLMs). While LLMs demonstrate remarkable natural language understanding capabilities, engineering them to reliably execute complex business workflows remains challenging. The proposed CR framework enables the development of Conversation Agentic Systems (CAS) through natural language specifications, embedding task-oriented logic within LLM prompts. This approach provides a systematic methodology for designing and implementing complex conversational workflows while maintaining behavioral consistency. We demonstrate the framework's effectiveness through two proof of concept implementations: a Train Ticket Booking System and an Interactive Troubleshooting Copilot. These case studies validate CR's capability to encode sophisticated behavioral patterns and decision logic while preserving natural conversational flexibility. Results show that CR enables domain experts to design conversational workflows in natural language while leveraging custom enterprise functionalities (tools) developed by software engineers, creating an efficient division of responsibilities where developers focus on core API implementation and domain experts handle conversation design. While the framework shows promise in accessibility and adaptability, we identify key challenges including computational overhead, non-deterministic behavior, and domain-specific logic optimization. Future research directions include enhancing system robustness, improving scalability for complex multi-agent interactions, and addressing the identified limitations across diverse business applications.</li>
</ul>

<h3>Title: Enhancing IoT Network Security through Adaptive Curriculum Learning and XAI</h3>
<ul>
<li><strong>Authors: </strong>Sathwik Narkedimilli, Sujith Makam, Amballa Venkata Sriram, Sai Prashanth Mallellu, MSVPJ Sathvik, Ranga Rao Venkatesha Prasad</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11618">https://arxiv.org/abs/2501.11618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11618">https://arxiv.org/pdf/2501.11618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11618]] Enhancing IoT Network Security through Adaptive Curriculum Learning and XAI(https://arxiv.org/abs/2501.11618)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust</a></li>
<li><strong>Abstract: </strong>To address the critical need for secure IoT networks, this study presents a scalable and lightweight curriculum learning framework enhanced with Explainable AI (XAI) techniques, including LIME, to ensure transparency and adaptability. The proposed model employs novel neural network architecture utilized at every stage of Curriculum Learning to efficiently capture and focus on both short- and long-term temporal dependencies, improve learning stability, and enhance accuracy while remaining lightweight and robust against noise in sequential IoT data. Robustness is achieved through staged learning, where the model iteratively refines itself by removing low-relevance features and optimizing performance. The workflow includes edge-optimized quantization and pruning to ensure portability that could easily be deployed in the edge-IoT devices. An ensemble model incorporating Random Forest, XGBoost, and the staged learning base further enhances generalization. Experimental results demonstrate 98% accuracy on CIC-IoV-2024 and CIC-APT-IIoT-2024 datasets and 97% on EDGE-IIoT, establishing this framework as a robust, transparent, and high-performance solution for IoT network security.</li>
</ul>

<h3>Title: Trojan Detection Through Pattern Recognition for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Vedant Bhasin, Matthew Yudin, Razvan Stefanescu, Rauf Izmailov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11621">https://arxiv.org/abs/2501.11621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11621">https://arxiv.org/pdf/2501.11621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11621]] Trojan Detection Through Pattern Recognition for Large Language Models(https://arxiv.org/abs/2501.11621)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Trojan backdoors can be injected into large language models at various stages, including pretraining, fine-tuning, and in-context learning, posing a significant threat to the model's alignment. Due to the nature of causal language modeling, detecting these triggers is challenging given the vast search space. In this study, we propose a multistage framework for detecting Trojan triggers in large language models consisting of token filtration, trigger identification, and trigger verification. We discuss existing trigger identification methods and propose two variants of a black-box trigger inversion method that rely on output logits, utilizing beam search and greedy decoding respectively. We show that the verification stage is critical in the process and propose semantic-preserving prompts and special perturbations to differentiate between actual Trojan triggers and other adversarial strings that display similar characteristics. The evaluation of our approach on the TrojAI and RLHF poisoned model datasets demonstrates promising results.</li>
</ul>

<h3>Title: Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, Yuxiao Dong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11651">https://arxiv.org/abs/2501.11651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11651">https://arxiv.org/pdf/2501.11651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11651]] Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling(https://arxiv.org/abs/2501.11651)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks. However, existing approaches mainly rely on imitation learning and struggle to achieve effective test-time scaling. While reinforcement learning (RL) holds promise for enabling self-exploration and learning from feedback, recent attempts yield only modest improvements in complex reasoning. In this paper, we present T1 to scale RL by encouraging exploration and understand inference scaling. We first initialize the LLM using synthesized chain-of-thought data that integrates trial-and-error and self-verification. To scale RL training, we promote increased sampling diversity through oversampling. We further employ an entropy bonus as an auxiliary loss, alongside a dynamic anchor for regularization to facilitate reward optimization. We demonstrate that T1 with open LLMs as its base exhibits inference scaling behavior and achieves superior performance on challenging math reasoning benchmarks. For example, T1 with Qwen2.5-32B as the base model outperforms the recent Qwen QwQ-32B-Preview model on MATH500, AIME2024, and Omni-math-500. More importantly, we present a simple strategy to examine inference scaling, where increased inference budgets directly lead to T1's better performance without any additional verification. We will open-source the T1 models and the data used to train them at \url{this https URL}.</li>
</ul>

<h3>Title: BlindFL: Segmented Federated Learning with Fully Homomorphic Encryption</h3>
<ul>
<li><strong>Authors: </strong>Evan Gronberg, Liv d'Aliberti, Magnus Saebo, Aurora Hook</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11659">https://arxiv.org/abs/2501.11659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11659">https://arxiv.org/pdf/2501.11659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11659]] BlindFL: Segmented Federated Learning with Fully Homomorphic Encryption(https://arxiv.org/abs/2501.11659)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, attack, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a popular privacy-preserving edge-to-cloud technique used for training and deploying artificial intelligence (AI) models on edge devices. FL aims to secure local client data while also collaboratively training a global model. Under standard FL, clients within the federation send model updates, derived from local data, to a central server for aggregation into a global model. However, extensive research has demonstrated that private data can be reliably reconstructed from these model updates using gradient inversion attacks (GIAs). To protect client data from server-side GIAs, previous FL schemes have employed fully homomorphic encryption (FHE) to secure model updates while still enabling popular aggregation methods. However, current FHE-based FL schemes either incur substantial computational overhead or trade security and/or model accuracy for efficiency. We introduce BlindFL, a framework for global model aggregation in which clients encrypt and send a subset of their local model update. With choice over the subset size, BlindFL offers flexible efficiency gains while preserving full encryption of aggregated updates. Moreover, we demonstrate that implementing BlindFL can substantially lower space and time transmission costs per client, compared with plain FL with FHE, while maintaining global model accuracy. BlindFL also offers additional depth of security. While current single-key, FHE-based FL schemes explicitly defend against server-side adversaries, they do not address the realistic threat of malicious clients within the federation. By contrast, we theoretically and experimentally demonstrate that BlindFL significantly impedes client-side model poisoning attacks, a first for single-key, FHE-based FL schemes.</li>
</ul>

<h3>Title: Towards Improving IDS Using CTF Events</h3>
<ul>
<li><strong>Authors: </strong>Manuel Kern, Florian Skopik, Max Landauer, Edgar Weippl</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11685">https://arxiv.org/abs/2501.11685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11685">https://arxiv.org/pdf/2501.11685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11685]] Towards Improving IDS Using CTF Events(https://arxiv.org/abs/2501.11685)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>In cybersecurity, Intrusion Detection Systems (IDS) serve as a vital defensive layer against adversarial threats. Accurate benchmarking is critical to evaluate and improve IDS effectiveness, yet traditional methodologies face limitations due to their reliance on previously known attack signatures and lack of creativity of automated tests. This paper introduces a novel approach to evaluating IDS through Capture the Flag (CTF) events, specifically designed to uncover weaknesses within IDS. CTFs, known for engaging a diverse community in tackling complex security challenges, offer a dynamic platform for this purpose. Our research investigates the effectiveness of using tailored CTF challenges to identify weaknesses in IDS by integrating them into live CTF competitions. This approach leverages the creativity and technical skills of the CTF community, enhancing both the benchmarking process and the participants' practical security skills. We present a methodology that supports the development of IDS-specific challenges, a scoring system that fosters learning and engagement, and the insights of running such a challenge in a real Jeopardy-style CTF event. Our findings highlight the potential of CTFs as a tool for IDS evaluation, demonstrating the ability to effectively expose vulnerabilities while also providing insights into necessary improvements for future implementations.</li>
</ul>

<h3>Title: Trustformer: A Trusted Federated Transformer</h3>
<ul>
<li><strong>Authors: </strong>Ali Abbasi Tadi, Dima Alhadidi, Luis Rueda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11706">https://arxiv.org/abs/2501.11706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11706">https://arxiv.org/pdf/2501.11706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11706]] Trustformer: A Trusted Federated Transformer(https://arxiv.org/abs/2501.11706)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, federate, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformers, a cornerstone of deep-learning architectures for sequential data, have achieved state-of-the-art results in tasks like Natural Language Processing (NLP). Models such as BERT and GPT-3 exemplify their success and have driven the rise of large language models (LLMs). However, a critical challenge persists: safeguarding the privacy of data used in LLM training. Privacy-preserving techniques like Federated Learning (FL) offer potential solutions, but practical limitations hinder their effectiveness for Transformer training. Two primary issues are (I) the risk of sensitive information leakage due to aggregation methods like FedAvg or FedSGD, and (II) the high communication overhead caused by the large size of Transformer models. This paper introduces a novel FL method that reduces communication overhead while maintaining competitive utility. Our approach avoids sharing full model weights by simulating a global model locally. We apply k-means clustering to each Transformer layer, compute centroids locally, and transmit only these centroids to the server instead of full weights or gradients. To enhance security, we leverage Intel SGX for secure transmission of centroids. Evaluated on a translation task, our method achieves utility comparable to state-of-the-art baselines while significantly reducing communication costs. This provides a more efficient and privacy-preserving FL solution for Transformer models.</li>
</ul>

<h3>Title: Key Concepts and Principles of Blockchain Technology</h3>
<ul>
<li><strong>Authors: </strong>Mohsen Ghorbian, Mostafa Ghobaei-Arani</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11707">https://arxiv.org/abs/2501.11707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11707">https://arxiv.org/pdf/2501.11707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11707]] Key Concepts and Principles of Blockchain Technology(https://arxiv.org/abs/2501.11707)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>In recent years, blockchain technology has been recognized as a transformative innovation in the tech world, and it has quickly become the core infrastructure of digital currencies such as Bitcoin and an important tool in various industries. This technology facilitates the recording and tracking of transactions across a vast network of computers by providing a distributed and decentralized ledger. Blockchain's decentralized structure significantly enhances security and transparency and prevents a single entity from dominating the network. This chapter examines blockchain's advantages, disadvantages, and applications in various industries and analyzes the implementation environments and reasons for using this technology. Also, this chapter discusses challenges such as scalability and high energy consumption that inhibit the expansion of this technology and examines blockchain technology's role in increasing efficiency and security in economic and social interactions. Finally, a comprehensive conclusion of blockchain applications and challenges has been presented by comparing blockchain applications in various industries and analyzing future trends.</li>
</ul>

<h3>Title: Leveraging graph neural networks and mobility data for COVID-19 forecasting</h3>
<ul>
<li><strong>Authors: </strong>Fernando H. O. Duarte, Gladston J. P. Moreira, Eduardo J. S. Luz, Leonardo B. L. Santos, Vander L. S. Freitas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11711">https://arxiv.org/abs/2501.11711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11711">https://arxiv.org/pdf/2501.11711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11711]] Leveraging graph neural networks and mobility data for COVID-19 forecasting(https://arxiv.org/abs/2501.11711)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The COVID-19 pandemic has victimized over 7 million people to date, prompting diverse research efforts. Spatio-temporal models combining mobility data with machine learning have gained attention for disease forecasting. Here, we explore Graph Convolutional Recurrent Network (GCRN) and Graph Convolutional Long Short-Term Memory (GCLSTM), which combine the power of Graph Neural Networks (GNN) with traditional architectures that deal with sequential data. The aim is to forecast future values of COVID-19 cases in Brazil and China by leveraging human mobility networks, whose nodes represent geographical locations and links are flows of vehicles or people. We show that employing backbone extraction to filter out negligible connections in the mobility network enhances predictive stability. Comparing regression and classification tasks demonstrates that binary classification yields smoother, more interpretable results. Interestingly, we observe qualitatively equivalent results for both Brazil and China datasets by introducing sliding windows of variable size and prediction horizons. Compared to prior studies, introducing the sliding window and the network backbone extraction strategies yields improvements of about 80% in root mean squared errors.</li>
</ul>

<h3>Title: YouLeQD: Decoding the Cognitive Complexity of Questions and Engagement in Online Educational Videos from Learners' Perspectives</h3>
<ul>
<li><strong>Authors: </strong>Nong Ming, Sachin Sharma, Jiho Noh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11712">https://arxiv.org/abs/2501.11712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11712">https://arxiv.org/pdf/2501.11712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11712]] YouLeQD: Decoding the Cognitive Complexity of Questions and Engagement in Online Educational Videos from Learners' Perspectives(https://arxiv.org/abs/2501.11712)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Questioning is a fundamental aspect of education, as it helps assess students' understanding, promotes critical thinking, and encourages active engagement. With the rise of artificial intelligence in education, there is a growing interest in developing intelligent systems that can automatically generate and answer questions and facilitate interactions in both virtual and in-person education settings. However, to develop effective AI models for education, it is essential to have a fundamental understanding of questioning. In this study, we created the YouTube Learners' Questions on Bloom's Taxonomy Dataset (YouLeQD), which contains learner-posed questions from YouTube lecture video comments. Along with the dataset, we developed two RoBERTa-based classification models leveraging Large Language Models to detect questions and analyze their cognitive complexity using Bloom's Taxonomy. This dataset and our findings provide valuable insights into the cognitive complexity of learner-posed questions in educational videos and their relationship with interaction metrics. This can aid in the development of more effective AI models for education and improve the overall learning experience for students.</li>
</ul>

<h3>Title: GL-ICNN: An End-To-End Interpretable Convolutional Neural Network for the Diagnosis and Prediction of Alzheimer's Disease</h3>
<ul>
<li><strong>Authors: </strong>Wenjie Kang, Lize Jiskoot, Peter De Deyn, Geert Biessels, Huiberdina Koek, Jurgen Claassen, Huub Middelkoop, Wiesje Flier, Willemijn J. Jansen, Stefan Klein, Esther Bron</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11715">https://arxiv.org/abs/2501.11715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11715">https://arxiv.org/pdf/2501.11715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11715]] GL-ICNN: An End-To-End Interpretable Convolutional Neural Network for the Diagnosis and Prediction of Alzheimer's Disease(https://arxiv.org/abs/2501.11715)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Deep learning methods based on Convolutional Neural Networks (CNNs) have shown great potential to improve early and accurate diagnosis of Alzheimer's disease (AD) dementia based on imaging data. However, these methods have yet to be widely adopted in clinical practice, possibly due to the limited interpretability of deep learning models. The Explainable Boosting Machine (EBM) is a glass-box model but cannot learn features directly from input imaging data. In this study, we propose a novel interpretable model that combines CNNs and EBMs for the diagnosis and prediction of AD. We develop an innovative training strategy that alternatingly trains the CNN component as a feature extractor and the EBM component as the output block to form an end-to-end model. The model takes imaging data as input and provides both predictions and interpretable feature importance measures. We validated the proposed model on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset and the Health-RI Parelsnoer Neurodegenerative Diseases Biobank (PND) as an external testing set. The proposed model achieved an area-under-the-curve (AUC) of 0.956 for AD and control classification, and 0.694 for the prediction of conversion of mild cognitive impairment (MCI) to AD on the ADNI cohort. The proposed model is a glass-box model that achieves a comparable performance with other state-of-the-art black-box models. Our code is publicly available at: this https URL.</li>
</ul>

<h3>Title: Explain-Query-Test: Self-Evaluating LLMs Via Explanation and Comprehension Discrepancy</h3>
<ul>
<li><strong>Authors: </strong>Saeid Asgari Taghanaki, Joao Monteiro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11721">https://arxiv.org/abs/2501.11721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11721">https://arxiv.org/pdf/2501.11721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11721]] Explain-Query-Test: Self-Evaluating LLMs Via Explanation and Comprehension Discrepancy(https://arxiv.org/abs/2501.11721)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable proficiency in generating detailed and coherent explanations of complex concepts. However, the extent to which these models truly comprehend the concepts they articulate remains unclear. To assess the level of comprehension of a model relative to the content it generates, we implemented a self-evaluation pipeline where models: (i) given a topic generate an excerpt with information about the topic, (ii) given an excerpt generate question-answer pairs, and finally (iii) given a question generate an answer. We refer to this self-evaluation approach as Explain-Query-Test (EQT). Interestingly, the accuracy on generated questions resulting from running the EQT pipeline correlates strongly with the model performance as verified by typical benchmarks such as MMLU-Pro. In other words, EQT's performance is predictive of MMLU-Pro's, and EQT can be used to rank models without the need for any external source of evaluation data other than lists of topics of interest. Moreover, our results reveal a disparity between the models' ability to produce detailed explanations and their performance on questions related to those explanations. This gap highlights fundamental limitations in the internal knowledge representation and reasoning abilities of current LLMs. We release the code at this https URL.</li>
</ul>

<h3>Title: SeRpEnt: Selective Resampling for Expressive State Space Models</h3>
<ul>
<li><strong>Authors: </strong>Stefano Rando, Luca Romani, Matteo Migliarini, Luca Franco, Denis Gudovskiy, Fabio Galasso</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11729">https://arxiv.org/abs/2501.11729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11729">https://arxiv.org/pdf/2501.11729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11729]] SeRpEnt: Selective Resampling for Expressive State Space Models(https://arxiv.org/abs/2501.11729)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>State Space Models (SSMs) have recently enjoyed a rise to prominence in the field of deep learning for sequence modeling, especially as an alternative to Transformers. Their success stems from avoiding two well-known drawbacks of attention-based models: quadratic complexity with respect to the sequence length and inability to model long-range dependencies. The SSM variant Mamba has demonstrated performance comparable to Transformers without any form of attention, thanks to the use of a selective mechanism for the state parameters. Selectivity, however, is only evaluated empirically and the reasons of its effectiveness remain unclear. In this work, we show how selectivity is related to the sequence processing. Our analysis shows that selective time intervals in Mamba act as linear approximators of information. Then, we propose our SeRpEnt architecture, a SSM that further exploits selectivity to compress sequences in an information-aware fashion. It employs a resampling mechanism that aggregates elements based on their information content. Our empirical results in the Long Range Arena benchmark and other language modeling tasks show benefits of the SeRpEnt's resampling mechanism.</li>
</ul>

<h3>Title: Transformer Vibration Forecasting for Advancing Rail Safety and Maintenance 4.0</h3>
<ul>
<li><strong>Authors: </strong>Darío C. Larese, Almudena Bravo Cerrada, Gabriel Dambrosio Tomei, Alejandro Guerrero-López, Pablo M. Olmos, María Jesús Gómez García</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11730">https://arxiv.org/abs/2501.11730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11730">https://arxiv.org/pdf/2501.11730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11730]] Transformer Vibration Forecasting for Advancing Rail Safety and Maintenance 4.0(https://arxiv.org/abs/2501.11730)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Maintaining railway axles is critical to preventing severe accidents and financial losses. The railway industry is increasingly interested in advanced condition monitoring techniques to enhance safety and efficiency, moving beyond traditional periodic inspections toward Maintenance 4.0. This study introduces a robust Deep Autoregressive solution that integrates seamlessly with existing systems to avert mechanical failures. Our approach simulates and predicts vibration signals under various conditions and fault scenarios, improving dataset robustness for more effective detection systems. These systems can alert maintenance needs, preventing accidents preemptively. We use experimental vibration signals from accelerometers on train axles. Our primary contributions include a transformer model, ShaftFormer, designed for processing time series data, and an alternative model incorporating spectral methods and enhanced observation models. Simulating vibration signals under diverse conditions mitigates the high cost of obtaining experimental signals for all scenarios. Given the non-stationary nature of railway vibration signals, influenced by speed and load changes, our models address these complexities, offering a powerful tool for predictive maintenance in the rail industry.</li>
</ul>

<h3>Title: FaceSORT: a Multi-Face Tracking Method based on Biometric and Appearance Features</h3>
<ul>
<li><strong>Authors: </strong>Robert Jöchl, Andreas Uhl</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11741">https://arxiv.org/abs/2501.11741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11741">https://arxiv.org/pdf/2501.11741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11741]] FaceSORT: a Multi-Face Tracking Method based on Biometric and Appearance Features(https://arxiv.org/abs/2501.11741)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>Tracking multiple faces is a difficult problem, as there may be partially occluded or lateral faces. In multiple face tracking, association is typically based on (biometric) face features. However, the models used to extract these face features usually require frontal face images, which can limit the tracking performance. In this work, a multi-face tracking method inspired by StrongSort, FaceSORT, is proposed. To mitigate the problem of partially occluded or lateral faces, biometric face features are combined with visual appearance features (i.e., generated by a generic object classifier), with both features are extracted from the same face patch. A comprehensive experimental evaluation is performed, including a comparison of different face descriptors, an evaluation of different parameter settings, and the application of a different similarity metric. All experiments are conducted with a new multi-face tracking dataset and a subset of the ChokePoint dataset. The `Paris Lodron University Salzburg Faces in a Queue' dataset consists of a total of seven fully annotated sequences (12730 frames) and is made publicly available as part of this work. Together with this dataset, annotations of 6 sequences from the ChokePoint dataset are also provided.</li>
</ul>

<h3>Title: SILO: Solving Inverse Problems with Latent Operators</h3>
<ul>
<li><strong>Authors: </strong>Ron Raphaeli, Sean Man, Michael Elad</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11746">https://arxiv.org/abs/2501.11746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11746">https://arxiv.org/pdf/2501.11746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11746]] SILO: Solving Inverse Problems with Latent Operators(https://arxiv.org/abs/2501.11746)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Consistent improvement of image priors over the years has led to the development of better inverse problem solvers. Diffusion models are the newcomers to this arena, posing the strongest known prior to date. Recently, such models operating in a latent space have become increasingly predominant due to their efficiency. In recent works, these models have been applied to solve inverse problems. Working in the latent space typically requires multiple applications of an Autoencoder during the restoration process, which leads to both computational and restoration quality challenges. In this work, we propose a new approach for handling inverse problems with latent diffusion models, where a learned degradation function operates within the latent space, emulating a known image space degradation. Usage of the learned operator reduces the dependency on the Autoencoder to only the initial and final steps of the restoration process, facilitating faster sampling and superior restoration quality. We demonstrate the effectiveness of our method on a variety of image restoration tasks and datasets, achieving significant improvements over prior art.</li>
</ul>

<h3>Title: Optimizing Pretraining Data Mixtures with LLM-Estimated Utility</h3>
<ul>
<li><strong>Authors: </strong>William Held, Bhargavi Paranjape, Punit Singh Koura, Mike Lewis, Frank Zhang, Todor Mihaylov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11747">https://arxiv.org/abs/2501.11747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11747">https://arxiv.org/pdf/2501.11747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11747]] Optimizing Pretraining Data Mixtures with LLM-Estimated Utility(https://arxiv.org/abs/2501.11747)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models improve with increasing amounts of high-quality training data. However, leveraging larger datasets requires balancing quality, quantity, and diversity across sources. After evaluating nine baseline methods under both compute- and data-constrained scenarios, we find token-count heuristics outperform manual and learned mixes, indicating that simple approaches accounting for dataset size and diversity are surprisingly effective. Building on this insight, we propose two complementary approaches: UtiliMax, which extends token-based heuristics by incorporating utility estimates from reduced-scale ablations, achieving up to a 10.6x speedup over manual baselines; and Model Estimated Data Utility (MEDU), which leverages LLMs to estimate data utility from small samples, matching ablation-based performance while reducing computational requirements by $\sim$200x. Together, these approaches establish a new framework for automated, compute-efficient data mixing that is robust across training regimes.</li>
</ul>

<h3>Title: Are generative models fair? A study of racial bias in dermatological image generation</h3>
<ul>
<li><strong>Authors: </strong>Miguel López-Pérez, Søren Hauberg, Aasa Feragen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11752">https://arxiv.org/abs/2501.11752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11752">https://arxiv.org/pdf/2501.11752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11752]] Are generative models fair? A study of racial bias in dermatological image generation(https://arxiv.org/abs/2501.11752)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative</a></li>
<li><strong>Abstract: </strong>Racial bias in medicine, particularly in dermatology, presents significant ethical and clinical challenges. It often results from the underrepresentation of darker skin tones in training datasets for machine learning models. While efforts to address bias in dermatology have focused on improving dataset diversity and mitigating disparities in discriminative models, the impact of racial bias on generative models remains underexplored. Generative models, such as Variational Autoencoders (VAEs), are increasingly used in healthcare applications, yet their fairness across diverse skin tones is currently not well understood. In this study, we evaluate the fairness of generative models in clinical dermatology with respect to racial bias. For this purpose, we first train a VAE with a perceptual loss to generate and reconstruct high-quality skin images across different skin tones. We utilize the Fitzpatrick17k dataset to examine how racial bias influences the representation and performance of these models. Our findings indicate that the VAE is influenced by the diversity of skin tones in the training dataset, with better performance observed for lighter skin tones. Additionally, the uncertainty estimates produced by the VAE are ineffective in assessing the model's fairness. These results highlight the need for improved uncertainty quantification mechanisms to detect and address racial bias in generative models for trustworthy healthcare technologies.</li>
</ul>

<h3>Title: A Review Paper of the Effects of Distinct Modalities and ML Techniques to Distracted Driving Detection</h3>
<ul>
<li><strong>Authors: </strong>Anthony. Dontoh, Stephanie. Ivey, Logan. Sirbaugh, Armstrong. Aboah</a></li>
<li><strong>Subjects: </strong>cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11758">https://arxiv.org/abs/2501.11758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11758">https://arxiv.org/pdf/2501.11758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11758]] A Review Paper of the Effects of Distinct Modalities and ML Techniques to Distracted Driving Detection(https://arxiv.org/abs/2501.11758)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Distracted driving remains a significant global challenge with severe human and economic repercussions, demanding improved detection and intervention strategies. While previous studies have extensively explored single-modality approaches, recent research indicates that these systems often fall short in identifying complex distraction patterns, particularly cognitive distractions. This systematic review addresses critical gaps by providing a comprehensive analysis of machine learning (ML) and deep learning (DL) techniques applied across various data modalities - visual,, sensory, auditory, and multimodal. By categorizing and evaluating studies based on modality, data accessibility, and methodology, this review clarifies which approaches yield the highest accuracy and are best suited for specific distracted driving detection goals. The findings offer clear guidance on the advantages of multimodal versus single-modal systems and capture the latest advancements in the field. Ultimately, this review contributes valuable insights for developing robust distracted driving detection frameworks, supporting enhanced road safety and mitigation strategies.</li>
</ul>

<h3>Title: Is logical analysis performed by transformers taking place in self-attention or in the fully connected part?</h3>
<ul>
<li><strong>Authors: </strong>Evgeniy Shin, Heinrich Matzinger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11765">https://arxiv.org/abs/2501.11765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11765">https://arxiv.org/pdf/2501.11765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11765]] Is logical analysis performed by transformers taking place in self-attention or in the fully connected part?(https://arxiv.org/abs/2501.11765)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers architecture apply self-attention to tokens represented as vectors, before a fully connected (neuronal network) layer. These two parts can be layered many times. Traditionally, self-attention is seen as a mechanism for aggregating information before logical operations are performed by the fully connected layer. In this paper, we show, that quite counter-intuitively, the logical analysis can also be performed within the self-attention. For this we implement a handcrafted single-level encoder layer which performs the logical analysis within self-attention. We then study the scenario in which a one-level transformer model undergoes self-learning using gradient descent. We investigate whether the model utilizes fully connected layers or self-attention mechanisms for logical analysis when it has the choice. Given that gradient descent can become stuck at undesired zeros, we explicitly calculate these unwanted zeros and find ways to avoid them. We do all this in the context of predicting grammatical category pairs of adjacent tokens in a text. We believe that our findings have broader implications for understanding the potential logical operations performed by self-attention.</li>
</ul>

<h3>Title: The Value of Nothing: Multimodal Extraction of Human Values Expressed by TikTok Influencers</h3>
<ul>
<li><strong>Authors: </strong>Alina Starovolsky-Shitrit, Alon Neduva, Naama Appel Doron, Ella Daniel, Oren Tsur</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11770">https://arxiv.org/abs/2501.11770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11770">https://arxiv.org/pdf/2501.11770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11770]] The Value of Nothing: Multimodal Extraction of Human Values Expressed by TikTok Influencers(https://arxiv.org/abs/2501.11770)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Societal and personal values are transmitted to younger generations through interaction and exposure. Traditionally, children and adolescents learned values from parents, educators, or peers. Nowadays, social platforms serve as a significant channel through which youth (and adults) consume information, as the main medium of entertainment, and possibly the medium through which they learn different values. In this paper we extract implicit values from TikTok movies uploaded by online influencers targeting children and adolescents. We curated a dataset of hundreds of TikTok movies and annotated them according to the Schwartz Theory of Personal Values. We then experimented with an array of Masked and Large language model, exploring how values can be detected. Specifically, we considered two pipelines -- direct extraction of values from video and a 2-step approach in which videos are first converted to elaborated scripts and then values are extracted. Achieving state-of-the-art results, we find that the 2-step approach performs significantly better than the direct approach and that using a trainable Masked Language Model as a second step significantly outperforms a few-shot application of a number of Large Language Models. We further discuss the impact of fine-tuning and compare the performance of the different models on identification of values present or contradicted in the TikTok. Finally, we share the first values-annotated dataset of TikTok videos. Our results pave the way to further research on influence and value transmission in video-based social platforms.</li>
</ul>

<h3>Title: Characterization of GPU TEE Overheads in Distributed Data Parallel ML Training</h3>
<ul>
<li><strong>Authors: </strong>Jonghytun Lee, Yongqin Wang, Rachit Rajat, Murali Annavaram</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11771">https://arxiv.org/abs/2501.11771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11771">https://arxiv.org/pdf/2501.11771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11771]] Characterization of GPU TEE Overheads in Distributed Data Parallel ML Training(https://arxiv.org/abs/2501.11771)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>Confidential computing (CC) or trusted execution enclaves (TEEs) is now the most common approach to enable secure computing in the cloud. The recent introduction of GPU TEEs by NVIDIA enables machine learning (ML) models to be trained without leaking model weights or data to the cloud provider. However, the potential performance implications of using GPU TEEs for ML training are not well characterized. In this work, we present an in-depth characterization study on performance overhead associated with running distributed data parallel (DDP) ML training with GPU Trusted Execution Environments (TEE). Our study reveals the performance challenges in DDP training within GPU TEEs. DDP uses ring-all-reduce, a well-known approach, to aggregate gradients from multiple devices. Ring all-reduce consists of multiple scatter-reduce and all-gather operations. In GPU TEEs only the GPU package (GPU and HBM memory) is trusted. Hence, any data communicated outside the GPU packages must be encrypted and authenticated for confidentiality and integrity verification. Hence, each phase of the ring-all-reduce requires encryption and message authentication code (MAC) generation from the sender, and decryption and MAC authentication on the receiver. As the number of GPUs participating in DDP increases, the overhead of secure inter-GPU communication during ring-all-reduce grows proportionally. Additionally, larger models lead to more asynchronous all-reduce operations, exacerbating the communication cost. Our results show that with four GPU TEEs, depending on the model that is being trained, the runtime per training iteration increases by an average of 8x and up to a maximum of 41.6x compared to DDP training without TEE.</li>
</ul>

<h3>Title: EfficientVITON: An Efficient Virtual Try-On Model using Optimized Diffusion Process</h3>
<ul>
<li><strong>Authors: </strong>Mostafa Atef, Mariam Ayman, Ahmed Rashed, Ashrakat Saeed, Abdelrahman Saeed, Ahmed Fares</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11776">https://arxiv.org/abs/2501.11776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11776">https://arxiv.org/pdf/2501.11776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11776]] EfficientVITON: An Efficient Virtual Try-On Model using Optimized Diffusion Process(https://arxiv.org/abs/2501.11776)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Would not it be much more convenient for everybody to try on clothes by only looking into a mirror ? The answer to that problem is virtual try-on, enabling users to digitally experiment with outfits. The core challenge lies in realistic image-to-image translation, where clothing must fit diverse human forms, poses, and figures. Early methods, which used 2D transformations, offered speed, but image quality was often disappointing and lacked the nuance of deep learning. Though GAN-based techniques enhanced realism, their dependence on paired data proved limiting. More adaptable methods offered great visuals but demanded significant computing power and time. Recent advances in diffusion models have shown promise for high-fidelity translation, yet the current crop of virtual try-on tools still struggle with detail loss and warping issues. To tackle these challenges, this paper proposes EfficientVITON, a new virtual try-on system leveraging the impressive pre-trained Stable Diffusion model for better images and deployment feasibility. The system includes a spatial encoder to maintain clothings finer details and zero cross-attention blocks to capture the subtleties of how clothes fit a human body. Input images are carefully prepared, and the diffusion process has been tweaked to significantly cut generation time without image quality loss. The training process involves two distinct stages of fine-tuning, carefully incorporating a balance of loss functions to ensure both accurate try-on results and high-quality visuals. Rigorous testing on the VITON-HD dataset, supplemented with real-world examples, has demonstrated that EfficientVITON achieves state-of-the-art results.</li>
</ul>

<h3>Title: Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Pouya Hamadanian, Sadjad Fouladi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11779">https://arxiv.org/abs/2501.11779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11779">https://arxiv.org/pdf/2501.11779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11779]] Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference(https://arxiv.org/abs/2501.11779)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLM) have revolutionized natural language processing, but their inference demands substantial resources, while under-utilizing high-end accelerators like GPUs. A major bottleneck arises from the attention mechanism, which requires storing large key-value caches, limiting the maximum achievable throughput way below the available computing resources. Current approaches attempt to mitigate this issue through memory-efficient attention and paging mechanisms, but remained constrained by the assumption that all operations must be performed on high-end accelerators. In this work, we propose Glinthawk, a two-tiered architecture that decouples the attention mechanism from the rest of the Transformer model. This approach allows the memory requirements for attention to scale independently, enabling larger batch sizes and more efficient use of the high-end accelerators. We prototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the other. Compared to a traditional single-tier setup, it improves throughput by $5.9\times$ and reduces cost of generation by $2.8\times$. For longer sequence lengths, it achieves $16.3\times$ throughput improvement at $2.4\times$ less cost. Our evaluation shows that this architecture can tolerate moderate network latency with minimal performance degradation, making it highly effective for latency-tolerant, throughput-oriented applications such as batch processing. We shared our prototype publicly at \url{this https URL}.</li>
</ul>

<h3>Title: Synthetic Data Can Mislead Evaluations: Membership Inference as Machine Text Detection</h3>
<ul>
<li><strong>Authors: </strong>Ali Naseh, Niloofar Mireshghallah</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11786">https://arxiv.org/abs/2501.11786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11786">https://arxiv.org/pdf/2501.11786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11786]] Synthetic Data Can Mislead Evaluations: Membership Inference as Machine Text Detection(https://arxiv.org/abs/2501.11786)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>Recent work shows membership inference attacks (MIAs) on large language models (LLMs) produce inconclusive results, partly due to difficulties in creating non-member datasets without temporal shifts. While researchers have turned to synthetic data as an alternative, we show this approach can be fundamentally misleading. Our experiments indicate that MIAs function as machine-generated text detectors, incorrectly identifying synthetic data as training samples regardless of the data source. This behavior persists across different model architectures and sizes, from open-source models to commercial ones such as GPT-3.5. Even synthetic text generated by different, potentially larger models is classified as training data by the target model. Our findings highlight a serious concern: using synthetic data in membership evaluations may lead to false conclusions about model memorization and data leakage. We caution that this issue could affect other evaluations using model signals such as loss where synthetic or machine-generated translated data substitutes for real-world samples.</li>
</ul>

<h3>Title: Benchmarking Large Language Models via Random Variables</h3>
<ul>
<li><strong>Authors: </strong>Zijin Hong, Hao Wu, Su Dong, Junnan Dong, Yilin Xiao, Yujing Zhang, Zhu Wang, Feiran Huang, Linyi Li, Hongxia Yang, Xiao Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11790">https://arxiv.org/abs/2501.11790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11790">https://arxiv.org/pdf/2501.11790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11790]] Benchmarking Large Language Models via Random Variables(https://arxiv.org/abs/2501.11790)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the continuous advancement of large language models (LLMs) in mathematical reasoning, evaluating their performance in this domain has become a prominent research focus. Recent studies have raised concerns about the reliability of current mathematical benchmarks, highlighting issues such as simplistic design and potential data leakage. Therefore, creating a reliable benchmark that effectively evaluates the genuine capabilities of LLMs in mathematical reasoning remains a significant challenge. To address this, we propose RV-Bench, a framework for Benchmarking LLMs via Random Variables in mathematical reasoning. Specifically, the background content of a random variable question (RV question) mirrors the original problem in existing standard benchmarks, but the variable combinations are randomized into different values. LLMs must fully understand the problem-solving process for the original problem to correctly answer RV questions with various combinations of variable values. As a result, the LLM's genuine capability in mathematical reasoning is reflected by its accuracy on RV-Bench. Extensive experiments are conducted with 29 representative LLMs across 900+ RV questions. A leaderboard for RV-Bench ranks the genuine capability of these LLMs. Further analysis of accuracy dropping indicates that current LLMs still struggle with complex mathematical reasoning problems.</li>
</ul>

<h3>Title: Provably effective detection of effective data poisoning attacks</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Gallagher, Yasaman Esfandiari, Callen MacPhee, Michael Warren</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11795">https://arxiv.org/abs/2501.11795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11795">https://arxiv.org/pdf/2501.11795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11795]] Provably effective detection of effective data poisoning attacks(https://arxiv.org/abs/2501.11795)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>This paper establishes a mathematically precise definition of dataset poisoning attack and proves that the very act of effectively poisoning a dataset ensures that the attack can be effectively detected. On top of a mathematical guarantee that dataset poisoning is identifiable by a new statistical test that we call the Conformal Separability Test, we provide experimental evidence that we can adequately detect poisoning attempts in the real world.</li>
</ul>

<h3>Title: Blockchain Security Risk Assessment in Quantum Era, Migration Strategies and Proactive Defense</h3>
<ul>
<li><strong>Authors: </strong>Yaser Baseri, Abdelhakim Hafid, Yahya Shahsavari, Dimitrios Makrakis, Hassan Khodaiemehr</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11798">https://arxiv.org/abs/2501.11798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11798">https://arxiv.org/pdf/2501.11798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11798]] Blockchain Security Risk Assessment in Quantum Era, Migration Strategies and Proactive Defense(https://arxiv.org/abs/2501.11798)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack</a></li>
<li><strong>Abstract: </strong>The emergence of quantum computing presents a formidable challenge to the security of blockchain systems. Traditional cryptographic algorithms, foundational to digital signatures, message encryption, and hashing functions, become vulnerable to the immense computational power of quantum computers. This paper conducts a thorough risk assessment of transitioning to quantum-resistant blockchains, comprehensively analyzing potential threats targeting vital blockchain components: the network, mining pools, transaction verification mechanisms, smart contracts, and user wallets. By elucidating the intricate challenges and strategic considerations inherent in transitioning to quantum-resistant algorithms, the paper evaluates risks and highlights obstacles in securing blockchain components with quantum-resistant cryptography. It offers a hybrid migration strategy to facilitate a smooth transition from classical to quantum-resistant cryptography. The analysis extends to prominent blockchains such as Bitcoin, Ethereum, Ripple, Litecoin, and Zcash, assessing vulnerable components, potential impacts, and associated STRIDE threats, thereby identifying areas susceptible to quantum attacks. Beyond analysis, the paper provides actionable guidance for designing secure and resilient blockchain ecosystems in the quantum computing era. Recognizing the looming threat of quantum computers, this research advocates for a proactive transition to quantum-resistant blockchain networks. It proposes a tailored security blueprint that strategically fortifies each component against the evolving landscape of quantum-induced cyber threats. Emphasizing the critical need for blockchain stakeholders to adopt proactive measures and implement quantum-resistant solutions, the paper underscores the importance of embracing these insights to navigate the complexities of the quantum era with resilience and confidence.</li>
</ul>

<h3>Title: TFLOP: Table Structure Recognition Framework with Layout Pointer Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Minsoo Khang, Teakgyu Hong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11800">https://arxiv.org/abs/2501.11800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11800">https://arxiv.org/pdf/2501.11800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11800]] TFLOP: Table Structure Recognition Framework with Layout Pointer Mechanism(https://arxiv.org/abs/2501.11800)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, watermark</a></li>
<li><strong>Abstract: </strong>Table Structure Recognition (TSR) is a task aimed at converting table images into a machine-readable format (e.g. HTML), to facilitate other applications such as information retrieval. Recent works tackle this problem by identifying the HTML tags and text regions, where the latter is used for text extraction from the table document. These works however, suffer from misalignment issues when mapping text into the identified text regions. In this paper, we introduce a new TSR framework, called TFLOP (TSR Framework with LayOut Pointer mechanism), which reformulates the conventional text region prediction and matching into a direct text region pointing problem. Specifically, TFLOP utilizes text region information to identify both the table's structure tags and its aligned text regions, simultaneously. Without the need for region prediction and alignment, TFLOP circumvents the additional text region matching stage, which requires finely-calibrated post-processing. TFLOP also employs span-aware contrastive supervision to enhance the pointing mechanism in tables with complex structure. As a result, TFLOP achieves the state-of-the-art performance across multiple benchmarks such as PubTabNet, FinTabNet, and SynthTabNet. In our extensive experiments, TFLOP not only exhibits competitive performance but also shows promising results on industrial document TSR scenarios such as documents with watermarks or in non-English domain.</li>
</ul>

<h3>Title: CogMorph: Cognitive Morphing Attacks for Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Zonglei Jing, Zonghao Ying, Le Wang, Siyuan Liang, Aishan Liu, Xianglong Liu, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11815">https://arxiv.org/abs/2501.11815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11815">https://arxiv.org/pdf/2501.11815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11815]] CogMorph: Cognitive Morphing Attacks for Text-to-Image Models(https://arxiv.org/abs/2501.11815)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, generative</a></li>
<li><strong>Abstract: </strong>The development of text-to-image (T2I) generative models, that enable the creation of high-quality synthetic images from textual prompts, has opened new frontiers in creative design and content generation. However, this paper reveals a significant and previously unrecognized ethical risk inherent in this technology and introduces a novel method, termed the Cognitive Morphing Attack (CogMorph), which manipulates T2I models to generate images that retain the original core subjects but embeds toxic or harmful contextual elements. This nuanced manipulation exploits the cognitive principle that human perception of concepts is shaped by the entire visual scene and its context, producing images that amplify emotional harm far beyond attacks that merely preserve the original semantics. To address this, we first construct an imagery toxicity taxonomy spanning 10 major and 48 sub-categories, aligned with human cognitive-perceptual dimensions, and further build a toxicity risk matrix resulting in 1,176 high-quality T2I toxic prompts. Based on this, our CogMorph first introduces Cognitive Toxicity Augmentation, which develops a cognitive toxicity knowledge base with rich external toxic representations for humans (e.g., fine-grained visual features) that can be utilized to further guide the optimization of adversarial prompts. In addition, we present Contextual Hierarchical Morphing, which hierarchically extracts critical parts of the original prompt (e.g., scenes, subjects, and body parts), and then iteratively retrieves and fuses toxic features to inject harmful contexts. Extensive experiments on multiple open-sourced T2I models and black-box commercial APIs (e.g., DALLE-3) demonstrate the efficacy of CogMorph which significantly outperforms other baselines by large margins (+20.62\% on average).</li>
</ul>

<h3>Title: Toward Scalable Graph Unlearning: A Node Influence Maximization based Approach</h3>
<ul>
<li><strong>Authors: </strong>Xunkai Li, Bowen Fan, Zhengyu Wu, Zhiyu Li, Rong-Hua Li, Guoren Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DB, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11823">https://arxiv.org/abs/2501.11823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11823">https://arxiv.org/pdf/2501.11823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11823]] Toward Scalable Graph Unlearning: A Node Influence Maximization based Approach(https://arxiv.org/abs/2501.11823)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Machine unlearning, as a pivotal technology for enhancing model robustness and data privacy, has garnered significant attention in prevalent web mining applications, especially in thriving graph-based scenarios. However, most existing graph unlearning (GU) approaches face significant challenges due to the intricate interactions among web-scale graph elements during the model training: (1) The gradient-driven node entanglement hinders the complete knowledge removal in response to unlearning requests; (2) The billion-level graph elements in the web scenarios present inevitable scalability issues. To break the above limitations, we open up a new perspective by drawing a connection between GU and conventional social influence maximization. To this end, we propose Node Influence Maximization (NIM) through the decoupled influence propagation model and fine-grained influence function in a scalable manner, which is crafted to be a plug-and-play strategy to identify potential nodes affected by unlearning entities. This approach enables offline execution independent of GU, allowing it to be seamlessly integrated into most GU methods to improve their unlearning performance. Based on this, we introduce Scalable Graph Unlearning (SGU) as a new fine-tuned framework, which balances the forgetting and reasoning capability of the unlearned model by entity-specific optimizations. Extensive experiments on 14 datasets, including large-scale ogbn-papers100M, have demonstrated the effectiveness of our approach. Specifically, NIM enhances the forgetting capability of most GU methods, while SGU achieves comprehensive SOTA performance and maintains scalability.</li>
</ul>

<h3>Title: PXGen: A Post-hoc Explainable Method for Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Yen-Lung Huang, Ming-Hsi Weng, Hao-Tsung Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11827">https://arxiv.org/abs/2501.11827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11827">https://arxiv.org/pdf/2501.11827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11827]] PXGen: A Post-hoc Explainable Method for Generative Models(https://arxiv.org/abs/2501.11827)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>With the rapid growth of generative AI in numerous applications, explainable AI (XAI) plays a crucial role in ensuring the responsible development and deployment of generative AI technologies. XAI has undergone notable advancements and widespread adoption in recent years, reflecting a concerted push to enhance the transparency, interpretability, and credibility of AI systems. Recent research emphasizes that a proficient XAI method should adhere to a set of criteria, primarily focusing on two key areas. Firstly, it should ensure the quality and fluidity of explanations, encompassing aspects like faithfulness, plausibility, completeness, and tailoring to individual needs. Secondly, the design principle of the XAI system or mechanism should cover the following factors such as reliability, resilience, the verifiability of its outputs, and the transparency of its algorithm. However, research in XAI for generative models remains relatively scarce, with little exploration into how such methods can effectively meet these criteria in that domain. In this work, we propose PXGen, a post-hoc explainable method for generative models. Given a model that needs to be explained, PXGen prepares two materials for the explanation, the Anchor set and intrinsic & extrinsic criteria. Those materials are customizable by users according to their purpose and requirements. Via the calculation of each criterion, each anchor has a set of feature values and PXGen provides examplebased explanation methods according to the feature values among all the anchors and illustrated and visualized to the users via tractable algorithms such as k-dispersion or k-center.</li>
</ul>

<h3>Title: ShadowGenes: Leveraging Recurring Patterns within Computational Graphs for Model Genealogy</h3>
<ul>
<li><strong>Authors: </strong>Kasimir Schulz, Kieran Evans</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11830">https://arxiv.org/abs/2501.11830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11830">https://arxiv.org/pdf/2501.11830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11830]] ShadowGenes: Leveraging Recurring Patterns within Computational Graphs for Model Genealogy(https://arxiv.org/abs/2501.11830)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Machine learning model genealogy enables practitioners to determine which architectural family a neural network belongs to. In this paper, we introduce ShadowGenes, a novel, signature-based method for identifying a given model's architecture, type, and family. Our method involves building a computational graph of the model that is agnostic of its serialization format, then analyzing its internal operations to identify unique patterns, and finally building and refining signatures based on these. We highlight important workings of the underlying engine and demonstrate the technique used to construct a signature and scan a given model. This approach to model genealogy can be applied to model files without the need for additional external information. We test ShadowGenes on a labeled dataset of over 1,400 models and achieve a mean true positive rate of 97.49% and a precision score of 99.51%; which validates the technique as a practical method for model genealogy. This enables practitioners to understand the use cases of a given model, the internal computational process, and identify possible security risks, such as the potential for model backdooring.</li>
</ul>

<h3>Title: Data-driven Detection and Evaluation of Damages in Concrete Structures: Using Deep Learning and Computer Vision</h3>
<ul>
<li><strong>Authors: </strong>Saeid Ataei, Saeed Adibnazari, Seyyed Taghi Ataei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11836">https://arxiv.org/abs/2501.11836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11836">https://arxiv.org/pdf/2501.11836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11836]] Data-driven Detection and Evaluation of Damages in Concrete Structures: Using Deep Learning and Computer Vision(https://arxiv.org/abs/2501.11836)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Structural integrity is vital for maintaining the safety and longevity of concrete infrastructures such as bridges, tunnels, and walls. Traditional methods for detecting damages like cracks and spalls are labor-intensive, time-consuming, and prone to human error. To address these challenges, this study explores advanced data-driven techniques using deep learning for automated damage detection and analysis. Two state-of-the-art instance segmentation models, YOLO-v7 instance segmentation and Mask R-CNN, were evaluated using a dataset comprising 400 images, augmented to 10,995 images through geometric and color-based transformations to enhance robustness. The models were trained and validated using a dataset split into 90% training set, validation and test set 10%. Performance metrics such as precision, recall, mean average precision (mAP@0.5), and frames per second (FPS) were used for evaluation. YOLO-v7 achieved a superior mAP@0.5 of 96.1% and processed 40 FPS, outperforming Mask R-CNN, which achieved a mAP@0.5 of 92.1% with a slower processing speed of 18 FPS. The findings recommend YOLO-v7 instance segmentation model for real-time, high-speed structural health monitoring, while Mask R-CNN is better suited for detailed offline assessments. This study demonstrates the potential of deep learning to revolutionize infrastructure maintenance, offering a scalable and efficient solution for automated damage detection.</li>
</ul>

<h3>Title: Supervised Learning for Analog and RF Circuit Design: Benchmarks and Comparative Insights</h3>
<ul>
<li><strong>Authors: </strong>Asal Mehradfar, Xuzhe Zhao, Yue Niu, Sara Babakniya, Mahdi Alesheikh, Hamidreza Aghasi, Salman Avestimehr</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11839">https://arxiv.org/abs/2501.11839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11839">https://arxiv.org/pdf/2501.11839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11839]] Supervised Learning for Analog and RF Circuit Design: Benchmarks and Comparative Insights(https://arxiv.org/abs/2501.11839)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Automating analog and radio-frequency (RF) circuit design using machine learning (ML) significantly reduces the time and effort required for parameter optimization. This study explores supervised ML-based approaches for designing circuit parameters from performance specifications across various circuit types, including homogeneous and heterogeneous designs. By evaluating diverse ML models, from neural networks like transformers to traditional methods like random forests, we identify the best-performing models for each circuit. Our results show that simpler circuits, such as low-noise amplifiers, achieve exceptional accuracy with mean relative errors as low as 0.3% due to their linear parameter-performance relationships. In contrast, complex circuits, like power amplifiers and voltage-controlled oscillators, present challenges due to their non-linear interactions and larger design spaces. For heterogeneous circuits, our approach achieves an 88% reduction in errors with increased training data, with the receiver achieving a mean relative error as low as 0.23%, showcasing the scalability and accuracy of the proposed methodology. Additionally, we provide insights into model strengths, with transformers excelling in capturing non-linear mappings and k-nearest neighbors performing robustly in moderately linear parameter spaces, especially in heterogeneous circuits with larger datasets. This work establishes a foundation for extending ML-driven design automation, enabling more efficient and scalable circuit design workflows.</li>
</ul>

<h3>Title: Survey on Monocular Metric Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Jiuling Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11841">https://arxiv.org/abs/2501.11841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11841">https://arxiv.org/pdf/2501.11841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11841]] Survey on Monocular Metric Depth Estimation(https://arxiv.org/abs/2501.11841)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Monocular Depth Estimation (MDE) is a fundamental computer vision task underpinning applications such as spatial understanding, 3D reconstruction, and autonomous driving. While deep learning-based MDE methods can predict relative depth from a single image, their lack of metric scale information often results in scale inconsistencies, limiting their utility in downstream tasks like visual SLAM, 3D reconstruction, and novel view synthesis. Monocular Metric Depth Estimation (MMDE) addresses these challenges by enabling precise, scene-scale depth inference. MMDE improves depth consistency, enhances sequential task stability, simplifies integration into downstream applications, and broadens practical use cases. This paper provides a comprehensive review of depth estimation technologies, highlighting the evolution from geometry-based methods to state-of-the-art deep learning approaches. It emphasizes advancements in scale-agnostic methods, which are crucial for enabling zero-shot generalization as the foundational capability for MMDE. Recent progress in zero-shot MMDE research is explored, focusing on challenges such as model generalization and the loss of detail at scene boundaries. Innovative strategies to address these issues include unlabelled data augmentation, image patching, architectural optimization, and generative techniques. These advancements, analyzed in detail, demonstrate significant contributions to overcoming existing limitations. Finally, this paper synthesizes recent developments in zero-shot MMDE, identifies unresolved challenges, and outlines future research directions. By offering a clear roadmap and cutting-edge insights, this work aims to deepen understanding of MMDE, inspire novel applications, and drive technological innovation.</li>
</ul>

<h3>Title: A Survey on Memory-Efficient Large-Scale Model Training in AI for Science</h3>
<ul>
<li><strong>Authors: </strong>Kaiyuan Tian, Linbo Qiao, Baihui Liu, Gongqingjian Jiang, Dongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11847">https://arxiv.org/abs/2501.11847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11847">https://arxiv.org/pdf/2501.11847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11847]] A Survey on Memory-Efficient Large-Scale Model Training in AI for Science(https://arxiv.org/abs/2501.11847)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Scientific research faces high costs and inefficiencies with traditional methods, but the rise of deep learning and large language models (LLMs) offers innovative solutions. This survey reviews LLM applications across scientific fields such as biology, medicine, chemistry, and meteorology, underscoring their role in advancing research. However, the continuous expansion of model size has led to significant memory demands, hindering further development and application of LLMs for science. To address this, we review memory-efficient training techniques for LLMs based on the transformer architecture, including distributed training, mixed precision training, and gradient checkpointing. Using AlphaFold 2 as an example, we demonstrate how tailored memory optimization methods can reduce storage needs while preserving prediction accuracy. We also discuss the challenges of memory optimization in practice and potential future directions, hoping to provide valuable insights for researchers and engineers.</li>
</ul>

<h3>Title: FedMUA: Exploring the Vulnerabilities of Federated Learning to Malicious Unlearning Attacks</h3>
<ul>
<li><strong>Authors: </strong>Jian Chen, Zehui Lin, Wanyu Lin, Wenlong Shi, Xiaoyan Yin, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11848">https://arxiv.org/abs/2501.11848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11848">https://arxiv.org/pdf/2501.11848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11848]] FedMUA: Exploring the Vulnerabilities of Federated Learning to Malicious Unlearning Attacks(https://arxiv.org/abs/2501.11848)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, federate</a></li>
<li><strong>Abstract: </strong>Recently, the practical needs of ``the right to be forgotten'' in federated learning gave birth to a paradigm known as federated unlearning, which enables the server to forget personal data upon the client's removal request. Existing studies on federated unlearning have primarily focused on efficiently eliminating the influence of requested data from the client's model without retraining from scratch, however, they have rarely doubted the reliability of the global model posed by the discrepancy between its prediction performance before and after unlearning. To bridge this gap, we take the first step by introducing a novel malicious unlearning attack dubbed FedMUA, aiming to unveil potential vulnerabilities emerging from federated learning during the unlearning process. The crux of FedMUA is to mislead the global model into unlearning more information associated with the influential samples for the target sample than anticipated, thus inducing adverse effects on target samples from other clients. To achieve this, we design a novel two-step method, known as Influential Sample Identification and Malicious Unlearning Generation, to identify and subsequently generate malicious feature unlearning requests within the influential samples. By doing so, we can significantly alter the predictions pertaining to the target sample by initiating the malicious feature unlearning requests, leading to the deliberate manipulation for the user adversely. Additionally, we design a new defense mechanism that is highly resilient against malicious unlearning attacks. Extensive experiments on three realistic datasets reveal that FedMUA effectively induces misclassification on target samples and can achieve an 80% attack success rate by triggering only 0.3% malicious unlearning requests.</li>
</ul>

<h3>Title: Network-informed Prompt Engineering against Organized Astroturf Campaigns under Extreme Class Imbalance</h3>
<ul>
<li><strong>Authors: </strong>Nikos Kanakaris, Heng Ping, Xiongye Xiao, Nesreen K. Ahmed, Luca Luceri, Emilio Ferrara, Paul Bogdan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11849">https://arxiv.org/abs/2501.11849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11849">https://arxiv.org/pdf/2501.11849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11849]] Network-informed Prompt Engineering against Organized Astroturf Campaigns under Extreme Class Imbalance(https://arxiv.org/abs/2501.11849)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Detecting organized political campaigns is of paramount importance in fighting against disinformation on social media. Existing approaches for the identification of such organized actions employ techniques mostly from network science, graph machine learning and natural language processing. Their ultimate goal is to analyze the relationships and interactions (e.g. re-posting) among users and the textual similarities of their posts. Despite their effectiveness in recognizing astroturf campaigns, these methods face significant challenges, notably the class imbalance in available training datasets. To mitigate this issue, recent methods usually resort to data augmentation or increasing the number of positive samples, which may not always be feasible or sufficient in real-world settings. Following a different path, in this paper, we propose a novel framework for identifying astroturf campaigns based solely on large language models (LLMs), introducing a Balanced Retrieval-Augmented Generation (Balanced RAG) component. Our approach first gives both textual information concerning the posts (in our case tweets) and the user interactions of the social network as input to a language model. Then, through prompt engineering and the proposed Balanced RAG method, it effectively detects coordinated disinformation campaigns on X (Twitter). The proposed framework does not require any training or fine-tuning of the language model. Instead, by strategically harnessing the strengths of prompt engineering and Balanced RAG, it facilitates LLMs to overcome the effects of class imbalance and effectively identify coordinated political campaigns. The experimental results demonstrate that by incorporating the proposed prompt engineering and Balanced RAG methods, our framework outperforms the traditional graph-based baselines, achieving 2x-3x improvements in terms of precision, recall and F1 scores.</li>
</ul>

<h3>Title: Challenges in Expanding Portuguese Resources: A View from Open Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Marlo Souza, Bruno Cabral, Daniela Claro, Lais Salvador</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11851">https://arxiv.org/abs/2501.11851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11851">https://arxiv.org/pdf/2501.11851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11851]] Challenges in Expanding Portuguese Resources: A View from Open Information Extraction(https://arxiv.org/abs/2501.11851)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Open Information Extraction (Open IE) is the task of extracting structured information from textual documents, independent of domain. While traditional Open IE methods were based on unsupervised approaches, recently, with the emergence of robust annotated datasets, new data-based approaches have been developed to achieve better results. These innovations, however, have focused mainly on the English language due to a lack of datasets and the difficulty of constructing such resources for other languages. In this work, we present a high-quality manually annotated corpus for Open Information Extraction in the Portuguese language, based on a rigorous methodology grounded in established semantic theories. We discuss the challenges encountered in the annotation process, propose a set of structural and contextual annotation rules, and validate our corpus by evaluating the performance of state-of-the-art Open IE systems. Our resource addresses the lack of datasets for Open IE in Portuguese and can support the development and evaluation of new methods and systems in this area.</li>
</ul>

<h3>Title: Cross-Entropy Attacks to Language Models via Rare Event Simulation</h3>
<ul>
<li><strong>Authors: </strong>Mingze Ni, Yongshun Gong, Wei Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11852">https://arxiv.org/abs/2501.11852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11852">https://arxiv.org/pdf/2501.11852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11852]] Cross-Entropy Attacks to Language Models via Rare Event Simulation(https://arxiv.org/abs/2501.11852)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Black-box textual adversarial attacks are challenging due to the lack of model information and the discrete, non-differentiable nature of text. Existing methods often lack versatility for attacking different models, suffer from limited attacking performance due to the inefficient optimization with word saliency ranking, and frequently sacrifice semantic integrity to achieve better attack outcomes. This paper introduces a novel approach to textual adversarial attacks, which we call Cross-Entropy Attacks (CEA), that uses Cross-Entropy optimization to address the above issues. Our CEA approach defines adversarial objectives for both soft-label and hard-label settings and employs CE optimization to identify optimal replacements. Through extensive experiments on document classification and language translation problems, we demonstrate that our attack method excels in terms of attacking performance, imperceptibility, and sentence quality.</li>
</ul>

<h3>Title: EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents</h3>
<ul>
<li><strong>Authors: </strong>Zhili Cheng, Yuge Tu, Ran Li, Shiqi Dai, Jinyi Hu, Shengding Hu, Jiahao Li, Yang Shi, Tianyu Yu, Weize Chen, Lei Shi, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11858">https://arxiv.org/abs/2501.11858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11858">https://arxiv.org/pdf/2501.11858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11858]] EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents(https://arxiv.org/abs/2501.11858)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have shown significant advancements, providing a promising future for embodied agents. Existing benchmarks for evaluating MLLMs primarily utilize static images or videos, limiting assessments to non-interactive scenarios. Meanwhile, existing embodied AI benchmarks are task-specific and not diverse enough, which do not adequately evaluate the embodied capabilities of MLLMs. To address this, we propose EmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs with embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied 3D scenes, each of which is rigorously selected and annotated. It covers a broad spectrum of existing embodied AI tasks with significantly enhanced diversity, all within a unified simulation and evaluation framework tailored for MLLMs. The tasks are organized into five categories: navigation, object interaction, social interaction, attribute question answering, and spatial question answering to assess different capabilities of the agents. We evaluated the state-of-the-art MLLMs on EmbodiedEval and found that they have a significant shortfall compared to human level on embodied tasks. Our analysis demonstrates the limitations of existing MLLMs in embodied capabilities, providing insights for their future development. We open-source all evaluation data and simulation framework at this https URL.</li>
</ul>

<h3>Title: From Drafts to Answers: Unlocking LLM Potential via Aggregation Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yafu Li, Zhilin Wang, Tingchen Fu, Ganqu Cui, Sen Yang, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11877">https://arxiv.org/abs/2501.11877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11877">https://arxiv.org/pdf/2501.11877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11877]] From Drafts to Answers: Unlocking LLM Potential via Aggregation Fine-Tuning(https://arxiv.org/abs/2501.11877)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Scaling data and model size has been proven effective for boosting the performance of large language models. In addition to training-time scaling, recent studies have revealed that increasing test-time computational resources can further improve performance. In this work, we introduce Aggregation Fine-Tuning (AFT), a supervised finetuning paradigm where the model learns to synthesize multiple draft responses, referred to as proposals, into a single, refined answer, termed aggregation. At inference time, a propose-and-aggregate strategy further boosts performance by iteratively generating proposals and aggregating them. Empirical evaluations on benchmark datasets show that AFT-trained models substantially outperform standard SFT. Notably, an AFT model, fine-tuned from Llama3.1-8B-Base with only 64k data, achieves a 41.3% LC win rate on AlpacaEval 2, surpassing significantly larger LLMs such as Llama3.1-405B-Instruct and GPT4. By combining sequential refinement and parallel sampling, the propose-and-aggregate framework scales inference-time computation in a flexible manner. Overall, These findings position AFT as a promising approach to unlocking additional capabilities of LLMs without resorting to increasing data volume or model size.</li>
</ul>

<h3>Title: Community-Aware Temporal Walks: Parameter-Free Representation Learning on Continuous-Time Dynamic Graphs</h3>
<ul>
<li><strong>Authors: </strong>He Yu, Jing Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11880">https://arxiv.org/abs/2501.11880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11880">https://arxiv.org/pdf/2501.11880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11880]] Community-Aware Temporal Walks: Parameter-Free Representation Learning on Continuous-Time Dynamic Graphs(https://arxiv.org/abs/2501.11880)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Dynamic graph representation learning plays a crucial role in understanding evolving behaviors. However, existing methods often struggle with flexibility, adaptability, and the preservation of temporal and structural dynamics. To address these issues, we propose Community-aware Temporal Walks (CTWalks), a novel framework for representation learning on continuous-time dynamic graphs. CTWalks integrates three key components: a community-based parameter-free temporal walk sampling mechanism, an anonymization strategy enriched with community labels, and an encoding process that leverages continuous temporal dynamics modeled via ordinary differential equations (ODEs). This design enables precise modeling of both intra- and inter-community interactions, offering a fine-grained representation of evolving temporal patterns in continuous-time dynamic graphs. CTWalks theoretically overcomes locality bias in walks and establishes its connection to matrix factorization. Experiments on benchmark datasets demonstrate that CTWalks outperforms established methods in temporal link prediction tasks, achieving higher accuracy while maintaining robustness.</li>
</ul>

<h3>Title: Med-R$^2$: Crafting Trustworthy LLM Physicians through Retrieval and Reasoning of Evidence-Based Medicine</h3>
<ul>
<li><strong>Authors: </strong>Keer Lu, Zheng Liang, Da Pan, Shusen Zhang, Xin Wu, Weipeng Chen, Zenan Zhou, Guosheng Dong, Bin Cui, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11885">https://arxiv.org/abs/2501.11885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11885">https://arxiv.org/pdf/2501.11885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11885]] Med-R$^2$: Crafting Trustworthy LLM Physicians through Retrieval and Reasoning of Evidence-Based Medicine(https://arxiv.org/abs/2501.11885)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>In recent years, Large Language Models (LLMs) have exhibited remarkable capabilities in clinical scenarios. However, despite their potential, existing works face challenges when applying LLMs to medical settings. Strategies relying on training with medical datasets are highly cost-intensive and may suffer from outdated training data. Leveraging external knowledge bases is a suitable alternative, yet it faces obstacles such as limited retrieval precision and poor effectiveness in answer extraction. These issues collectively prevent LLMs from demonstrating the expected level of proficiency in mastering medical expertise. To address these challenges, we introduce Med-R^2, a novel LLM physician framework that adheres to the Evidence-Based Medicine (EBM) process, efficiently integrating retrieval mechanisms as well as the selection and reasoning processes of evidence, thereby enhancing the problem-solving capabilities of LLMs in healthcare scenarios and fostering a trustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2 achieves a 14.87\% improvement over vanilla RAG methods and even a 3.59\% enhancement compared to fine-tuning strategies, without incurring additional training costs.</li>
</ul>

<h3>Title: LASER: Lip Landmark Assisted Speaker Detection for Robustness</h3>
<ul>
<li><strong>Authors: </strong>Le Thien Phuc Nguyen, Zhuoran Yu, Yong Jae Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11899">https://arxiv.org/abs/2501.11899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11899">https://arxiv.org/pdf/2501.11899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11899]] LASER: Lip Landmark Assisted Speaker Detection for Robustness(https://arxiv.org/abs/2501.11899)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Active Speaker Detection (ASD) aims to identify speaking individuals in complex visual scenes. While humans can easily detect speech by matching lip movements to audio, current ASD models struggle to establish this correspondence, often misclassifying non-speaking instances when audio and lip movements are unsynchronized. To address this limitation, we propose Lip landmark Assisted Speaker dEtection for Robustness (LASER). Unlike models that rely solely on facial frames, LASER explicitly focuses on lip movements by integrating lip landmarks in training. Specifically, given a face track, LASER extracts frame-level visual features and the 2D coordinates of lip landmarks using a lightweight detector. These coordinates are encoded into dense feature maps, providing spatial and structural information on lip positions. Recognizing that landmark detectors may sometimes fail under challenging conditions (e.g., low resolution, occlusions, extreme angles), we incorporate an auxiliary consistency loss to align predictions from both lip-aware and face-only features, ensuring reliable performance even when lip data is absent. Extensive experiments across multiple datasets show that LASER outperforms state-of-the-art models, especially in scenarios with desynchronized audio and visuals, demonstrating robust performance in real-world video contexts. Code is available at \url{this https URL}.</li>
</ul>

<h3>Title: Panoramic Interests: Stylistic-Content Aware Personalized Headline Generation</h3>
<ul>
<li><strong>Authors: </strong>Junhong Lian, Xiang Ao, Xinyu Liu, Yang Liu, Qing He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11900">https://arxiv.org/abs/2501.11900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11900">https://arxiv.org/pdf/2501.11900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11900]] Panoramic Interests: Stylistic-Content Aware Personalized Headline Generation(https://arxiv.org/abs/2501.11900)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Personalized news headline generation aims to provide users with attention-grabbing headlines that are tailored to their preferences. Prevailing methods focus on user-oriented content preferences, but most of them overlook the fact that diverse stylistic preferences are integral to users' panoramic interests, leading to suboptimal personalization. In view of this, we propose a novel Stylistic-Content Aware Personalized Headline Generation (SCAPE) framework. SCAPE extracts both content and stylistic features from headlines with the aid of large language model (LLM) collaboration. It further adaptively integrates users' long- and short-term interests through a contrastive learning-based hierarchical fusion network. By incorporating the panoramic interests into the headline generator, SCAPE reflects users' stylistic-content preferences during the generation process. Extensive experiments on the real-world dataset PENS demonstrate the superiority of SCAPE over baselines.</li>
</ul>

<h3>Title: Enhancing Adversarial Transferability via Component-Wise Augmentation Method</h3>
<ul>
<li><strong>Authors: </strong>Hangyu Liu, Bo Peng, Pengxiang Ding, Donglin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11901">https://arxiv.org/abs/2501.11901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11901">https://arxiv.org/pdf/2501.11901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11901]] Enhancing Adversarial Transferability via Component-Wise Augmentation Method(https://arxiv.org/abs/2501.11901)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, transformer</a></li>
<li><strong>Abstract: </strong>Deep Neural Networks (DNNs) are highly vulnerable to adversarial examples, which pose significant challenges in security-sensitive applications. Among various adversarial attack strategies, input transformation-based attacks have demonstrated remarkable effectiveness in enhancing adversarial transferability. However, existing methods fail to diversify attention regions across models adequately and introduce excessive information loss during transformations. In this paper, we introduce a novel input transformation-based method, termed Component-Wise Augmentation (CWA), designed to enhance transferability by locally applying block-wise transformations. CWA strategically integrates interpolation and selective rotation on individual image blocks to diversify model attention regions while preserving semantic integrity. Extensive experiments on the standard ImageNet dataset show that CWA consistently outperforms state-of-the-art methods in both attack success rates and stability across CNN- and Transformer-based models, while also demonstrating superior performance against multiple defense methods.</li>
</ul>

<h3>Title: LuxVeri at GenAI Detection Task 1: Inverse Perplexity Weighted Ensemble for Robust Detection of AI-Generated Text across English and Multilingual Contexts</h3>
<ul>
<li><strong>Authors: </strong>Md Kamrujjaman Mobin, Md Saiful Islam</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11914">https://arxiv.org/abs/2501.11914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11914">https://arxiv.org/pdf/2501.11914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11914]] LuxVeri at GenAI Detection Task 1: Inverse Perplexity Weighted Ensemble for Robust Detection of AI-Generated Text across English and Multilingual Contexts(https://arxiv.org/abs/2501.11914)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents a system developed for Task 1 of the COLING 2025 Workshop on Detecting AI-Generated Content, focusing on the binary classification of machine-generated versus human-written text. Our approach utilizes an ensemble of models, with weights assigned according to each model's inverse perplexity, to enhance classification accuracy. For the English text detection task, we combined RoBERTa-base, RoBERTa-base with the OpenAI detector, and BERT-base-cased, achieving a Macro F1-score of 0.7458, which ranked us 12th out of 35 teams. We ensembled RemBERT, XLM-RoBERTa-base, and BERT-base-multilingual-case for the multilingual text detection task, employing the same inverse perplexity weighting technique. This resulted in a Macro F1-score of 0.7513, positioning us 4th out of 25 teams. Our results demonstrate the effectiveness of inverse perplexity weighting in improving the robustness of machine-generated text detection across both monolingual and multilingual settings, highlighting the potential of ensemble methods for this challenging task.</li>
</ul>

<h3>Title: LuxVeri at GenAI Detection Task 3: Cross-Domain Detection of AI-Generated Text Using Inverse Perplexity-Weighted Ensemble of Fine-Tuned Transformer Models</h3>
<ul>
<li><strong>Authors: </strong>Md Kamrujjaman Mobin, Md Saiful Islam</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11918">https://arxiv.org/abs/2501.11918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11918">https://arxiv.org/pdf/2501.11918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11918]] LuxVeri at GenAI Detection Task 3: Cross-Domain Detection of AI-Generated Text Using Inverse Perplexity-Weighted Ensemble of Fine-Tuned Transformer Models(https://arxiv.org/abs/2501.11918)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper presents our approach for Task 3 of the GenAI content detection workshop at COLING-2025, focusing on Cross-Domain Machine-Generated Text (MGT) Detection. We propose an ensemble of fine-tuned transformer models, enhanced by inverse perplexity weighting, to improve classification accuracy across diverse text domains. For Subtask A (Non-Adversarial MGT Detection), we combined a fine-tuned RoBERTa-base model with an OpenAI detector-integrated RoBERTa-base model, achieving an aggregate TPR score of 0.826, ranking 10th out of 23 detectors. In Subtask B (Adversarial MGT Detection), our fine-tuned RoBERTa-base model achieved a TPR score of 0.801, securing 8th out of 22 detectors. Our results demonstrate the effectiveness of inverse perplexity-based weighting for enhancing generalization and performance in both non-adversarial and adversarial MGT detection, highlighting the potential for transformer models in cross-domain AI-generated content detection.</li>
</ul>

<h3>Title: Progressive Cross Attention Network for Flood Segmentation using Multispectral Satellite Imagery</h3>
<ul>
<li><strong>Authors: </strong>Vicky Feliren, Fithrothul Khikmah, Irfan Dwiki Bhaswara, Bahrul I. Nasution, Alex M. Lechner, Muhamad Risqi U. Saputra</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11923">https://arxiv.org/abs/2501.11923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11923">https://arxiv.org/pdf/2501.11923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11923]] Progressive Cross Attention Network for Flood Segmentation using Multispectral Satellite Imagery(https://arxiv.org/abs/2501.11923)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In recent years, the integration of deep learning techniques with remote sensing technology has revolutionized the way natural hazards, such as floods, are monitored and managed. However, existing methods for flood segmentation using remote sensing data often overlook the utility of correlative features among multispectral satellite information. In this study, we introduce a progressive cross attention network (ProCANet), a deep learning model that progressively applies both self- and cross-attention mechanisms to multispectral features, generating optimal feature combinations for flood segmentation. The proposed model was compared with state-of-the-art approaches using Sen1Floods11 dataset and our bespoke flood data generated for the Citarum River basin, Indonesia. Our model demonstrated superior performance with the highest Intersection over Union (IoU) score of 0.815. Our results in this study, coupled with the ablation assessment comparing scenarios with and without attention across various modalities, opens a promising path for enhancing the accuracy of flood analysis using remote sensing technology.</li>
</ul>

<h3>Title: ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Peter Devine</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11929">https://arxiv.org/abs/2501.11929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11929">https://arxiv.org/pdf/2501.11929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11929]] ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation(https://arxiv.org/abs/2501.11929)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval Augmented Generation (RAG) systems have been shown to improve the accuracy of Large Language Model (LLM) outputs. However, these models can often achieve low accuracy when applied to new data domains. We introduce the Automatic Local Fine Tuning of Retrieval Augmented Generation models (ALoFTRAG) framework, designed to improve the accuracy of RAG systems on a given domain by training LLMs without manually labeled data or using larger teacher models. By generating and filtering synthetic training data and performing LoRA fine-tuning, ALoFTRAG improves citation and answer accuracy across 20 datasets in 26 languages by, on average, 8.3% and 3.0% respectively. Our results demonstrate that ALoFTRAG offers a practical, cost-effective, and data-secure solution for improving RAG accuracy, making it particularly applicable to sensitive domains such as healthcare and finance.</li>
</ul>

<h3>Title: BRC20 Snipping Attack</h3>
<ul>
<li><strong>Authors: </strong>Minfeng Qi, Qin Wang, Ningran Li, Shiping Chen, Tianqing Zhu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11942">https://arxiv.org/abs/2501.11942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11942">https://arxiv.org/pdf/2501.11942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11942]] BRC20 Snipping Attack(https://arxiv.org/abs/2501.11942)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, fair</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce and implement BRC20 sniping attack. Our attack manipulates the BRC20 token transfers in open markets and disrupts the fairness among bidding participants. The long-standing principle of ``highest bidder wins'' is rendered ineffective. Typically, open BRC20 token markets rely on Partially Signed Bitcoin Transactions (PSBT) to broadcast selling intents and wait for buying auctions. Our attack targets the BRC20 buying process (i.e., transfer) by injecting a front-running transaction to complete the full signature of the PSBT. At its core, the attack exploits the mempool's fee-based transaction selection mechanism to snipe the victim transaction, replicate metadata, and front-run the legesmate transaction. This attack applies to platforms using PSBT for BRC20 token transfers, including popular Bitcoin exchanges and marketplaces (e.g., Magic Eden, Unisat, this http URL, OKX). We implemented and tested the attack on a Bitcoin testnet (regtest), validating its effectiveness through multiple experimental rounds. Results show that the attacker consistently replaces legitimate transactions by submitting higher-fee PSBTs. We have also made responsible disclosures to the mentioned exchanges.</li>
</ul>

<h3>Title: Proverbs Run in Pairs: Evaluating Proverb Translation Capability of Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Minghan Wang, Viet-Thanh Pham, Farhad Moghimifar, Thuy-Trang Vu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11953">https://arxiv.org/abs/2501.11953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11953">https://arxiv.org/pdf/2501.11953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11953]] Proverbs Run in Pairs: Evaluating Proverb Translation Capability of Large Language Model(https://arxiv.org/abs/2501.11953)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite achieving remarkable performance, machine translation (MT) research remains underexplored in terms of translating cultural elements in languages, such as idioms, proverbs, and colloquial expressions. This paper investigates the capability of state-of-the-art neural machine translation (NMT) and large language models (LLMs) in translating proverbs, which are deeply rooted in cultural contexts. We construct a translation dataset of standalone proverbs and proverbs in conversation for four language pairs. Our experiments show that the studied models can achieve good translation between languages with similar cultural backgrounds, and LLMs generally outperform NMT models in proverb translation. Furthermore, we find that current automatic evaluation metrics such as BLEU, CHRF++ and COMET are inadequate for reliably assessing the quality of proverb translation, highlighting the need for more culturally aware evaluation metrics.</li>
</ul>

<h3>Title: Noise-Resilient Point-wise Anomaly Detection in Time Series Using Weak Segment Labels</h3>
<ul>
<li><strong>Authors: </strong>Yaxuan Wang, Hao Cheng, Jing Xiong, Qingsong Wen, Han Jia, Ruixuan Song, Liyuan Zhang, Zhaowei Zhu, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11959">https://arxiv.org/abs/2501.11959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11959">https://arxiv.org/pdf/2501.11959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11959]] Noise-Resilient Point-wise Anomaly Detection in Time Series Using Weak Segment Labels(https://arxiv.org/abs/2501.11959)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Detecting anomalies in temporal data has gained significant attention across various real-world applications, aiming to identify unusual events and mitigate potential hazards. In practice, situations often involve a mix of segment-level labels (detected abnormal events with segments of time points) and unlabeled data (undetected events), while the ideal algorithmic outcome should be point-level predictions. Therefore, the huge label information gap between training data and targets makes the task challenging. In this study, we formulate the above imperfect information as noisy labels and propose NRdetector, a noise-resilient framework that incorporates confidence-based sample selection, robust segment-level learning, and data-centric point-level detection for multivariate time series anomaly detection. Particularly, to bridge the information gap between noisy segment-level labels and missing point-level labels, we develop a novel loss function that can effectively mitigate the label noise and consider the temporal features. It encourages the smoothness of consecutive points and the separability of points from segments with different labels. Extensive experiments on real-world multivariate time series datasets with 11 different evaluation metrics demonstrate that NRdetector consistently achieves robust results across multiple real-world datasets, outperforming various baselines adapted to operate in our setting.</li>
</ul>

<h3>Title: TAD-Bench: A Comprehensive Benchmark for Embedding-Based Text Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Yang Cao, Sikun Yang, Chen Li, Haolong Xiang, Lianyong Qi, Bo Liu, Rongsheng Li, Ming Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11960">https://arxiv.org/abs/2501.11960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11960">https://arxiv.org/pdf/2501.11960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11960]] TAD-Bench: A Comprehensive Benchmark for Embedding-Based Text Anomaly Detection(https://arxiv.org/abs/2501.11960)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Text anomaly detection is crucial for identifying spam, misinformation, and offensive language in natural language processing tasks. Despite the growing adoption of embedding-based methods, their effectiveness and generalizability across diverse application scenarios remain under-explored. To address this, we present TAD-Bench, a comprehensive benchmark designed to systematically evaluate embedding-based approaches for text anomaly detection. TAD-Bench integrates multiple datasets spanning different domains, combining state-of-the-art embeddings from large language models with a variety of anomaly detection algorithms. Through extensive experiments, we analyze the interplay between embeddings and detection methods, uncovering their strengths, weaknesses, and applicability to different tasks. These findings offer new perspectives on building more robust, efficient, and generalizable anomaly detection systems for real-world applications.</li>
</ul>

<h3>Title: A Hybrid Attention Framework for Fake News Detection with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaochuan Xu, Peiyang Yu, Zeqiu Xu, Jiani Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11967">https://arxiv.org/abs/2501.11967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11967">https://arxiv.org/pdf/2501.11967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11967]] A Hybrid Attention Framework for Fake News Detection with Large Language Models(https://arxiv.org/abs/2501.11967)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>With the rapid growth of online information, the spread of fake news has become a serious social challenge. In this study, we propose a novel detection framework based on Large Language Models (LLMs) to identify and classify fake news by integrating textual statistical features and deep semantic features. Our approach utilizes the contextual understanding capability of the large language model for text analysis and introduces a hybrid attention mechanism to focus on feature combinations that are particularly important for fake news identification. Extensive experiments on the WELFake news dataset show that our model significantly outperforms existing methods, with a 1.5\% improvement in F1 score. In addition, we assess the interpretability of the model through attention heat maps and SHAP values, providing actionable insights for content review strategies. Our framework provides a scalable and efficient solution to deal with the spread of fake news and helps build a more reliable online information ecosystem.</li>
</ul>

<h3>Title: SMamba: Sparse Mamba for Event-based Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Nan Yang, Yang Wang, Zhanwen Liu, Meng Li, Yisheng An, Xiangmo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11971">https://arxiv.org/abs/2501.11971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11971">https://arxiv.org/pdf/2501.11971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11971]] SMamba: Sparse Mamba for Event-based Object Detection(https://arxiv.org/abs/2501.11971)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based methods have achieved remarkable performance in event-based object detection, owing to the global modeling ability. However, they neglect the influence of non-event and noisy regions and process them uniformly, leading to high computational overhead. To mitigate computation cost, some researchers propose window attention based sparsification strategies to discard unimportant regions, which sacrifices the global modeling ability and results in suboptimal performance. To achieve better trade-off between accuracy and efficiency, we propose Sparse Mamba (SMamba), which performs adaptive sparsification to reduce computational effort while maintaining global modeling capability. Specifically, a Spatio-Temporal Continuity Assessment module is proposed to measure the information content of tokens and discard uninformative ones by leveraging the spatiotemporal distribution differences between activity and noise events. Based on the assessment results, an Information-Prioritized Local Scan strategy is designed to shorten the scan distance between high-information tokens, facilitating interactions among them in the spatial dimension. Furthermore, to extend the global interaction from 2D space to 3D representations, a Global Channel Interaction module is proposed to aggregate channel information from a global spatial perspective. Results on three datasets (Gen1, 1Mpx, and eTram) demonstrate that our model outperforms other methods in both performance and efficiency.</li>
</ul>

<h3>Title: "FRAME: Forward Recursive Adaptive Model Extraction -- A Technique for Advance Feature Selection"</h3>
<ul>
<li><strong>Authors: </strong>Nachiket Kapure, Harsh Joshi, Parul Kumari, Rajeshwari mistri, Manasi Mali</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11972">https://arxiv.org/abs/2501.11972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11972">https://arxiv.org/pdf/2501.11972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11972]] "FRAME: Forward Recursive Adaptive Model Extraction -- A Technique for Advance Feature Selection"(https://arxiv.org/abs/2501.11972)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, interpretability</a></li>
<li><strong>Abstract: </strong>Feature selection is a crucial preprocessing step in machine learning, impacting model performance, interpretability, and computational efficiency. This study introduces a novel hybrid approach, the Forward Recursive Adaptive Model Extraction Technique (FRAME), which combines Forward Selection and Recursive Feature Elimination (RFE) to enhance feature selection across diverse datasets. FRAME integrates the strengths of both methods, balancing exploration and exploitation of features to optimize selection. A comprehensive evaluation of FRAME was conducted against traditional methods such as SelectKBest and Lasso Regression, using high-dimensional, noisy, and heterogeneous datasets. The results demonstrate that FRAME consistently delivers superior predictive performance based on downstream machine learning evaluation metrics. It effectively reduces dimensionality while maintaining robust model performance, making it particularly valuable for applications requiring interpretable and accurate predictions, such as biomedical diagnostics. This study highlights the importance of assessing feature selection methods across varied datasets to ensure their robustness and generalizability. The findings suggest that FRAME has significant potential for further enhancement, particularly through integration with deep learning architectures for adaptive and real-time feature selection in dynamic environments. By advancing feature selection methodologies, FRAME offers a practical and effective solution to improve machine learning applications across multiple domains.</li>
</ul>

<h3>Title: Leveraging Graph Structures and Large Language Models for End-to-End Synthetic Task-Oriented Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Maya Medjad, Hugo Imbert, Bruno Yun, Raphaël Szymocha, Frédéric Armetta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11977">https://arxiv.org/abs/2501.11977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11977">https://arxiv.org/pdf/2501.11977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11977]] Leveraging Graph Structures and Large Language Models for End-to-End Synthetic Task-Oriented Dialogues(https://arxiv.org/abs/2501.11977)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Training task-oriented dialogue systems is both costly and time-consuming, due to the need for high-quality datasets encompassing diverse intents. Traditional methods depend on extensive human annotation, while recent advancements leverage large language models (LLMs) to generate synthetic data. However, these approaches often require custom prompts or code, limiting accessibility for non-technical users. We introduce GraphTOD, an end-to-end framework that simplifies the generation of task-oriented dialogues. Users can create dialogues by specifying transition graphs in JSON format. Our evaluation demonstrates that GraphTOD generates high-quality dialogues across various domains, significantly lowering the cost and complexity of dataset creation.</li>
</ul>

<h3>Title: Linear Feedback Control Systems for Iterative Prompt Optimization in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Rupesh Raj Karn</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11979">https://arxiv.org/abs/2501.11979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11979">https://arxiv.org/pdf/2501.11979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11979]] Linear Feedback Control Systems for Iterative Prompt Optimization in Large Language Models(https://arxiv.org/abs/2501.11979)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized various applications by generating outputs based on given prompts. However, achieving the desired output requires iterative prompt refinement. This paper presents a novel approach that draws parallels between the iterative prompt optimization process in LLMs and feedback control systems. We iteratively refine the prompt by treating the deviation between the LLM output and the desired result as an error term until the output criteria are met. This process is akin to a feedback control system, where the LLM, despite being non-linear and non-deterministic, is managed using principles from linear feedback control systems. We explore the application of different types of controllers within this framework, providing a mathematical foundation for integrating linear feedback control mechanisms with LLMs.</li>
</ul>

<h3>Title: Survey on Hand Gesture Recognition from Visual Input</h3>
<ul>
<li><strong>Authors: </strong>Manousos Linardakis, Iraklis Varlamis, Georgios Th. Papadopoulos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.11992">https://arxiv.org/abs/2501.11992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.11992">https://arxiv.org/pdf/2501.11992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.11992]] Survey on Hand Gesture Recognition from Visual Input(https://arxiv.org/abs/2501.11992)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Hand gesture recognition has become an important research area, driven by the growing demand for human-computer interaction in fields such as sign language recognition, virtual and augmented reality, and robotics. Despite the rapid growth of the field, there are few surveys that comprehensively cover recent research developments, available solutions, and benchmark datasets. This survey addresses this gap by examining the latest advancements in hand gesture and 3D hand pose recognition from various types of camera input data including RGB images, depth images, and videos from monocular or multiview cameras, examining the differing methodological requirements of each approach. Furthermore, an overview of widely used datasets is provided, detailing their main characteristics and application domains. Finally, open challenges such as achieving robust recognition in real-world environments, handling occlusions, ensuring generalization across diverse users, and addressing computational efficiency for real-time applications are highlighted to guide future research directions. By synthesizing the objectives, methodologies, and applications of recent studies, this survey offers valuable insights into current trends, challenges, and opportunities for future research in human hand gesture recognition.</li>
</ul>

<h3>Title: The Dilemma of Privacy Protection for Developers in the Metaverse</h3>
<ul>
<li><strong>Authors: </strong>Argianto Rahartomo, Leonel Merino, Mohammad Ghafari, Yoshiki Ohshima</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12006">https://arxiv.org/abs/2501.12006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12006">https://arxiv.org/pdf/2501.12006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12006]] The Dilemma of Privacy Protection for Developers in the Metaverse(https://arxiv.org/abs/2501.12006)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>To investigate the level of support and awareness developers possess for dealing with sensitive data in the metaverse, we surveyed developers, consulted legal frameworks, and analyzed API documentation in the metaverse. Our preliminary results suggest that privacy is a major concern, but developer awareness and existing support are limited. Developers lack strategies to identify sensitive data that are exclusive to the metaverse. The API documentation contains guidelines for collecting sensitive information, but it omits instructions for identifying and protecting it. Legal frameworks include definitions that are subject to individual interpretation. These findings highlight the urgent need to build a transparent and common ground for privacy definitions, identify sensitive data, and implement usable protection measures.</li>
</ul>

<h3>Title: Ratio Attack on G+G Convoluted Gaussian Signature</h3>
<ul>
<li><strong>Authors: </strong>Chik How Tan, Theo Fanuela Prabowo, Wei Guo Foo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12009">https://arxiv.org/abs/2501.12009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12009">https://arxiv.org/pdf/2501.12009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12009]] Ratio Attack on G+G Convoluted Gaussian Signature(https://arxiv.org/abs/2501.12009)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack</a></li>
<li><strong>Abstract: </strong>A lattice-based signature, called G+G convoluted Gaussian signature was proposed in ASIACRYPT 2023 and was proved secure in the quantum random oracle model. In this paper, we propose a ratio attack on the G+G convoluted Gaussian signature to recover the secret key. The attack exploits the fact, proved in this paper, that the secret key can be obtained from the expected value of the ratio of signatures which follows a truncated Cauchy distribution. Moreover, we also compute the number of signatures required to successfully recover the secret key. Furthermore, we simulate the ratio attack in Sagemath with a few different parameters as a proof-of-concept of the ratio attack.</li>
</ul>

<h3>Title: TabularARGN: A Flexible and Efficient Auto-Regressive Framework for Generating High-Fidelity Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Paul Tiwald, Ivona Krchova, Andrey Sidorenko, Mariana Vargas-Vieyra, Mario Scriminaci, Michael Platzer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12012">https://arxiv.org/abs/2501.12012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12012">https://arxiv.org/pdf/2501.12012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12012]] TabularARGN: A Flexible and Efficient Auto-Regressive Framework for Generating High-Fidelity Synthetic Data(https://arxiv.org/abs/2501.12012)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative</a></li>
<li><strong>Abstract: </strong>Synthetic data generation for tabular datasets must balance fidelity, efficiency, and versatility to meet the demands of real-world applications. We introduce the Tabular Auto-Regressive Generative Network (TabularARGN), a flexible framework designed to handle mixed-type, multivariate, and sequential datasets. By training on all possible conditional probabilities, TabularARGN supports advanced features such as fairness-aware generation, imputation, and conditional generation on any subset of columns. The framework achieves state-of-the-art synthetic data quality while significantly reducing training and inference times, making it ideal for large-scale datasets with diverse structures. Evaluated across established benchmarks, including realistic datasets with complex relationships, TabularARGN demonstrates its capability to synthesize high-quality data efficiently. By unifying flexibility and performance, this framework paves the way for practical synthetic data generation across industries.</li>
</ul>

<h3>Title: On the "Illusion" of Gender Bias in Face Recognition: Explaining the Fairness Issue Through Non-demographic Attributes</h3>
<ul>
<li><strong>Authors: </strong>Paul Jonas Kurz, Haiyu Wu, Kevin W. Bowyer, Philipp Terhörst</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12020">https://arxiv.org/abs/2501.12020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12020">https://arxiv.org/pdf/2501.12020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12020]] On the "Illusion" of Gender Bias in Face Recognition: Explaining the Fairness Issue Through Non-demographic Attributes(https://arxiv.org/abs/2501.12020)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric, fair</a></li>
<li><strong>Abstract: </strong>Face recognition systems (FRS) exhibit significant accuracy differences based on the user's gender. Since such a gender gap reduces the trustworthiness of FRS, more recent efforts have tried to find the causes. However, these studies make use of manually selected, correlated, and small-sized sets of facial features to support their claims. In this work, we analyse gender bias in face recognition by successfully extending the search domain to decorrelated combinations of 40 non-demographic facial characteristics. First, we propose a toolchain to effectively decorrelate and aggregate facial attributes to enable a less-biased gender analysis on large-scale data. Second, we introduce two new fairness metrics to measure fairness with and without context. Based on these grounds, we thirdly present a novel unsupervised algorithm able to reliably identify attribute combinations that lead to vanishing bias when used as filter predicates for balanced testing datasets. The experiments show that the gender gap vanishes when images of male and female subjects share specific attributes, clearly indicating that the issue is not a question of biology but of the social definition of appearance. These findings could reshape our understanding of fairness in face biometrics and provide insights into FRS, helping to address gender bias issues.</li>
</ul>

<h3>Title: Foreign object segmentation in chest x-rays through anatomy-guided shape insertion</h3>
<ul>
<li><strong>Authors: </strong>Constantin Seibold, Hamza Kalisch, Lukas Heine, Simon Reiß, Jens Kleesiek</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12022">https://arxiv.org/abs/2501.12022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12022">https://arxiv.org/pdf/2501.12022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12022]] Foreign object segmentation in chest x-rays through anatomy-guided shape insertion(https://arxiv.org/abs/2501.12022)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we tackle the challenge of instance segmentation for foreign objects in chest radiographs, commonly seen in postoperative follow-ups with stents, pacemakers, or ingested objects in children. The diversity of foreign objects complicates dense annotation, as shown in insufficient existing datasets. To address this, we propose the simple generation of synthetic data through (1) insertion of arbitrary shapes (lines, polygons, ellipses) with varying contrasts and opacities, and (2) cut-paste augmentations from a small set of semi-automatically extracted labels. These insertions are guided by anatomy labels to ensure realistic placements, such as stents appearing only in relevant vessels. Our approach enables networks to segment complex structures with minimal manually labeled data. Notably, it achieves performance comparable to fully supervised models while using 93\% fewer manual annotations.</li>
</ul>

<h3>Title: Comparative Analysis of Pre-trained Deep Learning Models and DINOv2 for Cushing's Syndrome Diagnosis in Facial Analysis</h3>
<ul>
<li><strong>Authors: </strong>Hongjun Liu, Changwei Song, Jiaqi Qiang, Jianqiang Li, Hui Pan, Lin Lu, Xiao Long, Qing Zhao, Jiuzuo Huang, Shi Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12023">https://arxiv.org/abs/2501.12023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12023">https://arxiv.org/pdf/2501.12023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12023]] Comparative Analysis of Pre-trained Deep Learning Models and DINOv2 for Cushing's Syndrome Diagnosis in Facial Analysis(https://arxiv.org/abs/2501.12023)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Cushing's syndrome is a condition caused by excessive glucocorticoid secretion from the adrenal cortex, often manifesting with moon facies and plethora, making facial data crucial for diagnosis. Previous studies have used pre-trained convolutional neural networks (CNNs) for diagnosing Cushing's syndrome using frontal facial images. However, CNNs are better at capturing local features, while Cushing's syndrome often presents with global facial features. Transformer-based models like ViT and SWIN, which utilize self-attention mechanisms, can better capture long-range dependencies and global features. Recently, DINOv2, a foundation model based on visual Transformers, has gained interest. This study compares the performance of various pre-trained models, including CNNs, Transformer-based models, and DINOv2, in diagnosing Cushing's syndrome. We also analyze gender bias and the impact of freezing mechanisms on DINOv2. Our results show that Transformer-based models and DINOv2 outperformed CNNs, with ViT achieving the highest F1 score of 85.74%. Both the pre-trained model and DINOv2 had higher accuracy for female samples. DINOv2 also showed improved performance when freezing parameters. In conclusion, Transformer-based models and DINOv2 are effective for Cushing's syndrome classification.</li>
</ul>

<h3>Title: Application of Machine Learning Techniques for Secure Traffic in NoC-based Manycores</h3>
<ul>
<li><strong>Authors: </strong>Geaninne Lopes, César Marcon, Fernando Moraes</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12034">https://arxiv.org/abs/2501.12034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12034">https://arxiv.org/pdf/2501.12034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12034]] Application of Machine Learning Techniques for Secure Traffic in NoC-based Manycores(https://arxiv.org/abs/2501.12034)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Like most computer systems, a manycore can also be the target of security attacks. It is essential to ensure the security of the NoC since all information travels through its channels, and any interference in the traffic of messages can reflect on the entire chip, causing communication problems. Among the possible attacks on NoC, Denial of Service (DoS) attacks are the most cited in the literature. The state of the art shows a lack of work that can detect such attacks through learning techniques. On the other hand, these techniques are widely explored in computer network security via an Intrusion Detection System (IDS). In this context, the main goal of this document is to present the progress of a work that explores an IDS technique using machine learning and temporal series for detecting DoS attacks in NoC-based manycore systems. To fulfill this goal, it is necessary to extract traffic data from a manycore NoC and execute the learning techniques in the extracted data. However, while low-level platforms offer precision and slow execution, high-level platforms offer higher speed and data incompatible with reality. Therefore, a platform is being developed using the OVP tool, which has a higher level of abstraction. To solve the low precision problem, the developed platform will have its data validated with a low-level platform.</li>
</ul>

<h3>Title: Communication-Efficient and Privacy-Adaptable Mechanism for Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Chih Wei Ling, Youqi Wu, Jiande Sun, Cheuk Ting Li, Linqi Song, Weitao Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12046">https://arxiv.org/abs/2501.12046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12046">https://arxiv.org/pdf/2501.12046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12046]] Communication-Efficient and Privacy-Adaptable Mechanism for Federated Learning(https://arxiv.org/abs/2501.12046)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Training machine learning models on decentralized private data via federated learning (FL) poses two key challenges: communication efficiency and privacy protection. In this work, we address these challenges within the trusted aggregator model by introducing a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), achieving both objectives simultaneously. In particular, CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a construction of randomized vector quantizer whose resulting distortion is equivalent to a prescribed noise, such as Gaussian or Laplace noise, enabling joint differential privacy and compression. Moreover, we analyze the trade-offs among user privacy, global utility, and transmission rate of CEPAM by defining appropriate metrics for FL with differential privacy and compression. Our CEPAM provides the additional benefit of privacy adaptability, allowing clients and the server to customize privacy protection based on required accuracy and protection. We assess CEPAM's utility performance using MNIST dataset, demonstrating that CEPAM surpasses baseline models in terms of learning accuracy.</li>
</ul>

<h3>Title: MedS$^3$: Towards Medical Small Language Models with Self-Evolved Slow Thinking</h3>
<ul>
<li><strong>Authors: </strong>Shuyang Jiang, Yusheng Liao, Zhe Chen, Ya Zhang, Yanfeng Wang, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12051">https://arxiv.org/abs/2501.12051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12051">https://arxiv.org/pdf/2501.12051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12051]] MedS$^3$: Towards Medical Small Language Models with Self-Evolved Slow Thinking(https://arxiv.org/abs/2501.12051)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Medical language models (MLMs) have become pivotal in advancing medical natural language processing. However, prior models that rely on pre-training or supervised fine-tuning often exhibit low data efficiency and limited practicality in real-world clinical applications. While OpenAIs O1 highlights test-time scaling in mathematics, attempts to replicate this approach in medicine typically distill responses from GPT-series models to open-source models, focusing primarily on multiple-choice tasks. This strategy, though straightforward, neglects critical concerns like data privacy and realistic deployment in clinical settings. In this work, we present a deployable, small-scale medical language model, \mone, designed for long-chain reasoning in clinical tasks using a self-evolution paradigm. Starting with a seed dataset of around 8,000 instances spanning five domains and 16 datasets, we prompt a base policy model to perform Monte Carlo Tree Search (MCTS) to construct verifiable reasoning chains. Each reasoning step is assigned an evolution rollout value, allowing verified trajectories to train the policy model and the reward model. During inference, the policy model generates multiple responses, and the reward model selects the one with the highest reward score. Experiments on eleven evaluation datasets demonstrate that \mone outperforms prior open-source models by 2 points, with the addition of the reward model further boosting performance ($\sim$13 points), surpassing GPT-4o-mini. Code and data are available at \url{this https URL}.</li>
</ul>

<h3>Title: Aggrotech: Leveraging Deep Learning for Sustainable Tomato Disease Management</h3>
<ul>
<li><strong>Authors: </strong>MD Mehraz Hosen, Md. Hasibul Islam</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12052">https://arxiv.org/abs/2501.12052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12052">https://arxiv.org/pdf/2501.12052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12052]] Aggrotech: Leveraging Deep Learning for Sustainable Tomato Disease Management(https://arxiv.org/abs/2501.12052)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Tomato crop health plays a critical role in ensuring agricultural productivity and food security. Timely and accurate detection of diseases affecting tomato plants is vital for effective disease management. In this study, we propose a deep learning-based approach for Tomato Leaf Disease Detection using two well-established convolutional neural networks (CNNs), namely VGG19 and Inception v3. The experiment is conducted on the Tomato Villages Dataset, encompassing images of both healthy tomato leaves and leaves afflicted by various diseases. The VGG19 model is augmented with fully connected layers, while the Inception v3 model is modified to incorporate a global average pooling layer and a dense classification layer. Both models are trained on the prepared dataset, and their performances are evaluated on a separate test set. This research employs VGG19 and Inception v3 models on the Tomato Villages dataset (4525 images) for tomato leaf disease detection. The models' accuracy of 93.93% with dropout layers demonstrates their usefulness for crop health monitoring. The paper suggests a deep learning-based strategy that includes normalization, resizing, dataset preparation, and unique model architectures. During training, VGG19 and Inception v3 serve as feature extractors, with possible data augmentation and fine-tuning. Metrics like accuracy, precision, recall, and F1 score are obtained through evaluation on a test set and offer important insights into the strengths and shortcomings of the model. The method has the potential for practical use in precision agriculture and could help tomato crops prevent illness early on.</li>
</ul>

<h3>Title: Unified 3D MRI Representations via Sequence-Invariant Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Liam Chalcroft, Jenny Cronin, Cathy J. Price, John Ashburner</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.med-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12057">https://arxiv.org/abs/2501.12057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12057">https://arxiv.org/pdf/2501.12057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12057]] Unified 3D MRI Representations via Sequence-Invariant Contrastive Learning(https://arxiv.org/abs/2501.12057)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Self-supervised deep learning has accelerated 2D natural image analysis but remains difficult to translate into 3D MRI, where data are scarce and pre-trained 2D backbones cannot capture volumetric context. We present a sequence-invariant self-supervised framework leveraging quantitative MRI (qMRI). By simulating multiple MRI contrasts from a single 3D qMRI scan and enforcing consistent representations across these contrasts, we learn anatomy-centric rather than sequence-specific features. This yields a robust 3D encoder that performs strongly across varied tasks and protocols. Experiments on healthy brain segmentation (IXI), stroke lesion segmentation (ARC), and MRI denoising show significant gains over baseline SSL approaches, especially in low-data settings (up to +8.3% Dice, +4.2 dB PSNR). Our model also generalises effectively to unseen sites, demonstrating potential for more scalable and clinically reliable volumetric analysis. All code and trained models are publicly available.</li>
</ul>

<h3>Title: Tackling Uncertainties in Multi-Agent Reinforcement Learning through Integration of Agent Termination Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Somnath Hazra, Pallab Dasgupta, Soumyajit Dey</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12061">https://arxiv.org/abs/2501.12061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12061">https://arxiv.org/pdf/2501.12061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12061]] Tackling Uncertainties in Multi-Agent Reinforcement Learning through Integration of Agent Termination Dynamics(https://arxiv.org/abs/2501.12061)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-Agent Reinforcement Learning (MARL) has gained significant traction for solving complex real-world tasks, but the inherent stochasticity and uncertainty in these environments pose substantial challenges to efficient and robust policy learning. While Distributional Reinforcement Learning has been successfully applied in single-agent settings to address risk and uncertainty, its application in MARL is substantially limited. In this work, we propose a novel approach that integrates distributional learning with a safety-focused loss function to improve convergence in cooperative MARL tasks. Specifically, we introduce a Barrier Function based loss that leverages safety metrics, identified from inherent faults in the system, into the policy learning process. This additional loss term helps mitigate risks and encourages safer exploration during the early stages of training. We evaluate our method in the StarCraft II micromanagement benchmark, where our approach demonstrates improved convergence and outperforms state-of-the-art baselines in terms of both safety and task completion. Our results suggest that incorporating safety considerations can significantly enhance learning performance in complex, multi-agent environments.</li>
</ul>

<h3>Title: Optimizing Portfolio Performance through Clustering and Sharpe Ratio-Based Optimization: A Comparative Backtesting Approach</h3>
<ul>
<li><strong>Authors: </strong>Keon Vin Park</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.PM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12074">https://arxiv.org/abs/2501.12074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12074">https://arxiv.org/pdf/2501.12074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12074]] Optimizing Portfolio Performance through Clustering and Sharpe Ratio-Based Optimization: A Comparative Backtesting Approach(https://arxiv.org/abs/2501.12074)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Optimizing portfolio performance is a fundamental challenge in financial modeling, requiring the integration of advanced clustering techniques and data-driven optimization strategies. This paper introduces a comparative backtesting approach that combines clustering-based portfolio segmentation and Sharpe ratio-based optimization to enhance investment decision-making. First, we segment a diverse set of financial assets into clusters based on their historical log-returns using K-Means clustering. This segmentation enables the grouping of assets with similar return characteristics, facilitating targeted portfolio construction. Next, for each cluster, we apply a Sharpe ratio-based optimization model to derive optimal weights that maximize risk-adjusted returns. Unlike traditional mean-variance optimization, this approach directly incorporates the trade-off between returns and volatility, resulting in a more balanced allocation of resources within each cluster. The proposed framework is evaluated through a backtesting study using historical data spanning multiple asset classes. Optimized portfolios for each cluster are constructed and their cumulative returns are compared over time against a traditional equal-weighted benchmark portfolio.</li>
</ul>

<h3>Title: Phishing Awareness via Game-Based Learning</h3>
<ul>
<li><strong>Authors: </strong>Argianto Rahartomo, Ahmed Tareq Ali Ghaleb, Mohammad Ghafari</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12077">https://arxiv.org/abs/2501.12077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12077">https://arxiv.org/pdf/2501.12077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12077]] Phishing Awareness via Game-Based Learning(https://arxiv.org/abs/2501.12077)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>The increased use of digital devices and applications has led to a rise in phishing attacks. We develop a serious game to raise awareness about phishing attacks and help people avoid these threats in a risk-free learning environment. This game targets three types of phishing-clone phishing, SMS phishing, and spear phishing-and uses a Large Language Model to generate dialogues and questions dynamically. It also incorporates state randomization and time-limited challenges to enhance the gameplay. We evaluated two groups of participants and found that those who played the game showed, on average, a 24% increase in awareness and a 30% boost in confidence.</li>
</ul>

<h3>Title: Balance-Based Cryptography: Physically Computing Any Boolean Function</h3>
<ul>
<li><strong>Authors: </strong>Suthee Ruangwises</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12080">https://arxiv.org/abs/2501.12080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12080">https://arxiv.org/pdf/2501.12080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12080]] Balance-Based Cryptography: Physically Computing Any Boolean Function(https://arxiv.org/abs/2501.12080)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>Secure multi-party computation is an area in cryptography which studies how multiple parties can compare their private information without revealing it. Besides digital protocols, many physical protocols for secure multi-party computation using portable objects found in everyday life have also been developed. The vast majority of them use cards as the main tools. In this paper, we introduce the use of a balance scale and coins as new physical tools for secure multi-party computation. In particular, we develop four protocols that can securely compute any $n$-variable Boolean function using a balance scale and coins.</li>
</ul>

<h3>Title: Scalable Whole Slide Image Representation Using K-Mean Clustering and Fisher Vector Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Ravi Kant Gupta, Shounak Das, Ardhendu Sekhar, Amit Sethi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12085">https://arxiv.org/abs/2501.12085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12085">https://arxiv.org/pdf/2501.12085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12085]] Scalable Whole Slide Image Representation Using K-Mean Clustering and Fisher Vector Aggregation(https://arxiv.org/abs/2501.12085)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Whole slide images (WSIs) are high-resolution, gigapixel sized images that pose significant computational challenges for traditional machine learning models due to their size and this http URL this paper, we present a scalable and efficient methodology for WSI classification by leveraging patch-based feature extraction, clustering, and Fisher vector encoding. Initially, WSIs are divided into fixed size patches, and deep feature embeddings are extracted from each patch using a pre-trained convolutional neural network (CNN). These patch-level embeddings are subsequently clustered using K-means clustering, where each cluster aggregates semantically similar regions of the WSI. To effectively summarize each cluster, Fisher vector representations are computed by modeling the distribution of patch embeddings in each cluster as a parametric Gaussian mixture model (GMM). The Fisher vectors from each cluster are concatenated into a high-dimensional feature vector, creating a compact and informative representation of the entire WSI. This feature vector is then used by a classifier to predict the WSI's diagnostic label. Our method captures local and global tissue structures and yields robust performance for large-scale WSI classification, demonstrating superior accuracy and scalability compared to other approaches.</li>
</ul>

<h3>Title: DSTSA-GCN: Advancing Skeleton-Based Gesture Recognition with Semantic-Aware Spatio-Temporal Topology Modeling</h3>
<ul>
<li><strong>Authors: </strong>Hu Cui, Renjing Huang, Ruoyu Zhang, Tessai Hayama</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12086">https://arxiv.org/abs/2501.12086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12086">https://arxiv.org/pdf/2501.12086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12086]] DSTSA-GCN: Advancing Skeleton-Based Gesture Recognition with Semantic-Aware Spatio-Temporal Topology Modeling(https://arxiv.org/abs/2501.12086)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Graph convolutional networks (GCNs) have emerged as a powerful tool for skeleton-based action and gesture recognition, thanks to their ability to model spatial and temporal dependencies in skeleton data. However, existing GCN-based methods face critical limitations: (1) they lack effective spatio-temporal topology modeling that captures dynamic variations in skeletal motion, and (2) they struggle to model multiscale structural relationships beyond local joint connectivity. To address these issues, we propose a novel framework called Dynamic Spatial-Temporal Semantic Awareness Graph Convolutional Network (DSTSA-GCN). DSTSA-GCN introduces three key modules: Group Channel-wise Graph Convolution (GC-GC), Group Temporal-wise Graph Convolution (GT-GC), and Multi-Scale Temporal Convolution (MS-TCN). GC-GC and GT-GC operate in parallel to independently model channel-specific and frame-specific correlations, enabling robust topology learning that accounts for temporal variations. Additionally, both modules employ a grouping strategy to adaptively capture multiscale structural relationships. Complementing this, MS-TCN enhances temporal modeling through group-wise temporal convolutions with diverse receptive fields. Extensive experiments demonstrate that DSTSA-GCN significantly improves the topology modeling capabilities of GCNs, achieving state-of-the-art performance on benchmark datasets for gesture and action recognition, including SHREC17 Track, DHG-14\/28, NTU-RGB+D, and NTU-RGB+D-120.</li>
</ul>

<h3>Title: UAV-Assisted Real-Time Disaster Detection Using Optimized Transformer Model</h3>
<ul>
<li><strong>Authors: </strong>Branislava Jankovic, Sabina Jangirova, Waseem Ullah, Latif U. Khan, Mohsen Guizani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12087">https://arxiv.org/abs/2501.12087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12087">https://arxiv.org/pdf/2501.12087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12087]] UAV-Assisted Real-Time Disaster Detection Using Optimized Transformer Model(https://arxiv.org/abs/2501.12087)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, transformer</a></li>
<li><strong>Abstract: </strong>Disaster recovery and management present significant challenges, particularly in unstable environments and hard-to-reach terrains. These difficulties can be overcome by employing unmanned aerial vehicles (UAVs) equipped with onboard embedded platforms and camera sensors. In this work, we address the critical need for accurate and timely disaster detection by enabling onboard aerial imagery processing and avoiding connectivity, privacy, and latency issues despite the challenges posed by limited onboard hardware resources. We propose a UAV-assisted edge framework for real-time disaster management, leveraging our proposed model optimized for real-time aerial image classification. The optimization of the model employs post-training quantization techniques. For real-world disaster scenarios, we introduce a novel dataset, DisasterEye, featuring UAV-captured disaster scenes as well as ground-level images taken by individuals on-site. Experimental results demonstrate the effectiveness of our model, achieving high accuracy with reduced inference latency and memory usage on resource-constrained devices. The framework's scalability and adaptability make it a robust solution for real-time disaster detection on resource-limited UAV platforms.</li>
</ul>

<h3>Title: Proxies for Distortion and Consistency with Applications for Real-World Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Sean Man, Guy Ohayon, Ron Raphaeli, Michael Elad</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12102">https://arxiv.org/abs/2501.12102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12102">https://arxiv.org/pdf/2501.12102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12102]] Proxies for Distortion and Consistency with Applications for Real-World Image Restoration(https://arxiv.org/abs/2501.12102)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Real-world image restoration deals with the recovery of images suffering from an unknown degradation. This task is typically addressed while being given only degraded images, without their corresponding ground-truth versions. In this hard setting, designing and evaluating restoration algorithms becomes highly challenging. This paper offers a suite of tools that can serve both the design and assessment of real-world image restoration algorithms. Our work starts by proposing a trained model that predicts the chain of degradations a given real-world measured input has gone through. We show how this estimator can be used to approximate the consistency -- the match between the measurements and any proposed recovered image. We also use this estimator as a guiding force for the design of a simple and highly-effective plug-and-play real-world image restoration algorithm, leveraging a pre-trained diffusion-based image prior. Furthermore, this work proposes no-reference proxy measures of MSE and LPIPS, which, without access to the ground-truth images, allow ranking of real-world image restoration algorithms according to their (approximate) MSE and LPIPS. The proposed suite provides a versatile, first of its kind framework for evaluating and comparing blind image restoration algorithms in real-world scenarios.</li>
</ul>

<h3>Title: Teacher Encoder-Student Decoder Denoising Guided Segmentation Network for Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>ShiXuan Song, Hao Chen, Shu Hu, Xin Wang, Jinrong Hu, Xi Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12104">https://arxiv.org/abs/2501.12104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12104">https://arxiv.org/pdf/2501.12104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12104]] Teacher Encoder-Student Decoder Denoising Guided Segmentation Network for Anomaly Detection(https://arxiv.org/abs/2501.12104)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Visual anomaly detection is a highly challenging task, often categorized as a one-class classification and segmentation problem. Recent studies have demonstrated that the student-teacher (S-T) framework effectively addresses this challenge. However, most S-T frameworks rely solely on pre-trained teacher networks to guide student networks in learning multi-scale similar features, overlooking the potential of the student networks to enhance learning through multi-scale feature fusion. In this study, we propose a novel model named PFADSeg, which integrates a pre-trained teacher network, a denoising student network with multi-scale feature fusion, and a guided anomaly segmentation network into a unified framework. By adopting a unique teacher-encoder and student-decoder denoising mode, the model improves the student network's ability to learn from teacher network features. Furthermore, an adaptive feature fusion mechanism is introduced to train a self-supervised segmentation network that synthesizes anomaly masks autonomously, significantly increasing detection performance. Evaluated on the MVTec AD dataset, PFADSeg achieves state-of-the-art results with an image-level AUC of 98.9%, a pixel-level mean precision of 76.4%, and an instance-level mean precision of 78.7%.</li>
</ul>

<h3>Title: Can open source large language models be used for tumor documentation in Germany? -- An evaluation on urological doctors' notes</h3>
<ul>
<li><strong>Authors: </strong>Stefan Lenz, Arsenij Ustjanzew, Marco Jeray, Torsten Panholzer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12106">https://arxiv.org/abs/2501.12106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12106">https://arxiv.org/pdf/2501.12106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12106]] Can open source large language models be used for tumor documentation in Germany? -- An evaluation on urological doctors' notes(https://arxiv.org/abs/2501.12106)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Tumor documentation in Germany is largely done manually, requiring reading patient records and entering data into structured databases. Large language models (LLMs) could potentially enhance this process by improving efficiency and reliability. This evaluation tests eleven different open source LLMs with sizes ranging from 1-70 billion model parameters on three basic tasks of the tumor documentation process: identifying tumor diagnoses, assigning ICD-10 codes, and extracting the date of first diagnosis. For evaluating the LLMs on these tasks, a dataset of annotated text snippets based on anonymized doctors' notes from urology was prepared. Different prompting strategies were used to investigate the effect of the number of examples in few-shot prompting and to explore the capabilities of the LLMs in general. The models Llama 3.1 8B, Mistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks. Models with less extensive training data or having fewer than 7 billion parameters showed notably lower performance, while larger models did not display performance gains. Examples from a different medical domain than urology could also improve the outcome in few-shot prompting, which demonstrates the ability of LLMs to handle tasks needed for tumor documentation. Open source LLMs show a strong potential for automating tumor documentation. Models from 7-12 billion parameters could offer an optimal balance between performance and resource efficiency. With tailored fine-tuning and well-designed prompting, these models might become important tools for clinical documentation in the future. The code for the evaluation is available from this https URL. We also release the dataset as a new valuable resource that addresses the shortage of authentic and easily accessible benchmarks in German-language medical NLP.</li>
</ul>

<h3>Title: BotDetect: A Decentralized Federated Learning Framework for Detecting Financial Bots on the EVM Blockchains</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Mounsf Rafik Bendada, Abdelaziz Amara Korba, Mouhamed Amine Bouchiha, Yacine Ghamri-Doudane</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12112">https://arxiv.org/abs/2501.12112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12112">https://arxiv.org/pdf/2501.12112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12112]] BotDetect: A Decentralized Federated Learning Framework for Detecting Financial Bots on the EVM Blockchains(https://arxiv.org/abs/2501.12112)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>The rapid growth of decentralized finance (DeFi) has led to the widespread use of automated agents, or bots, within blockchain ecosystems like Ethereum, Binance Smart Chain, and Solana. While these bots enhance market efficiency and liquidity, they also raise concerns due to exploitative behaviors that threaten network integrity and user trust. This paper presents a decentralized federated learning (DFL) approach for detecting financial bots within Ethereum Virtual Machine (EVM)-based blockchains. The proposed framework leverages federated learning, orchestrated through smart contracts, to detect malicious bot behavior while preserving data privacy and aligning with the decentralized nature of blockchain networks. Addressing the limitations of both centralized and rule-based approaches, our system enables each participating node to train local models on transaction history and smart contract interaction data, followed by on-chain aggregation of model updates through a permissioned consensus mechanism. This design allows the model to capture complex and evolving bot behaviors without requiring direct data sharing between nodes. Experimental results demonstrate that our DFL framework achieves high detection accuracy while maintaining scalability and robustness, providing an effective solution for bot detection across distributed blockchain networks.</li>
</ul>

<h3>Title: FedCLEAN: byzantine defense by CLustering Errors of Activation maps in Non-IID federated learning environments</h3>
<ul>
<li><strong>Authors: </strong>Mehdi Ben Ghali, Reda Bellafqira, Gouenou Coatrieux</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12123">https://arxiv.org/abs/2501.12123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12123">https://arxiv.org/pdf/2501.12123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12123]] FedCLEAN: byzantine defense by CLustering Errors of Activation maps in Non-IID federated learning environments(https://arxiv.org/abs/2501.12123)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables clients to collaboratively train a global model using their local datasets while reinforcing data privacy. However, FL is susceptible to poisoning attacks. Existing defense mechanisms assume that clients' data are independent and identically distributed (IID), making them ineffective in real-world applications where data are non-IID. This paper presents FedCLEAN, the first defense capable of filtering attackers' model updates in a non-IID FL environment. The originality of FedCLEAN is twofold. First, it relies on a client confidence score derived from the reconstruction errors of each client's model activation maps for a given trigger set, with reconstruction errors obtained by means of a Conditional Variational Autoencoder trained according to a novel server-side strategy. Second, we propose an ad-hoc trust propagation algorithm based on client scores, which allows building a cluster of benign clients while flagging potential attackers. Experimental results on the datasets MNIST and FashionMNIST demonstrate the robustness of FedCLEAN against Byzantine attackers in non-IID scenarios and a close-to-zero benign client misclassification rate, even in the absence of an attack.</li>
</ul>

<h3>Title: Heterogeneous Federated Learning System for Sparse Healthcare Time-Series Prediction</h3>
<ul>
<li><strong>Authors: </strong>Jia-Hao Syu, Jerry Chun-Wei Lin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12125">https://arxiv.org/abs/2501.12125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12125">https://arxiv.org/pdf/2501.12125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12125]] Heterogeneous Federated Learning System for Sparse Healthcare Time-Series Prediction(https://arxiv.org/abs/2501.12125)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, federate</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a heterogeneous federated learning (HFL) system for sparse time series prediction in healthcare, which is a decentralized federated learning algorithm with heterogeneous transfers. We design dense and sparse feature tensors to deal with the sparsity of data sources. Heterogeneous federated learning is developed to share asynchronous parts of networks and select appropriate models for knowledge transfer. Experimental results show that the proposed HFL achieves the lowest prediction error among all benchmark systems on eight out of ten prediction tasks, with MSE reduction of 94.8%, 48.3%, and 52.1% compared to the benchmark systems. These results demonstrate the effectiveness of HFL in transferring knowledge from heterogeneous domains, especially in the smaller target domain. Ablation studies then demonstrate the effectiveness of the designed mechanisms for heterogeneous domain selection and switching in predicting healthcare time series with privacy, model security, and heterogeneous knowledge transfer.</li>
</ul>

<h3>Title: Distributed Multi-Head Learning Systems for Power Consumption Prediction</h3>
<ul>
<li><strong>Authors: </strong>Jia-Hao Syu, Jerry Chun-Wei Lin, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12133">https://arxiv.org/abs/2501.12133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12133">https://arxiv.org/pdf/2501.12133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12133]] Distributed Multi-Head Learning Systems for Power Consumption Prediction(https://arxiv.org/abs/2501.12133)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>As more and more automatic vehicles, power consumption prediction becomes a vital issue for task scheduling and energy management. Most research focuses on automatic vehicles in transportation, but few focus on automatic ground vehicles (AGVs) in smart factories, which face complex environments and generate large amounts of data. There is an inevitable trade-off between feature diversity and interference. In this paper, we propose Distributed Multi-Head learning (DMH) systems for power consumption prediction in smart factories. Multi-head learning mechanisms are proposed in DMH to reduce noise interference and improve accuracy. Additionally, DMH systems are designed as distributed and split learning, reducing the client-to-server transmission cost, sharing knowledge without sharing local data and models, and enhancing the privacy and security levels. Experimental results show that the proposed DMH systems rank in the top-2 on most datasets and scenarios. DMH-E system reduces the error of the state-of-the-art systems by 14.5% to 24.0%. Effectiveness studies demonstrate the effectiveness of Pearson correlation-based feature engineering, and feature grouping with the proposed multi-head learning further enhances prediction performance.</li>
</ul>

<h3>Title: Heterogeneous Federated Learning Systems for Time-Series Power Consumption Prediction with Multi-Head Embedding Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Jia-Hao Syu, Jerry Chun-Wei Lin, Gautam Srivastava, Unil Yun</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12136">https://arxiv.org/abs/2501.12136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12136">https://arxiv.org/pdf/2501.12136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12136]] Heterogeneous Federated Learning Systems for Time-Series Power Consumption Prediction with Multi-Head Embedding Mechanism(https://arxiv.org/abs/2501.12136)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Time-series prediction is increasingly popular in a variety of applications, such as smart factories and smart transportation. Researchers have used various techniques to predict power consumption, but existing models lack discussion of collaborative learning and privacy issues among multiple clients. To address these issues, we propose Multi-Head Heterogeneous Federated Learning (MHHFL) systems that consist of multiple head networks, which independently act as carriers for federated learning. In the federated period, each head network is embedded into 2-dimensional vectors and shared with the centralized source pool. MHHFL then selects appropriate source networks and blends the head networks as knowledge transfer in federated learning. The experimental results show that the proposed MHHFL systems significantly outperform the benchmark and state-of-the-art systems and reduce the prediction error by 24.9% to 94.1%. The ablation studies demonstrate the effectiveness of the proposed mechanisms in the MHHFL (head network embedding and selection mechanisms), which significantly outperforms traditional federated average and random transfer.</li>
</ul>

<h3>Title: Improving Influence-based Instruction Tuning Data Selection for Balanced Learning of Diverse Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Qirun Dai, Dylan Zhang, Jiaqi W. Ma, Hao Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12147">https://arxiv.org/abs/2501.12147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12147">https://arxiv.org/pdf/2501.12147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12147]] Improving Influence-based Instruction Tuning Data Selection for Balanced Learning of Diverse Capabilities(https://arxiv.org/abs/2501.12147)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Selecting appropriate training data is crucial for effective instruction fine-tuning of large language models (LLMs), which aims to (1) elicit strong capabilities, and (2) achieve balanced performance across a diverse range of tasks. Influence-based methods show promise in achieving (1) by estimating the contribution of each training example to the model's predictions, but often struggle with (2). Our systematic investigation reveals that this underperformance can be attributed to an inherent bias where certain tasks intrinsically have greater influence than others. As a result, data selection is often biased towards these tasks, not only hurting the model's performance on others but also, counterintuitively, harms performance on these high-influence tasks themselves. As a remedy, we propose BIDS, a Balanced and Influential Data Selection algorithm. BIDS first normalizes influence scores of the training data, and then iteratively balances data selection by choosing the training example with the highest influence on the most underrepresented task. Experiments with both Llama-3 and Mistral-v0.3 on seven benchmarks spanning five diverse capabilities show that BIDS consistently outperforms both state-of-the-art influence-based algorithms and other non-influence-based selection frameworks. Surprisingly, training on a 15% subset selected by BIDS can even outperform full-dataset training with a much more balanced performance. Our analysis further highlights the importance of both instance-level normalization and iterative optimization of selected data for balanced learning of diverse capabilities.</li>
</ul>

<h3>Title: Fast-RF-Shimming: Accelerate RF Shimming in 7T MRI using Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhengyi Lu, Hao Liang, Ming Lu, Xiao Wang, Xinqiang Yan, Yuankai Huo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12157">https://arxiv.org/abs/2501.12157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12157">https://arxiv.org/pdf/2501.12157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12157]] Fast-RF-Shimming: Accelerate RF Shimming in 7T MRI using Deep Learning(https://arxiv.org/abs/2501.12157)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Ultrahigh field (UHF) Magnetic Resonance Imaging (MRI) provides a high signal-to-noise ratio (SNR), enabling exceptional spatial resolution for clinical diagnostics and research. However, higher fields introduce challenges such as transmit radiofrequency (RF) field inhomogeneities, which result in uneven flip angles and image intensity artifacts. These artifacts degrade image quality and limit clinical adoption. Traditional RF shimming methods, including Magnitude Least Squares (MLS) optimization, mitigate RF field inhomogeneity but are time-intensive and often require the presence of the patient. Recent machine learning methods, such as RF Shim Prediction by Iteratively Projected Ridge Regression and other deep learning architectures, offer alternative approaches but face challenges such as extensive training requirements, limited complexity, and practical data constraints. This paper introduces a holistic learning-based framework called Fast RF Shimming, which achieves a 5000-fold speedup compared to MLS methods. First, random-initialized Adaptive Moment Estimation (Adam) derives reference shimming weights from multichannel RF fields. Next, a Residual Network (ResNet) maps RF fields to shimming outputs while incorporating a confidence parameter into the loss function. Finally, a Non-uniformity Field Detector (NFD) identifies extreme non-uniform outcomes. Comparative evaluations demonstrate significant improvements in both speed and predictive accuracy. The proposed pipeline also supports potential extensions, such as the integration of anatomical priors or multi-echo data, to enhance the robustness of RF field correction. This approach offers a faster and more efficient solution to RF shimming challenges in UHF MRI.</li>
</ul>

<h3>Title: SVGS-DSGAT: An IoT-Enabled Innovation in Underwater Robotic Object Detection Technology</h3>
<ul>
<li><strong>Authors: </strong>Dongli Wu, Ling Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12169">https://arxiv.org/abs/2501.12169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12169">https://arxiv.org/pdf/2501.12169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12169]] SVGS-DSGAT: An IoT-Enabled Innovation in Underwater Robotic Object Detection Technology(https://arxiv.org/abs/2501.12169)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>With the advancement of Internet of Things (IoT) technology, underwater target detection and tracking have become increasingly important for ocean monitoring and resource management. Existing methods often fall short in handling high-noise and low-contrast images in complex underwater environments, lacking precision and robustness. This paper introduces a novel SVGS-DSGAT model that combines GraphSage, SVAM, and DSGAT modules, enhancing feature extraction and target detection capabilities through graph neural networks and attention mechanisms. The model integrates IoT technology to facilitate real-time data collection and processing, optimizing resource allocation and model responsiveness. Experimental results demonstrate that the SVGS-DSGAT model achieves an mAP of 40.8% on the URPC 2020 dataset and 41.5% on the SeaDronesSee dataset, significantly outperforming existing mainstream models. This IoT-enhanced approach not only excels in high-noise and complex backgrounds but also improves the overall efficiency and scalability of the system. This research provides an effective IoT solution for underwater target detection technology, offering significant practical application value and broad development prospects.</li>
</ul>

<h3>Title: ComposeAnyone: Controllable Layout-to-Human Generation with Decoupled Multimodal Conditions</h3>
<ul>
<li><strong>Authors: </strong>Shiyue Zhang, Zheng Chong, Xi Lu, Wenqing Zhang, Haoxiang Li, Xujie Zhang, Jiehui Huang, Xiao Dong, Xiaodan Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12173">https://arxiv.org/abs/2501.12173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12173">https://arxiv.org/pdf/2501.12173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12173]] ComposeAnyone: Controllable Layout-to-Human Generation with Decoupled Multimodal Conditions(https://arxiv.org/abs/2501.12173)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Building on the success of diffusion models, significant advancements have been made in multimodal image generation tasks. Among these, human image generation has emerged as a promising technique, offering the potential to revolutionize the fashion design process. However, existing methods often focus solely on text-to-image or image reference-based human generation, which fails to satisfy the increasingly sophisticated demands. To address the limitations of flexibility and precision in human generation, we introduce ComposeAnyone, a controllable layout-to-human generation method with decoupled multimodal conditions. Specifically, our method allows decoupled control of any part in hand-drawn human layouts using text or reference images, seamlessly integrating them during the generation process. The hand-drawn layout, which utilizes color-blocked geometric shapes such as ellipses and rectangles, can be easily drawn, offering a more flexible and accessible way to define spatial layouts. Additionally, we introduce the ComposeHuman dataset, which provides decoupled text and reference image annotations for different components of each human image, enabling broader applications in human image generation tasks. Extensive experiments on multiple datasets demonstrate that ComposeAnyone generates human images with better alignment to given layouts, text descriptions, and reference images, showcasing its multi-task capability and controllability.</li>
</ul>

<h3>Title: BiMarker: Enhancing Text Watermark Detection for Large Language Models with Bipolar Watermarks</h3>
<ul>
<li><strong>Authors: </strong>Zhuang Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12174">https://arxiv.org/abs/2501.12174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12174">https://arxiv.org/pdf/2501.12174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12174]] BiMarker: Enhancing Text Watermark Detection for Large Language Models with Bipolar Watermarks(https://arxiv.org/abs/2501.12174)</code><input type="text"></li>
<li><strong>Keywords: </strong>watermark, large language model</a></li>
<li><strong>Abstract: </strong>The rapid proliferation of Large Language Models (LLMs) has raised concerns about misuse and the challenges of distinguishing AI-generated text from human-written content. Existing watermarking techniques, such as \kgw, still face limitations under low watermark strength, stringent false-positive requirements, and low-entropy scenarios. Our analysis reveals that current detection methods rely on coarse estimates of non-watermarked text, which constrains watermark detectability. We propose the Bipolar Watermark (BiMarker), a novel approach that divides generated text into positive and negative poles, leveraging the difference in green token counts for detection. This differential mechanism significantly enhances the detectability of watermarked text. Theoretical analysis and experimental results demonstrate BiMarker's effectiveness and compatibility with existing optimization techniques, offering a new optimization dimension for watermarking in LLM-generated content.</li>
</ul>

<h3>Title: Extend Adversarial Policy Against Neural Machine Translation via Unknown Token</h3>
<ul>
<li><strong>Authors: </strong>Wei Zou, Shujian Huang, Jiajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12183">https://arxiv.org/abs/2501.12183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12183">https://arxiv.org/pdf/2501.12183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12183]] Extend Adversarial Policy Against Neural Machine Translation via Unknown Token(https://arxiv.org/abs/2501.12183)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Generating adversarial examples contributes to mainstream neural machine translation~(NMT) robustness. However, popular adversarial policies are apt for fixed tokenization, hindering its efficacy for common character perturbations involving versatile tokenization. Based on existing adversarial generation via reinforcement learning~(RL), we propose the `DexChar policy' that introduces character perturbations for the existing mainstream adversarial policy based on token substitution. Furthermore, we improve the self-supervised matching that provides feedback in RL to cater to the semantic constraints required during training adversaries. Experiments show that our method is compatible with the scenario where baseline adversaries fail, and can generate high-efficiency adversarial examples for analysis and optimization of the system.</li>
</ul>

<h3>Title: A margin-based replacement for cross-entropy loss</h3>
<ul>
<li><strong>Authors: </strong>Michael W. Spratling, Heiko H. Schütt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12191">https://arxiv.org/abs/2501.12191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12191">https://arxiv.org/pdf/2501.12191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12191]] A margin-based replacement for cross-entropy loss(https://arxiv.org/abs/2501.12191)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Cross-entropy (CE) loss is the de-facto standard for training deep neural networks to perform classification. However, CE-trained deep neural networks struggle with robustness and generalisation issues. To alleviate these issues, we propose high error margin (HEM) loss, a variant of multi-class margin loss that overcomes the training issues of other margin-based losses. We evaluate HEM extensively on a range of architectures and datasets. We find that HEM loss is more effective than cross-entropy loss across a wide range of tasks: unknown class rejection, adversarial robustness, learning with imbalanced data, continual learning, and semantic segmentation (a pixel-level classification task). Despite all training hyper-parameters being chosen for CE loss, HEM is inferior to CE only in terms of clean accuracy and this difference is insignificant. We also compare HEM to specialised losses that have previously been proposed to improve performance on specific tasks. LogitNorm, a loss achieving state-of-the-art performance on unknown class rejection, produces similar performance to HEM for this task, but is much poorer for continual learning and semantic segmentation. Logit-adjusted loss, designed for imbalanced data, has superior results to HEM for that task, but performs more poorly on unknown class rejection and semantic segmentation. DICE, a popular loss for semantic segmentation, is inferior to HEM loss on all tasks, including semantic segmentation. Thus, HEM often out-performs specialised losses, and in contrast to them, is a general-purpose replacement for CE loss.</li>
</ul>

<h3>Title: MyDigiTwin: A Privacy-Preserving Framework for Personalized Cardiovascular Risk Prediction and Scenario Exploration</h3>
<ul>
<li><strong>Authors: </strong>Héctor Cadavid, Hyunho Mo, Bauke Arends, Katarzyna Dziopa, Esther E. Bron, Daniel Bos, Sonja Georgievska, Pim van der Harst</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12193">https://arxiv.org/abs/2501.12193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12193">https://arxiv.org/pdf/2501.12193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12193]] MyDigiTwin: A Privacy-Preserving Framework for Personalized Cardiovascular Risk Prediction and Scenario Exploration(https://arxiv.org/abs/2501.12193)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Cardiovascular disease (CVD) remains a leading cause of death, and primary prevention through personalized interventions is crucial. This paper introduces MyDigiTwin, a framework that integrates health digital twins with personal health environments to empower patients in exploring personalized health scenarios while ensuring data privacy. MyDigiTwin uses federated learning to train predictive models across distributed datasets without transferring raw data, and a novel data harmonization framework addresses semantic and format inconsistencies in health data. A proof-of-concept demonstrates the feasibility of harmonizing and using cohort data to train privacy-preserving CVD prediction models. This framework offers a scalable solution for proactive, personalized cardiovascular care and sets the stage for future applications in real-world healthcare settings.</li>
</ul>

<h3>Title: Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation</h3>
<ul>
<li><strong>Authors: </strong>Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, Huiwen Shi, Sicong Liu, Junta Wu, Yihang Lian, Fan Yang, Ruining Tang, Zebin He, Xinzhou Wang, Jian Liu, Xuhui Zuo, Zhuo Chen, Biwen Lei, Haohan Weng, Jing Xu, Yiling Zhu, Xinhai Liu, Lixin Xu, Changrong Hu, Tianyu Huang, Lifu Wang, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Yulin Cai, Jiaao Yu, Yixuan Tang, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Chao Zhang, Yonghao Tan, Jie Xiao, Yangyu Tao, Jianchen Zhu, Jinbao Xue, Kai Liu, Chongqing Zhao, Xinming Wu, Zhichao Hu, Lei Qin, Jianbing Peng, Zhan Li, Minghui Chen, Xipeng Zhang, Lin Niu, Paige Wang, Yingkai Wang, Haozhao Kuang, Zhongyi Fan, Xu Zheng, Weihao Zhuang, YingPing He, Tian Liu, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, Jingwei Huang, Chunchao Guo (refer to the report for detailed contributions)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12202">https://arxiv.org/abs/2501.12202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12202">https://arxiv.org/pdf/2501.12202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12202]] Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation(https://arxiv.org/abs/2501.12202)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for generating high-resolution textured 3D assets. This system includes two foundation components: a large-scale shape generation model -- Hunyuan3D-DiT, and a large-scale texture synthesis model -- Hunyuan3D-Paint. The shape generative model, built on a scalable flow-based diffusion transformer, aims to create geometry that properly aligns with a given condition image, laying a solid foundation for downstream applications. The texture synthesis model, benefiting from strong geometric and diffusion priors, produces high-resolution and vibrant texture maps for either generated or hand-crafted meshes. Furthermore, we build Hunyuan3D-Studio -- a versatile, user-friendly production platform that simplifies the re-creation process of 3D assets. It allows both professional and amateur users to manipulate or even animate their meshes efficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0 outperforms previous state-of-the-art models, including the open-source models and closed-source models in geometry details, condition alignment, texture quality, and etc. Hunyuan3D 2.0 is publicly released in order to fill the gaps in the open-source 3D community for large-scale foundation generative models. The code and pre-trained weights of our models are available at: this https URL</li>
</ul>

<h3>Title: Explainability for Vision Foundation Models: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Rémi Kazmierczak, Eloïse Berthier, Goran Frehse, Gianni Franchi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12203">https://arxiv.org/abs/2501.12203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12203">https://arxiv.org/pdf/2501.12203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12203]] Explainability for Vision Foundation Models: A Survey(https://arxiv.org/abs/2501.12203)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>As artificial intelligence systems become increasingly integrated into daily life, the field of explainability has gained significant attention. This trend is particularly driven by the complexity of modern AI models and their decision-making processes. The advent of foundation models, characterized by their extensive generalization capabilities and emergent uses, has further complicated this landscape. Foundation models occupy an ambiguous position in the explainability domain: their complexity makes them inherently challenging to interpret, yet they are increasingly leveraged as tools to construct explainable models. In this survey, we explore the intersection of foundation models and eXplainable AI (XAI) in the vision domain. We begin by compiling a comprehensive corpus of papers that bridge these fields. Next, we categorize these works based on their architectural characteristics. We then discuss the challenges faced by current research in integrating XAI within foundation models. Furthermore, we review common evaluation methodologies for these combined approaches. Finally, we present key observations and insights from our survey, offering directions for future research in this rapidly evolving field.</li>
</ul>

<h3>Title: Fixing Imbalanced Attention to Mitigate In-Context Hallucination of Large Vision-Language Model</h3>
<ul>
<li><strong>Authors: </strong>Kazi Hasan Ibn Arif, Sajib Acharjee Dip, Khizar Hussain, Lang Zhang, Chris Thomas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12206">https://arxiv.org/abs/2501.12206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12206">https://arxiv.org/pdf/2501.12206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12206]] Fixing Imbalanced Attention to Mitigate In-Context Hallucination of Large Vision-Language Model(https://arxiv.org/abs/2501.12206)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities in understanding and describing visual content, achieving state-of-the-art performance across various vision-language tasks. However, these models frequently exhibit hallucination behavior, where they generate descriptions containing objects or details absent in the input image. Our work investigates this phenomenon by analyzing attention patterns across transformer layers and heads, revealing that hallucinations often stem from progressive degradation of visual grounding in deeper layers. We propose a novel attention modification approach that combines selective token emphasis and head-specific modulation to maintain visual grounding throughout the generation process. Our method introduces two key components: (1) a dual-stream token selection mechanism that identifies and prioritizes both locally informative and spatially significant visual tokens, and (2) an attention head-specific modulation strategy that differentially amplifies visual information processing based on measured visual sensitivity of individual attention heads. Through extensive experimentation on the MSCOCO dataset, we demonstrate that our approach reduces hallucination rates by up to 62.3\% compared to baseline models while maintaining comparable task performance. Our analysis reveals that selectively modulating tokens across attention heads with varying levels of visual sensitivity can significantly improve visual grounding without requiring model retraining.</li>
</ul>

<h3>Title: You Can't Eat Your Cake and Have It Too: The Performance Degradation of LLMs with Jailbreak Defense</h3>
<ul>
<li><strong>Authors: </strong>Wuyuao Mai, Geng Hong, Pei Chen, Xudong Pan, Baojun Liu, Yuan Zhang, Haixin Duan, Min Yang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12210">https://arxiv.org/abs/2501.12210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12210">https://arxiv.org/pdf/2501.12210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12210]] You Can't Eat Your Cake and Have It Too: The Performance Degradation of LLMs with Jailbreak Defense(https://arxiv.org/abs/2501.12210)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, generative, large language model</a></li>
<li><strong>Abstract: </strong>With the rise of generative large language models (LLMs) like LLaMA and ChatGPT, these models have significantly transformed daily life and work by providing advanced insights. However, as jailbreak attacks continue to circumvent built-in safety mechanisms, exploiting carefully crafted scenarios or tokens, the safety risks of LLMs have come into focus. While numerous defense strategies--such as prompt detection, modification, and model fine-tuning--have been proposed to counter these attacks, a critical question arises: do these defenses compromise the utility and usability of LLMs for legitimate users? Existing research predominantly focuses on the effectiveness of defense strategies without thoroughly examining their impact on performance, leaving a gap in understanding the trade-offs between LLM safety and performance. Our research addresses this gap by conducting a comprehensive study on the utility degradation, safety elevation, and exaggerated-safety escalation of LLMs with jailbreak defense strategies. We propose USEBench, a novel benchmark designed to evaluate these aspects, along with USEIndex, a comprehensive metric for assessing overall model performance. Through experiments on seven state-of-the-art LLMs, we found that mainstream jailbreak defenses fail to ensure both safety and performance simultaneously. Although model-finetuning performs the best overall, their effectiveness varies across LLMs. Furthermore, vertical comparisons reveal that developers commonly prioritize performance over safety when iterating or fine-tuning their LLMs.</li>
</ul>

<h3>Title: Automatic selection of the best neural architecture for time series forecasting via multi-objective optimization and Pareto optimality conditions</h3>
<ul>
<li><strong>Authors: </strong>Qianying Cao, Shanqing Liu, Alan John Varghese, Jerome Darbon, Michael Triantafyllou, George Em Karniadakis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12215">https://arxiv.org/abs/2501.12215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12215">https://arxiv.org/pdf/2501.12215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12215]] Automatic selection of the best neural architecture for time series forecasting via multi-objective optimization and Pareto optimality conditions(https://arxiv.org/abs/2501.12215)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Time series forecasting plays a pivotal role in a wide range of applications, including weather prediction, healthcare, structural health monitoring, predictive maintenance, energy systems, and financial markets. While models such as LSTM, GRU, Transformers, and State-Space Models (SSMs) have become standard tools in this domain, selecting the optimal architecture remains a challenge. Performance comparisons often depend on evaluation metrics and the datasets under analysis, making the choice of a universally optimal model controversial. In this work, we introduce a flexible automated framework for time series forecasting that systematically designs and evaluates diverse network architectures by integrating LSTM, GRU, multi-head Attention, and SSM blocks. Using a multi-objective optimization approach, our framework determines the number, sequence, and combination of blocks to align with specific requirements and evaluation objectives. From the resulting Pareto-optimal architectures, the best model for a given context is selected via a user-defined preference function. We validate our framework across four distinct real-world applications. Results show that a single-layer GRU or LSTM is usually optimal when minimizing training time alone. However, when maximizing accuracy or balancing multiple objectives, the best architectures are often composite designs incorporating multiple block types in specific configurations. By employing a weighted preference function, users can resolve trade-offs between objectives, revealing novel, context-specific optimal architectures. Our findings underscore that no single neural architecture is universally optimal for time series forecasting. Instead, the best-performing model emerges as a data-driven composite architecture tailored to user-defined criteria and evaluation objectives.</li>
</ul>

<h3>Title: RL-RC-DoT: A Block-level RL agent for Task-Aware Video Compression</h3>
<ul>
<li><strong>Authors: </strong>Uri Gadot, Assaf Shocher, Shie Mannor, Gal Chechik, Assaf Hallak</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12216">https://arxiv.org/abs/2501.12216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12216">https://arxiv.org/pdf/2501.12216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12216]] RL-RC-DoT: A Block-level RL agent for Task-Aware Video Compression(https://arxiv.org/abs/2501.12216)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Video encoders optimize compression for human perception by minimizing reconstruction error under bit-rate constraints. In many modern applications such as autonomous driving, an overwhelming majority of videos serve as input for AI systems performing tasks like object recognition or segmentation, rather than being watched by humans. It is therefore useful to optimize the encoder for a downstream task instead of for perceptual image quality. However, a major challenge is how to combine such downstream optimization with existing standard video encoders, which are highly efficient and popular. Here, we address this challenge by controlling the Quantization Parameters (QPs) at the macro-block level to optimize the downstream task. This granular control allows us to prioritize encoding for task-relevant regions within each frame. We formulate this optimization problem as a Reinforcement Learning (RL) task, where the agent learns to balance long-term implications of choosing QPs on both task performance and bit-rate constraints. Notably, our policy does not require the downstream task as an input during inference, making it suitable for streaming applications and edge devices such as vehicles. We demonstrate significant improvements in two tasks, car detection, and ROI (saliency) encoding. Our approach improves task performance for a given bit rate compared to traditional task agnostic encoding methods, paving the way for more efficient task-aware video compression.</li>
</ul>

<h3>Title: Exploring Temporally-Aware Features for Point Tracking</h3>
<ul>
<li><strong>Authors: </strong>Inès Hyeonsu Kim, Seokju Cho, Jiahui Huang, Jung Yi, Joon-Young Lee, Seungryong Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12218">https://arxiv.org/abs/2501.12218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12218">https://arxiv.org/pdf/2501.12218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12218]] Exploring Temporally-Aware Features for Point Tracking(https://arxiv.org/abs/2501.12218)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Point tracking in videos is a fundamental task with applications in robotics, video editing, and more. While many vision tasks benefit from pre-trained feature backbones to improve generalizability, point tracking has primarily relied on simpler backbones trained from scratch on synthetic data, which may limit robustness in real-world scenarios. Additionally, point tracking requires temporal awareness to ensure coherence across frames, but using temporally-aware features is still underexplored. Most current methods often employ a two-stage process: an initial coarse prediction followed by a refinement stage to inject temporal information and correct errors from the coarse stage. These approach, however, is computationally expensive and potentially redundant if the feature backbone itself captures sufficient temporal information. In this work, we introduce Chrono, a feature backbone specifically designed for point tracking with built-in temporal awareness. Leveraging pre-trained representations from self-supervised learner DINOv2 and enhanced with a temporal adapter, Chrono effectively captures long-term temporal context, enabling precise prediction even without the refinement stage. Experimental results demonstrate that Chrono achieves state-of-the-art performance in a refiner-free setting on the TAP-Vid-DAVIS and TAP-Vid-Kinetics datasets, among common feature backbones used in point tracking as well as DINOv2, with exceptional efficiency. Project page: this https URL</li>
</ul>

<h3>Title: TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space</h3>
<ul>
<li><strong>Authors: </strong>Daniel Garibi, Shahar Yadin, Roni Paiss, Omer Tov, Shiran Zada, Ariel Ephrat, Tomer Michaeli, Inbar Mosseri, Tali Dekel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12224">https://arxiv.org/abs/2501.12224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12224">https://arxiv.org/pdf/2501.12224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12224]] TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space(https://arxiv.org/abs/2501.12224)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present TokenVerse -- a method for multi-concept personalization, leveraging a pre-trained text-to-image diffusion model. Our framework can disentangle complex visual elements and attributes from as little as a single image, while enabling seamless plug-and-play generation of combinations of concepts extracted from multiple images. As opposed to existing works, TokenVerse can handle multiple images with multiple concepts each, and supports a wide-range of concepts, including objects, accessories, materials, pose, and lighting. Our work exploits a DiT-based text-to-image model, in which the input text affects the generation through both attention and modulation (shift and scale). We observe that the modulation space is semantic and enables localized control over complex concepts. Building on this insight, we devise an optimization-based framework that takes as input an image and a text description, and finds for each word a distinct direction in the modulation space. These directions can then be used to generate new images that combine the learned concepts in a desired configuration. We demonstrate the effectiveness of TokenVerse in challenging personalization settings, and showcase its advantages over existing methods. project's webpage in this https URL</li>
</ul>

<h3>Title: CDW-CoT: Clustered Distance-Weighted Chain-of-Thoughts Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yuanheng Fang, Guoqing Chao, Wenqiang Lei, Shaobo Li, Dianhui Chu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12226">https://arxiv.org/abs/2501.12226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12226">https://arxiv.org/pdf/2501.12226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12226]] CDW-CoT: Clustered Distance-Weighted Chain-of-Thoughts Reasoning(https://arxiv.org/abs/2501.12226)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have recently achieved impressive results in complex reasoning tasks through Chain of Thought (CoT) prompting. However, most existing CoT methods rely on using the same prompts, whether manually designed or automatically generated, to handle the entire dataset. This one-size-fits-all approach may fail to meet the specific needs arising from the diversities within a single dataset. To solve this problem, we propose the Clustered Distance-Weighted Chain of Thought (CDW-CoT) method, which dynamically constructs prompts tailored to the characteristics of each data instance by integrating clustering and prompt optimization techniques. Our method employs clustering algorithms to categorize the dataset into distinct groups, from which a candidate pool of prompts is selected to reflect the inherent diversity within the dataset. For each cluster, CDW-CoT trains the optimal prompt probability distribution tailored to their specific characteristics. Finally, it dynamically constructs a unique prompt probability distribution for each test instance, based on its proximity to cluster centers, from which prompts are selected for reasoning. CDW-CoT consistently outperforms traditional CoT methods across six datasets, including commonsense, symbolic, and mathematical reasoning tasks. Specifically, when compared to manual CoT, CDW-CoT achieves an average accuracy improvement of 25.34% on LLaMA2 (13B) and 15.72% on LLaMA3 (8B).</li>
</ul>

<h3>Title: Empower Healthcare through a Self-Sovereign Identity Infrastructure for Secure Electronic Health Data Access</h3>
<ul>
<li><strong>Authors: </strong>Antonio López Martínez, Montassar Naghmouchi, Maryline Laurent, Joaquin Garcia-Alfaro, Manuel Gil Pérez, Antonio Ruiz Martínez, Pantaleone Nespoli</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12229">https://arxiv.org/abs/2501.12229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12229">https://arxiv.org/pdf/2501.12229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12229]] Empower Healthcare through a Self-Sovereign Identity Infrastructure for Secure Electronic Health Data Access(https://arxiv.org/abs/2501.12229)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect</a></li>
<li><strong>Abstract: </strong>Health data is one of the most sensitive data for people, which attracts the attention of malicious activities. We propose an open-source health data management framework, that follows a patient-centric approach. The proposed framework implements the Self-Sovereign Identity paradigm with innovative technologies such as Decentralized Identifiers and Verifiable Credentials. The framework uses Blockchain technology to provide immutability, verifiable data registry, and auditability, as well as an agent-based model to provide protection and privacy for the patient data. We also define different use cases regarding the daily patient-practitioner-laboratory interactions and specific functions to cover patient data loss, data access revocation, and emergency cases where patients are unable to give consent and access to their data. To address this design, a proof of concept is created with an interaction between patient and doctor. The most feasible technologies are selected and the created design is validated. We discuss the differences and novelties of this framework, which includes the patient-centric approach also for data storage, the designed recovery and emergency plan, the defined backup procedure, and the selected blockchain platform.</li>
</ul>

<h3>Title: InsTALL: Context-aware Instructional Task Assistance with Multi-modal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Pha Nguyen, Sailik Sengupta, Girik Malik, Arshit Gupta, Bonan Min</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12231">https://arxiv.org/abs/2501.12231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12231">https://arxiv.org/pdf/2501.12231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12231]] InsTALL: Context-aware Instructional Task Assistance with Multi-modal Large Language Models(https://arxiv.org/abs/2501.12231)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The improved competence of generative models can help building multi-modal virtual assistants that leverage modalities beyond language. By observing humans performing multi-step tasks, one can build assistants that have situational awareness of actions and tasks being performed, enabling them to cater assistance based on this understanding. In this paper, we develop a Context-aware Instructional Task Assistant with Multi-modal Large Language Models (InsTALL) that leverages an online visual stream (e.g. a user's screen share or video recording) and responds in real-time to user queries related to the task at hand. To enable useful assistance, InsTALL 1) trains a multi-modal model on task videos and paired textual data, and 2) automatically extracts task graph from video data and leverages it at training and inference time. We show InsTALL achieves state-of-the-art performance across proposed sub-tasks considered for multimodal activity understanding -- task recognition (TR), action recognition (AR), next action prediction (AP), and plan prediction (PP) -- and outperforms existing baselines on two novel sub-tasks related to automatic error identification.</li>
</ul>

<h3>Title: DLEN: Dual Branch of Transformer for Low-Light Image Enhancement in Dual Domains</h3>
<ul>
<li><strong>Authors: </strong>Junyu Xia, Jiesong Bai, Yihang Dong</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12235">https://arxiv.org/abs/2501.12235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12235">https://arxiv.org/pdf/2501.12235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12235]] DLEN: Dual Branch of Transformer for Low-Light Image Enhancement in Dual Domains(https://arxiv.org/abs/2501.12235)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Low-light image enhancement (LLE) aims to improve the visual quality of images captured in poorly lit conditions, which often suffer from low brightness, low contrast, noise, and color distortions. These issues hinder the performance of computer vision tasks such as object detection, facial recognition, and autonomous this http URL enhancement techniques, such as multi-scale fusion and histogram equalization, fail to preserve fine details and often struggle with maintaining the natural appearance of enhanced images under complex lighting conditions. Although the Retinex theory provides a foundation for image decomposition, it often amplifies noise, leading to suboptimal image quality. In this paper, we propose the Dual Light Enhance Network (DLEN), a novel architecture that incorporates two distinct attention mechanisms, considering both spatial and frequency domains. Our model introduces a learnable wavelet transform module in the illumination estimation phase, preserving high- and low-frequency components to enhance edge and texture details. Additionally, we design a dual-branch structure that leverages the power of the Transformer architecture to enhance both the illumination and structural components of the this http URL extensive experiments, our model outperforms state-of-the-art methods on standard this http URL is available here: this https URL</li>
</ul>

<h3>Title: FOCUS: First Order Concentrated Updating Scheme</h3>
<ul>
<li><strong>Authors: </strong>Yizhou Liu, Ziming Liu, Jeff Gore</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12243">https://arxiv.org/abs/2501.12243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12243">https://arxiv.org/pdf/2501.12243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12243]] FOCUS: First Order Concentrated Updating Scheme(https://arxiv.org/abs/2501.12243)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate remarkable performance, and improving their pre-training process appears to be key to enhancing their capabilities further. Based on the documented success of Adam, learning rate decay, and weight decay, we hypothesize that the pre-training loss landscape features a narrowing valley structure. Through experiments with synthetic loss functions, we discover that when gradient query noise is high relative to the valley's sharpness, Adam's performance falls behind that of Signum because Adam reduces the effective step size too drastically. This observation led us to develop FOCUS, an optimizer that enhances Signum by incorporating attraction toward moving averaged parameters, allowing it to handle noise better while maintaining larger step sizes. In training GPT-2, FOCUS proves to be more stable than Signum and faster than Adam. These results suggest that gradient noise may be an underappreciated limiting factor in LLM training, and FOCUS offers promising solutions.</li>
</ul>

<h3>Title: Video Deblurring by Sharpness Prior Detection and Edge Information</h3>
<ul>
<li><strong>Authors: </strong>Yang Tian, Fabio Brau, Giulio Rossolini, Giorgio Buttazzo, Hao Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12246">https://arxiv.org/abs/2501.12246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12246">https://arxiv.org/pdf/2501.12246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12246]] Video Deblurring by Sharpness Prior Detection and Edge Information(https://arxiv.org/abs/2501.12246)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, extraction</a></li>
<li><strong>Abstract: </strong>Video deblurring is essential task for autonomous driving, facial recognition, and security surveillance. Traditional methods directly estimate motion blur kernels, often introducing artifacts and leading to poor results. Recent approaches utilize the detection of sharp frames within video sequences to enhance deblurring. However, existing datasets rely on fixed number of sharp frames, which may be too restrictive for some applications and may introduce a bias during model training. To address these limitations and enhance domain adaptability, this work first introduces GoPro Random Sharp (GoProRS), a new dataset where the the frequency of sharp frames within the sequence is customizable, allowing more diverse training and testing scenarios. Furthermore, it presents a novel video deblurring model, called SPEINet, that integrates sharp frame features into blurry frame reconstruction through an attention-based encoder-decoder architecture, a lightweight yet robust sharp frame detection and an edge extraction phase. Extensive experimental results demonstrate that SPEINet outperforms state-of-the-art methods across multiple datasets, achieving an average of +3.2% PSNR improvement over recent techniques. Given such promising results, we believe that both the proposed model and dataset pave the way for future advancements in video deblurring based on the detection of sharp frames.</li>
</ul>

<h3>Title: Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos</h3>
<ul>
<li><strong>Authors: </strong>Yanlai Yang, Mengye Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12254">https://arxiv.org/abs/2501.12254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12254">https://arxiv.org/pdf/2501.12254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12254]] Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos(https://arxiv.org/abs/2501.12254)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Self-supervised learning holds the promise to learn good representations from real-world continuous uncurated data streams. However, most existing works in visual self-supervised learning focus on static images or artificial data streams. Towards exploring a more realistic learning substrate, we investigate streaming self-supervised learning from long-form real-world egocentric video streams. Inspired by the event segmentation mechanism in human perception and memory, we propose "Memory Storyboard" that groups recent past frames into temporal segments for more effective summarization of the past visual streams for memory replay. To accommodate efficient temporal segmentation, we propose a two-tier memory hierarchy: the recent past is stored in a short-term memory, and the storyboard temporal segments are then transferred to a long-term memory. Experiments on real-world egocentric video datasets including SAYCam and KrishnaCam show that contrastive learning objectives on top of storyboard frames result in semantically meaningful representations which outperform those produced by state-of-the-art unsupervised continual learning methods.</li>
</ul>

<h3>Title: mmCooper: A Multi-agent Multi-stage Communication-efficient and Collaboration-robust Cooperative Perception Framework</h3>
<ul>
<li><strong>Authors: </strong>Bingyi Liu, Jian Teng, Hongfei Xue, Enshu Wang, Chuanhui Zhu, Pu Wang, Libing Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12263">https://arxiv.org/abs/2501.12263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12263">https://arxiv.org/pdf/2501.12263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12263]] mmCooper: A Multi-agent Multi-stage Communication-efficient and Collaboration-robust Cooperative Perception Framework(https://arxiv.org/abs/2501.12263)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Collaborative perception significantly enhances individual vehicle perception performance through the exchange of sensory information among agents. However, real-world deployment faces challenges due to bandwidth constraints and inevitable calibration errors during information exchange. To address these issues, we propose mmCooper, a novel multi-agent, multi-stage, communication-efficient, and collaboration-robust cooperative perception framework. Our framework leverages a multi-stage collaboration strategy that dynamically and adaptively balances intermediate- and late-stage information to share among agents, enhancing perceptual performance while maintaining communication efficiency. To support robust collaboration despite potential misalignments and calibration errors, our framework captures multi-scale contextual information for robust fusion in the intermediate stage and calibrates the received detection results to improve accuracy in the late stage. We validate the effectiveness of mmCooper through extensive experiments on real-world and simulated datasets. The results demonstrate the superiority of our proposed framework and the effectiveness of each component.</li>
</ul>

<h3>Title: CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Cristiano Patrício, Isabel Rio-Torto, Jaime S. Cardoso, Luís F. Teixeira, João C. Neves</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12266">https://arxiv.org/abs/2501.12266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12266">https://arxiv.org/pdf/2501.12266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12266]] CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification(https://arxiv.org/abs/2501.12266)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>The main challenges limiting the adoption of deep learning-based solutions in medical workflows are the availability of annotated data and the lack of interpretability of such systems. Concept Bottleneck Models (CBMs) tackle the latter by constraining the final disease prediction on a set of predefined and human-interpretable concepts. However, the increased interpretability achieved through these concept-based explanations implies a higher annotation burden. Moreover, if a new concept needs to be added, the whole system needs to be retrained. Inspired by the remarkable performance shown by Large Vision-Language Models (LVLMs) in few-shot settings, we propose a simple, yet effective, methodology, CBVLM, which tackles both of the aforementioned challenges. First, for each concept, we prompt the LVLM to answer if the concept is present in the input image. Then, we ask the LVLM to classify the image based on the previous concept predictions. Moreover, in both stages, we incorporate a retrieval module responsible for selecting the best examples for in-context learning. By grounding the final diagnosis on the predicted concepts, we ensure explainability, and by leveraging the few-shot capabilities of LVLMs, we drastically lower the annotation cost. We validate our approach with extensive experiments across four medical datasets and twelve LVLMs (both generic and medical) and show that CBVLM consistently outperforms CBMs and task-specific supervised methods without requiring any training and using just a few annotated examples. More information on our project page: this https URL.</li>
</ul>

<h3>Title: VipDiff: Towards Coherent and Diverse Video Inpainting via Training-free Denoising Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Chaohao Xie, Kai Han, Kwan-Yee K. Wong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12267">https://arxiv.org/abs/2501.12267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12267">https://arxiv.org/pdf/2501.12267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12267]] VipDiff: Towards Coherent and Diverse Video Inpainting via Training-free Denoising Diffusion Models(https://arxiv.org/abs/2501.12267)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent video inpainting methods have achieved encouraging improvements by leveraging optical flow to guide pixel propagation from reference frames either in the image space or feature space. However, they would produce severe artifacts in the mask center when the masked area is too large and no pixel correspondences can be found for the center. Recently, diffusion models have demonstrated impressive performance in generating diverse and high-quality images, and have been exploited in a number of works for image inpainting. These methods, however, cannot be applied directly to videos to produce temporal-coherent inpainting results. In this paper, we propose a training-free framework, named VipDiff, for conditioning diffusion model on the reverse diffusion process to produce temporal-coherent inpainting results without requiring any training data or fine-tuning the pre-trained diffusion models. VipDiff takes optical flow as guidance to extract valid pixels from reference frames to serve as constraints in optimizing the randomly sampled Gaussian noise, and uses the generated results for further pixel propagation and conditional generation. VipDiff also allows for generating diverse video inpainting results over different sampled noise. Experiments demonstrate that VipDiff can largely outperform state-of-the-art video inpainting methods in terms of both spatial-temporal coherence and fidelity.</li>
</ul>

<h3>Title: Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement</h3>
<ul>
<li><strong>Authors: </strong>Maosong Cao, Taolin Zhang, Mo Li, Chuyu Zhang, Yunxin Liu, Haodong Duan, Songyang Zhang, Kai Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12273">https://arxiv.org/abs/2501.12273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12273">https://arxiv.org/pdf/2501.12273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12273]] Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement(https://arxiv.org/abs/2501.12273)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The quality of Supervised Fine-Tuning (SFT) data plays a critical role in enhancing the conversational capabilities of Large Language Models (LLMs). However, as LLMs become more advanced, the availability of high-quality human-annotated SFT data has become a significant bottleneck, necessitating a greater reliance on synthetic training data. In this work, we introduce Condor, a novel two-stage synthetic data generation framework that incorporates World Knowledge Tree and Self-Reflection Refinement to produce high-quality SFT data at scale. Our experimental results demonstrate that a base model fine-tuned on only 20K Condor-generated samples achieves superior performance compared to counterparts. The additional refinement stage in Condor further enables iterative self-improvement for LLMs at various scales (up to 72B), validating the effectiveness of our approach. Furthermore, our investigation into the scaling for synthetic data in post-training reveals substantial unexplored potential for performance improvements, opening promising avenues for future research.</li>
</ul>

<h3>Title: With Great Backbones Comes Great Adversarial Transferability</h3>
<ul>
<li><strong>Authors: </strong>Erik Arakelyan, Karen Hambardzumyan, Davit Papikyan, Pasquale Minervini, Albert Gordo, Isabelle Augenstein, Aram H. Markosyan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12275">https://arxiv.org/abs/2501.12275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12275">https://arxiv.org/pdf/2501.12275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12275]] With Great Backbones Comes Great Adversarial Transferability(https://arxiv.org/abs/2501.12275)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Advances in self-supervised learning (SSL) for machine vision have improved representation robustness and model performance, giving rise to pre-trained backbones like \emph{ResNet} and \emph{ViT} models tuned with SSL methods such as \emph{SimCLR}. Due to the computational and data demands of pre-training, the utilization of such backbones becomes a strenuous necessity. However, employing these backbones may inherit vulnerabilities to adversarial attacks. While adversarial robustness has been studied under \emph{white-box} and \emph{black-box} settings, the robustness of models tuned on pre-trained backbones remains largely unexplored. Additionally, the role of tuning meta-information in mitigating exploitation risks is unclear. This work systematically evaluates the adversarial robustness of such models across $20,000$ combinations of tuning meta-information, including fine-tuning techniques, backbone families, datasets, and attack types. We propose using proxy models to transfer attacks, simulating varying levels of target knowledge by fine-tuning these proxies with diverse configurations. Our findings reveal that proxy-based attacks approach the effectiveness of \emph{white-box} methods, even with minimal tuning knowledge. We also introduce a naive "backbone attack," leveraging only the backbone to generate adversarial samples, which outperforms \emph{black-box} attacks and rivals \emph{white-box} methods, highlighting critical risks in model-sharing practices. Finally, our ablations reveal how increasing tuning meta-information impacts attack transferability, measuring each meta-information combination.</li>
</ul>

<h3>Title: MoGERNN: An Inductive Traffic Predictor for Unobserved Locations in Dynamic Sensing Networks</h3>
<ul>
<li><strong>Authors: </strong>Qishen Zhou, Yifan Zhang, Michail A. Makridis, Anastasios Kouvelas, Yibing Wang, Simon Hu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12281">https://arxiv.org/abs/2501.12281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12281">https://arxiv.org/pdf/2501.12281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12281]] MoGERNN: An Inductive Traffic Predictor for Unobserved Locations in Dynamic Sensing Networks(https://arxiv.org/abs/2501.12281)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Given a partially observed road network, how can we predict the traffic state of unobserved locations? While deep learning approaches show exceptional performance in traffic prediction, most assume sensors at all locations of interest, which is impractical due to financial constraints. Furthermore, these methods typically require costly retraining when sensor configurations change. We propose MoGERNN, an inductive spatio-temporal graph representation model, to address these challenges. Inspired by the Mixture of Experts approach in Large Language Models, we introduce a Mixture of Graph Expert (MoGE) block to model complex spatial dependencies through multiple graph message aggregators and a sparse gating network. This block estimates initial states for unobserved locations, which are then processed by a GRU-based Encoder-Decoder that integrates a graph message aggregator to capture spatio-temporal dependencies and predict future states. Experiments on two real-world datasets show MoGERNN consistently outperforms baseline methods for both observed and unobserved locations. MoGERNN can accurately predict congestion evolution even in areas without sensors, offering valuable information for traffic management. Moreover, MoGERNN is adaptable to dynamic sensing networks, maintaining competitive performance even compared to its retrained counterpart. Tests with different numbers of available sensors confirm its consistent superiority, and ablation studies validate the effectiveness of its key modules.</li>
</ul>

<h3>Title: Implementation of an Asymmetric Adjusted Activation Function for Class Imbalance Credit Scoring</h3>
<ul>
<li><strong>Authors: </strong>Xia Li, Hanghang Zheng, Kunpeng Tao, Mao Mao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-fin.RM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12285">https://arxiv.org/abs/2501.12285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12285">https://arxiv.org/pdf/2501.12285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12285]] Implementation of an Asymmetric Adjusted Activation Function for Class Imbalance Credit Scoring(https://arxiv.org/abs/2501.12285)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Credit scoring is a systematic approach to evaluate a borrower's probability of default (PD) on a bank loan. The data associated with such scenarios are characteristically imbalanced, complicating binary classification owing to the often-underestimated cost of misclassification during the classifier's learning process. Considering the high imbalance ratio (IR) of these datasets, we introduce an innovative yet straightforward optimized activation function by incorporating an IR-dependent asymmetric adjusted factor embedded Sigmoid activation function (ASIG). The embedding of ASIG makes the sensitive margin of the Sigmoid function auto-adjustable, depending on the imbalance nature of the datasets distributed, thereby giving the activation function an asymmetric characteristic that prevents the underrepresentation of the minority class (positive samples) during the classifier's learning process. The experimental results show that the ASIG-embedded-classifier outperforms traditional classifiers on datasets across wide-ranging IRs in the downstream credit-scoring task. The algorithm also shows robustness and stability, even when the IR is ultra-high. Therefore, the algorithm provides a competitive alternative in the financial industry, especially in credit scoring, possessing the ability to effectively process highly imbalanced distribution data.</li>
</ul>

<h3>Title: Regressor-Guided Image Editing Regulates Emotional Response to Reduce Online Engagement</h3>
<ul>
<li><strong>Authors: </strong>Christoph Gebhardt, Robin Willardt, Seyedmorteza Sadat, Chih-Wei Ning, Andreas Brombach, Jie Song, Otmar Hilliges, Christian Holz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12289">https://arxiv.org/abs/2501.12289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12289">https://arxiv.org/pdf/2501.12289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12289]] Regressor-Guided Image Editing Regulates Emotional Response to Reduce Online Engagement(https://arxiv.org/abs/2501.12289)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Emotions are known to mediate the relationship between users' content consumption and their online engagement, with heightened emotional intensity leading to increased engagement. Building on this insight, we propose three regressor-guided image editing approaches aimed at diminishing the emotional impact of images. These include (i) a parameter optimization approach based on global image transformations known to influence emotions, (ii) an optimization approach targeting the style latent space of a generative adversarial network, and (iii) a diffusion-based approach employing classifier guidance and classifier-free guidance. Our findings demonstrate that approaches can effectively alter the emotional properties of images while maintaining high visual quality. Optimization-based methods primarily adjust low-level properties like color hues and brightness, whereas the diffusion-based approach introduces semantic changes, such as altering appearance or facial expressions. Notably, results from a behavioral study reveal that only the diffusion-based approach successfully elicits changes in viewers' emotional responses while preserving high perceived image quality. In future work, we will investigate the impact of these image adaptations on internet user behavior.</li>
</ul>

<h3>Title: Library-Attack: Reverse Engineering Approach for Evaluating Hardware IP Protection</h3>
<ul>
<li><strong>Authors: </strong>Aritra Dasgupta, Sudipta Paria, Christopher Sozio, Andrew Lukefahr, Swarup Bhunia</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12292">https://arxiv.org/abs/2501.12292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12292">https://arxiv.org/pdf/2501.12292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12292]] Library-Attack: Reverse Engineering Approach for Evaluating Hardware IP Protection(https://arxiv.org/abs/2501.12292)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>Existing countermeasures for hardware IP protection, such as obfuscation, camouflaging, and redaction, aim to defend against confidentiality and integrity attacks. However, within the current threat model, these techniques overlook the potential risks posed by a highly skilled adversary with privileged access to the IC supply chain, who may be familiar with critical IP blocks and the countermeasures implemented in the design. To address this scenario, we introduce Library-Attack, a novel reverse engineering technique that leverages privileged design information and prior knowledge of security countermeasures to recover sensitive hardware IP. During Library-Attack, a privileged attacker uses known design features to curate a design library of candidate IPs and employs structural comparison metrics from commercial EDA tools to identify the closest match. We evaluate Library-Attack on transformed ISCAS89 benchmarks to demonstrate potential vulnerabilities in existing IP-level countermeasures and propose an updated threat model to incorporate them.</li>
</ul>

<h3>Title: Towards Accurate Unified Anomaly Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Wenxin Ma, Qingsong Yao, Xiang Zhang, Zhelong Huang, Zihang Jiang, S. Kevin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12295">https://arxiv.org/abs/2501.12295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12295">https://arxiv.org/pdf/2501.12295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12295]] Towards Accurate Unified Anomaly Segmentation(https://arxiv.org/abs/2501.12295)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Unsupervised anomaly detection (UAD) from images strives to model normal data distributions, creating discriminative representations to distinguish and precisely localize anomalies. Despite recent advancements in the efficient and unified one-for-all scheme, challenges persist in accurately segmenting anomalies for further monitoring. Moreover, this problem is obscured by the widely-used AUROC metric under imbalanced UAD settings. This motivates us to emphasize the significance of precise segmentation of anomaly pixels using pAP and DSC as metrics. To address the unsolved segmentation task, we introduce the Unified Anomaly Segmentation (UniAS). UniAS presents a multi-level hybrid pipeline that progressively enhances normal information from coarse to fine, incorporating a novel multi-granularity gated CNN (MGG-CNN) into Transformer layers to explicitly aggregate local details from different granularities. UniAS achieves state-of-the-art anomaly segmentation performance, attaining 65.12/59.33 and 40.06/32.50 in pAP/DSC on the MVTec-AD and VisA datasets, respectively, surpassing previous methods significantly. The codes are shared at this https URL.</li>
</ul>

<h3>Title: RALAD: Bridging the Real-to-Sim Domain Gap in Autonomous Driving with Retrieval-Augmented Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Zuo, Haibo Hu, Zikang Zhou, Yufei Cui, Ziquan Liu, Jianping Wang, Nan Guan, Jin Wang, Chun Jason Xue</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12296">https://arxiv.org/abs/2501.12296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12296">https://arxiv.org/pdf/2501.12296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12296]] RALAD: Bridging the Real-to-Sim Domain Gap in Autonomous Driving with Retrieval-Augmented Learning(https://arxiv.org/abs/2501.12296)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In the pursuit of robust autonomous driving systems, models trained on real-world datasets often struggle to adapt to new environments, particularly when confronted with corner cases such as extreme weather conditions. Collecting these corner cases in the real world is non-trivial, which necessitates the use of simulators for validation. However,the high computational cost and the domain gap in data distribution have hindered the seamless transition between real and simulated driving scenarios. To tackle this challenge, we propose Retrieval-Augmented Learning for Autonomous Driving (RALAD), a novel framework designed to bridge the real-to-sim gap at a low cost. RALAD features three primary designs, including (1) domain adaptation via an enhanced Optimal Transport (OT) method that accounts for both individual and grouped image distances, (2) a simple and unified framework that can be applied to various models, and (3) efficient fine-tuning techniques that freeze the computationally expensive layers while maintaining robustness. Experimental results demonstrate that RALAD compensates for the performance degradation in simulated environments while maintaining accuracy in real-world scenarios across three different models. Taking Cross View as an example, the mIOU and mAP metrics in real-world scenarios remain stable before and after RALAD fine-tuning, while in simulated environments,the mIOU and mAP metrics are improved by 10.30% and 12.29%, respectively. Moreover, the re-training cost of our approach is reduced by approximately 88.1%. Our code is available at this https URL.</li>
</ul>

<h3>Title: Metric for Evaluating Performance of Reference-Free Demorphing Methods</h3>
<ul>
<li><strong>Authors: </strong>Nitish Shukla, Arun Ross</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12319">https://arxiv.org/abs/2501.12319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12319">https://arxiv.org/pdf/2501.12319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12319]] Metric for Evaluating Performance of Reference-Free Demorphing Methods(https://arxiv.org/abs/2501.12319)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>A facial morph is an image created by combining two (or more) face images pertaining to two (or more) distinct identities. Reference-free face demorphing inverts the process and tries to recover the face images constituting a facial morph without using any other information. However, there is no consensus on the evaluation metrics to be used to evaluate and compare such demorphing techniques. In this paper, we first analyze the shortcomings of the demorphing metrics currently used in the literature. We then propose a new metric called biometrically cross-weighted IQA that overcomes these issues and extensively benchmark current methods on the proposed metric to show its efficacy. Experiments on three existing demorphing methods and six datasets on two commonly used face matchers validate the efficacy of our proposed metric.</li>
</ul>

<h3>Title: VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Xianwei Zhuang, Yuxin Xie, Yufan Deng, Liming Liang, Jinghan Ru, Yuguo Yin, Yuexian Zou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12327">https://arxiv.org/abs/2501.12327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12327">https://arxiv.org/pdf/2501.12327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12327]] VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model(https://arxiv.org/abs/2501.12327)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present VARGPT, a novel multimodal large language model (MLLM) that unifies visual understanding and generation within a single autoregressive framework. VARGPT employs a next-token prediction paradigm for visual understanding and a next-scale prediction paradigm for visual autoregressive generation. VARGPT innovatively extends the LLaVA architecture, achieving efficient scale-wise autoregressive visual generation within MLLMs while seamlessly accommodating mixed-modal input and output within a single model framework. Our VARGPT undergoes a three-stage unified training process on specially curated datasets, comprising a pre-training phase and two mixed visual instruction-tuning phases. The unified training strategy are designed to achieve alignment between visual and textual features, enhance instruction following for both understanding and generation, and improve visual generation quality, respectively. Despite its LLAVA-based architecture for multimodel understanding, VARGPT significantly outperforms LLaVA-1.5 across various vision-centric benchmarks, such as visual question-answering and reasoning tasks. Notably, VARGPT naturally supports capabilities in autoregressive visual generation and instruction-to-image synthesis, showcasing its versatility in both visual understanding and generation tasks. Project page is at: \url{this https URL}</li>
</ul>

<h3>Title: Automatic Labelling with Open-source LLMs using Dynamic Label Schema Integration</h3>
<ul>
<li><strong>Authors: </strong>Thomas Walshe, Sae Young Moon, Chunyang Xiao, Yawwani Gunawardana, Fran Silavong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12332">https://arxiv.org/abs/2501.12332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12332">https://arxiv.org/pdf/2501.12332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12332]] Automatic Labelling with Open-source LLMs using Dynamic Label Schema Integration(https://arxiv.org/abs/2501.12332)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Acquiring labelled training data remains a costly task in real world machine learning projects to meet quantity and quality requirements. Recently Large Language Models (LLMs), notably GPT-4, have shown great promises in labelling data with high accuracy. However, privacy and cost concerns prevent the ubiquitous use of GPT-4. In this work, we explore effectively leveraging open-source models for automatic labelling. We identify integrating label schema as a promising technology but found that naively using the label description for classification leads to poor performance on high cardinality tasks. To address this, we propose Retrieval Augmented Classification (RAC) for which LLM performs inferences for one label at a time using corresponding label schema; we start with the most related label and iterates until a label is chosen by the LLM. We show that our method, which dynamically integrates label description, leads to performance improvements in labelling tasks. We further show that by focusing only on the most promising labels, RAC can trade off between label quality and coverage - a property we leverage to automatically label our internal datasets.</li>
</ul>

<h3>Title: FuocChuVIP123 at CoMeDi Shared Task: Disagreement Ranking with XLM-Roberta Sentence Embeddings and Deep Neural Regression</h3>
<ul>
<li><strong>Authors: </strong>Phuoc Duong Huy Chu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12336">https://arxiv.org/abs/2501.12336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12336">https://arxiv.org/pdf/2501.12336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12336]] FuocChuVIP123 at CoMeDi Shared Task: Disagreement Ranking with XLM-Roberta Sentence Embeddings and Deep Neural Regression(https://arxiv.org/abs/2501.12336)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents results of our system for CoMeDi Shared Task, focusing on Subtask 2: Disagreement Ranking. Our system leverages sentence embeddings generated by the paraphrase-xlm-r-multilingual-v1 model, combined with a deep neural regression model incorporating batch normalization and dropout for improved generalization. By predicting the mean of pairwise judgment differences between annotators, our method explicitly targets disagreement ranking, diverging from traditional "gold label" aggregation approaches. We optimized our system with a customized architecture and training procedure, achieving competitive performance in Spearman correlation against mean disagreement labels. Our results highlight the importance of robust embeddings, effective model architecture, and careful handling of judgment differences for ranking disagreement in multilingual contexts. These findings provide insights into the use of contextualized representations for ordinal judgment tasks and open avenues for further refinement of disagreement prediction models.</li>
</ul>

<h3>Title: CYCle: Choosing Your Collaborators Wisely to Enhance Collaborative Fairness in Decentralized Learning</h3>
<ul>
<li><strong>Authors: </strong>Nurbek Tastan, Samuel Horvath, Karthik Nandakumar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12344">https://arxiv.org/abs/2501.12344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12344">https://arxiv.org/pdf/2501.12344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12344]] CYCle: Choosing Your Collaborators Wisely to Enhance Collaborative Fairness in Decentralized Learning(https://arxiv.org/abs/2501.12344)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Collaborative learning (CL) enables multiple participants to jointly train machine learning (ML) models on decentralized data sources without raw data sharing. While the primary goal of CL is to maximize the expected accuracy gain for each participant, it is also important to ensure that the gains are fairly distributed. Specifically, no client should be negatively impacted by the collaboration, and the individual gains must ideally be commensurate with the contributions. Most existing CL algorithms require central coordination and focus on the gain maximization objective while ignoring collaborative fairness. In this work, we first show that the existing measure of collaborative fairness based on the correlation between accuracy values without and with collaboration has drawbacks because it does not account for negative collaboration gain. We argue that maximizing mean collaboration gain (MCG) while simultaneously minimizing the collaboration gain spread (CGS) is a fairer alternative. Next, we propose the CYCle protocol that enables individual participants in a private decentralized learning (PDL) framework to achieve this objective through a novel reputation scoring method based on gradient alignment between the local cross-entropy and distillation losses. Experiments on the CIFAR-10, CIFAR-100, and Fed-ISIC2019 datasets empirically demonstrate the effectiveness of the CYCle protocol to ensure positive and fair collaboration gain for all participants, even in cases where the data distributions of participants are highly skewed. For the simple mean estimation problem with two participants, we also theoretically show that CYCle performs better than standard FedAvg, especially when there is large statistical heterogeneity.</li>
</ul>

<h3>Title: Test-time regression: a unifying framework for designing sequence models with associative memory</h3>
<ul>
<li><strong>Authors: </strong>Ke Alexander Wang, Jiaxin Shi, Emily B. Fox</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12352">https://arxiv.org/abs/2501.12352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12352">https://arxiv.org/pdf/2501.12352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12352]] Test-time regression: a unifying framework for designing sequence models with associative memory(https://arxiv.org/abs/2501.12352)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Sequences provide a remarkably general way to represent and process information. This powerful abstraction has placed sequence modeling at the center of modern deep learning applications, inspiring numerous architectures from transformers to recurrent networks. While this fragmented development has yielded powerful models, it has left us without a unified framework to understand their fundamental similarities and explain their effectiveness. We present a unifying framework motivated by an empirical observation: effective sequence models must be able to perform associative recall. Our key insight is that memorizing input tokens through an associative memory is equivalent to performing regression at test-time. This regression-memory correspondence provides a framework for deriving sequence models that can perform associative recall, offering a systematic lens to understand seemingly ad-hoc architectural choices. We show numerous recent architectures -- including linear attention models, their gated variants, state-space models, online learners, and softmax attention -- emerge naturally as specific approaches to test-time regression. Each architecture corresponds to three design choices: the relative importance of each association, the regressor function class, and the optimization algorithm. This connection leads to new understanding: we provide theoretical justification for QKNorm in softmax attention, and we motivate higher-order generalizations of softmax attention. Beyond unification, our work unlocks decades of rich statistical tools that can guide future development of more powerful yet principled sequence models.</li>
</ul>

<h3>Title: Diffusion-aware Censored Gaussian Processes for Demand Modelling</h3>
<ul>
<li><strong>Authors: </strong>Filipe Rodrigues</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12354">https://arxiv.org/abs/2501.12354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12354">https://arxiv.org/pdf/2501.12354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12354]] Diffusion-aware Censored Gaussian Processes for Demand Modelling(https://arxiv.org/abs/2501.12354)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Inferring the true demand for a product or a service from aggregate data is often challenging due to the limited available supply, thus resulting in observations that are censored and correspond to the realized demand, thereby not accounting for the unsatisfied demand. Censored regression models are able to account for the effect of censoring due to the limited supply, but they don't consider the effect of substitutions, which may cause the demand for similar alternative products or services to increase. This paper proposes Diffusion-aware Censored Demand Models, which combine a Tobit likelihood with a graph diffusion process in order to model the latent process of transfer of unsatisfied demand between similar products or services. We instantiate this new class of models under the framework of GPs and, based on both simulated and real-world data for modeling sales, bike-sharing demand, and EV charging demand, demonstrate its ability to better recover the true demand and produce more accurate out-of-sample predictions.</li>
</ul>

<h3>Title: Vision-Language Models for Automated Chest X-ray Interpretation: Leveraging ViT and GPT-2</h3>
<ul>
<li><strong>Authors: </strong>Md. Rakibul Islam, Md. Zahid Hossain, Mustofa Ahmed, Most. Sharmin Sultana Samu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12356">https://arxiv.org/abs/2501.12356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12356">https://arxiv.org/pdf/2501.12356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12356]] Vision-Language Models for Automated Chest X-ray Interpretation: Leveraging ViT and GPT-2(https://arxiv.org/abs/2501.12356)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Radiology plays a pivotal role in modern medicine due to its non-invasive diagnostic capabilities. However, the manual generation of unstructured medical reports is time consuming and prone to errors. It creates a significant bottleneck in clinical workflows. Despite advancements in AI-generated radiology reports, challenges remain in achieving detailed and accurate report generation. In this study we have evaluated different combinations of multimodal models that integrate Computer Vision and Natural Language Processing to generate comprehensive radiology reports. We employed a pretrained Vision Transformer (ViT-B16) and a SWIN Transformer as the image encoders. The BART and GPT-2 models serve as the textual decoders. We used Chest X-ray images and reports from the IU-Xray dataset to evaluate the usability of the SWIN Transformer-BART, SWIN Transformer-GPT-2, ViT-B16-BART and ViT-B16-GPT-2 models for report generation. We aimed at finding the best combination among the models. The SWIN-BART model performs as the best-performing model among the four models achieving remarkable results in almost all the evaluation metrics like ROUGE, BLEU and BERTScore.</li>
</ul>

<h3>Title: Budget-constrained Collaborative Renewable Energy Forecasting Market</h3>
<ul>
<li><strong>Authors: </strong>Carla Goncalves, Ricardo J. Bessa, Tiago Teixeira, Joao Vinagre</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12367">https://arxiv.org/abs/2501.12367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12367">https://arxiv.org/pdf/2501.12367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12367]] Budget-constrained Collaborative Renewable Energy Forecasting Market(https://arxiv.org/abs/2501.12367)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Accurate power forecasting from renewable energy sources (RES) is crucial for integrating additional RES capacity into the power system and realizing sustainability goals. This work emphasizes the importance of integrating decentralized spatio-temporal data into forecasting models. However, decentralized data ownership presents a critical obstacle to the success of such spatio-temporal models, and incentive mechanisms to foster data-sharing need to be considered. The main contributions are a) a comparative analysis of the forecasting models, advocating for efficient and interpretable spline LASSO regression models, and b) a bidding mechanism within the data/analytics market to ensure fair compensation for data providers and enable both buyers and sellers to express their data price requirements. Furthermore, an incentive mechanism for time series forecasting is proposed, effectively incorporating price constraints and preventing redundant feature allocation. Results show significant accuracy improvements and potential monetary gains for data sellers. For wind power data, an average root mean squared error improvement of over 10% was achieved by comparing forecasts generated by the proposal with locally generated ones.</li>
</ul>

<h3>Title: InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, Kai Chen, Dahua Lin, Jiaqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12368">https://arxiv.org/abs/2501.12368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12368">https://arxiv.org/pdf/2501.12368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12368]] InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model(https://arxiv.org/abs/2501.12368)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite the promising performance of Large Vision Language Models (LVLMs) in visual understanding, they occasionally generate incorrect outputs. While reward models (RMs) with reinforcement learning or test-time scaling offer the potential for improving generation quality, a critical gap remains: publicly available multi-modal RMs for LVLMs are scarce, and the implementation details of proprietary models are often unclear. We bridge this gap with InternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective multi-modal reward model that aligns LVLMs with human preferences. To ensure the robustness and versatility of IXC-2.5-Reward, we set up a high-quality multi-modal preference corpus spanning text, image, and video inputs across diverse domains, such as instruction following, general understanding, text-rich documents, mathematical reasoning, and video understanding. IXC-2.5-Reward achieves excellent results on the latest multi-modal reward model benchmark and shows competitive performance on text-only reward model benchmarks. We further demonstrate three key applications of IXC-2.5-Reward: (1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Reward with Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which shows consistent improvements in instruction following and multi-modal open-ended dialogue; (2) Selecting the best response from candidate responses for test-time scaling; and (3) Filtering outlier or noisy samples from existing image and video instruction tuning training data. To ensure reproducibility and facilitate further research, we have open-sourced all model weights and training recipes at this https URL</li>
</ul>

<h3>Title: Parallel Sequence Modeling via Generalized Spatial Propagation Network</h3>
<ul>
<li><strong>Authors: </strong>Hongjun Wang, Wonmin Byeon, Jiarui Xu, Jinwei Gu, Ka Chun Cheung, Xiaolong Wang, Kai Han, Jan Kautz, Sifei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12381">https://arxiv.org/abs/2501.12381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12381">https://arxiv.org/pdf/2501.12381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12381]] Parallel Sequence Modeling via Generalized Spatial Propagation Network(https://arxiv.org/abs/2501.12381)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present the Generalized Spatial Propagation Network (GSPN), a new attention mechanism optimized for vision tasks that inherently captures 2D spatial structures. Existing attention models, including transformers, linear attention, and state-space models like Mamba, process multi-dimensional data as 1D sequences, compromising spatial coherence and efficiency. GSPN overcomes these limitations by directly operating on spatially coherent image data and forming dense pairwise connections through a line-scan approach. Central to GSPN is the Stability-Context Condition, which ensures stable, context-aware propagation across 2D sequences and reduces the effective sequence length to $\sqrt{N}$ for a square map with N elements, significantly enhancing computational efficiency. With learnable, input-dependent weights and no reliance on positional embeddings, GSPN achieves superior spatial fidelity and state-of-the-art performance in vision tasks, including ImageNet classification, class-guided image generation, and text-to-image generation. Notably, GSPN accelerates SD-XL with softmax-attention by over $84\times$ when generating 16K images.</li>
</ul>

<h3>Title: DiffDoctor: Diagnosing Image Diffusion Models Before Treating</h3>
<ul>
<li><strong>Authors: </strong>Yiyang Wang, Xi Chen, Xiaogang Xu, Sihui Ji, Yu Liu, Yujun Shen, Hengshuang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12382">https://arxiv.org/abs/2501.12382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12382">https://arxiv.org/pdf/2501.12382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12382]] DiffDoctor: Diagnosing Image Diffusion Models Before Treating(https://arxiv.org/abs/2501.12382)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>In spite of the recent progress, image diffusion models still produce artifacts. A common solution is to refine an established model with a quality assessment system, which generally rates an image in its entirety. In this work, we believe problem-solving starts with identification, yielding the request that the model should be aware of not just the presence of defects in an image, but their specific locations. Motivated by this, we propose DiffDoctor, a two-stage pipeline to assist image diffusion models in generating fewer artifacts. Concretely, the first stage targets developing a robust artifact detector, for which we collect a dataset of over 1M flawed synthesized images and set up an efficient human-in-the-loop annotation process, incorporating a carefully designed class-balance strategy. The learned artifact detector is then involved in the second stage to tune the diffusion model through assigning a per-pixel confidence map for each synthesis. Extensive experiments on text-to-image diffusion models demonstrate the effectiveness of our artifact detector as well as the soundness of our diagnose-then-treat design.</li>
</ul>

<h3>Title: CCESAR: Coastline Classification-Extraction From SAR Images Using CNN-U-Net Combination</h3>
<ul>
<li><strong>Authors: </strong>Vidhu Arora, Shreyan Gupta, Ananthakrishna Kudupu, Aditya Priyadarshi, Aswathi Mundayatt, Jaya Sreevalsan-Nair</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12384">https://arxiv.org/abs/2501.12384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12384">https://arxiv.org/pdf/2501.12384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12384]] CCESAR: Coastline Classification-Extraction From SAR Images Using CNN-U-Net Combination(https://arxiv.org/abs/2501.12384)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>In this article, we improve the deep learning solution for coastline extraction from Synthetic Aperture Radar (SAR) images by proposing a two-stage model involving image classification followed by segmentation. We hypothesize that a single segmentation model usually used for coastline detection is insufficient to characterize different coastline types. We demonstrate that the need for a two-stage workflow prevails through different compression levels of these images. Our results from experiments using a combination of CNN and U-Net models on Sentinel-1 images show that the two-stage workflow, coastline classification-extraction from SAR images (CCESAR) outperforms a single U-Net segmentation model.</li>
</ul>

<h3>Title: InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, Min Dou, Kai Chen, Wenhai Wang, Yu Qiao, Yali Wang, Limin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12386">https://arxiv.org/abs/2501.12386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12386">https://arxiv.org/pdf/2501.12386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12386]] InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling(https://arxiv.org/abs/2501.12386)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>This paper aims to improve the performance of video multimodal large language models (MLLM) via long and rich context (LRC) modeling. As a result, we develop a new version of InternVideo2.5 with a focus on enhancing the original MLLMs' ability to perceive fine-grained details and capture long-form temporal structure in videos. Specifically, our approach incorporates dense vision task annotations into MLLMs using direct preference optimization and develops compact spatiotemporal representations through adaptive hierarchical token compression. Experimental results demonstrate this unique design of LRC greatly improves the results of video MLLM in mainstream video understanding benchmarks (short & long), enabling the MLLM to memorize significantly longer video inputs (at least 6x longer than the original), and master specialized vision capabilities like object tracking and segmentation. Our work highlights the importance of multimodal context richness (length and fineness) in empowering MLLM's innate abilites (focus and memory), providing new insights for future research on video MLLM. Code and models are available at this https URL</li>
</ul>

<h3>Title: Continuous 3D Perception Model with Persistent State</h3>
<ul>
<li><strong>Authors: </strong>Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei A. Efros, Angjoo Kanazawa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12387">https://arxiv.org/abs/2501.12387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12387">https://arxiv.org/pdf/2501.12387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12387]] Continuous 3D Perception Model with Persistent State(https://arxiv.org/abs/2501.12387)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present a unified framework capable of solving a broad range of 3D tasks. Our approach features a stateful recurrent model that continuously updates its state representation with each new observation. Given a stream of images, this evolving state can be used to generate metric-scale pointmaps (per-pixel 3D points) for each new input in an online fashion. These pointmaps reside within a common coordinate system, and can be accumulated into a coherent, dense scene reconstruction that updates as new images arrive. Our model, called CUT3R (Continuous Updating Transformer for 3D Reconstruction), captures rich priors of real-world scenes: not only can it predict accurate pointmaps from image observations, but it can also infer unseen regions of the scene by probing at virtual, unobserved views. Our method is simple yet highly flexible, naturally accepting varying lengths of images that may be either video streams or unordered photo collections, containing both static and dynamic content. We evaluate our method on various 3D/4D tasks and demonstrate competitive or state-of-the-art performance in each. Project Page: this https URL</li>
</ul>

<h3>Title: GPS as a Control Signal for Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Chao Feng, Ziyang Chen, Aleksander Holynski, Alexei A. Efros, Andrew Owens</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12390">https://arxiv.org/abs/2501.12390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12390">https://arxiv.org/pdf/2501.12390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12390]] GPS as a Control Signal for Image Generation(https://arxiv.org/abs/2501.12390)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We show that the GPS tags contained in photo metadata provide a useful control signal for image generation. We train GPS-to-image models and use them for tasks that require a fine-grained understanding of how images vary within a city. In particular, we train a diffusion model to generate images conditioned on both GPS and text. The learned model generates images that capture the distinctive appearance of different neighborhoods, parks, and landmarks. We also extract 3D models from 2D GPS-to-image models through score distillation sampling, using GPS conditioning to constrain the appearance of the reconstruction from each viewpoint. Our evaluations suggest that our GPS-conditioned models successfully learn to generate images that vary based on location, and that GPS conditioning improves estimated 3D structure.</li>
</ul>

<h3>Title: Learning segmentation from point trajectories</h3>
<ul>
<li><strong>Authors: </strong>Laurynas Karazija, Iro Laina, Christian Rupprecht, Andrea Vedaldi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12392">https://arxiv.org/abs/2501.12392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12392">https://arxiv.org/pdf/2501.12392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12392]] Learning segmentation from point trajectories(https://arxiv.org/abs/2501.12392)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We consider the problem of segmenting objects in videos based on their motion and no other forms of supervision. Prior work has often approached this problem by using the principle of common fate, namely the fact that the motion of points that belong to the same object is strongly correlated. However, most authors have only considered instantaneous motion from optical flow. In this work, we present a way to train a segmentation network using long-term point trajectories as a supervisory signal to complement optical flow. The key difficulty is that long-term motion, unlike instantaneous motion, is difficult to model -- any parametric approximation is unlikely to capture complex motion patterns over long periods of time. We instead draw inspiration from subspace clustering approaches, proposing a loss function that seeks to group the trajectories into low-rank matrices where the motion of object points can be approximately explained as a linear combination of other point tracks. Our method outperforms the prior art on motion-based segmentation, which shows the utility of long-term motion and the effectiveness of our formulation.</li>
</ul>

<h3>Title: Towards Affordance-Aware Articulation Synthesis for Rigged Objects</h3>
<ul>
<li><strong>Authors: </strong>Yu-Chu Yu, Chieh Hubert Lin, Hsin-Ying Lee, Chaoyang Wang, Yu-Chiang Frank Wang, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.12393">https://arxiv.org/abs/2501.12393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.12393">https://arxiv.org/pdf/2501.12393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.12393]] Towards Affordance-Aware Articulation Synthesis for Rigged Objects(https://arxiv.org/abs/2501.12393)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Rigged objects are commonly used in artist pipelines, as they can flexibly adapt to different scenes and postures. However, articulating the rigs into realistic affordance-aware postures (e.g., following the context, respecting the physics and the personalities of the object) remains time-consuming and heavily relies on human labor from experienced artists. In this paper, we tackle the novel problem and design A3Syn. With a given context, such as the environment mesh and a text prompt of the desired posture, A3Syn synthesizes articulation parameters for arbitrary and open-domain rigged objects obtained from the Internet. The task is incredibly challenging due to the lack of training data, and we do not make any topological assumptions about the open-domain rigs. We propose using 2D inpainting diffusion model and several control techniques to synthesize in-context affordance information. Then, we develop an efficient bone correspondence alignment using a combination of differentiable rendering and semantic correspondence. A3Syn has stable convergence, completes in minutes, and synthesizes plausible affordance on different combinations of in-the-wild object rigs and scenes.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
