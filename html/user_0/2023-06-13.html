<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Iterative Design of An Accessible Crypto Wallet for Blind Users. (arXiv:2306.06261v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06261">http://arxiv.org/abs/2306.06261</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06261] Iterative Design of An Accessible Crypto Wallet for Blind Users](http://arxiv.org/abs/2306.06261) #secure</code></li>
<li>Summary: <p>Crypto wallets are a key touch-point for cryptocurrency use. People use
crypto wallets to make transactions, manage crypto assets, and interact with
decentralized apps (dApps). However, as is often the case with emergent
technologies, little attention has been paid to understanding and improving
accessibility barriers in crypto wallet software. We present a series of user
studies that explored how both blind and sighted individuals use MetaMask, one
of the most popular non-custodial crypto wallets. We uncovered inter-related
accessibility, learnability, and security issues with MetaMask. We also report
on an iterative redesign of MetaMask to make it more accessible for blind
users. This process involved multiple evaluations with 44 novice crypto wallet
users, including 20 sighted users, 23 blind users, and one user with low
vision. Our study results show notable improvements for accessibility after two
rounds of design iterations. Based on the results, we discuss design
implications for creating more accessible and secure crypto wallets for blind
users.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: NeRFool: Uncovering the Vulnerability of Generalizable Neural Radiance Fields against Adversarial Perturbations. (arXiv:2306.06359v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06359">http://arxiv.org/abs/2306.06359</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06359] NeRFool: Uncovering the Vulnerability of Generalizable Neural Radiance Fields against Adversarial Perturbations](http://arxiv.org/abs/2306.06359) #security</code></li>
<li>Summary: <p>Generalizable Neural Radiance Fields (GNeRF) are one of the most promising
real-world solutions for novel view synthesis, thanks to their cross-scene
generalization capability and thus the possibility of instant rendering on new
scenes. While adversarial robustness is essential for real-world applications,
little study has been devoted to understanding its implication on GNeRF. We
hypothesize that because GNeRF is implemented by conditioning on the source
views from new scenes, which are often acquired from the Internet or
third-party providers, there are potential new security concerns regarding its
real-world applications. Meanwhile, existing understanding and solutions for
neural networks' adversarial robustness may not be applicable to GNeRF, due to
its 3D nature and uniquely diverse operations. To this end, we present NeRFool,
which to the best of our knowledge is the first work that sets out to
understand the adversarial robustness of GNeRF. Specifically, NeRFool unveils
the vulnerability patterns and important insights regarding GNeRF's adversarial
robustness. Built upon the above insights gained from NeRFool, we further
develop NeRFool+, which integrates two techniques capable of effectively
attacking GNeRF across a wide range of target views, and provide guidelines for
defending against our proposed attacks. We believe that our NeRFool/NeRFool+
lays the initial foundation for future innovations in developing robust
real-world GNeRF solutions. Our codes are available at:
https://github.com/GATECH-EIC/NeRFool.
</p></li>
</ul>

<h3>Title: Zero-Day Threats Detection for Critical Infrastructures. (arXiv:2306.06366v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06366">http://arxiv.org/abs/2306.06366</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06366] Zero-Day Threats Detection for Critical Infrastructures](http://arxiv.org/abs/2306.06366) #security</code></li>
<li>Summary: <p>Technological advancements in various industries, such as network
intelligence, vehicle networks, e-commerce, the Internet of Things (IoT),
ubiquitous computing, and cloud-based applications, have led to an exponential
increase in the volume of information flowing through critical systems. As a
result, protecting critical infrastructures from intrusions and security
threats have become a paramount concern in the field of intrusion detection
systems (IDS). To address this concern, this research paper focuses on the
importance of defending critical infrastructures against intrusions and
security threats. It proposes a computational framework that incorporates
feature selection through fuzzification. The effectiveness and performance of
the proposed framework is evaluated using the NSL-KDD and UGRansome datasets in
combination with selected machine learning (ML) models. The findings of the
study highlight the effectiveness of fuzzy logic and the use of ensemble
learning to enhance the performance of ML models. The research identifies
Random Forest (RF) and Extreme Gradient Boosting (XGB) as the top performing
algorithms to detect zero-day attacks. The results obtained from the
implemented computational framework outperform previous methods documented in
the IDS literature, reaffirming the significance of safeguarding critical
infrastructures from intrusions and security threats.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Integrating Usage Control into Distributed Ledger Technology for Internet of Things Privacy. (arXiv:2306.06143v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06143">http://arxiv.org/abs/2306.06143</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06143] Integrating Usage Control into Distributed Ledger Technology for Internet of Things Privacy](http://arxiv.org/abs/2306.06143) #privacy</code></li>
<li>Summary: <p>The Internet of Things brings new ways to collect privacy-sensitive data from
billions of devices. Well-tailored distributed ledger technologies (DLTs) can
provide high transaction processing capacities to IoT devices in a
decentralized fashion. However, privacy aspects are often neglected or
unsatisfying, with a focus mainly on performance and security. In this paper,
we introduce decentralized usage control mechanisms to empower IoT devices to
control the data they generate. Usage control defines obligations, i.e.,
actions to be fulfilled to be granted access, and conditions on the system in
addition to data dissemination control. The originality of this paper is to
consider the usage control system as a component of distributed ledger
networks, instead of an external tool. With this integration, both technologies
work in synergy, benefiting their privacy, security and performance. We
evaluated the performance improvements of integration using the IOTA
technology, particularly suitable due to the participation of small devices in
the consensus. The results of the tests on a private network show an
approximate 90% decrease of the time needed for the UCS to push a transaction
and make its access decision in the integrated setting, regardless of the
number of nodes in the network.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Protect Your Prompts: Protocols for IP Protection in LLM Applications. (arXiv:2306.06297v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06297">http://arxiv.org/abs/2306.06297</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06297] Protect Your Prompts: Protocols for IP Protection in LLM Applications](http://arxiv.org/abs/2306.06297) #protect</code></li>
<li>Summary: <p>With the rapid adoption of AI in the form of large language models (LLMs),
the potential value of carefully engineered prompts has become significant.
However, to realize this potential, prompts should be tradable on an open
market. Since prompts are, at present, generally economically non-excludable,
by virtue of their nature as text, no general competitive market has yet been
established. This note discusses two protocols intended to provide protection
of prompts, elevating their status as intellectual property, thus confirming
the intellectual property rights of prompt engineers, and potentially
supporting the flourishing of an open market for LLM prompts.
</p></li>
</ul>

<h3>Title: ModelObfuscator: Obfuscating Model Information to Protect Deployed ML-based Systems. (arXiv:2306.06112v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06112">http://arxiv.org/abs/2306.06112</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06112] ModelObfuscator: Obfuscating Model Information to Protect Deployed ML-based Systems](http://arxiv.org/abs/2306.06112) #protect</code></li>
<li>Summary: <p>More and more edge devices and mobile apps are leveraging deep learning (DL)
capabilities. Deploying such models on devices -- referred to as on-device
models -- rather than as remote cloud-hosted services, has gained popularity
because it avoids transmitting user data off of the device and achieves high
response time. However, on-device models can be easily attacked, as they can be
accessed by unpacking corresponding apps and the model is fully exposed to
attackers. Recent studies show that attackers can easily generate
white-box-like attacks for an on-device model or even inverse its training
data. To protect on-device models from white-box attacks, we propose a novel
technique called model obfuscation. Specifically, model obfuscation hides and
obfuscates the key information -- structure, parameters and attributes -- of
models by renaming, parameter encapsulation, neural structure obfuscation
obfuscation, shortcut injection, and extra layer injection. We have developed a
prototype tool ModelObfuscator to automatically obfuscate on-device TFLite
models. Our experiments show that this proposed approach can dramatically
improve model security by significantly increasing the difficulty of parsing
models inner information, without increasing the latency of DL models. Our
proposed on-device model obfuscation has the potential to be a fundamental
technique for on-device model deployment. Our prototype tool is publicly
available at: https://github.com/zhoumingyi/ModelObfuscator.
</p></li>
</ul>

<h3>Title: Spoofing Against Spoofing: Towards Caller ID Verification In Heterogeneous Telecommunication Systems. (arXiv:2306.06198v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06198">http://arxiv.org/abs/2306.06198</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06198] Spoofing Against Spoofing: Towards Caller ID Verification In Heterogeneous Telecommunication Systems](http://arxiv.org/abs/2306.06198) #protect</code></li>
<li>Summary: <p>Caller ID spoofing is a global industry problem and often acts as a critical
enabler for telephone fraud. To address this problem, the Federal
Communications Commission (FCC) has mandated telecom providers in the US to
implement STIR/SHAKEN, an industry-driven solution based on digital signatures.
STIR/SHAKEN relies on a public key infrastructure (PKI) to manage digital
certificates, but scaling up this PKI for the global telecom industry is
extremely difficult, if not impossible. Furthermore, it only works with the SIP
(VoIP) system, leaving the traditional SS7 (landline and cellular) systems
unprotected. So far the alternatives to the STIR/SHAKEN have not been
sufficiently studied. In this paper, we propose a PKI-free solution, Caller ID
Verification (CIV), to combat caller ID spoofing. CIV authenticates the caller
ID based on a challenge-response process instead of digital signatures. It
supports both SIP and SS7 systems. Perhaps counter-intuitively, we show that
number spoofing can be leveraged, in conjunction with Dual-Tone Multi-Frequency
(DTMF), to efficiently implement the challenge-response process, i.e., using
spoofing to fight against spoofing. We implement CIV for VoIP, cellular, and
landline phones across heterogeneous networks (SS7/SIP) by only updating the
software on the user's phone. This is the first caller ID authentication
solution with working prototypes for all three types of telephone systems in
the current telecom architecture. Finally, we show how the implementation of
CIV can be optimized by integrating it into telecom clouds as a service, which
users may subscribe to.
</p></li>
</ul>

<h3>Title: Artificial intelligence and radiation protection. A game changer or an update?. (arXiv:2306.06148v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06148">http://arxiv.org/abs/2306.06148</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06148] Artificial intelligence and radiation protection](http://arxiv.org/abs/2306.06148) #protect</code></li>
<li>Summary: <p>Artificial intelligence (AI) is regarded as one of the most disruptive
technology of the century and with countless applications. What does it mean
for radiation protection? This article describes the fundamentals of machine
learning (ML) based methods and presents the inaugural applications in
different fields of radiation protection. It is foreseen that the usage of AI
will increase in radiation protection. Consequently, this article explores some
of the benefits and also the potential barriers and questions, including
ethical ones, that can come out. The article proposes that collaboration
between radiation protection professionals and data scientist experts can
accelerate and guide the development of the algorithms for effective scientific
and technological outcomes.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Adversarial Attacks and Defenses in Explainable Artificial Intelligence: A Survey. (arXiv:2306.06123v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06123">http://arxiv.org/abs/2306.06123</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06123] Adversarial Attacks and Defenses in Explainable Artificial Intelligence: A Survey](http://arxiv.org/abs/2306.06123) #defense</code></li>
<li>Summary: <p>Explainable artificial intelligence (XAI) methods are portrayed as a remedy
for debugging and trusting statistical and deep learning models, as well as
interpreting their predictions. However, recent advances in adversarial machine
learning highlight the limitations and vulnerabilities of state-of-the-art
explanations, putting their security and trustworthiness into question. The
possibility of manipulating, fooling or fairwashing evidence of the model's
reasoning has detrimental consequences when applied in high-stakes
decision-making and knowledge discovery. This concise survey of over 50 papers
summarizes research concerning adversarial attacks on explanations of machine
learning models, as well as fairness metrics. We discuss how to defend against
attacks and design robust interpretation methods. We contribute a list of
existing insecurities in XAI and outline the emerging research directions in
adversarial XAI (AdvXAI).
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Backdoor Attack with Sparse and Invisible Trigger. (arXiv:2306.06209v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06209">http://arxiv.org/abs/2306.06209</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06209] Backdoor Attack with Sparse and Invisible Trigger](http://arxiv.org/abs/2306.06209) #attack</code></li>
<li>Summary: <p>Deep neural networks (DNNs) are vulnerable to backdoor attacks, where the
adversary manipulates a small portion of training data such that the victim
model predicts normally on the benign samples but classifies the triggered
samples as the target class. The backdoor attack is an emerging yet threatening
training-phase threat, leading to serious risks in DNN-based applications. In
this paper, we revisit the trigger patterns of existing backdoor attacks. We
reveal that they are either visible or not sparse and therefore are not
stealthy enough. More importantly, it is not feasible to simply combine
existing methods to design an effective sparse and invisible backdoor attack.
To address this problem, we formulate the trigger generation as a bi-level
optimization problem with sparsity and invisibility constraints and propose an
effective method to solve it. The proposed method is dubbed sparse and
invisible backdoor attack (SIBA). We conduct extensive experiments on benchmark
datasets under different settings, which verify the effectiveness of our attack
and its resistance to existing backdoor defenses. The codes for reproducing
main experiments are available at \url{https://github.com/YinghuaGao/SIBA}.
</p></li>
</ul>

<h3>Title: Adversarial Attacks on Leakage Detectors in Water Distribution Networks. (arXiv:2306.06107v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06107">http://arxiv.org/abs/2306.06107</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06107] Adversarial Attacks on Leakage Detectors in Water Distribution Networks](http://arxiv.org/abs/2306.06107) #attack</code></li>
<li>Summary: <p>Many Machine Learning models are vulnerable to adversarial attacks: There
exist methodologies that add a small (imperceptible) perturbation to an input
such that the model comes up with a wrong prediction. Better understanding of
such attacks is crucial in particular for models used in security-critical
domains, such as monitoring of water distribution networks, in order to devise
counter-measures enhancing model robustness and trustworthiness.
</p></li>
</ul>

<p>We propose a taxonomy for adversarial attacks against machine learning based
leakage detectors in water distribution networks. Following up on this, we
focus on a particular type of attack: an adversary searching the least
sensitive point, that is, the location in the water network where the largest
possible undetected leak could occur. Based on a mathematical formalization of
the least sensitive point problem, we use three different algorithmic
approaches to find a solution. Results are evaluated on two benchmark water
distribution networks.
</p>

<h3>Title: Front-running Attack in Distributed Sharded Ledgers and Fair Cross-shard Consensus. (arXiv:2306.06299v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06299">http://arxiv.org/abs/2306.06299</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06299] Front-running Attack in Distributed Sharded Ledgers and Fair Cross-shard Consensus](http://arxiv.org/abs/2306.06299) #attack</code></li>
<li>Summary: <p>Sharding is a prominent technique for scaling blockchains. By dividing the
network into smaller components known as shards, a sharded blockchain can
process transactions in parallel without introducing inconsistencies through
the coordination of intra-shard and cross-shard consensus protocols. However,
we observe a critical security issue with sharded systems: transaction ordering
manipulations can occur when coordinating intra-shard and cross-shard consensus
protocols, leaving the system vulnerable to attack. Specifically, we identify a
novel security issue known as finalization fairness, which can be exploited
through a front-running attack. This attack allows an attacker to manipulate
the execution order of transactions, even if the victim's transaction has
already been processed and added to the blockchain by a fair intra-shard
consensus.
</p></li>
</ul>

<p>To address the issue, we offer Haechi, a novel cross-shard protocol that is
immune to front-running attacks. Haechi introduces an ordering phase between
transaction processing and execution, ensuring that the execution order of
transactions is the same as the processing order and achieving finalization
fairness. To accommodate different consensus speeds among shards, Haechi
incorporates a finalization fairness algorithm to achieve a globally fair order
with minimal performance loss. By providing a global order, Haechi ensures
strong consistency among shards, enabling better parallelism in handling
conflicting transactions across shards. These features make Haechi a promising
solution for supporting popular smart contracts in the real world. To evaluate
Haechi's performance, we implemented the protocol using Tendermint and
conducted extensive experiments on a geo-distributed AWS environment. Our
results demonstrate that Haechi achieves finalization fairness with little
performance sacrifice compared to existing cross-shard consensus protocols.
</p>

<h2>robust</h2>
<h3>Title: PotatoPestNet: A CTInceptionV3-RS-Based Neural Network for Accurate Identification of Potato Pests. (arXiv:2306.06206v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06206">http://arxiv.org/abs/2306.06206</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06206] PotatoPestNet: A CTInceptionV3-RS-Based Neural Network for Accurate Identification of Potato Pests](http://arxiv.org/abs/2306.06206) #robust</code></li>
<li>Summary: <p>Potatoes are the third-largest food crop globally, but their production
frequently encounters difficulties because of aggressive pest infestations. The
aim of this study is to investigate the various types and characteristics of
these pests and propose an efficient PotatoPestNet AI-based automatic potato
pest identification system. To accomplish this, we curated a reliable dataset
consisting of eight types of potato pests. We leveraged the power of transfer
learning by employing five customized, pre-trained transfer learning models:
CMobileNetV2, CNASLargeNet, CXception, CDenseNet201, and CInceptionV3, in
proposing a robust PotatoPestNet model to accurately classify potato pests. To
improve the models' performance, we applied various augmentation techniques,
incorporated a global average pooling layer, and implemented proper
regularization methods. To further enhance the performance of the models, we
utilized random search (RS) optimization for hyperparameter tuning. This
optimization technique played a significant role in fine-tuning the models and
achieving improved performance. We evaluated the models both visually and
quantitatively, utilizing different evaluation metrics. The robustness of the
models in handling imbalanced datasets was assessed using the Receiver
Operating Characteristic (ROC) curve. Among the models, the Customized Tuned
Inception V3 (CTInceptionV3) model, optimized through random search,
demonstrated outstanding performance. It achieved the highest accuracy (91%),
precision (91%), recall (91%), and F1-score (91%), showcasing its superior
ability to accurately identify and classify potato pests.
</p></li>
</ul>

<h3>Title: A Differential Testing Framework to Evaluate Image Recognition Model Robustness. (arXiv:2306.06208v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06208">http://arxiv.org/abs/2306.06208</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06208] A Differential Testing Framework to Evaluate Image Recognition Model Robustness](http://arxiv.org/abs/2306.06208) #robust</code></li>
<li>Summary: <p>Image recognition tasks typically use deep learning and require enormous
processing power, thus relying on hardware accelerators like GPUs and TPUs for
fast, timely processing. Failure in real-time image recognition tasks can occur
due to sub-optimal mapping on hardware accelerators during model deployment,
which may lead to timing uncertainty and erroneous behavior. Mapping on
hardware accelerators is done through multiple software components like deep
learning frameworks, compilers, device libraries, that we refer to as the
computational environment. Owing to the increased use of image recognition
tasks in safety-critical applications like autonomous driving and medical
imaging, it is imperative to assess their robustness to changes in the
computational environment, as the impact of parameters like deep learning
frameworks, compiler optimizations, and hardware devices on model performance
and correctness is not well understood.
</p></li>
</ul>

<p>In this paper we present a differential testing framework, which allows deep
learning model variant generation, execution, differential analysis and testing
for a number of computational environment parameters. Using our framework, we
conduct an empirical study of robustness analysis of three popular image
recognition models using the ImageNet dataset, assessing the impact of changing
deep learning frameworks, compiler optimizations, and hardware devices. We
report the impact in terms of misclassifications and inference time differences
across different settings. In total, we observed up to 72% output label
differences across deep learning frameworks, and up to 82% unexpected
performance degradation in terms of inference time, when applying compiler
optimizations. Using the analysis tools in our framework, we also perform fault
analysis to understand the reasons for the observed differences.
</p>

<h3>Title: Aladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions. (arXiv:2306.06212v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06212">http://arxiv.org/abs/2306.06212</a></li>
<li>Code URL: <a href="https://github.com/ianhuang0630/aladdin">https://github.com/ianhuang0630/aladdin</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06212] Aladdin: Zero-Shot Hallucination of Stylized 3D Assets from Abstract Scene Descriptions](http://arxiv.org/abs/2306.06212) #robust</code></li>
<li>Summary: <p>What constitutes the "vibe" of a particular scene? What should one find in "a
busy, dirty city street", "an idyllic countryside", or "a crime scene in an
abandoned living room"? The translation from abstract scene descriptions to
stylized scene elements cannot be done with any generality by extant systems
trained on rigid and limited indoor datasets. In this paper, we propose to
leverage the knowledge captured by foundation models to accomplish this
translation. We present a system that can serve as a tool to generate stylized
assets for 3D scenes described by a short phrase, without the need to enumerate
the objects to be found within the scene or give instructions on their
appearance. Additionally, it is robust to open-world concepts in a way that
traditional methods trained on limited data are not, affording more creative
freedom to the 3D artist. Our system demonstrates this using a foundation model
"team" composed of a large language model, a vision-language model and several
image diffusion models, which communicate using an interpretable and
user-editable intermediate representation, thus allowing for more versatile and
controllable stylized asset generation for 3D artists. We introduce novel
metrics for this task, and show through human evaluations that in 91% of the
cases, our system outputs are judged more faithful to the semantics of the
input scene description than the baseline, thus highlighting the potential of
this approach to radically accelerate the 3D content creation process for 3D
artists.
</p></li>
</ul>

<h3>Title: EventCLIP: Adapting CLIP for Event-based Object Recognition. (arXiv:2306.06354v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06354">http://arxiv.org/abs/2306.06354</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06354] EventCLIP: Adapting CLIP for Event-based Object Recognition](http://arxiv.org/abs/2306.06354) #robust</code></li>
<li>Summary: <p>Recent advances in 2D zero-shot and few-shot recognition often leverage large
pre-trained vision-language models (VLMs) such as CLIP. Due to a shortage of
suitable datasets, it is currently infeasible to train such models for event
camera data. Thus, leveraging existing models across modalities is an important
research challenge. In this work, we propose EventCLIP, a new method that
utilizes CLIP for zero-shot and few-shot recognition on event camera data.
First, we demonstrate the suitability of CLIP's image embeddings for zero-shot
event classification by converting raw events to 2D grid-based representations.
Second, we propose a feature adapter that aggregates temporal information over
event frames and refines text embeddings to better align with the visual
inputs. We evaluate our work on N-Caltech, N-Cars, and N-ImageNet datasets
under the few-shot learning setting, where EventCLIP achieves state-of-the-art
performance. Finally, we show that the robustness of existing event-based
classifiers against data variations can be further boosted by ensembling with
EventCLIP.
</p></li>
</ul>

<h3>Title: SentiGOLD: A Large Bangla Gold Standard Multi-Domain Sentiment Analysis Dataset and its Evaluation. (arXiv:2306.06147v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06147">http://arxiv.org/abs/2306.06147</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06147] SentiGOLD: A Large Bangla Gold Standard Multi-Domain Sentiment Analysis Dataset and its Evaluation](http://arxiv.org/abs/2306.06147) #robust</code></li>
<li>Summary: <p>This study introduces SentiGOLD, a Bangla multi-domain sentiment analysis
dataset. Comprising 70,000 samples, it was created from diverse sources and
annotated by a gender-balanced team of linguists. SentiGOLD adheres to
established linguistic conventions agreed upon by the Government of Bangladesh
and a Bangla linguistics committee. Unlike English and other languages, Bangla
lacks standard sentiment analysis datasets due to the absence of a national
linguistics framework. The dataset incorporates data from online video
comments, social media posts, blogs, news, and other sources while maintaining
domain and class distribution rigorously. It spans 30 domains (e.g., politics,
entertainment, sports) and includes 5 sentiment classes (strongly negative,
weakly negative, neutral, and strongly positive). The annotation scheme,
approved by the national linguistics committee, ensures a robust Inter
Annotator Agreement (IAA) with a Fleiss' kappa score of 0.88. Intra- and
cross-dataset evaluation protocols are applied to establish a standard
classification system. Cross-dataset evaluation on the noisy SentNoB dataset
presents a challenging test scenario. Additionally, zero-shot experiments
demonstrate the generalizability of SentiGOLD. The top model achieves a macro
f1 score of 0.62 (intra-dataset) across 5 classes, setting a benchmark, and
0.61 (cross-dataset from SentNoB) across 3 classes, comparable to the
state-of-the-art. Fine-tuned sentiment analysis model can be accessed at
https://sentiment.bangla.gov.bd.
</p></li>
</ul>

<h3>Title: Probing self-supervised speech models for phonetic and phonemic information: a case study in aspiration. (arXiv:2306.06232v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06232">http://arxiv.org/abs/2306.06232</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06232] Probing self-supervised speech models for phonetic and phonemic information: a case study in aspiration](http://arxiv.org/abs/2306.06232) #robust</code></li>
<li>Summary: <p>Textless self-supervised speech models have grown in capabilities in recent
years, but the nature of the linguistic information they encode has not yet
been thoroughly examined. We evaluate the extent to which these models' learned
representations align with basic representational distinctions made by humans,
focusing on a set of phonetic (low-level) and phonemic (more abstract)
contrasts instantiated in word-initial stops. We find that robust
representations of both phonetic and phonemic distinctions emerge in early
layers of these models' architectures, and are preserved in the principal
components of deeper layer representations. Our analyses suggest two sources
for this success: some can only be explained by the optimization of the models
on speech data, while some can be attributed to these models' high-dimensional
architectures. Our findings show that speech-trained HuBERT derives a low-noise
and low-dimensional subspace corresponding to abstract phonological
distinctions.
</p></li>
</ul>

<h3>Title: Robustness Testing for Multi-Agent Reinforcement Learning: State Perturbations on Critical Agents. (arXiv:2306.06136v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06136">http://arxiv.org/abs/2306.06136</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06136] Robustness Testing for Multi-Agent Reinforcement Learning: State Perturbations on Critical Agents](http://arxiv.org/abs/2306.06136) #robust</code></li>
<li>Summary: <p>Multi-Agent Reinforcement Learning (MARL) has been widely applied in many
fields such as smart traffic and unmanned aerial vehicles. However, most MARL
algorithms are vulnerable to adversarial perturbations on agent states.
Robustness testing for a trained model is an essential step for confirming the
trustworthiness of the model against unexpected perturbations. This work
proposes a novel Robustness Testing framework for MARL that attacks states of
Critical Agents (RTCA). The RTCA has two innovations: 1) a Differential
Evolution (DE) based method to select critical agents as victims and to advise
the worst-case joint actions on them; and 2) a team cooperation policy
evaluation method employed as the objective function for the optimization of
DE. Then, adversarial state perturbations of the critical agents are generated
based on the worst-case joint actions. This is the first robustness testing
framework with varying victim agents. RTCA demonstrates outstanding performance
in terms of the number of victim agents and destroying cooperation policies.
</p></li>
</ul>

<h3>Title: Robust Twin Parametric Margin Support Vector Machine for Multiclass Classification. (arXiv:2306.06213v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06213">http://arxiv.org/abs/2306.06213</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06213] Robust Twin Parametric Margin Support Vector Machine for Multiclass Classification](http://arxiv.org/abs/2306.06213) #robust</code></li>
<li>Summary: <p>In this paper we present a Twin Parametric-Margin Support Vector Machine
(TPMSVM) model to tackle the problem of multiclass classification. In the
spirit of one-versus-all paradigm, for each class we construct a classifier by
solving a TPMSVM-type model. Once all classifiers have been determined, they
are combined into an aggregate decision function. We consider the cases of both
linear and nonlinear kernel-induced classifiers. In addition, we robustify the
proposed approach through robust optimization techniques. Indeed, in real-world
applications observations are subject to measurement errors and noise,
affecting the quality of the solutions. Consequently, data uncertainties need
to be included within the model in order to prevent low accuracies in the
classification process. Preliminary computational experiments on real-world
datasets show the good performance of the proposed approach.
</p></li>
</ul>

<h3>Title: Machine Learning Based Missing Values Imputation in Categorical Datasets. (arXiv:2306.06338v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06338">http://arxiv.org/abs/2306.06338</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06338] Machine Learning Based Missing Values Imputation in Categorical Datasets](http://arxiv.org/abs/2306.06338) #robust</code></li>
<li>Summary: <p>This study explored the use of machine learning algorithms for predicting and
imputing missing values in categorical datasets. We focused on ensemble models
that use the error correction output codes (ECOC) framework, including
SVM-based and KNN-based ensemble models, as well as an ensemble classifier that
combines SVM, KNN, and MLP models. We applied these algorithms to three
datasets: the CPU dataset, the hypothyroid dataset, and the Breast Cancer
dataset. Our experiments showed that the machine learning algorithms were able
to achieve good performance in predicting and imputing the missing values, with
some variations depending on the specific dataset and missing value pattern.
The ensemble models using the error correction output codes (ECOC) framework
were particularly effective in improving the accuracy and robustness of the
predictions, compared to individual models. However, there are also challenges
and limitations to using deep learning for missing value imputation, including
the need for large amounts of labeled data and the potential for overfitting.
Further research is needed to evaluate the effectiveness and efficiency of deep
learning algorithms for missing value imputation and to develop strategies for
addressing the challenges and limitations that may arise.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Zero-Shot Dialogue Relation Extraction by Relating Explainable Triggers and Relation Names. (arXiv:2306.06141v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06141">http://arxiv.org/abs/2306.06141</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06141] Zero-Shot Dialogue Relation Extraction by Relating Explainable Triggers and Relation Names](http://arxiv.org/abs/2306.06141) #extraction</code></li>
<li>Summary: <p>Developing dialogue relation extraction (DRE) systems often requires a large
amount of labeled data, which can be costly and time-consuming to annotate. In
order to improve scalability and support diverse, unseen relation extraction,
this paper proposes a method for leveraging the ability to capture triggers and
relate them to previously unseen relation names. Specifically, we introduce a
model that enables zero-shot dialogue relation extraction by utilizing
trigger-capturing capabilities. Our experiments on a benchmark DialogRE dataset
demonstrate that the proposed model achieves significant improvements for both
seen and unseen relations. Notably, this is the first attempt at zero-shot
dialogue relation extraction using trigger-capturing capabilities, and our
results suggest that this approach is effective for inferring previously unseen
relation types. Overall, our findings highlight the potential for this method
to enhance the scalability and practicality of DRE systems.
</p></li>
</ul>

<h3>Title: Towards Arabic Multimodal Dataset for Sentiment Analysis. (arXiv:2306.06322v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06322">http://arxiv.org/abs/2306.06322</a></li>
<li>Code URL: <a href="https://github.com/belgats/arabic-multimodal-dataset">https://github.com/belgats/arabic-multimodal-dataset</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06322] Towards Arabic Multimodal Dataset for Sentiment Analysis](http://arxiv.org/abs/2306.06322) #extraction</code></li>
<li>Summary: <p>Multimodal Sentiment Analysis (MSA) has recently become a centric research
direction for many real-world applications. This proliferation is due to the
fact that opinions are central to almost all human activities and are key
influencers of our behaviors. In addition, the recent deployment of Deep
Learning-based (DL) models has proven their high efficiency for a wide range of
Western languages. In contrast, Arabic DL-based multimodal sentiment analysis
(MSA) is still in its infantile stage due, mainly, to the lack of standard
datasets. In this paper, our investigation is twofold. First, we design a
pipeline that helps building our Arabic Multimodal dataset leveraging both
state-of-the-art transformers and feature extraction tools within word
alignment techniques. Thereafter, we validate our dataset using
state-of-the-art transformer-based model dealing with multimodality. Despite
the small size of the outcome dataset, experiments show that Arabic
multimodality is very promising
</p></li>
</ul>

<h3>Title: AVScan2Vec: Feature Learning on Antivirus Scan Data for Production-Scale Malware Corpora. (arXiv:2306.06228v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06228">http://arxiv.org/abs/2306.06228</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06228] AVScan2Vec: Feature Learning on Antivirus Scan Data for Production-Scale Malware Corpora](http://arxiv.org/abs/2306.06228) #extraction</code></li>
<li>Summary: <p>When investigating a malicious file, searching for related files is a common
task that malware analysts must perform. Given that production malware corpora
may contain over a billion files and consume petabytes of storage, many feature
extraction and similarity search approaches are computationally infeasible. Our
work explores the potential of antivirus (AV) scan data as a scalable source of
features for malware. This is possible because AV scan reports are widely
available through services such as VirusTotal and are ~100x smaller than the
average malware sample. The information within an AV scan report is abundant
with information and can indicate a malicious file's family, behavior, target
operating system, and many other characteristics. We introduce AVScan2Vec, a
language model trained to comprehend the semantics of AV scan data. AVScan2Vec
ingests AV scan data for a malicious file and outputs a meaningful vector
representation. AVScan2Vec vectors are ~3 to 85x smaller than popular
alternatives in use today, enabling faster vector comparisons and lower memory
usage. By incorporating Dynamic Continuous Indexing, we show that
nearest-neighbor queries on AVScan2Vec vectors can scale to even the largest
malware production datasets. We also demonstrate that AVScan2Vec vectors are
superior to other leading malware feature vector representations across nearly
all classification, clustering, and nearest-neighbor lookup algorithms that we
evaluated.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h2>fair</h2>
<h3>Title: Safety and Fairness for Content Moderation in Generative Models. (arXiv:2306.06135v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06135">http://arxiv.org/abs/2306.06135</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06135] Safety and Fairness for Content Moderation in Generative Models](http://arxiv.org/abs/2306.06135) #fair</code></li>
<li>Summary: <p>With significant advances in generative AI, new technologies are rapidly
being deployed with generative components. Generative models are typically
trained on large datasets, resulting in model behaviors that can mimic the
worst of the content in the training data. Responsible deployment of generative
technologies requires content moderation strategies, such as safety input and
output filters. Here, we provide a theoretical framework for conceptualizing
responsible content moderation of text-to-image generative technologies,
including a demonstration of how to empirically measure the constructs we
enumerate. We define and distinguish the concepts of safety, fairness, and
metric equity, and enumerate example harms that can come in each domain. We
then provide a demonstration of how the defined harms can be quantified. We
conclude with a summary of how the style of harms quantification we demonstrate
enables data-driven content moderation decisions.
</p></li>
</ul>

<h2>interpretability</h2>
<h2>explainability</h2>
<h3>Title: Understanding the Benefits of Image Augmentations. (arXiv:2306.06254v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06254">http://arxiv.org/abs/2306.06254</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06254] Understanding the Benefits of Image Augmentations](http://arxiv.org/abs/2306.06254) #explainability</code></li>
<li>Summary: <p>Image Augmentations are widely used to reduce overfitting in neural networks.
However, the explainability of their benefits largely remains a mystery. We
study which layers of residual neural networks (ResNets) are most affected by
augmentations using Centered Kernel Alignment (CKA). We do so by analyzing
models of varying widths and depths, as well as whether their weights are
initialized randomly or through transfer learning. We find that the pattern of
how the layers are affected depends on the model's depth, and that networks
trained with augmentation that use information from two images affect the
learned weights significantly more than augmentations that operate on a single
image. Deeper layers of ResNets initialized with ImageNet-1K weights and
fine-tuned receive more impact from the augmentations than early layers.
Understanding the effects of image augmentations on CNNs will have a variety of
applications, such as determining how far back one needs to fine-tune a network
and which layers should be frozen when implementing layer freezing algorithms.
</p></li>
</ul>

<h3>Title: Sound Explanation for Trustworthy Machine Learning. (arXiv:2306.06134v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06134">http://arxiv.org/abs/2306.06134</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06134] Sound Explanation for Trustworthy Machine Learning](http://arxiv.org/abs/2306.06134) #explainability</code></li>
<li>Summary: <p>We take a formal approach to the explainability problem of machine learning
systems. We argue against the practice of interpreting black-box models via
attributing scores to input components due to inherently conflicting goals of
attribution-based interpretation. We prove that no attribution algorithm
satisfies specificity, additivity, completeness, and baseline invariance. We
then formalize the concept, sound explanation, that has been informally adopted
in prior work. A sound explanation entails providing sufficient information to
causally explain the predictions made by a system. Finally, we present the
application of feature selection as a sound explanation for cancer prediction
models to cultivate trust among clinicians.
</p></li>
</ul>

<h3>Title: Explaining a machine learning decision to physicians via counterfactuals. (arXiv:2306.06325v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06325">http://arxiv.org/abs/2306.06325</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06325] Explaining a machine learning decision to physicians via counterfactuals](http://arxiv.org/abs/2306.06325) #explainability</code></li>
<li>Summary: <p>Machine learning models perform well on several healthcare tasks and can help
reduce the burden on the healthcare system. However, the lack of explainability
is a major roadblock to their adoption in hospitals. \textit{How can the
decision of an ML model be explained to a physician?} The explanations
considered in this paper are counterfactuals (CFs), hypothetical scenarios that
would have resulted in the opposite outcome. Specifically, time-series CFs are
investigated, inspired by the way physicians converse and reason out decisions
`I would have given the patient a vasopressor if their blood pressure was lower
and falling'. Key properties of CFs that are particularly meaningful in
clinical settings are outlined: physiological plausibility, relevance to the
task and sparse perturbations. Past work on CF generation does not satisfy
these properties, specifically plausibility in that realistic time-series CFs
are not generated. A variational autoencoder (VAE)-based approach is proposed
that captures these desired properties. The method produces CFs that improve on
prior approaches quantitatively (more plausible CFs as evaluated by their
likelihood w.r.t original data distribution, and 100$\times$ faster at
generating CFs) and qualitatively (2$\times$ more plausible and relevant) as
evaluated by three physicians.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Surrogate Modeling of Car Drag Coefficient with Depth and Normal Renderings. (arXiv:2306.06110v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06110">http://arxiv.org/abs/2306.06110</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06110] Surrogate Modeling of Car Drag Coefficient with Depth and Normal Renderings](http://arxiv.org/abs/2306.06110) #diffusion</code></li>
<li>Summary: <p>Generative AI models have made significant progress in automating the
creation of 3D shapes, which has the potential to transform car design. In
engineering design and optimization, evaluating engineering metrics is crucial.
To make generative models performance-aware and enable them to create
high-performing designs, surrogate modeling of these metrics is necessary.
However, the currently used representations of three-dimensional (3D) shapes
either require extensive computational resources to learn or suffer from
significant information loss, which impairs their effectiveness in surrogate
modeling. To address this issue, we propose a new two-dimensional (2D)
representation of 3D shapes. We develop a surrogate drag model based on this
representation to verify its effectiveness in predicting 3D car drag. We
construct a diverse dataset of 9,070 high-quality 3D car meshes labeled by drag
coefficients computed from computational fluid dynamics (CFD) simulations to
train our model. Our experiments demonstrate that our model can accurately and
efficiently evaluate drag coefficients with an $R^2$ value above 0.84 for
various car categories. Moreover, the proposed representation method can be
generalized to many other product categories beyond cars. Our model is
implemented using deep neural networks, making it compatible with recent AI
image generation tools (such as Stable Diffusion) and a significant step
towards the automatic generation of drag-optimized car designs. We have made
the dataset and code publicly available at
https://decode.mit.edu/projects/dragprediction/.
</p></li>
</ul>

<h3>Title: A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering. (arXiv:2306.06211v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06211">http://arxiv.org/abs/2306.06211</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06211] A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering](http://arxiv.org/abs/2306.06211) #diffusion</code></li>
<li>Summary: <p>Segment anything model (SAM) developed by Meta AI Research has recently
attracted significant attention. Trained on a large segmentation dataset of
over 1 billion masks, SAM is capable of segmenting any object on a certain
image. In the original SAM work, the authors turned to zero-short transfer
tasks (like edge detection) for evaluating the performance of SAM. Recently,
numerous works have attempted to investigate the performance of SAM in various
scenarios to recognize and segment objects. Moreover, numerous projects have
emerged to show the versatility of SAM as a foundation model by combining it
with other models, like Grounding DINO, Stable Diffusion, ChatGPT, etc. With
the relevant papers and projects increasing exponentially, it is challenging
for the readers to catch up with the development of SAM. To this end, this work
conducts the first yet comprehensive survey on SAM. This is an ongoing project
and we intend to update the manuscript on a regular basis. Therefore, readers
are welcome to contact us if they complete new works related to SAM so that we
can include them in our next version.
</p></li>
</ul>

<h3>Title: How to Learn and Generalize From Three Minutes of Data: Physics-Constrained and Uncertainty-Aware Neural Stochastic Differential Equations. (arXiv:2306.06335v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06335">http://arxiv.org/abs/2306.06335</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06335] How to Learn and Generalize From Three Minutes of Data: Physics-Constrained and Uncertainty-Aware Neural Stochastic Differential Equations](http://arxiv.org/abs/2306.06335) #diffusion</code></li>
<li>Summary: <p>We present a framework and algorithms to learn controlled dynamics models
using neural stochastic differential equations (SDEs) -- SDEs whose drift and
diffusion terms are both parametrized by neural networks. We construct the
drift term to leverage a priori physics knowledge as inductive bias, and we
design the diffusion term to represent a distance-aware estimate of the
uncertainty in the learned model's predictions -- it matches the system's
underlying stochasticity when evaluated on states near those from the training
dataset, and it predicts highly stochastic dynamics when evaluated on states
beyond the training regime. The proposed neural SDEs can be evaluated quickly
enough for use in model predictive control algorithms, or they can be used as
simulators for model-based reinforcement learning. Furthermore, they make
accurate predictions over long time horizons, even when trained on small
datasets that cover limited regions of the state space. We demonstrate these
capabilities through experiments on simulated robotic systems, as well as by
using them to model and control a hexacopter's flight dynamics: A neural SDE
trained using only three minutes of manually collected flight data results in a
model-based control policy that accurately tracks aggressive trajectories that
push the hexacopter's velocity and Euler angles to nearly double the maximum
values observed in the training dataset.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Deep Learning Method for Object Tracking, Velocity Estimation and Projection of Sensor Data over Time. (arXiv:2306.06126v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06126">http://arxiv.org/abs/2306.06126</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06126] Deep Learning Method for Object Tracking, Velocity Estimation and Projection of Sensor Data over Time](http://arxiv.org/abs/2306.06126) #transformer</code></li>
<li>Summary: <p>Current Deep Learning methods for environment segmentation and velocity
estimation rely on Convolutional Recurrent Neural Networks to exploit
spatio-temporal relationships within obtained sensor data. These approaches
derive scene dynamics implicitly by correlating novel input and memorized data
utilizing ConvNets. We show how ConvNets suffer from architectural restrictions
for this task. Based on these findings, we then provide solutions to various
issues on exploiting spatio-temporal correlations in a sequence of sensor
recordings by presenting a novel Recurrent Neural Network unit utilizing
Transformer mechanisms. Within this unit, object encodings are tracked across
consecutive frames by correlating key-query pairs derived from sensor inputs
and memory states, respectively. We then use resulting tracking patterns to
obtain scene dynamics and regress velocities. In a last step, the memory state
of the Recurrent Neural Network is projected based on extracted velocity
estimates to resolve aforementioned spatio-temporal misalignment.
</p></li>
</ul>

<h3>Title: Read, look and detect: Bounding box annotation from image-caption pairs. (arXiv:2306.06149v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06149">http://arxiv.org/abs/2306.06149</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06149] Read, look and detect: Bounding box annotation from image-caption pairs](http://arxiv.org/abs/2306.06149) #transformer</code></li>
<li>Summary: <p>Various methods have been proposed to detect objects while reducing the cost
of data annotation. For instance, weakly supervised object detection (WSOD)
methods rely only on image-level annotations during training. Unfortunately,
data annotation remains expensive since annotators must provide the categories
describing the content of each image and labeling is restricted to a fixed set
of categories. In this paper, we propose a method to locate and label objects
in an image by using a form of weaker supervision: image-caption pairs. By
leveraging recent advances in vision-language (VL) models and self-supervised
vision transformers (ViTs), our method is able to perform phrase grounding and
object detection in a weakly supervised manner. Our experiments demonstrate the
effectiveness of our approach by achieving a 47.51% recall@1 score in phrase
grounding on Flickr30k Entities and establishing a new state-of-the-art in
object detection by achieving 21.1 mAP 50 and 10.5 mAP 50:95 on MS COCO when
exclusively relying on image-caption pairs.
</p></li>
</ul>

<h3>Title: FasterViT: Fast Vision Transformers with Hierarchical Attention. (arXiv:2306.06189v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06189">http://arxiv.org/abs/2306.06189</a></li>
<li>Code URL: <a href="https://github.com/NVlabs/FasterViT">https://github.com/NVlabs/FasterViT</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06189] FasterViT: Fast Vision Transformers with Hierarchical Attention](http://arxiv.org/abs/2306.06189) #transformer</code></li>
<li>Summary: <p>We design a new family of hybrid CNN-ViT neural networks, named FasterViT,
with a focus on high image throughput for computer vision (CV) applications.
FasterViT combines the benefits of fast local representation learning in CNNs
and global modeling properties in ViT. Our newly introduced Hierarchical
Attention (HAT) approach decomposes global self-attention with quadratic
complexity into a multi-level attention with reduced computational costs. We
benefit from efficient window-based self-attention. Each window has access to
dedicated carrier tokens that participate in local and global representation
learning. At a high level, global self-attentions enable the efficient
cross-window communication at lower costs. FasterViT achieves a SOTA
Pareto-front in terms of accuracy \vs image throughput. We have extensively
validated its effectiveness on various CV tasks including classification,
object detection and segmentation. We also show that HAT can be used as a
plug-and-play module for existing networks and enhance them. We further
demonstrate significantly faster and more accurate performance than competitive
counterparts for images with high resolution. Code is available at
https://github.com/NVlabs/FasterViT.
</p></li>
</ul>

<h3>Title: FLSL: Feature-level Self-supervised Learning. (arXiv:2306.06203v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06203">http://arxiv.org/abs/2306.06203</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06203] FLSL: Feature-level Self-supervised Learning](http://arxiv.org/abs/2306.06203) #transformer</code></li>
<li>Summary: <p>Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO, VICReg,
MOCOv3) target primarily on representations at instance level and do not
generalize well to dense prediction tasks, such as object detection and
segmentation. Towards aligning SSL with dense predictions, this paper
demonstrates for the first time the underlying mean-shift clustering process of
Vision Transformers (ViT), which aligns well with natural image semantics
(e.g., a world of objects and stuffs). By employing transformer for joint
embedding and clustering, we propose a two-level feature clustering SSL method,
coined Feature-Level Self-supervised Learning (FLSL). We present the formal
definition of the FLSL problem and construct the objectives from the mean-shift
and k-means perspectives. We show that FLSL promotes remarkable semantic
cluster representations and learns an embedding scheme amenable to intra-view
and inter-view feature clustering. Experiments show that FLSL yields
significant improvements in dense prediction tasks, achieving 44.9 (+2.8)% AP
and 46.5% AP in object detection, as well as 40.8 (+2.3)% AP and 42.1% AP in
instance segmentation on MS-COCO, using Mask R-CNN with ViT-S/16 and ViT-S/8 as
backbone, respectively. FLSL consistently outperforms existing SSL methods
across additional benchmarks, including UAV object detection on UAVDT, and
video instance segmentation on DAVIS 2017. We conclude by presenting
visualization and various ablation studies to better 20 understand the success
of FLSL.
</p></li>
</ul>

<h3>Title: SegViTv2: Exploring Efficient and Continual Semantic Segmentation with Plain Vision Transformers. (arXiv:2306.06289v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06289">http://arxiv.org/abs/2306.06289</a></li>
<li>Code URL: <a href="https://github.com/zbwxp/SegVit">https://github.com/zbwxp/SegVit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06289] SegViTv2: Exploring Efficient and Continual Semantic Segmentation with Plain Vision Transformers](http://arxiv.org/abs/2306.06289) #transformer</code></li>
<li>Summary: <p>We explore the capability of plain Vision Transformers (ViTs) for semantic
segmentation using the encoder-decoder framework and introduce SegViTv2. In our
work, we implement the decoder with the global attention mechanism inherent in
ViT backbones and propose the lightweight Attention-to-Mask module that
effectively converts the global attention map into semantic masks for
high-quality segmentation results. Our decoder can outperform the most
commonly-used decoder UpperNet in various ViT backbones while consuming only
about 5\% of the computational cost. For the encoder, we address the concern of
the relatively high computational cost in the ViT-based encoders and propose a
Shrunk++ structure that incorporates edge-aware query-based down-sampling (EQD)
and query-based up-sampling (QU) modules. The Shrunk++ structure reduces the
computational cost of the encoder by up to $50\%$ while maintaining competitive
performance. Furthermore, due to the flexibility of our ViT-based architecture,
SegVit can be easily extended to semantic segmentation under the setting of
continual learning, achieving nearly zero forgetting. Experiments show that our
proposed SegViT outperforms recent segmentation methods on three popular
benchmarks including ADE20k, COCO-Stuff-10k and PASCAL-Context datasets. The
code is available through the following link:
\url{https://github.com/zbwxp/SegVit}.
</p></li>
</ul>

<h3>Title: Shuffled Autoregression For Motion Interpolation. (arXiv:2306.06367v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06367">http://arxiv.org/abs/2306.06367</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06367] Shuffled Autoregression For Motion Interpolation](http://arxiv.org/abs/2306.06367) #transformer</code></li>
<li>Summary: <p>This work aims to provide a deep-learning solution for the motion
interpolation task. Previous studies solve it with geometric weight functions.
Some other works propose neural networks for different problem settings with
consecutive pose sequences as input. However, motion interpolation is a more
complex problem that takes isolated poses (e.g., only one start pose and one
end pose) as input. When applied to motion interpolation, these deep learning
methods have limited performance since they do not leverage the flexible
dependencies between interpolation frames as the original geometric formulas
do. To realize this interpolation characteristic, we propose a novel framework,
referred to as \emph{Shuffled AutoRegression}, which expands the autoregression
to generate in arbitrary (shuffled) order and models any inter-frame
dependencies as a directed acyclic graph. We further propose an approach to
constructing a particular kind of dependency graph, with three stages assembled
into an end-to-end spatial-temporal motion Transformer. Experimental results on
one of the current largest datasets show that our model generates vivid and
coherent motions from only one start frame to one end frame and outperforms
competing methods by a large margin. The proposed model is also extensible to
multiple keyframes' motion interpolation tasks and other areas' interpolation.
</p></li>
</ul>

<h3>Title: $FPDM$: Domain-Specific Fast Pre-training Technique using Document-Level Metadata. (arXiv:2306.06190v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06190">http://arxiv.org/abs/2306.06190</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06190] $FPDM$: Domain-Specific Fast Pre-training Technique using Document-Level Metadata](http://arxiv.org/abs/2306.06190) #transformer</code></li>
<li>Summary: <p>Pre-training Transformers has shown promising results on open-domain and
domain-specific downstream tasks. However, state-of-the-art Transformers
require an unreasonably large amount of pre-training data and compute. In this
paper, we propose $FPDM$ (Fast Pre-training Technique using Document Level
Metadata), a novel, compute-efficient framework that utilizes Document metadata
and Domain-Specific Taxonomy as supervision signals to pre-train transformer
encoder on a domain-specific corpus. The main innovation is that during
domain-specific pretraining, an open-domain encoder is continually pre-trained
using sentence-level embeddings as inputs (to accommodate long documents),
however, fine-tuning is done with token-level embeddings as inputs to this
encoder. We show that $FPDM$ outperforms several transformer-based baselines in
terms of character-level F1 scores and other automated metrics in the Customer
Support, Scientific, and Legal Domains, and shows a negligible drop in
performance on open-domain benchmarks. Importantly, the novel use of
document-level supervision along with sentence-level embedding input for
pre-training reduces pre-training compute by around $1,000$, $4,500$, and $500$
times compared to MLM and/or NSP in Customer Support, Scientific, and Legal
Domains, respectively. Code and datasets are available at
https://bit.ly/FPDMCode.
</p></li>
</ul>

<h3>Title: Morphosyntactic probing of multilingual BERT models. (arXiv:2306.06205v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06205">http://arxiv.org/abs/2306.06205</a></li>
<li>Code URL: <a href="https://github.com/juditacs/morphology-probes">https://github.com/juditacs/morphology-probes</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06205] Morphosyntactic probing of multilingual BERT models](http://arxiv.org/abs/2306.06205) #transformer</code></li>
<li>Summary: <p>We introduce an extensive dataset for multilingual probing of morphological
information in language models (247 tasks across 42 languages from 10
families), each consisting of a sentence with a target word and a morphological
tag as the desired label, derived from the Universal Dependencies treebanks. We
find that pre-trained Transformer models (mBERT and XLM-RoBERTa) learn features
that attain strong performance across these tasks. We then apply two methods to
locate, for each probing task, where the disambiguating information resides in
the input. The first is a new perturbation method that masks various parts of
context; the second is the classical method of Shapley values. The most
intriguing finding that emerges is a strong tendency for the preceding context
to hold more information relevant to the prediction than the following context.
</p></li>
</ul>

<h3>Title: Improving Non-autoregressive Translation Quality with Pretrained Language Model, Embedding Distillation and Upsampling Strategy for CTC. (arXiv:2306.06345v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06345">http://arxiv.org/abs/2306.06345</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06345] Improving Non-autoregressive Translation Quality with Pretrained Language Model, Embedding Distillation and Upsampling Strategy for CTC](http://arxiv.org/abs/2306.06345) #transformer</code></li>
<li>Summary: <p>Non-autoregressive approaches aim to improve the inference speed of
translation models, particularly those that generate output in a one-pass
forward manner. However, these approaches often suffer from a significant drop
in translation quality compared to autoregressive models. This paper introduces
a series of innovative techniques to enhance the translation quality of
Non-Autoregressive Translation (NAT) models while maintaining a substantial
acceleration in inference speed. We propose fine-tuning Pretrained Multilingual
Language Models (PMLMs) with the CTC loss to train NAT models effectively.
Furthermore, we adopt the MASK insertion scheme for up-sampling instead of
token duplication, and we present an embedding distillation method to further
enhance performance. In our experiments, our model outperforms the baseline
autoregressive model (Transformer \textit{base}) on multiple datasets,
including WMT'14 DE$\leftrightarrow$EN, WMT'16 RO$\leftrightarrow$EN, and
IWSLT'14 DE$\leftrightarrow$EN. Notably, our model achieves better performance
than the baseline autoregressive model on the IWSLT'14 En$\leftrightarrow$De
and WMT'16 En$\leftrightarrow$Ro datasets, even without using distillation data
during training. It is worth highlighting that on the IWSLT'14
DE$\rightarrow$EN dataset, our model achieves an impressive BLEU score of
39.59, setting a new state-of-the-art performance. Additionally, our model
exhibits a remarkable speed improvement of 16.35 times compared to the
autoregressive model.
</p></li>
</ul>

<h3>Title: A Comprehensive Review of State-of-The-Art Methods for Java Code Generation from Natural Language Text. (arXiv:2306.06371v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06371">http://arxiv.org/abs/2306.06371</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06371] A Comprehensive Review of State-of-The-Art Methods for Java Code Generation from Natural Language Text](http://arxiv.org/abs/2306.06371) #transformer</code></li>
<li>Summary: <p>Java Code Generation consists in generating automatically Java code from a
Natural Language Text. This NLP task helps in increasing programmers'
productivity by providing them with immediate solutions to the simplest and
most repetitive tasks. Code generation is a challenging task because of the
hard syntactic rules and the necessity of a deep understanding of the semantic
aspect of the programming language. Many works tried to tackle this task using
either RNN-based, or Transformer-based models. The latter achieved remarkable
advancement in the domain and they can be divided into three groups: (1)
encoder-only models, (2) decoder-only models, and (3) encoder-decoder models.
In this paper, we provide a comprehensive review of the evolution and progress
of deep learning models in Java code generation task. We focus on the most
important methods and present their merits and limitations, as well as the
objective functions used by the community. In addition, we provide a detailed
description of datasets and evaluation metrics used in the literature. Finally,
we discuss results of different models on CONCODE dataset, then propose some
future directions.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Single-Model Attribution via Final-Layer Inversion. (arXiv:2306.06210v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06210">http://arxiv.org/abs/2306.06210</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06210] Single-Model Attribution via Final-Layer Inversion](http://arxiv.org/abs/2306.06210) #generative</code></li>
<li>Summary: <p>Recent groundbreaking developments on generative modeling have sparked
interest in practical single-model attribution. Such methods predict whether a
sample was generated by a specific generator or not, for instance, to prove
intellectual property theft. However, previous works are either limited to the
closed-world setting or require undesirable changes of the generative model. We
address these shortcomings by proposing FLIPAD, a new approach for single-model
attribution in the open-world setting based on final-layer inversion and
anomaly detection. We show that the utilized final-layer inversion can be
reduced to a convex lasso optimization problem, making our approach
theoretically sound and computationally efficient. The theoretical findings are
accompanied by an experimental study demonstrating the effectiveness of our
approach, outperforming the existing methods.
</p></li>
</ul>

<h3>Title: Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models. (arXiv:2306.06253v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06253">http://arxiv.org/abs/2306.06253</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06253] Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models](http://arxiv.org/abs/2306.06253) #generative</code></li>
<li>Summary: <p>Reinforcement learning presents an attractive paradigm to reason about
several distinct aspects of sequential decision making, such as specifying
complex goals, planning future observations and actions, and critiquing their
utilities. However, the combined integration of these capabilities poses
competing algorithmic challenges in retaining maximal expressivity while
allowing for flexibility in modeling choices for efficient learning and
inference. We present Decision Stacks, a generative framework that decomposes
goal-conditioned policy agents into 3 generative modules. These modules
simulate the temporal evolution of observations, rewards, and actions via
independent generative models that can be learned in parallel via teacher
forcing. Our framework guarantees both expressivity and flexibility in
designing individual modules to account for key factors such as architectural
bias, optimization objective and dynamics, transferrability across domains, and
inference speed. Our empirical results demonstrate the effectiveness of
Decision Stacks for offline policy optimization for several MDP and POMDP
environments, outperforming existing methods and enabling flexible generative
decision making.
</p></li>
</ul>

<h3>Title: Attention-stacked Generative Adversarial Network (AS-GAN)-empowered Sensor Data Augmentation for Online Monitoring of Manufacturing System. (arXiv:2306.06268v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06268">http://arxiv.org/abs/2306.06268</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06268] Attention-stacked Generative Adversarial Network (AS-GAN)-empowered Sensor Data Augmentation for Online Monitoring of Manufacturing System](http://arxiv.org/abs/2306.06268) #generative</code></li>
<li>Summary: <p>Machine learning (ML) has been extensively adopted for the online
sensing-based monitoring in advanced manufacturing systems. However, the sensor
data collected under abnormal states are usually insufficient, leading to
significant data imbalanced issue for supervised machine learning. A common
solution for this issue is to incorporate data augmentation technique, i.e.,
augmenting the available abnormal states data (i.e., minority samples) via
synthetic generation. To generate the high-quality minority samples
effectively, it is vital to learn the underlying distribution of the abnormal
states data. In recent years, the generative adversarial network (GAN)-based
approaches become popular to learn data distribution as well as perform data
augmentation. However, in practice, the quality of generated samples from
GAN-based data augmentation may vary drastically. In addition, the sensor
signals are collected sequentially by time from the manufacturing systems,
which means the consideration of sequential information is also very important
in data augmentation. To address these limitations, inspired by the multi-head
attention mechanism, this paper proposed an attention-stacked GAN (AS-GAN)
architecture for the sensor data augmentation of online monitoring in advanced
manufacturing. In this proposed AS-GAN, a new attention-stacked framework is
incorporated to strengthen the generator in GAN with the learning capability of
considering sequential information. Furthermore, the developed
attention-stacked framework also greatly helps to improve the quality of
generated sensor signals. The case studies conducted in additive manufacturing
also successfully validate the effectiveness of AS-GAN to augment high-quality
artificial multi-channel sensor signals for online monitoring of manufacturing
systems.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording. (arXiv:2306.06199v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06199">http://arxiv.org/abs/2306.06199</a></li>
<li>Code URL: <a href="https://github.com/tanny411/gpt3-reliability-check">https://github.com/tanny411/gpt3-reliability-check</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06199] Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording](http://arxiv.org/abs/2306.06199) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have become mainstream technology with their
versatile use cases and impressive performance. Despite the countless
out-of-the-box applications, LLMs are still not reliable. A lot of work is
being done to improve the factual accuracy, consistency, and ethical standards
of these models through fine-tuning, prompting, and Reinforcement Learning with
Human Feedback (RLHF), but no systematic analysis of the responses of these
models to different categories of statements, or on their potential
vulnerabilities to simple prompting changes is available. In this work, we
analyze what confuses GPT-3: how the model responds to certain sensitive topics
and what effects the prompt wording has on the model response. We find that
GPT-3 correctly disagrees with obvious Conspiracies and Stereotypes but makes
mistakes with common Misconceptions and Controversies. The model responses are
inconsistent across prompts and settings, highlighting GPT-3's unreliability.
Dataset and code of our analysis is available in
https://github.com/tanny411/GPT3-Reliability-Check.
</p></li>
</ul>

<h3>Title: Measuring and Modifying Factual Knowledge in Large Language Models. (arXiv:2306.06264v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06264">http://arxiv.org/abs/2306.06264</a></li>
<li>Code URL: <a href="https://github.com/rit-git/lm-know">https://github.com/rit-git/lm-know</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06264] Measuring and Modifying Factual Knowledge in Large Language Models](http://arxiv.org/abs/2306.06264) #large language model</code></li>
<li>Summary: <p>Large Language Models (LLMs) store an extensive amount of factual knowledge
obtained from vast collections of text. To effectively utilize these models for
downstream tasks, it is crucial to have reliable methods for measuring their
knowledge. However, existing approaches for knowledge measurement have certain
limitations, and despite recent efforts, they fail to provide accurate
measurements and the necessary insights for modifying the knowledge within
LLMs. In this work, we employ information theory-based measurements to provide
a framework estimating the factual knowledge contained within large language
models. More specifically, we measure knowledge by analyzing the LLM's
prediction probability distribution before and after instilling the target
knowledge, employing metrics such as entropy and KL-divergence. Introducing our
metrics, we first assess their accuracy in comparison to previous ranking-based
methods, surpassing them by over $35\%$ in a synthetic experiment. Then, we
explore two prominent methods of knowledge instillation, discovering that LLMs
exhibit limitations in capturing new knowledge under specific circumstances for
one of these methods. Lastly, we demonstrate the applicability of our methods
in extracting unlearned and mislearned facts in LLMs through their application
to in-context learning. We make code and data for all methods and experiments
in this paper publicly available.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception. (arXiv:2306.06362v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06362">http://arxiv.org/abs/2306.06362</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06362] Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception](http://arxiv.org/abs/2306.06362) #segmentation</code></li>
<li>Summary: <p>We introduce the Aria Digital Twin (ADT) - an egocentric dataset captured
using Aria glasses with extensive object, environment, and human level ground
truth. This ADT release contains 200 sequences of real-world activities
conducted by Aria wearers in two real indoor scenes with 398 object instances
(324 stationary and 74 dynamic). Each sequence consists of: a) raw data of two
monochrome camera streams, one RGB camera stream, two IMU streams; b) complete
sensor calibration; c) ground truth data including continuous
6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eye
gaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d)
photo-realistic synthetic renderings. To the best of our knowledge, there is no
existing egocentric dataset with a level of accuracy, photo-realism and
comprehensiveness comparable to ADT. By contributing ADT to the research
community, our mission is to set a new standard for evaluation in the
egocentric machine perception domain, which includes very challenging research
problems such as 3D object detection and tracking, scene reconstruction and
understanding, sim-to-real learning, human pose prediction - while also
inspiring new machine perception tasks for augmented reality (AR) applications.
To kick start exploration of the ADT research use cases, we evaluated several
existing state-of-the-art methods for object detection, segmentation and image
translation tasks that demonstrate the usefulness of ADT as a benchmarking
dataset.
</p></li>
</ul>

<h3>Title: AutoSAM: Adapting SAM to Medical Images by Overloading the Prompt Encoder. (arXiv:2306.06370v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.06370">http://arxiv.org/abs/2306.06370</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.06370] AutoSAM: Adapting SAM to Medical Images by Overloading the Prompt Encoder](http://arxiv.org/abs/2306.06370) #segmentation</code></li>
<li>Summary: <p>The recently introduced Segment Anything Model (SAM) combines a clever
architecture and large quantities of training data to obtain remarkable image
segmentation capabilities. However, it fails to reproduce such results for
Out-Of-Distribution (OOD) domains such as medical images. Moreover, while SAM
is conditioned on either a mask or a set of points, it may be desirable to have
a fully automatic solution. In this work, we replace SAM's conditioning with an
encoder that operates on the same input image. By adding this encoder and
without further fine-tuning SAM, we obtain state-of-the-art results on multiple
medical images and video benchmarks. This new encoder is trained via gradients
provided by a frozen SAM. For inspecting the knowledge within it, and providing
a lightweight segmentation solution, we also learn to decode it into a mask by
a shallow deconvolution network.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
