<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Secret-Free Device Pairing in the mmWave Band. (arXiv:2306.17330v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17330">http://arxiv.org/abs/2306.17330</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17330] Secret-Free Device Pairing in the mmWave Band](http://arxiv.org/abs/2306.17330) #secure</code></li>
<li>Summary: <p>Many Next Generation (NextG) applications feature devices that are capable of
communicating and sensing in the Millimeter-Wave (mmWave) bands. Trust
establishment is an important first step to bootstrap secure mmWave
communication links, which is challenging due to the lack of prior secrets and
the fact that traditional cryptographic authentication methods cannot bind
digital trust with physical properties. Previously, context-based device
pairing approaches were proposed to extract shared secrets from common context,
using various sensing modalities. However, they suffer from various limitations
in practicality and security.
</p></li>
</ul>

<p>In this work, we propose the first secret-free device pairing scheme in the
mmWave band that explores the unique physical-layer properties of mmWave
communications. Our basic idea is to let Alice and Bob derive common randomness
by sampling physical activity in the surrounding environment that disturbs
their wireless channel. They construct reliable fingerprints of the activity by
extracting event timing information from the channel state. We further propose
an uncoordinated path hopping mechanism to resolve the challenges of beam
alignment for activity sensing without prior trust. A key novelty of our
protocol is that it remains secure against both co-located passive adversaries
and active Man-in-the-Middle attacks, which is not possible with existing
context-based pairing approaches. We implement our protocol in a 28GHz mmWave
testbed, and experimentally evaluate its security in realistic indoor
environments. Results show that our protocol can effectively thwart several
different types of adversaries.
</p>

<h2>security</h2>
<h3>Title: Limits of Machine Learning for Automatic Vulnerability Detection. (arXiv:2306.17193v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17193">http://arxiv.org/abs/2306.17193</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17193] Limits of Machine Learning for Automatic Vulnerability Detection](http://arxiv.org/abs/2306.17193) #security</code></li>
<li>Summary: <p>Recent results of machine learning for automatic vulnerability detection have
been very promising indeed: Given only the source code of a function $f$,
models trained by machine learning techniques can decide if $f$ contains a
security flaw with up to 70% accuracy.
</p></li>
</ul>

<p>But how do we know that these results are general and not specific to the
datasets? To study this question, researchers proposed to amplify the testing
set by injecting semantic preserving changes and found that the model's
accuracy significantly drops. In other words, the model uses some unrelated
features during classification. In order to increase the robustness of the
model, researchers proposed to train on amplified training data, and indeed
model accuracy increased to previous levels.
</p>
<p>In this paper, we replicate and continue this investigation, and provide an
actionable model benchmarking methodology to help researchers better evaluate
advances in machine learning for vulnerability detection. Specifically, we
propose (i) a cross validation algorithm, where a semantic preserving
transformation is applied during the amplification of either the training set
or the testing set, and (ii) the amplification of the testing set with code
snippets where the vulnerabilities are fixed. Using 11 transformations, 3 ML
techniques, and 2 datasets, we find that the improved robustness only applies
to the specific transformations used during training data amplification. In
other words, the robustified models still rely on unrelated features for
predicting the vulnerabilities in the testing data. Additionally, we find that
the trained models are unable to generalize to the modified setting which
requires to distinguish vulnerable functions from their patches.
</p>

<h3>Title: An ontological approach to compliance verification of the NIS 2 directive. (arXiv:2306.17494v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17494">http://arxiv.org/abs/2306.17494</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17494] An ontological approach to compliance verification of the NIS 2 directive](http://arxiv.org/abs/2306.17494) #security</code></li>
<li>Summary: <p>Cybersecurity, which notoriously concerns both human and technological
aspects, is becoming more and more regulated by a number of textual documents
spanning several pages, such as the European GDPR Regulation and the NIS
Directive. This paper introduces an approach that leverages techniques of
semantic representation and reasoning, hence an ontological approach, towards
the compliance check with the security measures that textual documents
prescribe. We choose the ontology instrument to achieve two fundamental
objectives: domain modelling and resource interrogation. The formalisation of
entities and relations from the directive, and the consequent improved
structuring with respect to sheer prose is dramatically helpful for any
organisation through the hard task of compliance verification. The semantic
approach is demonstrated with two articles of the new European NIS 2 directive.
</p></li>
</ul>

<h3>Title: A Quic(k) Security Overview: A Literature Research on Implemented Security Recommendations. (arXiv:2306.17568v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17568">http://arxiv.org/abs/2306.17568</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17568] A Quic(k) Security Overview: A Literature Research on Implemented Security Recommendations](http://arxiv.org/abs/2306.17568) #security</code></li>
<li>Summary: <p>Built on top of UDP, the relatively new QUIC protocol serves as the baseline
for modern web protocol stacks. Equipped with a rich feature set, the protocol
is defined by a 151 pages strong IETF standard complemented by several
additional documents. Enabling fast updates and feature iteration, most QUIC
implementations are implemented as user space libraries leading to a large and
fragmented ecosystem. This work addresses the research question, "if a complex
standard with a large number of different implementations leads to an insecure
ecosystem?". The relevant RFC documents were studied and "Security
Consideration" items describing conceptional problems were extracted. During
the research, 13 popular production ready QUIC implementations were compared by
evaluating 10 security considerations from RFC9000. While related studies
mostly focused on the functional part of QUIC, this study confirms that
available QUIC implementations are not yet mature enough from a security point
of view.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Deep Reinforcement Learning for Privacy-Preserving Task Offloading in Integrated Satellite-Terrestrial Networks. (arXiv:2306.17183v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17183">http://arxiv.org/abs/2306.17183</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17183] Deep Reinforcement Learning for Privacy-Preserving Task Offloading in Integrated Satellite-Terrestrial Networks](http://arxiv.org/abs/2306.17183) #privacy</code></li>
<li>Summary: <p>Satellite communication networks have attracted widespread attention for
seamless network coverage and collaborative computing. In satellite-terrestrial
networks, ground users can offload computing tasks to visible satellites that
with strong computational capabilities. Existing solutions on
satellite-assisted task computing generally focused on system performance
optimization such as task completion time and energy consumption. However, due
to the high-speed mobility pattern and unreliable communication channels,
existing methods still suffer from serious privacy leakages. In this paper, we
present an integrated satellite-terrestrial network to enable
satellite-assisted task offloading under dynamic mobility nature. We also
propose a privacy-preserving task offloading scheme to bridge the gap between
offloading performance and privacy leakage. In particular, we balance two
offloading privacy, called the usage pattern privacy and the location privacy,
with different offloading targets (e.g., completion time, energy consumption,
and communication reliability). Finally, we formulate it into a joint
optimization problem, and introduce a deep reinforcement learning-based
privacy-preserving algorithm for an optimal offloading policy. Experimental
results show that our proposed algorithm outperforms other benchmark algorithms
in terms of completion time, energy consumption, privacy-preserving level, and
communication reliability. We hope this work could provide improved solutions
for privacy-persevering task offloading in satellite-assisted edge computing.
</p></li>
</ul>

<h3>Title: Vision Through the Veil: Differential Privacy in Federated Learning for Medical Image Classification. (arXiv:2306.17794v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17794">http://arxiv.org/abs/2306.17794</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17794] Vision Through the Veil: Differential Privacy in Federated Learning for Medical Image Classification](http://arxiv.org/abs/2306.17794) #privacy</code></li>
<li>Summary: <p>The proliferation of deep learning applications in healthcare calls for data
aggregation across various institutions, a practice often associated with
significant privacy concerns. This concern intensifies in medical image
analysis, where privacy-preserving mechanisms are paramount due to the data
being sensitive in nature. Federated learning, which enables cooperative model
training without direct data exchange, presents a promising solution.
Nevertheless, the inherent vulnerabilities of federated learning necessitate
further privacy safeguards. This study addresses this need by integrating
differential privacy, a leading privacy-preserving technique, into a federated
learning framework for medical image classification. We introduce a novel
differentially private federated learning model and meticulously examine its
impacts on privacy preservation and model performance. Our research confirms
the existence of a trade-off between model accuracy and privacy settings.
However, we demonstrate that strategic calibration of the privacy budget in
differential privacy can uphold robust image classification performance while
providing substantial privacy protection.
</p></li>
</ul>

<h3>Title: A Survey on Blockchain-Based Federated Learning and Data Privacy. (arXiv:2306.17338v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17338">http://arxiv.org/abs/2306.17338</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17338] A Survey on Blockchain-Based Federated Learning and Data Privacy](http://arxiv.org/abs/2306.17338) #privacy</code></li>
<li>Summary: <p>Federated learning is a decentralized machine learning paradigm that allows
multiple clients to collaborate by leveraging local computational power and the
models transmission. This method reduces the costs and privacy concerns
associated with centralized machine learning methods while ensuring data
privacy by distributing training data across heterogeneous devices. On the
other hand, federated learning has the drawback of data leakage due to the lack
of privacy-preserving mechanisms employed during storage, transfer, and
sharing, thus posing significant risks to data owners and suppliers. Blockchain
technology has emerged as a promising technology for offering secure
data-sharing platforms in federated learning, especially in Industrial Internet
of Things (IIoT) settings. This survey aims to compare the performance and
security of various data privacy mechanisms adopted in blockchain-based
federated learning architectures. We conduct a systematic review of existing
literature on secure data-sharing platforms for federated learning provided by
blockchain technology, providing an in-depth overview of blockchain-based
federated learning, its essential components, and discussing its principles,
and potential applications. The primary contribution of this survey paper is to
identify critical research questions and propose potential directions for
future research in blockchain-based federated learning.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: A Fast Fourier Convolutional Deep Neural Network For Accurate and Explainable Discrimination Of Wheat Yellow Rust And Nitrogen Deficiency From Sentinel-2 Time-Series Data. (arXiv:2306.17207v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17207">http://arxiv.org/abs/2306.17207</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17207] A Fast Fourier Convolutional Deep Neural Network For Accurate and Explainable Discrimination Of Wheat Yellow Rust And Nitrogen Deficiency From Sentinel-2 Time-Series Data](http://arxiv.org/abs/2306.17207) #protect</code></li>
<li>Summary: <p>Accurate and timely detection of plant stress is essential for yield
protection, allowing better-targeted intervention strategies. Recent advances
in remote sensing and deep learning have shown great potential for rapid
non-invasive detection of plant stress in a fully automated and reproducible
manner. However, the existing models always face several challenges: 1)
computational inefficiency and the misclassifications between the different
stresses with similar symptoms; and 2) the poor interpretability of the
host-stress interaction. In this work, we propose a novel fast Fourier
Convolutional Neural Network (FFDNN) for accurate and explainable detection of
two plant stresses with similar symptoms (i.e. Wheat Yellow Rust And Nitrogen
Deficiency). Specifically, unlike the existing CNN models, the main components
of the proposed model include: 1) a fast Fourier convolutional block, a newly
fast Fourier transformation kernel as the basic perception unit, to substitute
the traditional convolutional kernel to capture both local and global responses
to plant stress in various time-scale and improve computing efficiency with
reduced learning parameters in Fourier domain; 2) Capsule Feature Encoder to
encapsulate the extracted features into a series of vector features to
represent part-to-whole relationship with the hierarchical structure of the
host-stress interactions of the specific stress. In addition, in order to
alleviate over-fitting, a photochemical vegetation indices-based filter is
placed as pre-processing operator to remove the non-photochemical noises from
the input Sentinel-2 time series.
</p></li>
</ul>

<h3>Title: Augmenting Holistic Review in University Admission using Natural Language Processing for Essays and Recommendation Letters. (arXiv:2306.17575v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17575">http://arxiv.org/abs/2306.17575</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17575] Augmenting Holistic Review in University Admission using Natural Language Processing for Essays and Recommendation Letters](http://arxiv.org/abs/2306.17575) #protect</code></li>
<li>Summary: <p>University admission at many highly selective institutions uses a holistic
review process, where all aspects of the application, including protected
attributes (e.g., race, gender), grades, essays, and recommendation letters are
considered, to compose an excellent and diverse class. In this study, we
empirically evaluate how influential protected attributes are for predicting
admission decisions using a machine learning (ML) model, and in how far textual
information (e.g., personal essay, teacher recommendation) may substitute for
the loss of protected attributes in the model. Using data from 14,915
applicants to an undergraduate admission office at a selective U.S. institution
in the 2022-2023 cycle, we find that the exclusion of protected attributes from
the ML model leads to substantially reduced admission-prediction performance.
The inclusion of textual information via both a TF-IDF representation and a
Latent Dirichlet allocation (LDA) model partially restores model performance,
but does not appear to provide a full substitute for admitting a similarly
diverse class. In particular, while the text helps with gender diversity, the
proportion of URM applicants is severely impacted by the exclusion of protected
attributes, and the inclusion of new attributes generated from the textual
information does not recover this performance loss.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Defense against Adversarial Cloud Attack on Remote Sensing Salient Object Detection. (arXiv:2306.17431v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17431">http://arxiv.org/abs/2306.17431</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17431] Defense against Adversarial Cloud Attack on Remote Sensing Salient Object Detection](http://arxiv.org/abs/2306.17431) #defense</code></li>
<li>Summary: <p>etecting the salient objects in a remote sensing image has wide applications
for the interdisciplinary research. Many existing deep learning methods have
been proposed for Salient Object Detection (SOD) in remote sensing images and
get remarkable results. However, the recent adversarial attack examples,
generated by changing a few pixel values on the original remote sensing image,
could result in a collapse for the well-trained deep learning based SOD model.
Different with existing methods adding perturbation to original images, we
propose to jointly tune adversarial exposure and additive perturbation for
attack and constrain image close to cloudy image as Adversarial Cloud. Cloud is
natural and common in remote sensing images, however, camouflaging cloud based
adversarial attack and defense for remote sensing images are not well studied
before. Furthermore, we design DefenseNet as a learn-able pre-processing to the
adversarial cloudy images so as to preserve the performance of the deep
learning based remote sensing SOD model, without tuning the already deployed
deep SOD model. By considering both regular and generalized adversarial
examples, the proposed DefenseNet can defend the proposed Adversarial Cloud in
white-box setting and other attack methods in black-box setting. Experimental
results on a synthesized benchmark from the public remote sensing SOD dataset
(EORSSD) show the promising defense against adversarial cloud attacks.
</p></li>
</ul>

<h3>Title: Efficient Backdoor Removal Through Natural Gradient Fine-tuning. (arXiv:2306.17441v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17441">http://arxiv.org/abs/2306.17441</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17441] Efficient Backdoor Removal Through Natural Gradient Fine-tuning](http://arxiv.org/abs/2306.17441) #defense</code></li>
<li>Summary: <p>The success of a deep neural network (DNN) heavily relies on the details of
the training scheme; e.g., training data, architectures, hyper-parameters, etc.
Recent backdoor attacks suggest that an adversary can take advantage of such
training details and compromise the integrity of a DNN. Our studies show that a
backdoor model is usually optimized to a bad local minima, i.e. sharper minima
as compared to a benign model. Intuitively, a backdoor model can be purified by
reoptimizing the model to a smoother minima through fine-tuning with a few
clean validation data. However, fine-tuning all DNN parameters often requires
huge computational costs and often results in sub-par clean test performance.
To address this concern, we propose a novel backdoor purification technique,
Natural Gradient Fine-tuning (NGF), which focuses on removing the backdoor by
fine-tuning only one layer. Specifically, NGF utilizes a loss surface
geometry-aware optimizer that can successfully overcome the challenge of
reaching a smooth minima under a one-layer optimization scenario. To enhance
the generalization performance of our proposed method, we introduce a clean
data distribution-aware regularizer based on the knowledge of loss surface
curvature matrix, i.e., Fisher Information Matrix. Extensive experiments show
that the proposed method achieves state-of-the-art performance on a wide range
of backdoor defense benchmarks: four different datasets- CIFAR10, GTSRB,
Tiny-ImageNet, and ImageNet; 13 recent backdoor attacks, e.g. Blend, Dynamic,
WaNet, ISSBA, etc.
</p></li>
</ul>

<h3>Title: Research on Virus Cyberattack-Defense Based on Electromagnetic Radiation. (arXiv:2306.17508v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17508">http://arxiv.org/abs/2306.17508</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17508] Research on Virus Cyberattack-Defense Based on Electromagnetic Radiation](http://arxiv.org/abs/2306.17508) #defense</code></li>
<li>Summary: <p>Information technology and telecommunications have rapidly permeated various
domains, resulting in a significant influx of data traversing the networks
between computers. Consequently, research of cyberattacks in computer systems
has become crucial for many organizations. Accordingly, recent cybersecurity
incidents have underscored the rapidly evolving nature of future threats and
attack methods, particularly those involving computer viruses wireless
injection. This paper aims to study and demonstrate the feasibility of remote
computer virus radiation injection. To achieve this objective, digital signal
processing (DSP) plays a vital role. By studying the principles and models of
radiation attacks and computer virus propagation, the modulation of the binary
data stream of the simulated virus into a terahertz radar carrier signal by
Phase-Shift Keying (PSK) is simulated, enabling the implementation of an attack
through the "field to line" coupling of electromagnetic signals. Finally, the
defense and countermeasures based on signal recognition are discussed for such
attacks. Additionally, an idea of establishing a virus library for cyberattack
signals and employing artificial intelligence (AI) algorithms for automated
intrusion detection is proposed as a means to achieve cybersecurity situation
awareness.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: On the Exploitability of Instruction Tuning. (arXiv:2306.17194v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17194">http://arxiv.org/abs/2306.17194</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17194] On the Exploitability of Instruction Tuning](http://arxiv.org/abs/2306.17194) #attack</code></li>
<li>Summary: <p>Instruction tuning is an effective technique to align large language models
(LLMs) with human intents. In this work, we investigate how an adversary can
exploit instruction tuning by injecting specific instruction-following examples
into the training data that intentionally changes the model's behavior. For
example, an adversary can achieve content injection by injecting training
examples that mention target content and eliciting such behavior from
downstream models. To achieve this goal, we propose \textit{AutoPoison}, an
automated data poisoning pipeline. It naturally and coherently incorporates
versatile attack goals into poisoned data with the help of an oracle LLM. We
showcase two example attacks: content injection and over-refusal attacks, each
aiming to induce a specific exploitable behavior. We quantify and benchmark the
strength and the stealthiness of our data poisoning scheme. Our results show
that AutoPoison allows an adversary to change a model's behavior by poisoning
only a small fraction of data while maintaining a high level of stealthiness in
the poisoned examples. We hope our work sheds light on how data quality affects
the behavior of instruction-tuned models and raises awareness of the importance
of data quality for responsible deployments of LLMs. Code is available at
\url{https://github.com/azshue/AutoPoison}.
</p></li>
</ul>

<h3>Title: A New Task and Dataset on Detecting Attacks on Human Rights Defenders. (arXiv:2306.17695v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17695">http://arxiv.org/abs/2306.17695</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17695] A New Task and Dataset on Detecting Attacks on Human Rights Defenders](http://arxiv.org/abs/2306.17695) #attack</code></li>
<li>Summary: <p>The ability to conduct retrospective analyses of attacks on human rights
defenders over time and by location is important for humanitarian organizations
to better understand historical or ongoing human rights violations and thus
better manage the global impact of such events. We hypothesize that NLP can
support such efforts by quickly processing large collections of news articles
to detect and summarize the characteristics of attacks on human rights
defenders. To that end, we propose a new dataset for detecting Attacks on Human
Rights Defenders (HRDsAttack) consisting of crowdsourced annotations on 500
online news articles. The annotations include fine-grained information about
the type and location of the attacks, as well as information about the
victim(s). We demonstrate the usefulness of the dataset by using it to train
and evaluate baseline models on several sub-tasks to predict the annotated
characteristics.
</p></li>
</ul>

<h3>Title: Steganographic Capacity of Deep Learning Models. (arXiv:2306.17189v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17189">http://arxiv.org/abs/2306.17189</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17189] Steganographic Capacity of Deep Learning Models](http://arxiv.org/abs/2306.17189) #attack</code></li>
<li>Summary: <p>As machine learning and deep learning models become ubiquitous, it is
inevitable that there will be attempts to exploit such models in various attack
scenarios. For example, in a steganographic-based attack, information could be
hidden in a learning model, which might then be used to distribute malware, or
for other malicious purposes. In this research, we consider the steganographic
capacity of several learning models. Specifically, we train a Multilayer
Perceptron (MLP), Convolutional Neural Network (CNN), and Transformer model on
a challenging malware classification problem. For each of the resulting models,
we determine the number of low-order bits of the trained parameters that can be
altered without significantly affecting the performance of the model. We find
that the steganographic capacity of the learning models tested is surprisingly
high, and that in each case, there is a clear threshold after which model
performance rapidly degrades.
</p></li>
</ul>

<h3>Title: Classification and Explanation of Distributed Denial-of-Service (DDoS) Attack Detection using Machine Learning and Shapley Additive Explanation (SHAP) Methods. (arXiv:2306.17190v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17190">http://arxiv.org/abs/2306.17190</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17190] Classification and Explanation of Distributed Denial-of-Service (DDoS) Attack Detection using Machine Learning and Shapley Additive Explanation (SHAP) Methods](http://arxiv.org/abs/2306.17190) #attack</code></li>
<li>Summary: <p>DDoS attacks involve overwhelming a target system with a large number of
requests or traffic from multiple sources, disrupting the normal traffic of a
targeted server, service, or network. Distinguishing between legitimate traffic
and malicious traffic is a challenging task. It is possible to classify
legitimate traffic and malicious traffic and analysis the network traffic by
using machine learning and deep learning techniques. However, an inter-model
explanation implemented to classify a traffic flow whether is benign or
malicious is an important investigation of the inner working theory of the
model to increase the trustworthiness of the model. Explainable Artificial
Intelligence (XAI) can explain the decision-making of the machine learning
models that can be classified and identify DDoS traffic. In this context, we
proposed a framework that can not only classify legitimate traffic and
malicious traffic of DDoS attacks but also use SHAP to explain the
decision-making of the classifier model. To address this concern, we first
adopt feature selection techniques to select the top 20 important features
based on feature importance techniques (e.g., XGB-based SHAP feature
importance). Following that, the Multi-layer Perceptron Network (MLP) part of
our proposed model uses the optimized features of the DDoS attack dataset as
inputs to classify legitimate and malicious traffic. We perform extensive
experiments with all features and selected features. The evaluation results
show that the model performance with selected features achieves above 99\%
accuracy. Finally, to provide interpretability, XAI can be adopted to explain
the model performance between the prediction results and features based on
global and local explanations by SHAP, which can better explain the results
achieved by our proposed framework.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Robust Roadside Perception for Autonomous Driving: an Annotation-free Strategy with Synthesized Data. (arXiv:2306.17302v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17302">http://arxiv.org/abs/2306.17302</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17302] Robust Roadside Perception for Autonomous Driving: an Annotation-free Strategy with Synthesized Data](http://arxiv.org/abs/2306.17302) #robust</code></li>
<li>Summary: <p>Recently, with the rapid development in vehicle-to-infrastructure
communication technologies, the infrastructure-based, roadside perception
system for cooperative driving has become a rising field. This paper focuses on
one of the most critical challenges - the data-insufficiency problem. The
lacking of high-quality labeled roadside sensor data with high diversity leads
to low robustness, and low transfer-ability of current roadside perception
systems. In this paper, a novel approach is proposed to address this problem by
creating synthesized training data using Augmented Reality and Generative
Adversarial Network. This method creates synthesized dataset that is capable of
training or fine-tuning a roadside perception detector which is robust to
different weather and lighting conditions, or to adapt a new deployment
location. We validate our approach at two intersections: Mcity intersection and
State St/Ellsworth Rd roundabout. Our experiments show that (1) the detector
can achieve good performance in all conditions when trained on synthesized data
only, and (2) the performance of an existing detector trained with labeled data
can be enhanced by synthesized data in harsh conditions.
</p></li>
</ul>

<h3>Title: CausalVLR: A Toolbox and Benchmark for Visual-Linguistic Causal Reasoning. (arXiv:2306.17462v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17462">http://arxiv.org/abs/2306.17462</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17462] CausalVLR: A Toolbox and Benchmark for Visual-Linguistic Causal Reasoning](http://arxiv.org/abs/2306.17462) #robust</code></li>
<li>Summary: <p>We present CausalVLR (Causal Visual-Linguistic Reasoning), an open-source
toolbox containing a rich set of state-of-the-art causal relation discovery and
causal inference methods for various visual-linguistic reasoning tasks, such as
VQA, image/video captioning, medical report generation, model generalization
and robustness, etc. These methods have been included in the toolbox with
PyTorch implementations under NVIDIA computing system. It not only includes
training and inference codes, but also provides model weights. We believe this
toolbox is by far the most complete visual-linguitic causal reasoning toolbox.
We wish that the toolbox and benchmark could serve the growing research
community by providing a flexible toolkit to re-implement existing methods and
develop their own new causal reasoning methods. Code and models are available
at https://github.com/HCPLab-SYSU/Causal-VLReasoning. The project is under
active development by HCP-Lab's contributors and we will keep this document
updated.
</p></li>
</ul>

<h3>Title: Towards the extraction of robust sign embeddings for low resource sign language recognition. (arXiv:2306.17558v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17558">http://arxiv.org/abs/2306.17558</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17558] Towards the extraction of robust sign embeddings for low resource sign language recognition](http://arxiv.org/abs/2306.17558) #robust</code></li>
<li>Summary: <p>Isolated Sign Language Recognition (SLR) has mostly been applied on
relatively large datasets containing signs executed slowly and clearly by a
limited group of signers. In real-world scenarios, however, we are met with
challenging visual conditions, coarticulated signing, small datasets, and the
need for signer independent models. To tackle this difficult problem, we
require a robust feature extractor to process the sign language videos. One
could expect human pose estimators to be ideal candidates. However, due to a
domain mismatch with their training sets and challenging poses in sign
language, they lack robustness on sign language data and image based models
often still outperform keypoint based models. Furthermore, whereas the common
practice of transfer learning with image based models yields even higher
accuracy, keypoint based models are typically trained from scratch on every SLR
dataset. These factors limit their usefulness for SLR. From the existing
literature, it is also not clear which, if any, pose estimator performs best
for SLR. We compare the three most popular pose estimators for SLR: OpenPose,
MMPose and MediaPipe. We show that through keypoint normalization, missing
keypoint imputation, and learning a pose embedding, we can obtain significantly
better results and enable transfer learning. We show that keypoint-based
embeddings contain cross-lingual features: they can transfer between sign
languages and achieve competitive performance even when fine-tuning only the
classifier layer of an SLR model on a target sign language. We furthermore
achieve better performance using fine-tuned transferred embeddings than models
trained only on the target sign language. The application of these embeddings
could prove particularly useful for low resource sign languages in the future.
</p></li>
</ul>

<h3>Title: Polarimetric iToF: Measuring High-Fidelity Depth through Scattering Media. (arXiv:2306.17618v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17618">http://arxiv.org/abs/2306.17618</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17618] Polarimetric iToF: Measuring High-Fidelity Depth through Scattering Media](http://arxiv.org/abs/2306.17618) #robust</code></li>
<li>Summary: <p>Indirect time-of-flight (iToF) imaging allows us to capture dense depth
information at a low cost. However, iToF imaging often suffers from multipath
interference (MPI) artifacts in the presence of scattering media, resulting in
severe depth-accuracy degradation. For instance, iToF cameras cannot measure
depth accurately through fog because ToF active illumination scatters back to
the sensor before reaching the farther target surface. In this work, we propose
a polarimetric iToF imaging method that can capture depth information robustly
through scattering media. Our observations on the principle of indirect ToF
imaging and polarization of light allow us to formulate a novel computational
model of scattering-aware polarimetric phase measurements that enables us to
correct MPI errors. We first devise a scattering-aware polarimetric iToF model
that can estimate the phase of unpolarized backscattered light. We then combine
the optical filtering of polarization and our computational modeling of
unpolarized backscattered light via scattering analysis of phase and amplitude.
This allows us to tackle the MPI problem by estimating the scattering energy
through the participating media. We validate our method on an experimental
setup using a customized off-the-shelf iToF camera. Our method outperforms
baseline methods by a significant margin by means of our scattering model and
polarimetric phase measurements.
</p></li>
</ul>

<h3>Title: Masked Contrastive Graph Representation Learning for Age Estimation. (arXiv:2306.17798v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17798">http://arxiv.org/abs/2306.17798</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17798] Masked Contrastive Graph Representation Learning for Age Estimation](http://arxiv.org/abs/2306.17798) #robust</code></li>
<li>Summary: <p>Age estimation of face images is a crucial task with various practical
applications in areas such as video surveillance and Internet access control.
While deep learning-based age estimation frameworks, e.g., convolutional neural
network (CNN), multi-layer perceptrons (MLP), and transformers have shown
remarkable performance, they have limitations when modelling complex or
irregular objects in an image that contains a large amount of redundant
information. To address this issue, this paper utilizes the robustness property
of graph representation learning in dealing with image redundancy information
and proposes a novel Masked Contrastive Graph Representation Learning (MCGRL)
method for age estimation. Specifically, our approach first leverages CNN to
extract semantic features of the image, which are then partitioned into patches
that serve as nodes in the graph. Then, we use a masked graph convolutional
network (GCN) to derive image-based node representations that capture rich
structural information. Finally, we incorporate multiple losses to explore the
complementary relationship between structural information and semantic
features, which improves the feature representation capability of GCN.
Experimental results on real-world face image datasets demonstrate the
superiority of our proposed method over other state-of-the-art age estimation
approaches.
</p></li>
</ul>

<h3>Title: Provable Robust Watermarking for AI-Generated Text. (arXiv:2306.17439v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17439">http://arxiv.org/abs/2306.17439</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17439] Provable Robust Watermarking for AI-Generated Text](http://arxiv.org/abs/2306.17439) #robust</code></li>
<li>Summary: <p>As AI-generated text increasingly resembles human-written content, the
ability to detect machine-generated text becomes crucial. To address this
challenge, we present GPTWatermark, a robust and high-quality solution designed
to ascertain whether a piece of text originates from a specific model. Our
approach extends existing watermarking strategies and employs a fixed group
design to enhance robustness against editing and paraphrasing attacks. We show
that our watermarked language model enjoys strong provable guarantees on
generation quality, correctness in detection, and security against evasion
attacks. Experimental results on various large language models (LLMs) and
diverse datasets demonstrate that our method achieves superior detection
accuracy and comparable generation quality in perplexity, thus promoting the
responsible use of LLMs.
</p></li>
</ul>

<h3>Title: Biomedical Language Models are Robust to Sub-optimal Tokenization. (arXiv:2306.17649v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17649">http://arxiv.org/abs/2306.17649</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17649] Biomedical Language Models are Robust to Sub-optimal Tokenization](http://arxiv.org/abs/2306.17649) #robust</code></li>
<li>Summary: <p>As opposed to general English, many concepts in biomedical terminology have
been designed in recent history by biomedical professionals with the goal of
being precise and concise. This is often achieved by concatenating meaningful
biomedical morphemes to create new semantic units. Nevertheless, most modern
biomedical language models (LMs) are pre-trained using standard domain-specific
tokenizers derived from large scale biomedical corpus statistics without
explicitly leveraging the agglutinating nature of biomedical language. In this
work, we first find that standard open-domain and biomedical tokenizers are
largely unable to segment biomedical terms into meaningful components.
Therefore, we hypothesize that using a tokenizer which segments biomedical
terminology more accurately would enable biomedical LMs to improve their
performance on downstream biomedical NLP tasks, especially ones which involve
biomedical terms directly such as named entity recognition (NER) and entity
linking. Surprisingly, we find that pre-training a biomedical LM using a more
accurate biomedical tokenizer does not improve the entity representation
quality of a language model as measured by several intrinsic and extrinsic
measures such as masked language modeling prediction (MLM) accuracy as well as
NER and entity linking performance. These quantitative findings, along with a
case study which explores entity representation quality more directly, suggest
that the biomedical pre-training process is quite robust to instances of
sub-optimal tokenization.
</p></li>
</ul>

<h3>Title: Fast and Robust State Estimation and Tracking via Hierarchical Learning. (arXiv:2306.17267v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17267">http://arxiv.org/abs/2306.17267</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17267] Fast and Robust State Estimation and Tracking via Hierarchical Learning](http://arxiv.org/abs/2306.17267) #robust</code></li>
<li>Summary: <p>Fully distributed estimation and tracking solutions to large-scale
multi-agent networks suffer slow convergence and are vulnerable to network
failures. In this paper, we aim to speed up the convergence and enhance the
resilience of state estimation and tracking using a simple hierarchical system
architecture wherein agents are clusters into smaller networks, and a parameter
server exists to aid the information exchanges among networks. The information
exchange among networks is expensive and occurs only once in a while.
</p></li>
</ul>

<p>We propose two consensus + innovation algorithms for the state estimation and
tracking problems, respectively. In both algorithms, we use a novel
hierarchical push-sum consensus component. For the state estimation, we use
dual averaging as the local innovation component. State tracking is much harder
to tackle in the presence of dropping-link failures and the standard
integration of the consensus and innovation approaches are no longer
applicable. Moreover, dual averaging is no longer feasible. Our algorithm
introduces a pair of additional variables per link and ensure the relevant
local variables evolve according to the state dynamics, and use projected local
gradient descent as the local innovation component. We also characterize the
convergence rates of both of the algorithms under linear local observation
model and minimal technical assumptions. We numerically validate our algorithm
through simulation of both state estimation and tracking problems.
</p>

<h3>Title: Designing Stable Neural Networks using Convex Analysis and ODEs. (arXiv:2306.17332v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17332">http://arxiv.org/abs/2306.17332</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17332] Designing Stable Neural Networks using Convex Analysis and ODEs](http://arxiv.org/abs/2306.17332) #robust</code></li>
<li>Summary: <p>Motivated by classical work on the numerical integration of ordinary
differential equations we present a ResNet-styled neural network architecture
that encodes non-expansive (1-Lipschitz) operators, as long as the spectral
norms of the weights are appropriately constrained. This is to be contrasted
with the ordinary ResNet architecture which, even if the spectral norms of the
weights are constrained, has a Lipschitz constant that, in the worst case,
grows exponentially with the depth of the network. Further analysis of the
proposed architecture shows that the spectral norms of the weights can be
further constrained to ensure that the network is an averaged operator, making
it a natural candidate for a learned denoiser in Plug-and-Play algorithms.
Using a novel adaptive way of enforcing the spectral norm constraints, we show
that, even with these constraints, it is possible to train performant networks.
The proposed architecture is applied to the problem of adversarially robust
image classification, to image denoising, and finally to the inverse problem of
deblurring.
</p></li>
</ul>

<h3>Title: Impact of Noise on Calibration and Generalisation of Neural Networks. (arXiv:2306.17630v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17630">http://arxiv.org/abs/2306.17630</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17630] Impact of Noise on Calibration and Generalisation of Neural Networks](http://arxiv.org/abs/2306.17630) #robust</code></li>
<li>Summary: <p>Noise injection and data augmentation strategies have been effective for
enhancing the generalisation and robustness of neural networks (NNs). Certain
types of noise such as label smoothing and MixUp have also been shown to
improve calibration. Since noise can be added in various stages of the NN's
training, it motivates the question of when and where the noise is the most
effective. We study a variety of noise types to determine how much they improve
calibration and generalisation, and under what conditions. More specifically we
evaluate various noise-injection strategies in both in-distribution (ID) and
out-of-distribution (OOD) scenarios. The findings highlight that activation
noise was the most transferable and effective in improving generalisation,
while input augmentation noise was prominent in improving calibration on OOD
but not necessarily ID data.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: FarSight: A Physics-Driven Whole-Body Biometric System at Large Distance and Altitude. (arXiv:2306.17206v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17206">http://arxiv.org/abs/2306.17206</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17206] FarSight: A Physics-Driven Whole-Body Biometric System at Large Distance and Altitude](http://arxiv.org/abs/2306.17206) #biometric</code></li>
<li>Summary: <p>Whole-body biometric recognition is an important area of research due to its
vast applications in law enforcement, border security, and surveillance. This
paper presents the end-to-end design, development and evaluation of FarSight,
an innovative software system designed for whole-body (fusion of face, gait and
body shape) biometric recognition. FarSight accepts videos from elevated
platforms and drones as input and outputs a candidate list of identities from a
gallery. The system is designed to address several challenges, including (i)
low-quality imagery, (ii) large yaw and pitch angles, (iii) robust feature
extraction to accommodate large intra-person variabilities and large
inter-person similarities, and (iv) the large domain gap between training and
test sets. FarSight combines the physics of imaging and deep learning models to
enhance image restoration and biometric feature encoding. We test FarSight's
effectiveness using the newly acquired IARPA Biometric Recognition and
Identification at Altitude and Range (BRIAR) dataset. Notably, FarSight
demonstrated a substantial performance increase on the BRIAR dataset, with
gains of +11.82% Rank-20 identification and +11.3% TAR@1% FAR.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: FANET Experiment: Real-Time Surveillance Applications Connected to Image Processing System. (arXiv:2306.17172v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17172">http://arxiv.org/abs/2306.17172</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17172] FANET Experiment: Real-Time Surveillance Applications Connected to Image Processing System](http://arxiv.org/abs/2306.17172) #extraction</code></li>
<li>Summary: <p>The major goal of this paper is to use image enhancement techniques for
enhancing and extracting data in FANET applications to improve the efficiency
of surveillance. The proposed conceptual system design can improve the
likelihood of FANET operations in oil pipeline surveillance, and sports and
media coverage with the ultimate goal of providing efficient services to those
who are interested. The system architecture model is based on current
scientific principles and developing technologies. A FANET, which is capable of
gathering image data from video-enabled drones, and an image processing system
that permits data collection and analysis are the two primary components of the
system. Based on the image processing technique, a proof of concept for
efficient data extraction and enhancement in FANET situations and possible
services is illustrated.
</p></li>
</ul>

<h3>Title: Training-Free Neural Matte Extraction for Visual Effects. (arXiv:2306.17321v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17321">http://arxiv.org/abs/2306.17321</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17321] Training-Free Neural Matte Extraction for Visual Effects](http://arxiv.org/abs/2306.17321) #extraction</code></li>
<li>Summary: <p>Alpha matting is widely used in video conferencing as well as in movies,
television, and social media sites. Deep learning approaches to the matte
extraction problem are well suited to video conferencing due to the consistent
subject matter (front-facing humans), however training-based approaches are
somewhat pointless for entertainment videos where varied subjects (spaceships,
monsters, etc.) may appear only a few times in a single movie -- if a method of
creating ground truth for training exists, just use that method to produce the
desired mattes. We introduce a training-free high quality neural matte
extraction approach that specifically targets the assumptions of visual effects
production. Our approach is based on the deep image prior, which optimizes a
deep neural network to fit a single image, thereby providing a deep encoding of
the particular image. We make use of the representations in the penultimate
layer to interpolate coarse and incomplete "trimap" constraints. Videos
processed with this approach are temporally consistent. The algorithm is both
very simple and surprisingly effective.
</p></li>
</ul>

<h3>Title: GPT-FinRE: In-context Learning for Financial Relation Extraction using Large Language Models. (arXiv:2306.17519v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17519">http://arxiv.org/abs/2306.17519</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17519] GPT-FinRE: In-context Learning for Financial Relation Extraction using Large Language Models](http://arxiv.org/abs/2306.17519) #extraction</code></li>
<li>Summary: <p>Relation extraction (RE) is a crucial task in natural language processing
(NLP) that aims to identify and classify relationships between entities
mentioned in text. In the financial domain, relation extraction plays a vital
role in extracting valuable information from financial documents, such as news
articles, earnings reports, and company filings. This paper describes our
solution to relation extraction on one such dataset REFinD. The dataset was
released along with shared task as a part of the Fourth Workshop on Knowledge
Discovery from Unstructured Data in Financial Services, co-located with SIGIR</li>
<li>In this paper, we employed OpenAI models under the framework of
in-context learning (ICL). We utilized two retrieval strategies to find top K
relevant in-context learning demonstrations / examples from training data for a
given test example. The first retrieval mechanism, we employed, is a
learning-free dense retriever and the other system is a learning-based
retriever. We were able to achieve 4th rank on the leaderboard. Our best
F1-score is 0.718.
</p></li>
</ul>

<h3>Title: Token-Event-Role Structure-based Multi-Channel Document-Level Event Extraction. (arXiv:2306.17733v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17733">http://arxiv.org/abs/2306.17733</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17733] Token-Event-Role Structure-based Multi-Channel Document-Level Event Extraction](http://arxiv.org/abs/2306.17733) #extraction</code></li>
<li>Summary: <p>Document-level event extraction is a long-standing challenging information
retrieval problem involving a sequence of sub-tasks: entity extraction, event
type judgment, and event type-specific multi-event extraction. However,
addressing the problem as multiple learning tasks leads to increased model
complexity. Also, existing methods insufficiently utilize the correlation of
entities crossing different events, resulting in limited event extraction
performance. This paper introduces a novel framework for document-level event
extraction, incorporating a new data structure called token-event-role and a
multi-channel argument role prediction module. The proposed data structure
enables our model to uncover the primary role of tokens in multiple events,
facilitating a more comprehensive understanding of event relationships. By
leveraging the multi-channel prediction module, we transform entity and
multi-event extraction into a single task of predicting token-event pairs,
thereby reducing the overall parameter size and enhancing model efficiency. The
results demonstrate that our approach outperforms the state-of-the-art method
by 9.5 percentage points in terms of the F1 score, highlighting its superior
performance in event extraction. Furthermore, an ablation study confirms the
significant value of the proposed data structure in improving event extraction
tasks, further validating its importance in enhancing the overall performance
of the framework.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Object Detection for Quality Inspection in Shared Production. (arXiv:2306.17645v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17645">http://arxiv.org/abs/2306.17645</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17645] Federated Object Detection for Quality Inspection in Shared Production](http://arxiv.org/abs/2306.17645) #federate</code></li>
<li>Summary: <p>Federated learning (FL) has emerged as a promising approach for training
machine learning models on decentralized data without compromising data
privacy. In this paper, we propose a FL algorithm for object detection in
quality inspection tasks using YOLOv5 as the object detection algorithm and
Federated Averaging (FedAvg) as the FL algorithm. We apply this approach to a
manufacturing use-case where multiple factories/clients contribute data for
training a global object detection model while preserving data privacy on a
non-IID dataset. Our experiments demonstrate that our FL approach achieves
better generalization performance on the overall clients' test dataset and
generates improved bounding boxes around the objects compared to models trained
using local clients' datasets. This work showcases the potential of FL for
quality inspection tasks in the manufacturing industry and provides valuable
insights into the performance and feasibility of utilizing YOLOv5 and FedAvg
for federated object detection.
</p></li>
</ul>

<h3>Title: Federated Ensemble YOLOv5 - A Better Generalized Object Detection Algorithm. (arXiv:2306.17829v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17829">http://arxiv.org/abs/2306.17829</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17829] Federated Ensemble YOLOv5 - A Better Generalized Object Detection Algorithm](http://arxiv.org/abs/2306.17829) #federate</code></li>
<li>Summary: <p>Federated learning (FL) has gained significant traction as a
privacy-preserving algorithm, but the underlying resembles of federated
learning algorithm like Federated averaging (FED Avg) or Federated SGD (FED
SGD) to ensemble learning algorithms has not been fully explored. The purpose
of this paper is to examine the application of FL to object detection as a
method to enhance generalizability, and to compare its performance against a
centralized training approach for an object detection algorithm. Specifically,
we investigate the performance of a YOLOv5 model trained using FL across
multiple clients and employ a random sampling strategy without replacement, so
each client holds a portion of the same dataset used for centralized training.
Our experimental results showcase the superior efficiency of the FL object
detector's global model in generating accurate bounding boxes for unseen
objects, with the test set being a mixture of objects from two distinct clients
not represented in the training dataset. These findings suggest that FL can be
viewed from an ensemble algorithm perspective, akin to a synergistic blend of
Bagging and Boosting techniques. As a result, FL can be seen not only as a
method to enhance privacy, but also as a method to enhance the performance of a
machine learning model.
</p></li>
</ul>

<h3>Title: Improving Federated Aggregation with Deep Unfolding Networks. (arXiv:2306.17362v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17362">http://arxiv.org/abs/2306.17362</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17362] Improving Federated Aggregation with Deep Unfolding Networks](http://arxiv.org/abs/2306.17362) #federate</code></li>
<li>Summary: <p>The performance of Federated learning (FL) is negatively affected by device
differences and statistical characteristics between participating clients. To
address this issue, we introduce a deep unfolding network (DUN)-based technique
that learns adaptive weights that unbiasedly ameliorate the adverse impacts of
heterogeneity. The proposed method demonstrates impressive accuracy and
quality-aware aggregation. Furthermore, it evaluated the best-weighted
normalization approach to define less computational power on the aggregation
method. The numerical experiments in this study demonstrate the effectiveness
of this approach and provide insights into the interpretability of the unbiased
weights learned.
</p></li>
</ul>

<p>By incorporating unbiased weights into the model, the proposed approach
effectively addresses quality-aware aggregation under the heterogeneity of the
participating clients and the FL environment. Codes and details are
\href{https://github.com/shanikairoshi/Improved_DUN_basedFL_Aggregation}{here}.
</p>

<h3>Title: FedBone: Towards Large-Scale Federated Multi-Task Learning. (arXiv:2306.17465v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17465">http://arxiv.org/abs/2306.17465</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17465] FedBone: Towards Large-Scale Federated Multi-Task Learning](http://arxiv.org/abs/2306.17465) #federate</code></li>
<li>Summary: <p>Heterogeneous federated multi-task learning (HFMTL) is a federated learning
technique that combines heterogeneous tasks of different clients to achieve
more accurate, comprehensive predictions. In real-world applications, visual
and natural language tasks typically require large-scale models to extract
high-level abstract features. However, large-scale models cannot be directly
applied to existing federated multi-task learning methods. Existing HFML
methods also disregard the impact of gradient conflicts on multi-task
optimization during the federated aggregation process. In this work, we propose
an innovative framework called FedBone, which enables the construction of
large-scale models with better generalization from the perspective of
server-client split learning and gradient projection. We split the entire model
into two components: a large-scale general model (referred to as the general
model) on the cloud server and multiple task-specific models (referred to as
the client model) on edge clients, solving the problem of insufficient
computing power on edge clients. The conflicting gradient projection technique
is used to enhance the generalization of the large-scale general model between
different tasks. The proposed framework is evaluated on two benchmark datasets
and a real ophthalmic dataset. Comprehensive results demonstrate that FedBone
efficiently adapts to heterogeneous local tasks of each client and outperforms
existing federated learning algorithms in most dense prediction and
classification tasks with off-the-shelf computational resources on the client
side.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Understanding Unfairness via Training Concept Influence. (arXiv:2306.17828v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17828">http://arxiv.org/abs/2306.17828</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17828] Understanding Unfairness via Training Concept Influence](http://arxiv.org/abs/2306.17828) #fair</code></li>
<li>Summary: <p>Knowing the causes of a model's unfairness helps practitioners better
understand their data and algorithms. This is an important yet relatively
unexplored task. We look into this problem through the lens of the training
data - one of the major sources of unfairness. We ask the following questions:
how would a model's fairness performance change if, in its training data, some
samples (1) were collected from a different (e.g. demographic) group, (2) were
labeled differently, or (3) some features were changed? In other words, we
quantify the fairness influence of training samples by counterfactually
intervening and changing samples based on predefined concepts, i.e. data
attributes such as features (X), labels (Y), or sensitive attributes (A). To
calculate a training sample's influence on the model's unfairness w.r.t a
concept, we first generate counterfactual samples based on the concept, i.e.
the counterfactual versions of the sample if the concept were changed. We then
calculate the resulting impact on the unfairness, via influence function, if
the counterfactual samples were used in training. Our framework not only helps
practitioners understand the observed unfairness and repair their training
data, but also leads to many other applications, e.g. detecting mislabeling,
fixing imbalanced representations, and detecting fairness-targeted poisoning
attacks.
</p></li>
</ul>

<h2>interpretability</h2>
<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Class-Incremental Learning using Diffusion Model for Distillation and Replay. (arXiv:2306.17560v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17560">http://arxiv.org/abs/2306.17560</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17560] Class-Incremental Learning using Diffusion Model for Distillation and Replay](http://arxiv.org/abs/2306.17560) #diffusion</code></li>
<li>Summary: <p>Class-incremental learning aims to learn new classes in an incremental
fashion without forgetting the previously learned ones. Several research works
have shown how additional data can be used by incremental models to help
mitigate catastrophic forgetting. In this work, following the recent
breakthrough in text-to-image generative models and their wide distribution, we
propose the use of a pretrained Stable Diffusion model as a source of
additional data for class-incremental learning. Compared to competitive methods
that rely on external, often unlabeled, datasets of real images, our approach
can generate synthetic samples belonging to the same classes as the previously
encountered images. This allows us to use those additional data samples not
only in the distillation loss but also for replay in the classification loss.
Experiments on the competitive benchmarks CIFAR100, ImageNet-Subset, and
ImageNet demonstrate how this new approach can be used to further improve the
performance of state-of-the-art methods for class-incremental learning on large
scale datasets.
</p></li>
</ul>

<h3>Title: Counting Guidance for High Fidelity Text-to-Image Synthesis. (arXiv:2306.17567v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17567">http://arxiv.org/abs/2306.17567</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17567] Counting Guidance for High Fidelity Text-to-Image Synthesis](http://arxiv.org/abs/2306.17567) #diffusion</code></li>
<li>Summary: <p>Recently, the quality and performance of text-to-image generation
significantly advanced due to the impressive results of diffusion models.
However, text-to-image diffusion models still fail to generate high fidelity
content with respect to the input prompt. One problem where text-to-diffusion
models struggle is generating the exact number of objects specified in the text
prompt. E.g. given a prompt "five apples and ten lemons on a table",
diffusion-generated images usually contain the wrong number of objects. In this
paper, we propose a method to improve diffusion models to focus on producing
the correct object count given the input prompt. We adopt a counting network
that performs reference-less class-agnostic counting for any given image. We
calculate the gradients of the counting network and refine the predicted noise
for each step. To handle multiple types of objects in the prompt, we use novel
attention map guidance to obtain high-fidelity masks for each object. Finally,
we guide the denoising process by the calculated gradients for each object.
Through extensive experiments and evaluation, we demonstrate that our proposed
guidance method greatly improves the fidelity of diffusion models to object
count.
</p></li>
</ul>

<h3>Title: Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors. (arXiv:2306.17843v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17843">http://arxiv.org/abs/2306.17843</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17843] Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors](http://arxiv.org/abs/2306.17843) #diffusion</code></li>
<li>Summary: <p>We present Magic123, a two-stage coarse-to-fine approach for high-quality,
textured 3D meshes generation from a single unposed image in the wild using
both2D and 3D priors. In the first stage, we optimize a neural radiance field
to produce a coarse geometry. In the second stage, we adopt a memory-efficient
differentiable mesh representation to yield a high-resolution mesh with a
visually appealing texture. In both stages, the 3D content is learned through
reference view supervision and novel views guided by a combination of 2D and 3D
diffusion priors. We introduce a single trade-off parameter between the 2D and
3D priors to control exploration (more imaginative) and exploitation (more
precise) of the generated geometry. Additionally, we employ textual inversion
and monocular depth regularization to encourage consistent appearances across
views and to prevent degenerate solutions, respectively. Magic123 demonstrates
a significant improvement over previous image-to-3D techniques, as validated
through extensive experiments on synthetic benchmarks and diverse real-world
images. Our code, models, and generated 3D assets are available at
https://github.com/guochengqian/Magic123.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h3>Title: Designing strong baselines for ternary neural network quantization through support and mass equalization. (arXiv:2306.17442v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17442">http://arxiv.org/abs/2306.17442</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17442] Designing strong baselines for ternary neural network quantization through support and mass equalization](http://arxiv.org/abs/2306.17442) #data-free</code></li>
<li>Summary: <p>Deep neural networks (DNNs) offer the highest performance in a wide range of
applications in computer vision. These results rely on over-parameterized
backbones, which are expensive to run. This computational burden can be
dramatically reduced by quantizing (in either data-free (DFQ), post-training
(PTQ) or quantization-aware training (QAT) scenarios) floating point values to
ternary values (2 bits, with each weight taking value in {-1,0,1}). In this
context, we observe that rounding to nearest minimizes the expected error given
a uniform distribution and thus does not account for the skewness and kurtosis
of the weight distribution, which strongly affects ternary quantization
performance. This raises the following question: shall one minimize the highest
or average quantization error? To answer this, we design two operators: TQuant
and MQuant that correspond to these respective minimization tasks. We show
experimentally that our approach allows to significantly improve the
performance of ternary quantization through a variety of scenarios in DFQ, PTQ
and QAT and give strong insights to pave the way for future research in deep
neural network quantization.
</p></li>
</ul>

<h2>transformer</h2>
<h3>Title: Replace and Report: NLP Assisted Radiology Report Generation. (arXiv:2306.17180v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17180">http://arxiv.org/abs/2306.17180</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17180] Replace and Report: NLP Assisted Radiology Report Generation](http://arxiv.org/abs/2306.17180) #transformer</code></li>
<li>Summary: <p>Clinical practice frequently uses medical imaging for diagnosis and
treatment. A significant challenge for automatic radiology report generation is
that the radiology reports are long narratives consisting of multiple sentences
for both abnormal and normal findings. Therefore, applying conventional image
captioning approaches to generate the whole report proves to be insufficient,
as these are designed to briefly describe images with short sentences. We
propose a template-based approach to generate radiology reports from
radiographs. Our approach involves the following: i) using a multilabel image
classifier, produce the tags for the input radiograph; ii) using a
transformer-based model, generate pathological descriptions (a description of
abnormal findings seen on radiographs) from the tags generated in step (i);
iii) using a BERT-based multi-label text classifier, find the spans in the
normal report template to replace with the generated pathological descriptions;
and iv) using a rule-based system, replace the identified span with the
generated pathological description. We performed experiments with the two most
popular radiology report datasets, IU Chest X-ray and MIMIC-CXR and
demonstrated that the BLEU-1, ROUGE-L, METEOR, and CIDEr scores are better than
the State-of-the-Art models by 25%, 36%, 44% and 48% respectively, on the IU
X-RAY dataset. To the best of our knowledge, this is the first attempt to
generate chest X-ray radiology reports by first creating small sentences for
abnormal findings and then replacing them in the normal report template.
</p></li>
</ul>

<h3>Title: MPM: A Unified 2D-3D Human Pose Representation via Masked Pose Modeling. (arXiv:2306.17201v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17201">http://arxiv.org/abs/2306.17201</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17201] MPM: A Unified 2D-3D Human Pose Representation via Masked Pose Modeling](http://arxiv.org/abs/2306.17201) #transformer</code></li>
<li>Summary: <p>Estimating 3D human poses only from a 2D human pose sequence is thoroughly
explored in recent years. Yet, prior to this, no such work has attempted to
unify 2D and 3D pose representations in the shared feature space. In this
paper, we propose MPM, a unified 2D-3D human pose representation framework via
masked pose modeling. We treat 2D and 3D poses as two different modalities like
vision and language and build a single-stream transformer-based architecture.
We apply three pretext tasks, which are masked 2D pose modeling, masked 3D pose
modeling, and masked 2D pose lifting to pre-train our network and use
full-supervision to perform further fine-tuning. A high masking ratio of 72.5%
in total with a spatio-temporal mask sampling strategy leading to better
relation modeling both in spatial and temporal domains. MPM can handle multiple
tasks including 3D human pose estimation, 3D pose estimation from occluded 2D
pose, and 3D pose completion in a single framework. We conduct extensive
experiments and ablation studies on several widely used human pose datasets and
achieve state-of-the-art performance on Human3.6M and MPI-INF-3DHP. Codes and
model checkpoints are available at https://github.com/vvirgooo2/MPM
</p></li>
</ul>

<h3>Title: HVTSurv: Hierarchical Vision Transformer for Patient-Level Survival Prediction from Whole Slide Image. (arXiv:2306.17373v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17373">http://arxiv.org/abs/2306.17373</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17373] HVTSurv: Hierarchical Vision Transformer for Patient-Level Survival Prediction from Whole Slide Image](http://arxiv.org/abs/2306.17373) #transformer</code></li>
<li>Summary: <p>Survival prediction based on whole slide images (WSIs) is a challenging task
for patient-level multiple instance learning (MIL). Due to the vast amount of
data for a patient (one or multiple gigapixels WSIs) and the irregularly shaped
property of WSI, it is difficult to fully explore spatial, contextual, and
hierarchical interaction in the patient-level bag. Many studies adopt random
sampling pre-processing strategy and WSI-level aggregation models, which
inevitably lose critical prognostic information in the patient-level bag. In
this work, we propose a hierarchical vision Transformer framework named
HVTSurv, which can encode the local-level relative spatial information,
strengthen WSI-level context-aware communication, and establish patient-level
hierarchical interaction. Firstly, we design a feature pre-processing strategy,
including feature rearrangement and random window masking. Then, we devise
three layers to progressively obtain patient-level representation, including a
local-level interaction layer adopting Manhattan distance, a WSI-level
interaction layer employing spatial shuffle, and a patient-level interaction
layer using attention pooling. Moreover, the design of hierarchical network
helps the model become more computationally efficient. Finally, we validate
HVTSurv with 3,104 patients and 3,752 WSIs across 6 cancer types from The
Cancer Genome Atlas (TCGA). The average C-Index is 2.50-11.30% higher than all
the prior weakly supervised methods over 6 TCGA datasets. Ablation study and
attention visualization further verify the superiority of the proposed HVTSurv.
Implementation is available at: https://github.com/szc19990412/HVTSurv.
</p></li>
</ul>

<h3>Title: SpATr: MoCap 3D Human Action Recognition based on Spiral Auto-encoder and Transformer Network. (arXiv:2306.17574v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17574">http://arxiv.org/abs/2306.17574</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17574] SpATr: MoCap 3D Human Action Recognition based on Spiral Auto-encoder and Transformer Network](http://arxiv.org/abs/2306.17574) #transformer</code></li>
<li>Summary: <p>Recent advancements in technology have expanded the possibilities of human
action recognition by leveraging 3D data, which offers a richer representation
of actions through the inclusion of depth information, enabling more accurate
analysis of spatial and temporal characteristics. However, 3D human action
recognition is a challenging task due to the irregularity and Disarrangement of
the data points in action sequences. In this context, we present our novel
model for human action recognition from fixed topology mesh sequences based on
Spiral Auto-encoder and Transformer Network, namely SpATr. The proposed method
first disentangles space and time in the mesh sequences. Then, an auto-encoder
is utilized to extract spatial geometrical features, and tiny transformer is
used to capture the temporal evolution of the sequence. Previous methods either
use 2D depth images, sample skeletons points or they require a huge amount of
memory leading to the ability to process short sequences only. In this work, we
show competitive recognition rate and high memory efficiency by building our
auto-encoder based on spiral convolutions, which are light weight convolution
directly applied to mesh data with fixed topologies, and by modeling temporal
evolution using a attention, that can handle large sequences. The proposed
method is evaluated on on two 3D human action datasets: MoVi and BMLrub from
the Archive of Motion Capture As Surface Shapes (AMASS). The results analysis
shows the effectiveness of our method in 3D human action recognition while
maintaining high memory efficiency. The code will soon be made publicly
available.
</p></li>
</ul>

<h3>Title: S.T.A.R.-Track: Latent Motion Models for End-to-End 3D Object Tracking with Adaptive Spatio-Temporal Appearance Representations. (arXiv:2306.17602v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17602">http://arxiv.org/abs/2306.17602</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17602] S](http://arxiv.org/abs/2306.17602) #transformer</code></li>
<li>Summary: <p>Following the tracking-by-attention paradigm, this paper introduces an
object-centric, transformer-based framework for tracking in 3D. Traditional
model-based tracking approaches incorporate the geometric effect of object- and
ego motion between frames with a geometric motion model. Inspired by this, we
propose S.T.A.R.-Track, which uses a novel latent motion model (LMM) to
additionally adjust object queries to account for changes in viewing direction
and lighting conditions directly in the latent space, while still modeling the
geometric motion explicitly. Combined with a novel learnable track embedding
that aids in modeling the existence probability of tracks, this results in a
generic tracking framework that can be integrated with any query-based
detector. Extensive experiments on the nuScenes benchmark demonstrate the
benefits of our approach, showing state-of-the-art performance for DETR3D-based
trackers while drastically reducing the number of identity switches of tracks
at the same time.
</p></li>
</ul>

<h3>Title: MTR++: Multi-Agent Motion Prediction with Symmetric Scene Modeling and Guided Intention Querying. (arXiv:2306.17770v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17770">http://arxiv.org/abs/2306.17770</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17770] MTR++: Multi-Agent Motion Prediction with Symmetric Scene Modeling and Guided Intention Querying](http://arxiv.org/abs/2306.17770) #transformer</code></li>
<li>Summary: <p>Motion prediction is crucial for autonomous driving systems to understand
complex driving scenarios and make informed decisions. However, this task is
challenging due to the diverse behaviors of traffic participants and complex
environmental contexts. In this paper, we propose Motion TRansformer (MTR)
frameworks to address these challenges. The initial MTR framework utilizes a
transformer encoder-decoder structure with learnable intention queries,
enabling efficient and accurate prediction of future trajectories. By
customizing intention queries for distinct motion modalities, MTR improves
multimodal motion prediction while reducing reliance on dense goal candidates.
The framework comprises two essential processes: global intention localization,
identifying the agent's intent to enhance overall efficiency, and local
movement refinement, adaptively refining predicted trajectories for improved
accuracy. Moreover, we introduce an advanced MTR++ framework, extending the
capability of MTR to simultaneously predict multimodal motion for multiple
agents. MTR++ incorporates symmetric context modeling and mutually-guided
intention querying modules to facilitate future behavior interaction among
multiple agents, resulting in scene-compliant future trajectories. Extensive
experimental results demonstrate that the MTR framework achieves
state-of-the-art performance on the highly-competitive motion prediction
benchmarks, while the MTR++ framework surpasses its precursor, exhibiting
enhanced performance and efficiency in predicting accurate multimodal future
trajectories for multiple agents.
</p></li>
</ul>

<h3>Title: HIDFlowNet: A Flow-Based Deep Network for Hyperspectral Image Denoising. (arXiv:2306.17797v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17797">http://arxiv.org/abs/2306.17797</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17797] HIDFlowNet: A Flow-Based Deep Network for Hyperspectral Image Denoising](http://arxiv.org/abs/2306.17797) #transformer</code></li>
<li>Summary: <p>Hyperspectral image (HSI) denoising is essentially ill-posed since a noisy
HSI can be degraded from multiple clean HSIs. However, current deep
learning-based approaches ignore this fact and restore the clean image with
deterministic mapping (i.e., the network receives a noisy HSI and outputs a
clean HSI). To alleviate this issue, this paper proposes a flow-based HSI
denoising network (HIDFlowNet) to directly learn the conditional distribution
of the clean HSI given the noisy HSI and thus diverse clean HSIs can be sampled
from the conditional distribution. Overall, our HIDFlowNet is induced from the
flow methodology and contains an invertible decoder and a conditional encoder,
which can fully decouple the learning of low-frequency and high-frequency
information of HSI. Specifically, the invertible decoder is built by staking a
succession of invertible conditional blocks (ICBs) to capture the local
high-frequency details since the invertible network is information-lossless.
The conditional encoder utilizes down-sampling operations to obtain
low-resolution images and uses transformers to capture correlations over a long
distance so that global low-frequency information can be effectively extracted.
Extensive experimental results on simulated and real HSI datasets verify the
superiority of our proposed HIDFlowNet compared with other state-of-the-art
methods both quantitatively and visually.
</p></li>
</ul>

<h3>Title: Hardwiring ViT Patch Selectivity into CNNs using Patch Mixing. (arXiv:2306.17848v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17848">http://arxiv.org/abs/2306.17848</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17848] Hardwiring ViT Patch Selectivity into CNNs using Patch Mixing](http://arxiv.org/abs/2306.17848) #transformer</code></li>
<li>Summary: <p>Vision transformers (ViTs) have significantly changed the computer vision
landscape and have periodically exhibited superior performance in vision tasks
compared to convolutional neural networks (CNNs). Although the jury is still
out on which model type is superior, each has unique inductive biases that
shape their learning and generalization performance. For example, ViTs have
interesting properties with respect to early layer non-local feature
dependence, as well as self-attention mechanisms which enhance learning
flexibility, enabling them to ignore out-of-context image information more
effectively. We hypothesize that this power to ignore out-of-context
information (which we name $\textit{patch selectivity}$), while integrating
in-context information in a non-local manner in early layers, allows ViTs to
more easily handle occlusion. In this study, our aim is to see whether we can
have CNNs $\textit{simulate}$ this ability of patch selectivity by effectively
hardwiring this inductive bias using Patch Mixing data augmentation, which
consists of inserting patches from another image onto a training image and
interpolating labels between the two image classes. Specifically, we use Patch
Mixing to train state-of-the-art ViTs and CNNs, assessing its impact on their
ability to ignore out-of-context patches and handle natural occlusions. We find
that ViTs do not improve nor degrade when trained using Patch Mixing, but CNNs
acquire new capabilities to ignore out-of-context information and improve on
occlusion benchmarks, leaving us to conclude that this training method is a way
of simulating in CNNs the abilities that ViTs already possess. We will release
our Patch Mixing implementation and proposed datasets for public use. Project
page: https://arielnlee.github.io/PatchMixing/
</p></li>
</ul>

<h3>Title: A Cost-aware Study of Depression Language on Social Media using Topic and Affect Contextualization. (arXiv:2306.17564v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17564">http://arxiv.org/abs/2306.17564</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17564] A Cost-aware Study of Depression Language on Social Media using Topic and Affect Contextualization](http://arxiv.org/abs/2306.17564) #transformer</code></li>
<li>Summary: <p>Depression is a growing issue in society's mental health that affects all
areas of life and can even lead to suicide. Fortunately, prevention programs
can be effective in its treatment. In this context, this work proposes an
automatic system for detecting depression on social media based on machine
learning and natural language processing methods. This paper presents the
following contributions: (i) an ensemble learning system that combines several
types of text representations for depression detection, including recent
advances in the field; (ii) a contextualization schema through topic and
affective information; (iii) an analysis of models' energy consumption,
establishing a trade-off between classification performance and overall
computational costs. To assess the proposed models' effectiveness, a thorough
evaluation is performed in two datasets that model depressive text. Experiments
indicate that the proposed contextualization strategies can improve the
classification and that approaches that use Transformers can improve the
overall F-score by 2% while augmenting the energy cost a hundred times.
Finally, this work paves the way for future energy-wise systems by considering
both the performance classification and the energy consumption.
</p></li>
</ul>

<h3>Title: Towards Improving the Performance of Pre-Trained Speech Models for Low-Resource Languages Through Lateral Inhibition. (arXiv:2306.17792v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17792">http://arxiv.org/abs/2306.17792</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17792] Towards Improving the Performance of Pre-Trained Speech Models for Low-Resource Languages Through Lateral Inhibition](http://arxiv.org/abs/2306.17792) #transformer</code></li>
<li>Summary: <p>With the rise of bidirectional encoder representations from Transformer
models in natural language processing, the speech community has adopted some of
their development methodologies. Therefore, the Wav2Vec models were introduced
to reduce the data required to obtain state-of-the-art results. This work
leverages this knowledge and improves the performance of the pre-trained speech
models by simply replacing the fine-tuning dense layer with a lateral
inhibition layer inspired by the biological process. Our experiments on
Romanian, a low-resource language, show an average improvement of 12.5% word
error rate (WER) using the lateral inhibition layer. In addition, we obtain
state-of-the-art results on both the Romanian Speech Corpus and the Robin
Technical Acquisition Corpus with 1.78% WER and 29.64% WER, respectively.
</p></li>
</ul>

<h3>Title: Graphtester: Exploring Theoretical Boundaries of GNNs on Graph Datasets. (arXiv:2306.17482v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17482">http://arxiv.org/abs/2306.17482</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17482] Graphtester: Exploring Theoretical Boundaries of GNNs on Graph Datasets](http://arxiv.org/abs/2306.17482) #transformer</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) have emerged as a powerful tool for learning
from graph-structured data. However, even state-of-the-art architectures have
limitations on what structures they can distinguish, imposing theoretical
limits on what the networks can achieve on different datasets. In this paper,
we provide a new tool called Graphtester for a comprehensive analysis of the
theoretical capabilities of GNNs for various datasets, tasks, and scores. We
use Graphtester to analyze over 40 different graph datasets, determining upper
bounds on the performance of various GNNs based on the number of layers.
Further, we show that the tool can also be used for Graph Transformers using
positional node encodings, thereby expanding its scope. Finally, we demonstrate
that features generated by Graphtester can be used for practical applications
such as Graph Transformers, and provide a synthetic dataset to benchmark node
and edge features, such as positional encodings. The package is freely
available at the following URL: https://github.com/meakbiyik/graphtester.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: EyeBAG: Accurate Control of Eye Blink and Gaze Based on Data Augmentation Leveraging Style Mixing. (arXiv:2306.17391v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17391">http://arxiv.org/abs/2306.17391</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17391] EyeBAG: Accurate Control of Eye Blink and Gaze Based on Data Augmentation Leveraging Style Mixing](http://arxiv.org/abs/2306.17391) #generative</code></li>
<li>Summary: <p>Recent developments in generative models have enabled the generation of
photo-realistic human face images, and downstream tasks utilizing face
generation technology have advanced accordingly. However, models for downstream
tasks are yet substandard at eye control (e.g. eye blink, gaze redirection). To
overcome such eye control problems, we introduce a novel framework consisting
of two distinct modules: a blink control module and a gaze redirection module.
We also propose a novel data augmentation method to train each module,
leveraging style mixing to obtain images with desired features. We show that
our framework produces eye-controlled images of high quality, and demonstrate
how it can be used to improve the performance of downstream tasks.
</p></li>
</ul>

<h3>Title: Sphere2Vec: A General-Purpose Location Representation Learning over a Spherical Surface for Large-Scale Geospatial Predictions. (arXiv:2306.17624v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17624">http://arxiv.org/abs/2306.17624</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17624] Sphere2Vec: A General-Purpose Location Representation Learning over a Spherical Surface for Large-Scale Geospatial Predictions](http://arxiv.org/abs/2306.17624) #generative</code></li>
<li>Summary: <p>Generating learning-friendly representations for points in space is a
fundamental and long-standing problem in ML. Recently, multi-scale encoding
schemes (such as Space2Vec and NeRF) were proposed to directly encode any point
in 2D/3D Euclidean space as a high-dimensional vector, and has been
successfully applied to various geospatial prediction and generative tasks.
However, all current 2D and 3D location encoders are designed to model point
distances in Euclidean space. So when applied to large-scale real-world GPS
coordinate datasets, which require distance metric learning on the spherical
surface, both types of models can fail due to the map projection distortion
problem (2D) and the spherical-to-Euclidean distance approximation error (3D).
To solve these problems, we propose a multi-scale location encoder called
Sphere2Vec which can preserve spherical distances when encoding point
coordinates on a spherical surface. We developed a unified view of
distance-reserving encoding on spheres based on the DFS. We also provide
theoretical proof that the Sphere2Vec preserves the spherical surface distance
between any two points, while existing encoding schemes do not. Experiments on
20 synthetic datasets show that Sphere2Vec can outperform all baseline models
on all these datasets with up to 30.8% error rate reduction. We then apply
Sphere2Vec to three geo-aware image classification tasks - fine-grained species
recognition, Flickr image recognition, and remote sensing image classification.
Results on 7 real-world datasets show the superiority of Sphere2Vec over
multiple location encoders on all three tasks. Further analysis shows that
Sphere2Vec outperforms other location encoder models, especially in the polar
regions and data-sparse areas because of its nature for spherical surface
distance preservation. Code and data are available at
https://gengchenmai.github.io/sphere2vec-website/.
</p></li>
</ul>

<h3>Title: Multimodal Prompt Retrieval for Generative Visual Question Answering. (arXiv:2306.17675v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17675">http://arxiv.org/abs/2306.17675</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17675] Multimodal Prompt Retrieval for Generative Visual Question Answering](http://arxiv.org/abs/2306.17675) #generative</code></li>
<li>Summary: <p>Recent years have witnessed impressive results of pre-trained vision-language
models on knowledge-intensive tasks such as visual question answering (VQA).
Despite the recent advances in VQA, existing methods mainly adopt a
discriminative formulation that predicts answers within a pre-defined label
set, leading to easy overfitting on low-resource domains with limited labeled
data (e.g., medicine) and poor generalization under domain shift to another
dataset. To tackle this limitation, we propose a novel generative model
enhanced by multimodal prompt retrieval (MPR) that integrates retrieved prompts
and multimodal features to generate answers in free text. Our generative model
enables rapid zero-shot dataset adaptation to unseen data distributions and
open-set answer labels across datasets. Our experiments on medical VQA tasks
show that MPR outperforms its non-retrieval counterpart by up to 30% accuracy
points in a few-shot domain adaptation setting.
</p></li>
</ul>

<h3>Title: RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19 Assessment in Primary Care. (arXiv:2306.17175v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17175">http://arxiv.org/abs/2306.17175</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17175] RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19 Assessment in Primary Care](http://arxiv.org/abs/2306.17175) #generative</code></li>
<li>Summary: <p>Clinical decision-making is a fundamental stage in delivering appropriate
care to patients. In recent years several decision-making systems designed to
aid the clinician in this process have been developed. However, technical
solutions currently in use are based on simple regression models and are only
able to take into account simple pre-defined multiple-choice features, such as
patient age, pre-existing conditions, smoker status, etc. One particular source
of patient data, that available decision-making systems are incapable of
processing is the collection of patient consultation GP notes. These contain
crucial signs and symptoms - the information used by clinicians in order to
make a final decision and direct the patient to the appropriate care.
Extracting information from GP notes is a technically challenging problem, as
they tend to include abbreviations, typos, and incomplete sentences.
</p></li>
</ul>

<p>This paper addresses this open challenge. We present a framework that
performs knowledge graph construction from raw GP medical notes written during
or after patient consultations. By relying on support phrases mined from the
SNOMED ontology, as well as predefined supported facts from values used in the
RECAP (REmote COVID-19 Assessment in Primary Care) patient risk prediction
tool, our graph generative framework is able to extract structured knowledge
graphs from the highly unstructured and inconsistent format that consultation
notes are written in. Our knowledge graphs include information about existing
patient symptoms, their duration, and their severity.
</p>
<p>We apply our framework to consultation notes of COVID-19 patients in the UK
COVID-19 Clinical Assesment Servcie (CCAS) patient dataset. We provide a
quantitative evaluation of the performance of our framework, demonstrating that
our approach has better accuracy than traditional NLP methods when answering
questions about patients.
</p>

<h3>Title: Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis. (arXiv:2306.17181v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17181">http://arxiv.org/abs/2306.17181</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17181] Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis](http://arxiv.org/abs/2306.17181) #generative</code></li>
<li>Summary: <p>Generative Adversarial Networks (GAN) is a model for data synthesis, which
creates plausible data through the competition of generator and discriminator.
Although GAN application to image synthesis is extensively studied, it has
inherent limitations to natural language generation. Because natural language
is composed of discrete tokens, a generator has difficulty updating its
gradient through backpropagation; therefore, most text-GAN studies generate
sentences starting with a random token based on a reward system. Thus, the
generators of previous studies are pre-trained in an autoregressive way before
adversarial training, causing data memorization that synthesized sentences
reproduce the training data. In this paper, we synthesize sentences using a
framework similar to the original GAN. More specifically, we propose Text
Embedding Space Generative Adversarial Networks (TESGAN) which generate
continuous text embedding spaces instead of discrete tokens to solve the
gradient backpropagation problem. Furthermore, TESGAN conducts unsupervised
learning which does not directly refer to the text of the training data to
overcome the data memorization issue. By adopting this novel method, TESGAN can
synthesize new sentences, showing the potential of unsupervised learning for
text synthesis. We expect to see extended research combining Large Language
Models with a new perspective of viewing text as an continuous space.
</p></li>
</ul>

<h3>Title: The power of motifs as inductive bias for learning molecular distributions. (arXiv:2306.17246v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17246">http://arxiv.org/abs/2306.17246</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17246] The power of motifs as inductive bias for learning molecular distributions](http://arxiv.org/abs/2306.17246) #generative</code></li>
<li>Summary: <p>Machine learning for molecules holds great potential for efficiently
exploring the vast chemical space and thus streamlining the drug discovery
process by facilitating the design of new therapeutic molecules. Deep
generative models have shown promising results for molecule generation, but the
benefits of specific inductive biases for learning distributions over small
graphs are unclear. Our study aims to investigate the impact of subgraph
structures and vocabulary design on distribution learning, using small drug
molecules as a case study. To this end, we introduce Subcover, a new
subgraph-based fragmentation scheme, and evaluate it through a two-step
variational auto-encoder. Our results show that Subcover's improved
identification of chemically meaningful subgraphs leads to a relative
improvement of the FCD score by 30%, outperforming previous methods. Our
findings highlight the potential of Subcover to enhance the performance and
scalability of existing methods, contributing to the advancement of drug
discovery.
</p></li>
</ul>

<h3>Title: TemperatureGAN: Generative Modeling of Regional Atmospheric Temperatures. (arXiv:2306.17248v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17248">http://arxiv.org/abs/2306.17248</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17248] TemperatureGAN: Generative Modeling of Regional Atmospheric Temperatures](http://arxiv.org/abs/2306.17248) #generative</code></li>
<li>Summary: <p>Stochastic generators are useful for estimating climate impacts on various
sectors. Projecting climate risk in various sectors, e.g. energy systems,
requires generators that are accurate (statistical resemblance to
ground-truth), reliable (do not produce erroneous examples), and efficient.
Leveraging data from the North American Land Data Assimilation System, we
introduce TemperatureGAN, a Generative Adversarial Network conditioned on
months, locations, and time periods, to generate 2m above ground atmospheric
temperatures at an hourly resolution. We propose evaluation methods and metrics
to measure the quality of generated samples. We show that TemperatureGAN
produces high-fidelity examples with good spatial representation and temporal
dynamics consistent with known diurnal cycles.
</p></li>
</ul>

<h3>Title: Thompson sampling for improved exploration in GFlowNets. (arXiv:2306.17693v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17693">http://arxiv.org/abs/2306.17693</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17693] Thompson sampling for improved exploration in GFlowNets](http://arxiv.org/abs/2306.17693) #generative</code></li>
<li>Summary: <p>Generative flow networks (GFlowNets) are amortized variational inference
algorithms that treat sampling from a distribution over compositional objects
as a sequential decision-making problem with a learnable action policy. Unlike
other algorithms for hierarchical sampling that optimize a variational bound,
GFlowNet algorithms can stably run off-policy, which can be advantageous for
discovering modes of the target distribution. Despite this flexibility in the
choice of behaviour policy, the optimal way of efficiently selecting
trajectories for training has not yet been systematically explored. In this
paper, we view the choice of trajectories for training as an active learning
problem and approach it using Bayesian techniques inspired by methods for
multi-armed bandits. The proposed algorithm, Thompson sampling GFlowNets
(TS-GFN), maintains an approximate posterior distribution over policies and
samples trajectories from this posterior for training. We show in two domains
that TS-GFN yields improved exploration and thus faster convergence to the
target distribution than the off-policy exploration strategies used in past
work.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Look, Remember and Reason: Visual Reasoning with Grounded Rationales. (arXiv:2306.17778v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17778">http://arxiv.org/abs/2306.17778</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17778] Look, Remember and Reason: Visual Reasoning with Grounded Rationales](http://arxiv.org/abs/2306.17778) #large language model</code></li>
<li>Summary: <p>Large language models have recently shown human level performance on a
variety of reasoning tasks. However, the ability of these models to perform
complex visual reasoning has not been studied in detail yet. A key challenge in
many visual reasoning tasks is that the visual information needs to be tightly
integrated in the reasoning process. We propose to address this challenge by
drawing inspiration from human visual problem solving which depends on a
variety of low-level visual capabilities. It can often be cast as the three
step-process of ``Look, Remember, Reason'': visual information is incrementally
extracted using low-level visual routines in a step-by-step fashion until a
final answer is reached. We follow the same paradigm to enable existing large
language models, with minimal changes to the architecture, to solve visual
reasoning problems. To this end, we introduce rationales over the visual input
that allow us to integrate low-level visual capabilities, such as object
recognition and tracking, as surrogate tasks. We show competitive performance
on diverse visual reasoning tasks from the CLEVR, CATER, and ACRE datasets over
state-of-the-art models designed specifically for these tasks.
</p></li>
</ul>

<h3>Title: Preference Ranking Optimization for Human Alignment. (arXiv:2306.17492v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17492">http://arxiv.org/abs/2306.17492</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17492] Preference Ranking Optimization for Human Alignment](http://arxiv.org/abs/2306.17492) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) often contain misleading content, emphasizing
the need to align them with human values to ensure secur AI systems.
Reinforcement learning from human feedback (RLHF) has been employed to achieve
this alignment by combining a reward model, typically based on Bradley-Terry
paired comparison, with an RL algorithm such as Proximal Policy Optimization
(PPO) to optimize LLM responses. However, RLHF exhibits complexity,
instability, and sensitivity to hyperparameters. In this paper, we propose
Preference Ranking Optimization (PRO) as an alternative to PPO for directly
aligning LLMs with the Bradley-Terry comparison. PRO extends the pairwise
Bradley-Terry comparison to accommodate preference rankings of any length. By
iteratively contrasting the likelihood of generating responses, PRO instructs
the LLM to prioritize the best response while progressively ranking the
remaining responses. In this manner, PRO effectively transforms human alignment
into aligning the probability ranking of $n$ responses generated by LLM with
the preference ranking of humans towards these responses. Experiments have
shown that PRO outperforms existing alignment algorithms, achieving comparable
results to ChatGPT and human responses through automatic-based, reward-based,
GPT-4, and human evaluations. Furthermore, we demonstrate that longer, more
diverse, and higher-quality preference ranking sequences can consistently
enhance the performance of human alignment.
</p></li>
</ul>

<h3>Title: Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models. (arXiv:2306.17820v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17820">http://arxiv.org/abs/2306.17820</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17820] Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models](http://arxiv.org/abs/2306.17820) #large language model</code></li>
<li>Summary: <p>Symbolization methods in large language models (LLMs) have been shown
effective to improve LLMs' reasoning ability. However, most of these approaches
hinge on mapping natural languages to formal languages (e.g., Python, SQL) that
are more syntactically complete and free of ambiguity. Although effective, they
depart from the natural language itself and deviate from the habits of human
thinking, and instead cater more to the execution mindset of computers. In
contrast, we hope to simplify natural language by starting from the concept of
symbols in linguistics itself, so that LLMs can learn the common formulation
and general solution of reasoning problems wrapped in different natural
semantics. From this consideration, we propose \textbf{Meta-Reasoning}, which
allows LLMs to automatically accomplish semantic-symbol deconstruction, i.e.,
semantic resolution, to maximally reduce different questions of certain
reasoning tasks to similar natural language representation, thus gaining the
ability to learn by analogy and facilitating data-efficient in-context
learning. Our experiments show that the Meta-Reasoning paradigm saliently
enhances LLMs' reasoning performance with fewer demonstrations. They can learn
not only reasoning chains but also general solutions to certain types of tasks.
In particular, for symbolic reasoning tasks, such as 7-step Tracking Shuffled
Objects, GPT-3 (text-davinci-002) achieves over 99% accuracy with only one
Meta-Reasoning demonstration, outperforming all current LLMs with the standard
chain-of-thought prompting.
</p></li>
</ul>

<h3>Title: DisasterResponseGPT: Large Language Models for Accelerated Plan of Action Development in Disaster Response Scenarios. (arXiv:2306.17271v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17271">http://arxiv.org/abs/2306.17271</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17271] DisasterResponseGPT: Large Language Models for Accelerated Plan of Action Development in Disaster Response Scenarios](http://arxiv.org/abs/2306.17271) #large language model</code></li>
<li>Summary: <p>The development of plans of action in disaster response scenarios is a
time-consuming process. Large Language Models (LLMs) offer a powerful solution
to expedite this process through in-context learning. This study presents
DisasterResponseGPT, an algorithm that leverages LLMs to generate valid plans
of action quickly by incorporating disaster response and planning guidelines in
the initial prompt. In DisasterResponseGPT, users input the scenario
description and receive a plan of action as output. The proposed method
generates multiple plans within seconds, which can be further refined following
the user's feedback. Preliminary results indicate that the plans of action
developed by DisasterResponseGPT are comparable to human-generated ones while
offering greater ease of modification in real-time. This approach has the
potential to revolutionize disaster response operations by enabling rapid
updates and adjustments during the plan's execution.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: ReMaX: Relaxing for Better Training on Efficient Panoptic Segmentation. (arXiv:2306.17319v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17319">http://arxiv.org/abs/2306.17319</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17319] ReMaX: Relaxing for Better Training on Efficient Panoptic Segmentation](http://arxiv.org/abs/2306.17319) #segmentation</code></li>
<li>Summary: <p>This paper presents a new mechanism to facilitate the training of mask
transformers for efficient panoptic segmentation, democratizing its deployment.
We observe that due to its high complexity, the training objective of panoptic
segmentation will inevitably lead to much higher false positive penalization.
Such unbalanced loss makes the training process of the end-to-end
mask-transformer based architectures difficult, especially for efficient
models. In this paper, we present ReMaX that adds relaxation to mask
predictions and class predictions during training for panoptic segmentation. We
demonstrate that via these simple relaxation techniques during training, our
model can be consistently improved by a clear margin \textbf{without} any extra
computational cost on inference. By combining our method with efficient
backbones like MobileNetV3-Small, our method achieves new state-of-the-art
results for efficient panoptic segmentation on COCO, ADE20K and Cityscapes.
Code and pre-trained checkpoints will be available at
\url{https://github.com/google-research/deeplab2}.
</p></li>
</ul>

<h3>Title: Topological Data Analysis Guided Segment Anything Model Prompt Optimization for Zero-Shot Segmentation in Biological Imaging. (arXiv:2306.17400v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17400">http://arxiv.org/abs/2306.17400</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17400] Topological Data Analysis Guided Segment Anything Model Prompt Optimization for Zero-Shot Segmentation in Biological Imaging](http://arxiv.org/abs/2306.17400) #segmentation</code></li>
<li>Summary: <p>Emerging foundation models in machine learning are models trained on vast
amounts of data that have been shown to generalize well to new tasks. Often
these models can be prompted with multi-modal inputs that range from natural
language descriptions over images to point clouds. In this paper, we propose
topological data analysis (TDA) guided prompt optimization for the Segment
Anything Model (SAM) and show preliminary results in the biological image
segmentation domain. Our approach replaces the standard grid search approach
that is used in the original implementation and finds point locations based on
their topological significance. Our results show that the TDA optimized point
cloud is much better suited for finding small objects and massively reduces
computational complexity despite the extra step in scenarios which require many
segmentations.
</p></li>
</ul>

<h3>Title: Detection-segmentation convolutional neural network for autonomous vehicle perception. (arXiv:2306.17485v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17485">http://arxiv.org/abs/2306.17485</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17485] Detection-segmentation convolutional neural network for autonomous vehicle perception](http://arxiv.org/abs/2306.17485) #segmentation</code></li>
<li>Summary: <p>Object detection and segmentation are two core modules of an autonomous
vehicle perception system. They should have high efficiency and low latency
while reducing computational complexity. Currently, the most commonly used
algorithms are based on deep neural networks, which guarantee high efficiency
but require high-performance computing platforms. In the case of autonomous
vehicles, i.e. cars, but also drones, it is necessary to use embedded platforms
with limited computing power, which makes it difficult to meet the requirements
described above. A reduction in the complexity of the network can be achieved
by using an appropriate: architecture, representation (reduced numerical
precision, quantisation, pruning), and computing platform. In this paper, we
focus on the first factor - the use of so-called detection-segmentation
networks as a component of a perception system. We considered the task of
segmenting the drivable area and road markings in combination with the
detection of selected objects (pedestrians, traffic lights, and obstacles). We
compared the performance of three different architectures described in the
literature: MultiTask V3, HybridNets, and YOLOP. We conducted the experiments
on a custom dataset consisting of approximately 500 images of the drivable area
and lane markings, and 250 images of detected objects. Of the three methods
analysed, MultiTask V3 proved to be the best, achieving 99% mAP_50 for
detection, 97% MIoU for drivable area segmentation, and 91% MIoU for lane
segmentation, as well as 124 fps on the RTX 3060 graphics card. This
architecture is a good solution for embedded perception systems for autonomous
vehicles. The code is available at: https://github.com/vision-agh/MMAR_2023.
</p></li>
</ul>

<h3>Title: Achieving RGB-D level Segmentation Performance from a Single ToF Camera. (arXiv:2306.17636v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17636">http://arxiv.org/abs/2306.17636</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17636] Achieving RGB-D level Segmentation Performance from a Single ToF Camera](http://arxiv.org/abs/2306.17636) #segmentation</code></li>
<li>Summary: <p>Depth is a very important modality in computer vision, typically used as
complementary information to RGB, provided by RGB-D cameras. In this work, we
show that it is possible to obtain the same level of accuracy as RGB-D cameras
on a semantic segmentation task using infrared (IR) and depth images from a
single Time-of-Flight (ToF) camera. In order to fuse the IR and depth
modalities of the ToF camera, we introduce a method utilizing depth-specific
convolutions in a multi-task learning framework. In our evaluation on an in-car
segmentation dataset, we demonstrate the competitiveness of our method against
the more costly RGB-D approaches.
</p></li>
</ul>

<h3>Title: Scaling Model Checking for DNN Analysis via State-Space Reduction and Input Segmentation (Extended Version). (arXiv:2306.17323v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2306.17323">http://arxiv.org/abs/2306.17323</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2306.17323] Scaling Model Checking for DNN Analysis via State-Space Reduction and Input Segmentation (Extended Version)](http://arxiv.org/abs/2306.17323) #segmentation</code></li>
<li>Summary: <p>Owing to their remarkable learning capabilities and performance in real-world
applications, the use of machine learning systems based on Neural Networks
(NNs) has been continuously increasing. However, various case studies and
empirical findings in the literature suggest that slight variations to NN
inputs can lead to erroneous and undesirable NN behavior. This has led to
considerable interest in their formal analysis, aiming to provide guarantees
regarding a given NN's behavior. Existing frameworks provide robustness and/or
safety guarantees for the trained NNs, using satisfiability solving and linear
programming. We proposed FANNet, the first model checking-based framework for
analyzing a broader range of NN properties. However, the state-space explosion
associated with model checking entails a scalability problem, making the FANNet
applicable only to small NNs. This work develops state-space reduction and
input segmentation approaches, to improve the scalability and timing efficiency
of formal NN analysis. Compared to the state-of-the-art FANNet, this enables
our new model checking-based framework to reduce the verification's timing
overhead by a factor of up to 8000, making the framework applicable to NNs even
with approximately $80$ times more network parameters. This in turn allows the
analysis of NN safety properties using the new framework, in addition to all
the NN properties already included with FANNet. The framework is shown to be
efficiently able to analyze properties of NNs trained on healthcare datasets as
well as the well--acknowledged ACAS Xu NNs.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
