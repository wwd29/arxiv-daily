<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: Towards Zero-trust Security for the Metaverse. (arXiv:2302.08885v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08885">http://arxiv.org/abs/2302.08885</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08885] Towards Zero-trust Security for the Metaverse](http://arxiv.org/abs/2302.08885) #security</code></li>
<li>Summary: <p>By focusing on immersive interaction among users, the burgeoning Metaverse
can be viewed as a natural extension of existing social media. Similar to
traditional online social networks, there are numerous security and privacy
issues in the Metaverse (e.g., attacks on user authentication and
impersonation). In this paper, we develop a holistic research agenda for
zero-trust user authentication in social virtual reality (VR), an early
prototype of the Metaverse. Our proposed research includes four concrete steps:
investigating biometrics-based authentication that is suitable for continuously
authenticating VR users, leveraging federated learning (FL) for protecting user
privacy in biometric data, improving the accuracy of continuous VR
authentication with multimodal data, and boosting the usability of zero-trust
security with adaptive VR authentication. Our preliminary study demonstrates
that conventional FL algorithms are not well suited for biometrics-based
authentication of VR users, leading to an accuracy of less than 10%. We discuss
the root cause of this problem, the associated open challenges, and several
future directions for realizing our research vision.
</p></li>
</ul>

<h3>Title: Unique Identification of 50,000+ Virtual Reality Users from Head &amp; Hand Motion Data. (arXiv:2302.08927v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08927">http://arxiv.org/abs/2302.08927</a></li>
<li>Code URL: <a href="https://github.com/metaguard/identification">https://github.com/metaguard/identification</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08927] Unique Identification of 50,000+ Virtual Reality Users from Head &amp; Hand Motion Data](http://arxiv.org/abs/2302.08927) #security</code></li>
<li>Summary: <p>With the recent explosive growth of interest and investment in virtual
reality (VR) and the so-called "metaverse," public attention has rightly
shifted toward the unique security and privacy threats that these platforms may
pose. While it has long been known that people reveal information about
themselves via their motion, the extent to which this makes an individual
globally identifiable within virtual reality has not yet been widely
understood. In this study, we show that a large number of real VR users
(N=55,541) can be uniquely and reliably identified across multiple sessions
using just their head and hand motion relative to virtual objects. After
training a classification model on 5 minutes of data per person, a user can be
uniquely identified amongst the entire pool of 50,000+ with 94.33% accuracy
from 100 seconds of motion, and with 73.20% accuracy from just 10 seconds of
motion. This work is the first to truly demonstrate the extent to which
biomechanics may serve as a unique identifier in VR, on par with widely used
biometrics such as facial or fingerprint recognition.
</p></li>
</ul>

<h3>Title: Towards Automated Homomorphic Encryption Parameter Selection with Fuzzy Logic and Linear Programming. (arXiv:2302.08930v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08930">http://arxiv.org/abs/2302.08930</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08930] Towards Automated Homomorphic Encryption Parameter Selection with Fuzzy Logic and Linear Programming](http://arxiv.org/abs/2302.08930) #security</code></li>
<li>Summary: <p>Homomorphic Encryption (HE) is a set of powerful properties of certain
cryptosystems that allow for privacy-preserving operation over the encrypted
text. Still, HE is not widespread due to limitations in terms of efficiency and
usability. Among the challenges of HE, scheme parametrization (i.e., the
selection of appropriate parameters within the algorithms) is a relevant
multi-faced problem. First, the parametrization needs to comply with a set of
properties to guarantee the security of the underlying scheme. Second,
parametrization requires a deep understanding of the low-level primitives since
the parameters have a confronting impact on the precision, performance, and
security of the scheme. Finally, the circuit to be executed influences, and it
is influenced by, the parametrization. Thus, there is no general optimal
selection of parameters, and this selection depends on the circuit and the
scenario of the application. Currently, most of the existing HE frameworks
require cryptographers to address these considerations manually. It requires a
minimum of expertise acquired through a steep learning curve. In this paper, we
propose a unified solution for the aforementioned challenges. Concretely, we
present an expert system combining Fuzzy Logic and Linear Programming. The
Fuzzy Logic Modules receive a user selection of high-level priorities for the
security, efficiency, and performance of the cryptosystem. Based on these
preferences, the expert system generates a Linear Programming Model that
obtains optimal combinations of parameters by considering those priorities
while preserving a minimum level of security for the cryptosystem. We conduct
an extended evaluation where we show that an expert system generates optimal
parameter selections that maintain user preferences without undergoing the
inherent complexity of analyzing the circuit.
</p></li>
</ul>

<h3>Title: DETER: Design for Trust utilizing Rareness Reduction. (arXiv:2302.08984v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08984">http://arxiv.org/abs/2302.08984</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08984] DETER: Design for Trust utilizing Rareness Reduction](http://arxiv.org/abs/2302.08984) #security</code></li>
<li>Summary: <p>Increasing design complexity and reduced time-to-market have motivated
manufacturers to outsource some parts of the System-on-Chip (SoC) design flow
to third-party vendors. This provides an opportunity for attackers to introduce
hardware Trojans by constructing stealthy triggers consisting of rare events
(e.g., rare signals, states, and transitions). There are promising test
generation-based hardware Trojan detection techniques that rely on the
activation of rare events. In this paper, we investigate rareness reduction as
a design-for-trust solution to make it harder for an adversary to hide Trojans
(easier for Trojan detection). Specifically, we analyze different avenues to
reduce the potential rare trigger cases, including design diversity and area
optimization. While there is a good understanding of the relationship between
area, power, energy, and performance, this research provides a better insight
into the dependency between area and security. Our experimental evaluation
demonstrates that area reduction leads to a reduction in rareness. It also
reveals that reducing rareness leads to faster Trojan detection as well as
improved coverage by Trojan detection methods.
</p></li>
</ul>

<h3>Title: Measuring Equality in Machine Learning Security Defenses. (arXiv:2302.08973v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08973">http://arxiv.org/abs/2302.08973</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08973] Measuring Equality in Machine Learning Security Defenses](http://arxiv.org/abs/2302.08973) #security</code></li>
<li>Summary: <p>The machine learning security community has developed myriad defenses for
evasion attacks over the past decade. An understudied question in that
community is: for whom do these defenses defend? In this work, we consider some
common approaches to defending learned systems and whether those approaches may
offer unexpected performance inequities when used by different sub-populations.
We outline simple parity metrics and a framework for analysis that can begin to
answer this question through empirical results of the fairness implications of
machine learning security methods. Many methods have been proposed that can
cause direct harm, which we describe as biased vulnerability and biased
rejection. Our framework and metric can be applied to robustly trained models,
preprocessing-based methods, and rejection methods to capture behavior over
security budgets. We identify a realistic dataset with a reasonable
computational cost suitable for measuring the equality of defenses. Through a
case study in speech command recognition, we show how such defenses do not
offer equal protection for social subgroups and how to perform such analyses
for robustness training, and we present a comparison of fairness between two
rejection-based defenses: randomized smoothing and neural rejection. We offer
further analysis of factors that correlate to equitable defenses to stimulate
the future investigation of how to assist in building such defenses. To the
best of our knowledge, this is the first work that examines the fairness
disparity in the accuracy-robustness trade-off in speech data and addresses
fairness evaluation for rejection-based defenses.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Uncertainty-aware Self-training for Low-resource Neural Sequence Labeling. (arXiv:2302.08659v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08659">http://arxiv.org/abs/2302.08659</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08659] Uncertainty-aware Self-training for Low-resource Neural Sequence Labeling](http://arxiv.org/abs/2302.08659) #privacy</code></li>
<li>Summary: <p>Neural sequence labeling (NSL) aims at assigning labels for input language
tokens, which covers a broad range of applications, such as named entity
recognition (NER) and slot filling, etc. However, the satisfying results
achieved by traditional supervised-based approaches heavily depend on the large
amounts of human annotation data, which may not be feasible in real-world
scenarios due to data privacy and computation efficiency issues. This paper
presents SeqUST, a novel uncertain-aware self-training framework for NSL to
address the labeled data scarcity issue and to effectively utilize unlabeled
data. Specifically, we incorporate Monte Carlo (MC) dropout in Bayesian neural
network (BNN) to perform uncertainty estimation at the token level and then
select reliable language tokens from unlabeled data based on the model
confidence and certainty. A well-designed masked sequence labeling task with a
noise-robust loss supports robust training, which aims to suppress the problem
of noisy pseudo labels. In addition, we develop a Gaussian-based consistency
regularization technique to further improve the model robustness on
Gaussian-distributed perturbed representations. This effectively alleviates the
over-fitting dilemma originating from pseudo-labeled augmented data. Extensive
experiments over six benchmarks demonstrate that our SeqUST framework
effectively improves the performance of self-training, and consistently
outperforms strong baselines by a large margin in low-resource scenarios
</p></li>
</ul>

<h3>Title: More Data Types More Problems: A Temporal Analysis of Complexity, Stability, and Sensitivity in Privacy Policies. (arXiv:2302.08936v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08936">http://arxiv.org/abs/2302.08936</a></li>
<li>Code URL: <a href="https://github.com/juniperlovato/privacypolicypaper">https://github.com/juniperlovato/privacypolicypaper</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08936] More Data Types More Problems: A Temporal Analysis of Complexity, Stability, and Sensitivity in Privacy Policies](http://arxiv.org/abs/2302.08936) #privacy</code></li>
<li>Summary: <p>Collecting personally identifiable information (PII) on data subjects has
become big business. Data brokers and data processors are part of a
multi-billion-dollar industry that profits from collecting, buying, and selling
consumer data. Yet there is little transparency in the data collection industry
which makes it difficult to understand what types of data are being collected,
used, and sold, and thus the risk to individual data subjects. In this study,
we examine a large textual dataset of privacy policies from 1997-2019 in order
to investigate the data collection activities of data brokers and data
processors. We also develop an original lexicon of PII-related terms
representing PII data types curated from legislative texts. This mesoscale
analysis looks at privacy policies overtime on the word, topic, and network
levels to understand the stability, complexity, and sensitivity of privacy
policies over time. We find that (1) privacy legislation correlates with
changes in stability and turbulence of PII data types in privacy policies; (2)
the complexity of privacy policies decreases over time and becomes more
regularized; (3) sensitivity rises over time and shows spikes that are
correlated with events when new privacy legislation is introduced.
</p></li>
</ul>

<h3>Title: A Review and a Taxonomy of Edge Machine Learning: Requirements, Paradigms, and Techniques. (arXiv:2302.08571v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08571">http://arxiv.org/abs/2302.08571</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08571] A Review and a Taxonomy of Edge Machine Learning: Requirements, Paradigms, and Techniques](http://arxiv.org/abs/2302.08571) #privacy</code></li>
<li>Summary: <p>The union of Edge Computing (EC) and Artificial Intelligence (AI) has brought
forward the Edge AI concept to provide intelligent solutions close to end-user
environment, for privacy preservation, low latency to real-time performance, as
well as resource optimization. Machine Learning (ML), as the most advanced
branch of AI in the past few years, has shown encouraging results and
applications in the edge environment. Nevertheless, edge powered ML solutions
are more complex to realize due to the joint constraints from both edge
computing and AI domains, and the corresponding solutions are expected to be
efficient and adapted in technologies such as data processing, model
compression, distributed inference, and advanced learning paradigms for Edge ML
requirements. Despite that a great attention of Edge ML is gained in both
academic and industrial communities, we noticed the lack of a complete survey
on existing Edge ML technologies to provide a common understanding of this
concept. To tackle this, this paper aims at providing a comprehensive taxonomy
and a systematic review of Edge ML techniques: we start by identifying the Edge
ML requirements driven by the joint constraints. We then survey more than
twenty paradigms and techniques along with their representative work, covering
two main parts: edge inference, and edge learning. In particular, we analyze
how each technique fits into Edge ML by meeting a subset of the identified
requirements. We also summarize Edge ML open issues to shed light on future
directions for Edge ML.
</p></li>
</ul>

<h3>Title: Metropolitan Segment Traffic Speeds from Massive Floating Car Data in 10 Cities. (arXiv:2302.08761v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08761">http://arxiv.org/abs/2302.08761</a></li>
<li>Code URL: <a href="https://github.com/iarai/mets-10">https://github.com/iarai/mets-10</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08761] Metropolitan Segment Traffic Speeds from Massive Floating Car Data in 10 Cities](http://arxiv.org/abs/2302.08761) #privacy</code></li>
<li>Summary: <p>Traffic analysis is crucial for urban operations and planning, while the
availability of dense urban traffic data beyond loop detectors is still scarce.
We present a large-scale floating vehicle dataset of per-street segment traffic
information, Metropolitan Segment Traffic Speeds from Massive Floating Car Data
in 10 Cities (MeTS-10), available for 10 global cities with a 15-minute
resolution for collection periods ranging between 108 and 361 days in 2019-2021
and covering more than 1500 square kilometers per metropolitan area. MeTS-10
features traffic speed information at all street levels from main arterials to
local streets for Antwerp, Bangkok, Barcelona, Berlin, Chicago, Istanbul,
London, Madrid, Melbourne and Moscow. The dataset leverages the
industrial-scale floating vehicle Traffic4cast data with speeds and vehicle
counts provided in a privacy-preserving spatio-temporal aggregation. We detail
the efficient matching approach mapping the data to the OpenStreetMap road
graph. We evaluate the dataset by comparing it with publicly available
stationary vehicle detector data (for Berlin, London, and Madrid) and the Uber
traffic speed dataset (for Barcelona, Berlin, and London). The comparison
highlights the differences across datasets in spatio-temporal coverage and
variations in the reported traffic caused by the binning method. MeTS-10
enables novel, city-wide analysis of mobility and traffic patterns for ten
major world cities, overcoming current limitations of spatially sparse vehicle
detector data. The large spatial and temporal coverage offers an opportunity
for joining the MeTS-10 with other datasets, such as traffic surveys in traffic
planning studies or vehicle detector data in traffic control settings.
</p></li>
</ul>

<h3>Title: Efficiently Forgetting What You Have Learned in Graph Representation Learning via Projection. (arXiv:2302.08990v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08990">http://arxiv.org/abs/2302.08990</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08990] Efficiently Forgetting What You Have Learned in Graph Representation Learning via Projection](http://arxiv.org/abs/2302.08990) #privacy</code></li>
<li>Summary: <p>As privacy protection receives much attention, unlearning the effect of a
specific node from a pre-trained graph learning model has become equally
important. However, due to the node dependency in the graph-structured data,
representation unlearning in Graph Neural Networks (GNNs) is challenging and
less well explored. In this paper, we fill in this gap by first studying the
unlearning problem in linear-GNNs, and then introducing its extension to
non-linear structures. Given a set of nodes to unlearn, we propose PROJECTOR
that unlearns by projecting the weight parameters of the pre-trained model onto
a subspace that is irrelevant to features of the nodes to be forgotten.
PROJECTOR could overcome the challenges caused by node dependency and enjoys a
perfect data removal, i.e., the unlearned model parameters do not contain any
information about the unlearned node features which is guaranteed by
algorithmic construction. Empirical results on real-world datasets illustrate
the effectiveness and efficiency of PROJECTOR.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Complex QA and language models hybrid architectures, Survey. (arXiv:2302.09051v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.09051">http://arxiv.org/abs/2302.09051</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.09051] Complex QA and language models hybrid architectures, Survey](http://arxiv.org/abs/2302.09051) #protect</code></li>
<li>Summary: <p>This paper provides a survey of the state of the art of hybrid language
models architectures and strategies for "complex" question-answering (QA, CQA,
CPS). Very large language models are good at leveraging public data on standard
problems but once you want to tackle more specific complex questions or
problems you may need specific architecture, knowledge, skills, tasks, methods,
sensitive data, performance, human approval and versatile feedback... This
survey extends findings from the robust community edited research papers BIG,
BLOOM and HELM which open source, benchmark and analyze limits and challenges
of large language models in terms of tasks complexity and strict evaluation on
accuracy (e.g. fairness, robustness, toxicity, ...). It identifies the key
elements used with Large Language Models (LLM) to solve complex questions or
problems. Recent projects like ChatGPT and GALACTICA have allowed
non-specialists to grasp the great potential as well as the equally strong
limitations of language models in complex QA. Hybridizing these models with
different components could allow to overcome these different limits and go much
further. We discuss some challenges associated with complex QA, including
domain adaptation, decomposition and efficient multi-step QA, long form QA,
non-factoid QA, safety and multi-sensitivity data protection, multimodal
search, hallucinations, QA explainability and truthfulness, time dimension.
Therefore we review current solutions and promising strategies, using elements
such as hybrid LLM architectures, human-in-the-loop reinforcement learning,
prompting adaptation, neuro-symbolic and structured knowledge grounding,
program synthesis, and others. We analyze existing solutions and provide an
overview of the current research and trends in the area of complex QA.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Defense Mechanisms Against Training-Hijacking Attacks in Split Learning. (arXiv:2302.08618v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08618">http://arxiv.org/abs/2302.08618</a></li>
<li>Code URL: <a href="https://github.com/ege-erdogan/splitguard">https://github.com/ege-erdogan/splitguard</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08618] Defense Mechanisms Against Training-Hijacking Attacks in Split Learning](http://arxiv.org/abs/2302.08618) #defense</code></li>
<li>Summary: <p>Distributed deep learning frameworks enable more efficient and privacy-aware
training of deep neural networks across multiple clients. Split learning
achieves this by splitting a neural network between a client and a server such
that the client computes the initial set of layers, and the server computes the
rest. However, this method introduces a unique attack vector for a malicious
server attempting to recover the client's private inputs: the server can direct
the client model towards learning any task of its choice, e.g. towards
outputting easily invertible values. With a concrete example already proposed
(Pasquini et al., ACM CCS '21), such \textit{training-hijacking} attacks
present a significant risk for the data privacy of split learning clients.
</p></li>
</ul>

<p>We propose two methods for a split learning client to detect if it is being
targeted by a training-hijacking attack or not. We experimentally evaluate our
methods' effectiveness, compare them with other potential solutions, and
discuss various points related to their use. Our conclusion is that by using
the method that best suits their use case, split learning clients can
consistently detect training-hijacking attacks and thus keep the information
gained by the attacker at a minimum.
</p>

<h3>Title: High-frequency Matters: An Overwriting Attack and defense for Image-processing Neural Network Watermarking. (arXiv:2302.08637v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08637">http://arxiv.org/abs/2302.08637</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08637] High-frequency Matters: An Overwriting Attack and defense for Image-processing Neural Network Watermarking](http://arxiv.org/abs/2302.08637) #defense</code></li>
<li>Summary: <p>In recent years, there has been significant advancement in the field of model
watermarking techniques. However, the protection of image-processing neural
networks remains a challenge, with only a limited number of methods being
developed. The objective of these techniques is to embed a watermark in the
output images of the target generative network, so that the watermark signal
can be detected in the output of a surrogate model obtained through model
extraction attacks. This promising technique, however, has certain limits.
Analysis of the frequency domain reveals that the watermark signal is mainly
concealed in the high-frequency components of the output. Thus, we propose an
overwriting attack that involves forging another watermark in the output of the
generative network. The experimental results demonstrate the efficacy of this
attack in sabotaging existing watermarking schemes for image-processing
networks, with an almost 100% success rate. To counter this attack, we devise
an adversarial framework for the watermarking network. The framework
incorporates a specially designed adversarial training step, where the
watermarking network is trained to defend against the overwriting network,
thereby enhancing its robustness. Additionally, we observe an overfitting
phenomenon in the existing watermarking method, which can render it
ineffective. To address this issue, we modify the training process to eliminate
the overfitting problem.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Adversarial Contrastive Distillation with Adaptive Denoising. (arXiv:2302.08764v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08764">http://arxiv.org/abs/2302.08764</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08764] Adversarial Contrastive Distillation with Adaptive Denoising](http://arxiv.org/abs/2302.08764) #attack</code></li>
<li>Summary: <p>Adversarial Robustness Distillation (ARD) is a novel method to boost the
robustness of small models. Unlike general adversarial training, its robust
knowledge transfer can be less easily restricted by the model capacity.
However, the teacher model that provides the robustness of knowledge does not
always make correct predictions, interfering with the student's robust
performances. Besides, in the previous ARD methods, the robustness comes
entirely from one-to-one imitation, ignoring the relationship between examples.
To this end, we propose a novel structured ARD method called Contrastive
Relationship DeNoise Distillation (CRDND). We design an adaptive compensation
module to model the instability of the teacher. Moreover, we utilize the
contrastive relationship to explore implicit robustness knowledge among
multiple examples. Experimental results on multiple attack benchmarks show
CRDND can transfer robust knowledge efficiently and achieves state-of-the-art
performances.
</p></li>
</ul>

<h3>Title: PACMAN Attack: A Mobility-Powered Attack in Private 5G-Enabled Industrial Automation System. (arXiv:2302.08563v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08563">http://arxiv.org/abs/2302.08563</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08563] PACMAN Attack: A Mobility-Powered Attack in Private 5G-Enabled Industrial Automation System](http://arxiv.org/abs/2302.08563) #attack</code></li>
<li>Summary: <p>3GPP has introduced Private 5G to support the next-generation industrial
automation system (IAS) due to the versatility and flexibility of 5G
architecture. Besides the 3.5GHz CBRS band, unlicensed spectrum bands, like
5GHz, are considered as an additional medium because of their free and abundant
nature. However, while utilizing the unlicensed band, industrial equipment must
coexist with incumbents, e.g., Wi-Fi, which could introduce new security
threats and resuscitate old ones. In this paper, we propose a novel attack
strategy conducted by a mobility-enabled malicious Wi-Fi access point (mmAP),
namely \textit{PACMAN} attack, to exploit vulnerabilities introduced by
heterogeneous coexistence. A mmAP is capable of moving around the physical
surface to identify mission-critical devices, hopping through the frequency
domain to detect the victim's operating channel, and launching traditional MAC
layer-based attacks. The multi-dimensional mobility of the attacker makes it
impervious to state-of-the-art detection techniques that assume static
adversaries. In addition, we propose a novel Markov Decision Process (MDP)
based framework to intelligently design an attacker's multi-dimensional
mobility in space and frequency. Mathematical analysis and extensive simulation
results exhibit the adverse effect of the proposed mobility-powered attack.
</p></li>
</ul>

<h3>Title: Beware of Pickpockets: A Practical Attack against Blocking Cards. (arXiv:2302.08992v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08992">http://arxiv.org/abs/2302.08992</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08992] Beware of Pickpockets: A Practical Attack against Blocking Cards](http://arxiv.org/abs/2302.08992) #attack</code></li>
<li>Summary: <p>Today, we rely on contactless smart cards to perform several critical
operations (e.g., payments and accessing buildings). Attacking smart cards can
have severe consequences, such as losing money or leaking sensitive
information. Although the security protections embedded in smart cards have
evolved over the years, those with weak security properties are still commonly
used. Among the different solutions, blocking cards are affordable devices to
protect smart cards. These devices are placed close to the smart cards,
generating a noisy jamming signal or shielding them. Whereas vendors claim the
reliability of their blocking cards, no previous study has ever focused on
evaluating their effectiveness. In this paper, we shed light on the security
threats on smart cards even in the presence of blocking cards, showing the
possibility of being bypassed by an attacker. We analyze blocking cards by
inspecting their emitted signal and assessing a vulnerability in their internal
design. We propose a novel attack that bypasses the jamming signal emitted by a
blocking card and reads the content of the smart card. We evaluate the
effectiveness of 14 blocking cards when protecting a MIFARE Ultralight smart
card and a MIFARE Classic card. We demonstrate that the protection of the 8
blocking cards among the 14 we evaluate can be successfully bypassed to dump
the content of the smart card. Based on this observation, we propose a
countermeasure that may lead to the design of effective blocking cards. To
assist further security improvement, the tool that we developed to inspect the
spectrum emitted by blocking cards and set up our attack is made available in
open source.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: AutoFed: Heterogeneity-Aware Federated Multimodal Learning for Robust Autonomous Driving. (arXiv:2302.08646v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08646">http://arxiv.org/abs/2302.08646</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08646] AutoFed: Heterogeneity-Aware Federated Multimodal Learning for Robust Autonomous Driving](http://arxiv.org/abs/2302.08646) #robust</code></li>
<li>Summary: <p>Object detection with on-board sensors (e.g., lidar, radar, and camera) play
a crucial role in autonomous driving (AD), and these sensors complement each
other in modalities. While crowdsensing may potentially exploit these sensors
(of huge quantity) to derive more comprehensive knowledge, \textit{federated
learning} (FL) appears to be the necessary tool to reach this potential: it
enables autonomous vehicles (AVs) to train machine learning models without
explicitly sharing raw sensory data. However, the multimodal sensors introduce
various data heterogeneity across distributed AVs (e.g., label quantity skews
and varied modalities), posing critical challenges to effective FL. To this
end, we present AutoFed as a heterogeneity-aware FL framework to fully exploit
multimodal sensory data on AVs and thus enable robust AD. Specifically, we
first propose a novel model leveraging pseudo-labeling to avoid mistakenly
treating unlabeled objects as the background. We also propose an
autoencoder-based data imputation method to fill missing data modality (of
certain AVs) with the available ones. To further reconcile the heterogeneity,
we finally present a client selection mechanism exploiting the similarities
among client models to improve both training stability and convergence rate.
Our experiments on benchmark dataset confirm that AutoFed substantially
improves over status quo approaches in both precision and recall, while
demonstrating strong robustness to adverse weather conditions.
</p></li>
</ul>

<h3>Title: Multimodal Subtask Graph Generation from Instructional Videos. (arXiv:2302.08672v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08672">http://arxiv.org/abs/2302.08672</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08672] Multimodal Subtask Graph Generation from Instructional Videos](http://arxiv.org/abs/2302.08672) #robust</code></li>
<li>Summary: <p>Real-world tasks consist of multiple inter-dependent subtasks (e.g., a dirty
pan needs to be washed before it can be used for cooking). In this work, we aim
to model the causal dependencies between such subtasks from instructional
videos describing the task. This is a challenging problem since complete
information about the world is often inaccessible from videos, which demands
robust learning mechanisms to understand the causal structure of events. We
present Multimodal Subtask Graph Generation (MSG2), an approach that constructs
a Subtask Graph defining the dependency between a task's subtasks relevant to a
task from noisy web videos. Graphs generated by our multimodal approach are
closer to human-annotated graphs compared to prior approaches. MSG2 further
performs the downstream task of next subtask prediction 85% and 30% more
accurately than recent video transformer models in the ProceL and CrossTask
datasets, respectively.
</p></li>
</ul>

<h3>Title: MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis from Sparse Inputs. (arXiv:2302.08788v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08788">http://arxiv.org/abs/2302.08788</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08788] MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis from Sparse Inputs](http://arxiv.org/abs/2302.08788) #robust</code></li>
<li>Summary: <p>Neural Radiance Field (NeRF) has broken new ground in the novel view
synthesis due to its simple concept and state-of-the-art quality. However, it
suffers from severe performance degradation unless trained with a dense set of
images with different camera poses, which hinders its practical applications.
Although previous methods addressing this problem achieved promising results,
they relied heavily on the additional training resources, which goes against
the philosophy of sparse-input novel-view synthesis pursuing the training
efficiency. In this work, we propose MixNeRF, an effective training strategy
for novel view synthesis from sparse inputs by modeling a ray with a mixture
density model. Our MixNeRF estimates the joint distribution of RGB colors along
the ray samples by modeling it with mixture of distributions. We also propose a
new task of ray depth estimation as a useful training objective, which is
highly correlated with 3D scene geometry. Moreover, we remodel the colors with
regenerated blending weights based on the estimated ray depth and further
improves the robustness for colors and viewpoints. Our MixNeRF outperforms
other state-of-the-art methods in various standard benchmarks with superior
efficiency of training and inference.
</p></li>
</ul>

<h3>Title: Self-supervised Action Representation Learning from Partial Spatio-Temporal Skeleton Sequences. (arXiv:2302.09018v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.09018">http://arxiv.org/abs/2302.09018</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.09018] Self-supervised Action Representation Learning from Partial Spatio-Temporal Skeleton Sequences](http://arxiv.org/abs/2302.09018) #robust</code></li>
<li>Summary: <p>Self-supervised learning has demonstrated remarkable capability in
representation learning for skeleton-based action recognition. Existing methods
mainly focus on applying global data augmentation to generate different views
of the skeleton sequence for contrastive learning. However, due to the rich
action clues in the skeleton sequences, existing methods may only take a global
perspective to learn to discriminate different skeletons without thoroughly
leveraging the local relationship between different skeleton joints and video
frames, which is essential for real-world applications. In this work, we
propose a Partial Spatio-Temporal Learning (PSTL) framework to exploit the
local relationship from a partial skeleton sequences built by a unique
spatio-temporal masking strategy. Specifically, we construct a
negative-sample-free triplet steam structure that is composed of an anchor
stream without any masking, a spatial masking stream with Central Spatial
Masking (CSM), and a temporal masking stream with Motion Attention Temporal
Masking (MATM). The feature cross-correlation matrix is measured between the
anchor stream and the other two masking streams, respectively. (1) Central
Spatial Masking discards selected joints from the feature calculation process,
where the joints with a higher degree of centrality have a higher possibility
of being selected. (2) Motion Attention Temporal Masking leverages the motion
of action and remove frames that move faster with a higher possibility. Our
method achieves state-of-the-art performance on NTURGB+D 60, NTURGB+D 120 and
PKU-MMD under various downstream tasks. Furthermore, a practical evaluation is
performed where some skeleton joints are lost in downstream tasks.In contrast
to previous methods that suffer from large performance drops, our PSTL can
still achieve remarkable results under this challenging setting, validating the
robustness of our method.
</p></li>
</ul>

<h3>Title: Robust expected improvement for Bayesian optimization. (arXiv:2302.08612v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08612">http://arxiv.org/abs/2302.08612</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08612] Robust expected improvement for Bayesian optimization](http://arxiv.org/abs/2302.08612) #robust</code></li>
<li>Summary: <p>Bayesian Optimization (BO) links Gaussian Process (GP) surrogates with
sequential design toward optimizing expensive-to-evaluate black-box functions.
Example design heuristics, or so-called acquisition functions, like expected
improvement (EI), balance exploration and exploitation to furnish global
solutions under stringent evaluation budgets. However, they fall short when
solving for robust optima, meaning a preference for solutions in a wider domain
of attraction. Robust solutions are useful when inputs are imprecisely
specified, or where a series of solutions is desired. A common mathematical
programming technique in such settings involves an adversarial objective,
biasing a local solver away from ``sharp'' troughs. Here we propose a surrogate
modeling and active learning technique called robust expected improvement (REI)
that ports adversarial methodology into the BO/GP framework. After describing
the methods, we illustrate and draw comparisons to several competitors on
benchmark synthetic and real problems of varying complexity.
</p></li>
</ul>

<h3>Title: Generative Causal Representation Learning for Out-of-Distribution Motion Forecasting. (arXiv:2302.08635v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08635">http://arxiv.org/abs/2302.08635</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08635] Generative Causal Representation Learning for Out-of-Distribution Motion Forecasting](http://arxiv.org/abs/2302.08635) #robust</code></li>
<li>Summary: <p>Conventional supervised learning methods typically assume i.i.d samples and
are found to be sensitive to out-of-distribution (OOD) data. We propose
Generative Causal Representation Learning (GCRL) which leverages causality to
facilitate knowledge transfer under distribution shifts. While we evaluate the
effectiveness of our proposed method in human trajectory prediction models,
GCRL can be applied to other domains as well. First, we propose a novel causal
model that explains the generative factors in motion forecasting datasets using
features that are common across all environments and with features that are
specific to each environment. Selection variables are used to determine which
parts of the model can be directly transferred to a new environment without
fine-tuning. Second, we propose an end-to-end variational learning paradigm to
learn the causal mechanisms that generate observations from features. GCRL is
supported by strong theoretical results that imply identifiability of the
causal model under certain assumptions. Experimental results on synthetic and
real-world motion forecasting datasets show the robustness and effectiveness of
our proposed method for knowledge transfer under zero-shot and low-shot
settings by substantially outperforming the prior motion forecasting models on
out-of-distribution prediction.
</p></li>
</ul>

<h3>Title: Quantile LSTM: A Robust LSTM for Anomaly Detection In Time Series Data. (arXiv:2302.08712v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08712">http://arxiv.org/abs/2302.08712</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08712] Quantile LSTM: A Robust LSTM for Anomaly Detection In Time Series Data](http://arxiv.org/abs/2302.08712) #robust</code></li>
<li>Summary: <p>Anomalies refer to the departure of systems and devices from their normal
behaviour in standard operating conditions. An anomaly in an industrial device
can indicate an upcoming failure, often in the temporal direction. In this
paper, we make two contributions: 1) we estimate conditional quantiles and
consider three different ways to define anomalies based on the estimated
quantiles. 2) we use a new learnable activation function in the popular Long
Short Term Memory networks (LSTM) architecture to model temporal long-range
dependency. In particular, we propose Parametric Elliot Function (PEF) as an
activation function (AF) inside LSTM, which saturates lately compared to
sigmoid and tanh. The proposed algorithms are compared with other well-known
anomaly detection algorithms, such as Isolation Forest (iForest), Elliptic
Envelope, Autoencoder, and modern Deep Learning models such as Deep
Autoencoding Gaussian Mixture Model (DAGMM), Generative Adversarial Networks
(GAN). The algorithms are evaluated in terms of various performance metrics,
such as Precision and Recall. The algorithms have been tested on multiple
industrial time-series datasets such as Yahoo, AWS, GE, and machine sensors. We
have found that the LSTM-based quantile algorithms are very effective and
outperformed the existing algorithms in identifying anomalies.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: OTB-morph: One-Time Biometrics via Morphing. (arXiv:2302.09053v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.09053">http://arxiv.org/abs/2302.09053</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.09053] OTB-morph: One-Time Biometrics via Morphing](http://arxiv.org/abs/2302.09053) #biometric</code></li>
<li>Summary: <p>Cancelable biometrics are a group of techniques to transform the input
biometric to an irreversible feature intentionally using a transformation
function and usually a key in order to provide security and privacy in
biometric recognition systems. This transformation is repeatable enabling
subsequent biometric comparisons. This paper is introducing a new idea to
exploit as a transformation function for cancelable biometrics aimed at
protecting the templates against iterative optimization attacks. Our proposed
scheme is based on time-varying keys (random biometrics in our case) and
morphing transformations. An experimental implementation of the proposed scheme
is given for face biometrics. The results confirm that the proposed approach is
able to withstand against leakage attacks while improving the recognition
performance.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Dynamic Spatial-temporal Hypergraph Convolutional Network for Skeleton-based Action Recognition. (arXiv:2302.08689v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08689">http://arxiv.org/abs/2302.08689</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08689] Dynamic Spatial-temporal Hypergraph Convolutional Network for Skeleton-based Action Recognition](http://arxiv.org/abs/2302.08689) #extraction</code></li>
<li>Summary: <p>Skeleton-based action recognition relies on the extraction of
spatial-temporal topological information. Hypergraphs can establish prior
unnatural dependencies for the skeleton. However, the existing methods only
focus on the construction of spatial topology and ignore the time-point
dependence. This paper proposes a dynamic spatial-temporal hypergraph
convolutional network (DST-HCN) to capture spatial-temporal information for
skeleton-based action recognition. DST-HCN introduces a time-point hypergraph
(TPH) to learn relationships at time points. With multiple spatial static
hypergraphs and dynamic TPH, our network can learn more complete
spatial-temporal features. In addition, we use the high-order information
fusion module (HIF) to fuse spatial-temporal information synchronously.
Extensive experiments on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets show
that our model achieves state-of-the-art, especially compared with hypergraph
methods.
</p></li>
</ul>

<h3>Title: InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08624">http://arxiv.org/abs/2302.08624</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08624] InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis](http://arxiv.org/abs/2302.08624) #extraction</code></li>
<li>Summary: <p>In this paper, we present InstructABSA, Aspect-Based Sentiment Analysis
(ABSA) using instruction learning paradigm for all ABSA subtasks: Aspect Term
Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint Task
modeling. Our method introduces positive, negative, and neutral examples to
each training sample, and instruction tunes the model (Tk-Instruct Base) for
each ABSA subtask, yielding significant performance improvements. Experimental
results on the Sem Eval 2014 dataset demonstrate that InstructABSA outperforms
the previous state-of-the-art (SOTA) approaches on all three ABSA subtasks
(ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x larger
models. In particular, InstructABSA surpasses the SOTA on the restaurant ATE
subtask by 7.31% points and on the Laptop Joint Task by 8.63% points. Our
results also suggest a strong generalization ability to unseen tasks across all
three subtasks.
</p></li>
</ul>

<h3>Title: DREEAM: Guiding Attention with Evidence for Improving Document-Level Relation Extraction. (arXiv:2302.08675v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08675">http://arxiv.org/abs/2302.08675</a></li>
<li>Code URL: <a href="https://github.com/youmima/dreeam">https://github.com/youmima/dreeam</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08675] DREEAM: Guiding Attention with Evidence for Improving Document-Level Relation Extraction](http://arxiv.org/abs/2302.08675) #extraction</code></li>
<li>Summary: <p>Document-level relation extraction (DocRE) is the task of identifying all
relations between each entity pair in a document. Evidence, defined as
sentences containing clues for the relationship between an entity pair, has
been shown to help DocRE systems focus on relevant texts, thus improving
relation extraction. However, evidence retrieval (ER) in DocRE faces two major
issues: high memory consumption and limited availability of annotations. This
work aims at addressing these issues to improve the usage of ER in DocRE.
First, we propose DREEAM, a memory-efficient approach that adopts evidence
information as the supervisory signal, thereby guiding the attention modules of
the DocRE system to assign high weights to evidence. Second, we propose a
self-training strategy for DREEAM to learn ER from automatically-generated
evidence on massive data without evidence annotations. Experimental results
reveal that our approach exhibits state-of-the-art performance on the DocRED
benchmark for both DocRE and ER. To the best of our knowledge, DREEAM is the
first approach to employ ER self-training.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Learning as a Network Effects Game. (arXiv:2302.08533v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08533">http://arxiv.org/abs/2302.08533</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08533] Federated Learning as a Network Effects Game](http://arxiv.org/abs/2302.08533) #federate</code></li>
<li>Summary: <p>Federated Learning (FL) aims to foster collaboration among a population of
clients to improve the accuracy of machine learning without directly sharing
local data. Although there has been rich literature on designing federated
learning algorithms, most prior works implicitly assume that all clients are
willing to participate in a FL scheme. In practice, clients may not benefit
from joining in FL, especially in light of potential costs related to issues
such as privacy and computation. In this work, we study the clients' incentives
in federated learning to help the service provider design better solutions and
ensure clients make better decisions. We are the first to model clients'
behaviors in FL as a network effects game, where each client's benefit depends
on other clients who also join the network. Using this setup we analyze the
dynamics of clients' participation and characterize the equilibrium, where no
client has incentives to alter their decision. Specifically, we show that
dynamics in the population naturally converge to equilibrium without needing
explicit interventions. Finally, we provide a cost-efficient payment scheme
that incentivizes clients to reach a desired equilibrium when the initial
network is empty.
</p></li>
</ul>

<h3>Title: Online Spatio-Temporal Correlation-Based Federated Learning for Traffic Flow Forecasting. (arXiv:2302.08658v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08658">http://arxiv.org/abs/2302.08658</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08658] Online Spatio-Temporal Correlation-Based Federated Learning for Traffic Flow Forecasting](http://arxiv.org/abs/2302.08658) #federate</code></li>
<li>Summary: <p>Traffic flow forecasting (TFF) is of great importance to the construction of
Intelligent Transportation Systems (ITS). To mitigate communication burden and
tackle with the problem of privacy leakage aroused by centralized forecasting
methods, Federated Learning (FL) has been applied to TFF. However, existing
FL-based approaches employ batch learning manner, which makes the pre-trained
models inapplicable to subsequent traffic data, thus exhibiting subpar
prediction performance. In this paper, we perform the first study of
forecasting traffic flow adopting Online Learning (OL) manner in FL framework
and then propose a novel prediction method named Online Spatio-Temporal
Correlation-based Federated Learning (FedOSTC), aiming to guarantee performance
gains regardless of traffic fluctuation. Specifically, clients employ Gated
Recurrent Unit (GRU)-based encoders to obtain the internal temporal patterns
inside traffic data sequences. Then, the central server evaluates spatial
correlation among clients via Graph Attention Network (GAT), catering to the
dynamic changes of spatial closeness caused by traffic fluctuation.
Furthermore, to improve the generalization of the global model for upcoming
traffic data, a period-aware aggregation mechanism is proposed to aggregate the
local models which are optimized using Online Gradient Descent (OGD) algorithm
at clients. We perform comprehensive experiments on two real-world datasets to
validate the efficiency and effectiveness of our proposed method and the
numerical results demonstrate the superiority of FedOSTC.
</p></li>
</ul>

<h3>Title: Efficient Classification of SARS-CoV-2 Spike Sequences Using Federated Learning. (arXiv:2302.08688v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08688">http://arxiv.org/abs/2302.08688</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08688] Efficient Classification of SARS-CoV-2 Spike Sequences Using Federated Learning](http://arxiv.org/abs/2302.08688) #federate</code></li>
<li>Summary: <p>This paper presents a federated learning (FL) approach to train an AI model
for SARS-Cov-2 coronavirus variant identification. We analyze the SARS-CoV-2
spike sequences in a distributed way, without data sharing, to detect different
variants of the rapidly mutating coronavirus. A vast amount of sequencing data
of SARS-CoV-2 is available due to various genomic monitoring initiatives by
several nations. However, privacy concerns involving patient health information
and national public health conditions could hinder openly sharing this data. In
this work, we propose a lightweight FL paradigm to cooperatively analyze the
spike protein sequences of SARS-CoV-2 privately, using the locally stored data
to train a prediction model from remote nodes. Our method maintains the
confidentiality of local data (that could be stored in different locations) yet
allows us to reliably detect and identify different known and unknown variants
of the novel coronavirus SARS-CoV-2. We compare the performance of our approach
on spike sequence data with the recently proposed state-of-the-art methods for
classification from spike sequences. Using the proposed approach, we achieve an
overall accuracy of $93\%$ on the coronavirus variant identification task. To
the best of our knowledge, this is the first work in the federated learning
paradigm for biological sequence analysis. Since the proposed model is
distributed in nature, it could scale on ``Big Data'' easily. We plan to use
this proof-of-concept to implement a privacy-preserving pandemic response
strategy.
</p></li>
</ul>

<h3>Title: Multimodal Federated Learning via Contrastive Representation Ensemble. (arXiv:2302.08888v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08888">http://arxiv.org/abs/2302.08888</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08888] Multimodal Federated Learning via Contrastive Representation Ensemble](http://arxiv.org/abs/2302.08888) #federate</code></li>
<li>Summary: <p>With the increasing amount of multimedia data on modern mobile systems and
IoT infrastructures, harnessing these rich multimodal data without breaching
user privacy becomes a critical issue. Federated learning (FL) serves as a
privacy-conscious alternative to centralized machine learning. However,
existing FL methods extended to multimodal data all rely on model aggregation
on single modality level, which restrains the server and clients to have
identical model architecture for each modality. This limits the global model in
terms of both model complexity and data capacity, not to mention task
diversity. In this work, we propose Contrastive Representation Ensemble and
Aggregation for Multimodal FL (CreamFL), a multimodal federated learning
framework that enables training larger server models from clients with
heterogeneous model architectures and data modalities, while only communicating
knowledge on public dataset. To achieve better multimodal representation
fusion, we design a global-local cross-modal ensemble strategy to aggregate
client representations. To mitigate local model drift caused by two
unprecedented heterogeneous factors stemming from multimodal discrepancy
(modality gap and task gap), we further propose two inter-modal and intra-modal
contrasts to regularize local training, which complements information of the
absent modality for uni-modal clients and regularizes local clients to head
towards global consensus. Thorough evaluations and ablation studies on
image-text retrieval and visual question answering tasks showcase the
superiority of CreamFL over state-of-the-art FL methods and its practical
value.
</p></li>
</ul>

<h3>Title: Welfare and Fairness Dynamics in Federated Learning: A Client Selection Perspective. (arXiv:2302.08976v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08976">http://arxiv.org/abs/2302.08976</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08976] Welfare and Fairness Dynamics in Federated Learning: A Client Selection Perspective](http://arxiv.org/abs/2302.08976) #federate</code></li>
<li>Summary: <p>Federated learning (FL) is a privacy-preserving learning technique that
enables distributed computing devices to train shared learning models across
data silos collaboratively. Existing FL works mostly focus on designing
advanced FL algorithms to improve the model performance. However, the economic
considerations of the clients, such as fairness and incentive, are yet to be
fully explored. Without such considerations, self-motivated clients may lose
interest and leave the federation. To address this problem, we designed a novel
incentive mechanism that involves a client selection process to remove
low-quality clients and a money transfer process to ensure a fair reward
distribution. Our experimental results strongly demonstrate that the proposed
incentive mechanism can effectively improve the duration and fairness of the
federation.
</p></li>
</ul>

<h3>Title: Privately Customizing Prefinetuning to Better Match User Data in Federated Learning. (arXiv:2302.09042v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.09042">http://arxiv.org/abs/2302.09042</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.09042] Privately Customizing Prefinetuning to Better Match User Data in Federated Learning](http://arxiv.org/abs/2302.09042) #federate</code></li>
<li>Summary: <p>In Federated Learning (FL), accessing private client data incurs
communication and privacy costs. As a result, FL deployments commonly
prefinetune pretrained foundation models on a (large, possibly public) dataset
that is held by the central server; they then FL-finetune the model on a
private, federated dataset held by clients. Evaluating prefinetuning dataset
quality reliably and privately is therefore of high importance. To this end, we
propose FreD (Federated Private Fr\'echet Distance) -- a privately computed
distance between a prefinetuning dataset and federated datasets. Intuitively,
it privately computes and compares a Fr\'echet distance between embeddings
generated by a large language model on both the central (public) dataset and
the federated private client data. To make this computation privacy-preserving,
we use distributed, differentially-private mean and covariance estimators. We
show empirically that FreD accurately predicts the best prefinetuning dataset
at minimal privacy cost. Altogether, using FreD we demonstrate a
proof-of-concept for a new approach in private FL training: (1) customize a
prefinetuning dataset to better match user data (2) prefinetune (3) perform
FL-finetuning.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Towards Reliable Assessments of Demographic Disparities in Multi-Label Image Classifiers. (arXiv:2302.08572v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08572">http://arxiv.org/abs/2302.08572</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08572] Towards Reliable Assessments of Demographic Disparities in Multi-Label Image Classifiers](http://arxiv.org/abs/2302.08572) #fair</code></li>
<li>Summary: <p>Disaggregated performance metrics across demographic groups are a hallmark of
fairness assessments in computer vision. These metrics successfully
incentivized performance improvements on person-centric tasks such as face
analysis and are used to understand risks of modern models. However, there is a
lack of discussion on the vulnerabilities of these measurements for more
complex computer vision tasks. In this paper, we consider multi-label image
classification and, specifically, object categorization tasks. First, we
highlight design choices and trade-offs for measurement that involve more
nuance than discussed in prior computer vision literature. These challenges are
related to the necessary scale of data, definition of groups for images, choice
of metric, and dataset imbalances. Next, through two case studies using modern
vision models, we demonstrate that naive implementations of these assessments
are brittle. We identify several design choices that look merely like
implementation details but significantly impact the conclusions of assessments,
both in terms of magnitude and direction (on which group the classifiers work
best) of disparities. Based on ablation studies, we propose some
recommendations to increase the reliability of these assessments. Finally,
through a qualitative analysis we find that concepts with large disparities
tend to have varying definitions and representations between groups, with
inconsistencies across datasets and annotators. While this result suggests
avenues for mitigation through more consistent data collection, it also
highlights that ambiguous label definitions remain a challenge when performing
model assessments. Vision models are expanding and becoming more ubiquitous; it
is even more important that our disparity assessments accurately reflect the
true performance of models.
</p></li>
</ul>

<h3>Title: Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales. (arXiv:2302.08961v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08961">http://arxiv.org/abs/2302.08961</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08961] Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales](http://arxiv.org/abs/2302.08961) #fair</code></li>
<li>Summary: <p>The quality of text-to-image generation is continuously improving, yet the
boundaries of its applicability are still unclear. In particular, refinement of
the text input with the objective of achieving better results - commonly called
prompt engineering - so far seems to have not been geared towards work with
pre-existing texts. We investigate whether text-to-image generation and prompt
engineering could be used to generate basic illustrations of popular
fairytales. Using Midjourney v4, we engage in action research with a dual aim:
to attempt to generate 5 believable illustrations for each of 5 popular
fairytales, and to define a prompt engineering process that starts from a
pre-existing text and arrives at an illustration of it. We arrive at a
tentative 4-stage process: i) initial prompt, ii) composition adjustment, iii)
style refinement, and iv) variation selection. We also discuss three reasons
why the generation model struggles with certain illustrations: difficulties
with counts, bias from stereotypical configurations and inability to depict
overly fantastic situations. Our findings are not limited to the specific
generation model and are intended to be generalisable to future ones.
</p></li>
</ul>

<h3>Title: The Unbearable Weight of Massive Privilege: Revisiting Bias-Variance Trade-Offs in the Context of Fair Prediction. (arXiv:2302.08704v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08704">http://arxiv.org/abs/2302.08704</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08704] The Unbearable Weight of Massive Privilege: Revisiting Bias-Variance Trade-Offs in the Context of Fair Prediction](http://arxiv.org/abs/2302.08704) #fair</code></li>
<li>Summary: <p>In this paper we revisit the bias-variance decomposition of model error from
the perspective of designing a fair classifier: we are motivated by the widely
held socio-technical belief that noise variance in large datasets in social
domains tracks demographic characteristics such as gender, race, disability,
etc. We propose a conditional-iid (ciid) model built from group-specific
classifiers that seeks to improve on the trade-offs made by a single model (iid
setting). We theoretically analyze the bias-variance decomposition of different
models in the Gaussian Mixture Model, and then empirically test our setup on
the COMPAS and folktables datasets. We instantiate the ciid model with two
procedures that improve "fairness" by conditioning out undesirable effects:
first, by conditioning directly on sensitive attributes, and second, by
clustering samples into groups and conditioning on cluster membership (blind to
protected group membership).
</p></li>
</ul>

<p>Our analysis suggests that there might be principled procedures and concrete
real-world use cases under which conditional models are preferred, and our
striking empirical results strongly indicate that non-iid settings, such as the
ciid setting proposed here, might be more suitable for big data applications in
social contexts.
</p>

<h3>Title: On (assessing) the fairness of risk score models. (arXiv:2302.08851v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08851">http://arxiv.org/abs/2302.08851</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08851] On (assessing) the fairness of risk score models](http://arxiv.org/abs/2302.08851) #fair</code></li>
<li>Summary: <p>Recent work on algorithmic fairness has largely focused on the fairness of
discrete decisions, or classifications. While such decisions are often based on
risk score models, the fairness of the risk models themselves has received
considerably less attention. Risk models are of interest for a number of
reasons, including the fact that they communicate uncertainty about the
potential outcomes to users, thus representing a way to enable meaningful human
oversight. Here, we address fairness desiderata for risk score models. We
identify the provision of similar epistemic value to different groups as a key
desideratum for risk score fairness. Further, we address how to assess the
fairness of risk score models quantitatively, including a discussion of metric
choices and meaningful statistical comparisons between groups. In this context,
we also introduce a novel calibration error metric that is less sample
size-biased than previously proposed metrics, enabling meaningful comparisons
between groups of different sizes. We illustrate our methodology - which is
widely applicable in many other settings - in two case studies, one in
recidivism risk prediction, and one in risk of major depressive disorder (MDD)
prediction.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: A Probabilistic Generative Model for Tracking Multi-Knowledge Concept Mastery Probability. (arXiv:2302.08673v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08673">http://arxiv.org/abs/2302.08673</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08673] A Probabilistic Generative Model for Tracking Multi-Knowledge Concept Mastery Probability](http://arxiv.org/abs/2302.08673) #interpretability</code></li>
<li>Summary: <p>Knowledge tracing aims to track students' knowledge status over time to
predict students' future performance accurately. Markov chain-based knowledge
tracking (MCKT) models can track knowledge concept mastery probability over
time. However, as the number of tracked knowledge concepts increases, the time
complexity of MCKT predicting student performance increases exponentially (also
called explaining away problem. In addition, the existing MCKT models only
consider the relationship between students' knowledge status and problems when
modeling students' responses but ignore the relationship between knowledge
concepts in the same problem. To address these challenges, we propose an
inTerpretable pRobAbilistiC gEnerative moDel (TRACED), which can track
students' numerous knowledge concepts mastery probabilities over time. To solve
\emph{explain away problem}, we design Long and Short-Term Memory (LSTM)-based
networks to approximate the posterior distribution, predict students' future
performance, and propose a heuristic algorithm to train LSTMs and probabilistic
graphical model jointly. To better model students' exercise responses, we
proposed a logarithmic linear model with three interactive strategies, which
models students' exercise responses by considering the relationship among
students' knowledge status, knowledge concept, and problems. We conduct
experiments with four real-world datasets in three knowledge-driven tasks. The
experimental results show that TRACED outperforms existing knowledge tracing
methods in predicting students' future performance and can learn the
relationship among students, knowledge concepts, and problems from students'
exercise sequences. We also conduct several case studies. The case studies show
that TRACED exhibits excellent interpretability and thus has the potential for
personalized automatic feedback in the real-world educational environment.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Less is More: The Influence of Pruning on the Explainability of CNNs. (arXiv:2302.08878v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08878">http://arxiv.org/abs/2302.08878</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08878] Less is More: The Influence of Pruning on the Explainability of CNNs](http://arxiv.org/abs/2302.08878) #explainability</code></li>
<li>Summary: <p>Modern, state-of-the-art Convolutional Neural Networks (CNNs) in computer
vision have millions of parameters. Thus, explaining the complex decisions of
such networks to humans is challenging. A technical approach to reduce CNN
complexity is network pruning, where less important parameters are deleted. The
work presented in this paper investigates whether this technical complexity
reduction also helps with perceived explainability. To do so, we conducted a
pre-study and two human-grounded experiments, assessing the effects of
different pruning ratios on CNN explainability. Overall, we evaluated four
different compression rates (i.e., CPR 2, 4, 8, and 32) with 37 500 tasks on
Mechanical Turk. Results indicate that lower compression rates have a positive
influence on explainability, while higher compression rates show negative
effects. Furthermore, we were able to identify sweet spots that increase both
the perceived explainability and the model's performance.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation. (arXiv:2302.08908v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08908">http://arxiv.org/abs/2302.08908</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08908] LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation](http://arxiv.org/abs/2302.08908) #diffusion</code></li>
<li>Summary: <p>Layout-to-image generation refers to the task of synthesizing photo-realistic
images based on semantic layouts. In this paper, we propose LayoutDiffuse that
adapts a foundational diffusion model pretrained on large-scale image or
text-image datasets for layout-to-image generation. By adopting a novel neural
adaptor based on layout attention and task-aware prompts, our method trains
efficiently, generates images with both high perceptual quality and layout
alignment, and needs less data. Experiments on three datasets show that our
method significantly outperforms other 10 generative models based on GANs,
VQ-VAE, and diffusion models.
</p></li>
</ul>

<h3>Title: LDFA: Latent Diffusion Face Anonymization for Self-driving Applications. (arXiv:2302.08931v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.08931">http://arxiv.org/abs/2302.08931</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.08931] LDFA: Latent Diffusion Face Anonymization for Self-driving Applications](http://arxiv.org/abs/2302.08931) #diffusion</code></li>
<li>Summary: <p>In order to protect vulnerable road users (VRUs), such as pedestrians or
cyclists, it is essential that intelligent transportation systems (ITS)
accurately identify them. Therefore, datasets used to train perception models
of ITS must contain a significant number of vulnerable road users. However,
data protection regulations require that individuals are anonymized in such
datasets. In this work, we introduce a novel deep learning-based pipeline for
face anonymization in the context of ITS. In contrast to related methods, we do
not use generative adversarial networks (GANs) but build upon recent advances
in diffusion models. We propose a two-stage method, which contains a face
detection model followed by a latent diffusion model to generate realistic face
in-paintings. To demonstrate the versatility of anonymized images, we train
segmentation methods on anonymized data and evaluate them on non-anonymized
data. Our experiment reveal that our pipeline is better suited to anonymize
data for segmentation than naive methods and performes comparably with recent
GAN-based methods. Moreover, face detectors achieve higher mAP scores for faces
anonymized by our method compared to naive or recent GAN-based methods.
</p></li>
</ul>

<h3>Title: Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent. (arXiv:2302.09057v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.09057">http://arxiv.org/abs/2302.09057</a></li>
<li>Code URL: <a href="https://github.com/giannisdaras/cdm">https://github.com/giannisdaras/cdm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.09057] Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent](http://arxiv.org/abs/2302.09057) #diffusion</code></li>
<li>Summary: <p>Imperfect score-matching leads to a shift between the training and the
sampling distribution of diffusion models. Due to the recursive nature of the
generation process, errors in previous steps yield sampling iterates that drift
away from the training distribution. Yet, the standard training objective via
Denoising Score Matching (DSM) is only designed to optimize over non-drifted
data. To train on drifted data, we propose to enforce a \emph{consistency}
property which states that predictions of the model on its own generated data
are consistent across time. Theoretically, we show that if the score is learned
perfectly on some non-drifted points (via DSM) and if the consistency property
is enforced everywhere, then the score is learned accurately everywhere.
Empirically we show that our novel training objective yields state-of-the-art
results for conditional and unconditional generation in CIFAR-10 and baseline
improvements in AFHQ and FFHQ. We open-source our code and models:
https://github.com/giannisdaras/cdm
</p></li>
</ul>

<h3>Title: MiDi: Mixed Graph and 3D Denoising Diffusion for Molecule Generation. (arXiv:2302.09048v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.09048">http://arxiv.org/abs/2302.09048</a></li>
<li>Code URL: <a href="https://github.com/cvignac/midi">https://github.com/cvignac/midi</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.09048] MiDi: Mixed Graph and 3D Denoising Diffusion for Molecule Generation](http://arxiv.org/abs/2302.09048) #diffusion</code></li>
<li>Summary: <p>This work introduces MiDi, a diffusion model for jointly generating molecular
graphs and corresponding 3D conformers. In contrast to existing models, which
derive molecular bonds from the conformation using predefined rules, MiDi
streamlines the molecule generation process with an end-to-end differentiable
model. Experimental results demonstrate the benefits of this approach: on the
complex GEOM-DRUGS dataset, our model generates significantly better molecular
graphs than 3D-based models and even surpasses specialized algorithms that
directly optimize the bond orders for validity. Our code is available at
github.com/cvignac/MiDi.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
