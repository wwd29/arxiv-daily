<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-02-20</h1>
<h3>Title: ShieldLearner: A New Paradigm for Jailbreak Attack Defense in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Ni, Hao Wang, Huacan Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13162">https://arxiv.org/abs/2502.13162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13162">https://arxiv.org/pdf/2502.13162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13162]] ShieldLearner: A New Paradigm for Jailbreak Attack Defense in LLMs(https://arxiv.org/abs/2502.13162)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable success in various domains but remain vulnerable to adversarial jailbreak attacks. Existing prompt-defense strategies, including parameter-modifying and parameter-free approaches, face limitations in adaptability, interpretability, and customization, constraining their effectiveness against evolving threats. To address these challenges, we propose ShieldLearner, a novel paradigm that mimics human learning in defense. Through trial and error, it autonomously distills attack signatures into a Pattern Atlas and synthesizes defense heuristics into a Meta-analysis Framework, enabling systematic and interpretable threat detection. Furthermore, we introduce Adaptive Adversarial Augmentation to generate adversarial variations of successfully defended prompts, enabling continuous self-improvement without model retraining. In addition to standard benchmarks, we create a hard test set by curating adversarial prompts from the Wildjailbreak dataset, emphasizing more concealed malicious intent. Experimental results show that ShieldLearner achieves a significantly higher defense success rate than existing baselines on both conventional and hard test sets, while also operating with lower computational overhead, making it a practical and efficient solution for real-world adversarial defense.</li>
</ul>

<h3>Title: SmartLLM: Smart Contract Auditing using Custom Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Jun Kevin, Pujianto Yugopuspito</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13167">https://arxiv.org/abs/2502.13167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13167">https://arxiv.org/pdf/2502.13167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13167]] SmartLLM: Smart Contract Auditing using Custom Generative AI(https://arxiv.org/abs/2502.13167)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Smart contracts are essential to decentralized finance (DeFi) and blockchain ecosystems but are increasingly vulnerable to exploits due to coding errors and complex attack vectors. Traditional static analysis tools and existing vulnerability detection methods often fail to address these challenges comprehensively, leading to high false-positive rates and an inability to detect dynamic vulnerabilities. This paper introduces SmartLLM, a novel approach leveraging fine-tuned LLaMA 3.1 models with Retrieval-Augmented Generation (RAG) to enhance the accuracy and efficiency of smart contract auditing. By integrating domain-specific knowledge from ERC standards and employing advanced techniques such as QLoRA for efficient fine-tuning, SmartLLM achieves superior performance compared to static analysis tools like Mythril and Slither, as well as zero-shot large language model (LLM) prompting methods such as GPT-3.5 and GPT-4. Experimental results demonstrate a perfect recall of 100% and an accuracy score of 70%, highlighting the model's robustness in identifying vulnerabilities, including reentrancy and access control issues. This research advances smart contract security by offering a scalable and effective auditing solution, supporting the secure adoption of decentralized applications.</li>
</ul>

<h3>Title: Web Phishing Net (WPN): A scalable machine learning approach for real-time phishing campaign detection</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Fahad Zia, Sri Harish Kalidass</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13171">https://arxiv.org/abs/2502.13171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13171">https://arxiv.org/pdf/2502.13171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13171]] Web Phishing Net (WPN): A scalable machine learning approach for real-time phishing campaign detection(https://arxiv.org/abs/2502.13171)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, generative</a></li>
<li><strong>Abstract: </strong>Phishing is the most prevalent type of cyber-attack today and is recognized as the leading source of data breaches with significant consequences for both individuals and corporations. Web-based phishing attacks are the most frequent with vectors such as social media posts and emails containing links to phishing URLs that once clicked on render host systems vulnerable to more sinister attacks. Research efforts to detect phishing URLs have involved the use of supervised learning techniques that use large amounts of data to train models and have high computational requirements. They also involve analysis of features derived from vectors including email contents thus affecting user privacy. Additionally, they suffer from a lack of resilience against evolution of threats especially with the advent of generative AI techniques to bypass these systems as with AI-generated phishing URLs. Unsupervised methods such as clustering techniques have also been used in phishing detection in the past, however, they are at times unscalable due to the use of pair-wise comparisons. They also lack high detection rates while detecting phishing campaigns. In this paper, we propose an unsupervised learning approach that is not only fast but scalable, as it does not involve pair-wise comparisons. It is able to detect entire campaigns at a time with a high detection rate while preserving user privacy; this includes the recent surge of campaigns with targeted phishing URLs generated by malicious entities using generative AI techniques.</li>
</ul>

<h3>Title: Unveiling Privacy Risks in LLM Agent Memory</h3>
<ul>
<li><strong>Authors: </strong>Bo Wang, Weiyi He, Pengfei He, Shenglai Zeng, Zhen Xiang, Yue Xing, Jiliang Tang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13172">https://arxiv.org/abs/2502.13172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13172">https://arxiv.org/pdf/2502.13172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13172]] Unveiling Privacy Risks in LLM Agent Memory(https://arxiv.org/abs/2502.13172)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) agents have become increasingly prevalent across various real-world applications. They enhance decision-making by storing private user-agent interactions in the memory module for demonstrations, introducing new privacy risks for LLM agents. In this work, we systematically investigate the vulnerability of LLM agents to our proposed Memory EXTRaction Attack (MEXTRA) under a black-box setting. To extract private information from memory, we propose an effective attacking prompt design and an automated prompt generation method based on different levels of knowledge about the LLM agent. Experiments on two representative agents demonstrate the effectiveness of MEXTRA. Moreover, we explore key factors influencing memory leakage from both the agent's and the attacker's perspectives. Our findings highlight the urgent need for effective memory safeguards in LLM agent design and deployment.</li>
</ul>

<h3>Title: Generative Topology Optimization: Exploring Diverse Solutions in Structural Design</h3>
<ul>
<li><strong>Authors: </strong>Andreas Radler, Eric Volkmann, Johannes Brandstetter, Arturs Berzins</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13174">https://arxiv.org/abs/2502.13174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13174">https://arxiv.org/pdf/2502.13174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13174]] Generative Topology Optimization: Exploring Diverse Solutions in Structural Design(https://arxiv.org/abs/2502.13174)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free, generative</a></li>
<li><strong>Abstract: </strong>Topology optimization (TO) is a family of computational methods that derive near-optimal geometries from formal problem descriptions. Despite their success, established TO methods are limited to generating single solutions, restricting the exploration of alternative designs. To address this limitation, we introduce Generative Topology Optimization (GenTO) - a data-free method that trains a neural network to generate structurally compliant shapes and explores diverse solutions through an explicit diversity constraint. The network is trained with a solver-in-the-loop, optimizing the material distribution in each iteration. The trained model produces diverse shapes that closely adhere to the design requirements. We validate GenTO on 2D and 3D TO problems. Our results demonstrate that GenTO produces more diverse solutions than any prior method while maintaining near-optimality and being an order of magnitude faster due to inherent parallelism. These findings open new avenues for engineering and design, offering enhanced flexibility and innovation in structural optimization.</li>
</ul>

<h3>Title: Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks</h3>
<ul>
<li><strong>Authors: </strong>Wenpeng Xing, Minghao Li, Mohan Li, Meng Han</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13175">https://arxiv.org/abs/2502.13175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13175">https://arxiv.org/pdf/2502.13175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13175]] Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks(https://arxiv.org/abs/2502.13175)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Embodied AI systems, including robots and autonomous vehicles, are increasingly integrated into real-world applications, where they encounter a range of vulnerabilities stemming from both environmental and system-level factors. These vulnerabilities manifest through sensor spoofing, adversarial attacks, and failures in task and motion planning, posing significant challenges to robustness and safety. Despite the growing body of research, existing reviews rarely focus specifically on the unique safety and security challenges of embodied AI systems. Most prior work either addresses general AI vulnerabilities or focuses on isolated aspects, lacking a dedicated and unified framework tailored to embodied AI. This survey fills this critical gap by: (1) categorizing vulnerabilities specific to embodied AI into exogenous (e.g., physical attacks, cybersecurity threats) and endogenous (e.g., sensor failures, software flaws) origins; (2) systematically analyzing adversarial attack paradigms unique to embodied AI, with a focus on their impact on perception, decision-making, and embodied interaction; (3) investigating attack vectors targeting large vision-language models (LVLMs) and large language models (LLMs) within embodied systems, such as jailbreak attacks and instruction misinterpretation; (4) evaluating robustness challenges in algorithms for embodied perception, decision-making, and task planning; and (5) proposing targeted strategies to enhance the safety and reliability of embodied AI systems. By integrating these dimensions, we provide a comprehensive framework for understanding the interplay between vulnerabilities and safety in embodied AI.</li>
</ul>

<h3>Title: BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Burak Gulhan, Krishna Teja Chitty-Venkata, Murali Emani, Mahmut Kandemir, Venkatram Vishwanath</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13176">https://arxiv.org/abs/2502.13176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13176">https://arxiv.org/pdf/2502.13176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13176]] BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference(https://arxiv.org/abs/2502.13176)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches) are essential for reducing time complexity. However, they result in a linear increase in GPU memory as the context length grows. While recent work explores KV-cache eviction and compression policies to reduce memory usage, they often consider uniform KV-caches across all attention heads, leading to suboptimal performance. We introduce BaKlaVa, a method to allocate optimal memory for individual KV-caches across the model by estimating the importance of each KV-cache. Our empirical analysis demonstrates that not all KV-caches are equally critical for LLM performance. Using a one-time profiling approach, BaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our method on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\% compression ratio while keeping baseline performance and delivering up to an order-of-magnitude accuracy improvement at higher compression levels.</li>
</ul>

<h3>Title: KL Penalty Control via Perturbation for Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Sangkyu Lee, Janghoon Han, Hosung Song, Stanley Jungkyu Choi, Honglak Lee, Youngjae Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13177">https://arxiv.org/abs/2502.13177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13177">https://arxiv.org/pdf/2502.13177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13177]] KL Penalty Control via Perturbation for Direct Preference Optimization(https://arxiv.org/abs/2502.13177)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Direct Preference Optimization (DPO) demonstrates the advantage of aligning a large language model with human preference using only an offline dataset. However, DPO has the limitation that the KL penalty, which prevents excessive deviation from the reference model, is static throughout the training process. Several methods try to turn this static KL penalty into a dynamic one, but no approach can adaptively assign different KL penalties for each preference pair. In this paper, we propose $\varepsilon$-Direct Preference Optimization ($\varepsilon$-DPO), which allows adaptive control of the KL penalty strength $\beta$ for each preference pair. Specifically, $\varepsilon$-DPO adaptively controls $\beta$ for each preference pair based on the monotonicity of logits as a preference model under the perturbation of $\beta$ during training by simply reusing the logit of the current policy and the reference policy. Experimental results show that $\varepsilon$-DPO outperforms existing direct alignment algorithms and KL penalty relaxation methods on general chatbot benchmarks, highlighting the significance of adaptive KL penalty relaxation at the instance-level in DPO.</li>
</ul>

<h3>Title: Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy, Unified Evaluation, and Comparative Analysis</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Zhao, Ming Wang, Miao Zhang, Yuzhang Shang, Xuebo Liu, Yaowei Wang, Min Zhang, Liqiang Nie</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13178">https://arxiv.org/abs/2502.13178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13178">https://arxiv.org/pdf/2502.13178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13178]] Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy, Unified Evaluation, and Comparative Analysis(https://arxiv.org/abs/2502.13178)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Post-training Quantization (PTQ) technique has been extensively adopted for large language models (LLMs) compression owing to its efficiency and low resource requirement. However, current research lacks a in-depth analysis of the superior and applicable scenarios of each PTQ strategy. In addition, existing algorithms focus primarily on performance, overlooking the trade-off among model size, performance, and quantization bitwidth. To mitigate these confusions, we provide a novel benchmark for LLMs PTQ in this paper. Firstly, in order to support our benchmark, we propose a comprehensive taxonomy for existing mainstream methods by scrutinizing their computational strategies (e.g., optimization-based, compensation-based, etc.). Then, we conduct extensive experiments with the baseline within each class, covering models with various sizes (7B-70B), bitwidths, training levels (LLaMA1/2/3/3.1), architectures (Mixtral, DeepSeekMoE and Mamba) and modality (LLaVA1.5 and VILA1.5) on a wide range of evaluation this http URL comparative analysis on the results, we summarize the superior of each PTQ strategy and modelsize-bitwidth trade-off considering the performance. For example, our benchmark reveals that compensation-based technique demonstrates outstanding cross-architecture robustness and extremely low-bit PTQ for ultra large models should be reexamined. Finally, we further accordingly claim that a practical combination of compensation and other PTQ strategy can achieve SOTA various robustness. We believe that our benchmark will provide valuable recommendations for the deployment of LLMs and future research on PTQ approaches.</li>
</ul>

<h3>Title: PTQ1.61: Push the Real Limit of Extremely Low-Bit Post-Training Quantization Methods for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Zhao, Miao Zhang, Ming Wang, Yuzhang Shang, Kaihao Zhang, Weili Guan, Yaowei Wang, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13179">https://arxiv.org/abs/2502.13179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13179">https://arxiv.org/pdf/2502.13179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13179]] PTQ1.61: Push the Real Limit of Extremely Low-Bit Post-Training Quantization Methods for Large Language Models(https://arxiv.org/abs/2502.13179)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) suffer severe performance degradation when facing extremely low-bit (sub 2-bit) quantization. Several existing sub 2-bit post-training quantization (PTQ) methods utilize a mix-precision scheme by leveraging an unstructured fine-grained mask to explicitly distinguish salient weights, while which introduces an extra 1-bit or more per weight. To explore the real limit of PTQ, we propose an extremely low-bit PTQ method called PTQ1.61, which enables weight quantization to 1.61-bit for the first time. Specifically, we first introduce a one-dimensional structured mask with negligibly additional 0.0002-bit per weight based on input activations from the perspective of reducing the upper bound of quantization error to allocate corresponding salient weight channels to 4-bit. For non-salient channels binarization, an efficient block-wise scaling factors optimization framework is then presented to take implicit row-wise correlations and angular biases into account. Different from prior works that concentrate on adjusting quantization methodologies, we further propose a novel paradigm called quantization preprocessing, where we argue that transforming the weight distribution of the pretrained model before quantization can alleviate the difficulty in per-channel extremely low-bit PTQ. Extensive experiments indicate our PTQ1.61 achieves state-of-the-art performance in extremely low-bit quantization. Codes are available at this https URL.</li>
</ul>

<h3>Title: Uncertain Multi-Objective Recommendation via Orthogonal Meta-Learning Enhanced Bayesian Optimization</h3>
<ul>
<li><strong>Authors: </strong>Hongxu Wang, Zhu Sun, Yingpeng Du, Lu Zhang, Tiantian He, Yew-Soon Ong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13180">https://arxiv.org/abs/2502.13180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13180">https://arxiv.org/pdf/2502.13180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13180]] Uncertain Multi-Objective Recommendation via Orthogonal Meta-Learning Enhanced Bayesian Optimization(https://arxiv.org/abs/2502.13180)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Recommender systems (RSs) play a crucial role in shaping our digital interactions, influencing how we access and engage with information across various domains. Traditional research has predominantly centered on maximizing recommendation accuracy, often leading to unintended side effects such as echo chambers and constrained user experiences. Drawing inspiration from autonomous driving, we introduce a novel framework that categorizes RS autonomy into five distinct levels, ranging from basic rule-based accuracy-driven systems to behavior-aware, uncertain multi-objective RSs - where users may have varying needs, such as accuracy, diversity, and fairness. In response, we propose an approach that dynamically identifies and optimizes multiple objectives based on individual user preferences, fostering more ethical and intelligent user-centric recommendations. To navigate the uncertainty inherent in multi-objective RSs, we develop a Bayesian optimization (BO) framework that captures personalized trade-offs between different objectives while accounting for their uncertain interdependencies. Furthermore, we introduce an orthogonal meta-learning paradigm to enhance BO efficiency and effectiveness by leveraging shared knowledge across similar tasks and mitigating conflicts among objectives through the discovery of orthogonal information. Finally, extensive empirical evaluations demonstrate the effectiveness of our method in optimizing uncertain multi-objectives for individual users, paving the way for more adaptive and user-focused RSs.</li>
</ul>

<h3>Title: RingFormer: Rethinking Recurrent Transformer with Adaptive Level Signals</h3>
<ul>
<li><strong>Authors: </strong>Jaemu Heo, Eldor Fozilov, Hyunmin Song, Taehwan Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13181">https://arxiv.org/abs/2502.13181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13181">https://arxiv.org/pdf/2502.13181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13181]] RingFormer: Rethinking Recurrent Transformer with Adaptive Level Signals(https://arxiv.org/abs/2502.13181)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers have achieved great success in effectively processing sequential data such as text. Their architecture consisting of several attention and feedforward blocks can model relations between elements of a sequence in parallel manner, which makes them very efficient to train and effective in sequence modeling. Even though they have shown strong performance in processing sequential data, the size of their parameters is considerably larger when compared to other architectures such as RNN and CNN based models. Therefore, several approaches have explored parameter sharing and recurrence in Transformer models to address their computational demands. However, such methods struggle to maintain high performance compared to the original transformer model. To address this challenge, we propose our novel approach, RingFormer, which employs one Transformer layer that processes input repeatedly in a circular, ring-like manner, while utilizing low-rank matrices to generate input-dependent level signals. This allows us to reduce the model parameters substantially while maintaining high performance in a variety of tasks such as translation and image classification, as validated in the experiments.</li>
</ul>

<h3>Title: MoBA: Mixture of Block Attention for Long-Context LLMs</h3>
<ul>
<li><strong>Authors: </strong>Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, Zhiqi Huang, Huan Yuan, Suting Xu, Xinran Xu, Guokun Lai, Yanru Chen, Huabin Zheng, Junjie Yan, Jianlin Su, Yuxin Wu, Neo Y. Zhang, Zhilin Yang, Xinyu Zhou, Mingxing Zhang, Jiezhong Qiu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13189">https://arxiv.org/abs/2502.13189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13189">https://arxiv.org/pdf/2502.13189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13189]] MoBA: Mixture of Block Attention for Long-Context LLMs(https://arxiv.org/abs/2502.13189)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored. In this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to support Kimi's long-context requests and demonstrates significant advancements in efficient attention computation for LLMs. Our code is available at this https URL.</li>
</ul>

<h3>Title: On the Privacy Risks of Spiking Neural Networks: A Membership Inference Analysis</h3>
<ul>
<li><strong>Authors: </strong>Junyi Guan, Abhijith Sharma, Chong Tian, Salem Lahlou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13191">https://arxiv.org/abs/2502.13191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13191">https://arxiv.org/pdf/2502.13191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13191]] On the Privacy Risks of Spiking Neural Networks: A Membership Inference Analysis(https://arxiv.org/abs/2502.13191)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, attack, robust, membership infer</a></li>
<li><strong>Abstract: </strong>Spiking Neural Networks (SNNs) are increasingly explored for their energy efficiency and robustness in real-world applications, yet their privacy risks remain largely unexamined. In this work, we investigate the susceptibility of SNNs to Membership Inference Attacks (MIAs) -- a major privacy threat where an adversary attempts to determine whether a given sample was part of the training dataset. While prior work suggests that SNNs may offer inherent robustness due to their discrete, event-driven nature, we find that its resilience diminishes as latency (T) increases. Furthermore, we introduce an input dropout strategy under black box setting, that significantly enhances membership inference in SNNs. Our findings challenge the assumption that SNNs are inherently more secure, and even though they are expected to be better, our results reveal that SNNs exhibit privacy vulnerabilities that are equally comparable to Artificial Neural Networks (ANNs). Our code is available at this https URL.</li>
</ul>

<h3>Title: Private Text Generation by Seeding Large Language Model Prompts</h3>
<ul>
<li><strong>Authors: </strong>Supriya Nagesh, Justin Y. Chen, Nina Mishra, Tal Wagner</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13193">https://arxiv.org/abs/2502.13193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13193">https://arxiv.org/pdf/2502.13193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13193]] Private Text Generation by Seeding Large Language Model Prompts(https://arxiv.org/abs/2502.13193)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>We explore how private synthetic text can be generated by suitably prompting a large language model (LLM). This addresses a challenge for organizations like hospitals, which hold sensitive text data like patient medical records, and wish to share it in order to train machine learning models for medical tasks, while preserving patient privacy. Methods that rely on training or finetuning a model may be out of reach, either due to API limits of third-party LLMs, or due to ethical and legal prohibitions on sharing the private data with the LLM itself. We propose Differentially Private Keyphrase Prompt Seeding (DP-KPS), a method that generates a private synthetic text corpus from a sensitive input corpus, by accessing an LLM only through privatized prompts. It is based on seeding the prompts with private samples from a distribution over phrase embeddings, thus capturing the input corpus while achieving requisite output diversity and maintaining differential privacy. We evaluate DP-KPS on downstream ML text classification tasks, and show that the corpora it generates preserve much of the predictive power of the original ones. Our findings offer hope that institutions can reap ML insights by privately sharing data with simple prompts and little compute.</li>
</ul>

<h3>Title: Girth of the Cayley graph and Cayley hash functions</h3>
<ul>
<li><strong>Authors: </strong>Vladimir Shpilrain</a></li>
<li><strong>Subjects: </strong>cs.CR, math.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13197">https://arxiv.org/abs/2502.13197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13197">https://arxiv.org/pdf/2502.13197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13197]] Girth of the Cayley graph and Cayley hash functions(https://arxiv.org/abs/2502.13197)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Cayley hash functions are based on a simple idea of using a pair of semigroup elements, A and B, to hash the 0 and 1 bit, respectively, and then to hash an arbitrary bit string in the natural way, by using multiplication of elements in the semigroup. The main advantage of Cayley hash functions compared to, say, hash functions in the SHA family is that when an already hashed document is amended, one does not have to hash the whole amended document all over again, but rather hash just the amended part and then multiply the result by the hash of the original document. In this article, we survey some of the previously proposed Cayley hash functions and single out a very simple hash function whose security has not been compromised up to date.</li>
</ul>

<h3>Title: Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Giorgio Franceschelli, Mirco Musolesi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13207">https://arxiv.org/abs/2502.13207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13207">https://arxiv.org/pdf/2502.13207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13207]] Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation(https://arxiv.org/abs/2502.13207)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the increasing use of large language models for creative tasks, their outputs often lack diversity. Common solutions, such as sampling at higher temperatures, can compromise the quality of the results. Drawing on information theory, we propose a context-based score to quantitatively evaluate value and originality. This score incentivizes accuracy and adherence to the request while fostering divergence from the learned distribution. We propose using our score as a reward in a reinforcement learning framework to fine-tune large language models for maximum performance. We validate our strategy through experiments in poetry generation and math problem solving, demonstrating that it enhances the value and originality of the generated solutions.</li>
</ul>

<h3>Title: Two Tickets are Better than One: Fair and Accurate Hiring Under Strategic LLM Manipulations</h3>
<ul>
<li><strong>Authors: </strong>Lee Cohen, Jack Hsieh, Connie Hong, Judy Hanwen Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13221">https://arxiv.org/abs/2502.13221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13221">https://arxiv.org/pdf/2502.13221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13221]] Two Tickets are Better than One: Fair and Accurate Hiring Under Strategic LLM Manipulations(https://arxiv.org/abs/2502.13221)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative, large language model</a></li>
<li><strong>Abstract: </strong>In an era of increasingly capable foundation models, job seekers are turning to generative AI tools to enhance their application materials. However, unequal access to and knowledge about generative AI tools can harm both employers and candidates by reducing the accuracy of hiring decisions and giving some candidates an unfair advantage. To address these challenges, we introduce a new variant of the strategic classification framework tailored to manipulations performed using large language models, accommodating varying levels of manipulations and stochastic outcomes. We propose a ``two-ticket'' scheme, where the hiring algorithm applies an additional manipulation to each submitted resume and considers this manipulated version together with the original submitted resume. We establish theoretical guarantees for this scheme, showing improvements for both the fairness and accuracy of hiring decisions when the true positive rate is maximized subject to a no false positives constraint. We further generalize this approach to an $n$-ticket scheme and prove that hiring outcomes converge to a fixed, group-independent decision, eliminating disparities arising from differential LLM access. Finally, we empirically validate our framework and the performance of our two-ticket scheme on real resumes using an open-source resume screening tool.</li>
</ul>

<h3>Title: SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question Answering?</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Shi, Tianze Yang, Canyu Chen, Quanzheng Li, Tianming Liu, Xiang Li, Ninghao Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13233">https://arxiv.org/abs/2502.13233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13233">https://arxiv.org/pdf/2502.13233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13233]] SearchRAG: Can Search Engines Be Helpful for LLM-based Medical Question Answering?(https://arxiv.org/abs/2502.13233)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable capabilities in general domains but often struggle with tasks requiring specialized knowledge. Conventional Retrieval-Augmented Generation (RAG) techniques typically retrieve external information from static knowledge bases, which can be outdated or incomplete, missing fine-grained clinical details essential for accurate medical question answering. In this work, we propose SearchRAG, a novel framework that overcomes these limitations by leveraging real-time search engines. Our method employs synthetic query generation to convert complex medical questions into search-engine-friendly queries and utilizes uncertainty-based knowledge selection to filter and incorporate the most relevant and informative medical knowledge into the LLM's input. Experimental results demonstrate that our method significantly improves response accuracy in medical question answering tasks, particularly for complex questions requiring detailed and up-to-date knowledge.</li>
</ul>

<h3>Title: MotionMatcher: Motion Customization of Text-to-Video Diffusion Models via Motion Feature Matching</h3>
<ul>
<li><strong>Authors: </strong>Yen-Siang Wu, Chi-Pin Huang, Fu-En Yang, Yu-Chiang Frank Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13234">https://arxiv.org/abs/2502.13234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13234">https://arxiv.org/pdf/2502.13234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13234]] MotionMatcher: Motion Customization of Text-to-Video Diffusion Models via Motion Feature Matching(https://arxiv.org/abs/2502.13234)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-video (T2V) diffusion models have shown promising capabilities in synthesizing realistic videos from input text prompts. However, the input text description alone provides limited control over the precise objects movements and camera framing. In this work, we tackle the motion customization problem, where a reference video is provided as motion guidance. While most existing methods choose to fine-tune pre-trained diffusion models to reconstruct the frame differences of the reference video, we observe that such strategy suffer from content leakage from the reference video, and they cannot capture complex motion accurately. To address this issue, we propose MotionMatcher, a motion customization framework that fine-tunes the pre-trained T2V diffusion model at the feature level. Instead of using pixel-level objectives, MotionMatcher compares high-level, spatio-temporal motion features to fine-tune diffusion models, ensuring precise motion learning. For the sake of memory efficiency and accessibility, we utilize a pre-trained T2V diffusion model, which contains considerable prior knowledge about video motion, to compute these motion features. In our experiments, we demonstrate state-of-the-art motion customization performances, validating the design of our framework.</li>
</ul>

<h3>Title: When People are Floods: Analyzing Dehumanizing Metaphors in Immigration Discourse with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Julia Mendelsohn, Ceren Budak</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13246">https://arxiv.org/abs/2502.13246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13246">https://arxiv.org/pdf/2502.13246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13246]] When People are Floods: Analyzing Dehumanizing Metaphors in Immigration Discourse with Large Language Models(https://arxiv.org/abs/2502.13246)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Metaphor, discussing one concept in terms of another, is abundant in politics and can shape how people understand important issues. We develop a computational approach to measure metaphorical language, focusing on immigration discourse on social media. Grounded in qualitative social science research, we identify seven concepts evoked in immigration discourse (e.g. "water" or "vermin"). We propose and evaluate a novel technique that leverages both word-level and document-level signals to measure metaphor with respect to these concepts. We then study the relationship between metaphor, political ideology, and user engagement in 400K US tweets about immigration. While conservatives tend to use dehumanizing metaphors more than liberals, this effect varies widely across concepts. Moreover, creature-related metaphor is associated with more retweets, especially for liberal authors. Our work highlights the potential for computational methods to complement qualitative approaches in understanding subtle and implicit language in political discourse.</li>
</ul>

<h3>Title: Grounding LLM Reasoning with Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Alfonso Amayuelas, Joy Sain, Simerjot Kaur, Charese Smiley</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13247">https://arxiv.org/abs/2502.13247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13247">https://arxiv.org/pdf/2502.13247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13247]] Grounding LLM Reasoning with Knowledge Graphs(https://arxiv.org/abs/2502.13247)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge Graphs (KGs) are valuable tools for representing relationships between entities in a structured format. Traditionally, these knowledge bases are queried to extract specific information. However, question-answering (QA) over such KGs poses a challenge due to the intrinsic complexity of natural language compared to the structured format and the size of these graphs. Despite these challenges, the structured nature of KGs can provide a solid foundation for grounding the outputs of Large Language Models (LLMs), offering organizations increased reliability and control. Recent advancements in LLMs have introduced reasoning methods at inference time to improve their performance and maximize their capabilities. In this work, we propose integrating these reasoning strategies with KGs to anchor every step or "thought" of the reasoning chains in KG data. Specifically, we evaluate both agentic and automated search methods across several reasoning strategies, including Chain-of-Thought (CoT), Tree-of-Thought (ToT), and Graph-of-Thought (GoT), using GRBench, a benchmark dataset for graph reasoning with domain-specific graphs. Our experiments demonstrate that this approach consistently outperforms baseline models, highlighting the benefits of grounding LLM reasoning processes in structured KG data.</li>
</ul>

<h3>Title: Neural Attention Search</h3>
<ul>
<li><strong>Authors: </strong>Difan Deng, Marius Lindauer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13251">https://arxiv.org/abs/2502.13251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13251">https://arxiv.org/pdf/2502.13251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13251]] Neural Attention Search(https://arxiv.org/abs/2502.13251)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>We present Neural Attention Search (NAtS), a framework that automatically evaluates the importance of each token within a sequence and determines if the corresponding token can be dropped after several steps. This approach can efficiently reduce the KV cache sizes required by transformer-based models during inference and thus reduce inference costs. In this paper, we design a search space that contains three token types: (i) Global Tokens will be preserved and queried by all the following tokens. (ii) Local Tokens survive until the next global token appears. (iii) Sliding Window Tokens have an impact on the inference of a fixed size of the next following tokens. Similar to the One-Shot Neural Architecture Search approach, this token-type information can be learned jointly with the architecture weights via a learnable attention mask. Experiments on both training a new transformer from scratch and fine-tuning existing large language models show that NAtS can efficiently reduce the KV cache size required for the models while maintaining the models' performance.</li>
</ul>

<h3>Title: Multilingual Language Model Pretraining using Machine-translated Data</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Wang, Yao Lu, Maurice Weber, Max Ryabinin, David Adelani, Yihong Chen, Raphael Tang, Pontus Stenetorp</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13252">https://arxiv.org/abs/2502.13252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13252">https://arxiv.org/pdf/2502.13252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13252]] Multilingual Language Model Pretraining using Machine-translated Data(https://arxiv.org/abs/2502.13252)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>High-resource languages such as English, enables the pretraining of high-quality large language models (LLMs). The same can not be said for most other languages as LLMs still underperform for non-English languages, likely due to a gap in the quality and diversity of the available multilingual pretraining corpora. In this work, we find that machine-translated texts from a single high-quality source language can contribute significantly to the pretraining quality of multilingual LLMs. We translate FineWeb-Edu, a high-quality English web dataset, into nine languages, resulting in a 1.7-trillion-token dataset, which we call TransWebEdu and pretrain a 1.3B-parameter model, TransWebLLM, from scratch on this dataset. Across nine non-English reasoning tasks, we show that TransWebLLM matches or outperforms state-of-the-art multilingual models trained using closed data, such as Llama3.2, Qwen2.5, and Gemma, despite using an order of magnitude less data. We demonstrate that adding less than 5% of TransWebEdu as domain-specific pretraining data sets a new state-of-the-art in Arabic, Italian, Indonesian, Swahili, and Welsh understanding and commonsense reasoning tasks. To promote reproducibility, we release our corpus, models, and training pipeline under Open Source Initiative-approved licenses.</li>
</ul>

<h3>Title: A Survey of Anomaly Detection in Cyber-Physical Systems</h3>
<ul>
<li><strong>Authors: </strong>Danial Abshari, Meera Sridhar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13256">https://arxiv.org/abs/2502.13256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13256">https://arxiv.org/pdf/2502.13256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13256]] A Survey of Anomaly Detection in Cyber-Physical Systems(https://arxiv.org/abs/2502.13256)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>In our increasingly interconnected world, Cyber-Physical Systems (CPS) play a crucial role in industries like healthcare, transportation, and manufacturing by combining physical processes with computing power. These systems, however, face many challenges, especially regarding security and system faults. Anomalies in CPS may indicate unexpected problems, from sensor malfunctions to cyber-attacks, and must be detected to prevent failures that can cause harm or disrupt services. This paper provides an overview of the different ways researchers have approached anomaly detection in CPS. We categorize and compare methods like machine learning, deep learning, mathematical models, invariant, and hybrid techniques. Our goal is to help readers understand the strengths and weaknesses of these methods and how they can be used to create safer, more reliable CPS. By identifying the gaps in current solutions, we aim to encourage future research that will make CPS more secure and adaptive in our increasingly automated world.</li>
</ul>

<h3>Title: Random Forest Autoencoders for Guided Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Adrien Aumon, Shuang Ni, Myriam Lizotte, Guy Wolf, Kevin R. Moon, Jake S. Rhodes</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13257">https://arxiv.org/abs/2502.13257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13257">https://arxiv.org/pdf/2502.13257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13257]] Random Forest Autoencoders for Guided Representation Learning(https://arxiv.org/abs/2502.13257)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, diffusion</a></li>
<li><strong>Abstract: </strong>Decades of research have produced robust methods for unsupervised data visualization, yet supervised visualization$\unicode{x2013}$where expert labels guide representations$\unicode{x2013}$remains underexplored, as most supervised approaches prioritize classification over visualization. Recently, RF-PHATE, a diffusion-based manifold learning method leveraging random forests and information geometry, marked significant progress in supervised visualization. However, its lack of an explicit mapping function limits scalability and prevents application to unseen data, posing challenges for large datasets and label-scarce scenarios. To overcome these limitations, we introduce Random Forest Autoencoders (RF-AE), a neural network-based framework for out-of-sample kernel extension that combines the flexibility of autoencoders with the supervised learning strengths of random forests and the geometry captured by RF-PHATE. RF-AE enables efficient out-of-sample supervised visualization and outperforms existing methods, including RF-PHATE's standard kernel extension, in both accuracy and interpretability. Additionally, RF-AE is robust to the choice of hyper-parameters and generalizes to any kernel-based dimensionality reduction method.</li>
</ul>

<h3>Title: Stepwise Perplexity-Guided Refinement for Efficient Chain-of-Thought Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yingqian Cui, Pengfei He, Jingying Zeng, Hui Liu, Xianfeng Tang, Zhenwei Dai, Yan Han, Chen Luo, Jing Huang, Zhen Li, Suhang Wang, Yue Xing, Jiliang Tang, Qi He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13260">https://arxiv.org/abs/2502.13260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13260">https://arxiv.org/pdf/2502.13260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13260]] Stepwise Perplexity-Guided Refinement for Efficient Chain-of-Thought Reasoning in Large Language Models(https://arxiv.org/abs/2502.13260)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) reasoning, which breaks down complex tasks into intermediate reasoning steps, has significantly enhanced the performance of large language models (LLMs) on challenging tasks. However, the detailed reasoning process in CoT often incurs long generation times and high computational costs, partly due to the inclusion of unnecessary steps. To address this, we propose a method to identify critical reasoning steps using perplexity as a measure of their importance: a step is deemed critical if its removal causes a significant increase in perplexity. Our method enables models to focus solely on generating these critical steps. This can be achieved through two approaches: refining demonstration examples in few-shot CoT or fine-tuning the model using selected examples that include only critical steps. Comprehensive experiments validate the effectiveness of our method, which achieves a better balance between the reasoning accuracy and efficiency of CoT.</li>
</ul>

<h3>Title: A Machine Learning Approach That Beats Large Rubik's Cubes</h3>
<ul>
<li><strong>Authors: </strong>Alexander Chervov, Kirill Khoruzhii, Nikita Bukhal, Jalal Naghiyev, Vladislav Zamkovoy, Ivan Koltsov, Lyudmila Cheldieva, Arsenii Sychev, Arsenii Lenin, Mark Obozov, Egor Urvanov, Alexey Romanov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13266">https://arxiv.org/abs/2502.13266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13266">https://arxiv.org/pdf/2502.13266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13266]] A Machine Learning Approach That Beats Large Rubik's Cubes(https://arxiv.org/abs/2502.13266)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The paper proposes a novel machine learning-based approach to the pathfinding problem on extremely large graphs. This method leverages diffusion distance estimation via a neural network and uses beam search for pathfinding. We demonstrate its efficiency by finding solutions for 4x4x4 and 5x5x5 Rubik's cubes with unprecedentedly short solution lengths, outperforming all available solvers and introducing the first machine learning solver beyond the 3x3x3 case. In particular, it surpasses every single case of the combined best results in the Kaggle Santa 2023 challenge, which involved over 1,000 teams. For the 3x3x3 Rubik's cube, our approach achieves an optimality rate exceeding 98%, matching the performance of task-specific solvers and significantly outperforming prior solutions such as DeepCubeA (60.3%) and EfficientCube (69.6%). Additionally, our solution is more than 26 times faster in solving 3x3x3 Rubik's cubes while requiring up to 18.5 times less model training time than the most efficient state-of-the-art competitor.</li>
</ul>

<h3>Title: Performance Evaluation of Sentiment Analysis on Text and Emoji Data Using End-to-End, Transfer Learning, Distributed and Explainable AI Models</h3>
<ul>
<li><strong>Authors: </strong>Sirisha Velampalli, Chandrashekar Muniyappa, Ashutosh Saxena</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13278">https://arxiv.org/abs/2502.13278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13278">https://arxiv.org/pdf/2502.13278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13278]] Performance Evaluation of Sentiment Analysis on Text and Emoji Data Using End-to-End, Transfer Learning, Distributed and Explainable AI Models(https://arxiv.org/abs/2502.13278)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Emojis are being frequently used in todays digital world to express from simple to complex thoughts more than ever before. Hence, they are also being used in sentiment analysis and targeted marketing campaigns. In this work, we performed sentiment analysis of Tweets as well as on emoji dataset from the Kaggle. Since tweets are sentences we have used Universal Sentence Encoder (USE) and Sentence Bidirectional Encoder Representations from Transformers (SBERT) end-to-end sentence embedding models to generate the embeddings which are used to train the Standard fully connected Neural Networks (NN), and LSTM NN models. We observe the text classification accuracy was almost the same for both the models around 98 percent. On the contrary, when the validation set was built using emojis that were not present in the training set then the accuracy of both the models reduced drastically to 70 percent. In addition, the models were also trained using the distributed training approach instead of a traditional singlethreaded model for better scalability. Using the distributed training approach, we were able to reduce the run-time by roughly 15% without compromising on accuracy. Finally, as part of explainable AI the Shap algorithm was used to explain the model behaviour and check for model biases for the given feature set.</li>
</ul>

<h3>Title: Breaking the bonds of generative artificial intelligence by minimizing the maximum entropy</h3>
<ul>
<li><strong>Authors: </strong>Mattia Miotto, Lorenzo Monacelli</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.stat-mech, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13287">https://arxiv.org/abs/2502.13287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13287">https://arxiv.org/pdf/2502.13287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13287]] Breaking the bonds of generative artificial intelligence by minimizing the maximum entropy(https://arxiv.org/abs/2502.13287)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The emergence of generative artificial intelligence (GenAI), comprising large language models, text-to-image generators, and AI algorithms for medical drug and material design, had a transformative impact on society. However, despite an initial exponential growth surpassing Moore's law, progress is now plateauing, suggesting we are approaching the limits of current technology. Indeed, these models are notoriously data-hungry, prone to overfitting, and challenging to direct during the generative process, hampering their effective professional employment. To cope with these limitations, we propose a paradigm shift in GenAI by introducing an ab initio method based on the minimal maximum entropy principle. Our approach does not fit the data. Instead, it compresses information in the training set by finding a latent representation parameterized by arbitrary nonlinear functions, such as neural networks. The result is a general physics-driven model, which is data-efficient, resistant to overfitting, and flexible, permitting to control and influence the generative process. Benchmarking shows that our method outperforms variational autoencoders (VAEs) with similar neural architectures, particularly on undersampled datasets. We demonstrate the methods effectiveness in generating images, even with limited training data, and its unprecedented capability to customize the generation process a posteriori without the need of any fine-tuning or retraining.</li>
</ul>

<h3>Title: Prediction of Clinical Complication Onset using Neural Point Processes</h3>
<ul>
<li><strong>Authors: </strong>Sachini Weerasekara, Sagar Kamarthi, Jacqueline Isaacs</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13290">https://arxiv.org/abs/2502.13290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13290">https://arxiv.org/pdf/2502.13290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13290]] Prediction of Clinical Complication Onset using Neural Point Processes(https://arxiv.org/abs/2502.13290)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Predicting medical events in advance within critical care settings is paramount for patient outcomes and resource management. Utilizing predictive models, healthcare providers can anticipate issues such as cardiac arrest, sepsis, or respiratory failure before they manifest. Recently, there has been a surge in research focusing on forecasting adverse medical event onsets prior to clinical manifestation using machine learning. However, while these models provide temporal prognostic predictions for the occurrence of a specific adverse event of interest within defined time intervals, their interpretability often remains a challenge. In this work, we explore the applicability of neural temporal point processes in the context of adverse event onset prediction, with the aim of explaining clinical pathways and providing interpretable insights. Our experiments span six state-of-the-art neural point processes and six critical care datasets, each focusing on the onset of distinct adverse events. This work represents a novel application class of neural temporal point processes in event prediction.</li>
</ul>

<h3>Title: Understanding and Tackling Label Errors in Individual-Level Nature Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Yunpeng Xiao, Youpeng Zhao, Kai Shu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13297">https://arxiv.org/abs/2502.13297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13297">https://arxiv.org/pdf/2502.13297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13297]] Understanding and Tackling Label Errors in Individual-Level Nature Language Understanding(https://arxiv.org/abs/2502.13297)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Natural language understanding (NLU) is a task that enables machines to understand human language. Some tasks, such as stance detection and sentiment analysis, are closely related to individual subjective perspectives, thus termed individual-level NLU. Previously, these tasks are often simplified to text-level NLU tasks, ignoring individual factors. This not only makes inference difficult and unexplainable but often results in a large number of label errors when creating datasets. To address the above limitations, we propose a new NLU annotation guideline based on individual-level factors. Specifically, we incorporate other posts by the same individual and then annotate individual subjective perspectives after considering all individual posts. We use this guideline to expand and re-annotate the stance detection and topic-based sentiment analysis datasets. We find that error rates in the samples were as high as 31.7\% and 23.3\%. We further use large language models to conduct experiments on the re-annotation datasets and find that the large language models perform well on both datasets after adding individual factors. Both GPT-4o and Llama3-70B can achieve an accuracy greater than 87\% on the re-annotation datasets. We also verify the effectiveness of individual factors through ablation studies. We call on future researchers to add individual factors when creating such datasets. Our re-annotation dataset can be found at this https URL</li>
</ul>

<h3>Title: Improving Multi-turn Task Completion in Task-Oriented Dialog Systems via Prompt Chaining and Fine-Grained Feedback</h3>
<ul>
<li><strong>Authors: </strong>Moghis Fereidouni, Md Sajid Ahmed, Adib Mosharrof, A.B. Siddique</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13298">https://arxiv.org/abs/2502.13298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13298">https://arxiv.org/pdf/2502.13298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13298]] Improving Multi-turn Task Completion in Task-Oriented Dialog Systems via Prompt Chaining and Fine-Grained Feedback(https://arxiv.org/abs/2502.13298)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Task-oriented dialog (TOD) systems facilitate users in accomplishing complex, multi-turn tasks through natural language. While traditional approaches rely on extensive fine-tuning and annotated data for each domain, instruction-tuned large language models (LLMs) offer a more flexible alternative. However, LLMs struggle to reliably handle multi-turn task completion, particularly with accurately generating API calls and adapting to new domains without explicit demonstrations. To address these challenges, we propose RealTOD, a novel framework that enhances TOD systems through prompt chaining and fine-grained feedback mechanisms. Prompt chaining enables zero-shot domain adaptation via a two-stage prompting strategy, eliminating the need for human-curated demonstrations. Meanwhile, the fine-grained feedback mechanism improves task completion by verifying API calls against domain schemas and providing precise corrective feedback when errors are detected. We conduct extensive experiments on the SGD and BiTOD benchmarks using four LLMs. RealTOD improves API accuracy, surpassing AutoTOD by 37.74% on SGD and SimpleTOD by 11.26% on BiTOD. Human evaluations further confirm that LLMs integrated with RealTOD achieve superior task completion, fluency, and informativeness compared to existing methods.</li>
</ul>

<h3>Title: A Label-Free Heterophily-Guided Approach for Unsupervised Graph Fraud Detection</h3>
<ul>
<li><strong>Authors: </strong>Junjun Pan, Yixin Liu, Xin Zheng, Yizhen Zheng, Alan Wee-Chung Liew, Fuyi Li, Shirui Pan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13308">https://arxiv.org/abs/2502.13308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13308">https://arxiv.org/pdf/2502.13308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13308]] A Label-Free Heterophily-Guided Approach for Unsupervised Graph Fraud Detection(https://arxiv.org/abs/2502.13308)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust</a></li>
<li><strong>Abstract: </strong>Graph fraud detection (GFD) has rapidly advanced in protecting online services by identifying malicious fraudsters. Recent supervised GFD research highlights that heterophilic connections between fraudsters and users can greatly impact detection performance, since fraudsters tend to camouflage themselves by building more connections to benign users. Despite the promising performance of supervised GFD methods, the reliance on labels limits their applications to unsupervised scenarios; Additionally, accurately capturing complex and diverse heterophily patterns without labels poses a further challenge. To fill the gap, we propose a Heterophily-guided Unsupervised Graph fraud dEtection approach (HUGE) for unsupervised GFD, which contains two essential components: a heterophily estimation module and an alignment-based fraud detection module. In the heterophily estimation module, we design a novel label-free heterophily metric called HALO, which captures the critical graph properties for GFD, enabling its outstanding ability to estimate heterophily from node attributes. In the alignment-based fraud detection module, we develop a joint MLP-GNN architecture with ranking loss and asymmetric alignment loss. The ranking loss aligns the predicted fraud score with the relative order of HALO, providing an extra robustness guarantee by comparing heterophily among non-adjacent nodes. Moreover, the asymmetric alignment loss effectively utilizes structural information while alleviating the feature-smooth effects of this http URL experiments on 6 datasets demonstrate that HUGE significantly outperforms competitors, showcasing its effectiveness and robustness. The source code of HUGE is at this https URL.</li>
</ul>

<h3>Title: Evaluating and Enhancing Out-of-Domain Generalization of Task-Oriented Dialog Systems for Task Completion without Turn-level Dialog Annotations</h3>
<ul>
<li><strong>Authors: </strong>Adib Mosharrof, Moghis Fereidouni, A.B. Siddique</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13310">https://arxiv.org/abs/2502.13310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13310">https://arxiv.org/pdf/2502.13310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13310]] Evaluating and Enhancing Out-of-Domain Generalization of Task-Oriented Dialog Systems for Task Completion without Turn-level Dialog Annotations(https://arxiv.org/abs/2502.13310)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Traditional task-oriented dialog (ToD) systems rely heavily on labor-intensive turn-level annotations, such as dialogue states and policy labels, for training. This work explores whether large language models (LLMs) can be fine-tuned solely on natural language dialogs to perform ToD tasks, without requiring such annotations. We evaluate their ability to generalize to unseen domains and compare their performance with models trained on fully annotated data. Through extensive experiments with three open-source LLMs of varying sizes and two diverse ToD datasets, we find that models fine-tuned without turn-level annotations generate coherent and contextually appropriate responses. However, their task completion performance - measured by accurate execution of API calls - remains suboptimal, with the best models achieving only around 53% success in unseen domains. To improve task completion, we propose ZeroToD, a framework that incorporates a schema augmentation mechanism to enhance API call accuracy and overall task completion rates, particularly in out-of-domain settings. We also compare ZeroToD with fine-tuning-free alternatives, such as prompting off-the-shelf LLMs, and find that our framework enables smaller, fine-tuned models that outperform large-scale proprietary LLMs in task completion. Additionally, a human study evaluating informativeness, fluency, and task completion confirms our empirical findings. These findings suggest the feasibility of developing cost-effective, scalable, and zero-shot generalizable ToD systems for real-world applications.</li>
</ul>

<h3>Title: Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors</h3>
<ul>
<li><strong>Authors: </strong>Jian Wang, Yinpei Dai, Yichi Zhang, Ziqiao Ma, Wenjie Li, Joyce Chai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13311">https://arxiv.org/abs/2502.13311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13311">https://arxiv.org/pdf/2502.13311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13311]] Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors(https://arxiv.org/abs/2502.13311)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Intelligent tutoring agents powered by large language models (LLMs) have been increasingly explored to deliver personalized guidance in areas such as language learning and science education. However, their capabilities in guiding users to solve complex real-world tasks remain underexplored. To address this limitation, in this work, we focus on coding tutoring, a challenging problem that requires tutors to proactively guide students toward completing predefined coding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER), which combines knowledge tracing to estimate a student's knowledge state and turn-by-turn verification to ensure effective guidance toward task completion. We introduce DICT, an automatic evaluation protocol that assesses tutor agents holistically using controlled student simulation and code generation tests. Extensive experiments reveal the challenges of coding tutoring and demonstrate that TRAVER achieves a significantly higher success rate. Although we use code tutoring as an example in this paper, our results and findings can be extended beyond coding, providing valuable insights into advancing tutoring agents for a variety of tasks.</li>
</ul>

<h3>Title: Debiasing Functions of Private Statistics in Postprocessing</h3>
<ul>
<li><strong>Authors: </strong>Flavio Calmon, Elbert Du, Cynthia Dwork, Brian Finley, Grigory Franguridi</a></li>
<li><strong>Subjects: </strong>cs.CR, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13314">https://arxiv.org/abs/2502.13314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13314">https://arxiv.org/pdf/2502.13314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13314]] Debiasing Functions of Private Statistics in Postprocessing(https://arxiv.org/abs/2502.13314)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Given a differentially private unbiased estimate $\tilde{q}=q(D) +\nu$ of a statistic $q(D)$, we wish to obtain unbiased estimates of functions of $q(D)$, such as $1/q(D)$, solely through post-processing of $\tilde{q}$, with no further access to the confidential dataset $D$. To this end, we adapt the deconvolution method used for unbiased estimation in the statistical literature, deriving unbiased estimators for a broad family of twice-differentiable functions when the privacy-preserving noise $\nu$ is drawn from the Laplace distribution (Dwork et al., 2006). We further extend this technique to a more general class of functions, deriving approximately optimal estimators that are unbiased for values in a user-specified interval (possibly extending to $\pm \infty$). We use these results to derive an unbiased estimator for private means when the size $n$ of the dataset is not publicly known. In a numerical application, we find that a mechanism that uses our estimator to return an unbiased sample size and mean outperforms a mechanism that instead uses the previously known unbiased privacy mechanism for such means (Kamath et al., 2023). We also apply our estimators to develop unbiased transformation mechanisms for per-record differential privacy, a privacy concept in which the privacy guarantee is a public function of a record's value (Seeman et al., 2024). Our mechanisms provide stronger privacy guarantees than those in prior work (Finley et al., 2024) by using Laplace, rather than Gaussian, noise. Finally, using a different approach, we go beyond Laplace noise by deriving unbiased estimators for polynomials under the weak condition that the noise distribution has sufficiently many moments.</li>
</ul>

<h3>Title: VUS: Effective and Efficient Accuracy Measures for Time-Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Paul Boniol, Ashwin K. Krishna, Marine Bruel, Qinghua Liu, Mingyi Huang, Themis Palpanas, Ruey S. Tsay, Aaron Elmore, Michael J. Franklin, John Paparrizos</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13318">https://arxiv.org/abs/2502.13318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13318">https://arxiv.org/pdf/2502.13318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13318]] VUS: Effective and Efficient Accuracy Measures for Time-Series Anomaly Detection(https://arxiv.org/abs/2502.13318)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Anomaly detection (AD) is a fundamental task for time-series analytics with important implications for the downstream performance of many applications. In contrast to other domains where AD mainly focuses on point-based anomalies (i.e., outliers in standalone observations), AD for time series is also concerned with range-based anomalies (i.e., outliers spanning multiple observations). Nevertheless, it is common to use traditional point-based information retrieval measures, such as Precision, Recall, and F-score, to assess the quality of methods by thresholding the anomaly score to mark each point as an anomaly or not. However, mapping discrete labels into continuous data introduces unavoidable shortcomings, complicating the evaluation of range-based anomalies. Notably, the choice of evaluation measure may significantly bias the experimental outcome. Despite over six decades of attention, there has never been a large-scale systematic quantitative and qualitative analysis of time-series AD evaluation measures. This paper extensively evaluates quality measures for time-series AD to assess their robustness under noise, misalignments, and different anomaly cardinality ratios. Our results indicate that measures producing quality values independently of a threshold (i.e., AUC-ROC and AUC-PR) are more suitable for time-series AD. Motivated by this observation, we first extend the AUC-based measures to account for range-based anomalies. Then, we introduce a new family of parameter-free and threshold-independent measures, Volume Under the Surface (VUS), to evaluate methods while varying parameters. We also introduce two optimized implementations for VUS that reduce significantly the execution time of the initial implementation. Our findings demonstrate that our four measures are significantly more robust in assessing the quality of time-series AD methods.</li>
</ul>

<h3>Title: Elucidating Mechanisms of Demographic Bias in LLMs for Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Hiba Ahsan, Arnab Sen Sharma, Silvio Amir, David Bau, Byron C. Wallace</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13319">https://arxiv.org/abs/2502.13319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13319">https://arxiv.org/pdf/2502.13319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13319]] Elucidating Mechanisms of Demographic Bias in LLMs for Healthcare(https://arxiv.org/abs/2502.13319)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We know from prior work that LLMs encode social biases, and that this manifests in clinical tasks. In this work we adopt tools from mechanistic interpretability to unveil sociodemographic representations and biases within LLMs in the context of healthcare. Specifically, we ask: Can we identify activations within LLMs that encode sociodemographic information (e.g., gender, race)? We find that gender information is highly localized in middle MLP layers and can be reliably manipulated at inference time via patching. Such interventions can surgically alter generated clinical vignettes for specific conditions, and also influence downstream clinical predictions which correlate with gender, e.g., patient risk of depression. We find that representation of patient race is somewhat more distributed, but can also be intervened upon, to a degree. To our knowledge, this is the first application of mechanistic interpretability methods to LLMs for healthcare.</li>
</ul>

<h3>Title: Geometry-Aware Diffusion Models for Multiview Scene Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Salimi, Tristan Aumentado-Armstrong, Marcus A. Brubaker, Konstantinos G. Derpanis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13335">https://arxiv.org/abs/2502.13335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13335">https://arxiv.org/pdf/2502.13335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13335]] Geometry-Aware Diffusion Models for Multiview Scene Inpainting(https://arxiv.org/abs/2502.13335)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we focus on 3D scene inpainting, where parts of an input image set, captured from different viewpoints, are masked out. The main challenge lies in generating plausible image completions that are geometrically consistent across views. Most recent work addresses this challenge by combining generative models with a 3D radiance field to fuse information across viewpoints. However, a major drawback of these methods is that they often produce blurry images due to the fusion of inconsistent cross-view images. To avoid blurry inpaintings, we eschew the use of an explicit or implicit radiance field altogether and instead fuse cross-view information in a learned space. In particular, we introduce a geometry-aware conditional generative model, capable of inpainting multi-view consistent images based on both geometric and appearance cues from reference images. A key advantage of our approach over existing methods is its unique ability to inpaint masked scenes with a limited number of views (i.e., few-view inpainting), whereas previous methods require relatively large image sets for their 3D model fitting step. Empirically, we evaluate and compare our scene-centric inpainting method on two datasets, SPIn-NeRF and NeRFiller, which contain images captured at narrow and wide baselines, respectively, and achieve state-of-the-art 3D inpainting performance on both. Additionally, we demonstrate the efficacy of our approach in the few-view setting compared to prior methods.</li>
</ul>

<h3>Title: Language Models are Few-Shot Graders</h3>
<ul>
<li><strong>Authors: </strong>Chenyan Zhao, Mariana Silva, Seth Poulsen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13337">https://arxiv.org/abs/2502.13337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13337">https://arxiv.org/pdf/2502.13337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13337]] Language Models are Few-Shot Graders(https://arxiv.org/abs/2502.13337)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Providing evaluations to student work is a critical component of effective student learning, and automating its process can significantly reduce the workload on human graders. Automatic Short Answer Grading (ASAG) systems, enabled by advancements in Large Language Models (LLMs), offer a promising solution for assessing and providing instant feedback for open-ended student responses. In this paper, we present an ASAG pipeline leveraging state-of-the-art LLMs. Our new LLM-based ASAG pipeline achieves better performances than existing custom-built models on the same datasets. We also compare the grading performance of three OpenAI models: GPT-4, GPT-4o, and o1-preview. Our results demonstrate that GPT-4o achieves the best balance between accuracy and cost-effectiveness. On the other hand, o1-preview, despite higher accuracy, exhibits a larger variance in error that makes it less practical for classroom use. We investigate the effects of incorporating instructor-graded examples into prompts using no examples, random selection, and Retrieval-Augmented Generation (RAG)-based selection strategies. Our findings indicate that providing graded examples enhances grading accuracy, with RAG-based selection outperforming random selection. Additionally, integrating grading rubrics improves accuracy by offering a structured standard for evaluation.</li>
</ul>

<h3>Title: Beyond De-Identification: A Structured Approach for Defining and Detecting Indirect Identifiers in Medical Texts</h3>
<ul>
<li><strong>Authors: </strong>Ibrahim Baroud, Lisa Raithel, Sebastian Mller, Roland Roller</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13342">https://arxiv.org/abs/2502.13342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13342">https://arxiv.org/pdf/2502.13342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13342]] Beyond De-Identification: A Structured Approach for Defining and Detecting Indirect Identifiers in Medical Texts(https://arxiv.org/abs/2502.13342)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Sharing sensitive texts for scientific purposes requires appropriate techniques to protect the privacy of patients and healthcare personnel. Anonymizing textual data is particularly challenging due to the presence of diverse unstructured direct and indirect identifiers. To mitigate the risk of re-identification, this work introduces a schema of nine categories of indirect identifiers designed to account for different potential adversaries, including acquaintances, family members and medical staff. Using this schema, we annotate 100 MIMIC-III discharge summaries and propose baseline models for identifying indirect identifiers. We will release the annotation guidelines, annotation spans (6,199 annotations in total) and the corresponding MIMIC-III document IDs to support further research in this area.</li>
</ul>

<h3>Title: K-Paths: Reasoning over Graph Paths for Drug Repurposing and Drug Interaction Prediction</h3>
<ul>
<li><strong>Authors: </strong>Tassallah Abdullahi, Ioanna Gemou, Nihal V. Nayak, Ghulam Murtaza, Stephen H. Bach, Carsten Eickhoff, Ritambhara Singh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13344">https://arxiv.org/abs/2502.13344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13344">https://arxiv.org/pdf/2502.13344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13344]] K-Paths: Reasoning over Graph Paths for Drug Repurposing and Drug Interaction Prediction(https://arxiv.org/abs/2502.13344)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Drug discovery is a complex and time-intensive process that requires identifying and validating new therapeutic candidates. Computational approaches using large-scale biomedical knowledge graphs (KGs) offer a promising solution to accelerate this process. However, extracting meaningful insights from large-scale KGs remains challenging due to the complexity of graph traversal. Existing subgraph-based methods are tailored to graph neural networks (GNNs), making them incompatible with other models, such as large language models (LLMs). We introduce K-Paths, a retrieval framework that extracts structured, diverse, and biologically meaningful paths from KGs. Integrating these paths enables LLMs and GNNs to effectively predict unobserved drug-drug and drug-disease interactions. Unlike traditional path-ranking approaches, K-Paths retrieves and transforms paths into a structured format that LLMs can directly process, facilitating explainable reasoning. K-Paths employs a diversity-aware adaptation of Yen's algorithm to retrieve the K shortest loopless paths between entities in an interaction query, prioritizing biologically relevant and diverse relationships. Our experiments on benchmark datasets show that K-Paths improves the zero-shot performance of Llama 8.1B's F1-score by 12.45 points on drug repurposing and 13.42 points on interaction severity prediction. We also show that Llama 70B achieves F1-score gains of 6.18 and 8.46 points, respectively. K-Paths also improves the supervised training efficiency of EmerGNN, a state-of-the-art GNN, by reducing KG size by 90% while maintaining strong predictive performance. Beyond its scalability and efficiency, K-Paths uniquely bridges the gap between KGs and LLMs, providing explainable rationales for predicted interactions. These capabilities show that K-Paths is a valuable tool for efficient data-driven drug discovery.</li>
</ul>

<h3>Title: Secure and Efficient Watermarking for Latent Diffusion Models in Model Distribution Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Liangqi Lei, Keke Gai, Jing Yu, Liehuang Zhu, Qi Wu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13345">https://arxiv.org/abs/2502.13345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13345">https://arxiv.org/pdf/2502.13345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13345]] Secure and Efficient Watermarking for Latent Diffusion Models in Model Distribution Scenarios(https://arxiv.org/abs/2502.13345)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust, watermark, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Latent diffusion models have exhibited considerable potential in generative tasks. Watermarking is considered to be an alternative to safeguard the copyright of generative models and prevent their misuse. However, in the context of model distribution scenarios, the accessibility of models to large scale of model users brings new challenges to the security, efficiency and robustness of existing watermark solutions. To address these issues, we propose a secure and efficient watermarking solution. A new security mechanism is designed to prevent watermark leakage and watermark escape, which considers watermark randomness and watermark-model association as two constraints for mandatory watermark injection. To reduce the time cost of training the security module, watermark injection and the security mechanism are decoupled, ensuring that fine-tuning VAE only accomplishes the security mechanism without the burden of learning watermark patterns. A watermark distribution-based verification strategy is proposed to enhance the robustness against diverse attacks in the model distribution scenarios. Experimental results prove that our watermarking consistently outperforms existing six baselines on effectiveness and robustness against ten image processing attacks and adversarial attacks, while enhancing security in the distribution scenarios.</li>
</ul>

<h3>Title: Craw4LLM: Efficient Web Crawling for LLM Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Shi Yu, Zhiyuan Liu, Chenyan Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13347">https://arxiv.org/abs/2502.13347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13347">https://arxiv.org/pdf/2502.13347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13347]] Craw4LLM: Efficient Web Crawling for LLM Pretraining(https://arxiv.org/abs/2502.13347)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Web crawl is a main source of large language models' (LLMs) pretraining data, but the majority of crawled web pages are discarded in pretraining due to low data quality. This paper presents Crawl4LLM, an efficient web crawling method that explores the web graph based on the preference of LLM pretraining. Specifically, it leverages the influence of a webpage in LLM pretraining as the priority score of the web crawler's scheduler, replacing the standard graph connectivity based priority. Our experiments on a web graph containing 900 million webpages from a commercial search engine's index demonstrate the efficiency of Crawl4LLM in obtaining high-quality pretraining data. With just 21% URLs crawled, LLMs pretrained on Crawl4LLM data reach the same downstream performances of previous crawls, significantly reducing the crawling waste and alleviating the burdens on websites. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: Event Segmentation Applications in Large Language Model Enabled Automated Recall Assessments</h3>
<ul>
<li><strong>Authors: </strong>Ryan A. Panela (1,2), Alex J. Barnett (2,3), Morgan D. Barense (1,2), Bjrn Herrmann (1,2) ((1) Rotman Research Institute, Baycrest Academy for Research and Education, (2) Department of Psychology, University of Toronto, (3) Department of Neurology and Neurosurgery, Montreal Neurological Institute and Hospital, McGill University)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13349">https://arxiv.org/abs/2502.13349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13349">https://arxiv.org/pdf/2502.13349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13349]] Event Segmentation Applications in Large Language Model Enabled Automated Recall Assessments(https://arxiv.org/abs/2502.13349)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Understanding how individuals perceive and recall information in their natural environments is critical to understanding potential failures in perception (e.g., sensory loss) and memory (e.g., dementia). Event segmentation, the process of identifying distinct events within dynamic environments, is central to how we perceive, encode, and recall experiences. This cognitive process not only influences moment-to-moment comprehension but also shapes event specific memory. Despite the importance of event segmentation and event memory, current research methodologies rely heavily on human judgements for assessing segmentation patterns and recall ability, which are subjective and time-consuming. A few approaches have been introduced to automate event segmentation and recall scoring, but validity with human responses and ease of implementation require further advancements. To address these concerns, we leverage Large Language Models (LLMs) to automate event segmentation and assess recall, employing chat completion and text-embedding models, respectively. We validated these models against human annotations and determined that LLMs can accurately identify event boundaries, and that human event segmentation is more consistent with LLMs than among humans themselves. Using this framework, we advanced an automated approach for recall assessments which revealed semantic similarity between segmented narrative events and participant recall can estimate recall performance. Our findings demonstrate that LLMs can effectively simulate human segmentation patterns and provide recall evaluations that are a scalable alternative to manual scoring. This research opens novel avenues for studying the intersection between perception, memory, and cognitive impairment using methodologies driven by artificial intelligence.</li>
</ul>

<h3>Title: Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text Modifications</h3>
<ul>
<li><strong>Authors: </strong>Yiming Zeng, Wanhao Yu, Zexin Li, Tao Ren, Yu Ma, Jinghan Cao, Xiyan Chen, Tingting Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13358">https://arxiv.org/abs/2502.13358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13358">https://arxiv.org/pdf/2502.13358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13358]] Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text Modifications(https://arxiv.org/abs/2502.13358)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have transformed natural language processing, yet they still struggle with direct text editing tasks that demand precise, context-aware modifications. While models like ChatGPT excel in text generation and analysis, their editing abilities often fall short, addressing only superficial issues rather than deeper structural or logical inconsistencies. In this work, we introduce a dual approach to enhance LLMs editing performance. First, we present InstrEditBench, a high-quality benchmark dataset comprising over 20,000 structured editing tasks spanning Wiki articles, LaTeX documents, code, and database Domain-specific Languages (DSL). InstrEditBench is generated using an innovative automated workflow that accurately identifies and evaluates targeted edits, ensuring that modifications adhere strictly to specified instructions without altering unrelated content. Second, we propose FineEdit, a specialized model trained on this curated benchmark. Experimental results demonstrate that FineEdit achieves significant improvements around {10\%} compared with Gemini on direct editing tasks, convincingly validating its effectiveness.</li>
</ul>

<h3>Title: RGAR: Recurrence Generation-augmented Retrieval for Factual-aware Medical Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Sichu Liang, Linhai Zhang, Hongyu Zhu, Wenwen Wang, Yulan He, Deyu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13361">https://arxiv.org/abs/2502.13361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13361">https://arxiv.org/pdf/2502.13361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13361]] RGAR: Recurrence Generation-augmented Retrieval for Factual-aware Medical Question Answering(https://arxiv.org/abs/2502.13361)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Medical question answering requires extensive access to specialized conceptual knowledge. The current paradigm, Retrieval-Augmented Generation (RAG), acquires expertise medical knowledge through large-scale corpus retrieval and uses this knowledge to guide a general-purpose large language model (LLM) for generating answers. However, existing retrieval approaches often overlook the importance of factual knowledge, which limits the relevance of retrieved conceptual knowledge and restricts its applicability in real-world scenarios, such as clinical decision-making based on Electronic Health Records (EHRs). This paper introduces RGAR, a recurrence generation-augmented retrieval framework that retrieves both relevant factual and conceptual knowledge from dual sources (i.e., EHRs and the corpus), allowing them to interact and refine each another. Through extensive evaluation across three factual-aware medical question answering benchmarks, RGAR establishes a new state-of-the-art performance among medical RAG systems. Notably, the Llama-3.1-8B-Instruct model with RGAR surpasses the considerably larger, RAG-enhanced GPT-3.5. Our findings demonstrate the benefit of extracting factual knowledge for retrieval, which consistently yields improved generation quality.</li>
</ul>

<h3>Title: Reducing Hallucinations in Language Model-based SPARQL Query Generation Using Post-Generation Memory Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Aditya Sharma, Luis Lara, Amal Zouaq, Christopher J. Pal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13369">https://arxiv.org/abs/2502.13369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13369">https://arxiv.org/pdf/2502.13369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13369]] Reducing Hallucinations in Language Model-based SPARQL Query Generation Using Post-Generation Memory Retrieval(https://arxiv.org/abs/2502.13369)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The ability to generate SPARQL queries from natural language questions is crucial for ensuring efficient and accurate retrieval of structured data from knowledge graphs (KG). While large language models (LLMs) have been widely adopted for SPARQL query generation, they are often susceptible to hallucinations and out-of-distribution errors when producing KG elements like Uniform Resource Identifiers (URIs) based on internal parametric knowledge. This often results in content that appears plausible but is factually incorrect, posing significant challenges for their use in real-world information retrieval (IR) applications. This has led to increased research aimed at detecting and mitigating such errors. In this paper, we introduce PGMR (Post-Generation Memory Retrieval), a modular framework that incorporates a non-parametric memory module to retrieve KG elements and enhance LLM-based SPARQL query generation. Our experimental results indicate that PGMR consistently delivers strong performance across diverse datasets, data distributions, and LLMs. Notably, PGMR significantly mitigates URI hallucinations, nearly eliminating the problem in several scenarios.</li>
</ul>

<h3>Title: Quantum Recurrent Neural Networks with Encoder-Decoder for Time-Dependent Partial Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Yuan Chen, Abdul Khaliq, Khaled M. Furati</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13370">https://arxiv.org/abs/2502.13370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13370">https://arxiv.org/pdf/2502.13370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13370]] Quantum Recurrent Neural Networks with Encoder-Decoder for Time-Dependent Partial Differential Equations(https://arxiv.org/abs/2502.13370)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Nonlinear time-dependent partial differential equations are essential in modeling complex phenomena across diverse fields, yet they pose significant challenges due to their computational complexity, especially in higher dimensions. This study explores Quantum Recurrent Neural Networks within an encoder-decoder framework, integrating Variational Quantum Circuits into Gated Recurrent Units and Long Short-Term Memory networks. Using this architecture, the model efficiently compresses high-dimensional spatiotemporal data into a compact latent space, facilitating more efficient temporal evolution. We evaluate the algorithms on the Hamilton-Jacobi-Bellman equation, Burgers' equation, the Gray-Scott reaction-diffusion system, and the three dimensional Michaelis-Menten reaction-diffusion equation. The results demonstrate the superior performance of the quantum-based algorithms in capturing nonlinear dynamics, handling high-dimensional spaces, and providing stable solutions, highlighting their potential as an innovative tool in solving challenging and complex systems.</li>
</ul>

<h3>Title: Task-agnostic Prompt Compression with Context-aware Sentence Embedding and Reward-guided Task Descriptor</h3>
<ul>
<li><strong>Authors: </strong>Barys Liskavets, Shuvendu Roy, Maxim Ushakov, Mark Klibanov, Ali Etemad, Shane Luke</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13374">https://arxiv.org/abs/2502.13374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13374">https://arxiv.org/pdf/2502.13374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13374]] Task-agnostic Prompt Compression with Context-aware Sentence Embedding and Reward-guided Task Descriptor(https://arxiv.org/abs/2502.13374)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rise of Large Language Models (LLMs) has led to significant interest in prompt compression, a technique aimed at reducing the length of input prompts while preserving critical information. However, the prominent approaches in prompt compression often require explicit questions or handcrafted templates for compression, limiting their generalizability. We propose Task-agnostic Prompt Compression (TPC), a novel framework that generalizes compression across tasks and domains without requiring input questions or templates. TPC generates a context-relevant task description using a task descriptor trained on a curated dataset of context and query pairs, and fine-tuned via reinforcement learning with a reward function designed to capture the most relevant information. The task descriptor is then utilized to compute the relevance of each sentence in the prompt to generate the compressed prompt. We introduce 3 model sizes (Base, Large, and Huge), where the largest model outperforms the existing state-of-the-art methods on LongBench and ZeroSCROLLS benchmarks, and our smallest model performs comparable to the existing solutions while being considerably smaller.</li>
</ul>

<h3>Title: AutoTEE: Automated Migration and Protection of Programs in Trusted Execution Environments</h3>
<ul>
<li><strong>Authors: </strong>Ruidong Han, Zhou Yang, Chengyan Ma, Ye Liu, Yuqing Niu, Siqi Ma, Debin Gao, David Lo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13379">https://arxiv.org/abs/2502.13379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13379">https://arxiv.org/pdf/2502.13379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13379]] AutoTEE: Automated Migration and Protection of Programs in Trusted Execution Environments(https://arxiv.org/abs/2502.13379)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, large language model</a></li>
<li><strong>Abstract: </strong>Trusted Execution Environments (TEEs) isolate a special space within a device's memory that is not accessible to the normal world (also known as Untrusted Environment), even when the device is compromised. Thus, developers can utilize TEEs to provide strong security guarantees for their programs, making sensitive operations like encrypted data storage, fingerprint verification, and remote attestation protected from malicious attacks. Despite the strong protections offered by TEEs, adapting existing programs to leverage such security guarantees is non-trivial, often requiring extensive domain knowledge and manual intervention, which makes TEEs less accessible to developers. This motivates us to design AutoTEE, the first Large Language Model (LLM)-enabled approach that can automatically identify, partition, transform, and port sensitive functions into TEEs with minimal developer intervention. By manually reviewing 68 repositories, we constructed a benchmark dataset consisting of 385 sensitive functions eligible for transformation, on which AutoTEE achieves a high F1 score of 0.91. AutoTEE effectively transforms these sensitive functions into their TEE-compatible counterparts, achieving success rates of 90\% and 83\% for Java and Python, respectively. We further provide a mechanism to automatically port the transformed code to different TEE platforms, including Intel SGX and AMD SEV, demonstrating that the transformed programs run successfully and correctly on these platforms.</li>
</ul>

<h3>Title: MM-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought Verification</h3>
<ul>
<li><strong>Authors: </strong>Linzhuang Sun, Hao Liang, Jingxuan Wei, Bihui Yu, Tianpeng Li, Fan Yang, Zenan Zhou, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13383">https://arxiv.org/abs/2502.13383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13383">https://arxiv.org/pdf/2502.13383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13383]] MM-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought Verification(https://arxiv.org/abs/2502.13383)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>According to the Test-Time Scaling, the integration of External Slow-Thinking with the Verify mechanism has been demonstrated to enhance multi-round reasoning in large language models (LLMs). However, in the multimodal (MM) domain, there is still a lack of a strong MM-Verifier. In this paper, we introduce MM-Verifier and MM-Reasoner to enhance multimodal reasoning through longer inference and more robust verification. First, we propose a two-step MM verification data synthesis method, which combines a simulation-based tree search with verification and uses rejection sampling to generate high-quality Chain-of-Thought (COT) data. This data is then used to fine-tune the verification model, MM-Verifier. Additionally, we present a more efficient method for synthesizing MMCOT data, bridging the gap between text-based and multimodal reasoning. The synthesized data is used to fine-tune MM-Reasoner. Our MM-Verifier outperforms all larger models on the MathCheck, MathVista, and MathVerse benchmarks. Moreover, MM-Reasoner demonstrates strong effectiveness and scalability, with performance improving as data size increases. Finally, our approach achieves strong performance when combining MM-Reasoner and MM-Verifier, reaching an accuracy of 65.3 on MathVista, surpassing GPT-4o (63.8) with 12 rollouts.</li>
</ul>

<h3>Title: SNN-Driven Multimodal Human Action Recognition via Event Camera and Skeleton Data Fusion</h3>
<ul>
<li><strong>Authors: </strong>Naichuan Zheng, Hailun Xia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13385">https://arxiv.org/abs/2502.13385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13385">https://arxiv.org/pdf/2502.13385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13385]] SNN-Driven Multimodal Human Action Recognition via Event Camera and Skeleton Data Fusion(https://arxiv.org/abs/2502.13385)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Multimodal human action recognition based on RGB and skeleton data fusion, while effective, is constrained by significant limitations such as high computational complexity, excessive memory consumption, and substantial energy demands, particularly when implemented with Artificial Neural Networks (ANN). These limitations restrict its applicability in resource-constrained scenarios. To address these challenges, we propose a novel Spiking Neural Network (SNN)-driven framework for multimodal human action recognition, utilizing event camera and skeleton data. Our framework is centered on two key innovations: (1) a novel multimodal SNN architecture that employs distinct backbone networks for each modality-an SNN-based Mamba for event camera data and a Spiking Graph Convolutional Network (SGN) for skeleton data-combined with a spiking semantic extraction module to capture deep semantic representations; and (2) a pioneering SNN-based discretized information bottleneck mechanism for modality fusion, which effectively balances the preservation of modality-specific semantics with efficient information compression. To validate our approach, we propose a novel method for constructing a multimodal dataset that integrates event camera and skeleton data, enabling comprehensive evaluation. Extensive experiments demonstrate that our method achieves superior performance in both recognition accuracy and energy efficiency, offering a promising solution for practical applications.</li>
</ul>

<h3>Title: Flow-based generative models as iterative algorithms in probability space</h3>
<ul>
<li><strong>Authors: </strong>Yao Xie, Xiuyuan Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13394">https://arxiv.org/abs/2502.13394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13394">https://arxiv.org/pdf/2502.13394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13394]] Flow-based generative models as iterative algorithms in probability space(https://arxiv.org/abs/2502.13394)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI (GenAI) has revolutionized data-driven modeling by enabling the synthesis of high-dimensional data across various applications, including image generation, language modeling, biomedical signal processing, and anomaly detection. Flow-based generative models provide a powerful framework for capturing complex probability distributions, offering exact likelihood estimation, efficient sampling, and deterministic transformations between distributions. These models leverage invertible mappings governed by Ordinary Differential Equations (ODEs), enabling precise density estimation and likelihood evaluation. This tutorial presents an intuitive mathematical framework for flow-based generative models, formulating them as neural network-based representations of continuous probability densities. We explore key theoretical principles, including the Wasserstein metric, gradient flows, and density evolution governed by ODEs, to establish convergence guarantees and bridge empirical advancements with theoretical insights. By providing a rigorous yet accessible treatment, we aim to equip researchers and practitioners with the necessary tools to effectively apply flow-based generative models in signal processing and machine learning.</li>
</ul>

<h3>Title: Prompting a Weighting Mechanism into LLM-as-a-Judge in Two-Step: A Case Study</h3>
<ul>
<li><strong>Authors: </strong>Wenwen Xie, Gray Gwizdz, Dongji Feng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13396">https://arxiv.org/abs/2502.13396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13396">https://arxiv.org/pdf/2502.13396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13396]] Prompting a Weighting Mechanism into LLM-as-a-Judge in Two-Step: A Case Study(https://arxiv.org/abs/2502.13396)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) have emerged as promising tools for evaluating Natural Language Generation (NLG) tasks, their effectiveness is limited by their inability to appropriately weigh the importance of different topics, often overemphasizing minor details while undervaluing critical information, leading to misleading assessments. Our work proposes an efficient prompt design mechanism to address this specific limitation and provide a case study. Through strategic prompt engineering that incorporates explicit importance weighting mechanisms, we enhance using LLM-as-a-Judge ability to prioritize relevant information effectively, as demonstrated by an average improvement of 6% in the Human Alignment Rate (HAR) metric.</li>
</ul>

<h3>Title: $\mathtt{GeLLM^3O}$: Generalizing Large Language Models for Multi-property Molecule Optimization</h3>
<ul>
<li><strong>Authors: </strong>Vishal Dey, Xiao Hu, Xia Ning</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, physics.chem-ph, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13398">https://arxiv.org/abs/2502.13398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13398">https://arxiv.org/pdf/2502.13398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13398]] $\mathtt{GeLLM^3O}$: Generalizing Large Language Models for Multi-property Molecule Optimization(https://arxiv.org/abs/2502.13398)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite recent advancements, most computational methods for molecule optimization are constrained to single- or double-property optimization tasks and suffer from poor scalability and generalizability to novel optimization tasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable out-of-domain generalizability to novel tasks. To demonstrate LLMs' potential for molecule optimization, we introduce $\mathtt{MoMUInstruct}$, the first high-quality instruction-tuning dataset specifically focused on complex multi-property molecule optimization tasks. Leveraging $\mathtt{MoMUInstruct}$, we develop $\mathtt{GeLLM^3O}$s, a series of instruction-tuned LLMs for molecule optimization. Extensive evaluations across 5 in-domain and 5 out-of-domain tasks demonstrate that $\mathtt{GeLLM^3O}$s consistently outperform state-of-the-art baselines. $\mathtt{GeLLM^3O}$s also exhibit outstanding zero-shot generalization to unseen tasks, significantly outperforming powerful closed-source LLMs. Such strong generalizability demonstrates the tremendous potential of $\mathtt{GeLLM^3O}$s as foundational models for molecule optimization, thereby tackling novel optimization tasks without resource-intensive retraining. $\mathtt{MoMUInstruct}$, models, and code are accessible through this https URL.</li>
</ul>

<h3>Title: MaizeEar-SAM: Zero-Shot Maize Ear Phenotyping</h3>
<ul>
<li><strong>Authors: </strong>Hossein Zaremehrjerdi, Lisa Coffey, Talukder Jubery, Huyu Liu, Jon Turkus, Kyle Linders, James C. Schnable, Patrick S. Schnable, Baskar Ganapathysubramanian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13399">https://arxiv.org/abs/2502.13399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13399">https://arxiv.org/pdf/2502.13399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13399]] MaizeEar-SAM: Zero-Shot Maize Ear Phenotyping(https://arxiv.org/abs/2502.13399)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Quantifying the variation in yield component traits of maize (Zea mays L.), which together determine the overall productivity of this globally important crop, plays a critical role in plant genetics research, plant breeding, and the development of improved farming practices. Grain yield per acre is calculated by multiplying the number of plants per acre, ears per plant, number of kernels per ear, and the average kernel weight. The number of kernels per ear is determined by the number of kernel rows per ear multiplied by the number of kernels per row. Traditional manual methods for measuring these two traits are time-consuming, limiting large-scale data collection. Recent automation efforts using image processing and deep learning encounter challenges such as high annotation costs and uncertain generalizability. We tackle these issues by exploring Large Vision Models for zero-shot, annotation-free maize kernel segmentation. By using an open-source large vision model, the Segment Anything Model (SAM), we segment individual kernels in RGB images of maize ears and apply a graph-based algorithm to calculate the number of kernels per row. Our approach successfully identifies the number of kernels per row across a wide range of maize ears, showing the potential of zero-shot learning with foundation vision models combined with image processing techniques to improve automation and reduce subjectivity in agronomic data collection. All our code is open-sourced to make these affordable phenotyping methods accessible to everyone.</li>
</ul>

<h3>Title: CipherGuard: Compiler-aided Mitigation against Ciphertext Side-channel Attacks</h3>
<ul>
<li><strong>Authors: </strong>Ke Jiang, Sen Deng, Yinshuai Li, Shuai Wang, Tianwei Zhang, Yinqian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13401">https://arxiv.org/abs/2502.13401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13401">https://arxiv.org/pdf/2502.13401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13401]] CipherGuard: Compiler-aided Mitigation against Ciphertext Side-channel Attacks(https://arxiv.org/abs/2502.13401)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Cryptographic implementations bolster security against timing side-channel attacks by integrating constant-time components. However, the new ciphertext side channels resulting from the deterministic memory encryption in Trusted Execution Environments (TEEs), enable ciphertexts to manifest identifiable patterns when being sequentially written to the same memory address. Attackers with read access to encrypted memory in TEEs can potentially deduce plaintexts by analyzing these changing ciphertext patterns. In this paper, we design CipherGuard, a compiler-aided mitigation methodology to counteract ciphertext side channels with high efficiency and security. CipherGuard is based on the LLVM ecosystem, and encompasses multiple mitigation strategies, including software-based probabilistic encryption and secret-aware register allocation. Through a comprehensive evaluation, we demonstrate that CipherGuard can strengthen the security of various cryptographic implementations more efficiently than existing state-of-the-art defense mechanism, i.e., CipherFix.</li>
</ul>

<h3>Title: JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework</h3>
<ul>
<li><strong>Authors: </strong>Ziyuan Liu, Ruifei Zhu, Long Gao, Yuanxiu Zhou, Jingyu Ma, Yuantao Gu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13407">https://arxiv.org/abs/2502.13407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13407">https://arxiv.org/pdf/2502.13407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13407]] JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework(https://arxiv.org/abs/2502.13407)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning has achieved significant success in the field of remote sensing image change detection (CD), yet two major challenges remain: the scarcity of sub-meter, all-inclusive open-source CD datasets, and the difficulty of achieving consistent and satisfactory detection results across images with varying change areas. To address these issues, we introduce the JL1-CD dataset, which contains 5,000 pairs of 512 x 512 pixel images with a resolution of 0.5 to 0.75 meters. Additionally, we propose a multi-teacher knowledge distillation (MTKD) framework for CD. Experimental results on the JL1-CD and SYSU-CD datasets demonstrate that the MTKD framework significantly improves the performance of CD models with various network architectures and parameter sizes, achieving new state-of-the-art results. The code is available at this https URL.</li>
</ul>

<h3>Title: Indifferential Privacy: A New Paradigm and Its Applications to Optimal Matching in Dark Pool Auctions</h3>
<ul>
<li><strong>Authors: </strong>Antigoni Polychroniadou, T.-H. Hubert Chan, Adya Agrawal</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13415">https://arxiv.org/abs/2502.13415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13415">https://arxiv.org/pdf/2502.13415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13415]] Indifferential Privacy: A New Paradigm and Its Applications to Optimal Matching in Dark Pool Auctions(https://arxiv.org/abs/2502.13415)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Public exchanges like the New York Stock Exchange and NASDAQ act as auctioneers in a public double auction system, where buyers submit their highest bids and sellers offer their lowest asking prices, along with the number of shares (volume) they wish to trade. The auctioneer matches compatible orders and executes the trades when a match is found. However, auctioneers involved in high-volume exchanges, such as dark pools, may not always be reliable. They could exploit their position by engaging in practices like front-running or face significant conflicts of interest, i.e., ethical breaches that have frequently resulted in hefty fines and regulatory scrutiny within the financial industry. Previous solutions, based on the use of fully homomorphic encryption (Asharov et al., AAMAS 2020), encrypt orders ensuring that information is revealed only when a match occurs. However, this approach introduces significant computational overhead, making it impractical for high-frequency trading environments such as dark pools. In this work, we propose a new system based on differential privacy combined with lightweight encryption, offering an efficient and practical solution that mitigates the risks of an untrustworthy auctioneer. Specifically, we introduce a new concept called Indifferential Privacy, which can be of independent interest, where a user is indifferent to whether certain information is revealed after some special event, unlike standard differential privacy. For example, in an auction, it's reasonable to disclose the true volume of a trade once all of it has been matched. Moreover, our new concept of Indifferential Privacy allows for maximum matching, which is impossible with conventional differential privacy.</li>
</ul>

<h3>Title: Detecting LLM Fact-conflicting Hallucinations Enhanced by Temporal-logic-based Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Ningke Li, Yahui Song, Kailong Wang, Yuekang Li, Ling Shi, Yi Liu, Haoyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13416">https://arxiv.org/abs/2502.13416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13416">https://arxiv.org/pdf/2502.13416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13416]] Detecting LLM Fact-conflicting Hallucinations Enhanced by Temporal-logic-based Reasoning(https://arxiv.org/abs/2502.13416)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) face the challenge of hallucinations -- outputs that seem coherent but are actually incorrect. A particularly damaging type is fact-conflicting hallucination (FCH), where generated content contradicts established facts. Addressing FCH presents three main challenges: 1) Automatically constructing and maintaining large-scale benchmark datasets is difficult and resource-intensive; 2) Generating complex and efficient test cases that the LLM has not been trained on -- especially those involving intricate temporal features -- is challenging, yet crucial for eliciting hallucinations; and 3) Validating the reasoning behind LLM outputs is inherently difficult, particularly with complex logical relationships, as it requires transparency in the model's decision-making process. This paper presents Drowzee, an innovative end-to-end metamorphic testing framework that utilizes temporal logic to identify fact-conflicting hallucinations (FCH) in large language models (LLMs). Drowzee builds a comprehensive factual knowledge base by crawling sources like Wikipedia and uses automated temporal-logic reasoning to convert this knowledge into a large, extensible set of test cases with ground truth answers. LLMs are tested using these cases through template-based prompts, which require them to generate both answers and reasoning steps. To validate the reasoning, we propose two semantic-aware oracles that compare the semantic structure of LLM outputs to the ground truths. Across nine LLMs in nine different knowledge domains, experimental results show that Drowzee effectively identifies rates of non-temporal-related hallucinations ranging from 24.7% to 59.8%, and rates of temporal-related hallucinations ranging from 16.7% to 39.2%.</li>
</ul>

<h3>Title: RLTHF: Targeted Human Feedback for LLM Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yifei Xu, Tusher Chakraborty, Emre Kcman, Bibek Aryal, Eduardo Rodrigues, Srinagesh Sharma, Roberto Estevao, Maria Angels de Luis Balaguer, Jessica Wolk, Rafael Padilha, Leonardo Nunes, Shobana Balakrishnan, Songwu Lu, Ranveer Chandra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13417">https://arxiv.org/abs/2502.13417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13417">https://arxiv.org/pdf/2502.13417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13417]] RLTHF: Targeted Human Feedback for LLM Alignment(https://arxiv.org/abs/2502.13417)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) to align with user preferences is challenging due to the high cost of quality human annotations in Reinforcement Learning from Human Feedback (RLHF) and the generalizability limitations of AI Feedback. To address these challenges, we propose RLTHF, a human-AI hybrid framework that combines LLM-based initial alignment with selective human annotations to achieve full-human annotation alignment with minimal effort. RLTHF identifies hard-to-annotate samples mislabeled by LLMs using a reward model's reward distribution and iteratively enhances alignment by integrating strategic human corrections while leveraging LLM's correctly labeled samples. Evaluations on HH-RLHF and TL;DR datasets show that RLTHF reaches full-human annotation-level alignment with only 6-7% of the human annotation effort. Furthermore, models trained on RLTHF's curated datasets for downstream tasks outperform those trained on fully human-annotated datasets, underscoring the effectiveness of RLTHF's strategic data curation.</li>
</ul>

<h3>Title: TabSD: Large Free-Form Table Question Answering with SQL-Based Table Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Wang, Junhao Gan, Jianzhong Qi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13422">https://arxiv.org/abs/2502.13422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13422">https://arxiv.org/pdf/2502.13422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13422]] TabSD: Large Free-Form Table Question Answering with SQL-Based Table Decomposition(https://arxiv.org/abs/2502.13422)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Question answering on free-form tables (TableQA) is challenging due to the absence of predefined schemas and the presence of noise in large tables. While Large Language Models (LLMs) have shown promise in TableQA, they struggle with large free-form tables and noise sensitivity. To address these challenges, we propose TabSD, a SQL-based decomposition model that enhances LLMs' ability to process large free-form tables. TabSD generates SQL queries to guide the table decomposition, remove noise, and processes sub-tables for better answer generation. Additionally, SQL Verifier refines SQL outputs to enhance decomposition accuracy. We introduce two TableQA datasets with large free-form tables, SLQA and SEQA, which consist solely of large free-form tables and will be publicly available. Experimental results on four benchmark datasets demonstrate that TABSD outperforms the best-existing baseline models by 23.07%, 2.84%, 23.24% and 9.32% in accuracy, respectively, highlighting its effectiveness in handling large and noisy free-form tables.</li>
</ul>

<h3>Title: MCTS-KBQA: Monte Carlo Tree Search for Knowledge Base Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Guanming Xiong, Haochen Li, Wen Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13428">https://arxiv.org/abs/2502.13428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13428">https://arxiv.org/pdf/2502.13428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13428]] MCTS-KBQA: Monte Carlo Tree Search for Knowledge Base Question Answering(https://arxiv.org/abs/2502.13428)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study explores how to enhance the reasoning capabilities of large language models (LLMs) in knowledge base question answering (KBQA) by leveraging Monte Carlo Tree Search (MCTS). Semantic parsing-based KBQA methods are particularly challenging as these approaches require locating elements from knowledge bases and generating logical forms, demanding not only extensive annotated data but also strong reasoning capabilities. Although recent approaches leveraging LLMs as agents have demonstrated considerable potential, these studies are inherently constrained by their linear decision-making processes. To address this limitation, we propose a MCTS-based framework that enhances LLMs' reasoning capabilities through tree search methodology. We design a carefully designed step-wise reward mechanism that requires only direct prompting of open-source instruction LLMs without additional fine-tuning. Experimental results demonstrate that our approach significantly outperforms linear decision-making methods, particularly in low-resource scenarios. Additionally, we contribute new data resources to the KBQA community by annotating intermediate reasoning processes for existing question-SPARQL datasets using distant supervision. Experimental results on the extended dataset demonstrate that our method achieves comparable performance to fully supervised models while using significantly less training data.</li>
</ul>

<h3>Title: The Self-Improvement Paradox: Can Language Models Bootstrap Reasoning Capabilities without External Scaffolding?</h3>
<ul>
<li><strong>Authors: </strong>Yutao Sun, Mingshuai Chen, Tiancheng Zhao, Ruochen Xu, Zilun Zhang, Jianwei Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13441">https://arxiv.org/abs/2502.13441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13441">https://arxiv.org/pdf/2502.13441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13441]] The Self-Improvement Paradox: Can Language Models Bootstrap Reasoning Capabilities without External Scaffolding?(https://arxiv.org/abs/2502.13441)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Self-improving large language models (LLMs) -- i.e., to improve the performance of an LLM by fine-tuning it with synthetic data generated by itself -- is a promising way to advance the capabilities of LLMs while avoiding extensive supervision. Existing approaches to self-improvement often rely on external supervision signals in the form of seed data and/or assistance from third-party models. This paper presents Crescent -- a simple yet effective framework for generating high-quality synthetic question-answer data in a fully autonomous manner. Crescent first elicits the LLM to generate raw questions via a bait prompt, then diversifies these questions leveraging a rejection sampling-based self-deduplication, and finally feeds the questions to the LLM and collects the corresponding answers by means of majority voting. We show that Crescent sheds light on the potential of true self-improvement with zero external supervision signals for math reasoning; in particular, Crescent-generated question-answer pairs suffice to (i) improve the reasoning capabilities of an LLM while preserving its general performance (especially in the 0-shot setting); and (ii) distil LLM knowledge to weaker models more effectively than existing methods based on seed-dataset augmentation.</li>
</ul>

<h3>Title: TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Jialin Ouyang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13442">https://arxiv.org/abs/2502.13442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13442">https://arxiv.org/pdf/2502.13442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13442]] TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation(https://arxiv.org/abs/2502.13442)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) now achieve near-human performance on standard math word problem benchmarks (e.g., GSM8K), yet their true reasoning ability remains disputed. A key concern is that models often produce confident, yet unfounded, answers to unanswerable problems. We introduce TreeCut, a synthetic dataset that systematically generates infinite unanswerable math word problems and their answerable counterparts, by representing each question as a tree and removing chosen necessary conditions. Experiments show TreeCut effectively induce hallucinations in large language models, including GPT-4o and o3-mini, with rates of 61% and 42% in their respective worst-case scenarios. Further analysis highlights that deeper or more complex trees, composite item names, and removing necessary condition near the middle of a path all increase the likelihood of hallucinations, underscoring the persistent challenges LLMs face in identifying unanswerable math problems.</li>
</ul>

<h3>Title: Enhancing Chest X-ray Classification through Knowledge Injection in Cross-Modality Learning</h3>
<ul>
<li><strong>Authors: </strong>Yang Yan, Bingqing Yue, Qiaxuan Li, Man Huang, Jingyu Chen, Zhenzhong Lan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13447">https://arxiv.org/abs/2502.13447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13447">https://arxiv.org/pdf/2502.13447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13447]] Enhancing Chest X-ray Classification through Knowledge Injection in Cross-Modality Learning(https://arxiv.org/abs/2502.13447)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The integration of artificial intelligence in medical imaging has shown tremendous potential, yet the relationship between pre-trained knowledge and performance in cross-modality learning remains unclear. This study investigates how explicitly injecting medical knowledge into the learning process affects the performance of cross-modality classification, focusing on Chest X-ray (CXR) images. We introduce a novel Set Theory-based knowledge injection framework that generates captions for CXR images with controllable knowledge granularity. Using this framework, we fine-tune CLIP model on captions with varying levels of medical information. We evaluate the model's performance through zero-shot classification on the CheXpert dataset, a benchmark for CXR classification. Our results demonstrate that injecting fine-grained medical knowledge substantially improves classification accuracy, achieving 72.5\% compared to 49.9\% when using human-generated captions. This highlights the crucial role of domain-specific knowledge in medical cross-modality learning. Furthermore, we explore the influence of knowledge density and the use of domain-specific Large Language Models (LLMs) for caption generation, finding that denser knowledge and specialized LLMs contribute to enhanced performance. This research advances medical image analysis by demonstrating the effectiveness of knowledge injection for improving automated CXR classification, paving the way for more accurate and reliable diagnostic tools.</li>
</ul>

<h3>Title: Interleaved Gibbs Diffusion for Constrained Generation</h3>
<ul>
<li><strong>Authors: </strong>Gautham Govind Anil, Sachin Yadav, Dheeraj Nagaraj, Karthikeyan Shanmugam, Prateek Jain</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13450">https://arxiv.org/abs/2502.13450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13450">https://arxiv.org/pdf/2502.13450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13450]] Interleaved Gibbs Diffusion for Constrained Generation(https://arxiv.org/abs/2502.13450)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling framework for mixed continuous-discrete data, focusing on constrained generation problems. Prior works on discrete and continuous-discrete diffusion models assume factorized denoising distribution for fast generation, which can hinder the modeling of strong dependencies between random variables encountered in constrained generation. IGD moves beyond this by interleaving continuous and discrete denoising algorithms via a discrete time Gibbs sampling type Markov chain. IGD provides flexibility in the choice of denoisers, allows conditional generation via state-space doubling and inference time scaling via the ReDeNoise method. Empirical evaluations on three challenging tasks-solving 3-SAT, generating molecule structures, and generating layouts-demonstrate state-of-the-art performance. Notably, IGD achieves a 7% improvement on 3-SAT out of the box and achieves state-of-the-art results in molecule generation without relying on equivariant diffusion or domain-specific architectures. We explore a wide range of modeling, and interleaving strategies along with hyperparameters in each of these problems.</li>
</ul>

<h3>Title: ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails</h3>
<ul>
<li><strong>Authors: </strong>Xiaofei Wen, Wenxuan Zhou, Wenjie Jacky Mo, Muhao Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13458">https://arxiv.org/abs/2502.13458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13458">https://arxiv.org/pdf/2502.13458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13458]] ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails(https://arxiv.org/abs/2502.13458)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Ensuring the safety of large language models (LLMs) is critical as they are deployed in real-world applications. Existing guardrails rely on rule-based filtering or single-pass classification, limiting their ability to handle nuanced safety violations. To address this, we propose ThinkGuard, a critique-augmented guardrail model that distills knowledge from high-capacity LLMs by generating structured critiques alongside safety labels. Fine-tuned on critique-augmented data, the captured deliberative thinking ability drastically enhances the guardrail's cautiousness and interpretability. Evaluated on multiple safety benchmarks, ThinkGuard achieves the highest average F1 and AUPRC, outperforming all baselines. Compared to LLaMA Guard 3, ThinkGuard improves accuracy by 16.1% and macro F1 by 27.0%. Moreover, it surpasses label-only fine-tuned models, confirming that structured critiques enhance both classification precision and nuanced safety reasoning while maintaining computational efficiency.</li>
</ul>

<h3>Title: Poisoned Source Code Detection in Code Models</h3>
<ul>
<li><strong>Authors: </strong>Ehab Ghannoum, Mohammad Ghafari</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13459">https://arxiv.org/abs/2502.13459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13459">https://arxiv.org/pdf/2502.13459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13459]] Poisoned Source Code Detection in Code Models(https://arxiv.org/abs/2502.13459)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep learning models have gained popularity for conducting various tasks involving source code. However, their black-box nature raises concerns about potential risks. One such risk is a poisoning attack, where an attacker intentionally contaminates the training set with malicious samples to mislead the model's predictions in specific scenarios. To protect source code models from poisoning attacks, we introduce CodeGarrison (CG), a hybrid deep-learning model that relies on code embeddings to identify poisoned code samples. We evaluated CG against the state-of-the-art technique ONION for detecting poisoned samples generated by DAMP, MHM, ALERT, as well as a novel poisoning technique named CodeFooler. Results showed that CG significantly outperformed ONION with an accuracy of 93.5%. We also tested CG's robustness against unknown attacks and achieved an average accuracy of 85.6% in identifying poisoned samples across the four attacks mentioned above.</li>
</ul>

<h3>Title: Estimating Commonsense Plausibility through Semantic Shifts</h3>
<ul>
<li><strong>Authors: </strong>Wanqing Cui, Keping Bi, Jiafeng Guo, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13464">https://arxiv.org/abs/2502.13464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13464">https://arxiv.org/pdf/2502.13464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13464]] Estimating Commonsense Plausibility through Semantic Shifts(https://arxiv.org/abs/2502.13464)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Commonsense plausibility estimation is critical for evaluating language models (LMs), yet existing generative approaches--reliant on likelihoods or verbalized judgments--struggle with fine-grained discrimination. In this paper, we propose ComPaSS, a novel discriminative framework that quantifies commonsense plausibility by measuring semantic shifts when augmenting sentences with commonsense-related information. Plausible augmentations induce minimal shifts in semantics, while implausible ones result in substantial deviations. Evaluations on two types of fine-grained commonsense plausibility estimation tasks across different backbones, including LLMs and vision-language models (VLMs), show that ComPaSS consistently outperforms baselines. It demonstrates the advantage of discriminative approaches over generative methods in fine-grained commonsense plausibility evaluation. Experiments also show that (1) VLMs yield superior performance to LMs, when integrated with ComPaSS, on vision-grounded commonsense tasks. (2) contrastive pre-training sharpens backbone models' ability to capture semantic nuances, thereby further enhancing ComPaSS.</li>
</ul>

<h3>Title: Some Insights of Construction of Feature Graph to Learn Pairwise Feature Interactions with Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Phaphontee Yamchote, Saw Nay Htet Win, Chainarong Amornbunchornvej, Thanapon Noraset</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13471">https://arxiv.org/abs/2502.13471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13471">https://arxiv.org/pdf/2502.13471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13471]] Some Insights of Construction of Feature Graph to Learn Pairwise Feature Interactions with Graph Neural Networks(https://arxiv.org/abs/2502.13471)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Feature interaction is crucial in predictive machine learning models, as it captures the relationships between features that influence model performance. In this work, we focus on pairwise interactions and investigate their importance in constructing feature graphs for Graph Neural Networks (GNNs). Rather than proposing new methods, we leverage existing GNN models and tools to explore the relationship between feature graph structures and their effectiveness in modeling interactions. Through experiments on synthesized datasets, we uncover that edges between interacting features are important for enabling GNNs to model feature interactions effectively. We also observe that including non-interaction edges can act as noise, degrading model performance. Furthermore, we provide theoretical support for sparse feature graph selection using the Minimum Description Length (MDL) principle. We prove that feature graphs retaining only necessary interaction edges yield a more efficient and interpretable representation than complete graphs, aligning with Occam's Razor. Our findings offer both theoretical insights and practical guidelines for designing feature graphs that improve the performance and interpretability of GNN models.</li>
</ul>

<h3>Title: Towards Lightweight, Adaptive and Attribute-Aware Multi-Aspect Controllable Text Generation with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Zhu, Yefeng Liu, Chenyang Lyu, Xue Yang, Guanhua Chen, Longyue Wang, Weihua Luo, Kaifu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13474">https://arxiv.org/abs/2502.13474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13474">https://arxiv.org/pdf/2502.13474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13474]] Towards Lightweight, Adaptive and Attribute-Aware Multi-Aspect Controllable Text Generation with Large Language Models(https://arxiv.org/abs/2502.13474)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-aspect controllable text generation aims to control text generation in attributes from multiple aspects, making it a complex but powerful task in natural language processing. Supervised fine-tuning methods are often employed for this task due to their simplicity and effectiveness. However, they still have some limitations: low rank adaptation (LoRA) only fine-tunes a few parameters and has suboptimal control effects, while full fine-tuning (FFT) requires significant computational resources and is susceptible to overfitting, particularly when data is limited. Moreover, existing works typically train multi-aspect controllable text generation models using only single-aspect annotated data, which results in discrepancies in data distribution; at the same time, accurately generating text with specific attributes is a challenge that requires strong attribute-aware capabilities. To address these limitations, we propose a lightweight, adaptive and attribute-aware framework for multi-aspect controllable text generation. Our framework can dynamically adjust model parameters according to different aspects of data to achieve controllable text generation, aiming to optimize performance across multiple aspects. Experimental results show that our framework outperforms other strong baselines, achieves state-of-the-art performance, adapts well to data discrepancies, and is more accurate in attribute perception.</li>
</ul>

<h3>Title: LLM should think and action as a human</h3>
<ul>
<li><strong>Authors: </strong>Haun Leung, ZiNan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13475">https://arxiv.org/abs/2502.13475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13475">https://arxiv.org/pdf/2502.13475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13475]] LLM should think and action as a human(https://arxiv.org/abs/2502.13475)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>It is popular lately to train large language models to be used as chat assistants, but in the conversation between the user and the chat assistant, there are prompts, require multi-turns between the chat assistant and the user. However, there are a number of issues with the multi-turns conversation: The response of the chat assistant is prone to errors and cannot help users achieve their goals; It is difficult for chat assistant to generate responses with different processes based on actual needs for the same command or request; Chat assistant require the use of tools, but the current approach is not elegant and efficient, and the number of tool calls that can be supported is limited. The main reason for these issues is that large language models do not have the thinking ability as a human, lack the reasoning ability and planning ability, and lack the ability to execute plans. To solve these issues, we propose a thinking method based on a built-in chain of thought: In the multi-turns conversation, for each user prompt, the large language model thinks based on elements such as chat history, thinking context, action calls, memory and knowledge, makes detailed reasoning and planning, and actions according to the plan. We also explored how the large language model enhances thinking ability through this thinking method: Collect training datasets according to the thinking method and fine tune the large language model through supervised learning; Train a consistency reward model and use it as a reward function to fine tune the large language model using reinforcement learning, and the reinforced large language model outputs according to this way of thinking. Our experimental results show that the reasoning ability and planning ability of the large language model are enhanced, and the issues in the multi-turns conversation are solved.</li>
</ul>

<h3>Title: Smoothed Normalization for Efficient Distributed Private Optimization</h3>
<ul>
<li><strong>Authors: </strong>Egor Shulgin, Sarit Khirirat, Peter Richtrik</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.DC, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13482">https://arxiv.org/abs/2502.13482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13482">https://arxiv.org/pdf/2502.13482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13482]] Smoothed Normalization for Efficient Distributed Private Optimization(https://arxiv.org/abs/2502.13482)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning enables training machine learning models while preserving the privacy of participants. Surprisingly, there is no differentially private distributed method for smooth, non-convex optimization problems. The reason is that standard privacy techniques require bounding the participants' contributions, usually enforced via $\textit{clipping}$ of the updates. Existing literature typically ignores the effect of clipping by assuming the boundedness of gradient norms or analyzes distributed algorithms with clipping but ignores DP constraints. In this work, we study an alternative approach via $\textit{smoothed normalization}$ of the updates motivated by its favorable performance in the single-node setting. By integrating smoothed normalization with an error-feedback mechanism, we design a new distributed algorithm $\alpha$-$\sf NormEC$. We prove that our method achieves a superior convergence rate over prior works. By extending $\alpha$-$\sf NormEC$ to the DP setting, we obtain the first differentially private distributed optimization algorithm with provable convergence guarantees. Finally, our empirical results from neural network training indicate robust convergence of $\alpha$-$\sf NormEC$ across different parameter settings.</li>
</ul>

<h3>Title: What are Models Thinking about? Understanding Large Language Model Hallucinations "Psychology" through Model Inner State Analysis</h3>
<ul>
<li><strong>Authors: </strong>Peiran Wang, Yang Liu, Yunfei Lu, Jue Hong, Ye Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13490">https://arxiv.org/abs/2502.13490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13490">https://arxiv.org/pdf/2502.13490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13490]] What are Models Thinking about? Understanding Large Language Model Hallucinations "Psychology" through Model Inner State Analysis(https://arxiv.org/abs/2502.13490)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) systems suffer from the models' unstable ability to generate valid and factual content, resulting in hallucination generation. Current hallucination detection methods heavily rely on out-of-model information sources, such as RAG to assist the detection, thus bringing heavy additional latency. Recently, internal states of LLMs' inference have been widely used in numerous research works, such as prompt injection detection, etc. Considering the interpretability of LLM internal states and the fact that they do not require external information sources, we introduce such states into LLM hallucination detection. In this paper, we systematically analyze different internal states' revealing features during inference forward and comprehensively evaluate their ability in hallucination detection. Specifically, we cut the forward process of a large language model into three stages: understanding, query, generation, and extracting the internal state from these stages. By analyzing these states, we provide a deep understanding of why the hallucinated content is generated and what happened in the internal state of the models. Then, we introduce these internal states into hallucination detection and conduct comprehensive experiments to discuss the advantages and limitations.</li>
</ul>

<h3>Title: Towards Geo-Culturally Grounded LLM Generations</h3>
<ul>
<li><strong>Authors: </strong>Piyawat Lertvittayakumjorn, David Kinney, Vinodkumar Prabhakaran, Donald Martin, Sunipa Dev</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13497">https://arxiv.org/abs/2502.13497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13497">https://arxiv.org/pdf/2502.13497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13497]] Towards Geo-Culturally Grounded LLM Generations(https://arxiv.org/abs/2502.13497)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Generative large language models (LLMs) have been demonstrated to have gaps in diverse, cultural knowledge across the globe. We investigate the effect of retrieval augmented generation and search-grounding techniques on the ability of LLMs to display familiarity with a diverse range of national cultures. Specifically, we compare the performance of standard LLMs, LLMs augmented with retrievals from a bespoke knowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a web search (i.e., search grounding) on a series of cultural familiarity benchmarks. We find that search grounding significantly improves the LLM performance on multiple-choice benchmarks that test propositional knowledge (e.g., the norms, artifacts, and institutions of national cultures), while KB grounding's effectiveness is limited by inadequate knowledge base coverage and a suboptimal retriever. However, search grounding also increases the risk of stereotypical judgments by language models, while failing to improve evaluators' judgments of cultural familiarity in a human evaluation with adequate statistical power. These results highlight the distinction between propositional knowledge about a culture and open-ended cultural fluency when it comes to evaluating the cultural familiarity of generative LLMs.</li>
</ul>

<h3>Title: PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own Deep Neural Net At Inference</h3>
<ul>
<li><strong>Authors: </strong>Burc Gokden</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13502">https://arxiv.org/abs/2502.13502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13502">https://arxiv.org/pdf/2502.13502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13502]] PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own Deep Neural Net At Inference(https://arxiv.org/abs/2502.13502)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We show that Large Language Model from Power Law Decoder Representations (PLDR-LLM) is a foundational model whose deductive outputs are invariant tensors up to a small perturbation. PLDR-LLM learns a singularity condition for the deductive outputs that enable the once-inferred energy-curvature tensor $\mathbf{G}_{LM}$ to replace the deep neural network of power law graph attention (PLGA) generating the deductive outputs at inference. We demonstrate that a cache for $\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in a straightforward manner to improve the inference time. The invariance and generalizable nature of deductive outputs is at a very high fidelity where deductive outputs have same RMSE and determinant values up to 15 decimal places after caching, and zero-shot benchmark scores remain unchanged. Ablation studies show that learned deductive outputs have distinct loss and accuracy characteristics from models pretrained with transferred, randomly initialized or identity tensors as a constant tensor operator and an LLM with scaled-dot product attention (SDPA) is a special case of PLDR-LLM where $\mathbf{G}_{LM}$ is predefined as identity. The observed invariance characteristic introduces a novel asymmetry between training and inference phases with caching. We outline observed common characteristics of the deductive outputs for the learned singularity condition. We provide an implementation of a training and inference framework for PLDR-LLM with KV-cache and G-cache.</li>
</ul>

<h3>Title: Unlocking Multimodal Integration in EHRs: A Prompt Learning Framework for Language and Time Series Fusion</h3>
<ul>
<li><strong>Authors: </strong>Shuai Niu, Jing Ma, Hongzhan Lin, Liang Bai, Zhihua Wang, Wei Bi, Yida Xu, Guo Li, Xian Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13509">https://arxiv.org/abs/2502.13509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13509">https://arxiv.org/pdf/2502.13509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13509]] Unlocking Multimodal Integration in EHRs: A Prompt Learning Framework for Language and Time Series Fusion(https://arxiv.org/abs/2502.13509)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable performance in vision-language tasks, but their application in the medical field remains underexplored, particularly for integrating structured time series data with unstructured clinical notes. In clinical practice, dynamic time series data such as lab test results capture critical temporal patterns, while clinical notes provide rich semantic context. Merging these modalities is challenging due to the inherent differences between continuous signals and discrete text. To bridge this gap, we introduce ProMedTS, a novel self-supervised multimodal framework that employs prompt-guided learning to unify these heterogeneous data types. Our approach leverages lightweight anomaly detection to generate anomaly captions that serve as prompts, guiding the encoding of raw time series data into informative embeddings. These embeddings are aligned with textual representations in a shared latent space, preserving fine-grained temporal nuances alongside semantic insights. Furthermore, our framework incorporates tailored self-supervised objectives to enhance both intra- and inter-modal alignment. We evaluate ProMedTS on disease diagnosis tasks using real-world datasets, and the results demonstrate that our method consistently outperforms state-of-the-art approaches.</li>
</ul>

<h3>Title: Phantom Events: Demystifying the Issues of Log Forgery in Blockchain</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Liu, Yuxin Dong, Ye Liu, Xiapu Luo, Yi Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13513">https://arxiv.org/abs/2502.13513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13513">https://arxiv.org/pdf/2502.13513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13513]] Phantom Events: Demystifying the Issues of Log Forgery in Blockchain(https://arxiv.org/abs/2502.13513)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>With the rapid development of blockchain technology, transaction logs play a central role in various applications, including decentralized exchanges, wallets, cross-chain bridges, and other third-party services. However, these logs, particularly those based on smart contract events, are highly susceptible to manipulation and forgery, creating substantial security risks across the ecosystem. To address this issue, we present the first in-depth security analysis of transaction log forgery in EVM-based blockchains, a phenomenon we term Phantom Events. We systematically model five types of attacks and propose a tool designed to detect event forgery vulnerabilities in smart contracts. Our evaluation demonstrates that our approach outperforms existing tools in identifying potential phantom events. Furthermore, we have successfully identified real-world instances for all five types of attacks across multiple decentralized applications. Finally, we call on community developers to take proactive steps to address these critical security vulnerabilities.</li>
</ul>

<h3>Title: Enhancing Machine Learning Potentials through Transfer Learning across Chemical Elements</h3>
<ul>
<li><strong>Authors: </strong>Sebastien Rcken, Julija Zavadlav</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13522">https://arxiv.org/abs/2502.13522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13522">https://arxiv.org/pdf/2502.13522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13522]] Enhancing Machine Learning Potentials through Transfer Learning across Chemical Elements(https://arxiv.org/abs/2502.13522)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Machine Learning Potentials (MLPs) can enable simulations of ab initio accuracy at orders of magnitude lower computational cost. However, their effectiveness hinges on the availability of considerable datasets to ensure robust generalization across chemical space and thermodynamic conditions. The generation of such datasets can be labor-intensive, highlighting the need for innovative methods to train MLPs in data-scarce scenarios. Here, we introduce transfer learning of potential energy surfaces between chemically similar elements. Specifically, we leverage the trained MLP for silicon to initialize and expedite the training of an MLP for germanium. Utilizing classical force field and ab initio datasets, we demonstrate that transfer learning surpasses traditional training from scratch in force prediction, leading to more stable simulations and improved temperature transferability. These advantages become even more pronounced as the training dataset size decreases. The out-of-target property analysis shows that transfer learning leads to beneficial but sometimes adversarial effects. Our findings demonstrate that transfer learning across chemical elements is a promising technique for developing accurate and numerically stable MLPs, particularly in a data-scarce regime.</li>
</ul>

<h3>Title: MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D Medical Image Analysis</h3>
<ul>
<li><strong>Authors: </strong>Wei Dai, Steven Wang, Jun Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13524">https://arxiv.org/abs/2502.13524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13524">https://arxiv.org/pdf/2502.13524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13524]] MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D Medical Image Analysis(https://arxiv.org/abs/2502.13524)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Efficient evaluation of three-dimensional (3D) medical images is crucial for diagnostic and therapeutic practices in healthcare. Recent years have seen a substantial uptake in applying deep learning and computer vision to analyse and interpret medical images. Traditional approaches, such as convolutional neural networks (CNNs) and vision transformers (ViTs), face significant computational challenges, prompting the need for architectural advancements. Recent efforts have led to the introduction of novel architectures like the ``Mamba'' model as alternative solutions to traditional CNNs or ViTs. The Mamba model excels in the linear processing of one-dimensional data with low computational demands. However, Mamba's potential for 3D medical image analysis remains underexplored and could face significant computational challenges as the dimension increases. This manuscript presents MobileViM, a streamlined architecture for efficient segmentation of 3D medical images. In the MobileViM network, we invent a new dimension-independent mechanism and a dual-direction traversing approach to incorporate with a vision-Mamba-based framework. MobileViM also features a cross-scale bridging technique to improve efficiency and accuracy across various medical imaging modalities. With these enhancements, MobileViM achieves segmentation speeds exceeding 90 frames per second (FPS) on a single graphics processing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster than the state-of-the-art deep learning models for processing 3D images with the same computational resources. In addition, experimental evaluations demonstrate that MobileViM delivers superior performance, with Dice similarity scores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024, ATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses existing models.</li>
</ul>

<h3>Title: AS-GCL: Asymmetric Spectral Augmentation on Graph Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Ruyue Liu, Rong Yin, Yong Liu, Xiaoshuai Hao, Haichao Shi, Can Ma, Weiping Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13525">https://arxiv.org/abs/2502.13525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13525">https://arxiv.org/pdf/2502.13525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13525]] AS-GCL: Asymmetric Spectral Augmentation on Graph Contrastive Learning(https://arxiv.org/abs/2502.13525)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Graph Contrastive Learning (GCL) has emerged as the foremost approach for self-supervised learning on graph-structured data. GCL reduces reliance on labeled data by learning robust representations from various augmented views. However, existing GCL methods typically depend on consistent stochastic augmentations, which overlook their impact on the intrinsic structure of the spectral domain, thereby limiting the model's ability to generalize effectively. To address these limitations, we propose a novel paradigm called AS-GCL that incorporates asymmetric spectral augmentation for graph contrastive learning. A typical GCL framework consists of three key components: graph data augmentation, view encoding, and contrastive loss. Our method introduces significant enhancements to each of these components. Specifically, for data augmentation, we apply spectral-based augmentation to minimize spectral variations, strengthen structural invariance, and reduce noise. With respect to encoding, we employ parameter-sharing encoders with distinct diffusion operators to generate diverse, noise-resistant graph views. For contrastive loss, we introduce an upper-bound loss function that promotes generalization by maintaining a balanced distribution of intra- and inter-class distance. To our knowledge, we are the first to encode augmentation views of the spectral domain using asymmetric encoders. Extensive experiments on eight benchmark datasets across various node-level tasks demonstrate the advantages of the proposed method.</li>
</ul>

<h3>Title: Exploiting Prefix-Tree in Structured Output Interfaces for Enhancing Jailbreak Attacking</h3>
<ul>
<li><strong>Authors: </strong>Yanzeng Li, Yunfan Xiong, Jialun Zhong, Jinchao Zhang, Jie Zhou, Lei Zou</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13527">https://arxiv.org/abs/2502.13527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13527">https://arxiv.org/pdf/2502.13527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13527]] Exploiting Prefix-Tree in Structured Output Interfaces for Enhancing Jailbreak Attacking(https://arxiv.org/abs/2502.13527)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>The rise of Large Language Models (LLMs) has led to significant applications but also introduced serious security threats, particularly from jailbreak attacks that manipulate output generation. These attacks utilize prompt engineering and logit manipulation to steer models toward harmful content, prompting LLM providers to implement filtering and safety alignment strategies. We investigate LLMs' safety mechanisms and their recent applications, revealing a new threat model targeting structured output interfaces, which enable attackers to manipulate the inner logit during LLM generation, requiring only API access permissions. To demonstrate this threat model, we introduce a black-box attack framework called AttackPrefixTree (APT). APT exploits structured output interfaces to dynamically construct attack patterns. By leveraging prefixes of models' safety refusal response and latent harmful outputs, APT effectively bypasses safety measures. Experiments on benchmark datasets indicate that this approach achieves higher attack success rate than existing methods. This work highlights the urgent need for LLM providers to enhance security protocols to address vulnerabilities arising from the interaction between safety patterns and structured outputs.</li>
</ul>

<h3>Title: Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Yang You, Guiming Xie, Xuejian Gong, Kunlong Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13533">https://arxiv.org/abs/2502.13533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13533">https://arxiv.org/pdf/2502.13533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13533]] Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models(https://arxiv.org/abs/2502.13533)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have significantly advanced natural language processing with exceptional task generalization capabilities. Low-Rank Adaption (LoRA) offers a cost-effective fine-tuning solution, freezing the original model parameters and training only lightweight, low-rank adapter matrices. However, the memory footprint of LoRA is largely dominated by the original model parameters. To mitigate this, we propose LoRAM, a memory-efficient LoRA training scheme founded on the intuition that many neurons in over-parameterized LLMs have low training utility but are essential for inference. LoRAM presents a unique twist: it trains on a pruned (small) model to obtain pruned low-rank matrices, which are then recovered and utilized with the original (large) model for inference. Additionally, minimal-cost continual pre-training, performed by the model publishers in advance, aligns the knowledge discrepancy between pruned and original models. Our extensive experiments demonstrate the efficacy of LoRAM across various pruning strategies and downstream tasks. For a model with 70 billion parameters, LoRAM enables training on a GPU with only 20G HBM, replacing an A100-80G GPU for LoRA training and 15 GPUs for full fine-tuning. Specifically, QLoRAM implemented by structured pruning combined with 4-bit quantization, for LLaMA-3.1-70B (LLaMA-2-70B), reduces the parameter storage cost that dominates the memory usage in low-rank matrix training by 15.81$\times$ (16.95$\times$), while achieving dominant performance gains over both the original LLaMA-3.1-70B (LLaMA-2-70B) and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B).</li>
</ul>

<h3>Title: Activation-aware Probe-Query: Effective Key-Value Retrieval for Long-Context LLMs Inference</h3>
<ul>
<li><strong>Authors: </strong>Qingfa Xiao, Jiachuan Wang, Haoyang Li, Cheng Deng, Jiaqi Tang, Shuangyin Li, Yongqi Zhang, Jun Wang, Lei Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13542">https://arxiv.org/abs/2502.13542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13542">https://arxiv.org/pdf/2502.13542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13542]] Activation-aware Probe-Query: Effective Key-Value Retrieval for Long-Context LLMs Inference(https://arxiv.org/abs/2502.13542)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have showcased exceptional performance in long-context tasks, while facing significant inference efficiency challenges with limited GPU memory. Existing solutions first proposed the sliding-window approach to accumulate a set of historical \textbf{key-value} (KV) pairs for reuse, then further improvements selectively retain its subsets at each step. However, due to the sparse attention distribution across a long context, it is hard to identify and recall relevant KV pairs, as the attention is distracted by massive candidate pairs. Additionally, we found it promising to select representative tokens as probe-Query in each sliding window to effectively represent the entire context, which is an approach overlooked by existing methods. Thus, we propose \textbf{ActQKV}, a training-free, \textbf{Act}ivation-aware approach that dynamically determines probe-\textbf{Q}uery and leverages it to retrieve the relevant \textbf{KV} pairs for inference. Specifically, ActQKV monitors a token-level indicator, Activation Bias, within each context window, enabling the proper construction of probe-Query for retrieval at pre-filling stage. To accurately recall the relevant KV pairs and minimize the irrelevant ones, we design a dynamic KV cut-off mechanism guided by information density across layers at the decoding stage. Experiments on the Long-Bench and $\infty$ Benchmarks demonstrate its state-of-the-art performance with competitive inference quality and resource efficiency.</li>
</ul>

<h3>Title: From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap for Text Length Control via MARKERGEN</h3>
<ul>
<li><strong>Authors: </strong>Peiwen Yuan, Chuyi Tan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Yueqi Zhang, Jiayi Shi, Boyuan Pan, Yao Hu, Kan Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13544">https://arxiv.org/abs/2502.13544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13544">https://arxiv.org/pdf/2502.13544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13544]] From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap for Text Length Control via MARKERGEN(https://arxiv.org/abs/2502.13544)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the rapid progress of large language models (LLMs), their length-controllable text generation (LCTG) ability remains below expectations, posing a major limitation for practical applications. Existing methods mainly focus on end-to-end training to reinforce adherence to length constraints. However, the lack of decomposition and targeted enhancement of LCTG sub-abilities restricts further this http URL bridge this gap, we conduct a bottom-up decomposition of LCTG sub-abilities with human patterns as reference and perform a detailed error this http URL this basis, we propose MarkerGen, a simple-yet-effective plug-and-play approach that:(1) mitigates LLM fundamental deficiencies via external tool integration;(2) conducts explicit length modeling with dynamically inserted markers;(3) employs a three-stage generation scheme to better align length constraints while maintaining content this http URL experiments demonstrate that MarkerGen significantly improves LCTG across various settings, exhibiting outstanding effectiveness and generalizability.</li>
</ul>

<h3>Title: Detecting Linguistic Bias in Government Documents Using Large language Models</h3>
<ul>
<li><strong>Authors: </strong>Milena de Swart, Floris den Hengst, Jieying Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13548">https://arxiv.org/abs/2502.13548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13548">https://arxiv.org/pdf/2502.13548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13548]] Detecting Linguistic Bias in Government Documents Using Large language Models(https://arxiv.org/abs/2502.13548)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>This paper addresses the critical need for detecting bias in government documents, an underexplored area with significant implications for governance. Existing methodologies often overlook the unique context and far-reaching impacts of governmental documents, potentially obscuring embedded biases that shape public policy and citizen-government interactions. To bridge this gap, we introduce the Dutch Government Data for Bias Detection (DGDB), a dataset sourced from the Dutch House of Representatives and annotated for bias by experts. We fine-tune several BERT-based models on this dataset and compare their performance with that of generative language models. Additionally, we conduct a comprehensive error analysis that includes explanations of the models' predictions. Our findings demonstrate that fine-tuned models achieve strong performance and significantly outperform generative language models, indicating the effectiveness of DGDB for bias detection. This work underscores the importance of labeled datasets for bias detection in various languages and contributes to more equitable governance practices.</li>
</ul>

<h3>Title: STaR-SQL: Self-Taught Reasoner for Text-to-SQL</h3>
<ul>
<li><strong>Authors: </strong>Mingqian He, Yongliang Shen, Wenqi Zhang, Qiuying Peng, Jun Wang, Weiming Lu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13550">https://arxiv.org/abs/2502.13550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13550">https://arxiv.org/pdf/2502.13550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13550]] STaR-SQL: Self-Taught Reasoner for Text-to-SQL(https://arxiv.org/abs/2502.13550)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Generating step-by-step "chain-of-thought" rationales has proven effective for improving the performance of large language models on complex reasoning tasks. However, applying such techniques to structured tasks, such as text-to-SQL, remains largely unexplored. In this paper, we introduce Self-Taught Reasoner for text-to-SQL (STaR-SQL), a novel approach that reframes SQL query generation as a reasoning-driven process. Our method prompts the LLM to produce detailed reasoning steps for SQL queries and fine-tunes it on rationales that lead to correct outcomes. Unlike traditional methods, STaR-SQL dedicates additional test-time computation to reasoning, thereby positioning LLMs as spontaneous reasoners rather than mere prompt-based agents. To further scale the inference process, we incorporate an outcome-supervised reward model (ORM) as a verifier, which enhances SQL query accuracy. Experimental results on the challenging Spider benchmark demonstrate that STaR-SQL significantly improves text-to-SQL performance, achieving an execution accuracy of 86.6%. This surpasses a few-shot baseline by 31.6% and a baseline fine-tuned to predict answers directly by 18.0%. Additionally, STaR-SQL outperforms agent-like prompting methods that leverage more powerful yet closed-source models such as GPT-4. These findings underscore the potential of reasoning-augmented training for structured tasks and open the door to extending self-improving reasoning models to text-to-SQL generation and beyond.</li>
</ul>

<h3>Title: Democratizing Large Language Model-Based Graph Data Augmentation via Latent Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Yushi Feng, Tsai Hor Chan, Guosheng Yin, Lequan Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13555">https://arxiv.org/abs/2502.13555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13555">https://arxiv.org/pdf/2502.13555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13555]] Democratizing Large Language Model-Based Graph Data Augmentation via Latent Knowledge Graphs(https://arxiv.org/abs/2502.13555)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Data augmentation is necessary for graph representation learning due to the scarcity and noise present in graph data. Most of the existing augmentation methods overlook the context information inherited from the dataset as they rely solely on the graph structure for augmentation. Despite the success of some large language model-based (LLM) graph learning methods, they are mostly white-box which require access to the weights or latent features from the open-access LLMs, making them difficult to be democratized for everyone as existing LLMs are mostly closed-source for commercial considerations. To overcome these limitations, we propose a black-box context-driven graph data augmentation approach, with the guidance of LLMs -- DemoGraph. Leveraging the text prompt as context-related information, we task the LLM with generating knowledge graphs (KGs), which allow us to capture the structural interactions from the text outputs. We then design a dynamic merging schema to stochastically integrate the LLM-generated KGs into the original graph during training. To control the sparsity of the augmented graph, we further devise a granularity-aware prompting strategy and an instruction fine-tuning module, which seamlessly generates text prompts according to different granularity levels of the dataset. Extensive experiments on various graph learning tasks validate the effectiveness of our method over existing graph data augmentation methods. Notably, our approach excels in scenarios involving electronic health records (EHRs), which validates its maximal utilization of contextual knowledge, leading to enhanced predictive performance and interpretability.</li>
</ul>

<h3>Title: Are Large Language Models In-Context Graph Learners?</h3>
<ul>
<li><strong>Authors: </strong>Jintang Li, Ruofan Wu, Yuchang Zhu, Huizhe Zhang, Liang Chen, Zibin Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13562">https://arxiv.org/abs/2502.13562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13562">https://arxiv.org/pdf/2502.13562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13562]] Are Large Language Models In-Context Graph Learners?(https://arxiv.org/abs/2502.13562)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable in-context reasoning capabilities across a wide range of tasks, particularly with unstructured inputs such as language or images. However, LLMs struggle to handle structured data, such as graphs, due to their lack of understanding of non-Euclidean structures. As a result, without additional fine-tuning, their performance significantly lags behind that of graph neural networks (GNNs) in graph learning tasks. In this paper, we show that learning on graph data can be conceptualized as a retrieval-augmented generation (RAG) process, where specific instances (e.g., nodes or edges) act as queries, and the graph itself serves as the retrieved context. Building on this insight, we propose a series of RAG frameworks to enhance the in-context learning capabilities of LLMs for graph learning tasks. Comprehensive evaluations demonstrate that our proposed RAG frameworks significantly improve LLM performance on graph-based tasks, particularly in scenarios where a pretrained LLM must be used without modification or accessed via an API.</li>
</ul>

<h3>Title: PRIV-QA: Privacy-Preserving Question Answering for Cloud Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Guangwei Li, Yuansen Zhang, Yinggui Wang, Shoumeng Yan, Lei Wang, Tao Wei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13564">https://arxiv.org/abs/2502.13564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13564">https://arxiv.org/pdf/2502.13564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13564]] PRIV-QA: Privacy-Preserving Question Answering for Cloud Large Language Models(https://arxiv.org/abs/2502.13564)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, large language model</a></li>
<li><strong>Abstract: </strong>The rapid development of large language models (LLMs) is redefining the landscape of human-computer interaction, and their integration into various user-service applications is becoming increasingly prevalent. However, transmitting user data to cloud-based LLMs presents significant risks of data breaches and unauthorized access to personal identification information. In this paper, we propose a privacy preservation pipeline for protecting privacy and sensitive information during interactions between users and LLMs in practical LLM usage scenarios. We construct SensitiveQA, the first privacy open-ended question-answering dataset. It comprises 57k interactions in Chinese and English, encompassing a diverse range of user-sensitive information within the conversations. Our proposed solution employs a multi-stage strategy aimed at preemptively securing user information while simultaneously preserving the response quality of cloud-based LLMs. Experimental validation underscores our method's efficacy in balancing privacy protection with maintaining robust interaction quality. The code and dataset are available at this https URL.</li>
</ul>

<h3>Title: Extracting Social Connections from Finnish Karelian Refugee Interviews Using LLMs</h3>
<ul>
<li><strong>Authors: </strong>Joonatan Laato, Jenna Kanerva, John Loehr, Virpi Lummaa, Filip Ginter</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13566">https://arxiv.org/abs/2502.13566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13566">https://arxiv.org/pdf/2502.13566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13566]] Extracting Social Connections from Finnish Karelian Refugee Interviews Using LLMs(https://arxiv.org/abs/2502.13566)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>We performed a zero-shot information extraction study on a historical collection of 89,339 brief Finnish-language interviews of refugee families relocated post-WWII from Finnish Eastern Karelia. Our research objective is two-fold. First, we aim to extract social organizations and hobbies from the free text of the interviews, separately for each family member. These can act as a proxy variable indicating the degree of social integration of refugees in their new environment. Second, we aim to evaluate several alternative ways to approach this task, comparing a number of generative models and a supervised learning approach, to gain a broader insight into the relative merits of these different approaches and their applicability in similar studies. We find that the best generative model (GPT-4) is roughly on par with human performance, at an F-score of 88.8%. Interestingly, the best open generative model (Llama-3-70B-Instruct) reaches almost the same performance, at 87.7% F-score, demonstrating that open models are becoming a viable alternative for some practical tasks even on non-English data. Additionally, we test a supervised learning alternative, where we fine-tune a Finnish BERT model (FinBERT) using GPT-4 generated training data. By this method, we achieved an F-score of 84.1% already with 6K interviews up to an F-score of 86.3% with 30k interviews. Such an approach would be particularly appealing in cases where the computational resources are limited, or there is a substantial mass of data to process.</li>
</ul>

<h3>Title: LSR-Adapt: Ultra-Efficient Parameter Tuning with Matrix Low Separation Rank Kernel Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Xin Li, Anand Sarwate</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13568">https://arxiv.org/abs/2502.13568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13568">https://arxiv.org/pdf/2502.13568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13568]] LSR-Adapt: Ultra-Efficient Parameter Tuning with Matrix Low Separation Rank Kernel Adaptation(https://arxiv.org/abs/2502.13568)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Imposing an effective structural assumption on neural network weight matrices has been the major paradigm for designing Parameter-Efficient Fine-Tuning (PEFT) systems for adapting modern large pre-trained models to various downstream tasks. However, low rank based adaptation has become increasingly challenging due to the sheer scale of modern large language models. In this paper, we propose an effective kernelization to further reduce the number of parameters required for adaptation tasks. Specifically, from the classical idea in numerical analysis regarding matrix Low-Separation-Rank (LSR) representations, we develop a kernel using this representation for the low rank adapter matrices of the linear layers from large networks, named the Low Separation Rank Adaptation (LSR-Adapt) kernel. With the ultra-efficient kernel representation of the low rank adapter matrices, we manage to achieve state-of-the-art performance with even higher accuracy with almost half the number of parameters as compared to conventional low rank based methods. This structural assumption also opens the door to further GPU-side optimizations due to the highly parallelizable nature of Kronecker computations.</li>
</ul>

<h3>Title: Unraveling the Localized Latents: Learning Stratified Manifold Structures in LLM Embedding Space with Sparse Mixture-of-Experts</h3>
<ul>
<li><strong>Authors: </strong>Xin Li, Anand Sarwate</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13577">https://arxiv.org/abs/2502.13577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13577">https://arxiv.org/pdf/2502.13577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13577]] Unraveling the Localized Latents: Learning Stratified Manifold Structures in LLM Embedding Space with Sparse Mixture-of-Experts(https://arxiv.org/abs/2502.13577)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>However, real-world data often exhibit complex local structures that can be challenging for single-model approaches with a smooth global manifold in the embedding space to unravel. In this work, we conjecture that in the latent space of these large language models, the embeddings live in a local manifold structure with different dimensions depending on the perplexities and domains of the input data, commonly referred to as a Stratified Manifold structure, which in combination form a structured space known as a Stratified Space. To investigate the validity of this structural claim, we propose an analysis framework based on a Mixture-of-Experts (MoE) model where each expert is implemented with a simple dictionary learning algorithm at varying sparsity levels. By incorporating an attention-based soft-gating network, we verify that our model learns specialized sub-manifolds for an ensemble of input data sources, reflecting the semantic stratification in LLM embedding space. We further analyze the intrinsic dimensions of these stratified sub-manifolds and present extensive statistics on expert assignments, gating entropy, and inter-expert distances. Our experimental results demonstrate that our method not only validates the claim of a stratified manifold structure in the LLM embedding space, but also provides interpretable clusters that align with the intrinsic semantic variations of the input data.</li>
</ul>

<h3>Title: Don't Stop the Multi-Party! On Generating Synthetic Multi-Party Conversations with Constraints</h3>
<ul>
<li><strong>Authors: </strong>Nicol Penzo, Marco Guerini, Bruno Lepri, Goran Glava, Sara Tonelli</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13592">https://arxiv.org/abs/2502.13592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13592">https://arxiv.org/pdf/2502.13592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13592]] Don't Stop the Multi-Party! On Generating Synthetic Multi-Party Conversations with Constraints(https://arxiv.org/abs/2502.13592)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Multi-Party Conversations (MPCs) are widely studied across disciplines, with social media as a primary data source due to their accessibility. However, these datasets raise privacy concerns and often reflect platform-specific properties. For example, interactions between speakers may be limited due to rigid platform structures (e.g., threads, tree-like discussions), which yield overly simplistic interaction patterns (e.g., as a consequence of ``reply-to'' links). This work explores the feasibility of generating diverse MPCs with instruction-tuned Large Language Models (LLMs) by providing deterministic constraints such as dialogue structure and participants' stance. We investigate two complementary strategies of leveraging LLMs in this context: (i.) LLMs as MPC generators, where we task the LLM to generate a whole MPC at once and (ii.) LLMs as MPC parties, where the LLM generates one turn of the conversation at a time, provided the conversation history. We next introduce an analytical framework to evaluate compliance with the constraints, content quality, and interaction complexity for both strategies. Finally, we assess the quality of obtained MPCs via human annotation and LLM-as-a-judge evaluations. We find stark differences among LLMs, with only some being able to generate high-quality MPCs. We also find that turn-by-turn generation yields better conformance to constraints and higher linguistic variability than generating MPCs in one pass. Nonetheless, our structural and qualitative evaluation indicates that both generation strategies can yield high-quality MPCs.</li>
</ul>

<h3>Title: Toward Robust Non-Transferable Learning: A Survey and Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Ziming Hong, Yongli Xiang, Tongliang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13593">https://arxiv.org/abs/2502.13593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13593">https://arxiv.org/pdf/2502.13593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13593]] Toward Robust Non-Transferable Learning: A Survey and Benchmark(https://arxiv.org/abs/2502.13593)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Over the past decades, researchers have primarily focused on improving the generalization abilities of models, with limited attention given to regulating such generalization. However, the ability of models to generalize to unintended data (e.g., harmful or unauthorized data) can be exploited by malicious adversaries in unforeseen ways, potentially resulting in violations of model ethics. Non-transferable learning (NTL), a task aimed at reshaping the generalization abilities of deep learning models, was proposed to address these challenges. While numerous methods have been proposed in this field, a comprehensive review of existing progress and a thorough analysis of current limitations remain lacking. In this paper, we bridge this gap by presenting the first comprehensive survey on NTL and introducing NTLBench, the first benchmark to evaluate NTL performance and robustness within a unified framework. Specifically, we first introduce the task settings, general framework, and criteria of NTL, followed by a summary of NTL approaches. Furthermore, we emphasize the often-overlooked issue of robustness against various attacks that can destroy the non-transferable mechanism established by NTL. Experiments conducted via NTLBench verify the limitations of existing NTL methods in robustness. Finally, we discuss the practical applications of NTL, along with its future directions and associated challenges.</li>
</ul>

<h3>Title: MMTEB: Massive Multilingual Text Embedding Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, Mrton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzemiski, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Jonathan Rystrm, Roman Solomatin, mer aatan, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, Rafa Powiata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras, Bjrn Plster, Jan Philipp Harries, Loc Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek uppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael Gnther, Mengzhou Xia, Weijia Shi, Xing Han L, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide, Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, Wen-Ding Li, Alessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, Niklas Muennighoff</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13595">https://arxiv.org/abs/2502.13595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13595">https://arxiv.org/pdf/2502.13595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13595]] MMTEB: Massive Multilingual Text Embedding Benchmark(https://arxiv.org/abs/2502.13595)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text embeddings are typically evaluated on a limited set of tasks, which are constrained by language, domain, and task diversity. To address these limitations and provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale, community-driven expansion of MTEB, covering over 500 quality-controlled evaluation tasks across 250+ languages. MMTEB includes a diverse set of challenging, novel tasks such as instruction following, long-document retrieval, and code retrieval, representing the largest multilingual collection of evaluation tasks for embedding models to date. Using this collection, we develop several highly multilingual benchmarks, which we use to evaluate a representative set of models. We find that while large language models (LLMs) with billions of parameters can achieve state-of-the-art performance on certain language subsets and task categories, the best-performing publicly available model is multilingual-e5-large-instruct with only 560 million parameters. To facilitate accessibility and reduce computational cost, we introduce a novel downsampling method based on inter-task correlation, ensuring a diverse selection while preserving relative model rankings. Furthermore, we optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks that drastically reduce computational demands. For instance, our newly introduced zero-shot English benchmark maintains a ranking order similar to the full-scale version but at a fraction of the computational cost.</li>
</ul>

<h3>Title: Efficient Safety Retrofitting Against Jailbreaking for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Dario Garcia-Gasulla, Anna Arias-Duart, Adrian Tormos, Daniel Hinjos, Oscar Molina-Sedano, Ashwin Kumar Gururajan, Maria Eugenia Cardello</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13603">https://arxiv.org/abs/2502.13603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13603">https://arxiv.org/pdf/2502.13603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13603]] Efficient Safety Retrofitting Against Jailbreaking for LLMs(https://arxiv.org/abs/2502.13603)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Direct Preference Optimization (DPO) is an efficient alignment technique that steers LLMs towards preferable outputs by training on preference data, bypassing the need for explicit reward models. Its simplicity enables easy adaptation to various domains and safety requirements. This paper examines DPO's effectiveness in model safety against jailbreaking attacks while minimizing data requirements and training costs. We introduce Egida, a dataset expanded from multiple sources, which includes 27 different safety topics and 18 different attack styles, complemented with synthetic and human labels. This data is used to boost the safety of state-of-the-art LLMs (Llama-3.1-8B/70B-Instruct, Qwen-2.5-7B/72B-Instruct) across topics and attack styles. In addition to safety evaluations, we assess their post-alignment performance degradation in general purpose tasks, and their tendency to over refusal. Following the proposed methodology, trained models reduce their Attack Success Rate by 10%-30%, using small training efforts (2,000 samples) with low computational cost (3\$ for 8B models, 20\$ for 72B models). Safety aligned models generalize to unseen topics and attack styles, with the most successful attack style reaching a success rate around 5%. Size and family are found to strongly influence model malleability towards safety, pointing at the importance of pre-training choices. To validate our findings, a large independent assessment of human preference agreement with Llama-Guard-3-8B is conducted by the authors and the associated dataset Egida-HSafe is released. Overall, this study illustrates how affordable and accessible it is to enhance LLM safety using DPO while outlining its current limitations. All datasets and models are released to enable reproducibility and further research.</li>
</ul>

<h3>Title: BeamLoRA: Beam-Constraint Low-Rank Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Naibin Gu, Zhenyu Zhang, Xiyu Liu, Peng Fu, Zheng Lin, Shuohuan Wang, Yu Sun, Hua Wu, Weiping Wang, Haifeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13604">https://arxiv.org/abs/2502.13604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13604">https://arxiv.org/pdf/2502.13604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13604]] BeamLoRA: Beam-Constraint Low-Rank Adaptation(https://arxiv.org/abs/2502.13604)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Due to the demand for efficient fine-tuning of large language models, Low-Rank Adaptation (LoRA) has been widely adopted as one of the most effective parameter-efficient fine-tuning methods. Nevertheless, while LoRA improves efficiency, there remains room for improvement in accuracy. Herein, we adopt a novel perspective to assess the characteristics of LoRA ranks. The results reveal that different ranks within the LoRA modules not only exhibit varying levels of importance but also evolve dynamically throughout the fine-tuning process, which may limit the performance of LoRA. Based on these findings, we propose BeamLoRA, which conceptualizes each LoRA module as a beam where each rank naturally corresponds to a potential sub-solution, and the fine-tuning process becomes a search for the optimal sub-solution combination. BeamLoRA dynamically eliminates underperforming sub-solutions while expanding the parameter space for promising ones, enhancing performance with a fixed rank. Extensive experiments across three base models and 12 datasets spanning math reasoning, code generation, and commonsense reasoning demonstrate that BeamLoRA consistently enhances the performance of LoRA, surpassing the other baseline methods.</li>
</ul>

<h3>Title: Complex Ontology Matching with Large Language Model Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Guilherme Sousa, Rinaldo Lima, Cassia Trojahn</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13619">https://arxiv.org/abs/2502.13619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13619">https://arxiv.org/pdf/2502.13619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13619]] Complex Ontology Matching with Large Language Model Embeddings(https://arxiv.org/abs/2502.13619)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Ontology, and more broadly, Knowledge Graph Matching is a challenging task in which expressiveness has not been fully addressed. Despite the increasing use of embeddings and language models for this task, approaches for generating expressive correspondences still do not take full advantage of these models, in particular, large language models (LLMs). This paper proposes to integrate LLMs into an approach for generating expressive correspondences based on alignment need and ABox-based relation discovery. The generation of correspondences is performed by matching similar surroundings of instance sub-graphs. The integration of LLMs results in different architectural modifications, including label similarity, sub-graph matching, and entity matching. The performance word embeddings, sentence embeddings, and LLM-based embeddings, was compared. The results demonstrate that integrating LLMs surpasses all other models, enhancing the baseline version of the approach with a 45\% increase in F-measure.</li>
</ul>

<h3>Title: REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>DongGeon Lee, Hwanjo Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13622">https://arxiv.org/abs/2502.13622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13622">https://arxiv.org/pdf/2502.13622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13622]] REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models(https://arxiv.org/abs/2502.13622)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Hallucinations in large language model (LLM) outputs severely limit their reliability in knowledge-intensive tasks such as question answering. To address this challenge, we introduce REFIND (Retrieval-augmented Factuality hallucINation Detection), a novel framework that detects hallucinated spans within LLM outputs by directly leveraging retrieved documents. As part of the REFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that quantifies the sensitivity of LLM outputs to retrieved evidence. This innovative approach enables REFIND to efficiently and accurately detect hallucinations, setting it apart from existing methods. In the evaluation, REFIND demonstrated robustness across nine languages, including low-resource settings, and significantly outperformed baseline models, achieving superior IoU scores in identifying hallucinated spans. This work highlights the effectiveness of quantifying context sensitivity for hallucination detection, thereby paving the way for more reliable and trustworthy LLM applications across diverse languages.</li>
</ul>

<h3>Title: CardiacMamba: A Multimodal RGB-RF Fusion Framework with State Space Models for Remote Physiological Measurement</h3>
<ul>
<li><strong>Authors: </strong>Zheng Wu, Yiping Xie, Bo Zhao, Jiguang He, Fei Luo, Ning Deng, Zitong Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13624">https://arxiv.org/abs/2502.13624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13624">https://arxiv.org/pdf/2502.13624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13624]] CardiacMamba: A Multimodal RGB-RF Fusion Framework with State Space Models for Remote Physiological Measurement(https://arxiv.org/abs/2502.13624)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, fair</a></li>
<li><strong>Abstract: </strong>Heart rate (HR) estimation via remote photoplethysmography (rPPG) offers a non-invasive solution for health monitoring. However, traditional single-modality approaches (RGB or Radio Frequency (RF)) face challenges in balancing robustness and accuracy due to lighting variations, motion artifacts, and skin tone bias. In this paper, we propose CardiacMamba, a multimodal RGB-RF fusion framework that leverages the complementary strengths of both modalities. It introduces the Temporal Difference Mamba Module (TDMM) to capture dynamic changes in RF signals using timing differences between frames, enhancing the extraction of local and global features. Additionally, CardiacMamba employs a Bidirectional SSM for cross-modal alignment and a Channel-wise Fast Fourier Transform (CFFT) to effectively capture and refine the frequency domain characteristics of RGB and RF signals, ultimately improving heart rate estimation accuracy and periodicity detection. Extensive experiments on the EquiPleth dataset demonstrate state-of-the-art performance, achieving marked improvements in accuracy and robustness. CardiacMamba significantly mitigates skin tone bias, reducing performance disparities across demographic groups, and maintains resilience under missing-modality scenarios. By addressing critical challenges in fairness, adaptability, and precision, the framework advances rPPG technology toward reliable real-world deployment in healthcare. The codes are available at: this https URL.</li>
</ul>

<h3>Title: Non-Euclidean Hierarchical Representational Learning using Hyperbolic Graph Neural Networks for Environmental Claim Detection</h3>
<ul>
<li><strong>Authors: </strong>Darpan Aswal, Manjira Sinha</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13628">https://arxiv.org/abs/2502.13628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13628">https://arxiv.org/pdf/2502.13628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13628]] Non-Euclidean Hierarchical Representational Learning using Hyperbolic Graph Neural Networks for Environmental Claim Detection(https://arxiv.org/abs/2502.13628)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based models dominate NLP tasks like sentiment analysis, machine translation, and claim verification. However, their massive computational demands and lack of interpretability pose challenges for real-world applications requiring efficiency and transparency. In this work, we explore Graph Neural Networks (GNNs) and Hyperbolic Graph Neural Networks (HGNNs) as lightweight yet effective alternatives for Environmental Claim Detection, reframing it as a graph classification problem. We construct dependency parsing graphs to explicitly model syntactic structures, using simple word embeddings (word2vec) for node features with dependency relations encoded as edge features. Our results demonstrate that these graph-based models achieve comparable or superior performance to state-of-the-art transformers while using 30x fewer parameters. This efficiency highlights the potential of structured, interpretable, and computationally efficient graph-based approaches.</li>
</ul>

<h3>Title: Concept Layers: Enhancing Interpretability and Intervenability via LLM Conceptualization</h3>
<ul>
<li><strong>Authors: </strong>Or Raphael Bidusa, Shaul Markovitch</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13632">https://arxiv.org/abs/2502.13632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13632">https://arxiv.org/pdf/2502.13632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13632]] Concept Layers: Enhancing Interpretability and Intervenability via LLM Conceptualization(https://arxiv.org/abs/2502.13632)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The opaque nature of Large Language Models (LLMs) has led to significant research efforts aimed at enhancing their interpretability, primarily through post-hoc methods. More recent in-hoc approaches, such as Concept Bottleneck Models (CBMs), offer both interpretability and intervenability by incorporating explicit concept representations. However, these methods suffer from key limitations, including reliance on labeled concept datasets and significant architectural modifications that challenges re-integration into existing system pipelines. In this work, we introduce a new methodology for incorporating interpretability and intervenability into an existing model by integrating Concept Layers (CLs) into its architecture. Our approach projects the model's internal vector representations into a conceptual, explainable vector space before reconstructing and feeding them back into the model. Furthermore, we eliminate the need for a human-selected concept set by algorithmically searching an ontology for a set of concepts that can be either task-specific or task-agnostic. We evaluate CLs across multiple tasks, demonstrating that they maintain the original model's performance and agreement while enabling meaningful interventions. Additionally, we present a proof of concept showcasing an intervenability interface, allowing users to adjust model behavior dynamically, such as mitigating biases during inference.</li>
</ul>

<h3>Title: Integrating Inverse and Forward Modeling for Sparse Temporal Data from Sensor Networks</h3>
<ul>
<li><strong>Authors: </strong>Julian Vexler, Bjrn Vieten, Martin Nelke, Stefan Kramer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13638">https://arxiv.org/abs/2502.13638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13638">https://arxiv.org/pdf/2502.13638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13638]] Integrating Inverse and Forward Modeling for Sparse Temporal Data from Sensor Networks(https://arxiv.org/abs/2502.13638)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We present CavePerception, a framework for the analysis of sparse data from sensor networks that incorporates elements of inverse modeling and forward modeling. By integrating machine learning with physical modeling in a hypotheses space, we aim to improve the interpretability of sparse, noisy, and potentially incomplete sensor data. The framework assumes data from a two-dimensional sensor network laid out in a graph structure that detects certain objects, with certain motion patterns. Examples of such sensors are magnetometers. Given knowledge about the objects and the way they act on the sensors, one can develop a data generator that produces data from simulated motions of the objects across the sensor field. The framework uses the simulated data to infer object behaviors across the sensor network. The approach is experimentally tested on real-world data, where magnetometers are used on an airport to detect and identify aircraft motions. Experiments demonstrate the value of integrating inverse and forward modeling, enabling intelligent systems to better understand and predict complex, sensor-driven events.</li>
</ul>

<h3>Title: Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts</h3>
<ul>
<li><strong>Authors: </strong>Maiya Goloburda, Nurkhan Laiyk, Diana Turmakhan, Yuxia Wang, Mukhammed Togmanov, Jonibek Mansurov, Askhat Sametov, Nurdaulet Mukhituly, Minghan Wang, Daniil Orel, Zain Muhammad Mujahid, Fajri Koto, Timothy Baldwin, Preslav Nakov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13640">https://arxiv.org/abs/2502.13640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13640">https://arxiv.org/pdf/2502.13640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13640]] Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts(https://arxiv.org/abs/2502.13640)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are known to have the potential to generate harmful content, posing risks to users. While significant progress has been made in developing taxonomies for LLM risks and safety evaluation prompts, most studies have focused on monolingual contexts, primarily in English. However, language- and region-specific risks in bilingual contexts are often overlooked, and core findings can diverge from those in monolingual settings. In this paper, we introduce Qorgau, a novel dataset specifically designed for safety evaluation in Kazakh and Russian, reflecting the unique bilingual context in Kazakhstan, where both Kazakh (a low-resource language) and Russian (a high-resource language) are spoken. Experiments with both multilingual and language-specific LLMs reveal notable differences in safety performance, emphasizing the need for tailored, region-specific datasets to ensure the responsible and safe deployment of LLMs in countries like Kazakhstan. Warning: this paper contains example data that may be offensive, harmful, or biased.</li>
</ul>

<h3>Title: D.Va: Validate Your Demonstration First Before You Use It</h3>
<ul>
<li><strong>Authors: </strong>Qi Zhang, Zhiqing Xiao, Ruixuan Xiao, Lirong Gao, Junbo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13646">https://arxiv.org/abs/2502.13646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13646">https://arxiv.org/pdf/2502.13646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13646]] D.Va: Validate Your Demonstration First Before You Use It(https://arxiv.org/abs/2502.13646)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) has demonstrated significant potential in enhancing the capabilities of large language models (LLMs) during inference. It's well-established that ICL heavily relies on selecting effective demonstrations to generate outputs that better align with the expected results. As for demonstration selection, previous approaches have typically relied on intuitive metrics to evaluate the effectiveness of demonstrations, which often results in limited robustness and poor cross-model generalization capabilities. To tackle these challenges, we propose a novel method, \textbf{D}emonstration \textbf{VA}lidation (\textbf{this http URL}), which integrates a demonstration validation perspective into this field. By introducing the demonstration validation mechanism, our method effectively identifies demonstrations that are both effective and highly generalizable. \textbf{this http URL} surpasses all existing demonstration selection techniques across both natural language understanding (NLU) and natural language generation (NLG) tasks. Additionally, we demonstrate the robustness and generalizability of our approach across various language models with different retrieval models.</li>
</ul>

<h3>Title: Instruction Tuning on Public Government and Cultural Data for Low-Resource Language: a Case Study in Kazakh</h3>
<ul>
<li><strong>Authors: </strong>Nurkhan Laiyk, Daniil Orel, Rituraj Joshi, Maiya Goloburda, Yuxia Wang, Preslav Nakov, Fajri Koto</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13647">https://arxiv.org/abs/2502.13647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13647">https://arxiv.org/pdf/2502.13647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13647]] Instruction Tuning on Public Government and Cultural Data for Low-Resource Language: a Case Study in Kazakh(https://arxiv.org/abs/2502.13647)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Instruction tuning in low-resource languages remains underexplored due to limited text data, particularly in government and cultural domains. To address this, we introduce and open-source a large-scale (10,600 samples) instruction-following (IFT) dataset, covering key institutional and cultural knowledge relevant to Kazakhstan. Our dataset enhances LLMs' understanding of procedural, legal, and structural governance topics. We employ LLM-assisted data generation, comparing open-weight and closed-weight models for dataset construction, and select GPT-4o as the backbone. Each entity of our dataset undergoes full manual verification to ensure high quality. We also show that fine-tuning Qwen, Falcon, and Gemma on our dataset leads to consistent performance improvements in both multiple-choice and generative tasks, demonstrating the potential of LLM-assisted instruction tuning for low-resource languages.</li>
</ul>

<h3>Title: Reliability Across Parametric and External Knowledge: Understanding Knowledge Handling in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Youna Kim, Minjoon Choi, Sungmin Cho, Hyuhng Joon Kim, Sang-goo Lee, Taeuk Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13648">https://arxiv.org/abs/2502.13648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13648">https://arxiv.org/pdf/2502.13648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13648]] Reliability Across Parametric and External Knowledge: Understanding Knowledge Handling in LLMs(https://arxiv.org/abs/2502.13648)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) enhance their problem-solving capability by leveraging both parametric and external knowledge. Beyond leveraging external knowledge to improve response accuracy, they require key capabilities for reliable knowledge-handling: resolving conflicts between knowledge sources, avoiding distraction from uninformative external knowledge, and abstaining when sufficient knowledge is unavailable. Prior studies have examined these scenarios in isolation or with limited scope. To systematically evaluate these capabilities, we introduce a comprehensive framework for analyzing knowledge-handling based on two key dimensions: the presence of parametric knowledge and the informativeness of external knowledge. Through analysis, we identify biases in knowledge utilization and examine how the ability to handle one scenario impacts performance in others. Furthermore, we demonstrate that training on data constructed based on the knowledge-handling scenarios improves LLMs' reliability in integrating and utilizing knowledge.</li>
</ul>

<h3>Title: C2T: A Classifier-Based Tree Construction Method in Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Feiye Huo, Jianchao Tan, Kefeng Zhang, Xunliang Cai, Shengli Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13652">https://arxiv.org/abs/2502.13652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13652">https://arxiv.org/pdf/2502.13652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13652]] C2T: A Classifier-Based Tree Construction Method in Speculative Decoding(https://arxiv.org/abs/2502.13652)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The growing scale of Large Language Models (LLMs) has exacerbated inference latency and computational costs. Speculative decoding methods, which aim to mitigate these issues, often face inefficiencies in the construction of token trees and the verification of candidate tokens. Existing strategies, including chain mode, static tree, and dynamic tree approaches, have limitations in accurately preparing candidate token trees for verification. We propose a novel method named C2T that adopts a lightweight classifier to generate and prune token trees dynamically. Our classifier considers additional feature variables beyond the commonly used joint probability to predict the confidence score for each draft token to determine whether it is the candidate token for verification. This method outperforms state-of-the-art (SOTA) methods such as EAGLE-2 on multiple benchmarks, by reducing the total number of candidate tokens by 25% while maintaining or even improving the acceptance length.</li>
</ul>

<h3>Title: Refining Sentence Embedding Model through Ranking Sentences Generation with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Liyang He, Chenglong Liu, Rui Li, Zhenya Huang, Shulan Ruan, Jun Zhou, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13656">https://arxiv.org/abs/2502.13656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13656">https://arxiv.org/pdf/2502.13656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13656]] Refining Sentence Embedding Model through Ranking Sentences Generation with Large Language Models(https://arxiv.org/abs/2502.13656)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Sentence embedding is essential for many NLP tasks, with contrastive learning methods achieving strong performance using annotated datasets like NLI. Yet, the reliance on manual labels limits scalability. Recent studies leverage large language models (LLMs) to generate sentence pairs, reducing annotation dependency. However, they overlook ranking information crucial for fine-grained semantic distinctions. To tackle this challenge, we propose a method for controlling the generation direction of LLMs in the latent space. Unlike unconstrained generation, the controlled approach ensures meaningful semantic divergence. Then, we refine exist sentence embedding model by integrating ranking information and semantic information. Experiments on multiple benchmarks demonstrate that our method achieves new SOTA performance with a modest cost in ranking sentence synthesis.</li>
</ul>

<h3>Title: What Skills Do Cyber Security Professionals Need?</h3>
<ul>
<li><strong>Authors: </strong>Faheem Ullah, Xiaohan Ye, Uswa Fatima, Zahid Akhtar, Yuxi Wu, Hussain Ahmad</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13658">https://arxiv.org/abs/2502.13658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13658">https://arxiv.org/pdf/2502.13658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13658]] What Skills Do Cyber Security Professionals Need?(https://arxiv.org/abs/2502.13658)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>Purpose: The increasing number of cyber-attacks has elevated the importance of cybersecurity for organizations. This has also increased the demand for professionals with the necessary skills to protect these organizations. As a result, many individuals are looking to enter the field of cybersecurity. However, there is a lack of clear understanding of the skills required for a successful career in this field. In this paper, we identify the skills required for cybersecurity professionals. We also determine how the demand for cyber skills relates to various cyber roles such as security analyst and security architect. Furthermore, we identify the programming languages that are important for cybersecurity professionals. Design/Methodology: For this study, we have collected and analyzed data from 12,161 job ads and 49,002 Stack Overflow posts. By examining this, we identified patterns and trends related to skill requirements, role-specific demands, and programming languages in cybersecurity. Findings: Our results reveal that (i) communication skills and project management skills are the most important soft skills, (ii) as compared to soft skills, the demand for technical skills varies more across various cyber roles, and (iii) Java is the most commonly used programming language. Originality: Our findings serve as a guideline for individuals aiming to get into the field of cybersecurity. Moreover, our findings are useful in terms of informing educational institutes to teach the correct set of skills to students doing degrees in cybersecurity.</li>
</ul>

<h3>Title: SCOPE: A Self-supervised Framework for Improving Faithfulness in Conditional Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Song Duong, Florian Le Bronnec, Alexandre Allauzen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13674">https://arxiv.org/abs/2502.13674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13674">https://arxiv.org/pdf/2502.13674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13674]] SCOPE: A Self-supervised Framework for Improving Faithfulness in Conditional Text Generation(https://arxiv.org/abs/2502.13674)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), when used for conditional text generation, often produce hallucinations, i.e., information that is unfaithful or not grounded in the input context. This issue arises in typical conditional text generation tasks, such as text summarization and data-to-text generation, where the goal is to produce fluent text based on contextual input. When fine-tuned on specific domains, LLMs struggle to provide faithful answers to a given context, often adding information or generating errors. One underlying cause of this issue is that LLMs rely on statistical patterns learned from their training data. This reliance can interfere with the model's ability to stay faithful to a provided context, leading to the generation of ungrounded information. We build upon this observation and introduce a novel self-supervised method for generating a training set of unfaithful samples. We then refine the model using a training process that encourages the generation of grounded outputs over unfaithful ones, drawing on preference-based training. Our approach leads to significantly more grounded text generation, outperforming existing self-supervised techniques in faithfulness, as evaluated through automatic metrics, LLM-based assessments, and human evaluations.</li>
</ul>

<h3>Title: MoM: Linear Sequence Modeling with Mixture-of-Memories</h3>
<ul>
<li><strong>Authors: </strong>Jusen Du, Weigao Sun, Disen Lan, Jiaxi Hu, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13685">https://arxiv.org/abs/2502.13685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13685">https://arxiv.org/pdf/2502.13685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13685]] MoM: Linear Sequence Modeling with Mixture-of-Memories(https://arxiv.org/abs/2502.13685)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into a single fixed-size memory state, which leads to suboptimal performance on recall-intensive downstream tasks. Drawing inspiration from neuroscience, particularly the brain's ability to maintain robust long-term memory while mitigating "memory interference", we introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes multiple independent memory states, with a router network directing input tokens to specific memory states. This approach greatly enhances the overall memory capacity while minimizing memory interference. As a result, MoM performs exceptionally well on recall-intensive tasks, surpassing existing linear sequence modeling techniques. Despite incorporating multiple memory states, the computation of each memory state remains linear in complexity, allowing MoM to retain the linear-complexity advantage during training, while constant-complexity during inference. Our experimental results show that MoM significantly outperforms current linear sequence models on downstream language tasks, particularly recall-intensive tasks, and even achieves performance comparable to Transformer models. The code is released at this https URL and is also released as a part of this https URL.</li>
</ul>

<h3>Title: Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora</h3>
<ul>
<li><strong>Authors: </strong>Tristan Karch, Luca Engel, Philippe Schwaller, Frdric Kaplan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13691">https://arxiv.org/abs/2502.13691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13691">https://arxiv.org/pdf/2502.13691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13691]] Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora(https://arxiv.org/abs/2502.13691)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) converge towards similar capabilities, the key to advancing their performance lies in identifying and incorporating valuable new information sources. However, evaluating which text collections are worth the substantial investment required for digitization, preprocessing, and integration into LLM systems remains a significant challenge. We present a novel approach to this challenge: an automated pipeline that evaluates the potential information gain from text collections without requiring model training or fine-tuning. Our method generates multiple choice questions (MCQs) from texts and measures an LLM's performance both with and without access to the source material. The performance gap between these conditions serves as a proxy for the collection's information potential. We validate our approach using three strategically selected datasets: EPFL PhD manuscripts (likely containing novel specialized knowledge), Wikipedia articles (presumably part of training data), and a synthetic baseline dataset. Our results demonstrate that this method effectively identifies collections containing valuable novel information, providing a practical tool for prioritizing data acquisition and integration efforts.</li>
</ul>

<h3>Title: Medical Image Classification with KAN-Integrated Transformers and Dilated Neighborhood Attention</h3>
<ul>
<li><strong>Authors: </strong>Omid Nejati Manzari, Hojat Asgariandehkordi, Taha Koleilat, Yiming Xiao, Hassan Rivaz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13693">https://arxiv.org/abs/2502.13693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13693">https://arxiv.org/pdf/2502.13693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13693]] Medical Image Classification with KAN-Integrated Transformers and Dilated Neighborhood Attention(https://arxiv.org/abs/2502.13693)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Convolutional networks, transformers, hybrid models, and Mamba-based architectures have demonstrated strong performance across various medical image classification tasks. However, these methods were primarily designed to classify clean images using labeled data. In contrast, real-world clinical data often involve image corruptions that are unique to multi-center studies and stem from variations in imaging equipment across manufacturers. In this paper, we introduce the Medical Vision Transformer (MedViTV2), a novel architecture incorporating Kolmogorov-Arnold Network (KAN) layers into the transformer architecture for the first time, aiming for generalized medical image classification. We have developed an efficient KAN block to reduce computational load while enhancing the accuracy of the original MedViT. Additionally, to counteract the fragility of our MedViT when scaled up, we propose an enhanced Dilated Neighborhood Attention (DiNA), an adaptation of the efficient fused dot-product attention kernel capable of capturing global context and expanding receptive fields to scale the model effectively and addressing feature collapse issues. Moreover, a hierarchical hybrid strategy is introduced to stack our Local Feature Perception and Global Feature Perception blocks in an efficient manner, which balances local and global feature perceptions to boost performance. Extensive experiments on 17 medical image classification datasets and 12 corrupted medical image datasets demonstrate that MedViTV2 achieved state-of-the-art results in 27 out of 29 experiments with reduced computational complexity. MedViTV2 is 44\% more computationally efficient than the previous version and significantly enhances accuracy, achieving improvements of 4.6\% on MedMNIST, 5.8\% on NonMNIST, and 13.4\% on the MedMNIST-C benchmark.</li>
</ul>

<h3>Title: Multi-Scale and Multi-Objective Optimization for Cross-Lingual Aspect-Based Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Chengyan Wu, Bolei Ma, Ningyuan Deng, Yanqing He, Yun Xue</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13718">https://arxiv.org/abs/2502.13718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13718">https://arxiv.org/pdf/2502.13718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13718]] Multi-Scale and Multi-Objective Optimization for Cross-Lingual Aspect-Based Sentiment Analysis(https://arxiv.org/abs/2502.13718)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Aspect-based sentiment analysis (ABSA) is a sequence labeling task that has garnered growing research interest in multilingual contexts. However, recent studies lack more robust feature alignment and finer aspect-level alignment. In this paper, we propose a novel framework, Multi-Scale and Multi-Objective optimization (MSMO) for cross-lingual ABSA. During multi-scale alignment, we achieve cross-lingual sentence-level and aspect-level alignment, aligning features of aspect terms in different contextual environments. Specifically, we introduce code-switched bilingual sentences into the language discriminator and consistency training modules to enhance the model's robustness. During multi-objective optimization, we design two optimization objectives: supervised training and consistency training, aiming to enhance cross-lingual semantic alignment. To further improve model performance, we incorporate distilled knowledge of the target language into the model. Results show that MSMO significantly enhances cross-lingual ABSA by achieving state-of-the-art performance across multiple languages and models.</li>
</ul>

<h3>Title: Learning Novel Transformer Architecture for Time-series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Juyuan Zhang, Wei Zhu, Jiechao Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13721">https://arxiv.org/abs/2502.13721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13721">https://arxiv.org/pdf/2502.13721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13721]] Learning Novel Transformer Architecture for Time-series Forecasting(https://arxiv.org/abs/2502.13721)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Despite the success of Transformer-based models in the time-series prediction (TSP) tasks, the existing Transformer architecture still face limitations and the literature lacks comprehensive explorations into alternative architectures. To address these challenges, we propose AutoFormer-TS, a novel framework that leverages a comprehensive search space for Transformer architectures tailored to TSP tasks. Our framework introduces a differentiable neural architecture search (DNAS) method, AB-DARTS, which improves upon existing DNAS approaches by enhancing the identification of optimal operations within the architecture. AutoFormer-TS systematically explores alternative attention mechanisms, activation functions, and encoding operations, moving beyond the traditional Transformer design. Extensive experiments demonstrate that AutoFormer-TS consistently outperforms state-of-the-art baselines across various TSP benchmarks, achieving superior forecasting accuracy while maintaining reasonable training efficiency.</li>
</ul>

<h3>Title: Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs with Refined Values</h3>
<ul>
<li><strong>Authors: </strong>Hongbo Zhang, Han Cui, Guangsheng Bao, Linyi Yang, Jun Wang, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13723">https://arxiv.org/abs/2502.13723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13723">https://arxiv.org/pdf/2502.13723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13723]] Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs with Refined Values(https://arxiv.org/abs/2502.13723)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce Direct Value Optimization (DVO), an innovative reinforcement learning framework for enhancing large language models in complex reasoning tasks. Unlike traditional methods relying on preference labels, DVO utilizes value signals at individual reasoning steps, optimizing models via a mean squared error loss. The key benefit of DVO lies in its fine-grained supervision, circumventing the need for labor-intensive human annotations. Target values within the DVO are estimated using either Monte Carlo Tree Search or an outcome value model. Our empirical analysis on both mathematical and commonsense reasoning tasks shows that DVO consistently outperforms existing offline preference optimization techniques, even with fewer training steps. These findings underscore the importance of value signals in advancing reasoning capabilities and highlight DVO as a superior methodology under scenarios lacking explicit human preference information.</li>
</ul>

<h3>Title: Adapting Large Language Models for Time Series Modeling via a Novel Parameter-efficient Adaptation Method</h3>
<ul>
<li><strong>Authors: </strong>Juyuan Zhang, Wei Zhu, Jiechao Gao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13725">https://arxiv.org/abs/2502.13725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13725">https://arxiv.org/pdf/2502.13725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13725]] Adapting Large Language Models for Time Series Modeling via a Novel Parameter-efficient Adaptation Method(https://arxiv.org/abs/2502.13725)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Time series modeling holds significant importance in many real-world applications and has been extensively studied. While pre-trained foundation models have made impressive strides in the fields of natural language processing (NLP) and computer vision (CV), their development in time series domains has been constrained by data sparsity. A series of recent studies have demonstrated that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the current literature have yet striked a high-quality balance between (a) effectively aligning the time series and natural language modalities, and (b) keeping the inference efficiency. To address the above issues, we now propose the Time-LlaMA framework. Time-LlaMA first converts the time series input into token embeddings through a linear tokenization mechanism. Second, the time series token embeddings are aligned with the text prompts. Third, to further adapt the LLM backbone for time series modeling, we have developed a dynamic low-rank adaptation technique (D-LoRA). D-LoRA dynamically chooses the most suitable LoRA modules at each layer of the Transformer backbone for each time series input, enhancing the model's predictive capabilities. Our experimental results on an extensive collection of challenging real-world time series tasks confirm that our proposed method achieves the state-of-the-art (SOTA) performance.</li>
</ul>

<h3>Title: Secure Federated Data Distillation</h3>
<ul>
<li><strong>Authors: </strong>Marco Arazzi, Mert Cihangiroglu, Serena Nicolazzo, Antonino Nocera</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13728">https://arxiv.org/abs/2502.13728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13728">https://arxiv.org/pdf/2502.13728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13728]] Secure Federated Data Distillation(https://arxiv.org/abs/2502.13728)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Dataset Distillation (DD) is a powerful technique for reducing large datasets into compact, representative synthetic datasets, accelerating Machine Learning training. However, traditional DD methods operate in a centralized manner, which poses significant privacy threats and reduces its applicability. To mitigate these risks, we propose a Secure Federated Data Distillation framework (SFDD) to decentralize the distillation process while preserving this http URL existing Federated Distillation techniques that focus on training global models with distilled knowledge, our approach aims to produce a distilled dataset without exposing local contributions. We leverage the gradient-matching-based distillation method, adapting it for a distributed setting where clients contribute to the distillation process without sharing raw data. The central aggregator iteratively refines a synthetic dataset by integrating client-side updates while ensuring data confidentiality. To make our approach resilient to inference attacks perpetrated by the server that could exploit gradient updates to reconstruct private data, we create an optimized Local Differential Privacy approach, called LDPO-RLD (Label Differential Privacy Obfuscation via Randomized Linear Dispersion). Furthermore, we assess the framework's resilience against malicious clients executing backdoor attacks and demonstrate robustness under the assumption of a sufficient number of participating clients. Our experimental results demonstrate the effectiveness of SFDD and that the proposed defense concretely mitigates the identified vulnerabilities, with minimal impact on the performance of the distilled dataset. By addressing the interplay between privacy and federation in dataset distillation, this work advances the field of privacy-preserving Machine Learning making our SFDD framework a viable solution for sensitive data-sharing applications.</li>
</ul>

<h3>Title: Homophily Heterogeneity Matters in Graph Federated Learning: A Spectrum Sharing and Complementing Perspective</h3>
<ul>
<li><strong>Authors: </strong>Wentao Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13732">https://arxiv.org/abs/2502.13732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13732">https://arxiv.org/pdf/2502.13732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13732]] Homophily Heterogeneity Matters in Graph Federated Learning: A Spectrum Sharing and Complementing Perspective(https://arxiv.org/abs/2502.13732)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Since heterogeneity presents a fundamental challenge in graph federated learning, many existing methods are proposed to deal with node feature heterogeneity and structure heterogeneity. However, they overlook the critical homophily heterogeneity, which refers to the substantial variation in homophily levels across graph data from different clients. The homophily level represents the proportion of edges connecting nodes that belong to the same class. Due to adapting to their local homophily, local models capture inconsistent spectral properties across different clients, significantly reducing the effectiveness of collaboration. Specifically, local models trained on graphs with high homophily tend to capture low-frequency information, whereas local models trained on graphs with low homophily tend to capture high-frequency information. To effectively deal with homophily heterophily, we introduce the spectral Graph Neural Network (GNN) and propose a novel Federated learning method by mining Graph Spectral Properties (FedGSP). On one hand, our proposed FedGSP enables clients to share generic spectral properties (i.e., low-frequency information), allowing all clients to benefit through collaboration. On the other hand, inspired by our theoretical findings, our proposed FedGSP allows clients to complement non-generic spectral properties by acquiring the spectral properties they lack (i.e., high-frequency information), thereby obtaining additional information gain. Extensive experiments conducted on six homophilic and five heterophilic graph datasets, across both non-overlapping and overlapping settings, validate the superiority of our method over eleven state-of-the-art methods. Notably, our FedGSP outperforms the second-best method by an average margin of 3.28% on all heterophilic datasets.</li>
</ul>

<h3>Title: CARE: Confidence-Aware Regression Estimation of building density fine-tuning EO Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Nikolaos Dionelis, Jente Bosmans, Nicolas Longp</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13734">https://arxiv.org/abs/2502.13734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13734">https://arxiv.org/pdf/2502.13734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13734]] CARE: Confidence-Aware Regression Estimation of building density fine-tuning EO Foundation Models(https://arxiv.org/abs/2502.13734)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Performing accurate confidence quantification and assessment is important for deep neural networks to predict their failures, improve their performance and enhance their capabilities in real-world applications, for their practical deployment in real life. For pixel-wise regression tasks, confidence quantification and assessment has not been well addressed in the literature, in contrast to classification tasks like semantic segmentation. The softmax output layer is not used in deep neural networks that solve pixel-wise regression problems. In this paper, to address these problems, we develop, train and evaluate the proposed model Confidence-Aware Regression Estimation (CARE). Our model CARE computes and assigns confidence to regression output results. We focus on solving regression problems as downstream tasks of an AI Foundation Model for Earth Observation (EO). We evaluate the proposed model CARE and experimental results on data from the Copernicus Sentinel-2 satellite constellation for estimating the density of buildings show that the proposed method can be successfully applied to regression problems. We also show that our approach outperforms other methods.</li>
</ul>

<h3>Title: Enhancing Input-Label Mapping in In-Context Learning with Contrastive Decoding</h3>
<ul>
<li><strong>Authors: </strong>Keqin Peng, Liang Ding, Yuanxin Ouyang, Meng Fang, Yancheng Yuan, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13738">https://arxiv.org/abs/2502.13738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13738">https://arxiv.org/pdf/2502.13738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13738]] Enhancing Input-Label Mapping in In-Context Learning with Contrastive Decoding(https://arxiv.org/abs/2502.13738)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel at a range of tasks through in-context learning (ICL), where only a few task examples guide their predictions. However, prior research highlights that LLMs often overlook input-label mapping information in ICL, relying more on their pre-trained knowledge. To address this issue, we introduce In-Context Contrastive Decoding (ICCD), a novel method that emphasizes input-label mapping by contrasting the output distributions between positive and negative in-context examples. Experiments on 7 natural language understanding (NLU) tasks show that our ICCD method brings consistent and significant improvement (up to +2.1 improvement on average) upon 6 different scales of LLMs without requiring additional training. Our approach is versatile, enhancing performance with various demonstration selection methods, demonstrating its broad applicability and effectiveness. The code and scripts will be publicly released.</li>
</ul>

<h3>Title: Reverse Markov Learning: Multi-Step Generative Models for Complex Distributions</h3>
<ul>
<li><strong>Authors: </strong>Xinwei Shen, Nicolai Meinshausen, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13747">https://arxiv.org/abs/2502.13747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13747">https://arxiv.org/pdf/2502.13747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13747]] Reverse Markov Learning: Multi-Step Generative Models for Complex Distributions(https://arxiv.org/abs/2502.13747)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Learning complex distributions is a fundamental challenge in contemporary applications. Generative models, such as diffusion models, have demonstrated remarkable success in overcoming many limitations of traditional statistical methods. Shen and Meinshausen (2024) introduced engression, a generative approach based on scoring rules that maps noise (and covariates, if available) directly to data. While effective, engression struggles with highly complex distributions, such as those encountered in image data. In this work, we extend engression to improve its capability in learning complex distributions. We propose a framework that defines a general forward process transitioning from the target distribution to a known distribution (e.g., Gaussian) and then learns a reverse Markov process using multiple engression models. This reverse process reconstructs the target distribution step by step. Our approach supports general forward processes, allows for dimension reduction, and naturally discretizes the generative process. As a special case, when using a diffusion-based forward process, our framework offers a method to discretize the training and inference of diffusion models efficiently. Empirical evaluations on simulated and climate data validate our theoretical insights, demonstrating the effectiveness of our approach in capturing complex distributions.</li>
</ul>

<h3>Title: RobustX: Robust Counterfactual Explanations Made Easy</h3>
<ul>
<li><strong>Authors: </strong>Junqi Jiang, Luca Marzari, Aaryan Purohit, Francesco Leofante</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13751">https://arxiv.org/abs/2502.13751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13751">https://arxiv.org/pdf/2502.13751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13751]] RobustX: Robust Counterfactual Explanations Made Easy(https://arxiv.org/abs/2502.13751)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>The increasing use of Machine Learning (ML) models to aid decision-making in high-stakes industries demands explainability to facilitate trust. Counterfactual Explanations (CEs) are ideally suited for this, as they can offer insights into the predictions of an ML model by illustrating how changes in its input data may lead to different outcomes. However, for CEs to realise their explanatory potential, significant challenges remain in ensuring their robustness under slight changes in the scenario being explained. Despite the widespread recognition of CEs' robustness as a fundamental requirement, a lack of standardised tools and benchmarks hinders a comprehensive and effective comparison of robust CE generation methods. In this paper, we introduce RobustX, an open-source Python library implementing a collection of CE generation and evaluation methods, with a focus on the robustness property. RobustX provides interfaces to several existing methods from the literature, enabling streamlined access to state-of-the-art techniques. The library is also easily extensible, allowing fast prototyping of novel robust CE generation and evaluation methods.</li>
</ul>

<h3>Title: SCALAR: Scientific Citation-based Live Assessment of Long-context Academic Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Renxi Wang, Honglin Mu, Liqun Ma, Lizhi Lin, Yunlong Feng, Timothy Baldwin, Xudong Han, Haonan Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13753">https://arxiv.org/abs/2502.13753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13753">https://arxiv.org/pdf/2502.13753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13753]] SCALAR: Scientific Citation-based Live Assessment of Long-context Academic Reasoning(https://arxiv.org/abs/2502.13753)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Evaluating large language models' (LLMs) long-context understanding capabilities remains challenging. We present SCALAR (Scientific Citation-based Live Assessment of Long-context Academic Reasoning), a novel benchmark that leverages academic papers and their citation networks. SCALAR features automatic generation of high-quality ground truth labels without human annotation, controllable difficulty levels, and a dynamic updating mechanism that prevents data contamination. Using ICLR 2025 papers, we evaluate 8 state-of-the-art LLMs, revealing key insights about their capabilities and limitations in processing long scientific documents across different context lengths and reasoning types. Our benchmark provides a reliable and sustainable way to track progress in long-context understanding as LLM capabilities evolve.</li>
</ul>

<h3>Title: Capturing Rich Behavior Representations: A Dynamic Action Semantic-Aware Graph Transformer for Video Captioning</h3>
<ul>
<li><strong>Authors: </strong>Caihua Liu, Xu Li, Wenjing Xue, Wei Tang, Xia Feng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13754">https://arxiv.org/abs/2502.13754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13754">https://arxiv.org/pdf/2502.13754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13754]] Capturing Rich Behavior Representations: A Dynamic Action Semantic-Aware Graph Transformer for Video Captioning(https://arxiv.org/abs/2502.13754)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Existing video captioning methods merely provide shallow or simplistic representations of object behaviors, resulting in superficial and ambiguous descriptions. However, object behavior is dynamic and complex. To comprehensively capture the essence of object behavior, we propose a dynamic action semantic-aware graph transformer. Firstly, a multi-scale temporal modeling module is designed to flexibly learn long and short-term latent action features. It not only acquires latent action features across time scales, but also considers local latent action details, enhancing the coherence and sensitiveness of latent action representations. Secondly, a visual-action semantic aware module is proposed to adaptively capture semantic representations related to object behavior, enhancing the richness and accurateness of action representations. By harnessing the collaborative efforts of these two modules,we can acquire rich behavior representations to generate human-like natural descriptions. Finally, this rich behavior representations and object representations are used to construct a temporal objects-action graph, which is fed into the graph transformer to model the complex temporal dependencies between objects and actions. To avoid adding complexity in the inference phase, the behavioral knowledge of the objects will be distilled into a simple network through knowledge distillation. The experimental results on MSVD and MSR-VTT datasets demonstrate that the proposed method achieves significant performance improvements across multiple metrics.</li>
</ul>

<h3>Title: Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and Human-Like Reasoning Framework</h3>
<ul>
<li><strong>Authors: </strong>Zirui Song, Jingpu Yang, Yuan Huang, Jonathan Tonglet, Zeyu Zhang, Tao Cheng, Meng Fang, Iryna Gurevych, Xiuying Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13759">https://arxiv.org/abs/2502.13759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13759">https://arxiv.org/pdf/2502.13759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13759]] Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and Human-Like Reasoning Framework(https://arxiv.org/abs/2502.13759)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Geolocation, the task of identifying an image's location, requires complex reasoning and is crucial for navigation, monitoring, and cultural preservation. However, current methods often produce coarse, imprecise, and non-interpretable localization. A major challenge lies in the quality and scale of existing geolocation datasets. These datasets are typically small-scale and automatically constructed, leading to noisy data and inconsistent task difficulty, with images that either reveal answers too easily or lack sufficient clues for reliable inference. To address these challenges, we introduce a comprehensive geolocation framework with three key components: GeoComp, a large-scale dataset; GeoCoT, a novel reasoning method; and GeoEval, an evaluation metric, collectively designed to address critical challenges and drive advancements in geolocation research. At the core of this framework is GeoComp (Geolocation Competition Dataset), a large-scale dataset collected from a geolocation game platform involving 740K users over two years. It comprises 25 million entries of metadata and 3 million geo-tagged locations spanning much of the globe, with each location annotated thousands to tens of thousands of times by human users. The dataset offers diverse difficulty levels for detailed analysis and highlights key gaps in current models. Building on this dataset, we propose Geographical Chain-of-Thought (GeoCoT), a novel multi-step reasoning framework designed to enhance the reasoning capabilities of Large Vision Models (LVMs) in geolocation tasks. GeoCoT improves performance by integrating contextual and spatial cues through a multi-step process that mimics human geolocation reasoning. Finally, using the GeoEval metric, we demonstrate that GeoCoT significantly boosts geolocation accuracy by up to 25% while enhancing interpretability.</li>
</ul>

<h3>Title: VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Anudeex Shetty, Amin Beheshti, Mark Dras, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13775">https://arxiv.org/abs/2502.13775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13775">https://arxiv.org/pdf/2502.13775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13775]] VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare(https://arxiv.org/abs/2502.13775)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Alignment techniques have become central to ensuring that Large Language Models (LLMs) generate outputs consistent with human values. However, existing alignment paradigms often model an averaged or monolithic preference, failing to account for the diversity of perspectives across cultures, demographics, and communities. This limitation is particularly critical in health-related scenarios, where plurality is essential due to the influence of culture, religion, personal values, and conflicting opinions. Despite progress in pluralistic alignment, no prior work has focused on health, likely due to the unavailability of publicly available datasets. To address this gap, we introduce VITAL, a new benchmark dataset comprising 13.1K value-laden situations and 5.4K multiple-choice questions focused on health, designed to assess and benchmark pluralistic alignment methodologies. Through extensive evaluation of eight LLMs of varying sizes, we demonstrate that existing pluralistic alignment techniques fall short in effectively accommodating diverse healthcare beliefs, underscoring the need for tailored AI alignment in specific domains. This work highlights the limitations of current approaches and lays the groundwork for developing health-specific alignment solutions.</li>
</ul>

<h3>Title: Herglotz-NET: Implicit Neural Representation of Spherical~Data with Harmonic Positional Encoding</h3>
<ul>
<li><strong>Authors: </strong>Tho Hanon, Nicolas Mil-Homens Cavaco, John Kiely, Laurent Jacques</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13777">https://arxiv.org/abs/2502.13777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13777">https://arxiv.org/pdf/2502.13777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13777]] Herglotz-NET: Implicit Neural Representation of Spherical~Data with Harmonic Positional Encoding(https://arxiv.org/abs/2502.13777)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Representing and processing data in spherical domains presents unique challenges, primarily due to the curvature of the domain, which complicates the application of classical Euclidean techniques. Implicit neural representations (INRs) have emerged as a promising alternative for high-fidelity data representation; however, to effectively handle spherical domains, these methods must be adapted to the inherent geometry of the sphere to maintain both accuracy and stability. In this context, we propose Herglotz-NET (HNET), a novel INR architecture that employs a harmonic positional encoding based on complex Herglotz mappings. This encoding yields a well-posed representation on the sphere with interpretable and robust spectral properties. Moreover, we present a unified expressivity analysis showing that any spherical-based INR satisfying a mild condition exhibits a predictable spectral expansion that scales with network depth. Our results establish HNET as a scalable and flexible framework for accurate modeling of spherical data.</li>
</ul>

<h3>Title: Poster: SpiderSim: Multi-Agent Driven Theoretical Cybersecurity Simulation for Industrial Digitalization</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Li, Xizhong Guo, Yang Zhao, Lvyang Zhang, Lidong Zhai</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13778">https://arxiv.org/abs/2502.13778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13778">https://arxiv.org/pdf/2502.13778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13778]] Poster: SpiderSim: Multi-Agent Driven Theoretical Cybersecurity Simulation for Industrial Digitalization(https://arxiv.org/abs/2502.13778)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Rapid industrial digitalization has created intricate cybersecurity demands that necessitate effective validation methods. While cyber ranges and simulation platforms are widely deployed, they frequently face limitations in scenario diversity and creation efficiency. In this paper, we present SpiderSim, a theoretical cybersecurity simulation platform enabling rapid and lightweight scenario generation for industrial digitalization security research. At its core, our platform introduces three key innovations: a structured framework for unified scenario modeling, a multi-agent collaboration mechanism for automated generation, and modular atomic security capabilities for flexible scenario composition. Extensive implementation trials across multiple industrial digitalization contexts, including marine ranch monitoring systems, validate our platform's capacity for broad scenario coverage with efficient generation processes. Built on solid theoretical foundations and released as open-source software, SpiderSim facilitates broader research and development in automated security testing for industrial digitalization.</li>
</ul>

<h3>Title: Translation in the Hands of Many:Centering Lay Users in Machine Translation Interactions</h3>
<ul>
<li><strong>Authors: </strong>Beatrice Savoldi, Alan Ramponi, Matteo Negri, Luisa Bentivogli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13780">https://arxiv.org/abs/2502.13780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13780">https://arxiv.org/pdf/2502.13780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13780]] Translation in the Hands of Many:Centering Lay Users in Machine Translation Interactions(https://arxiv.org/abs/2502.13780)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Converging societal and technical factors have transformed language technologies into user-facing applications employed across languages. Machine Translation (MT) has become a global tool, with cross-lingual services now also supported by dialogue systems powered by multilingual Large Language Models (LLMs). This accessibility has expanded MT's reach to a vast base of lay users, often with little to no expertise in the languages or the technology itself. Despite this, the understanding of MT consumed by this diverse group of users -- their needs, experiences, and interactions with these systems -- remains limited. This paper traces the shift in MT user profiles, focusing on non-expert users and how their engagement with these systems may change with LLMs. We identify three key factors -- usability, trust, and literacy -- that shape these interactions and must be addressed to align MT with user needs. By exploring these dimensions, we offer insights to guide future MT with a user-centered approach.</li>
</ul>

<h3>Title: From Correctness to Comprehension: AI Agents for Personalized Error Diagnosis in Education</h3>
<ul>
<li><strong>Authors: </strong>Yi-Fan Zhang, Hang Li, Dingjie Song, Lichao Sun, Tianlong Xu, Qingsong Wen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13789">https://arxiv.org/abs/2502.13789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13789">https://arxiv.org/pdf/2502.13789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13789]] From Correctness to Comprehension: AI Agents for Personalized Error Diagnosis in Education(https://arxiv.org/abs/2502.13789)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), such as GPT-4, have demonstrated impressive mathematical reasoning capabilities, achieving near-perfect performance on benchmarks like GSM8K. However, their application in personalized education remains limited due to an overemphasis on correctness over error diagnosis and feedback generation. Current models fail to provide meaningful insights into the causes of student mistakes, limiting their utility in educational contexts. To address these challenges, we present three key contributions. First, we introduce \textbf{MathCCS} (Mathematical Classification and Constructive Suggestions), a multi-modal benchmark designed for systematic error analysis and tailored feedback. MathCCS includes real-world problems, expert-annotated error categories, and longitudinal student data. Evaluations of state-of-the-art models, including \textit{Qwen2-VL}, \textit{LLaVA-OV}, \textit{Claude-3.5-Sonnet} and \textit{GPT-4o}, reveal that none achieved classification accuracy above 30\% or generated high-quality suggestions (average scores below 4/10), highlighting a significant gap from human-level performance. Second, we develop a sequential error analysis framework that leverages historical data to track trends and improve diagnostic precision. Finally, we propose a multi-agent collaborative framework that combines a Time Series Agent for historical analysis and an MLLM Agent for real-time refinement, enhancing error classification and feedback generation. Together, these contributions provide a robust platform for advancing personalized education, bridging the gap between current AI capabilities and the demands of real-world teaching.</li>
</ul>

<h3>Title: From Tools to Teammates: Evaluating LLMs in Multi-Session Coding Interactions</h3>
<ul>
<li><strong>Authors: </strong>Nathanal Carraz Rakotonirina, Mohammed Hamdy, Jon Ander Campos, Lucas Weber, Alberto Testoni, Marzieh Fadaee, Sandro Pezzelle, Marco Del Tredici</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13791">https://arxiv.org/abs/2502.13791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13791">https://arxiv.org/pdf/2502.13791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13791]] From Tools to Teammates: Evaluating LLMs in Multi-Session Coding Interactions(https://arxiv.org/abs/2502.13791)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used in working environments for a wide range of tasks, excelling at solving individual problems in isolation. However, are they also able to effectively collaborate over long-term interactions? To investigate this, we introduce MemoryCode, a synthetic multi-session dataset designed to test LLMs' ability to track and execute simple coding instructions amid irrelevant information, simulating a realistic setting. While all the models we tested handle isolated instructions well, even the performance of state-of-the-art models like GPT-4o deteriorates when instructions are spread across sessions. Our analysis suggests this is due to their failure to retrieve and integrate information over long instruction chains. Our results highlight a fundamental limitation of current LLMs, restricting their ability to collaborate effectively in long interactions.</li>
</ul>

<h3>Title: LESA: Learnable LLM Layer Scaling-Up</h3>
<ul>
<li><strong>Authors: </strong>Yifei Yang, Zouying Cao, Xinbei Ma, Yao Yao, Libo Qin, Zhi Chen, Hai Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13794">https://arxiv.org/abs/2502.13794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13794">https://arxiv.org/pdf/2502.13794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13794]] LESA: Learnable LLM Layer Scaling-Up(https://arxiv.org/abs/2502.13794)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Training Large Language Models (LLMs) from scratch requires immense computational resources, making it prohibitively expensive. Model scaling-up offers a promising solution by leveraging the parameters of smaller models to create larger ones. However, existing depth scaling-up methods rely on empirical heuristic rules for layer duplication, which result in poorer initialization and slower convergence during continual pre-training. We propose \textbf{LESA}, a novel learnable method for depth scaling-up. By concatenating parameters from each layer and applying Singular Value Decomposition, we uncover latent patterns between layers, suggesting that inter-layer parameters can be learned. LESA uses a neural network to predict the parameters inserted between adjacent layers, enabling better initialization and faster training. Experiments show that LESA outperforms existing baselines, achieving superior performance with less than half the computational cost during continual pre-training. Extensive analyses demonstrate its effectiveness across different model sizes and tasks.</li>
</ul>

<h3>Title: Building Age Estimation: A New Multi-Modal Benchmark Dataset and Community Challenge</h3>
<ul>
<li><strong>Authors: </strong>Nikolaos Dionelis, Nicolas Longp, Alessandra Feliciotti, Mattia Marconcini, Devis Peressutti, Nika Oman Kadunc, JaeWan Park, Hagai Raja Sinulingga, Steve Andreas Immanuel, Ba Tran, Caroline Arnold</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13818">https://arxiv.org/abs/2502.13818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13818">https://arxiv.org/pdf/2502.13818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13818]] Building Age Estimation: A New Multi-Modal Benchmark Dataset and Community Challenge(https://arxiv.org/abs/2502.13818)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Estimating the construction year of buildings is of great importance for sustainability. Sustainable buildings minimize energy consumption and are a key part of responsible and sustainable urban planning and development to effectively combat climate change. By using Artificial Intelligence (AI) and recently proposed Transformer models, we are able to estimate the construction epoch of buildings from a multi-modal dataset. In this paper, we introduce a new benchmark multi-modal dataset, i.e. the Map your City Dataset (MyCD), containing top-view Very High Resolution (VHR) images, Earth Observation (EO) multi-spectral data from the Copernicus Sentinel-2 satellite constellation, and street-view images in many different cities in Europe, co-localized with respect to the building under study and labelled with the construction epoch. We assess EO generalization performance on new/ previously unseen cities that have been held-out from training and appear only during inference. In this work, we present the community-based data challenge we organized based on MyCD. The ESA AI4EO Challenge MapYourCity was opened in 2024 for 4 months. Here, we present the Top-4 performing models, and the main evaluation results. During inference, the performance of the models using both all three input modalities and only the two top-view modalities, i.e. without the street-view images, is examined. The evaluation results show that the models are effective and can achieve good performance on this difficult real-world task of estimating the age of buildings, even on previously unseen cities, as well as even using only the two top-view modalities (i.e. VHR and Sentinel-2) during inference.</li>
</ul>

<h3>Title: Software Security in Software-Defined Networking: A Systematic Literature Review</h3>
<ul>
<li><strong>Authors: </strong>Moustapha Awwalou Diouf, Samuel Ouya, Jacques Klein, Tegawend F. Bissyand</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13828">https://arxiv.org/abs/2502.13828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13828">https://arxiv.org/pdf/2502.13828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13828]] Software Security in Software-Defined Networking: A Systematic Literature Review(https://arxiv.org/abs/2502.13828)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Software-defined networking (SDN) has shifted network management by decoupling the data and control planes. This enables programmatic control via software applications using open APIs. SDN's programmability has fueled its popularity but may have opened issues extending the attack surface by introducing vulnerable software. Therefore, the research community needs to have a deep and broad understanding of the risks posed by SDN to propose mitigating measures. The literature, however, lacks a comprehensive review of the current state of research in this direction. This paper addresses this gap by providing a comprehensive overview of the state-of-the-art research in SDN security focusing on the software (i.e., the controller, APIs, applications) part. We systematically reviewed 58 relevant publications to analyze trends, identify key testing and analysis methodologies, and categorize studied vulnerabilities. We further explore areas where the research community can make significant contributions. This work offers the most extensive and in-depth analysis of SDN software security to date.</li>
</ul>

<h3>Title: Contrastive Learning-Based privacy metrics in Tabular Synthetic Datasets</h3>
<ul>
<li><strong>Authors: </strong>Milton Nicols Plasencia Palacios, Sebastiano Saccani, Gabriele Sgroi, Alexander Boudewijn, Luca Bortolussi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13833">https://arxiv.org/abs/2502.13833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13833">https://arxiv.org/pdf/2502.13833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13833]] Contrastive Learning-Based privacy metrics in Tabular Synthetic Datasets(https://arxiv.org/abs/2502.13833)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>Synthetic data has garnered attention as a Privacy Enhancing Technology (PET) in sectors such as healthcare and finance. When using synthetic data in practical applications, it is important to provide protection guarantees. In the literature, two family of approaches are proposed for tabular data: on the one hand, Similarity-based methods aim at finding the level of similarity between training and synthetic data. Indeed, a privacy breach can occur if the generated data is consistently too similar or even identical to the train data. On the other hand, Attack-based methods conduce deliberate attacks on synthetic datasets. The success rates of these attacks reveal how secure the synthetic datasets are. In this paper, we introduce a contrastive method that improves privacy assessment of synthetic datasets by embedding the data in a more representative space. This overcomes obstacles surrounding the multitude of data types and attributes. It also makes the use of intuitive distance metrics possible for similarity measurements and as an attack vector. In a series of experiments with publicly available datasets, we compare the performances of similarity-based and attack-based methods, both with and without use of the contrastive learning-based embeddings. Our results show that relatively efficient, easy to implement privacy metrics can perform equally well as more advanced metrics explicitly modeling conditions for privacy referred to by the GDPR.</li>
</ul>

<h3>Title: Quantifying Memorization and Retriever Performance in Retrieval-Augmented Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Peter Carragher, Abhinand Jha, R Raghav, Kathleen M. Carley</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13836">https://arxiv.org/abs/2502.13836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13836">https://arxiv.org/pdf/2502.13836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13836]] Quantifying Memorization and Retriever Performance in Retrieval-Augmented Vision-Language Models(https://arxiv.org/abs/2502.13836)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate remarkable capabilities in question answering (QA), but metrics for assessing their reliance on memorization versus retrieval remain underdeveloped. Moreover, while finetuned models are state-of-the-art on closed-domain tasks, general-purpose models like GPT-4o exhibit strong zero-shot performance. This raises questions about the trade-offs between memorization, generalization, and retrieval. In this work, we analyze the extent to which multimodal retrieval-augmented VLMs memorize training data compared to baseline VLMs. Using the WebQA benchmark, we contrast finetuned models with baseline VLMs on multihop retrieval and question answering, examining the impact of finetuning on data memorization. To quantify memorization in end-to-end retrieval and QA systems, we propose several proxy metrics by investigating instances where QA succeeds despite retrieval failing. Our results reveal the extent to which finetuned models rely on memorization. In contrast, retrieval-augmented VLMs have lower memorization scores, at the cost of accuracy (72% vs 52% on WebQA test set). As such, our measures pose a challenge for future work to reconcile memorization and generalization in both Open-Domain QA and joint Retrieval-QA tasks.</li>
</ul>

<h3>Title: Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking</h3>
<ul>
<li><strong>Authors: </strong>Yilong Chen, Junyuan Shang, Zhenyu Zhang, Yanxi Xie, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13842">https://arxiv.org/abs/2502.13842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13842">https://arxiv.org/pdf/2502.13842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13842]] Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking(https://arxiv.org/abs/2502.13842)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) face inherent performance bottlenecks under parameter constraints, particularly in processing critical tokens that demand complex reasoning. Empirical analysis reveals challenging tokens induce abrupt gradient spikes across layers, exposing architectural stress points in standard Transformers. Building on this insight, we propose Inner Thinking Transformer (ITT), which reimagines layer computations as implicit thinking steps. ITT dynamically allocates computation through Adaptive Token Routing, iteratively refines representations via Residual Thinking Connections, and distinguishes reasoning phases using Thinking Step Encoding. ITT enables deeper processing of critical tokens without parameter expansion. Evaluations across 162M-466M parameter models show ITT achieves 96.5\% performance of a 466M Transformer using only 162M parameters, reduces training data by 43.2\%, and outperforms Transformer/Loop variants in 11 benchmarks. By enabling elastic computation allocation during inference, ITT balances performance and efficiency through architecture-aware optimization of implicit thinking pathways.</li>
</ul>

<h3>Title: Fine-grained Fallacy Detection with Human Label Variation</h3>
<ul>
<li><strong>Authors: </strong>Alan Ramponi, Agnese Daffara, Sara Tonelli</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13853">https://arxiv.org/abs/2502.13853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13853">https://arxiv.org/pdf/2502.13853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13853]] Fine-grained Fallacy Detection with Human Label Variation(https://arxiv.org/abs/2502.13853)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce Faina, the first dataset for fallacy detection that embraces multiple plausible answers and natural disagreement. Faina includes over 11K span-level annotations with overlaps across 20 fallacy types on social media posts in Italian about migration, climate change, and public health given by two expert annotators. Through an extensive annotation study that allowed discussion over multiple rounds, we minimize annotation errors whilst keeping signals of human label variation. Moreover, we devise a framework that goes beyond "single ground truth" evaluation and simultaneously accounts for multiple (equally reliable) test sets and the peculiarities of the task, i.e., partial span matches, overlaps, and the varying severity of labeling errors. Our experiments across four fallacy detection setups show that multi-task and multi-label transformer-based approaches are strong baselines across all settings. We release our data, code, and annotation guidelines to foster research on fallacy detection and human label variation more broadly.</li>
</ul>

<h3>Title: MagicGeo: Training-Free Text-Guided Geometric Diagram Generation</h3>
<ul>
<li><strong>Authors: </strong>Junxiao Wang, Ting Zhang, Heng Yu, Jingdong Wang, Hua Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13855">https://arxiv.org/abs/2502.13855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13855">https://arxiv.org/pdf/2502.13855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13855]] MagicGeo: Training-Free Text-Guided Geometric Diagram Generation(https://arxiv.org/abs/2502.13855)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Geometric diagrams are critical in conveying mathematical and scientific concepts, yet traditional diagram generation methods are often manual and resource-intensive. While text-to-image generation has made strides in photorealistic imagery, creating accurate geometric diagrams remains a challenge due to the need for precise spatial relationships and the scarcity of geometry-specific datasets. This paper presents MagicGeo, a training-free framework for generating geometric diagrams from textual descriptions. MagicGeo formulates the diagram generation process as a coordinate optimization problem, ensuring geometric correctness through a formal language solver, and then employs coordinate-aware generation. The framework leverages the strong language translation capability of large language models, while formal mathematical solving ensures geometric correctness. We further introduce MagicGeoBench, a benchmark dataset of 220 geometric diagram descriptions, and demonstrate that MagicGeo outperforms current methods in both qualitative and quantitative evaluations. This work provides a scalable, accurate solution for automated diagram generation, with significant implications for educational and academic applications.</li>
</ul>

<h3>Title: MSVCOD:A Large-Scale Multi-Scene Dataset for Video Camouflage Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Shuyong Gao, Yu'ang Feng, Qishan Wang, Lingyi Hong, Xinyu Zhou, Liu Fei, Yan Wang, Wenqiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13859">https://arxiv.org/abs/2502.13859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13859">https://arxiv.org/pdf/2502.13859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13859]] MSVCOD:A Large-Scale Multi-Scene Dataset for Video Camouflage Object Detection(https://arxiv.org/abs/2502.13859)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, extraction</a></li>
<li><strong>Abstract: </strong>Video Camouflaged Object Detection (VCOD) is a challenging task which aims to identify objects that seamlessly concealed within the background in videos. The dynamic properties of video enable detection of camouflaged objects through motion cues or varied perspectives. Previous VCOD datasets primarily contain animal objects, limiting the scope of research to wildlife scenarios. However, the applications of VCOD extend beyond wildlife and have significant implications in security, art, and medical fields. Addressing this problem, we construct a new large-scale multi-domain VCOD dataset MSVCOD. To achieve high-quality annotations, we design a semi-automatic iterative annotation pipeline that reduces costs while maintaining annotation accuracy. Our MSVCOD is the largest VCOD dataset to date, introducing multiple object categories including human, animal, medical, and vehicle objects for the first time, while also expanding background diversity across various environments. This expanded scope increases the practical applicability of the VCOD task in camouflaged object detection. Alongside this dataset, we introduce a one-steam video camouflage object detection model that performs both feature extraction and information fusion without additional motion feature fusion modules. Our framework achieves state-of-the-art results on the existing VCOD animal dataset and the proposed MSVCOD. The dataset and code will be made publicly available.</li>
</ul>

<h3>Title: SPEX: Scaling Feature Interaction Explanations for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Justin Singh Kang, Landon Butler, Abhineet Agarwal, Yigit Efe Erginbas, Ramtin Pedarsani, Kannan Ramchandran, Bin Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13870">https://arxiv.org/abs/2502.13870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13870">https://arxiv.org/pdf/2502.13870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13870]] SPEX: Scaling Feature Interaction Explanations for LLMs(https://arxiv.org/abs/2502.13870)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have revolutionized machine learning due to their ability to capture complex interactions between input features. Popular post-hoc explanation methods like SHAP provide marginal feature attributions, while their extensions to interaction importances only scale to small input lengths ($\approx 20$). We propose Spectral Explainer (SPEX), a model-agnostic interaction attribution algorithm that efficiently scales to large input lengths ($\approx 1000)$. SPEX exploits underlying natural sparsity among interactions -- common in real-world data -- and applies a sparse Fourier transform using a channel decoding algorithm to efficiently identify important interactions. We perform experiments across three difficult long-context datasets that require LLMs to utilize interactions between inputs to complete the task. For large inputs, SPEX outperforms marginal attribution methods by up to 20% in terms of faithfully reconstructing LLM outputs. Further, SPEX successfully identifies key features and interactions that strongly influence model output. For one of our datasets, HotpotQA, SPEX provides interactions that align with human annotations. Finally, we use our model-agnostic approach to generate explanations to demonstrate abstract reasoning in closed-source LLMs (GPT-4o mini) and compositional reasoning in vision-language models.</li>
</ul>

<h3>Title: PSCon: Toward Conversational Product Search</h3>
<ul>
<li><strong>Authors: </strong>Jie Zou, Mohammad Aliannejadi, Evangelos Kanoulas, Shuxi Han, Heli Ma, Zheng Wang, Yang Yang, Heng Tao Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13881">https://arxiv.org/abs/2502.13881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13881">https://arxiv.org/pdf/2502.13881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13881]] PSCon: Toward Conversational Product Search(https://arxiv.org/abs/2502.13881)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Conversational Product Search (CPS) is confined to simulated conversations due to the lack of real-world CPS datasets that reflect human-like language. Additionally, current conversational datasets are limited to support cross-market and multi-lingual usage. In this paper, we introduce a new CPS data collection protocol and present PSCon, a novel CPS dataset designed to assist product search via human-like conversations. The dataset is constructed using a coached human-to-human data collection protocol and supports two languages and dual markets. Also, the dataset enables thorough exploration of six subtasks of CPS: user intent detection, keyword extraction, system action prediction, question selection, item ranking, and response generation. Furthermore, we also offer an analysis of the dataset and propose a benchmark model on the proposed CPS dataset.</li>
</ul>

<h3>Title: DataSciBench: An LLM Agent Benchmark for Data Science</h3>
<ul>
<li><strong>Authors: </strong>Dan Zhang, Sining Zhoubian, Min Cai, Fengzu Li, Lekang Yang, Wei Wang, Tianjiao Dong, Ziniu Hu, Jie Tang, Yisong Yue</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13897">https://arxiv.org/abs/2502.13897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13897">https://arxiv.org/pdf/2502.13897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13897]] DataSciBench: An LLM Agent Benchmark for Data Science(https://arxiv.org/abs/2502.13897)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents DataSciBench, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science. Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated. In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT by leveraging collected prompts, predefined task types, and aggregate functions (metrics). Furthermore, we propose an innovative Task - Function - Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules. Our experimental framework involves testing 6 API-based models, 8 open-source general models, and 9 open-source code generation models using the diverse set of prompts we have gathered. This approach aims to provide a more comprehensive and rigorous evaluation of LLMs in data science, revealing their strengths and weaknesses. Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. We release all code and data at this https URL.</li>
</ul>

<h3>Title: GroundCap: A Visually Grounded Image Captioning Dataset</h3>
<ul>
<li><strong>Authors: </strong>Daniel A. P. Oliveira, Loureno Teodoro, David Martins de Matos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13898">https://arxiv.org/abs/2502.13898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13898">https://arxiv.org/pdf/2502.13898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13898]] GroundCap: A Visually Grounded Image Captioning Dataset(https://arxiv.org/abs/2502.13898)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Current image captioning systems lack the ability to link descriptive text to specific visual elements, making their outputs difficult to verify. While recent approaches offer some grounding capabilities, they cannot track object identities across multiple references or ground both actions and objects simultaneously. We propose a novel ID-based grounding system that enables consistent object reference tracking and action-object linking, and present GroundCap, a dataset containing 52,016 images from 77 movies, with 344 human-annotated and 52,016 automatically generated captions. Each caption is grounded on detected objects (132 classes) and actions (51 classes) using a tag system that maintains object identity while linking actions to the corresponding objects. Our approach features persistent object IDs for reference tracking, explicit action-object linking, and segmentation of background elements through K-means clustering. We propose gMETEOR, a metric combining caption quality with grounding accuracy, and establish baseline performance by fine-tuning Pixtral-12B. Human evaluation demonstrates our approach's effectiveness in producing verifiable descriptions with coherent object references.</li>
</ul>

<h3>Title: How Do LLMs Perform Two-Hop Reasoning in Context?</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Guo, Hanlin Zhu, Ruiqi Zhang, Jiantao Jiao, Song Mei, Michael I. Jordan, Stuart Russell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13913">https://arxiv.org/abs/2502.13913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13913">https://arxiv.org/pdf/2502.13913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13913]] How Do LLMs Perform Two-Hop Reasoning in Context?(https://arxiv.org/abs/2502.13913)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>"Socrates is human. All humans are mortal. Therefore, Socrates is mortal." This classical example demonstrates two-hop reasoning, where a conclusion logically follows from two connected premises. While transformer-based Large Language Models (LLMs) can make two-hop reasoning, they tend to collapse to random guessing when faced with distracting premises. To understand the underlying mechanism, we train a three-layer transformer on synthetic two-hop reasoning tasks. The training dynamics show two stages: a slow learning phase, where the 3-layer transformer performs random guessing like LLMs, followed by an abrupt phase transitions, where the 3-layer transformer suddenly reaches $100%$ accuracy. Through reverse engineering, we explain the inner mechanisms for how models learn to randomly guess between distractions initially, and how they learn to ignore distractions eventually. We further propose a three-parameter model that supports the causal claims for the mechanisms to the training dynamics of the transformer. Finally, experiments on LLMs suggest that the discovered mechanisms generalize across scales. Our methodologies provide new perspectives for scientific understandings of LLMs and our findings provide new insights into how reasoning emerges during training.</li>
</ul>

<h3>Title: TESS 2: A Large-Scale Generalist Diffusion Language Model</h3>
<ul>
<li><strong>Authors: </strong>Jaesung Tae, Hamish Ivison, Sachin Kumar, Arman Cohan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13917">https://arxiv.org/abs/2502.13917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13917">https://arxiv.org/pdf/2502.13917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13917]] TESS 2: A Large-Scale Generalist Diffusion Language Model(https://arxiv.org/abs/2502.13917)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce TESS 2, a general instruction-following diffusion language model that outperforms contemporary instruction-tuned diffusion models, as well as matches and sometimes exceeds strong autoregressive (AR) models. We train TESS 2 by first adapting a strong AR model via continued pretraining with the usual cross-entropy as diffusion loss, and then performing further instruction tuning. We find that adaptation training as well as the choice of the base model is crucial for training good instruction-following diffusion models. We further propose reward guidance, a novel and modular inference-time guidance procedure to align model outputs without needing to train the underlying model. Finally, we show that TESS 2 further improves with increased inference-time compute, highlighting the utility of diffusion LMs in having fine-grained controllability over the amount of compute used at inference time. Code and models are available at this https URL.</li>
</ul>

<h3>Title: Exploring Code Language Models for Automated HLS-based Hardware Generation: Benchmark, Infrastructure and Analysis</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Gai, Hao (Mark)Chen, Zhican Wang, Hongyu Zhou, Wanru Zhao, Nicholas Lane, Hongxiang Fan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13921">https://arxiv.org/abs/2502.13921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13921">https://arxiv.org/pdf/2502.13921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13921]] Exploring Code Language Models for Automated HLS-based Hardware Generation: Benchmark, Infrastructure and Analysis(https://arxiv.org/abs/2502.13921)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in code generation have illuminated the potential of employing large language models (LLMs) for general-purpose programming languages such as Python and C++, opening new opportunities for automating software development and enhancing programmer productivity. The potential of LLMs in software programming has sparked significant interest in exploring automated hardware generation and automation. Although preliminary endeavors have been made to adopt LLMs in generating hardware description languages (HDLs), several challenges persist in this direction. First, the volume of available HDL training data is substantially smaller compared to that for software programming languages. Second, the pre-trained LLMs, mainly tailored for software code, tend to produce HDL designs that are more error-prone. Third, the generation of HDL requires a significantly higher number of tokens compared to software programming, leading to inefficiencies in cost and energy consumption. To tackle these challenges, this paper explores leveraging LLMs to generate High-Level Synthesis (HLS)-based hardware design. Although code generation for domain-specific programming languages is not new in the literature, we aim to provide experimental results, insights, benchmarks, and evaluation infrastructure to investigate the suitability of HLS over low-level HDLs for LLM-assisted hardware design generation. To achieve this, we first finetune pre-trained models for HLS-based hardware generation, using a collected dataset with text prompts and corresponding reference HLS designs. An LLM-assisted framework is then proposed to automate end-to-end hardware code generation, which also investigates the impact of chain-of-thought and feedback loops promoting techniques on HLS-design generation. Limited by the timeframe of this research, we plan to evaluate more advanced reasoning models in the future.</li>
</ul>

<h3>Title: LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Guanzheng Chen, Xin Li, Michael Qizhe Shieh, Lidong Bing</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13922">https://arxiv.org/abs/2502.13922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13922">https://arxiv.org/pdf/2502.13922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13922]] LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization(https://arxiv.org/abs/2502.13922)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities through pretraining and alignment. However, superior short-context LLMs may underperform in long-context scenarios due to insufficient long-context alignment. This alignment process remains challenging due to the impracticality of human annotation for extended contexts and the difficulty in balancing short- and long-context performance. To address these challenges, we introduce LongPO, that enables short-context LLMs to self-evolve to excel on long-context tasks by internally transferring short-context capabilities. LongPO harnesses LLMs to learn from self-generated short-to-long preference data, comprising paired responses generated for identical instructions with long-context inputs and their compressed short-context counterparts, respectively. This preference reveals capabilities and potentials of LLMs cultivated during short-context alignment that may be diminished in under-aligned long-context scenarios. Additionally, LongPO incorporates a short-to-long KL constraint to mitigate short-context performance decline during long-context alignment. When applied to Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully retains short-context performance and largely outperforms naive SFT and DPO in both long- and short-context tasks. Specifically, \ourMethod-trained models can achieve results on long-context benchmarks comparable to, or even surpassing, those of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context annotation and larger parameter scales.</li>
</ul>

<h3>Title: Qwen2.5-VL Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin,  (additional authors not shown)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13923">https://arxiv.org/abs/2502.13923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13923">https://arxiv.org/pdf/2502.13923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13923]] Qwen2.5-VL Technical Report(https://arxiv.org/abs/2502.13923)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language series, which demonstrates significant advancements in both foundational capabilities and innovative functionalities. Qwen2.5-VL achieves a major leap forward in understanding and interacting with the world through enhanced visual recognition, precise object localization, robust document parsing, and long-video comprehension. A standout feature of Qwen2.5-VL is its ability to localize objects using bounding boxes or points accurately. It provides robust structured data extraction from invoices, forms, and tables, as well as detailed analysis of charts, diagrams, and layouts. To handle complex inputs, Qwen2.5-VL introduces dynamic resolution processing and absolute time encoding, enabling it to process images of varying sizes and videos of extended durations (up to hours) with second-level event localization. This allows the model to natively perceive spatial scales and temporal dynamics without relying on traditional normalization techniques. By training a native dynamic-resolution Vision Transformer (ViT) from scratch and incorporating Window Attention, we reduce computational overhead while maintaining native resolution. As a result, Qwen2.5-VL excels not only in static image and document understanding but also as an interactive visual agent capable of reasoning, tool usage, and task execution in real-world scenarios such as operating computers and mobile devices. Qwen2.5-VL is available in three sizes, addressing diverse use cases from edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model matches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly excelling in document and diagram understanding. Additionally, Qwen2.5-VL maintains robust linguistic performance, preserving the core language competencies of the Qwen2.5 LLM.</li>
</ul>

<h3>Title: Formal verification in Solidity and Move: insights from a comparative analysis</h3>
<ul>
<li><strong>Authors: </strong>Massimo Bartoletti, Silvia Crafa, Enrico Lipparini</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13929">https://arxiv.org/abs/2502.13929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13929">https://arxiv.org/pdf/2502.13929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13929]] Formal verification in Solidity and Move: insights from a comparative analysis(https://arxiv.org/abs/2502.13929)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Formal verification plays a crucial role in making smart contracts safer, being able to find bugs or to guarantee their absence, as well as checking whether the business logic is correctly implemented. For Solidity, even though there already exist several mature verification tools, the semantical quirks of the language can make verification quite hard in practice. Move, on the other hand, has been designed with security and verification in mind, and it has been accompanied since its early stages by a formal verification tool, the Move Prover. In this paper, we investigate through a comparative analysis: 1) how the different designs of the two contract languages impact verification, and 2) what is the state-of-the-art of verification tools for the two languages, and how do they compare on three paradigmatic use cases. Our investigation is supported by an open dataset of verification tasks performed in Certora and in the Aptos Move Prover.</li>
</ul>

<h3>Title: Image compositing is all you need for data augmentation</h3>
<ul>
<li><strong>Authors: </strong>Ang Jia Ning Shermaine, Michalis Lazarou, Tania Stathaki</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13936">https://arxiv.org/abs/2502.13936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13936">https://arxiv.org/pdf/2502.13936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13936]] Image compositing is all you need for data augmentation(https://arxiv.org/abs/2502.13936)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper investigates the impact of various data augmentation techniques on the performance of object detection models. Specifically, we explore classical augmentation methods, image compositing, and advanced generative models such as Stable Diffusion XL and ControlNet. The objective of this work is to enhance model robustness and improve detection accuracy, particularly when working with limited annotated data. Using YOLOv8, we fine-tune the model on a custom dataset consisting of commercial and military aircraft, applying different augmentation strategies. Our experiments show that image compositing offers the highest improvement in detection performance, as measured by precision, recall, and mean Average Precision (mAP@0.50). Other methods, including Stable Diffusion XL and ControlNet, also demonstrate significant gains, highlighting the potential of advanced data augmentation techniques for object detection tasks. The results underline the importance of dataset diversity and augmentation in achieving better generalization and performance in real-world applications. Future work will explore the integration of semi-supervised learning methods and further optimizations to enhance model performance across larger and more complex datasets.</li>
</ul>

<h3>Title: Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region</h3>
<ul>
<li><strong>Authors: </strong>Chak Tou Leong, Qingyu Yin, Jian Wang, Wenjie Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13946">https://arxiv.org/abs/2502.13946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13946">https://arxiv.org/pdf/2502.13946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13946]] Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region(https://arxiv.org/abs/2502.13946)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The safety alignment of large language models (LLMs) remains vulnerable, as their initial behavior can be easily jailbroken by even relatively simple attacks. Since infilling a fixed template between the input instruction and initial model output is a common practice for existing LLMs, we hypothesize that this template is a key factor behind their vulnerabilities: LLMs' safety-related decision-making overly relies on the aggregated information from the template region, which largely influences these models' safety behavior. We refer to this issue as template-anchored safety alignment. In this paper, we conduct extensive experiments and verify that template-anchored safety alignment is widespread across various aligned LLMs. Our mechanistic analyses demonstrate how it leads to models' susceptibility when encountering inference-time jailbreak attacks. Furthermore, we show that detaching safety mechanisms from the template region is promising in mitigating vulnerabilities to jailbreak attacks. We encourage future research to develop more robust safety alignment techniques that reduce reliance on the template region.</li>
</ul>

<h3>Title: IP-Composer: Semantic Composition of Visual Concepts</h3>
<ul>
<li><strong>Authors: </strong>Sara Dorfman, Dana Cohen-Bar, Rinon Gal, Daniel Cohen-Or</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13951">https://arxiv.org/abs/2502.13951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13951">https://arxiv.org/pdf/2502.13951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13951]] IP-Composer: Semantic Composition of Visual Concepts(https://arxiv.org/abs/2502.13951)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Content creators often draw inspiration from multiple visual sources, combining distinct elements to craft new compositions. Modern computational approaches now aim to emulate this fundamental creative process. Although recent diffusion models excel at text-guided compositional synthesis, text as a medium often lacks precise control over visual details. Image-based composition approaches can capture more nuanced features, but existing methods are typically limited in the range of concepts they can capture, and require expensive training procedures or specialized data. We present IP-Composer, a novel training-free approach for compositional image generation that leverages multiple image references simultaneously, while using natural language to describe the concept to be extracted from each image. Our method builds on IP-Adapter, which synthesizes novel images conditioned on an input image's CLIP embedding. We extend this approach to multiple visual inputs by crafting composite embeddings, stitched from the projections of multiple input images onto concept-specific CLIP-subspaces identified through text. Through comprehensive evaluation, we show that our approach enables more precise control over a larger range of visual concept compositions.</li>
</ul>

<h3>Title: Latent Distribution Decoupling: A Probabilistic Framework for Uncertainty-Aware Multimodal Emotion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Jingwang Huang, Jiang Zhong, Qin Lei, Jinpeng Gao, Yuming Yang, Sirui Wang, Peiguang Li, Kaiwen Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13954">https://arxiv.org/abs/2502.13954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13954">https://arxiv.org/pdf/2502.13954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13954]] Latent Distribution Decoupling: A Probabilistic Framework for Uncertainty-Aware Multimodal Emotion Recognition(https://arxiv.org/abs/2502.13954)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Multimodal multi-label emotion recognition (MMER) aims to identify the concurrent presence of multiple emotions in multimodal data. Existing studies primarily focus on improving fusion strategies and modeling modality-to-label dependencies. However, they often overlook the impact of \textbf{aleatoric uncertainty}, which is the inherent noise in the multimodal data and hinders the effectiveness of modality fusion by introducing ambiguity into feature representations. To address this issue and effectively model aleatoric uncertainty, this paper proposes Latent emotional Distribution Decomposition with Uncertainty perception (LDDU) framework from a novel perspective of latent emotional space probabilistic modeling. Specifically, we introduce a contrastive disentangled distribution mechanism within the emotion space to model the multimodal data, allowing for the extraction of semantic features and uncertainty. Furthermore, we design an uncertainty-aware fusion multimodal method that accounts for the dispersed distribution of uncertainty and integrates distribution information. Experimental results show that LDDU achieves state-of-the-art performance on the CMU-MOSEI and M$^3$ED datasets, highlighting the importance of uncertainty modeling in MMER. Code is available at this https URL\this http URL.</li>
</ul>

<h3>Title: LIDDIA: Language-based Intelligent Drug Discovery Agent</h3>
<ul>
<li><strong>Authors: </strong>Reza Averly, Frazier N. Baker, Xia Ning</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13959">https://arxiv.org/abs/2502.13959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13959">https://arxiv.org/pdf/2502.13959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13959]] LIDDIA: Language-based Intelligent Drug Discovery Agent(https://arxiv.org/abs/2502.13959)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Drug discovery is a long, expensive, and complex process, relying heavily on human medicinal chemists, who can spend years searching the vast space of potential therapies. Recent advances in artificial intelligence for chemistry have sought to expedite individual drug discovery tasks; however, there remains a critical need for an intelligent agent that can navigate the drug discovery process. Towards this end, we introduce LIDDiA, an autonomous agent capable of intelligently navigating the drug discovery process in silico. By leveraging the reasoning capabilities of large language models, LIDDiA serves as a low-cost and highly-adaptable tool for autonomous drug discovery. We comprehensively examine LIDDiA, demonstrating that (1) it can generate molecules meeting key pharmaceutical criteria on over 70% of 30 clinically relevant targets, (2) it intelligently balances exploration and exploitation in the chemical space, and (3) it can identify promising novel drug candidates on EGFR, a critical target for cancers.</li>
</ul>

<h3>Title: Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering</h3>
<ul>
<li><strong>Authors: </strong>William Jurayj, Jeffrey Cheng, Benjamin Van Durme</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13962">https://arxiv.org/abs/2502.13962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13962">https://arxiv.org/pdf/2502.13962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13962]] Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering(https://arxiv.org/abs/2502.13962)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Scaling the test-time compute of large language models has demonstrated impressive performance on reasoning benchmarks. However, existing evaluations of test-time scaling make the strong assumption that a reasoning system should always give an answer to any question provided. This overlooks concerns about whether a model is confident in its answer, and whether it is appropriate to always provide a response. To address these concerns, we extract confidence scores during reasoning for thresholding model responses. We find that increasing compute budget at inference time not only helps models answer more questions correctly, but also increases confidence in correct responses. We then extend the current paradigm of zero-risk responses during evaluation by considering settings with non-zero levels of response risk, and suggest a recipe for reporting evaluations under these settings.</li>
</ul>

<h3>Title: MuDAF: Long-Context Multi-Document Attention Focusing through Contrastive Learning on Attention Heads</h3>
<ul>
<li><strong>Authors: </strong>Weihao Liu, Ning Wu, Shiping Yang, Wenbiao Ding, Shining Liang, Ming Gong, Dongmei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13963">https://arxiv.org/abs/2502.13963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13963">https://arxiv.org/pdf/2502.13963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13963]] MuDAF: Long-Context Multi-Document Attention Focusing through Contrastive Learning on Attention Heads(https://arxiv.org/abs/2502.13963)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) frequently show distracted attention due to irrelevant information in the input, which severely impairs their long-context capabilities. Inspired by recent studies on the effectiveness of retrieval heads in long-context factutality, we aim at addressing this distraction issue through improving such retrieval heads directly. We propose Multi-Document Attention Focusing (MuDAF), a novel method that explicitly optimizes the attention distribution at the head level through contrastive learning. According to the experimental results, MuDAF can significantly improve the long-context question answering performance of LLMs, especially in multi-document question answering. Extensive evaluations on retrieval scores and attention visualizations show that MuDAF possesses great potential in making attention heads more focused on relevant information and reducing attention distractions.</li>
</ul>

<h3>Title: Autellix: An Efficient Serving Engine for LLM Agents as General Programs</h3>
<ul>
<li><strong>Authors: </strong>Michael Luo, Xiaoxiang Shi, Colin Cai, Tianjun Zhang, Justin Wong, Yichuan Wang, Chi Wang, Yanping Huang, Zhifeng Chen, Joseph E. Gonzalez, Ion Stoica</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13965">https://arxiv.org/abs/2502.13965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13965">https://arxiv.org/pdf/2502.13965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13965]] Autellix: An Efficient Serving Engine for LLM Agents as General Programs(https://arxiv.org/abs/2502.13965)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) applications are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which scale LLM calls and output tokens to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems ignore dependencies between programs and calls, missing significant opportunities for optimization. Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and the program. To address this, we introduce Autellix, an LLM serving system that treats programs as first-class citizens to minimize their end-to-end latencies. Autellix intercepts LLM calls submitted by programs, enriching schedulers with program-level context. We propose two scheduling algorithms-for single-threaded and distributed programs-that preempt and prioritize LLM calls based on their programs' previously completed calls. Our evaluation demonstrates that across diverse LLMs and agentic workloads, Autellix improves throughput of programs by 4-15x at the same latency compared to state-of-the-art systems, such as vLLM.</li>
</ul>

<h3>Title: FlexTok: Resampling Images into 1D Token Sequences of Flexible Length</h3>
<ul>
<li><strong>Authors: </strong>Roman Bachmann, Jesse Allardice, David Mizrahi, Enrico Fini, Ouzhan Fatih Kar, Elmira Amirloo, Alaaeldin El-Nouby, Amir Zamir, Afshin Dehghan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13967">https://arxiv.org/abs/2502.13967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13967">https://arxiv.org/pdf/2502.13967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13967]] FlexTok: Resampling Images into 1D Token Sequences of Flexible Length(https://arxiv.org/abs/2502.13967)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Image tokenization has enabled major advances in autoregressive image generation by providing compressed, discrete representations that are more efficient to process than raw pixels. While traditional approaches use 2D grid tokenization, recent methods like TiTok have shown that 1D tokenization can achieve high generation quality by eliminating grid redundancies. However, these methods typically use a fixed number of tokens and thus cannot adapt to an image's inherent complexity. We introduce FlexTok, a tokenizer that projects 2D images into variable-length, ordered 1D token sequences. For example, a 256x256 image can be resampled into anywhere from 1 to 256 discrete tokens, hierarchically and semantically compressing its information. By training a rectified flow model as the decoder and using nested dropout, FlexTok produces plausible reconstructions regardless of the chosen token sequence length. We evaluate our approach in an autoregressive generation setting using a simple GPT-style Transformer. On ImageNet, this approach achieves an FID<2 across 8 to 128 tokens, outperforming TiTok and matching state-of-the-art methods with far fewer tokens. We further extend the model to support to text-conditioned image generation and examine how FlexTok relates to traditional 2D tokenization. A key finding is that FlexTok enables next-token prediction to describe images in a coarse-to-fine "visual vocabulary", and that the number of tokens to generate depends on the complexity of the generation task.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
