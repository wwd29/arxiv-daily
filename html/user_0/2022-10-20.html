<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Prove You Owned Me: One Step beyond RFID Tag/Mutual Authentication. (arXiv:2210.10244v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10244">http://arxiv.org/abs/2210.10244</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10244] Prove You Owned Me: One Step beyond RFID Tag/Mutual Authentication](http://arxiv.org/abs/2210.10244)</code></li>
<li>Summary: <p>Radio Frequency Identification (RFID) is a key technology used in many
applications. In the past decades, plenty of secure and privacy-preserving RFID
tag/mutual authentication protocols as well as formal frameworks for evaluating
them have been proposed. However, we notice that a property, namely proof of
possession (PoP), has not been rigorously studied till now, despite it has
significant value in many RFID applications. For example, in RFID-enabled
supply chains, PoP helps prevent dis-honest parties from publishing information
about products/tags that they actually have never processed.
</p></li>
</ul>

<p>We propose the first formal framework for RFID tag/mutual authentication with
PoP after correcting deficiencies of some existing RFID formal frameworks. We
provide a generic construction to transform an RFID tag/mutual authentication
protocol to one that supports PoP using a cryptographic hash function, a
pseudorandom function (PRF) and a signature scheme. We prove that the
constructed protocol is secure and privacy-preserving under our framework if
all the building blocks possess desired security properties. Finally, we show
an RFID mutual authentication protocol with PoP. Arming tag/mutual
authentication protocols with PoP is an important step to strengthen
RFID-enabled systems as it bridges the security gap between physical layer and
data layer, and reduces the misuses of RFID-related data.
</p>

<h3>Title: Secure and Efficient Multi-Signature Schemes for Fabric: An Enterprise Blockchain Platform. (arXiv:2210.10294v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10294">http://arxiv.org/abs/2210.10294</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10294] Secure and Efficient Multi-Signature Schemes for Fabric: An Enterprise Blockchain Platform](http://arxiv.org/abs/2210.10294)</code></li>
<li>Summary: <p>Digital signature is a major component of transactions on Blockchain
platforms, especially in enterprise Blockchain platforms, where multiple
signatures from a set of peers need to be produced to endorse a transaction.
However, such process is often complex and time-consuming. Multi-signature,
which can improve transaction efficiency by having a set of signers cooperate
to produce a joint signature, has attracted extensive attentions. In this work,
we propose two multi-signature schemes, GMS and AGMS, which are proved to be
more secure and efficient than state-of-the-art multi-signature schemes.
Besides, we implement the proposed schemes in a real Enterprise Blockchain
platform, Fabric. Experiment results show that the proposed AGMS scheme helps
achieve the goal of high transaction efficiency, low storage complexity, as
well as high robustness against rogue-key attacks and k-sum problem attacks.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Performance of different machine learning methods on activity recognition and pose estimation datasets. (arXiv:2210.10247v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10247">http://arxiv.org/abs/2210.10247</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10247] Performance of different machine learning methods on activity recognition and pose estimation datasets](http://arxiv.org/abs/2210.10247)</code></li>
<li>Summary: <p>With advancements in computer vision taking place day by day, recently a lot
of light is being shed on activity recognition. With the range for real-world
applications utilizing this field of study increasing across a multitude of
industries such as security and healthcare, it becomes crucial for businesses
to distinguish which machine learning methods perform better than others in the
area. This paper strives to aid in this predicament i.e. building upon previous
related work, it employs both classical and ensemble approaches on rich pose
estimation (OpenPose) and HAR datasets. Making use of appropriate metrics to
evaluate the performance for each model, the results show that overall, random
forest yields the highest accuracy in classifying ADLs. Relatively all the
models have excellent performance across both datasets, except for logistic
regression and AdaBoost perform poorly in the HAR one. With the limitations of
this paper also discussed in the end, the scope for further research is vast,
which can use this paper as a base in aims of producing better results.
</p></li>
</ul>

<h3>Title: Online LiDAR-Camera Extrinsic Parameters Self-checking. (arXiv:2210.10537v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10537">http://arxiv.org/abs/2210.10537</a></li>
<li>Code URL: <a href="https://github.com/opencalib/lidar2camera_self-check">https://github.com/opencalib/lidar2camera_self-check</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10537] Online LiDAR-Camera Extrinsic Parameters Self-checking](http://arxiv.org/abs/2210.10537)</code></li>
<li>Summary: <p>With the development of neural networks and the increasing popularity of
automatic driving, the calibration of the LiDAR and the camera has attracted
more and more attention. This calibration task is multi-modal, where the rich
color and texture information captured by the camera and the accurate
three-dimensional spatial information from the LiDAR is incredibly significant
for downstream tasks. Current research interests mainly focus on obtaining
accurate calibration results through information fusion. However, they seldom
analyze whether the calibrated results are correct or not, which could be of
significant importance in real-world applications. For example, in large-scale
production, the LiDARs and the cameras of each smart car have to get
well-calibrated as the car leaves the production line, while in the rest of the
car life period, the poses of the LiDARs and cameras should also get
continually supervised to ensure the security. To this end, this paper proposes
a self-checking algorithm to judge whether the extrinsic parameters are
well-calibrated by introducing a binary classification network based on the
fused information from the camera and the LiDAR. Moreover, since there is no
such dataset for the task in this work, we further generate a new dataset
branch from the KITTI dataset tailored for the task. Our experiments on the
proposed dataset branch demonstrate the performance of our method. To the best
of our knowledge, this is the first work to address the significance of
continually checking the calibrated extrinsic parameters for autonomous
driving. The code is open-sourced on the Github website at
https://github.com/OpenCalib/LiDAR2camera_self-check.
</p></li>
</ul>

<h3>Title: Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP. (arXiv:2210.10683v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10683">http://arxiv.org/abs/2210.10683</a></li>
<li>Code URL: <a href="https://github.com/thunlp/advbench">https://github.com/thunlp/advbench</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10683] Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP](http://arxiv.org/abs/2210.10683)</code></li>
<li>Summary: <p>Textual adversarial samples play important roles in multiple subfields of NLP
research, including security, evaluation, explainability, and data
augmentation. However, most work mixes all these roles, obscuring the problem
definitions and research goals of the security role that aims to reveal the
practical concerns of NLP models. In this paper, we rethink the research
paradigm of textual adversarial samples in security scenarios. We discuss the
deficiencies in previous work and propose our suggestions that the research on
the Security-oriented adversarial NLP (SoadNLP) should: (1) evaluate their
methods on security tasks to demonstrate the real-world concerns; (2) consider
real-world attackers' goals, instead of developing impractical methods. To this
end, we first collect, process, and release a security datasets collection
Advbench. Then, we reformalize the task and adjust the emphasis on different
goals in SoadNLP. Next, we propose a simple method based on heuristic rules
that can easily fulfill the actual adversarial goals to simulate real-world
attack methods. We conduct experiments on both the attack and the defense sides
on Advbench. Experimental results show that our method has higher practical
value, indicating that the research paradigm in SoadNLP may start from our new
benchmark. All the code and data of Advbench can be obtained at
\url{https://github.com/thunlp/Advbench}.
</p></li>
</ul>

<h3>Title: An Empirical Analysis of SMS Scam Detection Systems. (arXiv:2210.10451v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10451">http://arxiv.org/abs/2210.10451</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10451] An Empirical Analysis of SMS Scam Detection Systems](http://arxiv.org/abs/2210.10451)</code></li>
<li>Summary: <p>The short message service (SMS) was introduced a generation ago to the mobile
phone users. They make up the world's oldest large-scale network, with billions
of users and therefore attracts a lot of fraud. Due to the convergence of
mobile network with internet, SMS based scams can potentially compromise the
security of internet services as well. In this study, we present a new SMS scam
dataset consisting of 153,551 SMSes. This dataset that we will release publicly
for research purposes represents the largest publicly-available SMS scam
dataset. We evaluate and compare the performance achieved by several
established machine learning methods on the new dataset, ranging from shallow
machine learning approaches to deep neural networks to syntactic and semantic
feature models. We then study the existing models from an adversarial viewpoint
by assessing its robustness against different level of adversarial
manipulation. This perspective consolidates the current state of the art in SMS
Spam filtering, highlights the limitations and the opportunities to improve the
existing approaches.
</p></li>
</ul>

<h3>Title: Miners in the Cloud: Measuring and Analyzing Cryptocurrency Mining in Public Clouds. (arXiv:2210.10512v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10512">http://arxiv.org/abs/2210.10512</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10512] Miners in the Cloud: Measuring and Analyzing Cryptocurrency Mining in Public Clouds](http://arxiv.org/abs/2210.10512)</code></li>
<li>Summary: <p>Cryptocurrencies, arguably the most prominent application of blockchains,
have been on the rise with a wide mainstream acceptance. A central concept in
cryptocurrencies is "mining pools", groups of cooperating cryptocurrency miners
who agree to share block rewards in proportion to their contributed mining
power. Despite many promised benefits of cryptocurrencies, they are equally
utilized for malicious activities; e.g., ransomware payments, stealthy command,
control, etc. Thus, understanding the interplay between cryptocurrencies,
particularly the mining pools, and other essential infrastructure for profiling
and modeling is important.
</p></li>
</ul>

<p>In this paper, we study the interplay between mining pools and public clouds
by analyzing their communication association through passive domain name system
(pDNS) traces. We observe that 24 cloud providers have some association with
mining pools as observed from the pDNS query traces, where popular public cloud
providers, namely Amazon and Google, have almost 48% of such an association.
Moreover, we found that the cloud provider presence and cloud
provider-to-mining pool association both exhibit a heavy-tailed distribution,
emphasizing an intrinsic preferential attachment model with both mining pools
and cloud providers. We measure the security risk and exposure of the cloud
providers, as that might aid in understanding the intent of the mining: among
the top two cloud providers, we found almost 35% and 30% of their associated
endpoints are positively detected to be associated with malicious activities,
per the virustotal.com scan. Finally, we found that the mining pools presented
in our dataset are predominantly used for mining Metaverse currencies,
highlighting a shift in cryptocurrency use, and demonstrating the prevalence of
mining using public clouds.
</p>

<h2>privacy</h2>
<h3>Title: How to Boost Face Recognition with StyleGAN?. (arXiv:2210.10090v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10090">http://arxiv.org/abs/2210.10090</a></li>
<li>Code URL: <a href="https://github.com/seva100/stylegan-for-facerec">https://github.com/seva100/stylegan-for-facerec</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10090] How to Boost Face Recognition with StyleGAN?](http://arxiv.org/abs/2210.10090)</code></li>
<li>Summary: <p>State-of-the-art face recognition systems require huge amounts of labeled
training data. Given the priority of privacy in face recognition applications,
the data is limited to celebrity web crawls, which have issues such as skewed
distributions of ethnicities and limited numbers of identities. On the other
hand, the self-supervised revolution in the industry motivates research on
adaptation of the related techniques to facial recognition. One of the most
popular practical tricks is to augment the dataset by the samples drawn from
the high-resolution high-fidelity models (e.g. StyleGAN-like), while preserving
the identity. We show that a simple approach based on fine-tuning an encoder
for StyleGAN allows to improve upon the state-of-the-art facial recognition and
performs better compared to training on synthetic face identities. We also
collect large-scale unlabeled datasets with controllable ethnic constitution --
AfricanFaceSet-5M (5 million images of different people) and AsianFaceSet-3M (3
million images of different people) and we show that pretraining on each of
them improves recognition of the respective ethnicities (as well as also
others), while combining all unlabeled datasets results in the biggest
performance increase. Our self-supervised strategy is the most useful with
limited amounts of labeled training data, which can be beneficial for more
tailored face recognition tasks and when facing privacy concerns. Evaluation is
provided based on a standard RFW dataset and a new large-scale RB-WebFace
benchmark.
</p></li>
</ul>

<h3>Title: Attaining Class-level Forgetting in Pretrained Model using Few Samples. (arXiv:2210.10670v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10670">http://arxiv.org/abs/2210.10670</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10670] Attaining Class-level Forgetting in Pretrained Model using Few Samples](http://arxiv.org/abs/2210.10670)</code></li>
<li>Summary: <p>In order to address real-world problems, deep learning models are jointly
trained on many classes. However, in the future, some classes may become
restricted due to privacy/ethical concerns, and the restricted class knowledge
has to be removed from the models that have been trained on them. The available
data may also be limited due to privacy/ethical concerns, and re-training the
model will not be possible. We propose a novel approach to address this problem
without affecting the model's prediction power for the remaining classes. Our
approach identifies the model parameters that are highly relevant to the
restricted classes and removes the knowledge regarding the restricted classes
from them using the limited available training data. Our approach is
significantly faster and performs similar to the model re-trained on the
complete data of the remaining classes.
</p></li>
</ul>

<h3>Title: Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective. (arXiv:2210.10488v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10488">http://arxiv.org/abs/2210.10488</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10488] Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective](http://arxiv.org/abs/2210.10488)</code></li>
<li>Summary: <p>Two interlocking research questions of growing interest and importance in
privacy research are Authorship Attribution (AA) and Authorship Obfuscation
(AO). Given an artifact, especially a text t in question, an AA solution aims
to accurately attribute t to its true author out of many candidate authors
while an AO solution aims to modify t to hide its true authorship.
Traditionally, the notion of authorship and its accompanying privacy concern is
only toward human authors. However, in recent years, due to the explosive
advancements in Neural Text Generation (NTG) techniques in NLP, capable of
synthesizing human-quality open-ended texts (so-called "neural texts"), one has
to now consider authorships by humans, machines, or their combination. Due to
the implications and potential threats of neural texts when used maliciously,
it has become critical to understand the limitations of traditional AA/AO
solutions and develop novel AA/AO solutions in dealing with neural texts. In
this survey, therefore, we make a comprehensive review of recent literature on
the attribution and obfuscation of neural text authorship from a Data Mining
perspective, and share our view on their limitations and promising research
directions.
</p></li>
</ul>

<h3>Title: STAMP: Lightweight TEE-Assisted MPC for Efficient Privacy-Preserving Machine Learning. (arXiv:2210.10133v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10133">http://arxiv.org/abs/2210.10133</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10133] STAMP: Lightweight TEE-Assisted MPC for Efficient Privacy-Preserving Machine Learning](http://arxiv.org/abs/2210.10133)</code></li>
<li>Summary: <p>In this paper, we propose STAMP, an end-to-end 3-party MPC protocol for
efficient privacy-preserving machine learning inference assisted by a
lightweight TEE (LTEE), which will be far easier to secure and deploy than
today's large TEEs. STAMP provides three main advantages over the
state-of-the-art; (i) STAMP achieves significant performance improvements
compared to state-of-the-art MPC protocols, with only a small \LTEE that is
comparable to a discrete security chip such as the Trusted Platform Module
(TPM) or on-chip security subsystems in SoCs similar to the Apple enclave
processor. In a semi-honest setting with WAN/GPU, STAMP is 4$\times$-63$\times$
faster than Falcon (PoPETs'21) and AriaNN (PoPETs'22) and
3.8$\times$-12$\times$ more communication efficient. We achieve even higher
performance improvements in a malicious setting. (ii) STAMP guarantees security
with abort against malicious adversaries under honest majority assumption.
(iii) STAMP is not limited by the size of secure memory in a TEE and can
support high-capacity modern neural networks like ResNet18 and Transformer.
</p></li>
</ul>

<h3>Title: Hope of Delivery: Extracting User Locations From Mobile Instant Messengers. (arXiv:2210.10523v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10523">http://arxiv.org/abs/2210.10523</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10523] Hope of Delivery: Extracting User Locations From Mobile Instant Messengers](http://arxiv.org/abs/2210.10523)</code></li>
<li>Summary: <p>Mobile instant messengers such as WhatsApp use delivery status notifications
in order to inform users if a sent message has successfully reached its
destination. This is useful and important information for the sender due to the
often asynchronous use of the messenger service. However, as we demonstrate in
this paper, this standard feature opens up a timing side channel with
unexpected consequences for user location privacy. We investigate this threat
conceptually and experimentally for three widely spread instant messengers. We
validate that this information leak even exists in privacy-friendly messengers
such as Signal and Threema.
</p></li>
</ul>

<p>Our results show that, after a training phase, a messenger user can
distinguish different locations of the message receiver. Our analyses involving
multiple rounds of measurements and evaluations show that the timing side
channel persists independent of distances between receiver locations -- the
attack works both for receivers in different countries as well as at small
scale in one city. For instance, out of three locations within the same city,
the sender can determine the correct one with more than 80% accuracy. Thus,
messenger users can secretly spy on each others' whereabouts when sending
instant messages. As our countermeasure evaluation shows, messenger providers
could effectively disable the timing side channel by randomly delaying delivery
confirmations within the range of a few seconds. For users themselves, the
threat is harder to prevent since there is no option to turn off delivery
confirmations.
</p>

<h3>Title: The Future of Consumer Edge-AI Computing. (arXiv:2210.10514v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10514">http://arxiv.org/abs/2210.10514</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10514] The Future of Consumer Edge-AI Computing](http://arxiv.org/abs/2210.10514)</code></li>
<li>Summary: <p>Deep Learning has proliferated dramatically across consumer devices in less
than a decade, but has been largely powered through the hardware acceleration
within isolated devices. Nonetheless, clear signals exist that the next decade
of consumer intelligence will require levels of resources, a mixing of
modalities and a collaboration of devices that will demand a significant pivot
beyond hardware alone. To accomplish this, we believe a new Edge-AI paradigm
will be necessary for this transition to be possible in a sustainable manner,
without trespassing user-privacy or hurting quality of experience.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Training set cleansing of backdoor poisoning by self-supervised representation learning. (arXiv:2210.10272v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10272">http://arxiv.org/abs/2210.10272</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10272] Training set cleansing of backdoor poisoning by self-supervised representation learning](http://arxiv.org/abs/2210.10272)</code></li>
<li>Summary: <p>A backdoor or Trojan attack is an important type of data poisoning attack
against deep neural network (DNN) classifiers, wherein the training dataset is
poisoned with a small number of samples that each possess the backdoor pattern
(usually a pattern that is either imperceptible or innocuous) and which are
mislabeled to the attacker's target class. When trained on a backdoor-poisoned
dataset, a DNN behaves normally on most benign test samples but makes incorrect
predictions to the target class when the test sample has the backdoor pattern
incorporated (i.e., contains a backdoor trigger). Here we focus on image
classification tasks and show that supervised training may build stronger
association between the backdoor pattern and the associated target class than
that between normal features and the true class of origin. By contrast,
self-supervised representation learning ignores the labels of samples and
learns a feature embedding based on images' semantic content. %We thus propose
to use unsupervised representation learning to avoid emphasising
backdoor-poisoned training samples and learn a similar feature embedding for
samples of the same class. Using a feature embedding found by self-supervised
representation learning, a data cleansing method, which combines sample
filtering and re-labeling, is developed. Experiments on CIFAR-10 benchmark
datasets show that our method achieves state-of-the-art performance in
mitigating backdoor attacks.
</p></li>
</ul>

<h3>Title: Analysis of Master Vein Attacks on Finger Vein Recognition Systems. (arXiv:2210.10667v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10667">http://arxiv.org/abs/2210.10667</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10667] Analysis of Master Vein Attacks on Finger Vein Recognition Systems](http://arxiv.org/abs/2210.10667)</code></li>
<li>Summary: <p>Finger vein recognition (FVR) systems have been commercially used, especially
in ATMs, for customer verification. Thus, it is essential to measure their
robustness against various attack methods, especially when a hand-crafted FVR
system is used without any countermeasure methods. In this paper, we are the
first in the literature to introduce master vein attacks in which we craft a
vein-looking image so that it can falsely match with as many identities as
possible by the FVR systems. We present two methods for generating master veins
for use in attacking these systems. The first uses an adaptation of the latent
variable evolution algorithm with a proposed generative model (a multi-stage
combination of beta-VAE and WGAN-GP models). The second uses an adversarial
machine learning attack method to attack a strong surrogate CNN-based
recognition system. The two methods can be easily combined to boost their
attack ability. Experimental results demonstrated that the proposed methods
alone and together achieved false acceptance rates up to 73.29% and 88.79%,
respectively, against Miura's hand-crafted FVR system. We also point out that
Miura's system is easily compromised by non-vein-looking samples generated by a
WGAN-GP model with false acceptance rates up to 94.21%. The results raise the
alarm about the robustness of such systems and suggest that master vein attacks
should be considered an important security measure.
</p></li>
</ul>

<h3>Title: Fant\^omas: Evaluating Reversibility of Face Anonymizations Using a General Deep Learning Attacker. (arXiv:2210.10651v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10651">http://arxiv.org/abs/2210.10651</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10651] Fant\^omas: Evaluating Reversibility of Face Anonymizations Using a General Deep Learning Attacker](http://arxiv.org/abs/2210.10651)</code></li>
<li>Summary: <p>Biometric data is a rich source of information that can be used to identify
individuals and infer private information about them. To mitigate this privacy
risk, anonymization techniques employ transformations on clear data to
obfuscate sensitive information, all while retaining some utility of the data.
Albeit published with impressive claims, they sometimes are not evaluated with
convincing methodology. We hence are interested to which extent recently
suggested anonymization techniques for obfuscating facial images are effective.
More specifically, we test how easily they can be automatically reverted, to
estimate the privacy they can provide. Our approach is agnostic to the
anonymization technique as we learn a machine learning model on the clear and
corresponding anonymized data. We find that 10 out of 14 tested face
anonymization techniques are at least partially reversible, and six of them are
at least highly reversible.
</p></li>
</ul>

<h3>Title: Few-shot Transferable Robust Representation Learning via Bilevel Attacks. (arXiv:2210.10485v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10485">http://arxiv.org/abs/2210.10485</a></li>
<li>Code URL: <a href="https://github.com/kim-minseon/troba">https://github.com/kim-minseon/troba</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10485] Few-shot Transferable Robust Representation Learning via Bilevel Attacks](http://arxiv.org/abs/2210.10485)</code></li>
<li>Summary: <p>Existing adversarial learning methods for enhancing the robustness of deep
neural networks assume the availability of a large amount of data from which we
can generate adversarial examples. However, in an adversarial meta-learning
setting, the model needs to train with only a few adversarial examples to learn
a robust model for unseen tasks, which is a very difficult goal to achieve.
Further, learning transferable robust representations for unseen domains is a
difficult problem even with a large amount of data. To tackle such a challenge,
we propose a novel adversarial self-supervised meta-learning framework with
bilevel attacks which aims to learn robust representations that can generalize
across tasks and domains. Specifically, in the inner loop, we update the
parameters of the given encoder by taking inner gradient steps using two
different sets of augmented samples, and generate adversarial examples for each
view by maximizing the instance classification loss. Then, in the outer loop,
we meta-learn the encoder parameter to maximize the agreement between the two
adversarial examples, which enables it to learn robust representations. We
experimentally validate the effectiveness of our approach on unseen domain
adaptation tasks, on which it achieves impressive performance. Specifically,
our method significantly outperforms the state-of-the-art meta-adversarial
learning methods on few-shot learning tasks, as well as self-supervised
learning baselines in standard learning settings with large-scale datasets.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Parallel Inversion of Neural Radiance Fields for Robust Pose Estimation. (arXiv:2210.10108v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10108">http://arxiv.org/abs/2210.10108</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10108] Parallel Inversion of Neural Radiance Fields for Robust Pose Estimation](http://arxiv.org/abs/2210.10108)</code></li>
<li>Summary: <p>We present a parallelized optimization method based on fast Neural Radiance
Fields (NeRF) for estimating 6-DoF target poses. Given a single observed RGB
image of the target, we can predict the translation and rotation of the camera
by minimizing the residual between pixels rendered from a fast NeRF model and
pixels in the observed image. We integrate a momentum-based camera extrinsic
optimization procedure into Instant Neural Graphics Primitives, a recent
exceptionally fast NeRF implementation. By introducing parallel Monte Carlo
sampling into the pose estimation task, our method overcomes local minima and
improves efficiency in a more extensive search space. We also show the
importance of adopting a more robust pixel-based loss function to reduce error.
Experiments demonstrate that our method can achieve improved generalization and
robustness on both synthetic and real-world benchmarks.
</p></li>
</ul>

<h3>Title: Intra-Source Style Augmentation for Improved Domain Generalization. (arXiv:2210.10175v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10175">http://arxiv.org/abs/2210.10175</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10175] Intra-Source Style Augmentation for Improved Domain Generalization](http://arxiv.org/abs/2210.10175)</code></li>
<li>Summary: <p>The generalization with respect to domain shifts, as they frequently appear
in applications such as autonomous driving, is one of the remaining big
challenges for deep learning models. Therefore, we propose an intra-source
style augmentation (ISSA) method to improve domain generalization in semantic
segmentation. Our method is based on a novel masked noise encoder for StyleGAN2
inversion. The model learns to faithfully reconstruct the image preserving its
semantic layout through noise prediction. Random masking of the estimated noise
enables the style mixing capability of our model, i.e. it allows to alter the
global appearance without affecting the semantic layout of an image. Using the
proposed masked noise encoder to randomize style and content combinations in
the training set, ISSA effectively increases the diversity of training data and
reduces spurious correlation. As a result, we achieve up to $12.4\%$ mIoU
improvements on driving-scene semantic segmentation under different types of
data shifts, i.e., changing geographic locations, adverse weather conditions,
and day to night. ISSA is model-agnostic and straightforwardly applicable with
CNNs and Transformers. It is also complementary to other domain generalization
techniques, e.g., it improves the recent state-of-the-art solution RobustNet by
$3\%$ mIoU in Cityscapes to Dark Z\"urich.
</p></li>
</ul>

<h3>Title: Vision-Based Lane Detection and Tracking under Different Challenging Environmental Conditions. (arXiv:2210.10233v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10233">http://arxiv.org/abs/2210.10233</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10233] Vision-Based Lane Detection and Tracking under Different Challenging Environmental Conditions](http://arxiv.org/abs/2210.10233)</code></li>
<li>Summary: <p>Driving is very challenging when the visibility of a road lane marking is
low, obscured or often invisible due to abrupt environmental change which may
lead to severe vehicle clash. A large volume of research has been done on lane
marking detection. Most of the lane detection methods suffer from four types of
major problems: (i) abrupt illumination change due to change in time (day,
night), weather, road, etc.; (ii) lane markings get obscured partially or fully
when they are colored, eroded or occluded; (iii) blurred view created by
adverse weather like rain or snow; and (iv) incorrect lane detection due to
presence of other lookalike lines e.g. guardrails, pavement marking, road
divider, vehicle lines, the shadow of trees, etc. In this paper, we proposed a
robust lane detection and tracking method to detect lane marking considering
the abovementioned challenging conditions. In this method, we introduced three
key technologies. First, the bilateral filter is applied to smooth and preserve
the edges and we introduced an optimized intensity threshold range (OITR) to
improve the performance of the canny operator which detects the edges of low
intensity (colored, eroded, or blurred) lane markings. Second, we proposed a
robust lane verification technique, the angle and length-based geometric
constraint (ALGC) algorithm followed by Hough Transform, to verify the
characteristics of lane marking and to prevent incorrect lane detection.
Finally, a novel lane tracking technique, the horizontally adjustable lane
repositioning range (HALRR) algorithm is proposed, which can keep track of the
lane position. To evaluate the performance of the proposed method we used the
DSDLDE dataset with 1080x1920 resolutions at 24 frames/sec. Experimental
results show that the average detection rate is 97.36%, and the average
detection time is 29.06msec per frame, which outperformed the state-of-the-art
method.
</p></li>
</ul>

<h3>Title: Discovering Limitations of Image Quality Assessments with Noised Deep Learning Image Sets. (arXiv:2210.10249v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10249">http://arxiv.org/abs/2210.10249</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10249] Discovering Limitations of Image Quality Assessments with Noised Deep Learning Image Sets](http://arxiv.org/abs/2210.10249)</code></li>
<li>Summary: <p>Image quality is important, and it can affect overall performance in image
processing and computer vision as well as for numerous other reasons. Image
quality assessment (IQA) is consequently a vital task in different applications
from aerial photography interpretation to object detection to medical image
analysis. In previous research, the BRISQUE algorithm and the PSNR algorithm
were evaluated with high resolution ( 512<em>384 pixels per image), but relatively
small image sets (4,744 images). However, scientists have not evaluated IQA
algorithms on low resolution (32</em>32 pixels per image), multi-perturbation, big
image sets (for example, 60,000 different images not counting their
perturbations). This study explores these two IQA algorithms through
experimental investigation. We first chose two deep learning image sets,
CIFAR-10 and MNIST. Then, we added 68 perturbations that add noise to the
images in specific sequences and noise intensities. In addition, we tracked the
performance outputs of the two IQA algorithms with singly and multiply noised
images. After quantitatively analyzing experimental results, we report the
limitations of the two IQAs with these noised CIFAR-10 and MNIST image sets. We
also explain three potential root causes for performance degradation. These
findings point out weaknesses of the two IQA algorithms. The research results
provide guidance to scientists and engineers developing accurate, robust IQA
algorithms. In addition to supporting future scientific research and industrial
projects, all source codes are shared on the website:
https://github.com/caperock/imagequality
</p></li>
</ul>

<h3>Title: On the Adversarial Robustness of Mixture of Experts. (arXiv:2210.10253v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10253">http://arxiv.org/abs/2210.10253</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10253] On the Adversarial Robustness of Mixture of Experts](http://arxiv.org/abs/2210.10253)</code></li>
<li>Summary: <p>Adversarial robustness is a key desirable property of neural networks. It has
been empirically shown to be affected by their sizes, with larger networks
being typically more robust. Recently, Bubeck and Sellke proved a lower bound
on the Lipschitz constant of functions that fit the training data in terms of
their number of parameters. This raises an interesting open question, do -- and
can -- functions with more parameters, but not necessarily more computational
cost, have better robustness? We study this question for sparse Mixture of
Expert models (MoEs), that make it possible to scale up the model size for a
roughly constant computational cost. We theoretically show that under certain
conditions on the routing and the structure of the data, MoEs can have
significantly smaller Lipschitz constants than their dense counterparts. The
robustness of MoEs can suffer when the highest weighted experts for an input
implement sufficiently different functions. We next empirically evaluate the
robustness of MoEs on ImageNet using adversarial attacks and show they are
indeed more robust than dense models with the same computational cost. We make
key observations showing the robustness of MoEs to the choice of experts,
highlighting the redundancy of experts in models trained in practice.
</p></li>
</ul>

<h3>Title: LAVA: Label-efficient Visual Learning and Adaptation. (arXiv:2210.10317v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10317">http://arxiv.org/abs/2210.10317</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10317] LAVA: Label-efficient Visual Learning and Adaptation](http://arxiv.org/abs/2210.10317)</code></li>
<li>Summary: <p>We present LAVA, a simple yet effective method for multi-domain visual
transfer learning with limited data. LAVA builds on a few recent innovations to
enable adapting to partially labelled datasets with class and domain shifts.
First, LAVA learns self-supervised visual representations on the source dataset
and ground them using class label semantics to overcome transfer collapse
problems associated with supervised pretraining. Secondly, LAVA maximises the
gains from unlabelled target data via a novel method which uses multi-crop
augmentations to obtain highly robust pseudo-labels. By combining these
ingredients, LAVA achieves a new state-of-the-art on ImageNet semi-supervised
protocol, as well as on 7 out of 10 datasets in multi-domain few-shot learning
on the Meta-dataset. Code and models are made available.
</p></li>
</ul>

<h3>Title: WebtoonMe: A Data-Centric Approach for Full-Body Portrait Stylization. (arXiv:2210.10335v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10335">http://arxiv.org/abs/2210.10335</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10335] WebtoonMe: A Data-Centric Approach for Full-Body Portrait Stylization](http://arxiv.org/abs/2210.10335)</code></li>
<li>Summary: <p>Full-body portrait stylization, which aims to translate portrait photography
into a cartoon style, has drawn attention recently. However, most methods have
focused only on converting face regions, restraining the feasibility of use in
real-world applications. A recently proposed two-stage method expands the
rendering area to full bodies, but the outputs are less plausible and fail to
achieve quality robustness of non-face regions. Furthermore, they cannot
reflect diverse skin tones. In this study, we propose a data-centric solution
to build a production-level full-body portrait stylization system. Based on the
two-stage scheme, we construct a novel and advanced dataset preparation
paradigm that can effectively resolve the aforementioned problems. Experiments
reveal that with our pipeline, high-quality portrait stylization can be
achieved without additional losses or architectural changes.
</p></li>
</ul>

<h3>Title: Segmentation-free Direct Iris Localization Networks. (arXiv:2210.10403v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10403">http://arxiv.org/abs/2210.10403</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10403] Segmentation-free Direct Iris Localization Networks](http://arxiv.org/abs/2210.10403)</code></li>
<li>Summary: <p>This paper proposes an efficient iris localization method without using iris
segmentation and circle fitting. Conventional iris localization methods first
extract iris regions by using semantic segmentation methods such as U-Net.
Afterward, the inner and outer iris circles are localized using the traditional
circle fitting algorithm. However, this approach requires high-resolution
encoder-decoder networks for iris segmentation, so it causes computational
costs to be high. In addition, traditional circle fitting tends to be sensitive
to noise in input images and fitting parameters, causing the iris recognition
performance to be poor. To solve these problems, we propose an iris
localization network (ILN), that can directly localize pupil and iris circles
with eyelid points from a low-resolution iris image. We also introduce a pupil
refinement network (PRN) to improve the accuracy of pupil localization.
Experimental results show that the combination of ILN and PRN works in 34.5 ms
for one iris image on a CPU, and its localization performance outperforms
conventional iris segmentation methods. In addition, generalized evaluation
results show that the proposed method has higher robustness for datasets in
different domain than other segmentation methods. Furthermore, we also confirm
that the proposed ILN and PRN improve the iris recognition accuracy.
</p></li>
</ul>

<h3>Title: p$^3$VAE: a physics-integrated generative model. Application to the semantic segmentation of optical remote sensing images. (arXiv:2210.10418v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10418">http://arxiv.org/abs/2210.10418</a></li>
<li>Code URL: <a href="https://github.com/romain3ch216/p3vae">https://github.com/romain3ch216/p3vae</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10418] p$^3$VAE: a physics-integrated generative model](http://arxiv.org/abs/2210.10418)</code></li>
<li>Summary: <p>The combination of machine learning models with physical models is a recent
research path to learn robust data representations. In this paper, we introduce
p$^3$VAE, a generative model that integrates a perfect physical model which
partially explains the true underlying factors of variation in the data. To
fully leverage our hybrid design, we propose a semi-supervised optimization
procedure and an inference scheme that comes along meaningful uncertainty
estimates. We apply p$^3$VAE to the semantic segmentation of high-resolution
hyperspectral remote sensing images. Our experiments on a simulated data set
demonstrated the benefits of our hybrid model against conventional machine
learning models in terms of extrapolation capabilities and interpretability. In
particular, we show that p$^3$VAE naturally has high disentanglement
capabilities. Our code and data have been made publicly available at
https://github.com/Romain3Ch216/p3VAE.
</p></li>
</ul>

<h3>Title: Estimating the coverage in 3d reconstructions of the colon from colonoscopy videos. (arXiv:2210.10459v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10459">http://arxiv.org/abs/2210.10459</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10459] Estimating the coverage in 3d reconstructions of the colon from colonoscopy videos](http://arxiv.org/abs/2210.10459)</code></li>
<li>Summary: <p>Colonoscopy is the most common procedure for early detection and removal of
polyps, a critical component of colorectal cancer prevention. Insufficient
visual coverage of the colon surface during the procedure often results in
missed polyps. To mitigate this issue, reconstructing the 3D surfaces of the
colon in order to visualize the missing regions has been proposed. However,
robustly estimating the local and global coverage from such a reconstruction
has not been thoroughly investigated until now. In this work, we present a new
method to estimate the coverage from a reconstructed colon pointcloud. Our
method splits a reconstructed colon into segments and estimates the coverage of
each segment by estimating the area of the missing surfaces. We achieve a mean
absolute coverage error of 3-6\% on colon segments generated from synthetic
colonoscopy data and real colonography CT scans. In addition, we show good
qualitative results on colon segments reconstructed from real colonoscopy
videos.
</p></li>
</ul>

<h3>Title: A Robust Pedestrian Detection Approach for Autonomous Vehicles. (arXiv:2210.10489v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10489">http://arxiv.org/abs/2210.10489</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10489] A Robust Pedestrian Detection Approach for Autonomous Vehicles](http://arxiv.org/abs/2210.10489)</code></li>
<li>Summary: <p>Nowadays, utilizing Advanced Driver-Assistance Systems (ADAS) has absorbed a
huge interest as a potential solution for reducing road traffic issues. Despite
recent technological advances in such systems, there are still many inquiries
that need to be overcome. For instance, ADAS requires accurate and real-time
detection of pedestrians in various driving scenarios. To solve the mentioned
problem, this paper aims to fine-tune the YOLOv5s framework for handling
pedestrian detection challenges on the real-world instances of Caltech
pedestrian dataset. We also introduce a developed toolbox for preparing
training and test data and annotations of Caltech pedestrian dataset into the
format recognizable by YOLOv5. Experimental results of utilizing our approach
show that the mean Average Precision (mAP) of our fine-tuned model for
pedestrian detection task is more than 91 percent when performing at the
highest rate of 70 FPS. Moreover, the experiments on the Caltech pedestrian
dataset samples have verified that our proposed approach is an effective and
accurate method for pedestrian detection and can outperform other existing
methodologies.
</p></li>
</ul>

<h3>Title: LaMAR: Benchmarking Localization and Mapping for Augmented Reality. (arXiv:2210.10770v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10770">http://arxiv.org/abs/2210.10770</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10770] LaMAR: Benchmarking Localization and Mapping for Augmented Reality](http://arxiv.org/abs/2210.10770)</code></li>
<li>Summary: <p>Localization and mapping is the foundational technology for augmented reality
(AR) that enables sharing and persistence of digital content in the real world.
While significant progress has been made, researchers are still mostly driven
by unrealistic benchmarks not representative of real-world AR scenarios. These
benchmarks are often based on small-scale datasets with low scene diversity,
captured from stationary cameras, and lack other sensor inputs like inertial,
radio, or depth data. Furthermore, their ground-truth (GT) accuracy is mostly
insufficient to satisfy AR requirements. To close this gap, we introduce LaMAR,
a new benchmark with a comprehensive capture and GT pipeline that co-registers
realistic trajectories and sensor streams captured by heterogeneous AR devices
in large, unconstrained scenes. To establish an accurate GT, our pipeline
robustly aligns the trajectories against laser scans in a fully automated
manner. As a result, we publish a benchmark dataset of diverse and large-scale
scenes recorded with head-mounted and hand-held AR devices. We extend several
state-of-the-art methods to take advantage of the AR-specific setup and
evaluate them on our benchmark. The results offer new insights on current
research and reveal promising avenues for future work in the field of
localization and mapping for AR.
</p></li>
</ul>

<h3>Title: ELASTIC: Numerical Reasoning with Adaptive Symbolic Compiler. (arXiv:2210.10105v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10105">http://arxiv.org/abs/2210.10105</a></li>
<li>Code URL: <a href="https://github.com/neurasearch/neurips-2022-submission-3358">https://github.com/neurasearch/neurips-2022-submission-3358</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10105] ELASTIC: Numerical Reasoning with Adaptive Symbolic Compiler](http://arxiv.org/abs/2210.10105)</code></li>
<li>Summary: <p>Numerical reasoning over text is a challenging task of Artificial
Intelligence (AI), requiring reading comprehension and numerical reasoning
abilities. Previous approaches use numerical reasoning programs to represent
the reasoning process. However, most works do not separate the generation of
operators and operands, which are key components of a numerical reasoning
program, thus limiting their ability to generate such programs for complicated
tasks. In this paper, we introduce the numEricaL reASoning with adapTive
symbolIc Compiler (ELASTIC) model, which is constituted of the RoBERTa as the
Encoder and a Compiler with four modules: Reasoning Manager, Operator
Generator, Operands Generator, and Memory Register. ELASTIC is robust when
conducting complicated reasoning. Also, it is domain agnostic by supporting the
expansion of diverse operators without caring about the number of operands it
contains. Experiments show that ELASTIC achieves 68.96 and 65.21 of execution
accuracy and program accuracy on the FinQA dataset and 83.00 program accuracy
on the MathQA dataset, outperforming previous state-of-the-art models
significantly.
</p></li>
</ul>

<h3>Title: A Data-Driven Investigation of Noise-Adaptive Utterance Generation with Linguistic Modification. (arXiv:2210.10252v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10252">http://arxiv.org/abs/2210.10252</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10252] A Data-Driven Investigation of Noise-Adaptive Utterance Generation with Linguistic Modification](http://arxiv.org/abs/2210.10252)</code></li>
<li>Summary: <p>In noisy environments, speech can be hard to understand for humans. Spoken
dialog systems can help to enhance the intelligibility of their output, either
by modifying the speech synthesis (e.g., imitate Lombard speech) or by
optimizing the language generation. We here focus on the second type of
approach, by which an intended message is realized with words that are more
intelligible in a specific noisy environment. By conducting a speech perception
experiment, we created a dataset of 900 paraphrases in babble noise, perceived
by native English speakers with normal hearing. We find that careful selection
of paraphrases can improve intelligibility by 33% at SNR -5 dB. Our analysis of
the data shows that the intelligibility differences between paraphrases are
mainly driven by noise-robust acoustic cues. Furthermore, we propose an
intelligibility-aware paraphrase ranking model, which outperforms baseline
models with a relative improvement of 31.37% at SNR -5 dB.
</p></li>
</ul>

<h3>Title: Hybrid-Regressive Neural Machine Translation. (arXiv:2210.10416v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10416">http://arxiv.org/abs/2210.10416</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10416] Hybrid-Regressive Neural Machine Translation](http://arxiv.org/abs/2210.10416)</code></li>
<li>Summary: <p>In this work, we empirically confirm that non-autoregressive translation with
an iterative refinement mechanism (IR-NAT) suffers from poor acceleration
robustness because it is more sensitive to decoding batch size and computing
device setting than autoregressive translation (AT). Inspired by it, we attempt
to investigate how to combine the strengths of autoregressive and
non-autoregressive translation paradigms better. To this end, we demonstrate
through synthetic experiments that prompting a small number of AT's predictions
can promote one-shot non-autoregressive translation to achieve the equivalent
performance of IR-NAT. Following this line, we propose a new two-stage
translation prototype called hybrid-regressive translation (HRT). Specifically,
HRT first generates discontinuous sequences via autoregression (e.g., make a
prediction every k tokens, k>1) and then fills in all previously skipped tokens
at once in a non-autoregressive manner. We also propose a bag of techniques to
effectively and efficiently train HRT without adding any model parameters. HRT
achieves the state-of-the-art BLEU score of 28.49 on the WMT En-De task and is
at least 1.5x faster than AT, regardless of batch size and device. In addition,
another bonus of HRT is that it successfully inherits the good characteristics
of AT in the deep-encoder-shallow-decoder architecture. Concretely, compared to
the vanilla HRT with a 6-layer encoder and 6-layer decoder, the inference speed
of HRT with a 12-layer encoder and 1-layer decoder is further doubled on both
GPU and CPU without BLEU loss.
</p></li>
</ul>

<h3>Title: LightEA: A Scalable, Robust, and Interpretable Entity Alignment Framework via Three-view Label Propagation. (arXiv:2210.10436v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10436">http://arxiv.org/abs/2210.10436</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10436] LightEA: A Scalable, Robust, and Interpretable Entity Alignment Framework via Three-view Label Propagation](http://arxiv.org/abs/2210.10436)</code></li>
<li>Summary: <p>Entity Alignment (EA) aims to find equivalent entity pairs between KGs, which
is the core step of bridging and integrating multi-source KGs. In this paper,
we argue that existing GNN-based EA methods inherit the inborn defects from
their neural network lineage: weak scalability and poor interpretability.
Inspired by recent studies, we reinvent the Label Propagation algorithm to
effectively run on KGs and propose a non-neural EA framework -- LightEA,
consisting of three efficient components: (i) Random Orthogonal Label
Generation, (ii) Three-view Label Propagation, and (iii) Sparse Sinkhorn
Iteration. According to the extensive experiments on public datasets, LightEA
has impressive scalability, robustness, and interpretability. With a mere tenth
of time consumption, LightEA achieves comparable results to state-of-the-art
methods across all datasets and even surpasses them on many.
</p></li>
</ul>

<h3>Title: Robustness of Demonstration-based Learning Under Limited Data Scenario. (arXiv:2210.10693v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10693">http://arxiv.org/abs/2210.10693</a></li>
<li>Code URL: <a href="https://github.com/salt-nlp/robustdemo">https://github.com/salt-nlp/robustdemo</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10693] Robustness of Demonstration-based Learning Under Limited Data Scenario](http://arxiv.org/abs/2210.10693)</code></li>
<li>Summary: <p>Demonstration-based learning has shown great potential in stimulating
pretrained language models' ability under limited data scenario. Simply
augmenting the input with some demonstrations can significantly improve
performance on few-shot NER. However, why such demonstrations are beneficial
for the learning process remains unclear since there is no explicit alignment
between the demonstrations and the predictions. In this paper, we design
pathological demonstrations by gradually removing intuitively useful
information from the standard ones to take a deep dive of the robustness of
demonstration-based sequence labeling and show that (1) demonstrations composed
of random tokens still make the model a better few-shot learner; (2) the length
of random demonstrations and the relevance of random tokens are the main
factors affecting the performance; (3) demonstrations increase the confidence
of model predictions on captured superficial patterns. We have publicly
released our code at https://github.com/SALT-NLP/RobustDemo.
</p></li>
</ul>

<h3>Title: Gaussian-Bernoulli RBMs Without Tears. (arXiv:2210.10318v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10318">http://arxiv.org/abs/2210.10318</a></li>
<li>Code URL: <a href="https://github.com/lrjconan/grbm">https://github.com/lrjconan/grbm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10318] Gaussian-Bernoulli RBMs Without Tears](http://arxiv.org/abs/2210.10318)</code></li>
<li>Summary: <p>We revisit the challenging problem of training Gaussian-Bernoulli restricted
Boltzmann machines (GRBMs), introducing two innovations. We propose a novel
Gibbs-Langevin sampling algorithm that outperforms existing methods like Gibbs
sampling. We propose a modified contrastive divergence (CD) algorithm so that
one can generate images with GRBMs starting from noise. This enables direct
comparison of GRBMs with deep generative models, improving evaluation protocols
in the RBM literature. Moreover, we show that modified CD and gradient clipping
are enough to robustly train GRBMs with large learning rates, thus removing the
necessity of various tricks in the literature. Experiments on Gaussian
Mixtures, MNIST, FashionMNIST, and CelebA show GRBMs can generate good samples,
despite their single-hidden-layer architecture. Our code is released at:
\url{https://github.com/lrjconan/GRBM}.
</p></li>
</ul>

<h3>Title: Robust Offline Reinforcement Learning with Gradient Penalty and Constraint Relaxation. (arXiv:2210.10469v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10469">http://arxiv.org/abs/2210.10469</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10469] Robust Offline Reinforcement Learning with Gradient Penalty and Constraint Relaxation](http://arxiv.org/abs/2210.10469)</code></li>
<li>Summary: <p>A promising paradigm for offline reinforcement learning (RL) is to constrain
the learned policy to stay close to the dataset behaviors, known as policy
constraint offline RL. However, existing works heavily rely on the purity of
the data, exhibiting performance degradation or even catastrophic failure when
learning from contaminated datasets containing impure trajectories of diverse
levels. e.g., expert level, medium level, etc., while offline contaminated data
logs exist commonly in the real world. To mitigate this, we first introduce
gradient penalty over the learned value function to tackle the exploding
Q-functions. We then relax the closeness constraints towards non-optimal
actions with critic weighted constraint relaxation. Experimental results show
that the proposed techniques effectively tame the non-optimal trajectories for
policy constraint offline RL methods, evaluated on a set of contaminated D4RL
Mujoco and Adroit datasets.
</p></li>
</ul>

<h3>Title: Targeted Adversarial Self-Supervised Learning. (arXiv:2210.10482v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10482">http://arxiv.org/abs/2210.10482</a></li>
<li>Code URL: <a href="https://github.com/kim-minseon/taross">https://github.com/kim-minseon/taross</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10482] Targeted Adversarial Self-Supervised Learning](http://arxiv.org/abs/2210.10482)</code></li>
<li>Summary: <p>Recently, unsupervised adversarial training (AT) has been extensively studied
to attain robustness with the models trained upon unlabeled data. To this end,
previous studies have applied existing supervised adversarial training
techniques to self-supervised learning (SSL) frameworks. However, all have
resorted to untargeted adversarial learning as obtaining targeted adversarial
examples is unclear in the SSL setting lacking of label information. In this
paper, we propose a novel targeted adversarial training method for the SSL
frameworks. Specifically, we propose a target selection algorithm for the
adversarial SSL frameworks; it is designed to select the most confusing sample
for each given instance based on similarity and entropy, and perturb the given
instance toward the selected target sample. Our method significantly enhances
the robustness of an SSL model without requiring large batches of images or
additional models, unlike existing works aimed at achieving the same goal.
Moreover, our method is readily applicable to general SSL frameworks that only
uses positive pairs. We validate our method on benchmark datasets, on which it
obtains superior robust accuracies, outperforming existing unsupervised
adversarial training methods.
</p></li>
</ul>

<h3>Title: A Segment-Wise Gaussian Process-Based Ground Segmentation With Local Smoothness Estimation. (arXiv:2210.10515v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10515">http://arxiv.org/abs/2210.10515</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10515] A Segment-Wise Gaussian Process-Based Ground Segmentation With Local Smoothness Estimation](http://arxiv.org/abs/2210.10515)</code></li>
<li>Summary: <p>Both in terrestrial and extraterrestrial environments, the precise and
informative model of the ground and the surface ahead is crucial for navigation
and obstacle avoidance. The ground surface is not always flat and it may be
sloped, bumpy and rough specially in off-road terrestrial scenes. In bumpy and
rough scenes the functional relationship of the surface-related features may
vary in different areas of the ground, as the structure of the ground surface
may vary suddenly and further the measured point cloud of the ground does not
bear smoothness. Thus, the ground-related features must be obtained based on
local estimates or even point estimates. To tackle this problem, the
segment-wise GP-based ground segmentation method with local smoothness
estimation is proposed. This method is an extension to our previous method in
which a realistic measurement of the length-scale values were provided for the
covariance kernel in each line-segment to give precise estimation of the ground
for sloped terrains. In this extension, the value of the length-scale is
estimated locally for each data point which makes it much more precise for the
rough scenes while being not computationally complex and more robust to
under-segmentation, sparsity and under-represent-ability. The segment-wise task
is performed to estimate a partial continuous model of the ground for each
radial range segment. Simulation results show the effectiveness of the proposed
method to give a continuous and precise estimation of the ground surface in
rough and bumpy scenes while being fast enough for real-world applications.
</p></li>
</ul>

<h3>Title: Robust Regression with Highly Corrupted Data via Physics Informed Neural Networks. (arXiv:2210.10646v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10646">http://arxiv.org/abs/2210.10646</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10646] Robust Regression with Highly Corrupted Data via Physics Informed Neural Networks](http://arxiv.org/abs/2210.10646)</code></li>
<li>Summary: <p>Physics-informed neural networks (PINNs) have been proposed to solve two main
classes of problems: data-driven solutions and data-driven discovery of partial
differential equations. This task becomes prohibitive when such data is highly
corrupted due to the possible sensor mechanism failing. We propose the Least
Absolute Deviation based PINN (LAD-PINN) to reconstruct the solution and
recover unknown parameters in PDEs - even if spurious data or outliers corrupt
a large percentage of the observations. To further improve the accuracy of
recovering hidden physics, the two-stage Median Absolute Deviation based PINN
(MAD-PINN) is proposed, where LAD-PINN is employed as an outlier detector
followed by MAD screening out the highly corrupted data. Then the vanilla PINN
or its variants can be subsequently applied to exploit the remaining normal
data. Through several examples, including Poisson's equation, wave equation,
and steady or unsteady Navier-Stokes equations, we illustrate the
generalizability, accuracy and efficiency of the proposed algorithms for
recovering governing equations from noisy and highly corrupted measurement
data.
</p></li>
</ul>

<h3>Title: Autoregressive Generative Modeling with Noise Conditional Maximum Likelihood Estimation. (arXiv:2210.10715v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10715">http://arxiv.org/abs/2210.10715</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10715] Autoregressive Generative Modeling with Noise Conditional Maximum Likelihood Estimation](http://arxiv.org/abs/2210.10715)</code></li>
<li>Summary: <p>We introduce a simple modification to the standard maximum likelihood
estimation (MLE) framework. Rather than maximizing a single unconditional
likelihood of the data under the model, we maximize a family of \textit{noise
conditional} likelihoods consisting of the data perturbed by a continuum of
noise levels. We find that models trained this way are more robust to noise,
obtain higher test likelihoods, and generate higher quality images. They can
also be sampled from via a novel score-based sampling scheme which combats the
classical \textit{covariate shift} problem that occurs during sample generation
in autoregressive models. Applying this augmentation to autoregressive image
models, we obtain 3.32 bits per dimension on the ImageNet 64x64 dataset, and
substantially improve the quality of generated samples in terms of the Frechet
Inception distance (FID) -- from 37.50 to 12.09 on the CIFAR-10 dataset.
</p></li>
</ul>

<h3>Title: "Why did the Model Fail?": Attributing Model Performance Changes to Distribution Shifts. (arXiv:2210.10769v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10769">http://arxiv.org/abs/2210.10769</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10769] "Why did the Model Fail?": Attributing Model Performance Changes to Distribution Shifts](http://arxiv.org/abs/2210.10769)</code></li>
<li>Summary: <p>Performance of machine learning models may differ between training and
deployment for many reasons. For instance, model performance can change between
environments due to changes in data quality, observing a different population
than the one in training, or changes in the relationship between labels and
features. These manifest as changes to the underlying data generating
mechanisms, and thereby result in distribution shifts across environments.
Attributing performance changes to specific shifts, such as covariate or
concept shifts, is critical for identifying sources of model failures, and for
taking mitigating actions that ensure robust models. In this work, we introduce
the problem of attributing performance differences between environments to
shifts in the underlying data generating mechanisms. We formulate the problem
as a cooperative game and derive an importance weighting method for computing
the value of a coalition (or a set) of distributions. The contribution of each
distribution to the total performance change is then quantified as its Shapley
value. We demonstrate the correctness and utility of our method on two
synthetic datasets and two real-world case studies, showing its effectiveness
in attributing performance changes to a wide range of distribution shifts.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Multi-view Gait Recognition based on Siamese Vision Transformer. (arXiv:2210.10421v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10421">http://arxiv.org/abs/2210.10421</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10421] Multi-view Gait Recognition based on Siamese Vision Transformer](http://arxiv.org/abs/2210.10421)</code></li>
<li>Summary: <p>While the Vision Transformer has been used in gait recognition, its
application in multi-view gait recognition is still limited. Different views
significantly affect the extraction and identification accuracy of the
characteristics of gait contour. To address this, this paper proposes a Siamese
Mobile Vision Transformer (SMViT). This model not only focuses on the local
characteristics of the human gait space but also considers the characteristics
of long-distance attention associations, which can extract multi-dimensional
step status characteristics. In addition, it describes how different
perspectives affect gait characteristics and generate reliable perspective
feature relationship factors. The average recognition rate of SMViT on the
CASIA B data set reached 96.4%. The experimental results show that SMViT can
attain state-of-the-art performance compared to advanced step recognition
models such as GaitGAN, Multi_view GAN, Posegait and other gait recognition
models.
</p></li>
</ul>

<h3>Title: Soil moisture estimation from Sentinel-1 interferometric observations over arid regions. (arXiv:2210.10665v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10665">http://arxiv.org/abs/2210.10665</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10665] Soil moisture estimation from Sentinel-1 interferometric observations over arid regions](http://arxiv.org/abs/2210.10665)</code></li>
<li>Summary: <p>We present a methodology based on interferometric synthetic aperture radar
(InSAR) time series analysis that can provide surface (top 5 cm) soil moisture
(SSM) estimations. The InSAR time series analysis consists of five processing
steps. A co-registered Single Look Complex (SLC) SAR stack as well as
meteorological information are required as input of the proposed workflow. In
the first step, ice/snow-free and zero-precipitation SAR images are identified
using meteorological data. In the second step, construction and phase
extraction of distributed scatterers (DSs) (over bare land) is performed. In
the third step, for each DS the ordering of surface soil moisture (SSM) levels
of SAR acquisitions based on interferometric coherence is calculated. In the
fourth step, for each DS the coherence due to SSM variations is calculated. In
the fifth step, SSM is estimated by a constrained inversion of an analytical
interferometric model using coherence and phase closure information. The
implementation of the proposed approach is provided as an open-source software
toolbox (INSAR4SM) available at www.github.com/kleok/INSAR4SM.
</p></li>
</ul>

<p>A case study over an arid region in California/Arizona is presented. The
proposed workflow was applied in Sentinel- 1 (C-band) VV-polarized InSAR
observations. The estimated SSM results were assessed with independent SSM
observations from a station of the International Soil Moisture Network (ISMN)
(RMSE: 0.027 $m^3/m^3$ R: 0.88) and ERA5-Land reanalysis model data (RMSE:
0.035 $m^3/m^3$ R: 0.71). The proposed methodology was able to provide accurate
SSM estimations at high spatial resolution (~250 m). A discussion of the
benefits and the limitations of the proposed methodology highlighted the
potential of interferometric observables for SSM estimation over arid regions.
</p>

<h3>Title: Cross-Domain Aspect Extraction using Transformers Augmented with Knowledge Graphs. (arXiv:2210.10144v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10144">http://arxiv.org/abs/2210.10144</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10144] Cross-Domain Aspect Extraction using Transformers Augmented with Knowledge Graphs](http://arxiv.org/abs/2210.10144)</code></li>
<li>Summary: <p>The extraction of aspect terms is a critical step in fine-grained sentiment
analysis of text. Existing approaches for this task have yielded impressive
results when the training and testing data are from the same domain. However,
these methods show a drastic decrease in performance when applied to
cross-domain settings where the domain of the testing data differs from that of
the training data. To address this lack of extensibility and robustness, we
propose a novel approach for automatically constructing domain-specific
knowledge graphs that contain information relevant to the identification of
aspect terms. We introduce a methodology for injecting information from these
knowledge graphs into Transformer models, including two alternative mechanisms
for knowledge insertion: via query enrichment and via manipulation of attention
patterns. We demonstrate state-of-the-art performance on benchmark datasets for
cross-domain aspect term extraction using our approach and investigate how the
amount of external knowledge available to the Transformer impacts model
performance.
</p></li>
</ul>

<h3>Title: BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining. (arXiv:2210.10341v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10341">http://arxiv.org/abs/2210.10341</a></li>
<li>Code URL: <a href="https://github.com/microsoft/biogpt">https://github.com/microsoft/biogpt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10341] BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining](http://arxiv.org/abs/2210.10341)</code></li>
<li>Summary: <p>Pre-trained language models have attracted increasing attention in the
biomedical domain, inspired by their great success in the general natural
language domain. Among the two main branches of pre-trained language models in
the general language domain, i.e., BERT (and its variants) and GPT (and its
variants), the first one has been extensively studied in the biomedical domain,
such as BioBERT and PubMedBERT. While they have achieved great success on a
variety of discriminative downstream biomedical tasks, the lack of generation
ability constrains their application scope. In this paper, we propose BioGPT, a
domain-specific generative Transformer language model pre-trained on large
scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and
demonstrate that our model outperforms previous models on most tasks.
Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI
end-to-end relation extraction tasks respectively, and 78.2% accuracy on
PubMedQA, creating a new record. Our case study on text generation further
demonstrates the advantage of BioGPT on biomedical literature to generate
fluent descriptions for biomedical terms. Code is available at
https://github.com/microsoft/BioGPT.
</p></li>
</ul>

<h3>Title: CEntRE: A paragraph-level Chinese dataset for Relation Extraction among Enterprises. (arXiv:2210.10581v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10581">http://arxiv.org/abs/2210.10581</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10581] CEntRE: A paragraph-level Chinese dataset for Relation Extraction among Enterprises](http://arxiv.org/abs/2210.10581)</code></li>
<li>Summary: <p>Enterprise relation extraction aims to detect pairs of enterprise entities
and identify the business relations between them from unstructured or
semi-structured text data, and it is crucial for several real-world
applications such as risk analysis, rating research and supply chain security.
However, previous work mainly focuses on getting attribute information about
enterprises like personnel and corporate business, and pays little attention to
enterprise relation extraction. To encourage further progress in the research,
we introduce the CEntRE, a new dataset constructed from publicly available
business news data with careful human annotation and intelligent data
processing. Extensive experiments on CEntRE with six excellent models
demonstrate the challenges of our proposed dataset.
</p></li>
</ul>

<h3>Title: Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study. (arXiv:2210.10678v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10678">http://arxiv.org/abs/2210.10678</a></li>
<li>Code URL: <a href="https://github.com/zjunlp/lrebench">https://github.com/zjunlp/lrebench</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10678] Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study](http://arxiv.org/abs/2210.10678)</code></li>
<li>Summary: <p>This paper presents an empirical study to build relation extraction systems
in low-resource settings. Based upon recent pre-trained language models, we
comprehensively investigate three schemes to evaluate the performance in
low-resource settings: (i) different types of prompt-based methods with
few-shot labeled data; (ii) diverse balancing methods to address the
long-tailed distribution issue; (iii) data augmentation technologies and
self-training to generate more labeled in-domain data. We create a benchmark
with 8 relation extraction (RE) datasets covering different languages, domains
and contexts and perform extensive comparisons over the proposed schemes with
combinations. Our experiments illustrate: (i) Though prompt-based tuning is
beneficial in low-resource RE, there is still much potential for improvement,
especially in extracting relations from cross-sentence contexts with multiple
relational triples; (ii) Balancing methods are not always helpful for RE with
long-tailed distribution; (iii) Data augmentation complements existing
baselines and can bring much performance gain, while self-training may not
consistently achieve advancement to low-resource RE. Code and datasets are in
https://github.com/zjunlp/LREBench.
</p></li>
</ul>

<h3>Title: Schema-aware Reference as Prompt Improves Data-Efficient Relational Triple and Event Extraction. (arXiv:2210.10709v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10709">http://arxiv.org/abs/2210.10709</a></li>
<li>Code URL: <a href="https://github.com/zjunlp/RAP">https://github.com/zjunlp/RAP</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10709] Schema-aware Reference as Prompt Improves Data-Efficient Relational Triple and Event Extraction](http://arxiv.org/abs/2210.10709)</code></li>
<li>Summary: <p>Information Extraction, which aims to extract structural relational triple or
event from unstructured texts, often suffers from data scarcity issues. With
the development of pre-trained language models, many prompt-based approaches to
data-efficient information extraction have been proposed and achieved
impressive performance. However, existing prompt learning methods for
information extraction are still susceptible to several potential limitations:
(i) semantic gap between natural language and output structure knowledge with
pre-defined schema; (ii) representation learning with locally individual
instances limits the performance given the insufficient features. In this
paper, we propose a novel approach of schema-aware Reference As Prompt (RAP),
which dynamically leverage schema and knowledge inherited from global
(few-shot) training data for each sample. Specifically, we propose a
schema-aware reference store, which unifies symbolic schema and relevant
textual instances. Then, we employ a dynamic reference integration module to
retrieve pertinent knowledge from the datastore as prompts during training and
inference. Experimental results demonstrate that RAP can be plugged into
various existing models and outperforms baselines in low-resource settings on
five datasets of relational triple extraction and event extraction. In
addition, we provide comprehensive empirical ablations and case analysis
regarding different types and scales of knowledge in order to better understand
the mechanisms of RAP. Code is available in https://github.com/zjunlp/RAP.
</p></li>
</ul>

<h2>membership infer</h2>
<h3>Title: Canary in a Coalmine: Better Membership Inference with Ensembled Adversarial Queries. (arXiv:2210.10750v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10750">http://arxiv.org/abs/2210.10750</a></li>
<li>Code URL: <a href="https://github.com/yuxinwenrick/canary-in-a-coalmine">https://github.com/yuxinwenrick/canary-in-a-coalmine</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10750] Canary in a Coalmine: Better Membership Inference with Ensembled Adversarial Queries](http://arxiv.org/abs/2210.10750)</code></li>
<li>Summary: <p>As industrial applications are increasingly automated by machine learning
models, enforcing personal data ownership and intellectual property rights
requires tracing training data back to their rightful owners. Membership
inference algorithms approach this problem by using statistical techniques to
discern whether a target sample was included in a model's training set.
However, existing methods only utilize the unaltered target sample or simple
augmentations of the target to compute statistics. Such a sparse sampling of
the model's behavior carries little information, leading to poor inference
capabilities. In this work, we use adversarial tools to directly optimize for
queries that are discriminative and diverse. Our improvements achieve
significantly more accurate membership inference than existing methods,
especially in offline scenarios and in the low false-positive regime which is
critical in legal settings. Code is available at
https://github.com/YuxinWenRick/canary-in-a-coalmine.
</p></li>
</ul>

<h2>federate</h2>
<h2>fair</h2>
<h3>Title: Towards Procedural Fairness: Uncovering Biases in How a Toxic Language Classifier Uses Sentiment Information. (arXiv:2210.10689v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10689">http://arxiv.org/abs/2210.10689</a></li>
<li>Code URL: <a href="https://github.com/isarnejad/procedural-fairness-sentiment">https://github.com/isarnejad/procedural-fairness-sentiment</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10689] Towards Procedural Fairness: Uncovering Biases in How a Toxic Language Classifier Uses Sentiment Information](http://arxiv.org/abs/2210.10689)</code></li>
<li>Summary: <p>Previous works on the fairness of toxic language classifiers compare the
output of models with different identity terms as input features but do not
consider the impact of other important concepts present in the context. Here,
besides identity terms, we take into account high-level latent features learned
by the classifier and investigate the interaction between these features and
identity terms. For a multi-class toxic language classifier, we leverage a
concept-based explanation framework to calculate the sensitivity of the model
to the concept of sentiment, which has been used before as a salient feature
for toxic language detection. Our results show that although for some classes,
the classifier has learned the sentiment information as expected, this
information is outweighed by the influence of identity terms as input features.
This work is a step towards evaluating procedural fairness, where unfair
processes lead to unfair outcomes. The produced knowledge can guide debiasing
techniques to ensure that important concepts besides identity terms are
well-represented in training datasets.
</p></li>
</ul>

<h3>Title: Group Fairness in Prediction-Based Decision Making: From Moral Assessment to Implementation. (arXiv:2210.10456v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10456">http://arxiv.org/abs/2210.10456</a></li>
<li>Code URL: <a href="https://github.com/joebaumann/fair-prediction-based-decision-making">https://github.com/joebaumann/fair-prediction-based-decision-making</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10456] Group Fairness in Prediction-Based Decision Making: From Moral Assessment to Implementation](http://arxiv.org/abs/2210.10456)</code></li>
<li>Summary: <p>Ensuring fairness of prediction-based decision making is based on statistical
group fairness criteria. Which one of these criteria is the morally most
appropriate one depends on the context, and its choice requires an ethical
analysis. In this paper, we present a step-by-step procedure integrating three
elements: (a) a framework for the moral assessment of what fairness means in a
given context, based on the recently proposed general principle of "Fair
equality of chances" (FEC) (b) a mapping of the assessment's results to
established statistical group fairness criteria, and (c) a method for
integrating the thus-defined fairness into optimal decision making. As a second
contribution, we show new applications of the FEC principle and show that, with
this extension, the FEC framework covers all types of group fairness criteria:
independence, separation, and sufficiency. Third, we introduce an extended
version of the FEC principle, which additionally allows accounting for morally
irrelevant elements of the fairness assessment and links to well-known
relaxations of the fairness criteria. This paper presents a framework to
develop fair decision systems in a conceptually sound way, combining the moral
and the computational elements of fair prediction-based decision-making in an
integrated approach. Data and code to reproduce our results are available at
https://github.com/joebaumann/fair-prediction-based-decision-making.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Towards Explaining Distribution Shifts. (arXiv:2210.10275v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.10275">http://arxiv.org/abs/2210.10275</a></li>
<li>Code URL: <a href="https://github.com/inouye-lab/explaining-distribution-shifts">https://github.com/inouye-lab/explaining-distribution-shifts</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.10275] Towards Explaining Distribution Shifts](http://arxiv.org/abs/2210.10275)</code></li>
<li>Summary: <p>A distribution shift can have fundamental consequences such as signaling a
change in the operating environment or significantly reducing the accuracy of
downstream models. Thus, understanding distribution shifts is critical for
examining and hopefully mitigating the effect of such a shift. Most prior work
has focused on merely detecting if a shift has occurred and assumes any
detected shift can be understood and handled appropriately by a human operator.
We hope to aid in these manual mitigation tasks by explaining the distribution
shift using interpretable transportation maps from the original distribution to
the shifted one. We derive our interpretable mappings from a relaxation of the
optimal transport problem, where the candidate mappings are restricted to a set
of interpretable mappings. We then use quintessential examples of distribution
shift in simulated and real-world cases to showcase how our explanatory
mappings provide a better balance between detail and interpretability than the
de facto standard mean shift explanation by both visual inspection and our
PercentExplained metric.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
