<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-09-04</h1>
<h3>Title: Evaluating Explainable AI Methods in Deep Learning Models for Early Detection of Cerebral Palsy</h3>
<ul>
<li><strong>Authors: </strong>Kimji N. Pellano, Inga Str√ºmke, Daniel Groos, Lars Adde, Espen Alexander F. Ihlen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00001">https://arxiv.org/abs/2409.00001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00001">https://arxiv.org/pdf/2409.00001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00001]] Evaluating Explainable AI Methods in Deep Learning Models for Early Detection of Cerebral Palsy(https://arxiv.org/abs/2409.00001)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Early detection of Cerebral Palsy (CP) is crucial for effective intervention and monitoring. This paper tests the reliability and applicability of Explainable AI (XAI) methods using a deep learning method that predicts CP by analyzing skeletal data extracted from video recordings of infant movements. Specifically, we use XAI evaluation metrics -- namely faithfulness and stability -- to quantitatively assess the reliability of Class Activation Mapping (CAM) and Gradient-weighted Class Activation Mapping (Grad-CAM) in this specific medical application. We utilize a unique dataset of infant movements and apply skeleton data perturbations without distorting the original dynamics of the infant movements. Our CP prediction model utilizes an ensemble approach, so we evaluate the XAI metrics performances for both the overall ensemble and the individual models. Our findings indicate that both XAI methods effectively identify key body points influencing CP predictions and that the explanations are robust against minor data perturbations. Grad-CAM significantly outperforms CAM in the RISv metric, which measures stability in terms of velocity. In contrast, CAM performs better in the RISb metric, which relates to bone stability, and the RRS metric, which assesses internal representation robustness. Individual models within the ensemble show varied results, and neither CAM nor Grad-CAM consistently outperform the other, with the ensemble approach providing a representation of outcomes from its constituent models.</li>
</ul>

<h3>Title: DivDiff: A Conditional Diffusion Model for Diverse Human Motion Prediction</h3>
<ul>
<li><strong>Authors: </strong>Hua Yu, Yaqing Hou, Wenbin Pei, Qiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00014">https://arxiv.org/abs/2409.00014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00014">https://arxiv.org/pdf/2409.00014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00014]] DivDiff: A Conditional Diffusion Model for Diverse Human Motion Prediction(https://arxiv.org/abs/2409.00014)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Diverse human motion prediction (HMP) aims to predict multiple plausible future motions given an observed human motion sequence. It is a challenging task due to the diversity of potential human motions while ensuring an accurate description of future human motions. Current solutions are either low-diversity or limited in expressiveness. Recent denoising diffusion models (DDPM) hold potential generative capabilities in generative tasks. However, introducing DDPM directly into diverse HMP incurs some issues. Although DDPM can increase the diversity of the potential patterns of human motions, the predicted human motions become implausible over time because of the significant noise disturbances in the forward process of DDPM. This phenomenon leads to the predicted human motions being hard to control, seriously impacting the quality of predicted motions and restricting their practical applicability in real-world scenarios. To alleviate this, we propose a novel conditional diffusion-based generative model, called DivDiff, to predict more diverse and realistic human motions. Specifically, the DivDiff employs DDPM as our backbone and incorporates Discrete Cosine Transform (DCT) and transformer mechanisms to encode the observed human motion sequence as a condition to instruct the reverse process of DDPM. More importantly, we design a diversified reinforcement sampling function (DRSF) to enforce human skeletal constraints on the predicted human motions. DRSF utilizes the acquired information from human skeletal as prior knowledge, thereby reducing significant disturbances introduced during the forward process. Extensive results received in the experiments on two widely-used datasets (Human3.6M and HumanEva-I) demonstrate that our model obtains competitive performance on both diversity and accuracy.</li>
</ul>

<h3>Title: A Novel Fusion of Optical and Radar Satellite Data for Crop Phenology Estimation using Machine Learning and Cloud Computing</h3>
<ul>
<li><strong>Authors: </strong>Shahab Aldin Shojaeezadeh, Abdelrazek Elnashar, Tobias Karl David Weber</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00020">https://arxiv.org/abs/2409.00020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00020">https://arxiv.org/pdf/2409.00020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00020]] A Novel Fusion of Optical and Radar Satellite Data for Crop Phenology Estimation using Machine Learning and Cloud Computing(https://arxiv.org/abs/2409.00020)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Crop phenology determines crop growth stages and is valuable information for decision makers to plant and adapt agricultural management strategies to enhance food security. In the era of big Earth observation data ubiquity, attempts have been made to accurately predict crop phenology based on Remote Sensing (RS) data. However, most studies either focused on large scale interpretations of phenology or developed methods which are not adequate to help crop modeler communities on leveraging the value of RS data evaluated using more accurate and confident methods. Here, we estimate phenological developments for eight major crops and 13 phenological stages across Germany at 30m scale using a novel framework which fuses Landsat and Sentinel 2 (Harmonized Landsat and Sentinel data base; HLS) and radar of Sentinel 1 with a Machine Learning (ML) model. We proposed a thorough feature fusion analysis to find the best combinations of RS data on detecting phenological developments based on the national phenology network of Germany (German Meteorological Service; DWD) between 2017 and 2021. The nation-wide predicted crop phenology at 30 m resolution showed a very high precision of R2 > 0.9 and a very low Mean Absolute Error (MAE) < 2 (days). These results indicate that our fusing strategy of optical and radar datasets is highly performant with an accuracy highly relevant for practical applications, too. The subsequent uncertainty analysis indicated that fusing optical and radar data increases the reliability of the RS predicted crop growth stages. These improvements are expected to be useful for crop model calibrations and evaluations, facilitate informed agricultural decisions, and contribute to sustainable food production to address the increasing global food demand.</li>
</ul>

<h3>Title: Attack Anything: Blind DNNs via Universal Background Adversarial Attack</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Lian, Shaohui Mei, Xiaofei Wang, Yi Wang, Lefan Wang, Yingjie Lu, Mingyang Ma, Lap-Pui Chau</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00029">https://arxiv.org/abs/2409.00029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00029">https://arxiv.org/pdf/2409.00029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00029]] Attack Anything: Blind DNNs via Universal Background Adversarial Attack(https://arxiv.org/abs/2409.00029)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>It has been widely substantiated that deep neural networks (DNNs) are susceptible and vulnerable to adversarial perturbations. Existing studies mainly focus on performing attacks by corrupting targeted objects (physical attack) or images (digital attack), which is intuitively acceptable and understandable in terms of the attack's effectiveness. In contrast, our focus lies in conducting background adversarial attacks in both digital and physical domains, without causing any disruptions to the targeted objects themselves. Specifically, an effective background adversarial attack framework is proposed to attack anything, by which the attack efficacy generalizes well between diverse objects, models, and tasks. Technically, we approach the background adversarial attack as an iterative optimization problem, analogous to the process of DNN learning. Besides, we offer a theoretical demonstration of its convergence under a set of mild but sufficient conditions. To strengthen the attack efficacy and transferability, we propose a new ensemble strategy tailored for adversarial perturbations and introduce an improved smooth constraint for the seamless connection of integrated perturbations. We conduct comprehensive and rigorous experiments in both digital and physical domains across various objects, models, and tasks, demonstrating the effectiveness of attacking anything of the proposed method. The findings of this research substantiate the significant discrepancy between human and machine vision on the value of background variations, which play a far more critical role than previously recognized, necessitating a reevaluation of the robustness and reliability of DNNs. The code will be publicly available at this https URL</li>
</ul>

<h3>Title: PolypDB: A Curated Multi-Center Dataset for Development of AI Algorithms in Colonoscopy</h3>
<ul>
<li><strong>Authors: </strong>Debesh Jha, Nikhil Kumar Tomar, Vanshali Sharma, Quoc-Huy Trinh, Koushik Biswas, Hongyi Pan, Ritika K. Jha, Gorkem Durak, Alexander Hann, Jonas Varkey, Hang Viet Dao, Long Van Dao, Binh Phuc Nguyen, Khanh Cong Pham, Quang Trung Tran, Nikolaos Papachrysos, Brandon Rieders, Peter Thelin Schmidt, Enrik Geissler, Tyler Berzin, P√•l Halvorsen, Michael A. Riegler, Thomas de Lange, Ulas Bagci</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00045">https://arxiv.org/abs/2409.00045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00045">https://arxiv.org/pdf/2409.00045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00045]] PolypDB: A Curated Multi-Center Dataset for Development of AI Algorithms in Colonoscopy(https://arxiv.org/abs/2409.00045)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, segmentation</a></li>
<li><strong>Abstract: </strong>Colonoscopy is the primary method for examination, detection, and removal of polyps. Regular screening helps detect and prevent colorectal cancer at an early curable stage. However, challenges such as variation among the endoscopists' skills, bowel quality preparation, and complex nature of the large intestine which cause large number of polyp miss-rate. These missed polyps can develop into cancer later on, which underscores the importance of improving the detection methods. A computer-aided diagnosis system can support physicians by assisting in detecting overlooked polyps. However, one of the important challenges for developing novel deep learning models for automatic polyp detection and segmentation is the lack of publicly available, multi-center large and diverse datasets. To address this gap, we introduce PolypDB, a large scale publicly available dataset that contains 3934 still polyp images and their corresponding ground truth from real colonoscopy videos to design efficient polyp detection and segmentation architectures. The dataset has been developed and verified by a team of 10 gastroenterologists. PolypDB comprises of images from five modalities: Blue Light Imaging (BLI), Flexible Imaging Color Enhancement (FICE), Linked Color Imaging (LCI), Narrow Band Imaging (NBI), and White Light Imaging (WLI) and three medical centers from Norway, Sweden and Vietnam. Thus, we split the dataset based on modality and medical center for modality-wise and center-wise analysis. We provide a benchmark on each modality using eight popular segmentation methods and six standard benchmark polyp detection methods. Furthermore, we also provide benchmark on center-wise under federated learning settings. Our dataset is public and can be downloaded at \url{this https URL}.</li>
</ul>

<h3>Title: Automating Knowledge Discovery from Scientific Literature via LLMs: A Dual-Agent Approach with Progressive Ontology Prompting</h3>
<ul>
<li><strong>Authors: </strong>Yuting Hu, Dancheng Liu, Qingyun Wang, Charles Yu, Heng Ji, Jinjun Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00054">https://arxiv.org/abs/2409.00054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00054">https://arxiv.org/pdf/2409.00054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00054]] Automating Knowledge Discovery from Scientific Literature via LLMs: A Dual-Agent Approach with Progressive Ontology Prompting(https://arxiv.org/abs/2409.00054)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>To address the challenge of automating knowledge discovery from a vast volume of literature, in this paper, we introduce a novel framework based on large language models (LLMs) that combines a progressive ontology prompting (POP) algorithm with a dual-agent system, named LLM-Duo, designed to enhance the automation of knowledge extraction from scientific articles. The POP algorithm utilizes a prioritized breadth-first search (BFS) across a predefined ontology to generate structured prompt templates and action orders, thereby guiding LLMs to discover knowledge in an automatic manner. Additionally, our LLM-Duo employs two specialized LLM agents: an explorer and an evaluator. These two agents work collaboratively and adversarially to enhance the reliability of the discovery and annotation processes. Experiments demonstrate that our method outperforms advanced baselines, enabling more accurate and complete annotations. To validate the effectiveness of our method in real-world scenarios, we employ our method in a case study of speech-language intervention discovery. Our method identifies 2,421 interventions from 64,177 research articles in the speech-language therapy domain. We curate these findings into a publicly accessible intervention knowledge base that holds significant potential to benefit the speech-language therapy community.</li>
</ul>

<h3>Title: SORSA: Singular Values and Orthonormal Regularized Singular Vectors Adaptation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Cao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00055">https://arxiv.org/abs/2409.00055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00055">https://arxiv.org/pdf/2409.00055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00055]] SORSA: Singular Values and Orthonormal Regularized Singular Vectors Adaptation of Large Language Models(https://arxiv.org/abs/2409.00055)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement in large language models (LLMs) comes with a significant increase in their parameter size, presenting challenges for adaptation and fine-tuning. Parameter-efficient fine-tuning (PEFT) methods are widely used to adapt LLMs for downstream tasks efficiently. In this paper, we propose Singular Values and Orthonormal Regularized Singular Vectors Adaptation, or SORSA, a novel PEFT method. We introduce a method to analyze the variation of the parameters by performing singular value decomposition (SVD) on weights and discuss SORSA's superiority in minimizing the deviation from the pre-trained weight. Each SORSA layer consists of two main parts: trainable principle singular weights $W_p = U_p \Sigma_p V^\top_p$, and frozen residual weights $W_r = U_r \Sigma_r V^\top_r$. These parts are initialized by performing SVD on pre-trained weights. Moreover, we implement an orthonormal regularizer and analyze its importance by performing gradient analysis. The analysis shows that the regularizer could effectively transfer the scaling information into $\Sigma_p$, which ensures the parameter updating of SORSA layers is evenly and minimized on $U_p$ and $V^\top_p$. SORSA layers could be merged during inference, thus eliminating inference latency. After all, SORSA shows a faster convergence speed than PiSSA and LoRA in our experiments. On the MATH benchmark, Llama 2 7B adapted using SORSA achieved 10.36% accuracy, outperforming LoRA (5.50%), Full FT (7.22%), and PiSSA (7.44%). On the GSM-8K benchmark, SORSA achieved 56.03% accuracy, surpassing LoRA (42.30%), Full FT (49.05%), and PiSSA (53.07%) We conclude that SORSA offers a new perspective on parameter-efficient fine-tuning, demonstrating remarkable performance. The code is available at this https URL.</li>
</ul>

<h3>Title: Understanding Literary Texts by LLMs: A Case Study of Ancient Chinese Poetry</h3>
<ul>
<li><strong>Authors: </strong>Cheng Zhao, Bin Wang, Zhen Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00060">https://arxiv.org/abs/2409.00060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00060">https://arxiv.org/pdf/2409.00060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00060]] Understanding Literary Texts by LLMs: A Case Study of Ancient Chinese Poetry(https://arxiv.org/abs/2409.00060)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The birth and rapid development of large language models (LLMs) have caused quite a stir in the field of literature. Once considered unattainable, AI's role in literary creation is increasingly becoming a reality. In genres such as poetry, jokes, and short stories, numerous AI tools have emerged, offering refreshing new perspectives. However, it's difficult to further improve the quality of these works. This is primarily because understanding and appreciating a good literary work involves a considerable threshold, such as knowledge of literary theory, aesthetic sensibility, interdisciplinary knowledge. Therefore, authoritative data in this area is quite lacking. Additionally, evaluating literary works is often complex and hard to fully quantify, which directly hinders the further development of AI creation. To address this issue, this paper attempts to explore the mysteries of literary texts from the perspective of LLMs, using ancient Chinese poetry as an example for experimentation. First, we collected a variety of ancient poems from different sources and had experts annotate a small portion of them. Then, we designed a range of comprehension metrics based on LLMs to evaluate all these poems. Finally, we analyzed the correlations and differences between various poem collections to identify literary patterns. Through our experiments, we observed a series of enlightening phenomena that provide technical support for the future development of high-level literary creation based on LLMs.</li>
</ul>

<h3>Title: Urban Mobility Assessment Using LLMs</h3>
<ul>
<li><strong>Authors: </strong>Prabin Bhandari, Antonios Anastasopoulos, Dieter Pfoser</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00063">https://arxiv.org/abs/2409.00063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00063">https://arxiv.org/pdf/2409.00063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00063]] Urban Mobility Assessment Using LLMs(https://arxiv.org/abs/2409.00063)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Understanding urban mobility patterns and analyzing how people move around cities helps improve the overall quality of life and supports the development of more livable, efficient, and sustainable urban areas. A challenging aspect of this work is the collection of mobility data by means of user tracking or travel surveys, given the associated privacy concerns, noncompliance, and high cost. This work proposes an innovative AI-based approach for synthesizing travel surveys by prompting large language models (LLMs), aiming to leverage their vast amount of relevant background knowledge and text generation capabilities. Our study evaluates the effectiveness of this approach across various U.S. metropolitan areas by comparing the results against existing survey data at different granularity levels. These levels include (i) pattern level, which compares aggregated metrics like the average number of locations traveled and travel time, (ii) trip level, which focuses on comparing trips as whole units using transition probabilities, and (iii) activity chain level, which examines the sequence of locations visited by individuals. Our work covers several proprietary and open-source LLMs, revealing that open-source base models like Llama-2, when fine-tuned on even a limited amount of actual data, can generate synthetic data that closely mimics the actual travel survey data, and as such provides an argument for using such data in mobility studies.</li>
</ul>

<h3>Title: Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Linda Zeng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00071">https://arxiv.org/abs/2409.00071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00071">https://arxiv.org/pdf/2409.00071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00071]] Generative-Adversarial Networks for Low-Resource Language Data Augmentation in Machine Translation(https://arxiv.org/abs/2409.00071)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Neural Machine Translation (NMT) systems struggle when translating to and from low-resource languages, which lack large-scale data corpora for models to use for training. As manual data curation is expensive and time-consuming, we propose utilizing a generative-adversarial network (GAN) to augment low-resource language data. When training on a very small amount of language data (under 20,000 sentences) in a simulated low-resource setting, our model shows potential at data augmentation, generating monolingual language data with sentences such as "ask me that healthy lunch im cooking up," and "my grandfather work harder than your grandfather before." Our novel data augmentation approach takes the first step in investigating the capability of GANs in low-resource NMT, and our results suggest that there is promise for future extension of GANs to low-resource NMT.</li>
</ul>

<h3>Title: Are LLM-based methods good enough for detecting unfair terms of service?</h3>
<ul>
<li><strong>Authors: </strong>Mirgita Frasheri, Arian Bakhtiarnia, Lukas Esterle, Aleksandros Iosifidis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00077">https://arxiv.org/abs/2409.00077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00077">https://arxiv.org/pdf/2409.00077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00077]] Are LLM-based methods good enough for detecting unfair terms of service?(https://arxiv.org/abs/2409.00077)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, fair, large language model</a></li>
<li><strong>Abstract: </strong>Countless terms of service (ToS) are being signed everyday by users all over the world while interacting with all kinds of apps and websites. More often than not, these online contracts spanning double-digit pages are signed blindly by users who simply want immediate access to the desired service. What would normally require a consultation with a legal team, has now become a mundane activity consisting of a few clicks where users potentially sign away their rights, for instance in terms of their data privacy, to countless online entities/companies. Large language models (LLMs) are good at parsing long text-based documents, and could potentially be adopted to help users when dealing with dubious clauses in ToS and their underlying privacy policies. To investigate the utility of existing models for this task, we first build a dataset consisting of 12 questions applied individually to a set of privacy policies crawled from popular websites. Thereafter, a series of open-source as well as commercial chatbots such as ChatGPT, are queried over each question, with the answers being compared to a given ground truth. Our results show that some open-source models are able to provide a higher accuracy compared to some commercial models. However, the best performance is recorded from a commercial chatbot (ChatGPT4). Overall, all models perform only slightly better than random at this task. Consequently, their performance needs to be significantly improved before they can be adopted at large for this purpose.</li>
</ul>

<h3>Title: Towards Human-Level Understanding of Complex Process Engineering Schematics: A Pedagogical, Introspective Multi-Agent Framework for Open-Domain Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Sagar Srinivas Sakhinana, Geethan Sannidhi, Venkataramana Runkana</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00082">https://arxiv.org/abs/2409.00082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00082">https://arxiv.org/pdf/2409.00082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00082]] Towards Human-Level Understanding of Complex Process Engineering Schematics: A Pedagogical, Introspective Multi-Agent Framework for Open-Domain Question Answering(https://arxiv.org/abs/2409.00082)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, explainability, generative</a></li>
<li><strong>Abstract: </strong>In the chemical and process industries, Process Flow Diagrams (PFDs) and Piping and Instrumentation Diagrams (P&IDs) are critical for design, construction, and maintenance. Recent advancements in Generative AI, such as Large Multimodal Models (LMMs) like GPT4 (Omni), have shown promise in understanding and interpreting process diagrams for Visual Question Answering (VQA). However, proprietary models pose data privacy risks, and their computational complexity prevents knowledge editing for domain-specific customization on consumer hardware. To overcome these challenges, we propose a secure, on-premises enterprise solution using a hierarchical, multi-agent Retrieval Augmented Generation (RAG) framework for open-domain question answering (ODQA) tasks, offering enhanced data privacy, explainability, and cost-effectiveness. Our novel multi-agent framework employs introspective and specialized sub-agents using open-source, small-scale multimodal models with the ReAct (Reason+Act) prompting technique for PFD and P&ID analysis, integrating multiple information sources to provide accurate and contextually relevant answers. Our approach, supported by iterative self-correction, aims to deliver superior performance in ODQA tasks. We conducted rigorous experimental studies, and the empirical results validated the proposed approach effectiveness.</li>
</ul>

<h3>Title: Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models</h3>
<ul>
<li><strong>Authors: </strong>Seyed Amir Ahmad Safavi-Naini, Shuhaib Ali, Omer Shahab, Zahra Shahhoseini, Thomas Savage, Sara Rafiee, Jamil S Samaan, Reem Al Shabeeb, Farah Ladak, Jamie O Yang, Juan Echavarria, Sumbal Babar, Aasma Shaukat, Samuel Margolis, Nicholas P Tatonetti, Girish Nadkarni, Bara El Kurdi, Ali Soroush</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00084">https://arxiv.org/abs/2409.00084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00084">https://arxiv.org/pdf/2409.00084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00084]] Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models(https://arxiv.org/abs/2409.00084)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Background and Aims: This study evaluates the medical reasoning performance of large language models (LLMs) and vision language models (VLMs) in gastroenterology. Methods: We used 300 gastroenterology board exam-style multiple-choice questions, 138 of which contain images to systematically assess the impact of model configurations and parameters and prompt engineering strategies utilizing GPT-3.5. Next, we assessed the performance of proprietary and open-source LLMs (versions), including GPT (3.5, 4, 4¬∞, 4omini), Claude (3, 3.5), Gemini (1.0), Mistral, Llama (2, 3, 3.1), Mixtral, and Phi (3), across different interfaces (web and API), computing environments (cloud and local), and model precisions (with and without quantization). Finally, we assessed accuracy using a semiautomated pipeline. Results: Among the proprietary models, GPT-4o (73.7%) and Claude3.5-Sonnet (74.0%) achieved the highest accuracy, whereas Llama3-70b (54.7%) and Mixtral8x7b (54.3%) were the most accurate open-source models. Among the quantized open-source models, the 6-bit quantized Phi3-14b (48.7%) performed best. The scores of the quantized models were comparable to those of the full-precision models Llama2--7b, Llama2--13b, and Gemma2--9b. Notably, VLM performance on image-containing questions did not improve when the images were provided and worsened when LLM-generated captions were provided. In contrast, a 10% increase in accuracy was observed when images were accompanied by one-sentence human-crafted image descriptions. Conclusion: In conclusion, while LLMs exhibit robust zero-shot performance in medical reasoning, the integration of visual data remains a challenge for VLMs. Effective deployment involves carefully determining optimal model configurations, encouraging users to consider either the high performance of proprietary models or the flexible adaptability of open-source models.</li>
</ul>

<h3>Title: Genetic Approach to Mitigate Hallucination in Generative IR</h3>
<ul>
<li><strong>Authors: </strong>Hrishikesh Kulkarni, Nazli Goharian, Ophir Frieder, Sean MacAvaney</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00085">https://arxiv.org/abs/2409.00085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00085">https://arxiv.org/pdf/2409.00085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00085]] Genetic Approach to Mitigate Hallucination in Generative IR(https://arxiv.org/abs/2409.00085)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative language models hallucinate. That is, at times, they generate factually flawed responses. These inaccuracies are particularly insidious because the responses are fluent and well-articulated. We focus on the task of Grounded Answer Generation (part of Generative IR), which aims to produce direct answers to a user's question based on results retrieved from a search engine. We address hallucination by adapting an existing genetic generation approach with a new 'balanced fitness function' consisting of a cross-encoder model for relevance and an n-gram overlap metric to promote grounding. Our balanced fitness function approach quadruples the grounded answer generation accuracy while maintaining high relevance.</li>
</ul>

<h3>Title: On-Device Language Models: A Comprehensive Review</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Xu, Zhiyuan Li, Wei Chen, Qun Wang, Xin Gao, Qi Cai, Ziyuan Ling</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00088">https://arxiv.org/abs/2409.00088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00088">https://arxiv.org/pdf/2409.00088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00088]] On-Device Language Models: A Comprehensive Review(https://arxiv.org/abs/2409.00088)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The advent of large language models (LLMs) revolutionized natural language processing applications, and running LLMs on edge devices has become increasingly attractive for reasons including reduced latency, data localization, and personalized user experiences. This comprehensive review examines the challenges of deploying computationally expensive LLMs on resource-constrained devices and explores innovative solutions across multiple domains. The paper investigates the development of on-device language models, their efficient architectures, including parameter sharing and modular designs, as well as state-of-the-art compression techniques like quantization, pruning, and knowledge distillation. Hardware acceleration strategies and collaborative edge-cloud deployment approaches are analyzed, highlighting the intricate balance between performance and resource utilization. Case studies of on-device language models from major mobile manufacturers demonstrate real-world applications and potential benefits. The review also addresses critical aspects such as adaptive learning, multi-modal capabilities, and personalization. By identifying key research directions and open challenges, this paper provides a roadmap for future advancements in on-device language models, emphasizing the need for interdisciplinary efforts to realize the full potential of ubiquitous, intelligent computing while ensuring responsible and ethical deployment. For a comprehensive review of research work and educational resources on on-device large language models (LLMs), please visit this https URL. To download and run on-device LLMs, visit this https URL.</li>
</ul>

<h3>Title: Watermarking Techniques for Large Language Models: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Yuqing Liang, Jiancheng Xiao, Wensheng Gan, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00089">https://arxiv.org/abs/2409.00089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00089">https://arxiv.org/pdf/2409.00089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00089]] Watermarking Techniques for Large Language Models: A Survey(https://arxiv.org/abs/2409.00089)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, watermark, large language model</a></li>
<li><strong>Abstract: </strong>With the rapid advancement and extensive application of artificial intelligence technology, large language models (LLMs) are extensively used to enhance production, creativity, learning, and work efficiency across various domains. However, the abuse of LLMs also poses potential harm to human society, such as intellectual property rights issues, academic misconduct, false content, and hallucinations. Relevant research has proposed the use of LLM watermarking to achieve IP protection for LLMs and traceability of multimedia data output by LLMs. To our knowledge, this is the first thorough review that investigates and analyzes LLM watermarking technology in detail. This review begins by recounting the history of traditional watermarking technology, then analyzes the current state of LLM watermarking research, and thoroughly examines the inheritance and relevance of these techniques. By analyzing their inheritance and relevance, this review can provide research with ideas for applying traditional digital watermarking techniques to LLM watermarking, to promote the cross-integration and innovation of watermarking technology. In addition, this review examines the pros and cons of LLM watermarking. Considering the current multimodal development trend of LLMs, it provides a detailed analysis of emerging multimodal LLM watermarking, such as visual and audio data, to offer more reference ideas for relevant research. This review delves into the challenges and future prospects of current watermarking technologies, offering valuable insights for future LLM watermarking research and applications.</li>
</ul>

<h3>Title: Evaluating ChatGPT on Nuclear Domain-Specific Data</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Anwar, Mischa de Costa, Issam Hammad, Daniel Lau</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00090">https://arxiv.org/abs/2409.00090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00090">https://arxiv.org/pdf/2409.00090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00090]] Evaluating ChatGPT on Nuclear Domain-Specific Data(https://arxiv.org/abs/2409.00090)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper examines the application of ChatGPT, a large language model (LLM), for question-and-answer (Q&A) tasks in the highly specialized field of nuclear data. The primary focus is on evaluating ChatGPT's performance on a curated test dataset, comparing the outcomes of a standalone LLM with those generated through a Retrieval Augmented Generation (RAG) approach. LLMs, despite their recent advancements, are prone to generating incorrect or 'hallucinated' information, which is a significant limitation in applications requiring high accuracy and reliability. This study explores the potential of utilizing RAG in LLMs, a method that integrates external knowledge bases and sophisticated retrieval techniques to enhance the accuracy and relevance of generated outputs. In this context, the paper evaluates ChatGPT's ability to answer domain-specific questions, employing two methodologies: A) direct response from the LLM, and B) response from the LLM within a RAG framework. The effectiveness of these methods is assessed through a dual mechanism of human and LLM evaluation, scoring the responses for correctness and other metrics. The findings underscore the improvement in performance when incorporating a RAG pipeline in an LLM, particularly in generating more accurate and contextually appropriate responses for nuclear domain-specific queries. Additionally, the paper highlights alternative approaches to further refine and improve the quality of answers in such specialized domains.</li>
</ul>

<h3>Title: Classification of Safety Events at Nuclear Sites using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mishca de Costa, Muhammad Anwar, Daniel Lau, Issam Hammad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00091">https://arxiv.org/abs/2409.00091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00091">https://arxiv.org/pdf/2409.00091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00091]] Classification of Safety Events at Nuclear Sites using Large Language Models(https://arxiv.org/abs/2409.00091)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper proposes the development of a Large Language Model (LLM) based machine learning classifier designed to categorize Station Condition Records (SCRs) at nuclear power stations into safety-related and non-safety-related categories. The primary objective is to augment the existing manual review process by enhancing the efficiency and accuracy of the safety classification process at nuclear stations. The paper discusses experiments performed to classify a labeled SCR dataset and evaluates the performance of the classifier. It explores the construction of several prompt variations and their observed effects on the LLM's decision-making process. Additionally, it introduces a numerical scoring mechanism that could offer a more nuanced and flexible approach to SCR safety classification. This method represents an innovative step in nuclear safety management, providing a scalable tool for the identification of safety events.</li>
</ul>

<h3>Title: PatentGPT: A Large Language Model for Patent Drafting Using Knowledge-based Fine-tuning Method</h3>
<ul>
<li><strong>Authors: </strong>Runtao Ren, Jian Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00092">https://arxiv.org/abs/2409.00092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00092">https://arxiv.org/pdf/2409.00092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00092]] PatentGPT: A Large Language Model for Patent Drafting Using Knowledge-based Fine-tuning Method(https://arxiv.org/abs/2409.00092)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, large language model</a></li>
<li><strong>Abstract: </strong>As humanity stands on the brink of a new era of technological innovation, the ability to rapidly transform creative ideas into protected intellectual property (IP) is more crucial than ever. However, the conventional processes for patent drafting are fraught with challenges, demanding a nuanced understanding of advanced field knowledge and technical concepts. Existing large language models (LLMs), while powerful, often fall short in this IP creation domain due to their lack of specialized knowledge and context-awareness necessary for generating technically accurate patent documents. To bridge this critical gap, we propose a groundbreaking framework for Knowledge Fine-Tuning (KFT) of LLMs, designed to endow AI with the ability to autonomously mine, understand, and apply domain-specific knowledge. Our model, PatentGPT leverages a unique combination of knowledge graph-based pre-training, domain-specific supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF). Through extensive evaluation, PatentGPT has demonstrated outstanding performance, scoring up to approximately 400% higher in patent related benchmark tests compared to state-of-the-art models. By KFT method the model's capability to not only assist but also augment human creativity and innovation, our approach sets a new standard for AI-driven intellectual property generation, paving the way for more efficient and effective invention processes.</li>
</ul>

<h3>Title: Examining Independence in Ensemble Sentiment Analysis: A Study on the Limits of Large Language Models Using the Condorcet Jury Theorem</h3>
<ul>
<li><strong>Authors: </strong>Baptiste Lefort, Eric Benhamou, Jean-Jacques Ohana, Beatrice Guez, David Saltiel, Thomas Jacquot</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00094">https://arxiv.org/abs/2409.00094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00094">https://arxiv.org/pdf/2409.00094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00094]] Examining Independence in Ensemble Sentiment Analysis: A Study on the Limits of Large Language Models Using the Condorcet Jury Theorem(https://arxiv.org/abs/2409.00094)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper explores the application of the Condorcet Jury theorem to the domain of sentiment analysis, specifically examining the performance of various large language models (LLMs) compared to simpler natural language processing (NLP) models. The theorem posits that a majority vote classifier should enhance predictive accuracy, provided that individual classifiers' decisions are independent. Our empirical study tests this theoretical framework by implementing a majority vote mechanism across different models, including advanced LLMs such as ChatGPT 4. Contrary to expectations, the results reveal only marginal improvements in performance when incorporating larger models, suggesting a lack of independence among them. This finding aligns with the hypothesis that despite their complexity, LLMs do not significantly outperform simpler models in reasoning tasks within sentiment analysis, showing the practical limits of model independence in the context of advanced NLP tasks.</li>
</ul>

<h3>Title: Non-instructional Fine-tuning: Enabling Instruction-Following Capabilities in Pre-trained Language Models without Instruction-Following Data</h3>
<ul>
<li><strong>Authors: </strong>Juncheng Xie, Shensian Syu, Hung-yi Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00096">https://arxiv.org/abs/2409.00096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00096">https://arxiv.org/pdf/2409.00096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00096]] Non-instructional Fine-tuning: Enabling Instruction-Following Capabilities in Pre-trained Language Models without Instruction-Following Data(https://arxiv.org/abs/2409.00096)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Instruction fine-tuning is crucial for today's large language models (LLMs) to learn to follow instructions and align with human preferences. Conventionally, supervised data, including the instruction and the correct response, is required for instruction fine-tuning. To obtain such data, some researchers prompted well-trained models like GPT-4 to generate instructions and correct responses. In this paper, we propose a novel approach that uses the first half of a random text from OpenWebText as the instruction and GPT-3.5-turbo or GPT-4-turbo to complete the text as the response. Despite the data being "non-instructional", we found that pre-trained LLMs fine-tuned on this data can gain instruction-following capabilities. This observation is verified by fine-tuning several well-known pre-trained LLMs (e.g., LLaMA-2-7B, LLaMA-3-8B, LLaMA-3-70B, Mistral-7B-v0.1). The "non-instructional data" also improved some models that underwent supervised fine-tuning and human preference alignment. Our LLaMA-3-70B-Instruct fine-tuned through "non-instructional data" is comparable with LLaMA-3.1-70B-Instruct on the Arena Hard leaderboard. We analyzed the "non-instructional data" and ensured it is devoid of content related to instruction fine-tuning. Our findings will inspire further investigation into how to develop instruction-following capabilities without explicit instruction-related data.</li>
</ul>

<h3>Title: Large Language Models for Disease Diagnosis: A Scoping Review</h3>
<ul>
<li><strong>Authors: </strong>Shuang Zhou, Zidu Xu, Mian Zhang, Chunpu Xu, Yawen Guo, Zaifu Zhan, Sirui Ding, Jiashuo Wang, Kaishuai Xu, Yi Fang, Liqiao Xia, Jeremy Yeung, Daochen Zha, Mingquan Lin, Rui Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00097">https://arxiv.org/abs/2409.00097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00097">https://arxiv.org/pdf/2409.00097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00097]] Large Language Models for Disease Diagnosis: A Scoping Review(https://arxiv.org/abs/2409.00097)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automatic disease diagnosis has become increasingly valuable in clinical practice. The advent of large language models (LLMs) has catalyzed a paradigm shift in artificial intelligence, with growing evidence supporting the efficacy of LLMs in diagnostic tasks. Despite the growing attention in this field, many critical research questions remain under-explored. For instance, what diseases and LLM techniques have been investigated for diagnostic tasks? How can suitable LLM techniques and evaluation methods be selected for clinical decision-making? To answer these questions, we performed a comprehensive analysis of LLM-based methods for disease diagnosis. This scoping review examined the types of diseases, associated organ systems, relevant clinical data, LLM techniques, and evaluation methods reported in existing studies. Furthermore, we offered guidelines for data preprocessing and the selection of appropriate LLM techniques and evaluation strategies for diagnostic tasks. We also assessed the limitations of current research and delineated the challenges and future directions in this research field. In summary, our review outlined a blueprint for LLM-based disease diagnosis, helping to streamline and guide future research endeavors.</li>
</ul>

<h3>Title: Nuance Matters: Probing Epistemic Consistency in Causal Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Shaobo Cui, Junyou Li, Luca Mouchel, Yiyang Feng, Boi Faltings</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00103">https://arxiv.org/abs/2409.00103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00103">https://arxiv.org/pdf/2409.00103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00103]] Nuance Matters: Probing Epistemic Consistency in Causal Reasoning(https://arxiv.org/abs/2409.00103)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To address this gap, our study introduces the concept of causal epistemic consistency, which focuses on the self-consistency of Large Language Models (LLMs) in differentiating intermediates with nuanced differences in causal reasoning. We propose a suite of novel metrics -- intensity ranking concordance, cross-group position agreement, and intra-group clustering -- to evaluate LLMs on this front. Through extensive empirical studies on 21 high-profile LLMs, including GPT-4, Claude3, and LLaMA3-70B, we have favoring evidence that current models struggle to maintain epistemic consistency in identifying the polarity and intensity of intermediates in causal reasoning. Additionally, we explore the potential of using internal token probabilities as an auxiliary tool to maintain causal epistemic consistency. In summary, our study bridges a critical gap in AI research by investigating the self-consistency over fine-grained intermediates involved in causal reasoning.</li>
</ul>

<h3>Title: Negation Blindness in Large Language Models: Unveiling the NO Syndrome in Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Nadeem, Shahab Saquib Sohail, Erik Cambria, Bj√∂rn W. Schuller, Amir Hussain</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00105">https://arxiv.org/abs/2409.00105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00105">https://arxiv.org/pdf/2409.00105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00105]] Negation Blindness in Large Language Models: Unveiling the NO Syndrome in Image Generation(https://arxiv.org/abs/2409.00105)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Foundational Large Language Models (LLMs) have changed the way we perceive technology. They have been shown to excel in tasks ranging from poem writing and coding to essay generation and puzzle solving. With the incorporation of image generation capability, they have become more comprehensive and versatile AI tools. At the same time, researchers are striving to identify the limitations of these tools to improve them further. Currently identified flaws include hallucination, biases, and bypassing restricted commands to generate harmful content. In the present work, we have identified a fundamental limitation related to the image generation ability of LLMs, and termed it The NO Syndrome. This negation blindness refers to LLMs inability to correctly comprehend NO related natural language prompts to generate the desired images. Interestingly, all tested LLMs including GPT-4, Gemini, and Copilot were found to be suffering from this syndrome. To demonstrate the generalization of this limitation, we carried out simulation experiments and conducted entropy-based and benchmark statistical analysis tests on various LLMs in multiple languages, including English, Hindi, and French. We conclude that the NO syndrome is a significant flaw in current LLMs that needs to be addressed. A related finding of this study showed a consistent discrepancy between image and textual responses as a result of this NO syndrome. We posit that the introduction of a negation context-aware reinforcement learning based feedback loop between the LLMs textual response and generated image could help ensure the generated text is based on both the LLMs correct contextual understanding of the negation query and the generated visual output.</li>
</ul>

<h3>Title: Zero-Shot Visual Reasoning by Vision-Language Models: Benchmarking and Analysis</h3>
<ul>
<li><strong>Authors: </strong>Aishik Nagar, Shantanu Jaiswal, Cheston Tan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00106">https://arxiv.org/abs/2409.00106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00106">https://arxiv.org/pdf/2409.00106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00106]] Zero-Shot Visual Reasoning by Vision-Language Models: Benchmarking and Analysis(https://arxiv.org/abs/2409.00106)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Vision-language models (VLMs) have shown impressive zero- and few-shot performance on real-world visual question answering (VQA) benchmarks, alluding to their capabilities as visual reasoning engines. However, the benchmarks being used conflate "pure" visual reasoning with world knowledge, and also have questions that involve a limited number of reasoning steps. Thus, it remains unclear whether a VLM's apparent visual reasoning performance is due to its world knowledge, or due to actual visual reasoning capabilities. To clarify this ambiguity, we systematically benchmark and dissect the zero-shot visual reasoning capabilities of VLMs through synthetic datasets that require minimal world knowledge, and allow for analysis over a broad range of reasoning steps. We focus on two novel aspects of zero-shot visual reasoning: i) evaluating the impact of conveying scene information as either visual embeddings or purely textual scene descriptions to the underlying large language model (LLM) of the VLM, and ii) comparing the effectiveness of chain-of-thought prompting to standard prompting for zero-shot visual reasoning. We find that the underlying LLMs, when provided textual scene descriptions, consistently perform better compared to being provided visual embeddings. In particular, 18% higher accuracy is achieved on the PTR dataset. We also find that CoT prompting performs marginally better than standard prompting only for the comparatively large GPT-3.5-Turbo (175B) model, and does worse for smaller-scale models. This suggests the emergence of CoT abilities for visual reasoning in LLMs at larger scales even when world knowledge is limited. Overall, we find limitations in the abilities of VLMs and LLMs for more complex visual reasoning, and highlight the important role that LLMs can play in visual reasoning.</li>
</ul>

<h3>Title: Toward Large Language Models as a Therapeutic Tool: Comparing Prompting Techniques to Improve GPT-Delivered Problem-Solving Therapy</h3>
<ul>
<li><strong>Authors: </strong>Daniil Filienko, Yinzhou Wang, Caroline El Jazmi, Serena Xie, Trevor Cohen, Martine De Cock, Weichao Yuwen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.ET, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00112">https://arxiv.org/abs/2409.00112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00112">https://arxiv.org/pdf/2409.00112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00112]] Toward Large Language Models as a Therapeutic Tool: Comparing Prompting Techniques to Improve GPT-Delivered Problem-Solving Therapy(https://arxiv.org/abs/2409.00112)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) are being quickly adapted to many domains, including healthcare, their strengths and pitfalls remain under-explored. In our study, we examine the effects of prompt engineering to guide Large Language Models (LLMs) in delivering parts of a Problem-Solving Therapy (PST) session via text, particularly during the symptom identification and assessment phase for personalized goal setting. We present evaluation results of the models' performances by automatic metrics and experienced medical professionals. We demonstrate that the models' capability to deliver protocolized therapy can be improved with the proper use of prompt engineering methods, albeit with limitations. To our knowledge, this study is among the first to assess the effects of various prompting techniques in enhancing a generalist model's ability to deliver psychotherapy, focusing on overall quality, consistency, and empathy. Exploring LLMs' potential in delivering psychotherapy holds promise with the current shortage of mental health professionals amid significant needs, enhancing the potential utility of AI-based and AI-enhanced care services.</li>
</ul>

<h3>Title: When All Options Are Wrong: Evaluating Large Language Model Robustness with Incorrect Multiple-Choice Options</h3>
<ul>
<li><strong>Authors: </strong>Gracjan G√≥ral, Emilia Wi≈õnios</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00113">https://arxiv.org/abs/2409.00113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00113">https://arxiv.org/pdf/2409.00113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00113]] When All Options Are Wrong: Evaluating Large Language Model Robustness with Incorrect Multiple-Choice Options(https://arxiv.org/abs/2409.00113)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper examines the zero-shot ability of Large Language Models (LLMs) to detect multiple-choice questions with no correct answer, a crucial aspect of educational assessment quality. We explore this ability not only as a measure of subject matter knowledge but also as an indicator of critical thinking within LLMs. Our experiments, utilizing a range of LLMs on diverse questions, highlight the significant performance gap between questions with a single correct answer and those without. Llama-3.1-405B stands out by successfully identifying the lack of a valid answer in many instances. These findings suggest that LLMs should prioritize critical thinking over blind instruction following and caution against their use in educational settings where questions with incorrect answers might lead to inaccurate evaluations. This research sets a benchmark for assessing critical thinking in LLMs and emphasizes the need for ongoing model alignment to ensure genuine user comprehension and assistance.</li>
</ul>

<h3>Title: FedMCP: Parameter-Efficient Federated Learning with Model-Contrastive Personalization</h3>
<ul>
<li><strong>Authors: </strong>Qianyi Zhao, Chen Qu, Cen Chen, Mingyuan Fan, Yanhao Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00116">https://arxiv.org/abs/2409.00116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00116">https://arxiv.org/pdf/2409.00116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00116]] FedMCP: Parameter-Efficient Federated Learning with Model-Contrastive Personalization(https://arxiv.org/abs/2409.00116)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>With increasing concerns and regulations on data privacy, fine-tuning pretrained language models (PLMs) in federated learning (FL) has become a common paradigm for NLP tasks. Despite being extensively studied, the existing methods for this problem still face two primary challenges. First, the huge number of parameters in large-scale PLMs leads to excessive communication and computational overhead. Second, the heterogeneity of data and tasks across clients poses a significant obstacle to achieving the desired fine-tuning performance. To address the above problems, we propose FedMCP, a novel parameter-efficient fine-tuning method with model-contrastive personalization for FL. Specifically, FedMCP adds two lightweight adapter modules, i.e., the global adapter and the private adapter, to the frozen PLMs within clients. In a communication round, each client sends only the global adapter to the server for federated aggregation. Furthermore, FedMCP introduces a model-contrastive regularization term between the two adapters. This, on the one hand, encourages the global adapter to assimilate universal knowledge and, on the other hand, the private adapter to capture client-specific knowledge. By leveraging both adapters, FedMCP can effectively provide fine-tuned personalized models tailored to individual clients. Extensive experiments on highly heterogeneous cross-task, cross-silo datasets show that FedMCP achieves substantial performance improvements over state-of-the-art FL fine-tuning approaches for PLMs.</li>
</ul>

<h3>Title: 3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient Batching and Composability</h3>
<ul>
<li><strong>Authors: </strong>Baohao Liao, Christof Monz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00119">https://arxiv.org/abs/2409.00119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00119">https://arxiv.org/pdf/2409.00119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00119]] 3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient Batching and Composability(https://arxiv.org/abs/2409.00119)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Parameter-efficient finetuning (PEFT) methods effectively adapt large language models (LLMs) to diverse downstream tasks, reducing storage and GPU memory demands. Despite these advantages, several applications pose new challenges to PEFT beyond mere parameter efficiency. One notable challenge involves the efficient deployment of LLMs equipped with multiple task- or user-specific adapters, particularly when different adapters are needed for distinct requests within the same batch. Another challenge is the interpretability of LLMs, which is crucial for understanding how LLMs function. Previous studies introduced various approaches to address different challenges. In this paper, we introduce a novel method, RoAd, which employs a straightforward 2D rotation to adapt LLMs and addresses all the above challenges: (1) RoAd is remarkably parameter-efficient, delivering optimal performance on GLUE, eight commonsense reasoning tasks and four arithmetic reasoning tasks with $<0.1\%$ trainable parameters; (2) RoAd facilitates the efficient serving of requests requiring different adapters within a batch, with an overhead comparable to element-wise multiplication instead of batch matrix multiplication; (3) RoAd enhances LLM's interpretability through integration within a framework of distributed interchange intervention, demonstrated via composition experiments.</li>
</ul>

<h3>Title: A Hybrid Framework for Spatial Interpolation: Merging Data-driven with Domain Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Cong Zhang, Shuyi Du, Hongqing Song, Yuhe Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00125">https://arxiv.org/abs/2409.00125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00125">https://arxiv.org/pdf/2409.00125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00125]] A Hybrid Framework for Spatial Interpolation: Merging Data-driven with Domain Knowledge(https://arxiv.org/abs/2409.00125)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Estimating spatially distributed information through the interpolation of scattered observation datasets often overlooks the critical role of domain knowledge in understanding spatial dependencies. Additionally, the features of these data sets are typically limited to the spatial coordinates of the scattered observation locations. In this paper, we propose a hybrid framework that integrates data-driven spatial dependency feature extraction with rule-assisted spatial dependency function mapping to augment domain knowledge. We demonstrate the superior performance of our framework in two comparative application scenarios, highlighting its ability to capture more localized spatial features in the reconstructed distribution fields. Furthermore, we underscore its potential to enhance nonlinear estimation capabilities through the application of transformed fuzzy rules and to quantify the inherent uncertainties associated with the observation data sets. Our framework introduces an innovative approach to spatial information estimation by synergistically combining observational data with rule-assisted domain knowledge.</li>
</ul>

<h3>Title: Can AI Replace Human Subjects? A Large-Scale Replication of Psychological Experiments with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ziyan Cui, Ning Li, Huaikang Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00128">https://arxiv.org/abs/2409.00128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00128">https://arxiv.org/pdf/2409.00128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00128]] Can AI Replace Human Subjects? A Large-Scale Replication of Psychological Experiments with LLMs(https://arxiv.org/abs/2409.00128)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence (AI) is increasingly being integrated into scientific research, particularly in the social sciences, where understanding human behavior is critical. Large Language Models (LLMs) like GPT-4 have shown promise in replicating human-like responses in various psychological experiments. However, the extent to which LLMs can effectively replace human subjects across diverse experimental contexts remains unclear. Here, we conduct a large-scale study replicating 154 psychological experiments from top social science journals with 618 main effects and 138 interaction effects using GPT-4 as a simulated participant. We find that GPT-4 successfully replicates 76.0 percent of main effects and 47.0 percent of interaction effects observed in the original studies, closely mirroring human responses in both direction and significance. However, only 19.44 percent of GPT-4's replicated confidence intervals contain the original effect sizes, with the majority of replicated effect sizes exceeding the 95 percent confidence interval of the original studies. Additionally, there is a 71.6 percent rate of unexpected significant results where the original studies reported null findings, suggesting potential overestimation or false positives. Our results demonstrate the potential of LLMs as powerful tools in psychological research but also emphasize the need for caution in interpreting AI-driven findings. While LLMs can complement human studies, they cannot yet fully replace the nuanced insights provided by human subjects.</li>
</ul>

<h3>Title: Logic Contrastive Reasoning with Lightweight Large Language Model for Math Word Problems</h3>
<ul>
<li><strong>Authors: </strong>Ding Kai, Ma Zhenguo, Yan Xiaoran</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00131">https://arxiv.org/abs/2409.00131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00131">https://arxiv.org/pdf/2409.00131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00131]] Logic Contrastive Reasoning with Lightweight Large Language Model for Math Word Problems(https://arxiv.org/abs/2409.00131)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study focuses on improving the performance of lightweight Large Language Models (LLMs) in mathematical reasoning tasks. We introduce a novel method for measuring mathematical logic similarity and design an automatic screening mechanism to construct a set of reference problems that integrate both semantic and logical similarity. By employing carefully crafted positive and negative example prompts, we guide the model towards adopting sound reasoning logic. To the best of our knowledge, this is the first attempt to utilize retrieval-enhanced generation for mathematical problem-solving. Experimental results demonstrate that our method achieves a 15.8% improvement over the Chain of Thought approach on the SVAMP dataset and a 21.5 % improvement on the GSM8K dataset. Further application of this method to a large-scale model with 175 billion parameters yields performance comparable to the best results on both aforementioned datasets. Finally, we conduct an analysis of errors during the reasoning process, providing valuable insights and directions for future research on reasoning tasks using large language models.</li>
</ul>

<h3>Title: A Survey for Large Language Models in Biomedicine</h3>
<ul>
<li><strong>Authors: </strong>Chong Wang, Mengyao Li, Junjun He, Zhongruo Wang, Erfan Darzi, Zan Chen, Jin Ye, Tianbin Li, Yanzhou Su, Jing Ke, Kaili Qu, Shuxin Li, Yi Yu, Pietro Li√≤, Tianyun Wang, Yu Guang Wang, Yiqing Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00133">https://arxiv.org/abs/2409.00133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00133">https://arxiv.org/pdf/2409.00133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00133]] A Survey for Large Language Models in Biomedicine(https://arxiv.org/abs/2409.00133)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Recent breakthroughs in large language models (LLMs) offer unprecedented natural language understanding and generation capabilities. However, existing surveys on LLMs in biomedicine often focus on specific applications or model architectures, lacking a comprehensive analysis that integrates the latest advancements across various biomedical domains. This review, based on an analysis of 484 publications sourced from databases including PubMed, Web of Science, and arXiv, provides an in-depth examination of the current landscape, applications, challenges, and prospects of LLMs in biomedicine, distinguishing itself by focusing on the practical implications of these models in real-world biomedical contexts. Firstly, we explore the capabilities of LLMs in zero-shot learning across a broad spectrum of biomedical tasks, including diagnostic assistance, drug discovery, and personalized medicine, among others, with insights drawn from 137 key studies. Then, we discuss adaptation strategies of LLMs, including fine-tuning methods for both uni-modal and multi-modal LLMs to enhance their performance in specialized biomedical contexts where zero-shot fails to achieve, such as medical question answering and efficient processing of biomedical literature. Finally, we discuss the challenges that LLMs face in the biomedicine domain including data privacy concerns, limited model interpretability, issues with dataset quality, and ethics due to the sensitive nature of biomedical data, the need for highly reliable model outputs, and the ethical implications of deploying AI in healthcare. To address these challenges, we also identify future research directions of LLM in biomedicine including federated learning methods to preserve data privacy and integrating explainable AI methodologies to enhance the transparency of LLMs.</li>
</ul>

<h3>Title: HoneyComb: A Flexible LLM-Based Agent System for Materials Science</h3>
<ul>
<li><strong>Authors: </strong>Huan Zhang, Yu Song, Ziyu Hou, Santiago Miret, Bang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00135">https://arxiv.org/abs/2409.00135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00135">https://arxiv.org/pdf/2409.00135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00135]] HoneyComb: A Flexible LLM-Based Agent System for Materials Science(https://arxiv.org/abs/2409.00135)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The emergence of specialized large language models (LLMs) has shown promise in addressing complex tasks for materials science. Many LLMs, however, often struggle with distinct complexities of material science tasks, such as materials science computational tasks, and often rely heavily on outdated implicit knowledge, leading to inaccuracies and hallucinations. To address these challenges, we introduce HoneyComb, the first LLM-based agent system specifically designed for materials science. HoneyComb leverages a novel, high-quality materials science knowledge base (MatSciKB) and a sophisticated tool hub (ToolHub) to enhance its reasoning and computational capabilities tailored to materials science. MatSciKB is a curated, structured knowledge collection based on reliable literature, while ToolHub employs an Inductive Tool Construction method to generate, decompose, and refine API tools for materials science. Additionally, HoneyComb leverages a retriever module that adaptively selects the appropriate knowledge source or tools for specific tasks, thereby ensuring accuracy and relevance. Our results demonstrate that HoneyComb significantly outperforms baseline models across various tasks in materials science, effectively bridging the gap between current LLM capabilities and the specialized needs of this domain. Furthermore, our adaptable framework can be easily extended to other scientific domains, highlighting its potential for broad applicability in advancing scientific research and applications.</li>
</ul>

<h3>Title: Emerging Vulnerabilities in Frontier Models: Multi-Turn Jailbreak Attacks</h3>
<ul>
<li><strong>Authors: </strong>Tom Gibbs, Ethan Kosak-Hine, George Ingebretsen, Jason Zhang, Julius Broomfield, Sara Pieri, Reihaneh Iranmanesh, Reihaneh Rabbany, Kellin Pelrine</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00137">https://arxiv.org/abs/2409.00137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00137">https://arxiv.org/pdf/2409.00137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00137]] Emerging Vulnerabilities in Frontier Models: Multi-Turn Jailbreak Attacks(https://arxiv.org/abs/2409.00137)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are improving at an exceptional rate. However, these models are still susceptible to jailbreak attacks, which are becoming increasingly dangerous as models become increasingly powerful. In this work, we introduce a dataset of jailbreaks where each example can be input in both a single or a multi-turn format. We show that while equivalent in content, they are not equivalent in jailbreak success: defending against one structure does not guarantee defense against the other. Similarly, LLM-based filter guardrails also perform differently depending on not just the input content but the input structure. Thus, vulnerabilities of frontier models should be studied in both single and multi-turn settings; this dataset provides a tool to do so.</li>
</ul>

<h3>Title: PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action</h3>
<ul>
<li><strong>Authors: </strong>Yijia Shao, Tianshi Li, Weiyan Shi, Yanchen Liu, Diyi Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00138">https://arxiv.org/abs/2409.00138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00138">https://arxiv.org/pdf/2409.00138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00138]] PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action(https://arxiv.org/abs/2409.00138)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>As language models (LMs) are widely utilized in personalized communication scenarios (e.g., sending emails, writing social media posts) and endowed with a certain level of agency, ensuring they act in accordance with the contextual privacy norms becomes increasingly critical. However, quantifying the privacy norm awareness of LMs and the emerging privacy risk in LM-mediated communication is challenging due to (1) the contextual and long-tailed nature of privacy-sensitive cases, and (2) the lack of evaluation approaches that capture realistic application scenarios. To address these challenges, we propose PrivacyLens, a novel framework designed to extend privacy-sensitive seeds into expressive vignettes and further into agent trajectories, enabling multi-level evaluation of privacy leakage in LM agents' actions. We instantiate PrivacyLens with a collection of privacy norms grounded in privacy literature and crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM performance in answering probing questions and their actual behavior when executing user instructions in an agent setup. State-of-the-art LMs, like GPT-4 and Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even when prompted with privacy-enhancing instructions. We also demonstrate the dynamic nature of PrivacyLens by extending each seed into multiple trajectories to red-team LM privacy leakage risk. Dataset and code are available at this https URL.</li>
</ul>

<h3>Title: Dynamic Depth Decoding: Faster Speculative Decoding for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Oscar Brown, Zhengjie Wang, Andrea Do, Nikhil Mathew, Cheng Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00142">https://arxiv.org/abs/2409.00142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00142">https://arxiv.org/pdf/2409.00142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00142]] Dynamic Depth Decoding: Faster Speculative Decoding for LLMs(https://arxiv.org/abs/2409.00142)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The acceleration of Large Language Models (LLMs) with speculative decoding provides a significant runtime improvement without any loss of accuracy. Currently, EAGLE-2 is the state-of-the-art speculative decoding method, improving on EAGLE with a dynamic draft tree. We introduce Dynamic Depth Decoding (DDD), which optimises EAGLE-2's tree drafting method using a dynamic depth. This extends the average speedup that EAGLE-2 achieves over EAGLE by $44\%$, giving DDD an average speedup of $3.16$x.</li>
</ul>

<h3>Title: Robust Temporal-Invariant Learning in Multimodal Disentanglement</h3>
<ul>
<li><strong>Authors: </strong>Guoyang Xu, Junqi Xue, Zhenxi Song, Yuxin Liu, Zirui Wang, Min Zhang, Zhiguo Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00143">https://arxiv.org/abs/2409.00143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00143">https://arxiv.org/pdf/2409.00143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00143]] Robust Temporal-Invariant Learning in Multimodal Disentanglement(https://arxiv.org/abs/2409.00143)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal sentiment recognition aims to learn representations from different modalities to identify human emotions. However, previous works does not suppresses the frame-level redundancy inherent in continuous time series, resulting in incomplete modality representations with noise. To address this issue, we propose the Temporal-invariant learning, which minimizes the distributional differences between time steps to effectively capture smoother time series patterns, thereby enhancing the quality of the representations and robustness of the model. To fully exploit the rich semantic information in textual knowledge, we propose a Text-Driven Fusion Module (TDFM). To guide cross-modal interactions, TDFM evaluates the correlations between different modality through modality-invariant representations. Furthermore, we introduce a modality discriminator to disentangle modality-invariant and modality-specific subspaces. Experimental results on two public datasets demonstrate the superiority of our model.</li>
</ul>

<h3>Title: MultiMath: Bridging Visual and Mathematical Reasoning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shuai Peng, Di Fu, Liangcai Gao, Xiuqin Zhong, Hongguang Fu, Zhi Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00147">https://arxiv.org/abs/2409.00147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00147">https://arxiv.org/pdf/2409.00147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00147]] MultiMath: Bridging Visual and Mathematical Reasoning for Large Language Models(https://arxiv.org/abs/2409.00147)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid development of large language models (LLMs) has spurred extensive research into their domain-specific capabilities, particularly mathematical reasoning. However, most open-source LLMs focus solely on mathematical reasoning, neglecting the integration with visual injection, despite the fact that many mathematical tasks rely on visual inputs such as geometric diagrams, charts, and function plots. To fill this gap, we introduce \textbf{MultiMath-7B}, a multimodal large language model that bridges the gap between math and vision. \textbf{MultiMath-7B} is trained through a four-stage process, focusing on vision-language alignment, visual and math instruction-tuning, and process-supervised reinforcement learning. We also construct a novel, diverse and comprehensive multimodal mathematical dataset, \textbf{MultiMath-300K}, which spans K-12 levels with image captions and step-wise solutions. MultiMath-7B achieves state-of-the-art (SOTA) performance among open-source models on existing multimodal mathematical benchmarks and also excels on text-only mathematical benchmarks. Our model and dataset are available at {\textcolor{blue}{\url{this https URL}}}.</li>
</ul>

<h3>Title: Speaker Tagging Correction With Non-Autoregressive Language Models</h3>
<ul>
<li><strong>Authors: </strong>Grigor Kirakosyan, Davit Karamyan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00151">https://arxiv.org/abs/2409.00151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00151">https://arxiv.org/pdf/2409.00151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00151]] Speaker Tagging Correction With Non-Autoregressive Language Models(https://arxiv.org/abs/2409.00151)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Speech applications dealing with conversations require not only recognizing the spoken words but also determining who spoke when. The task of assigning words to speakers is typically addressed by merging the outputs of two separate systems, namely, an automatic speech recognition (ASR) system and a speaker diarization (SD) system. In practical settings, speaker diarization systems can experience significant degradation in performance due to a variety of factors, including uniform segmentation with a high temporal resolution, inaccurate word timestamps, incorrect clustering and estimation of speaker numbers, as well as background noise. Therefore, it is important to automatically detect errors and make corrections if possible. We used a second-pass speaker tagging correction system based on a non-autoregressive language model to correct mistakes in words placed at the borders of sentences spoken by different speakers. We first show that the employed error correction approach leads to reductions in word diarization error rate (WDER) on two datasets: TAL and test set of Fisher. Additionally, we evaluated our system in the Post-ASR Speaker Tagging Correction challenge and observed significant improvements in cpWER compared to baseline methods.</li>
</ul>

<h3>Title: Common Steps in Machine Learning Might Hinder The Explainability Aims in Medicine</h3>
<ul>
<li><strong>Authors: </strong>Ahmed M Salih</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00155">https://arxiv.org/abs/2409.00155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00155">https://arxiv.org/pdf/2409.00155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00155]] Common Steps in Machine Learning Might Hinder The Explainability Aims in Medicine(https://arxiv.org/abs/2409.00155)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Data pre-processing is a significant step in machine learning to improve the performance of the model and decreases the running time. This might include dealing with missing values, outliers detection and removing, data augmentation, dimensionality reduction, data normalization and handling the impact of confounding variables. Although it is found the steps improve the accuracy of the model, but they might hinder the explainability of the model if they are not carefully considered especially in medicine. They might block new findings when missing values and outliers removal are implemented inappropriately. In addition, they might make the model unfair against all the groups in the model when making the decision. Moreover, they turn the features into unitless and clinically meaningless and consequently not explainable. This paper discusses the common steps of the data preprocessing in machine learning and their impacts on the explainability and interpretability of the model. Finally, the paper discusses some possible solutions that improve the performance of the model while not decreasing its explainability.</li>
</ul>

<h3>Title: Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Zhou, Jiaming Ji, Juntao Dai, Yaodong Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00162">https://arxiv.org/abs/2409.00162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00162">https://arxiv.org/pdf/2409.00162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00162]] Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback(https://arxiv.org/abs/2409.00162)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Aligning the behavior of Large language models (LLMs) with human intentions and values remains a critical challenge. Reinforcement learning from human feedback (RLHF) aligns LLMs by training a reward model (RM) on human preferences and fine-tuning the LLMs to maximize RM feedback. Despite its effectiveness and popularity, RLHF is prone to biased local optimization. It means RM fails to provide feedback that accurately aligns with human preference, causing LLMs to explore unexpected generalizations, and failing to achieve alignment objectives. To mitigate this issue, we propose a novel \textit{sequence-to-sequence (seq2seq) reward modeling} method. Its key insight is that learning from language feedback rather than scalar feedback improves RLHF without additional annotations. We replaced the reward modeling target from binary maximum likelihood estimation (MLE) with sequence MLE. This method enables richer and fine-grained language feedback without additional annotations, models, or training stages. Our experiments demonstrated its effectiveness, specifically, reducing the refusal-to-response paradigm in single-turn safety dialogues and the long-response bias in text summarization tasks. We provide further analysis that seq2seq RM improves RLHF performance across 2B and 7B LLMs on 3 NLP tasks, achieving an average win rate of 76.9\%. We further show that seq2seq RM can still improve the performance of RLHF under out-of-distribution prompts.</li>
</ul>

<h3>Title: A Generative Adversarial Network-based Method for LiDAR-Assisted Radar Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Thakshila Thilakanayake, Oscar De Silva, Thumeera R. Wanasinghe, George K. Mann, Awantha Jayasiri</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00196">https://arxiv.org/abs/2409.00196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00196">https://arxiv.org/pdf/2409.00196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00196]] A Generative Adversarial Network-based Method for LiDAR-Assisted Radar Image Enhancement(https://arxiv.org/abs/2409.00196)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>This paper presents a generative adversarial network (GAN) based approach for radar image enhancement. Although radar sensors remain robust for operations under adverse weather conditions, their application in autonomous vehicles (AVs) is commonly limited by the low-resolution data they produce. The primary goal of this study is to enhance the radar images to better depict the details and features of the environment, thereby facilitating more accurate object identification in AVs. The proposed method utilizes high-resolution, two-dimensional (2D) projected light detection and ranging (LiDAR) point clouds as ground truth images and low-resolution radar images as inputs to train the GAN. The ground truth images were obtained through two main steps. First, a LiDAR point cloud map was generated by accumulating raw LiDAR scans. Then, a customized LiDAR point cloud cropping and projection method was employed to obtain 2D projected LiDAR point clouds. The inference process of the proposed method relies solely on radar images to generate an enhanced version of them. The effectiveness of the proposed method is demonstrated through both qualitative and quantitative results. These results show that the proposed method can generate enhanced images with clearer object representation compared to the input radar images, even under adverse weather conditions.</li>
</ul>

<h3>Title: The creative psychometric item generator: a framework for item generation and validation using large language models</h3>
<ul>
<li><strong>Authors: </strong>Antonio Laverghetta Jr., Simone Luchini, Averie Linell, Roni Reiter-Palmon, Roger Beaty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00202">https://arxiv.org/abs/2409.00202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00202">https://arxiv.org/pdf/2409.00202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00202]] The creative psychometric item generator: a framework for item generation and validation using large language models(https://arxiv.org/abs/2409.00202)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Increasingly, large language models (LLMs) are being used to automate workplace processes requiring a high degree of creativity. While much prior work has examined the creativity of LLMs, there has been little research on whether they can generate valid creativity assessments for humans despite the increasingly central role of creativity in modern economies. We develop a psychometrically inspired framework for creating test items (questions) for a classic free-response creativity test: the creative problem-solving (CPS) task. Our framework, the creative psychometric item generator (CPIG), uses a mixture of LLM-based item generators and evaluators to iteratively develop new prompts for writing CPS items, such that items from later iterations will elicit more creative responses from test takers. We find strong empirical evidence that CPIG generates valid and reliable items and that this effect is not attributable to known biases in the evaluation process. Our findings have implications for employing LLMs to automatically generate valid and reliable creativity tests for humans and AI.</li>
</ul>

<h3>Title: Enhancing Event Reasoning in Large Language Models through Instruction Fine-Tuning with Semantic Causal Graphs</h3>
<ul>
<li><strong>Authors: </strong>Mazal Bethany, Emet Bethany, Brandon Wherry, Cho-Yu Chiang, Nishant Vishwamitra, Anthony Rios, Peyman Najafirad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00209">https://arxiv.org/abs/2409.00209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00209">https://arxiv.org/pdf/2409.00209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00209]] Enhancing Event Reasoning in Large Language Models through Instruction Fine-Tuning with Semantic Causal Graphs(https://arxiv.org/abs/2409.00209)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Event detection and text reasoning have become critical applications across various domains. While LLMs have recently demonstrated impressive progress in reasoning abilities, they often struggle with event detection, particularly due to the absence of training methods that consider causal relationships between event triggers and types. To address this challenge, we propose a novel approach for instruction fine-tuning LLMs for event detection. Our method introduces Semantic Causal Graphs (SCGs) to capture both causal relationships and contextual information within text. Building off of SCGs, we propose SCG Instructions for fine-tuning LLMs by focusing on event triggers and their relationships to event types, and employ Low-Rank Adaptation (LoRA) to help preserve the general reasoning abilities of LLMs. Our evaluations demonstrate that training LLMs with SCG Instructions outperforms standard instruction fine-tuning by an average of 35.69\% on Event Trigger Classification. Notably, our fine-tuned Mistral 7B model also outperforms GPT-4 on key event detection metrics by an average of 31.01\% on Event Trigger Identification, 37.40\% on Event Trigger Classification, and 16.43\% on Event Classification. We analyze the retention of general capabilities, observing only a minimal average drop of 2.03 points across six benchmarks. This comprehensive study investigates multiple LLMs for the event detection task across various datasets, prompting strategies, and training approaches.</li>
</ul>

<h3>Title: Enhancing Document-level Argument Extraction with Definition-augmented Heuristic-driven Prompting for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tongyue Sun, Jiayi Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00214">https://arxiv.org/abs/2409.00214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00214">https://arxiv.org/pdf/2409.00214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00214]] Enhancing Document-level Argument Extraction with Definition-augmented Heuristic-driven Prompting for LLMs(https://arxiv.org/abs/2409.00214)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Event Argument Extraction (EAE) is pivotal for extracting structured information from unstructured text, yet it remains challenging due to the complexity of real-world document-level EAE. We propose a novel Definition-augmented Heuristic-driven Prompting (DHP) method to enhance the performance of Large Language Models (LLMs) in document-level EAE. Our method integrates argument extraction-related definitions and heuristic rules to guide the extraction process, reducing error propagation and improving task accuracy. We also employ the Chain-of-Thought (CoT) method to simulate human reasoning, breaking down complex problems into manageable sub-problems. Experiments have shown that our method achieves a certain improvement in performance over existing prompting methods and few-shot supervised learning on document-level EAE datasets. The DHP method enhances the generalization capability of LLMs and reduces reliance on large annotated datasets, offering a novel research perspective for document-level EAE.</li>
</ul>

<h3>Title: ProGRes: Prompted Generative Rescoring on ASR n-Best</h3>
<ul>
<li><strong>Authors: </strong>Ada Defne Tur, Adel Moumen, Mirco Ravanelli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00217">https://arxiv.org/abs/2409.00217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00217">https://arxiv.org/pdf/2409.00217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00217]] ProGRes: Prompted Generative Rescoring on ASR n-Best(https://arxiv.org/abs/2409.00217)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown their ability to improve the performance of speech recognizers by effectively rescoring the n-best hypotheses generated during the beam search process. However, the best way to exploit recent generative instruction-tuned LLMs for hypothesis rescoring is still unclear. This paper proposes a novel method that uses instruction-tuned LLMs to dynamically expand the n-best speech recognition hypotheses with new hypotheses generated through appropriately-prompted LLMs. Specifically, we introduce a new zero-shot method for ASR n-best rescoring, which combines confidence scores, LLM sequence scoring, and prompt-based hypothesis generation. We compare Llama-3-Instruct, GPT-3.5 Turbo, and GPT-4 Turbo as prompt-based generators with Llama-3 as sequence scorer LLM. We evaluated our approach using different speech recognizers and observed significant relative improvement in the word error rate (WER) ranging from 5% to 25%.</li>
</ul>

<h3>Title: Can Large Language Models Address Open-Target Stance Detection?</h3>
<ul>
<li><strong>Authors: </strong>Abu Ubaida Akash, Ahmed Fahmy, Amine Trabelsi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00222">https://arxiv.org/abs/2409.00222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00222">https://arxiv.org/pdf/2409.00222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00222]] Can Large Language Models Address Open-Target Stance Detection?(https://arxiv.org/abs/2409.00222)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Stance detection (SD) assesses a text's position towards a target, typically labeled as "favor," "against," or "neutral." We introduce Open-Target Stance Detection (OTSD), where targets are neither seen during training nor provided as input. Evaluating Large Language Models (LLMs) like GPT-3.5, Llama 3, and Mistral, we compare their performance with the Target-Stance Extraction (TSE) approach, which has the advantage of using predefined targets. LLMs perform better than TSE in target generation when the real target is explicitly and not explicitly mentioned in the text. For stance detection, LLMs perform better in explicit scenarios but fail in non-explicit ones.</li>
</ul>

<h3>Title: Spatially-Aware Diffusion Models with Cross-Attention for Global Field Reconstruction with Sparse Observations</h3>
<ul>
<li><strong>Authors: </strong>Yilin Zhuang, Sibo Cheng, Karthik Duraisamy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00230">https://arxiv.org/abs/2409.00230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00230">https://arxiv.org/pdf/2409.00230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00230]] Spatially-Aware Diffusion Models with Cross-Attention for Global Field Reconstruction with Sparse Observations(https://arxiv.org/abs/2409.00230)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have gained attention for their ability to represent complex distributions and incorporate uncertainty, making them ideal for robust predictions in the presence of noisy or incomplete data. In this study, we develop and enhance score-based diffusion models in field reconstruction tasks, where the goal is to estimate complete spatial fields from partial observations. We introduce a condition encoding approach to construct a tractable mapping mapping between observed and unobserved regions using a learnable integration of sparse observations and interpolated fields as an inductive bias. With refined sensing representations and an unraveled temporal dimension, our method can handle arbitrary moving sensors and effectively reconstruct fields. Furthermore, we conduct a comprehensive benchmark of our approach against a deterministic interpolation-based method across various static and time-dependent PDEs. Our study attempts to addresses the gap in strong baselines for evaluating performance across varying sampling hyperparameters, noise levels, and conditioning methods. Our results show that diffusion models with cross-attention and the proposed conditional encoding generally outperform other methods under noisy conditions, although the deterministic method excels with noiseless data. Additionally, both the diffusion models and the deterministic method surpass the numerical approach in accuracy and computational cost for the steady problem. We also demonstrate the ability of the model to capture possible reconstructions and improve the accuracy of fused results in covariance-based correction tasks using ensemble sampling.</li>
</ul>

<h3>Title: Self-Supervised Learning for Building Robust Pediatric Chest X-ray Classification Models</h3>
<ul>
<li><strong>Authors: </strong>Sheng Cheng, Zbigniew A. Starosolski, Devika Subramanian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00231">https://arxiv.org/abs/2409.00231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00231">https://arxiv.org/pdf/2409.00231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00231]] Self-Supervised Learning for Building Robust Pediatric Chest X-ray Classification Models(https://arxiv.org/abs/2409.00231)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advancements in deep learning for Medical Artificial Intelligence have demonstrated that models can match the diagnostic performance of clinical experts in adult chest X-ray (CXR) interpretation. However, their application in the pediatric context remains limited due to the scarcity of large annotated pediatric image datasets. Additionally, significant challenges arise from the substantial variability in pediatric CXR images across different hospitals and the diverse age range of patients from 0 to 18 years. To address these challenges, we propose SCC, a novel approach that combines transfer learning with self-supervised contrastive learning, augmented by an unsupervised contrast enhancement technique. Transfer learning from a well-trained adult CXR model mitigates issues related to the scarcity of pediatric training data. Contrastive learning with contrast enhancement focuses on the lungs, reducing the impact of image variations and producing high-quality embeddings across diverse pediatric CXR images. We train SCC on one pediatric CXR dataset and evaluate its performance on two other pediatric datasets from different sources. Our results show that SCC's out-of-distribution (zero-shot) performance exceeds regular transfer learning in terms of AUC by 13.6% and 34.6% on the two test datasets. Moreover, with few-shot learning using 10 times fewer labeled images, SCC matches the performance of regular transfer learning trained on the entire labeled dataset. To test the generality of the framework, we verify its performance on three benchmark breast cancer datasets. Starting from a model trained on natural images and fine-tuned on one breast dataset, SCC outperforms the fully supervised learning baseline on the other two datasets in terms of AUC by 3.6% and 5.5% in zero-shot learning.</li>
</ul>

<h3>Title: Building Better Datasets: Seven Recommendations for Responsible Design from Dataset Creators</h3>
<ul>
<li><strong>Authors: </strong>Will Orr, Kate Crawford</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00252">https://arxiv.org/abs/2409.00252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00252">https://arxiv.org/pdf/2409.00252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00252]] Building Better Datasets: Seven Recommendations for Responsible Design from Dataset Creators(https://arxiv.org/abs/2409.00252)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The increasing demand for high-quality datasets in machine learning has raised concerns about the ethical and responsible creation of these datasets. Dataset creators play a crucial role in developing responsible practices, yet their perspectives and expertise have not yet been highlighted in the current literature. In this paper, we bridge this gap by presenting insights from a qualitative study that included interviewing 18 leading dataset creators about the current state of the field. We shed light on the challenges and considerations faced by dataset creators, and our findings underscore the potential for deeper collaboration, knowledge sharing, and collective development. Through a close analysis of their perspectives, we share seven central recommendations for improving responsible dataset creation, including issues such as data quality, documentation, privacy and consent, and how to mitigate potential harms from unintended use cases. By fostering critical reflection and sharing the experiences of dataset creators, we aim to promote responsible dataset creation practices and develop a nuanced understanding of this crucial but often undervalued aspect of machine learning research.</li>
</ul>

<h3>Title: DiverseDialogue: A Methodology for Designing Chatbots with Human-Like Diversity</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Lin, Xinkai Yu, Ankit Aich, Salvatore Giorgi, Lyle Ungar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00262">https://arxiv.org/abs/2409.00262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00262">https://arxiv.org/pdf/2409.00262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00262]] DiverseDialogue: A Methodology for Designing Chatbots with Human-Like Diversity(https://arxiv.org/abs/2409.00262)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), which simulate human users, are frequently employed to evaluate chatbots in applications such as tutoring and customer service. Effective evaluation necessitates a high degree of human-like diversity within these simulations. In this paper, we demonstrate that conversations generated by GPT-4o mini, when used as simulated human participants, systematically differ from those between actual humans across multiple linguistic features. These features include topic variation, lexical attributes, and both the average behavior and diversity (variance) of the language used. To address these discrepancies, we propose an approach that automatically generates prompts for user simulations by incorporating features derived from real human interactions, such as age, gender, emotional tone, and the topics discussed. We assess our approach using differential language analysis combined with deep linguistic inquiry. Our method of prompt optimization, tailored to target specific linguistic features, shows significant improvements. Specifically, it enhances the human-likeness of LLM chatbot conversations, increasing their linguistic diversity. On average, we observe a 54 percent reduction in the error of average features between human and LLM-generated conversations. This method of constructing chatbot sets with human-like diversity holds great potential for enhancing the evaluation process of user-facing bots.</li>
</ul>

<h3>Title: AWRaCLe: All-Weather Image Restoration using Visual In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Sudarshan Rajagopalan, Vishal M. Patel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00263">https://arxiv.org/abs/2409.00263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00263">https://arxiv.org/pdf/2409.00263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00263]] AWRaCLe: All-Weather Image Restoration using Visual In-Context Learning(https://arxiv.org/abs/2409.00263)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>All-Weather Image Restoration (AWIR) under adverse weather conditions is a challenging task due to the presence of different types of degradations. Prior research in this domain relies on extensive training data but lacks the utilization of additional contextual information for restoration guidance. Consequently, the performance of existing methods is limited by the degradation cues that are learnt from individual training samples. Recent advancements in visual in-context learning have introduced generalist models that are capable of addressing multiple computer vision tasks simultaneously by using the information present in the provided context as a prior. In this paper, we propose All-Weather Image Restoration using Visual In-Context Learning (AWRaCLe), a novel approach for AWIR that innovatively utilizes degradation-specific visual context information to steer the image restoration process. To achieve this, AWRaCLe incorporates Degradation Context Extraction (DCE) and Context Fusion (CF) to seamlessly integrate degradation-specific features from the context into an image restoration network. The proposed DCE and CF blocks leverage CLIP features and incorporate attention mechanisms to adeptly learn and fuse contextual information. These blocks are specifically designed for visual in-context learning under all-weather conditions and are crucial for effective context utilization. Through extensive experiments, we demonstrate the effectiveness of AWRaCLe for all-weather restoration and show that our method advances the state-of-the-art in AWIR.</li>
</ul>

<h3>Title: Leveraging a Cognitive Model to Measure Subjective Similarity of Human and GPT-4 Written Content</h3>
<ul>
<li><strong>Authors: </strong>Tyler Malloy, Maria Jos√© Ferreira, Fei Fang, Cleotilde Gonzalez</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00269">https://arxiv.org/abs/2409.00269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00269">https://arxiv.org/pdf/2409.00269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00269]] Leveraging a Cognitive Model to Measure Subjective Similarity of Human and GPT-4 Written Content(https://arxiv.org/abs/2409.00269)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Cosine similarity between two documents can be computed using token embeddings formed by Large Language Models (LLMs) such as GPT-4, and used to categorize those documents across a range of uses. However, these similarities are ultimately dependent on the corpora used to train these LLMs, and may not reflect subjective similarity of individuals or how their biases and constraints impact similarity metrics. This lack of cognitively-aware personalization of similarity metrics can be particularly problematic in educational and recommendation settings where there is a limited number of individual judgements of category or preference, and biases can be particularly relevant. To address this, we rely on an integration of an Instance-Based Learning (IBL) cognitive model with LLM embeddings to develop the Instance-Based Individualized Similarity (IBIS) metric. This similarity metric is beneficial in that it takes into account individual biases and constraints in a manner that is grounded in the cognitive mechanisms of decision making. To evaluate the IBIS metric, we also introduce a dataset of human categorizations of emails as being either dangerous (phishing) or safe (ham). This dataset is used to demonstrate the benefits of leveraging a cognitive model to measure the subjective similarity of human participants in an educational setting.</li>
</ul>

<h3>Title: Finding frames with BERT: A transformer-based approach to generic news frame detection</h3>
<ul>
<li><strong>Authors: </strong>Vihang Jumle, Mykola Makhortykh, Maryna Sydorova, Victoria Vziatysheva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00272">https://arxiv.org/abs/2409.00272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00272">https://arxiv.org/pdf/2409.00272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00272]] Finding frames with BERT: A transformer-based approach to generic news frame detection(https://arxiv.org/abs/2409.00272)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Framing is among the most extensively used concepts in the field of communication science. The availability of digital data offers new possibilities for studying how specific aspects of social reality are made more salient in online communication but also raises challenges related to the scaling of framing analysis and its adoption to new research areas (e.g. studying the impact of artificial intelligence-powered systems on representation of societally relevant issues). To address these challenges, we introduce a transformer-based approach for generic news frame detection in Anglophone online content. While doing so, we discuss the composition of the training and test datasets, the model architecture, and the validation of the approach and reflect on the possibilities and limitations of the automated detection of generic news frames.</li>
</ul>

<h3>Title: RealFace -- Pedestrian Face Dataset</h3>
<ul>
<li><strong>Authors: </strong>Leonardo Ramos Thomas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00283">https://arxiv.org/abs/2409.00283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00283">https://arxiv.org/pdf/2409.00283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00283]] RealFace -- Pedestrian Face Dataset(https://arxiv.org/abs/2409.00283)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The Real Face Dataset is a pedestrian face detection benchmark dataset in the wild, comprising over 11,000 images and over 55,000 detected faces in various ambient conditions. The dataset aims to provide a comprehensive and diverse collection of real-world face images for the evaluation and development of face detection and recognition algorithms. The Real Face Dataset is a valuable resource for researchers and developers working on face detection and recognition algorithms. With over 11,000 images and 55,000 detected faces, the dataset offers a comprehensive and diverse collection of real-world face images. This diversity is crucial for evaluating the performance of algorithms under various ambient conditions, such as lighting, scale, pose, and occlusion. The dataset's focus on real-world scenarios makes it particularly relevant for practical applications, where faces may be captured in challenging environments. In addition to its size, the dataset's inclusion of images with a high degree of variability in scale, pose, and occlusion, as well as its focus on practical application scenarios, sets it apart as a valuable resource for benchmarking and testing face detection and recognition methods. The challenges presented by the dataset align with the difficulties faced in real-world surveillance applications, where the ability to detect faces and extract discriminative features is paramount. The Real Face Dataset provides an opportunity to assess the performance of face detection and recognition methods on a large scale. Its relevance to real-world scenarios makes it an important resource for researchers and developers aiming to create robust and effective algorithms for practical applications.</li>
</ul>

<h3>Title: Reframing Data Value for Large Language Models Through the Lens of Plausability</h3>
<ul>
<li><strong>Authors: </strong>Mohamad Rida Rammal, Ruida Zhou, Suhas Diggavi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00284">https://arxiv.org/abs/2409.00284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00284">https://arxiv.org/pdf/2409.00284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00284]] Reframing Data Value for Large Language Models Through the Lens of Plausability(https://arxiv.org/abs/2409.00284)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Data valuation seeks to answer the important question, "How much is this data worth?" Existing data valuation methods have largely focused on discriminative models, primarily examining data value through the lens of its utility in training. However, with the push for ever-larger language models, relying on valuation methods that require training becomes increasingly expensive and dependent on specific techniques. We propose an alternative perspective on the data value problem for language models, centering around the plausibility of the data. We posit that data holds lesser value if it can be plausibly generated by the model itself. Starting from some intuitive criteria that align with our notions of valuable data, we develop a novel value function that is computationally tractable and derived from first principles with provable properties. We conduct a theoretical analysis of our value function and evaluate it across multiple scenarios and datasets.</li>
</ul>

<h3>Title: ContextVLM: Zero-Shot and Few-Shot Context Understanding for Autonomous Driving using Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shounak Sural, Naren, Ragunathan Rajkumar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00301">https://arxiv.org/abs/2409.00301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00301">https://arxiv.org/pdf/2409.00301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00301]] ContextVLM: Zero-Shot and Few-Shot Context Understanding for Autonomous Driving using Vision Language Models(https://arxiv.org/abs/2409.00301)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In recent years, there has been a notable increase in the development of autonomous vehicle (AV) technologies aimed at improving safety in transportation systems. While AVs have been deployed in the real-world to some extent, a full-scale deployment requires AVs to robustly navigate through challenges like heavy rain, snow, low lighting, construction zones and GPS signal loss in tunnels. To be able to handle these specific challenges, an AV must reliably recognize the physical attributes of the environment in which it operates. In this paper, we define context recognition as the task of accurately identifying environmental attributes for an AV to appropriately deal with them. Specifically, we define 24 environmental contexts capturing a variety of weather, lighting, traffic and road conditions that an AV must be aware of. Motivated by the need to recognize environmental contexts, we create a context recognition dataset called DrivingContexts with more than 1.6 million context-query pairs relevant for an AV. Since traditional supervised computer vision approaches do not scale well to a variety of contexts, we propose a framework called ContextVLM that uses vision-language models to detect contexts using zero- and few-shot approaches. ContextVLM is capable of reliably detecting relevant driving contexts with an accuracy of more than 95% on our dataset, while running in real-time on a 4GB Nvidia GeForce GTX 1050 Ti GPU on an AV with a latency of 10.5 ms per query.</li>
</ul>

<h3>Title: StimuVAR: Spatiotemporal Stimuli-aware Video Affective Reasoning with Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Guo, Faizan Siddiqui, Yang Zhao, Rama Chellappa, Shao-Yuan Lo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00304">https://arxiv.org/abs/2409.00304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00304">https://arxiv.org/pdf/2409.00304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00304]] StimuVAR: Spatiotemporal Stimuli-aware Video Affective Reasoning with Multimodal Large Language Models(https://arxiv.org/abs/2409.00304)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Predicting and reasoning how a video would make a human feel is crucial for developing socially intelligent systems. Although Multimodal Large Language Models (MLLMs) have shown impressive video understanding capabilities, they tend to focus more on the semantic content of videos, often overlooking emotional stimuli. Hence, most existing MLLMs fall short in estimating viewers' emotional reactions and providing plausible explanations. To address this issue, we propose StimuVAR, a spatiotemporal Stimuli-aware framework for Video Affective Reasoning (VAR) with MLLMs. StimuVAR incorporates a two-level stimuli-aware mechanism: frame-level awareness and token-level awareness. Frame-level awareness involves sampling video frames with events that are most likely to evoke viewers' emotions. Token-level awareness performs tube selection in the token space to make the MLLM concentrate on emotion-triggered spatiotemporal regions. Furthermore, we create VAR instruction data to perform affective training, steering MLLMs' reasoning strengths towards emotional focus and thereby enhancing their affective reasoning ability. To thoroughly assess the effectiveness of VAR, we provide a comprehensive evaluation protocol with extensive metrics. StimuVAR is the first MLLM-based method for viewer-centered VAR. Experiments demonstrate its superiority in understanding viewers' emotional responses to videos and providing coherent and insightful explanations.</li>
</ul>

<h3>Title: Training-Free Sketch-Guided Diffusion with Latent Optimization</h3>
<ul>
<li><strong>Authors: </strong>Sandra Zhang Ding, Jiafeng Mao, Kiyoharu Aizawa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00313">https://arxiv.org/abs/2409.00313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00313">https://arxiv.org/pdf/2409.00313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00313]] Training-Free Sketch-Guided Diffusion with Latent Optimization(https://arxiv.org/abs/2409.00313)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Based on recent advanced diffusion models, Text-to-image (T2I) generation models have demonstrated their capabilities in generating diverse and high-quality images. However, leveraging their potential for real-world content creation, particularly in providing users with precise control over the image generation result, poses a significant challenge. In this paper, we propose an innovative training-free pipeline that extends existing text-to-image generation models to incorporate a sketch as an additional condition. To generate new images with a layout and structure closely resembling the input sketch, we find that these core features of a sketch can be tracked with the cross-attention maps of diffusion models. We introduce latent optimization, a method that refines the noisy latent at each intermediate step of the generation process using cross-attention maps to ensure that the generated images closely adhere to the desired structure outlined in the reference sketch. Through latent optimization, our method enhances the fidelity and accuracy of image generation, offering users greater control and customization options in content creation.</li>
</ul>

<h3>Title: Towards Secure and Usable 3D Assets: A Novel Framework for Automatic Visible Watermarking</h3>
<ul>
<li><strong>Authors: </strong>Gursimran Singh, Tianxi Hu, Mohammad Akbari, Qiang Tang, Yong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00314">https://arxiv.org/abs/2409.00314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00314">https://arxiv.org/pdf/2409.00314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00314]] Towards Secure and Usable 3D Assets: A Novel Framework for Automatic Visible Watermarking(https://arxiv.org/abs/2409.00314)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, watermark</a></li>
<li><strong>Abstract: </strong>3D models, particularly AI-generated ones, have witnessed a recent surge across various industries such as entertainment. Hence, there is an alarming need to protect the intellectual property and avoid the misuse of these valuable assets. As a viable solution to address these concerns, we rigorously define the novel task of automated 3D visible watermarking in terms of two competing aspects: watermark quality and asset utility. Moreover, we propose a method of embedding visible watermarks that automatically determines the right location, orientation, and number of watermarks to be placed on arbitrary 3D assets for high watermark quality and asset utility. Our method is based on a novel rigid-body optimization that uses back-propagation to automatically learn transforms for ideal watermark placement. In addition, we propose a novel curvature-matching method for fusing the watermark into the 3D model that further improves readability and security. Finally, we provide a detailed experimental analysis on two benchmark 3D datasets validating the superior performance of our approach in comparison to baselines. Code and demo are available.</li>
</ul>

<h3>Title: An Empirical Study on Context Length for Open-Domain Dialog Generation</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Shen, Zuoquan Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00315">https://arxiv.org/abs/2409.00315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00315">https://arxiv.org/pdf/2409.00315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00315]] An Empirical Study on Context Length for Open-Domain Dialog Generation(https://arxiv.org/abs/2409.00315)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based open-domain dialog models have become increasingly popular in recent years. These models typically represent context as a concatenation of a dialog history. However, there is no criterion to decide how many utterances should be kept adequate in a context. We try to figure out how the choice of context length affects the model. We experiment on three questions from coarse to fine: (i) Does longer context help model training? (ii) Is it necessary to change the training context length when dealing with dialogs of different context lengths? (iii) Do different dialog samples have the same preference for context length? Our experimental results show that context length, an often overlooked setting, deserves attention when implementing Transformer-based dialog models.</li>
</ul>

<h3>Title: Differentially Private Synthetic High-dimensional Tabular Stream</h3>
<ul>
<li><strong>Authors: </strong>Girish Kumar, Thomas Strohmer, Roman Vershynin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.IT, cs.LG, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00322">https://arxiv.org/abs/2409.00322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00322">https://arxiv.org/pdf/2409.00322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00322]] Differentially Private Synthetic High-dimensional Tabular Stream(https://arxiv.org/abs/2409.00322)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>While differentially private synthetic data generation has been explored extensively in the literature, how to update this data in the future if the underlying private data changes is much less understood. We propose an algorithmic framework for streaming data that generates multiple synthetic datasets over time, tracking changes in the underlying private data. Our algorithm satisfies differential privacy for the entire input stream (continual differential privacy) and can be used for high-dimensional tabular data. Furthermore, we show the utility of our method via experiments on real-world datasets. The proposed algorithm builds upon a popular select, measure, fit, and iterate paradigm (used by offline synthetic data generation algorithms) and private counters for streams.</li>
</ul>

<h3>Title: From Prediction to Application: Language Model-based Code Knowledge Tracing with Domain Adaptive Pre-Training and Automatic Feedback System with Pedagogical Prompting for Comprehensive Programming Education</h3>
<ul>
<li><strong>Authors: </strong>Unggi Lee, Jiyeong Bae, Yeonji Jung, Minji Kang, Gyuri Byun, Yeonseo Lee, Dohee Kim, Sookbun Lee, Jaekwon Park, Taekyung Ahn, Gunho Lee, Hyeoncheol Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00323">https://arxiv.org/abs/2409.00323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00323">https://arxiv.org/pdf/2409.00323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00323]] From Prediction to Application: Language Model-based Code Knowledge Tracing with Domain Adaptive Pre-Training and Automatic Feedback System with Pedagogical Prompting for Comprehensive Programming Education(https://arxiv.org/abs/2409.00323)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Knowledge Tracing (KT) is a critical component in online learning, but traditional approaches face limitations in interpretability and cross-domain adaptability. This paper introduces Language Model-based Code Knowledge Tracing (CodeLKT), an innovative application of Language model-based Knowledge Tracing (LKT) to programming education. CodeLKT leverages pre-trained language models to process learning data, demonstrating superior performance over existing KT and Code KT models. We explore Domain Adaptive Pre-Training (DAPT) and Task Adaptive Pre-Training (TAPT), showing enhanced performance in the coding domain and investigating cross-domain transfer between mathematics and coding. Additionally, we present an theoretically-informed integrated system combining CodeLKT with large language models to generate personalized, in-depth feedback to support students' programming learning. This work advances the field of Code Knowledge Tracing by expanding the knowledge base with language model-based approach and offering practical implications for programming education through data-informed feedback.</li>
</ul>

<h3>Title: Demo: FedCampus: A Real-world Privacy-preserving Mobile Application for Smart Campus via Federated Learning & Analytics</h3>
<ul>
<li><strong>Authors: </strong>Jiaxiang Geng, Beilong Tang, Boyan Zhang, Jiaqi Shao, Bing Luo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00327">https://arxiv.org/abs/2409.00327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00327">https://arxiv.org/pdf/2409.00327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00327]] Demo: FedCampus: A Real-world Privacy-preserving Mobile Application for Smart Campus via Federated Learning & Analytics(https://arxiv.org/abs/2409.00327)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>In this demo, we introduce FedCampus, a privacy-preserving mobile application for smart \underline{campus} with \underline{fed}erated learning (FL) and federated analytics (FA). FedCampus enables cross-platform on-device FL/FA for both iOS and Android, supporting continuously models and algorithms deployment (MLOps). Our app integrates privacy-preserving processed data via differential privacy (DP) from smartwatches, where the processed parameters are used for FL/FA through the FedCampus backend platform. We distributed 100 smartwatches to volunteers at Duke Kunshan University and have successfully completed a series of smart campus tasks featuring capabilities such as sleep tracking, physical activity monitoring, personalized recommendations, and heavy hitters. Our project is opensourced at this https URL. See the FedCampus video at this https URL.</li>
</ul>

<h3>Title: GMFL-Net: A Global Multi-geometric Feature Learning Network for Repetitive Action Counting</h3>
<ul>
<li><strong>Authors: </strong>Jun Li, Jinying Wu, Qiming Li, Feifei Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00330">https://arxiv.org/abs/2409.00330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00330">https://arxiv.org/pdf/2409.00330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00330]] GMFL-Net: A Global Multi-geometric Feature Learning Network for Repetitive Action Counting(https://arxiv.org/abs/2409.00330)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>With the continuous development of deep learning, the field of repetitive action counting is gradually gaining notice from many researchers. Extraction of pose keypoints using human pose estimation networks is proven to be an effective pose-level method. However, existing pose-level methods suffer from the shortcomings that the single coordinate is not stable enough to handle action distortions due to changes in camera viewpoints, thus failing to accurately identify salient poses, and is vulnerable to misdetection during the transition from the exception to the actual action. To overcome these problems, we propose a simple but efficient Global Multi-geometric Feature Learning Network (GMFL-Net). Specifically, we design a MIA-Module that aims to improve information representation by fusing multi-geometric features, and learning the semantic similarity among the input multi-geometric features. Then, to improve the feature representation from a global perspective, we also design a GBFL-Module that enhances the inter-dependencies between point-wise and channel-wise elements and combines them with the rich local information generated by the MIA-Module to synthesise a comprehensive and most representative global feature representation. In addition, considering the insufficient existing dataset, we collect a new dataset called Countix-Fitness-pose (this https URL) which contains different cycle lengths and exceptions, a test set with longer duration, and annotate it with fine-grained annotations at the pose-level. We also add two new action classes, namely lunge and rope push-down. Finally, extensive experiments on the challenging RepCount-pose, UCFRep-pose, and Countix-Fitness-pose benchmarks show that our proposed GMFL-Net achieves state-of-the-art performance.</li>
</ul>

<h3>Title: WikiCausal: Corpus and Evaluation Framework for Causal Knowledge Graph Construction</h3>
<ul>
<li><strong>Authors: </strong>Oktie Hassanzadeh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00331">https://arxiv.org/abs/2409.00331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00331">https://arxiv.org/pdf/2409.00331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00331]] WikiCausal: Corpus and Evaluation Framework for Causal Knowledge Graph Construction(https://arxiv.org/abs/2409.00331)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Recently, there has been an increasing interest in the construction of general-domain and domain-specific causal knowledge graphs. Such knowledge graphs enable reasoning for causal analysis and event prediction, and so have a range of applications across different domains. While great progress has been made toward automated construction of causal knowledge graphs, the evaluation of such solutions has either focused on low-level tasks (e.g., cause-effect phrase extraction) or on ad hoc evaluation data and small manual evaluations. In this paper, we present a corpus, task, and evaluation framework for causal knowledge graph construction. Our corpus consists of Wikipedia articles for a collection of event-related concepts in Wikidata. The task is to extract causal relations between event concepts from the corpus. The evaluation is performed in part using existing causal relations in Wikidata to measure recall, and in part using Large Language Models to avoid the need for manual or crowd-sourced evaluation. We evaluate a pipeline for causal knowledge graph construction that relies on neural models for question answering and concept linking, and show how the corpus and the evaluation framework allow us to effectively find the right model for each task. The corpus and the evaluation framework are publicly available.</li>
</ul>

<h3>Title: Evaluating the Effectiveness of Large Language Models in Representing and Understanding Movement Trajectories</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Ji, Song Gao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00335">https://arxiv.org/abs/2409.00335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00335">https://arxiv.org/pdf/2409.00335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00335]] Evaluating the Effectiveness of Large Language Models in Representing and Understanding Movement Trajectories(https://arxiv.org/abs/2409.00335)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This research focuses on assessing the ability of AI foundation models in representing the trajectories of movements. We utilize one of the large language models (LLMs) (i.e., GPT-J) to encode the string format of trajectories and then evaluate the effectiveness of the LLM-based representation for trajectory data analysis. The experiments demonstrate that while the LLM-based embeddings can preserve certain trajectory distance metrics (i.e., the correlation coefficients exceed 0.74 between the Cosine distance derived from GPT-J embeddings and the Hausdorff and Dynamic Time Warping distances on raw trajectories), challenges remain in restoring numeric values and retrieving spatial neighbors in movement trajectory analytics. In addition, the LLMs can understand the spatiotemporal dependency contained in trajectories and have good accuracy in location prediction tasks. This research highlights the need for improvement in terms of capturing the nuances and complexities of the underlying geospatial data and integrating domain knowledge to support various GeoAI applications using LLMs.</li>
</ul>

<h3>Title: LightPure: Realtime Adversarial Image Purification for Mobile Devices Using Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Hossein Khalili, Seongbin Park, Vincent Li, Brandan Bright, Ali Payani, Ramana Rao Kompella, Nader Sehatbakhsh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00340">https://arxiv.org/abs/2409.00340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00340">https://arxiv.org/pdf/2409.00340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00340]] LightPure: Realtime Adversarial Image Purification for Mobile Devices Using Diffusion Models(https://arxiv.org/abs/2409.00340)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Autonomous mobile systems increasingly rely on deep neural networks for perception and decision-making. While effective, these systems are vulnerable to adversarial machine learning attacks where minor input perturbations can significantly impact outcomes. Common countermeasures involve adversarial training and/or data or network transformation. These methods, though effective, require full access to typically proprietary classifiers and are costly for large models. Recent solutions propose purification models, which add a "purification" layer before classification, eliminating the need to modify the classifier directly. Despite their effectiveness, these methods are compute-intensive, making them unsuitable for mobile systems where resources are limited and low latency is essential. This paper introduces LightPure, a new method that enhances adversarial image purification. It improves the accuracy of existing purification methods and provides notable enhancements in speed and computational efficiency, making it suitable for mobile devices with limited resources. Our approach uses a two-step diffusion and one-shot Generative Adversarial Network (GAN) framework, prioritizing latency without compromising robustness. We propose several new techniques to achieve a reasonable balance between classification accuracy and adversarial robustness while maintaining desired latency. We design and implement a proof-of-concept on a Jetson Nano board and evaluate our method using various attack scenarios and datasets. Our results show that LightPure can outperform existing methods by up to 10x in terms of latency while achieving higher accuracy and robustness for various attack scenarios. This method offers a scalable and effective solution for real-world mobile systems.</li>
</ul>

<h3>Title: Aligning Medical Images with General Knowledge from Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiao Fang, Yi Lin, Dong Zhang, Kwang-Ting Cheng, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00341">https://arxiv.org/abs/2409.00341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00341">https://arxiv.org/pdf/2409.00341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00341]] Aligning Medical Images with General Knowledge from Large Language Models(https://arxiv.org/abs/2409.00341)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Pre-trained large vision-language models (VLMs) like CLIP have revolutionized visual representation learning using natural language as supervisions, and demonstrated promising generalization ability. In this work, we propose ViP, a novel visual symptom-guided prompt learning framework for medical image analysis, which facilitates general knowledge transfer from CLIP. ViP consists of two key components: a visual symptom generator (VSG) and a dual-prompt network. Specifically, VSG aims to extract explicable visual symptoms from pre-trained large language models, while the dual-prompt network utilizes these visual symptoms to guide the training on two learnable prompt modules, i.e., context prompt and merge prompt, which effectively adapts our framework to medical image analysis via large VLMs. Extensive experimental results demonstrate that ViP can outperform state-of-the-art methods on two challenging datasets.</li>
</ul>

<h3>Title: AdaNAT: Exploring Adaptive Policy for Token-Based Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zanlin Ni, Yulin Wang, Renping Zhou, Rui Lu, Jiayi Guo, Jinyi Hu, Zhiyuan Liu, Yuan Yao, Gao Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00342">https://arxiv.org/abs/2409.00342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00342">https://arxiv.org/pdf/2409.00342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00342]] AdaNAT: Exploring Adaptive Policy for Token-Based Image Generation(https://arxiv.org/abs/2409.00342)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent studies have demonstrated the effectiveness of token-based methods for visual content generation. As a representative work, non-autoregressive Transformers (NATs) are able to synthesize images with decent quality in a small number of steps. However, NATs usually necessitate configuring a complicated generation policy comprising multiple manually-designed scheduling rules. These heuristic-driven rules are prone to sub-optimality and come with the requirements of expert knowledge and labor-intensive efforts. Moreover, their one-size-fits-all nature cannot flexibly adapt to the diverse characteristics of each individual sample. To address these issues, we propose AdaNAT, a learnable approach that automatically configures a suitable policy tailored for every sample to be generated. In specific, we formulate the determination of generation policies as a Markov decision process. Under this framework, a lightweight policy network for generation can be learned via reinforcement learning. Importantly, we demonstrate that simple reward designs such as FID or pre-trained reward models, may not reliably guarantee the desired quality or diversity of generated samples. Therefore, we propose an adversarial reward design to guide the training of policy networks effectively. Comprehensive experiments on four benchmark datasets, i.e., ImageNet-256 & 512, MS-COCO, and CC3M, validate the effectiveness of AdaNAT. Code and pre-trained models will be released at this https URL.</li>
</ul>

<h3>Title: EgoHDM: An Online Egocentric-Inertial Human Motion Capture, Localization, and Dense Mapping System</h3>
<ul>
<li><strong>Authors: </strong>Bonan Liu, Handi Yin, Manuel Kaufmann, Jinhao He, Sammy Christen, Jie Song, Pan Hui</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00343">https://arxiv.org/abs/2409.00343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00343">https://arxiv.org/pdf/2409.00343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00343]] EgoHDM: An Online Egocentric-Inertial Human Motion Capture, Localization, and Dense Mapping System(https://arxiv.org/abs/2409.00343)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present EgoHDM, an online egocentric-inertial human motion capture (mocap), localization, and dense mapping system. Our system uses 6 inertial measurement units (IMUs) and a commodity head-mounted RGB camera. EgoHDM is the first human mocap system that offers dense scene mapping in near real-time. Further, it is fast and robust to initialize and fully closes the loop between physically plausible map-aware global human motion estimation and mocap-aware 3D scene reconstruction. Our key idea is integrating camera localization and mapping information with inertial human motion capture bidirectionally in our system. To achieve this, we design a tightly coupled mocap-aware dense bundle adjustment and physics-based body pose correction module leveraging a local body-centric elevation map. The latter introduces a novel terrain-aware contact PD controller, which enables characters to physically contact the given local elevation map thereby reducing human floating or penetration. We demonstrate the performance of our system on established synthetic and real-world benchmarks. The results show that our method reduces human localization, camera pose, and mapping accuracy error by 41%, 71%, 46%, respectively, compared to the state of the art. Our qualitative evaluations on newly captured data further demonstrate that EgoHDM can cover challenging scenarios in non-flat terrain including stepping over stairs and outdoor scenes in the wild.</li>
</ul>

<h3>Title: SMAFormer: Synergistic Multi-Attention Transformer for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Fuchen Zheng, Xuhang Chen, Weihuang Liu, Haolun Li, Yingtie Lei, Jiahui He, Chi-Man Pun, Shounjun Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00346">https://arxiv.org/abs/2409.00346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00346">https://arxiv.org/pdf/2409.00346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00346]] SMAFormer: Synergistic Multi-Attention Transformer for Medical Image Segmentation(https://arxiv.org/abs/2409.00346)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>In medical image segmentation, specialized computer vision techniques, notably transformers grounded in attention mechanisms and residual networks employing skip connections, have been instrumental in advancing performance. Nonetheless, previous models often falter when segmenting small, irregularly shaped tumors. To this end, we introduce SMAFormer, an efficient, Transformer-based architecture that fuses multiple attention mechanisms for enhanced segmentation of small tumors and organs. SMAFormer can capture both local and global features for medical image segmentation. The architecture comprises two pivotal components. First, a Synergistic Multi-Attention (SMA) Transformer block is proposed, which has the benefits of Pixel Attention, Channel Attention, and Spatial Attention for feature enrichment. Second, addressing the challenge of information loss incurred during attention mechanism transitions and feature fusion, we design a Feature Fusion Modulator. This module bolsters the integration between the channel and spatial attention by mitigating reshaping-induced information attrition. To evaluate our method, we conduct extensive experiments on various medical image segmentation tasks, including multi-organ, liver tumor, and bladder tumor segmentation, achieving state-of-the-art results. Code and models are available at: \url{this https URL}.</li>
</ul>

<h3>Title: Chatting Up Attachment: Using LLMs to Predict Adult Bonds</h3>
<ul>
<li><strong>Authors: </strong>Paulo Soares, Sean McCurdy, Andrew J. Gerber, Peter Fonagy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00347">https://arxiv.org/abs/2409.00347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00347">https://arxiv.org/pdf/2409.00347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00347]] Chatting Up Attachment: Using LLMs to Predict Adult Bonds(https://arxiv.org/abs/2409.00347)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Obtaining data in the medical field is challenging, making the adoption of AI technology within the space slow and high-risk. We evaluate whether we can overcome this obstacle with synthetic data generated by large language models (LLMs). In particular, we use GPT-4 and Claude 3 Opus to create agents that simulate adults with varying profiles, childhood memories, and attachment styles. These agents participate in simulated Adult Attachment Interviews (AAI), and we use their responses to train models for predicting their underlying attachment styles. We evaluate our models using a transcript dataset from 9 humans who underwent the same interview protocol, analyzed and labeled by mental health professionals. Our findings indicate that training the models using only synthetic data achieves performance comparable to training the models on human data. Additionally, while the raw embeddings from synthetic answers occupy a distinct space compared to those from real human responses, the introduction of unlabeled human data and a simple standardization allows for a closer alignment of these representations. This adjustment is supported by qualitative analyses and is reflected in the enhanced predictive accuracy of the standardized embeddings.</li>
</ul>

<h3>Title: Does Alignment Tuning Really Break LLMs' Internal Confidence?</h3>
<ul>
<li><strong>Authors: </strong>Hongseok Oh, Wonseok Hwang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00352">https://arxiv.org/abs/2409.00352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00352">https://arxiv.org/pdf/2409.00352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00352]] Does Alignment Tuning Really Break LLMs' Internal Confidence?(https://arxiv.org/abs/2409.00352)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable progress, but their real-world application necessitates reliable calibration. This study conducts a comprehensive analysis of calibration degradation of LLMs across four dimensions: models, calibration metrics, tasks, and confidence extraction methods. Initial analysis showed that the relationship between alignment and calibration is not always a trade-off, but under stricter analysis conditions, we found the alignment process consistently harms calibration. This highlights the need for (1) a careful approach when measuring model confidences and calibration errors and (2) future research into algorithms that can help LLMs to achieve both instruction-following and calibration without sacrificing either.</li>
</ul>

<h3>Title: RI-MAE: Rotation-Invariant Masked AutoEncoders for Self-Supervised Point Cloud Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Kunming Su, Qiuxia Wu, Panpan Cai, Xiaogang Zhu, Xuequan Lu, Zhiyong Wang, Kun Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00353">https://arxiv.org/abs/2409.00353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00353">https://arxiv.org/pdf/2409.00353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00353]] RI-MAE: Rotation-Invariant Masked AutoEncoders for Self-Supervised Point Cloud Representation Learning(https://arxiv.org/abs/2409.00353)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Masked point modeling methods have recently achieved great success in self-supervised learning for point cloud data. However, these methods are sensitive to rotations and often exhibit sharp performance drops when encountering rotational variations. In this paper, we propose a novel Rotation-Invariant Masked AutoEncoders (RI-MAE) to address two major challenges: 1) achieving rotation-invariant latent representations, and 2) facilitating self-supervised reconstruction in a rotation-invariant manner. For the first challenge, we introduce RI-Transformer, which features disentangled geometry content, rotation-invariant relative orientation and position embedding mechanisms for constructing rotation-invariant point cloud latent space. For the second challenge, a novel dual-branch student-teacher architecture is devised. It enables the self-supervised learning via the reconstruction of masked patches within the learned rotation-invariant latent space. Each branch is based on an RI-Transformer, and they are connected with an additional RI-Transformer predictor. The teacher encodes all point patches, while the student solely encodes unmasked ones. Finally, the predictor predicts the latent features of the masked patches using the output latent embeddings from the student, supervised by the outputs from the teacher. Extensive experiments demonstrate that our method is robust to rotations, achieving the state-of-the-art performance on various downstream tasks.</li>
</ul>

<h3>Title: An Empirical Study on Information Extraction using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ridong Han, Chaohao Yang, Tao Peng, Prayag Tiwari, Xiang Wan, Lu Liu, Benyou Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00369">https://arxiv.org/abs/2409.00369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00369">https://arxiv.org/pdf/2409.00369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00369]] An Empirical Study on Information Extraction using Large Language Models(https://arxiv.org/abs/2409.00369)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Human-like large language models (LLMs), especially the most powerful and popular ones in OpenAI's GPT family, have proven to be very helpful for many natural language processing (NLP) related tasks. Therefore, various attempts have been made to apply LLMs to information extraction (IE), which is a fundamental NLP task that involves extracting information from unstructured plain text. To demonstrate the latest representative progress in LLMs' information extraction ability, we assess the information extraction ability of GPT-4 (the latest version of GPT at the time of writing this paper) from four perspectives: Performance, Evaluation Criteria, Robustness, and Error Types. Our results suggest a visible performance gap between GPT-4 and state-of-the-art (SOTA) IE methods. To alleviate this problem, considering the LLMs' human-like characteristics, we propose and analyze the effects of a series of simple prompt-based methods, which can be generalized to other LLMs and NLP tasks. Rich experiments show our methods' effectiveness and some of their remaining issues in improving GPT-4's information extraction ability.</li>
</ul>

<h3>Title: First Competition on Presentation Attack Detection on ID Card</h3>
<ul>
<li><strong>Authors: </strong>Juan E. Tapia, Naser Damer, Christoph Busch, Juan M. Espin, Javier Barrachina, Alvaro S. Rocamora, Kristof Ocvirk, Leon Alessio, Borut Batagelj, Sushrut Patwardhan, Raghavendra Ramachandra, Raghavendra Mudgalgundurao, Kiran Raja, Daniel Schulz, Carlos Aravena</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00372">https://arxiv.org/abs/2409.00372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00372">https://arxiv.org/pdf/2409.00372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00372]] First Competition on Presentation Attack Detection on ID Card(https://arxiv.org/abs/2409.00372)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, biometric</a></li>
<li><strong>Abstract: </strong>This paper summarises the Competition on Presentation Attack Detection on ID Cards (PAD-IDCard) held at the 2024 International Joint Conference on Biometrics (IJCB2024). The competition attracted a total of ten registered teams, both from academia and industry. In the end, the participating teams submitted five valid submissions, with eight models to be evaluated by the organisers. The competition presented an independent assessment of current state-of-the-art algorithms. Today, no independent evaluation on cross-dataset is available; therefore, this work determined the state-of-the-art on ID cards. To reach this goal, a sequestered test set and baseline algorithms were used to evaluate and compare all the proposals. The sequestered test dataset contains ID cards from four different countries. In summary, a team that chose to be "Anonymous" reached the best average ranking results of 74.80%, followed very closely by the "IDVC" team with 77.65%.</li>
</ul>

<h3>Title: Towards understanding Diffusion Models (on Graphs)</h3>
<ul>
<li><strong>Authors: </strong>Solveig Klepper</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00374">https://arxiv.org/abs/2409.00374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00374">https://arxiv.org/pdf/2409.00374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00374]] Towards understanding Diffusion Models (on Graphs)(https://arxiv.org/abs/2409.00374)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged from various theoretical and methodological perspectives, each offering unique insights into their underlying principles. In this work, we provide an overview of the most prominent approaches, drawing attention to their striking analogies -- namely, how seemingly diverse methodologies converge to a similar mathematical formulation of the core problem. While our ultimate goal is to understand these models in the context of graphs, we begin by conducting experiments in a simpler setting to build foundational insights. Through an empirical investigation of different diffusion and sampling techniques, we explore three critical questions: (1) What role does noise play in these models? (2) How significantly does the choice of the sampling method affect outcomes? (3) What function is the neural network approximating, and is high complexity necessary for optimal performance? Our findings aim to enhance the understanding of diffusion models and in the long run their application in graph machine learning.</li>
</ul>

<h3>Title: Lyapunov Neural ODE Feedback Control Policies</h3>
<ul>
<li><strong>Authors: </strong>Joshua Hang Sai Ip, Georgios Makrygiorgos, Ali Mesbah</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00393">https://arxiv.org/abs/2409.00393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00393">https://arxiv.org/pdf/2409.00393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00393]] Lyapunov Neural ODE Feedback Control Policies(https://arxiv.org/abs/2409.00393)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks are increasingly used as an effective way to represent control policies in a wide-range of learning-based control methods. For continuous-time optimal control problems (OCPs), which are central to many decision-making tasks, control policy learning can be cast as a neural ordinary differential equation (NODE) problem wherein state and control constraints are naturally accommodated. This paper presents a Lyapunov-NODE control (L-NODEC) approach to solving continuous-time OCPs for the case of stabilizing a known constrained nonlinear system around a terminal equilibrium point. We propose a Lyapunov loss formulation that incorporates a control-theoretic Lyapunov condition into the problem of learning a state-feedback neural control policy. We establish that L-NODEC ensures exponential stability of the controlled system, as well as its adversarial robustness to uncertain initial conditions. The performance of L-NODEC is illustrated on a benchmark double integrator problem and for optimal control of thermal dose delivery using a cold atmospheric plasma biomedical system. L-NODEC can substantially reduce the inference time necessary to reach the equilibrium state.</li>
</ul>

<h3>Title: Self-supervised Fusarium Head Blight Detection with Hyperspectral Image and Feature Mining</h3>
<ul>
<li><strong>Authors: </strong>Yu-Fan Lin, Ching-Heng Cheng, Bo-Cheng Qiu, Cheng-Jun Kang, Chia-Ming Lee, Chih-Chung Hsu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00395">https://arxiv.org/abs/2409.00395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00395">https://arxiv.org/pdf/2409.00395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00395]] Self-supervised Fusarium Head Blight Detection with Hyperspectral Image and Feature Mining(https://arxiv.org/abs/2409.00395)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, extraction</a></li>
<li><strong>Abstract: </strong>Fusarium Head Blight (FHB) is a serious fungal disease affecting wheat (including durum), barley, oats, other small cereal grains, and corn. Effective monitoring and accurate detection of FHB are crucial to ensuring stable and reliable food security. Traditionally, trained agronomists and surveyors perform manual identification, a method that is labor-intensive, impractical, and challenging to scale. With the advancement of deep learning and Hyper-spectral Imaging (HSI) and Remote Sensing (RS) technologies, employing deep learning, particularly Convolutional Neural Networks (CNNs), has emerged as a promising solution. Notably, wheat infected with serious FHB may exhibit significant differences on the spectral compared to mild FHB one, which is particularly advantageous for hyperspectral image-based methods. In this study, we propose a self-unsupervised classification method based on HSI endmember extraction strategy and top-K bands selection, designed to analyze material signatures in HSIs to derive discriminative feature representations. This approach does not require expensive device or complicate algorithm design, making it more suitable for practical uses. Our method has been effectively validated in the Beyond Visible Spectrum: AI for Agriculture Challenge 2024. The source code is easy to reproduce and available at {this https URL}.</li>
</ul>

<h3>Title: Rethinking Backdoor Detection Evaluation for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jun Yan, Wenjie Jacky Mo, Xiang Ren, Robin Jia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00399">https://arxiv.org/abs/2409.00399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00399">https://arxiv.org/pdf/2409.00399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00399]] Rethinking Backdoor Detection Evaluation for Language Models(https://arxiv.org/abs/2409.00399)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Backdoor attacks, in which a model behaves maliciously when given an attacker-specified trigger, pose a major security risk for practitioners who depend on publicly released language models. Backdoor detection methods aim to detect whether a released model contains a backdoor, so that practitioners can avoid such vulnerabilities. While existing backdoor detection methods have high accuracy in detecting backdoored models on standard benchmarks, it is unclear whether they can robustly identify backdoors in the wild. In this paper, we examine the robustness of backdoor detectors by manipulating different factors during backdoor planting. We find that the success of existing methods highly depends on how intensely the model is trained on poisoned data during backdoor planting. Specifically, backdoors planted with either more aggressive or more conservative training are significantly more difficult to detect than the default ones. Our results highlight a lack of robustness of existing backdoor detectors and the limitations in current benchmark construction.</li>
</ul>

<h3>Title: A Hybrid Transformer-Mamba Network for Single Image Deraining</h3>
<ul>
<li><strong>Authors: </strong>Shangquan Sun, Wenqi Ren, Juxiang Zhou, Jianhou Gan, Rui Wang, Xiaochun Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00410">https://arxiv.org/abs/2409.00410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00410">https://arxiv.org/pdf/2409.00410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00410]] A Hybrid Transformer-Mamba Network for Single Image Deraining(https://arxiv.org/abs/2409.00410)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Existing deraining Transformers employ self-attention mechanisms with fixed-range windows or along channel dimensions, limiting the exploitation of non-local receptive fields. In response to this issue, we introduce a novel dual-branch hybrid Transformer-Mamba network, denoted as TransMamba, aimed at effectively capturing long-range rain-related dependencies. Based on the prior of distinct spectral-domain features of rain degradation and background, we design a spectral-banded Transformer blocks on the first branch. Self-attention is executed within the combination of the spectral-domain channel dimension to improve the ability of modeling long-range dependencies. To enhance frequency-specific information, we present a spectral enhanced feed-forward module that aggregates features in the spectral domain. In the second branch, Mamba layers are equipped with cascaded bidirectional state space model modules to additionally capture the modeling of both local and global information. At each stage of both the encoder and decoder, we perform channel-wise concatenation of dual-branch features and achieve feature fusion through channel reduction, enabling more effective integration of the multi-scale information from the Transformer and Mamba branches. To better reconstruct innate signal-level relations within clean images, we also develop a spectral coherence loss. Extensive experiments on diverse datasets and real-world images demonstrate the superiority of our method compared against the state-of-the-art approaches.</li>
</ul>

<h3>Title: Robust off-policy Reinforcement Learning via Soft Constrained Adversary</h3>
<ul>
<li><strong>Authors: </strong>Kosuke Nakanishi, Akihiro Kubo, Yuji Yasui, Shin Ishii</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00418">https://arxiv.org/abs/2409.00418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00418">https://arxiv.org/pdf/2409.00418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00418]] Robust off-policy Reinforcement Learning via Soft Constrained Adversary(https://arxiv.org/abs/2409.00418)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Recently, robust reinforcement learning (RL) methods against input observation have garnered significant attention and undergone rapid evolution due to RL's potential vulnerability. Although these advanced methods have achieved reasonable success, there have been two limitations when considering adversary in terms of long-term horizons. First, the mutual dependency between the policy and its corresponding optimal adversary limits the development of off-policy RL algorithms; although obtaining optimal adversary should depend on the current policy, this has restricted applications to off-policy RL. Second, these methods generally assume perturbations based only on the $L_p$-norm, even when prior knowledge of the perturbation distribution in the environment is available. We here introduce another perspective on adversarial RL: an f-divergence constrained problem with the prior knowledge distribution. From this, we derive two typical attacks and their corresponding robust learning frameworks. The evaluation of robustness is conducted and the results demonstrate that our proposed methods achieve excellent performance in sample-efficient off-policy RL.</li>
</ul>

<h3>Title: Reproducibility Study Of Learning Fair Graph Representations Via Automated Data Augmentations</h3>
<ul>
<li><strong>Authors: </strong>Thijmen Nijdam, Juell Sprott, Taiki Papandreou-Lazos, Jurgen de Heus</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00421">https://arxiv.org/abs/2409.00421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00421">https://arxiv.org/pdf/2409.00421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00421]] Reproducibility Study Of Learning Fair Graph Representations Via Automated Data Augmentations(https://arxiv.org/abs/2409.00421)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>In this study, we undertake a reproducibility analysis of 'Learning Fair Graph Representations Via Automated Data Augmentations' by Ling et al. (2022). We assess the validity of the original claims focused on node classification tasks and explore the performance of the Graphair framework in link prediction tasks. Our investigation reveals that we can partially reproduce one of the original three claims and fully substantiate the other two. Additionally, we broaden the application of Graphair from node classification to link prediction across various datasets. Our findings indicate that, while Graphair demonstrates a comparable fairness-accuracy trade-off to baseline models for mixed dyadic-level fairness, it has a superior trade-off for subgroup dyadic-level fairness. These findings underscore Graphair's potential for wider adoption in graph-based learning. Our code base can be found on GitHub at this https URL.</li>
</ul>

<h3>Title: Is Difficulty Calibration All We Need? Towards More Practical Membership Inference Attacks</h3>
<ul>
<li><strong>Authors: </strong>Yu He, Boheng Li, Yao Wang, Mengda Yang, Juan Wang, Hongxin Hu, Xingyu Zhao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00426">https://arxiv.org/abs/2409.00426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00426">https://arxiv.org/pdf/2409.00426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00426]] Is Difficulty Calibration All We Need? Towards More Practical Membership Inference Attacks(https://arxiv.org/abs/2409.00426)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, membership infer</a></li>
<li><strong>Abstract: </strong>The vulnerability of machine learning models to Membership Inference Attacks (MIAs) has garnered considerable attention in recent years. These attacks determine whether a data sample belongs to the model's training set or not. Recent research has focused on reference-based attacks, which leverage difficulty calibration with independently trained reference models. While empirical studies have demonstrated its effectiveness, there is a notable gap in our understanding of the circumstances under which it succeeds or fails. In this paper, we take a further step towards a deeper understanding of the role of difficulty calibration. Our observations reveal inherent limitations in calibration methods, leading to the misclassification of non-members and suboptimal performance, particularly on high-loss samples. We further identify that these errors stem from an imperfect sampling of the potential distribution and a strong dependence of membership scores on the model parameters. By shedding light on these issues, we propose RAPID: a query-efficient and computation-efficient MIA that directly \textbf{R}e-lever\textbf{A}ges the original membershi\textbf{P} scores to m\textbf{I}tigate the errors in \textbf{D}ifficulty calibration. Our experimental results, spanning 9 datasets and 5 model architectures, demonstrate that PETAL outperforms previous state-of-the-art attacks (e.g., LiRA and Canary offline) across different metrics while remaining computationally efficient. Our observations and analysis challenge the current de facto paradigm of difficulty calibration in high-precision inference, encouraging greater attention to the persistent risks posed by MIAs in more practical scenarios.</li>
</ul>

<h3>Title: Breaking Down Financial News Impact: A Novel AI Approach with Geometric Hypergraphs</h3>
<ul>
<li><strong>Authors: </strong>Anoushka Harit, Zhongtian Sun, Jongmin Yu, Noura Al Moubayed</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00438">https://arxiv.org/abs/2409.00438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00438">https://arxiv.org/pdf/2409.00438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00438]] Breaking Down Financial News Impact: A Novel AI Approach with Geometric Hypergraphs(https://arxiv.org/abs/2409.00438)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>In the fast-paced and volatile financial markets, accurately predicting stock movements based on financial news is critical for investors and analysts. Traditional models often struggle to capture the intricate and dynamic relationships between news events and market reactions, limiting their ability to provide actionable insights. This paper introduces a novel approach leveraging Explainable Artificial Intelligence (XAI) through the development of a Geometric Hypergraph Attention Network (GHAN) to analyze the impact of financial news on market behaviours. Geometric hypergraphs extend traditional graph structures by allowing edges to connect multiple nodes, effectively modelling high-order relationships and interactions among financial entities and news events. This unique capability enables the capture of complex dependencies, such as the simultaneous impact of a single news event on multiple stocks or sectors, which traditional models frequently overlook. By incorporating attention mechanisms within hypergraphs, GHAN enhances the model's ability to focus on the most relevant information, ensuring more accurate predictions and better interpretability. Additionally, we employ BERT-based embeddings to capture the semantic richness of financial news texts, providing a nuanced understanding of the content. Using a comprehensive financial news dataset, our GHAN model addresses key challenges in financial news impact analysis, including the complexity of high-order interactions, the necessity for model interpretability, and the dynamic nature of financial markets. Integrating attention mechanisms and SHAP values within GHAN ensures transparency, highlighting the most influential factors driving market predictions. Empirical validation demonstrates the superior effectiveness of our approach over traditional sentiment analysis and time-series models.</li>
</ul>

<h3>Title: Dynamical system prediction from sparse observations using deep neural networks with Voronoi tessellation and physics constraint</h3>
<ul>
<li><strong>Authors: </strong>Hanyang Wang, Hao Zhou, Sibo Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00458">https://arxiv.org/abs/2409.00458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00458">https://arxiv.org/pdf/2409.00458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00458]] Dynamical system prediction from sparse observations using deep neural networks with Voronoi tessellation and physics constraint(https://arxiv.org/abs/2409.00458)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite the success of various methods in addressing the issue of spatial reconstruction of dynamical systems with sparse observations, spatio-temporal prediction for sparse fields remains a challenge. Existing Kriging-based frameworks for spatio-temporal sparse field prediction fail to meet the accuracy and inference time required for nonlinear dynamic prediction problems. In this paper, we introduce the Dynamical System Prediction from Sparse Observations using Voronoi Tessellation (DSOVT) framework, an innovative methodology based on Voronoi tessellation which combines convolutional encoder-decoder (CED) and long short-term memory (LSTM) and utilizing Convolutional Long Short-Term Memory (ConvLSTM). By integrating Voronoi tessellations with spatio-temporal deep learning models, DSOVT is adept at predicting dynamical systems with unstructured, sparse, and time-varying observations. CED-LSTM maps Voronoi tessellations into a low-dimensional representation for time series prediction, while ConvLSTM directly uses these tessellations in an end-to-end predictive model. Furthermore, we incorporate physics constraints during the training process for dynamical systems with explicit formulas. Compared to purely data-driven models, our physics-based approach enables the model to learn physical laws within explicitly formulated dynamics, thereby enhancing the robustness and accuracy of rolling forecasts. Numerical experiments on real sea surface data and shallow water systems clearly demonstrate our framework's accuracy and computational efficiency with sparse and time-varying observations.</li>
</ul>

<h3>Title: Studying the Effects of Self-Attention on SAR Automatic Target Recognition</h3>
<ul>
<li><strong>Authors: </strong>Jacob Fein-Ashley, Rajgopal Kannan, Viktor Prasanna</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00473">https://arxiv.org/abs/2409.00473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00473">https://arxiv.org/pdf/2409.00473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00473]] Studying the Effects of Self-Attention on SAR Automatic Target Recognition(https://arxiv.org/abs/2409.00473)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Attention mechanisms are critically important in the advancement of synthetic aperture radar (SAR) automatic target recognition (ATR) systems. Traditional SAR ATR models often struggle with the noisy nature of the SAR data, frequently learning from background noise rather than the most relevant image features. Attention mechanisms address this limitation by focusing on crucial image components, such as the shadows and small parts of a vehicle, which are crucial for accurate target classification. By dynamically prioritizing these significant features, attention-based models can efficiently characterize the entire image with a few pixels, thus enhancing recognition performance. This capability allows for the discrimination of targets from background clutter, leading to more practical and robust SAR ATR models. We show that attention modules increase top-1 accuracy, improve input robustness, and are qualitatively more explainable on the MSTAR dataset.</li>
</ul>

<h3>Title: BaseMirror: Automatic Reverse Engineering of Baseband Commands from Android's Radio Interface Layer</h3>
<ul>
<li><strong>Authors: </strong>Wenqiang Li, Haohuang Wen, Zhiqiang Lin</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00475">https://arxiv.org/abs/2409.00475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00475">https://arxiv.org/pdf/2409.00475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00475]] BaseMirror: Automatic Reverse Engineering of Baseband Commands from Android's Radio Interface Layer(https://arxiv.org/abs/2409.00475)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>In modern mobile devices, baseband is an integral component running on top of cellular processors to handle crucial radio communications. However, recent research reveals significant vulnerabilities in these basebands, posing serious security risks like remote code execution. Yet, effectively scrutinizing basebands remains a daunting task, as they run closed-source and proprietary software on vendor-specific chipsets. Existing analysis methods are limited by their dependence on manual processes and heuristic approaches, reducing their scalability. This paper introduces a novel approach to unveil security issues in basebands from a unique perspective: to uncover vendor-specific baseband commands from the Radio Interface Layer (RIL), a hardware abstraction layer interfacing with basebands. To demonstrate this concept, we have designed and developed BaseMirror, a static binary analysis tool to automatically reverse engineer baseband commands from vendor-specific RIL binaries. It utilizes a bidirectional taint analysis algorithm to adeptly identify baseband commands from an enhanced control flow graph enriched with reconstructed virtual function calls. Our methodology has been applied to 28 vendor RIL libraries, encompassing a wide range of Samsung Exynos smartphone models on the market. Remarkably, BaseMirror has uncovered 873 unique baseband commands undisclosed to the public. Based on these results, we develop an automated attack discovery framework to successfully derive and validate 8 zero-day vulnerabilities that trigger denial of cellular service and arbitrary file access on a Samsung Galaxy A53 device. These findings have been reported and confirmed by Samsung and a bug bounty was awarded to us.</li>
</ul>

<h3>Title: Multi-scale Multi-instance Visual Sound Localization and Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Shentong Mo, Haofan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.MM, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00486">https://arxiv.org/abs/2409.00486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00486">https://arxiv.org/pdf/2409.00486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00486]] Multi-scale Multi-instance Visual Sound Localization and Segmentation(https://arxiv.org/abs/2409.00486)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Visual sound localization is a typical and challenging problem that predicts the location of objects corresponding to the sound source in a video. Previous methods mainly used the audio-visual association between global audio and one-scale visual features to localize sounding objects in each image. Despite their promising performance, they omitted multi-scale visual features of the corresponding image, and they cannot learn discriminative regions compared to ground truths. To address this issue, we propose a novel multi-scale multi-instance visual sound localization framework, namely M2VSL, that can directly learn multi-scale semantic features associated with sound sources from the input image to localize sounding objects. Specifically, our M2VSL leverages learnable multi-scale visual features to align audio-visual representations at multi-level locations of the corresponding image. We also introduce a novel multi-scale multi-instance transformer to dynamically aggregate multi-scale cross-modal representations for visual sound localization. We conduct extensive experiments on VGGSound-Instruments, VGG-Sound Sources, and AVSBench benchmarks. The results demonstrate that the proposed M2VSL can achieve state-of-the-art performance on sounding object localization and segmentation.</li>
</ul>

<h3>Title: Geospatial foundation models for image analysis: evaluating and enhancing NASA-IBM Prithvi's domain adaptability</h3>
<ul>
<li><strong>Authors: </strong>Chia-Yu Hsu, Wenwen Li, Sizhe Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00489">https://arxiv.org/abs/2409.00489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00489">https://arxiv.org/pdf/2409.00489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00489]] Geospatial foundation models for image analysis: evaluating and enhancing NASA-IBM Prithvi's domain adaptability(https://arxiv.org/abs/2409.00489)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Research on geospatial foundation models (GFMs) has become a trending topic in geospatial artificial intelligence (AI) research due to their potential for achieving high generalizability and domain adaptability, reducing model training costs for individual researchers. Unlike large language models, such as ChatGPT, constructing visual foundation models for image analysis, particularly in remote sensing, encountered significant challenges such as formulating diverse vision tasks into a general problem framework. This paper evaluates the recently released NASA-IBM GFM Prithvi for its predictive performance on high-level image analysis tasks across multiple benchmark datasets. Prithvi was selected because it is one of the first open-source GFMs trained on time-series of high-resolution remote sensing imagery. A series of experiments were designed to assess Prithvi's performance as compared to other pre-trained task-specific AI models in geospatial image analysis. New strategies, including band adaptation, multi-scale feature generation, and fine-tuning techniques, are introduced and integrated into an image analysis pipeline to enhance Prithvi's domain adaptation capability and improve model performance. In-depth analyses reveal Prithvi's strengths and weaknesses, offering insights for both improving Prithvi and developing future visual foundation models for geospatial tasks.</li>
</ul>

<h3>Title: Accurate Compression of Text-to-Image Diffusion Models via Vector Quantization</h3>
<ul>
<li><strong>Authors: </strong>Vage Egiazarian, Denis Kuznedelev, Anton Voronov, Ruslan Svirschevski, Michael Goin, Daniil Pavlov, Dan Alistarh, Dmitry Baranchuk</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00492">https://arxiv.org/abs/2409.00492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00492">https://arxiv.org/pdf/2409.00492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00492]] Accurate Compression of Text-to-Image Diffusion Models via Vector Quantization(https://arxiv.org/abs/2409.00492)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have emerged as a powerful framework for high-quality image generation given textual prompts. Their success has driven the rapid development of production-grade diffusion models that consistently increase in size and already contain billions of parameters. As a result, state-of-the-art text-to-image models are becoming less accessible in practice, especially in resource-limited environments. Post-training quantization (PTQ) tackles this issue by compressing the pretrained model weights into lower-bit representations. Recent diffusion quantization techniques primarily rely on uniform scalar quantization, providing decent performance for the models compressed to 4 bits. This work demonstrates that more versatile vector quantization (VQ) may achieve higher compression rates for large-scale text-to-image diffusion models. Specifically, we tailor vector-based PTQ methods to recent billion-scale text-to-image models (SDXL and SDXL-Turbo), and show that the diffusion models of 2B+ parameters compressed to around 3 bits using VQ exhibit the similar image quality and textual alignment as previous 4-bit compression techniques.</li>
</ul>

<h3>Title: LongRecipe: Recipe for Efficient Long Context Generalization in Large Languge Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Hu, Yuliang Liu, Jinman Zhao, Suyuchen Wang, Yan Wang, Wei Shen, Qing Gu, Anh Tuan Luu, See-Kiong Ng, Zhiwei Jiang, Bryan Hooi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00509">https://arxiv.org/abs/2409.00509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00509">https://arxiv.org/pdf/2409.00509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00509]] LongRecipe: Recipe for Efficient Long Context Generalization in Large Languge Models(https://arxiv.org/abs/2409.00509)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) face significant challenges in handling long-context tasks because of their limited effective context window size during pretraining, which restricts their ability to generalize over extended sequences. Meanwhile, extending the context window in LLMs through post-pretraining is highly resource-intensive. To address this, we introduce **LongRecipe**, an efficient training strategy for extending the context window of LLMs, including impactful token analysis, position index transformation, and training optimization strategies. It simulates long-sequence inputs while maintaining training efficiency and significantly improves the model's understanding of long-range dependencies. Experiments on three types of LLMs show that LongRecipe can utilize long sequences while requiring only 30% of the target context window size, and reduces computational training resource over 85% compared to full sequence training. Furthermore, LongRecipe also preserves the original LLM's capabilities in general tasks. Ultimately, *we can extend the effective context window of open-source LLMs from 8k to 128k, achieving performance close to GPT-4 with just one day of dedicated training using a single GPU with 80G memory.* Our code is released at the [link](this https URL).</li>
</ul>

<h3>Title: RevCD -- Reversed Conditional Diffusion for Generalized Zero-Shot Learning</h3>
<ul>
<li><strong>Authors: </strong>William Heyden, Habib Ullah, M. Salman Siddiqui, Fadi Al Machot</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00511">https://arxiv.org/abs/2409.00511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00511">https://arxiv.org/pdf/2409.00511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00511]] RevCD -- Reversed Conditional Diffusion for Generalized Zero-Shot Learning(https://arxiv.org/abs/2409.00511)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>In Generalized Zero-Shot Learning (GZSL), we aim to recognize both seen and unseen categories using a model trained only on seen categories. In computer vision, this translates into a classification problem, where knowledge from seen categories is transferred to unseen categories by exploiting the relationships between visual features and available semantic information, such as text corpora or manual annotations. However, learning this joint distribution is costly and requires one-to-one training with corresponding semantic information. We present a reversed conditional Diffusion-based model (RevCD) that mitigates this issue by generating semantic features synthesized from visual inputs by leveraging Diffusion models' conditional mechanisms. Our RevCD model consists of a cross Hadamard-Addition embedding of a sinusoidal time schedule and a multi-headed visual transformer for attention-guided embeddings. The proposed approach introduces three key innovations. First, we reverse the process of generating semantic space based on visual data, introducing a novel loss function that facilitates more efficient knowledge transfer. Second, we apply Diffusion models to zero-shot learning - a novel approach that exploits their strengths in capturing data complexity. Third, we demonstrate our model's performance through a comprehensive cross-dataset evaluation. The complete code will be available on GitHub.</li>
</ul>

<h3>Title: Plant detection from ultra high resolution remote sensing images: A Semantic Segmentation approach based on fuzzy loss</h3>
<ul>
<li><strong>Authors: </strong>Shivam Pande, Baki Uzun, Florent Guiotte, Thomas Corpetti, Florian Delerue, S√©bastien Lef√®vre</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00513">https://arxiv.org/abs/2409.00513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00513">https://arxiv.org/pdf/2409.00513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00513]] Plant detection from ultra high resolution remote sensing images: A Semantic Segmentation approach based on fuzzy loss(https://arxiv.org/abs/2409.00513)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this study, we tackle the challenge of identifying plant species from ultra high resolution (UHR) remote sensing images. Our approach involves introducing an RGB remote sensing dataset, characterized by millimeter-level spatial resolution, meticulously curated through several field expeditions across a mountainous region in France covering various landscapes. The task of plant species identification is framed as a semantic segmentation problem for its practical and efficient implementation across vast geographical areas. However, when dealing with segmentation masks, we confront instances where distinguishing boundaries between plant species and their background is challenging. We tackle this issue by introducing a fuzzy loss within the segmentation model. Instead of utilizing one-hot encoded ground truth (GT), our model incorporates Gaussian filter refined GT, introducing stochasticity during training. First experimental results obtained on both our UHR dataset and a public dataset are presented, showing the relevance of the proposed methodology, as well as the need for future improvement.</li>
</ul>

<h3>Title: Mapping earth mounds from space</h3>
<ul>
<li><strong>Authors: </strong>Baki Uzun, Shivam Pande, Gwendal Cachin-Bernard, Minh-Tan Pham, S√©bastien Lef√®vre, Rumais Blatrix, Doyle McKey</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00518">https://arxiv.org/abs/2409.00518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00518">https://arxiv.org/pdf/2409.00518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00518]] Mapping earth mounds from space(https://arxiv.org/abs/2409.00518)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Regular patterns of vegetation are considered widespread landscapes, although their global extent has never been estimated. Among them, spotted landscapes are of particular interest in the context of climate change. Indeed, regularly spaced vegetation spots in semi-arid shrublands result from extreme resource depletion and prefigure catastrophic shift of the ecosystem to a homogeneous desert, while termite mounds also producing spotted landscapes were shown to increase robustness to climate change. Yet, their identification at large scale calls for automatic methods, for instance using the popular deep learning framework, able to cope with a vast amount of remote sensing data, e.g., optical satellite imagery. In this paper, we tackle this problem and benchmark some state-of-the-art deep networks on several landscapes and geographical areas. Despite the promising results we obtained, we found that more research is needed to be able to map automatically these earth mounds from space.</li>
</ul>

<h3>Title: EraseDraw: Learning to Insert Objects by Erasing Them from Images</h3>
<ul>
<li><strong>Authors: </strong>Alper Canberk, Maksym Bondarenko, Ege Ozguroglu, Ruoshi Liu, Carl Vondrick</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00522">https://arxiv.org/abs/2409.00522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00522">https://arxiv.org/pdf/2409.00522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00522]] EraseDraw: Learning to Insert Objects by Erasing Them from Images(https://arxiv.org/abs/2409.00522)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Creative processes such as painting often involve creating different components of an image one by one. Can we build a computational model to perform this task? Prior works often fail by making global changes to the image, inserting objects in unrealistic spatial locations, and generating inaccurate lighting details. We observe that while state-of-the-art models perform poorly on object insertion, they can remove objects and erase the background in natural images very well. Inverting the direction of object removal, we obtain high-quality data for learning to insert objects that are spatially, physically, and optically consistent with the surroundings. With this scalable automatic data generation pipeline, we can create a dataset for learning object insertion, which is used to train our proposed text conditioned diffusion model. Qualitative and quantitative experiments have shown that our model achieves state-of-the-art results in object insertion, particularly for in-the-wild images. We show compelling results on diverse insertion prompts and images across various this http URL addition, we automate iterative insertion by combining our insertion model with beam search guided by CLIP.</li>
</ul>

<h3>Title: Post-OCR Text Correction for Bulgarian Historical Documents</h3>
<ul>
<li><strong>Authors: </strong>Angel Beshirov, Milena Dobreva, Dimitar Dimitrov, Momchil Hardalov, Ivan Koychev, Preslav Nakov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00527">https://arxiv.org/abs/2409.00527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00527">https://arxiv.org/pdf/2409.00527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00527]] Post-OCR Text Correction for Bulgarian Historical Documents(https://arxiv.org/abs/2409.00527)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The digitization of historical documents is crucial for preserving the cultural heritage of the society. An important step in this process is converting scanned images to text using Optical Character Recognition (OCR), which can enable further search, information extraction, etc. Unfortunately, this is a hard problem as standard OCR tools are not tailored to deal with historical orthography as well as with challenging layouts. Thus, it is standard to apply an additional text correction step on the OCR output when dealing with such documents. In this work, we focus on Bulgarian, and we create the first benchmark dataset for evaluating the OCR text correction for historical Bulgarian documents written in the first standardized Bulgarian orthography: the Drinov orthography from the 19th century. We further develop a method for automatically generating synthetic data in this orthography, as well as in the subsequent Ivanchev orthography, by leveraging vast amounts of contemporary literature Bulgarian texts. We then use state-of-the-art LLMs and encoder-decoder framework which we augment with diagonal attention loss and copy and coverage mechanisms to improve the post-OCR text correction. The proposed method reduces the errors introduced during recognition and improves the quality of the documents by 25\%, which is an increase of 16\% compared to the state-of-the-art on the ICDAR 2019 Bulgarian dataset. We release our data and code at \url{this https URL}.}</li>
</ul>

<h3>Title: Incremental Open-set Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Sayan Rakshit, Hmrishav Bandyopadhyay, Nibaran Das, Biplab Banerjee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00530">https://arxiv.org/abs/2409.00530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00530">https://arxiv.org/pdf/2409.00530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00530]] Incremental Open-set Domain Adaptation(https://arxiv.org/abs/2409.00530)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Catastrophic forgetting makes neural network models unstable when learning visual domains consecutively. The neural network model drifts to catastrophic forgetting-induced low performance of previously learnt domains when training with new domains. We illuminate this current neural network model weakness and develop a forgetting-resistant incremental learning strategy. Here, we propose a new unsupervised incremental open-set domain adaptation (IOSDA) issue for image classification. Open-set domain adaptation adds complexity to the incremental domain adaptation issue since each target domain has more classes than the Source domain. In IOSDA, the model learns training with domain streams phase by phase in incremented time. Inference uses test data from all target domains without revealing their identities. We proposed IOSDA-Net, a two-stage learning pipeline, to solve the problem. The first module replicates prior domains from random noise using a generative framework and creates a pseudo source domain. In the second step, this pseudo source is adapted to the present target domain. We test our model on Office-Home, DomainNet, and UPRN-RSDA, a newly curated optical remote sensing dataset.</li>
</ul>

<h3>Title: How Does Diverse Interpretability of Textual Prompts Impact Medical Vision-Language Zero-Shot Tasks?</h3>
<ul>
<li><strong>Authors: </strong>Sicheng Wang, Che Liu, Rossella Arcucci</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00543">https://arxiv.org/abs/2409.00543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00543">https://arxiv.org/pdf/2409.00543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00543]] How Does Diverse Interpretability of Textual Prompts Impact Medical Vision-Language Zero-Shot Tasks?(https://arxiv.org/abs/2409.00543)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Recent advancements in medical vision-language pre-training (MedVLP) have significantly enhanced zero-shot medical vision tasks such as image classification by leveraging large-scale medical image-text pair pre-training. However, the performance of these tasks can be heavily influenced by the variability in textual prompts describing the categories, necessitating robustness in MedVLP models to diverse prompt styles. Yet, this sensitivity remains underexplored. In this work, we are the first to systematically assess the sensitivity of three widely-used MedVLP methods to a variety of prompts across 15 different diseases. To achieve this, we designed six unique prompt styles to mirror real clinical scenarios, which were subsequently ranked by interpretability. Our findings indicate that all MedVLP models evaluated show unstable performance across different prompt styles, suggesting a lack of robustness. Additionally, the models' performance varied with increasing prompt interpretability, revealing difficulties in comprehending complex medical concepts. This study underscores the need for further development in MedVLP methodologies to enhance their robustness to diverse zero-shot prompts.</li>
</ul>

<h3>Title: Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors</h3>
<ul>
<li><strong>Authors: </strong>Jacqueline Lammert, Nicole Pfarr, Leonid Kuligin, Sonja Mathes, Tobias Dreyer, Luise Modersohn, Patrick Metzger, Dyke Ferber, Jakob Nikolas Kather, Daniel Truhn, Lisa Christine Adams, Keno Kyrill Bressem, Sebastian Lange, Kristina Schwamborn, Martin Boeker, Marion Kiechle, Ulrich A. Schatz, Holger Bronger, Maximilian Tschochohei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, q-bio.QM, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00544">https://arxiv.org/abs/2409.00544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00544">https://arxiv.org/pdf/2409.00544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00544]] Large Language Models-Enabled Digital Twins for Precision Medicine in Rare Gynecological Tumors(https://arxiv.org/abs/2409.00544)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Rare gynecological tumors (RGTs) present major clinical challenges due to their low incidence and heterogeneity. The lack of clear guidelines leads to suboptimal management and poor prognosis. Molecular tumor boards accelerate access to effective therapies by tailoring treatment based on biomarkers, beyond cancer type. Unstructured data that requires manual curation hinders efficient use of biomarker profiling for therapy matching. This study explores the use of large language models (LLMs) to construct digital twins for precision medicine in RGTs. Our proof-of-concept digital twin system integrates clinical and biomarker data from institutional and published cases (n=21) and literature-derived data (n=655 publications with n=404,265 patients) to create tailored treatment plans for metastatic uterine carcinosarcoma, identifying options potentially missed by traditional, single-source analysis. LLM-enabled digital twins efficiently model individual patient trajectories. Shifting to a biology-based rather than organ-based tumor definition enables personalized care that could advance RGT management and thus enhance patient outcomes.</li>
</ul>

<h3>Title: The Authentication Gap: Higher Education's Widespread Noncompliance with NIST Digital Identity Guidelines</h3>
<ul>
<li><strong>Authors: </strong>Noah Apthorpe, Boen Beavers, Yan Shvartzshnaider, Brett Frischmann</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00546">https://arxiv.org/abs/2409.00546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00546">https://arxiv.org/pdf/2409.00546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00546]] The Authentication Gap: Higher Education's Widespread Noncompliance with NIST Digital Identity Guidelines(https://arxiv.org/abs/2409.00546)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>We examine the authentication practices of a diverse set of 101 colleges and universities in the United States and Canada to determine compliance with five standards in NIST Special Publication 800-63-3 Digital Identity Guidelines. We find widespread noncompliance with standards for password expiration, password composition rules, and knowledge-based authentication. Many institutions still require or recommend noncompliant practices despite years of expert advice and standards to the contrary. Furthermore, we observe that regional and liberal arts colleges have generally lower documented compliance rates than national and global universities, motivating further investment in authentication security at these institutions. These results are a wake-up call that expert cybersecurity recommendations are not sufficiently influencing the policies of higher education institutions, leaving the sector vulnerable to increasingly prevalent ransomware and other cyberattacks.</li>
</ul>

<h3>Title: Data Augmentation for Image Classification using Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Fazle Rahat, M Shifat Hossain, Md Rubel Ahmed, Sumit Kumar Jha, Rickard Ewetz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00547">https://arxiv.org/abs/2409.00547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00547">https://arxiv.org/pdf/2409.00547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00547]] Data Augmentation for Image Classification using Generative AI(https://arxiv.org/abs/2409.00547)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion, generative, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Scaling laws dictate that the performance of AI models is proportional to the amount of available data. Data augmentation is a promising solution to expanding the dataset size. Traditional approaches focused on augmentation using rotation, translation, and resizing. Recent approaches use generative AI models to improve dataset diversity. However, the generative methods struggle with issues such as subject corruption and the introduction of irrelevant artifacts. In this paper, we propose the Automated Generative Data Augmentation (AGA). The framework combines the utility of large language models (LLMs), diffusion models, and segmentation models to augment data. AGA preserves foreground authenticity while ensuring background diversity. Specific contributions include: i) segment and superclass based object extraction, ii) prompt diversity with combinatorial complexity using prompt decomposition, and iii) affine subject manipulation. We evaluate AGA against state-of-the-art (SOTA) techniques on three representative datasets, ImageNet, CUB, and iWildCam. The experimental evaluation demonstrates an accuracy improvement of 15.6% and 23.5% for in and out-of-distribution data compared to baseline models, respectively. There is also a 64.3% improvement in SIC score compared to the baselines.</li>
</ul>

<h3>Title: Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity, and Fairness</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00551">https://arxiv.org/abs/2409.00551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00551">https://arxiv.org/pdf/2409.00551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00551]] Testing and Evaluation of Large Language Models: Correctness, Non-Toxicity, and Fairness(https://arxiv.org/abs/2409.00551)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), such as ChatGPT, have rapidly penetrated into people's work and daily lives over the past few years, due to their extraordinary conversational skills and intelligence. ChatGPT has become the fastest-growing software in terms of user numbers in human history and become an important foundational model for the next generation of artificial intelligence applications. However, the generations of LLMs are not entirely reliable, often producing content with factual errors, biases, and toxicity. Given their vast number of users and wide range of application scenarios, these unreliable responses can lead to many serious negative impacts. This thesis introduces the exploratory works in the field of language model reliability during the PhD study, focusing on the correctness, non-toxicity, and fairness of LLMs from both software testing and natural language processing perspectives. First, to measure the correctness of LLMs, we introduce two testing frameworks, FactChecker and LogicAsker, to evaluate factual knowledge and logical reasoning accuracy, respectively. Second, for the non-toxicity of LLMs, we introduce two works for red-teaming LLMs. Third, to evaluate the fairness of LLMs, we introduce two evaluation frameworks, BiasAsker and XCulturalBench, to measure the social bias and cultural bias of LLMs, respectively.</li>
</ul>

<h3>Title: Multi-Output Distributional Fairness via Post-Processing</h3>
<ul>
<li><strong>Authors: </strong>Gang Li, Qihang Lin, Ayush Ghosh, Tianbao Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00553">https://arxiv.org/abs/2409.00553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00553">https://arxiv.org/pdf/2409.00553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00553]] Multi-Output Distributional Fairness via Post-Processing(https://arxiv.org/abs/2409.00553)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>The post-processing approaches are becoming prominent techniques to enhance machine learning models' fairness because of their intuitiveness, low computational cost, and excellent scalability. However, most existing post-processing methods are designed for task-specific fairness measures and are limited to single-output models. In this paper, we introduce a post-processing method for multi-output models, such as the ones used for multi-task/multi-class classification and representation learning, to enhance a model's distributional parity, a task-agnostic fairness measure. Existing techniques to achieve distributional parity are based on the (inverse) cumulative density function of a model's output, which is limited to single-output models. Extending previous works, our method employs an optimal transport mapping to move a model's outputs across different groups towards their empirical Wasserstein barycenter. An approximation technique is applied to reduce the complexity of computing the exact barycenter and a kernel regression method is proposed for extending this process to out-of-sample data. Our empirical studies, which compare our method to current existing post-processing baselines on multi-task/multi-class classification and representation learning tasks, demonstrate the effectiveness of the proposed approach.</li>
</ul>

<h3>Title: FADE: Few-shot/zero-shot Anomaly Detection Engine using Large Vision-Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yuanwei Li, Elizaveta Ivanova, Martins Bruveris</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00556">https://arxiv.org/abs/2409.00556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00556">https://arxiv.org/pdf/2409.00556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00556]] FADE: Few-shot/zero-shot Anomaly Detection Engine using Large Vision-Language Model(https://arxiv.org/abs/2409.00556)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Automatic image anomaly detection is important for quality inspection in the manufacturing industry. The usual unsupervised anomaly detection approach is to train a model for each object class using a dataset of normal samples. However, a more realistic problem is zero-/few-shot anomaly detection where zero or only a few normal samples are available. This makes the training of object-specific models challenging. Recently, large foundation vision-language models have shown strong zero-shot performance in various downstream tasks. While these models have learned complex relationships between vision and language, they are not specifically designed for the tasks of anomaly detection. In this paper, we propose the Few-shot/zero-shot Anomaly Detection Engine (FADE) which leverages the vision-language CLIP model and adjusts it for the purpose of industrial anomaly detection. Specifically, we improve language-guided anomaly segmentation 1) by adapting CLIP to extract multi-scale image patch embeddings that are better aligned with language and 2) by automatically generating an ensemble of text prompts related to industrial anomaly detection. 3) We use additional vision-based guidance from the query and reference images to further improve both zero-shot and few-shot anomaly detection. On the MVTec-AD (and VisA) dataset, FADE outperforms other state-of-the-art methods in anomaly segmentation with pixel-AUROC of 89.6% (91.5%) in zero-shot and 95.4% (97.5%) in 1-normal-shot. Code is available at this https URL.</li>
</ul>

<h3>Title: Learning to Ask: When LLMs Meet Unclear Instruction</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Wang, Juluan Shi, Chaozheng Wang, Cheryl Lee, Youliang Yuan, Jen-tse Huang, Michael R. Lyu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00557">https://arxiv.org/abs/2409.00557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00557">https://arxiv.org/pdf/2409.00557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00557]] Learning to Ask: When LLMs Meet Unclear Instruction(https://arxiv.org/abs/2409.00557)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Equipped with the capability to call functions, modern large language models (LLMs) can leverage external tools for addressing a range of tasks unattainable through language skills alone. However, the effective execution of these tools relies heavily not just on the advanced capabilities of LLMs but also on precise user instructions, which often cannot be ensured in the real world. To evaluate the performance of LLMs tool-use under imperfect instructions, we meticulously examine the real-world instructions queried from users, analyze the error patterns, and build a challenging tool-use benchmark called Noisy ToolBench (NoisyToolBench). We find that due to the next-token prediction training objective, LLMs tend to arbitrarily generate the missed argument, which may lead to hallucinations and risks. To address this issue, we propose a novel framework, Ask-when-Needed (AwN), which prompts LLMs to ask questions to users whenever they encounter obstacles due to unclear instructions. Moreover, to reduce the manual labor involved in user-LLM interaction and assess LLMs performance in tool utilization from both accuracy and efficiency perspectives, we design an automated evaluation tool named ToolEvaluator. Our experiments demonstrate that the AwN significantly outperforms existing frameworks for tool learning in the NoisyToolBench. We will release all related code and datasets to support future research.</li>
</ul>

<h3>Title: Compositional 3D-aware Video Generation with LLM Director</h3>
<ul>
<li><strong>Authors: </strong>Hanxin Zhu, Tianyu He, Anni Tang, Junliang Guo, Zhibo Chen, Jiang Bian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00558">https://arxiv.org/abs/2409.00558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00558">https://arxiv.org/pdf/2409.00558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00558]] Compositional 3D-aware Video Generation with LLM Director(https://arxiv.org/abs/2409.00558)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>Significant progress has been made in text-to-video generation through the use of powerful generative models and large-scale internet data. However, substantial challenges remain in precisely controlling individual concepts within the generated video, such as the motion and appearance of specific characters and the movement of viewpoints. In this work, we propose a novel paradigm that generates each concept in 3D representation separately and then composes them with priors from Large Language Models (LLM) and 2D diffusion models. Specifically, given an input textual prompt, our scheme consists of three stages: 1) We leverage LLM as the director to first decompose the complex query into several sub-prompts that indicate individual concepts within the video~(\textit{e.g.}, scene, objects, motions), then we let LLM to invoke pre-trained expert models to obtain corresponding 3D representations of concepts. 2) To compose these representations, we prompt multi-modal LLM to produce coarse guidance on the scales and coordinates of trajectories for the objects. 3) To make the generated frames adhere to natural image distribution, we further leverage 2D diffusion priors and use Score Distillation Sampling to refine the composition. Extensive experiments demonstrate that our method can generate high-fidelity videos from text with diverse motion and flexible control over each concept. Project page: \url{this https URL}.</li>
</ul>

<h3>Title: Sparse Mamba: Reinforcing Controllability In Structural State Space Models</h3>
<ul>
<li><strong>Authors: </strong>Emadeldeen Hamdan, Hongyi Pan, Ahmet Enis Cetin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00563">https://arxiv.org/abs/2409.00563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00563">https://arxiv.org/pdf/2409.00563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00563]] Sparse Mamba: Reinforcing Controllability In Structural State Space Models(https://arxiv.org/abs/2409.00563)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>In this article, we introduce the concept of controllability and observability to the M amba architecture in our Sparse-Mamba (S-Mamba) for natural language processing (NLP) applications. The structured state space model (SSM) development in recent studies, such as Mamba and Mamba2, outperformed and solved the computational inefficiency of transformers and large language models (LLMs) on longer sequences in small to medium NLP tasks. The Mamba SSMs architecture drops the need for attention layer or MLB blocks in transformers. However, the current Mamba models do not reinforce the controllability on state space equations in the calculation of A, B, C, and D matrices at each time step, which increase the complexity and the computational cost needed. In this article we show that the number of parameters can be significantly decreased by reinforcing controllability in the state space equations in the proposed Sparse-Mamba (S-Mamba), while maintaining the performance. The controllable n x n state matrix A is sparse and it has only n free parameters. Our novel approach will ensure a controllable system and could be the gate key for Mamba 3.</li>
</ul>

<h3>Title: Enhancing Source Code Security with LLMs: Demystifying The Challenges and Generating Reliable Repairs</h3>
<ul>
<li><strong>Authors: </strong>Nafis Tanveer Islam, Joseph Khoury, Andrew Seong, Elias Bou-Harb, Peyman Najafirad</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00571">https://arxiv.org/abs/2409.00571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00571">https://arxiv.org/pdf/2409.00571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00571]] Enhancing Source Code Security with LLMs: Demystifying The Challenges and Generating Reliable Repairs(https://arxiv.org/abs/2409.00571)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>With the recent unprecedented advancements in Artificial Intelligence (AI) computing, progress in Large Language Models (LLMs) is accelerating rapidly, presenting challenges in establishing clear guidelines, particularly in the field of security. That being said, we thoroughly identify and describe three main technical challenges in the security and software engineering literature that spans the entire LLM workflow, namely; \textbf{\textit{(i)}} Data Collection and Labeling; \textbf{\textit{(ii)}} System Design and Learning; and \textbf{\textit{(iii)}} Performance Evaluation. Building upon these challenges, this paper introduces \texttt{SecRepair}, an instruction-based LLM system designed to reliably \textit{identify}, \textit{describe}, and automatically \textit{repair} vulnerable source code. Our system is accompanied by a list of actionable guides on \textbf{\textit{(i)}} Data Preparation and Augmentation Techniques; \textbf{\textit{(ii)}} Selecting and Adapting state-of-the-art LLM Models; \textbf{\textit{(iii)}} Evaluation Procedures. \texttt{SecRepair} uses a reinforcement learning-based fine-tuning with a semantic reward that caters to the functionality and security aspects of the generated code. Our empirical analysis shows that \texttt{SecRepair} achieves a \textit{12}\% improvement in security code repair compared to other LLMs when trained using reinforcement learning. Furthermore, we demonstrate the capabilities of \texttt{SecRepair} in generating reliable, functional, and compilable security code repairs against real-world test cases using automated evaluation metrics.</li>
</ul>

<h3>Title: McCaD: Multi-Contrast MRI Conditioned, Adaptive Adversarial Diffusion Model for High-Fidelity MRI Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Sanuwani Dayarathna, Kh Tohidul Islam, Bohan Zhuang, Guang Yang, Jianfei Cai, Meng Law, Zhaolin Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00585">https://arxiv.org/abs/2409.00585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00585">https://arxiv.org/pdf/2409.00585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00585]] McCaD: Multi-Contrast MRI Conditioned, Adaptive Adversarial Diffusion Model for High-Fidelity MRI Synthesis(https://arxiv.org/abs/2409.00585)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Magnetic Resonance Imaging (MRI) is instrumental in clinical diagnosis, offering diverse contrasts that provide comprehensive diagnostic information. However, acquiring multiple MRI contrasts is often constrained by high costs, long scanning durations, and patient discomfort. Current synthesis methods, typically focused on single-image contrasts, fall short in capturing the collective nuances across various contrasts. Moreover, existing methods for multi-contrast MRI synthesis often fail to accurately map feature-level information across multiple imaging contrasts. We introduce McCaD (Multi-Contrast MRI Conditioned Adaptive Adversarial Diffusion), a novel framework leveraging an adversarial diffusion model conditioned on multiple contrasts for high-fidelity MRI synthesis. McCaD significantly enhances synthesis accuracy by employing a multi-scale, feature-guided mechanism, incorporating denoising and semantic encoders. An adaptive feature maximization strategy and a spatial feature-attentive loss have been introduced to capture more intrinsic features across multiple contrasts. This facilitates a precise and comprehensive feature-guided denoising process. Extensive experiments on tumor and healthy multi-contrast MRI datasets demonstrated that the McCaD outperforms state-of-the-art baselines quantitively and qualitatively. The code is provided with supplementary materials.</li>
</ul>

<h3>Title: Change-Aware Siamese Network for Surface Defects Segmentation under Complex Background</h3>
<ul>
<li><strong>Authors: </strong>Biyuan Liu, Huaixin Chen, Huiyao Zhan, Sijie Luo, Zhou Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00589">https://arxiv.org/abs/2409.00589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00589">https://arxiv.org/pdf/2409.00589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00589]] Change-Aware Siamese Network for Surface Defects Segmentation under Complex Background(https://arxiv.org/abs/2409.00589)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Despite the eye-catching breakthroughs achieved by deep visual networks in detecting region-level surface defects, the challenge of high-quality pixel-wise defect detection remains due to diverse defect appearances and data scarcity. To avoid over-reliance on defect appearance and achieve accurate defect segmentation, we proposed a change-aware Siamese network that solves the defect segmentation in a change detection framework. A novel multi-class balanced contrastive loss is introduced to guide the Transformer-based encoder, which enables encoding diverse categories of defects as the unified class-agnostic difference between defect and defect-free images. The difference presented by a distance map is then skip-connected to the change-aware decoder to assist in the location of both inter-class and out-of-class pixel-wise defects. In addition, we proposed a synthetic dataset with multi-class liquid crystal display (LCD) defects under a complex and disjointed background context, to demonstrate the advantages of change-based modeling over appearance-based modeling for defect segmentation. In our proposed dataset and two public datasets, our model achieves superior performances than the leading semantic segmentation methods, while maintaining a relatively small model size. Moreover, our model achieves a new state-of-the-art performance compared to the semi-supervised approaches in various supervision settings.</li>
</ul>

<h3>Title: Attention-Guided Multi-scale Interaction Network for Face Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Xujie Wan, Wenjie Li, Guangwei Gao, Huimin Lu, Jian Yang, Chia-Wen Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00591">https://arxiv.org/abs/2409.00591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00591">https://arxiv.org/pdf/2409.00591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00591]] Attention-Guided Multi-scale Interaction Network for Face Super-Resolution(https://arxiv.org/abs/2409.00591)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Recently, CNN and Transformer hybrid networks demonstrated excellent performance in face super-resolution (FSR) tasks. Since numerous features at different scales in hybrid networks, how to fuse these multi-scale features and promote their complementarity is crucial for enhancing FSR. However, existing hybrid network-based FSR methods ignore this, only simply combining the Transformer and CNN. To address this issue, we propose an attention-guided Multi-scale interaction network (AMINet), which contains local and global feature interactions as well as encoder-decoder phases feature interactions. Specifically, we propose a Local and Global Feature Interaction Module (LGFI) to promote fusions of global features and different receptive fields' local features extracted by our Residual Depth Feature Extraction Module (RDFE). Additionally, we propose a Selective Kernel Attention Fusion Module (SKAF) to adaptively select fusions of different features within LGFI and encoder-decoder phases. Our above design allows the free flow of multi-scale features from within modules and between encoder and decoder, which can promote the complementarity of different scale features to enhance FSR. Comprehensive experiments confirm that our method consistently performs well with less computational consumption and faster inference.</li>
</ul>

<h3>Title: Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bang An, Sicheng Zhu, Ruiyi Zhang, Michael-Andrei Panaitescu-Liess, Yuancheng Xu, Furong Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00598">https://arxiv.org/abs/2409.00598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00598">https://arxiv.org/pdf/2409.00598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00598]] Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models(https://arxiv.org/abs/2409.00598)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Safety-aligned large language models (LLMs) sometimes falsely refuse pseudo-harmful prompts, like "how to kill a mosquito," which are actually harmless. Frequent false refusals not only frustrate users but also provoke a public backlash against the very values alignment seeks to protect. In this paper, we propose the first method to auto-generate diverse, content-controlled, and model-dependent pseudo-harmful prompts. Using this method, we construct an evaluation dataset called PHTest, which is ten times larger than existing datasets, covers more false refusal patterns, and separately labels controversial prompts. We evaluate 20 LLMs on PHTest, uncovering new insights due to its scale and labeling. Our findings reveal a trade-off between minimizing false refusals and improving safety against jailbreak attacks. Moreover, we show that many jailbreak defenses significantly increase the false refusal rates, thereby undermining usability. Our method and dataset can help developers evaluate and fine-tune safer and more usable LLMs. Our code and dataset are available at this https URL</li>
</ul>

<h3>Title: Style Transfer: From Stitching to Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Xinhe Xu, Zhuoer Wang, Yihan Zhang, Yizhou Liu, Zhaoyue Wang, Zhihao Xu, Muhan Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00606">https://arxiv.org/abs/2409.00606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00606">https://arxiv.org/pdf/2409.00606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00606]] Style Transfer: From Stitching to Neural Networks(https://arxiv.org/abs/2409.00606)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This article compares two style transfer methods in image processing: the traditional method, which synthesizes new images by stitching together small patches from existing images, and a modern machine learning-based approach that uses a segmentation network to isolate foreground objects and apply style transfer solely to the background. The traditional method excels in creating artistic abstractions but can struggle with seamlessness, whereas the machine learning method preserves the integrity of foreground elements while enhancing the background, offering improved aesthetic quality and computational efficiency. Our study indicates that machine learning-based methods are more suited for real-world applications where detail preservation in foreground elements is essential.</li>
</ul>

<h3>Title: TinyAgent: Function Calling at the Edge</h3>
<ul>
<li><strong>Authors: </strong>Lutfi Eren Erdogan, Nicholas Lee, Siddharth Jha, Sehoon Kim, Ryan Tabrizi, Suhong Moon, Coleman Hooper, Gopala Anumanchipalli, Kurt Keutzer, Amir Gholami</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00608">https://arxiv.org/abs/2409.00608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00608">https://arxiv.org/pdf/2409.00608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00608]] TinyAgent: Function Calling at the Edge(https://arxiv.org/abs/2409.00608)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent large language models (LLMs) have enabled the development of advanced agentic systems that can integrate various tools and APIs to fulfill user queries through function calling. However, the deployment of these LLMs on the edge has not been explored since they typically require cloud-based infrastructure due to their substantial model size and computational demands. To this end, we present TinyAgent, an end-to-end framework for training and deploying task-specific small language model agents capable of function calling for driving agentic systems at the edge. We first show how to enable accurate function calling for open-source models via the LLMCompiler framework. We then systematically curate a high-quality dataset for function calling, which we use to fine-tune two small language models, TinyAgent-1.1B and 7B. For efficient inference, we introduce a novel tool retrieval method to reduce the input prompt length and utilize quantization to further accelerate the inference speed. As a driving application, we demonstrate a local Siri-like system for Apple's MacBook that can execute user commands through text or voice input. Our results show that our models can achieve, and even surpass, the function-calling capabilities of larger models like GPT-4-Turbo, while being fully deployed at the edge. We open-source our dataset, models, and installable package and provide a demo video for our MacBook assistant agent.</li>
</ul>

<h3>Title: DAMe: Personalized Federated Social Event Detection with Dual Aggregation Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyan Yu, Yifan Wei, Pu Li, Shuaishuai Zhou, Hao Peng, Li Sun, Liehuang Zhu, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00614">https://arxiv.org/abs/2409.00614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00614">https://arxiv.org/pdf/2409.00614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00614]] DAMe: Personalized Federated Social Event Detection with Dual Aggregation Mechanism(https://arxiv.org/abs/2409.00614)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Training social event detection models through federated learning (FedSED) aims to improve participants' performance on the task. However, existing federated learning paradigms are inadequate for achieving FedSED's objective and exhibit limitations in handling the inherent heterogeneity in social data. This paper proposes a personalized federated learning framework with a dual aggregation mechanism for social event detection, namely DAMe. We present a novel local aggregation strategy utilizing Bayesian optimization to incorporate global knowledge while retaining local characteristics. Moreover, we introduce a global aggregation strategy to provide clients with maximum external knowledge of their preferences. In addition, we incorporate a global-local event-centric constraint to prevent local overfitting and ``client-drift''. Experiments within a realistic simulation of a natural federated setting, utilizing six social event datasets spanning six languages and two social media platforms, along with an ablation study, have demonstrated the effectiveness of the proposed framework. Further robustness analyses have shown that DAMe is resistant to injection attacks.</li>
</ul>

<h3>Title: Does Knowledge Localization Hold True? Surprising Differences Between Entity and Relation Perspectives in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yifan Wei, Xiaoyan Yu, Yixuan Weng, Huanhuan Ma, Yuanzhe Zhang, Jun Zhao, Kang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00617">https://arxiv.org/abs/2409.00617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00617">https://arxiv.org/pdf/2409.00617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00617]] Does Knowledge Localization Hold True? Surprising Differences Between Entity and Relation Perspectives in Language Models(https://arxiv.org/abs/2409.00617)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models encapsulate knowledge and have demonstrated superior performance on various natural language processing tasks. Recent studies have localized this knowledge to specific model parameters, such as the MLP weights in intermediate layers. This study investigates the differences between entity and relational knowledge through knowledge editing. Our findings reveal that entity and relational knowledge cannot be directly transferred or mapped to each other. This result is unexpected, as logically, modifying the entity or the relation within the same knowledge triplet should yield equivalent outcomes. To further elucidate the differences between entity and relational knowledge, we employ causal analysis to investigate how relational knowledge is stored in pre-trained models. Contrary to prior research suggesting that knowledge is stored in MLP weights, our experiments demonstrate that relational knowledge is also significantly encoded in attention modules. This insight highlights the multifaceted nature of knowledge storage in language models, underscoring the complexity of manipulating specific types of knowledge within these models.</li>
</ul>

<h3>Title: YOLOO: You Only Learn from Others Once</h3>
<ul>
<li><strong>Authors: </strong>Lipeng Gu, Mingqiang Wei, Xuefeng Yan, Dingkun Zhu, Wei Zhao, Haoran Xie, Yong-Jin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00618">https://arxiv.org/abs/2409.00618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00618">https://arxiv.org/pdf/2409.00618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00618]] YOLOO: You Only Learn from Others Once(https://arxiv.org/abs/2409.00618)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-modal 3D multi-object tracking (MOT) typically necessitates extensive computational costs of deep neural networks (DNNs) to extract multi-modal representations. In this paper, we propose an intriguing question: May we learn from multiple modalities only during training to avoid multi-modal input in the inference phase? To answer it, we propose \textbf{YOLOO}, a novel multi-modal 3D MOT paradigm: You Only Learn from Others Once. YOLOO empowers the point cloud encoder to learn a unified tri-modal representation (UTR) from point clouds and other modalities, such as images and textual cues, all at once. Leveraging this UTR, YOLOO achieves efficient tracking solely using the point cloud encoder without compromising its performance, fundamentally obviating the need for computationally intensive DNNs. Specifically, YOLOO includes two core components: a unified tri-modal encoder (UTEnc) and a flexible geometric constraint (F-GC) module. UTEnc integrates a point cloud encoder with image and text encoders adapted from pre-trained CLIP. It seamlessly fuses point cloud information with rich visual-textual knowledge from CLIP into the point cloud encoder, yielding highly discriminative UTRs that facilitate the association between trajectories and detections. Additionally, F-GC filters out mismatched associations with similar representations but significant positional discrepancies. It further enhances the robustness of UTRs without requiring any scene-specific tuning, addressing a key limitation of customized geometric constraints (e.g., 3D IoU). Lastly, high-quality 3D trajectories are generated by a traditional data association component. By integrating these advancements into a multi-modal 3D MOT scheme, our YOLOO achieves substantial gains in both robustness and efficiency.</li>
</ul>

<h3>Title: Enhancing Vectorized Map Perception with Historical Rasterized Maps</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Zhang, Guangwei Liu, Zihao Liu, Ningyi Xu, Yunhui Liu, Ji Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00620">https://arxiv.org/abs/2409.00620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00620">https://arxiv.org/pdf/2409.00620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00620]] Enhancing Vectorized Map Perception with Historical Rasterized Maps(https://arxiv.org/abs/2409.00620)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In autonomous driving, there is growing interest in end-to-end online vectorized map perception in bird's-eye-view (BEV) space, with an expectation that it could replace traditional high-cost offline high-definition (HD) maps. However, the accuracy and robustness of these methods can be easily compromised in challenging conditions, such as occlusion or adverse weather, when relying only on onboard sensors. In this paper, we propose HRMapNet, leveraging a low-cost Historical Rasterized Map to enhance online vectorized map perception. The historical rasterized map can be easily constructed from past predicted vectorized results and provides valuable complementary information. To fully exploit a historical map, we propose two novel modules to enhance BEV features and map element queries. For BEV features, we employ a feature aggregation module to encode features from both onboard images and the historical map. For map element queries, we design a query initialization module to endow queries with priors from the historical map. The two modules contribute to leveraging map information in online perception. Our HRMapNet can be integrated with most online vectorized map perception methods. We integrate it in two state-of-the-art methods, significantly improving their performance on both the nuScenes and Argoverse 2 datasets. The source code is released at this https URL.</li>
</ul>

<h3>Title: Assessing the Impact of Upselling in Online Fantasy Sports</h3>
<ul>
<li><strong>Authors: </strong>Aayush Chaudhary</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00629">https://arxiv.org/abs/2409.00629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00629">https://arxiv.org/pdf/2409.00629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00629]] Assessing the Impact of Upselling in Online Fantasy Sports(https://arxiv.org/abs/2409.00629)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This study explores the impact of upselling on user engagement. We model users' deposit behaviour on the fantasy sports platform Dream11. Subsequently, we develop an experimental framework to evaluate the effect of upselling using an intensity parameter. Our live experiments on user deposit behaviour reveal decreased user recall with heightened upselling intensity. Our findings indicate that increased upselling intensity improves user deposit metrics and concurrently diminishes user satisfaction and conversion rates. We conduct robust counterfactual analysis and train causal meta-learners to personalise users' upselling intensity levels to reach an optimal trade-off point.</li>
</ul>

<h3>Title: Make Your ViT-based Multi-view 3D Detectors Faster via Token Compression</h3>
<ul>
<li><strong>Authors: </strong>Dingyuan Zhang, Dingkang Liang, Zichang Tan, Xiaoqing Ye, Cheng Zhang, Jingdong Wang, Xiang Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00633">https://arxiv.org/abs/2409.00633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00633">https://arxiv.org/pdf/2409.00633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00633]] Make Your ViT-based Multi-view 3D Detectors Faster via Token Compression(https://arxiv.org/abs/2409.00633)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Slow inference speed is one of the most crucial concerns for deploying multi-view 3D detectors to tasks with high real-time requirements like autonomous driving. Although many sparse query-based methods have already attempted to improve the efficiency of 3D detectors, they neglect to consider the backbone, especially when using Vision Transformers (ViT) for better performance. To tackle this problem, we explore the efficient ViT backbones for multi-view 3D detection via token compression and propose a simple yet effective method called TokenCompression3D (ToC3D). By leveraging history object queries as foreground priors of high quality, modeling 3D motion information in them, and interacting them with image tokens through the attention mechanism, ToC3D can effectively determine the magnitude of information densities of image tokens and segment the salient foreground tokens. With the introduced dynamic router design, ToC3D can weigh more computing resources to important foreground tokens while compressing the information loss, leading to a more efficient ViT-based multi-view 3D detector. Extensive results on the large-scale nuScenes dataset show that our method can nearly maintain the performance of recent SOTA with up to 30% inference speedup, and the improvements are consistent after scaling up the ViT and input resolution. The code will be made at this https URL.</li>
</ul>

<h3>Title: Seed-to-Seed: Image Translation in Diffusion Seed Space</h3>
<ul>
<li><strong>Authors: </strong>Or Greenberg, Eran Kishon, Dani Lischinski</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00654">https://arxiv.org/abs/2409.00654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00654">https://arxiv.org/pdf/2409.00654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00654]] Seed-to-Seed: Image Translation in Diffusion Seed Space(https://arxiv.org/abs/2409.00654)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Seed-to-Seed Translation (StS), a novel approach for Image-to-Image Translation using diffusion models (DMs), aimed at translations that require close adherence to the structure of the source image. In contrast to existing methods that modify images during the diffusion sampling process, we leverage the semantic information encoded within the space of inverted seeds of a pretrained DM, dubbed as the seed-space. We demonstrate that inverted seeds can be used for discriminative tasks, and can also be manipulated to achieve desired transformations in an unpaired image-to-image translation setting. Our method involves training an sts-GAN, an unpaired translation model between source and target seeds, based on CycleGAN. The final translated images are obtained by initiating the DM's sampling process from the translated seeds. A ControlNet is used to ensure the structural preservation of the input image. We demonstrate the effectiveness of our approach for the task of translating automotive scenes, showcasing superior performance compared to existing GAN-based and diffusion-based methods, as well as for several other unpaired image translation tasks. Our approach offers a fresh perspective on leveraging the semantic information encoded within the seed-space of pretrained DMs for effective image editing and manipulation.</li>
</ul>

<h3>Title: Comprehensive Botnet Detection by Mitigating Adversarial Attacks, Navigating the Subtleties of Perturbation Distances and Fortifying Predictions with Conformal Layers</h3>
<ul>
<li><strong>Authors: </strong>Rahul Yumlembam, Biju Issac, Seibu Mary Jacob, Longzhi Yang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00667">https://arxiv.org/abs/2409.00667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00667">https://arxiv.org/pdf/2409.00667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00667]] Comprehensive Botnet Detection by Mitigating Adversarial Attacks, Navigating the Subtleties of Perturbation Distances and Fortifying Predictions with Conformal Layers(https://arxiv.org/abs/2409.00667)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, generative</a></li>
<li><strong>Abstract: </strong>Botnets are computer networks controlled by malicious actors that present significant cybersecurity challenges. They autonomously infect, propagate, and coordinate to conduct cybercrimes, necessitating robust detection methods. This research addresses the sophisticated adversarial manipulations posed by attackers, aiming to undermine machine learning-based botnet detection systems. We introduce a flow-based detection approach, leveraging machine learning and deep learning algorithms trained on the ISCX and ISOT datasets. The detection algorithms are optimized using the Genetic Algorithm and Particle Swarm Optimization to obtain a baseline detection method. The Carlini & Wagner (C&W) attack and Generative Adversarial Network (GAN) generate deceptive data with subtle perturbations, targeting each feature used for classification while preserving their semantic and syntactic relationships, which ensures that the adversarial samples retain meaningfulness and realism. An in-depth analysis of the required L2 distance from the original sample for the malware sample to misclassify is performed across various iteration checkpoints, showing different levels of misclassification at different L2 distances of the Pertrub sample from the original sample. Our work delves into the vulnerability of various models, examining the transferability of adversarial examples from a Neural Network surrogate model to Tree-based algorithms. Subsequently, models that initially misclassified the perturbed samples are retrained, enhancing their resilience and detection capabilities. In the final phase, a conformal prediction layer is integrated, significantly rejecting incorrect predictions, of 58.20 % in the ISCX dataset and 98.94 % in the ISOT dataset.</li>
</ul>

<h3>Title: Study of Dropout in PointPillars with 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxiang Sun, Geoffrey Fox</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00673">https://arxiv.org/abs/2409.00673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00673">https://arxiv.org/pdf/2409.00673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00673]] Study of Dropout in PointPillars with 3D Object Detection(https://arxiv.org/abs/2409.00673)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>3D object detection is critical for autonomous driving, leveraging deep learning techniques to interpret LiDAR data. The PointPillars architecture is a prominent model in this field, distinguished by its efficient use of LiDAR data. This study provides an analysis of enhancing the performance of PointPillars model under various dropout rates to address overfitting and improve model generalization. Dropout, a regularization technique, involves randomly omitting neurons during training, compelling the network to learn robust and diverse features. We systematically compare the effects of different enhancement techniques on the model's regression performance during training and its accuracy, measured by Average Precision (AP) and Average Orientation Similarity (AOS). Our findings offer insights into the optimal enhancements, contributing to improved 3D object detection in autonomous driving applications.</li>
</ul>

<h3>Title: Accurate Forgetting for All-in-One Image Restoration Model</h3>
<ul>
<li><strong>Authors: </strong>Xin Su, Zhuoran Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00685">https://arxiv.org/abs/2409.00685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00685">https://arxiv.org/pdf/2409.00685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00685]] Accurate Forgetting for All-in-One Image Restoration Model(https://arxiv.org/abs/2409.00685)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, robust</a></li>
<li><strong>Abstract: </strong>Privacy protection has always been an ongoing topic, especially for AI. Currently, a low-cost scheme called Machine Unlearning forgets the private data remembered in the model. Specifically, given a private dataset and a trained neural network, we need to use e.g. pruning, fine-tuning, and gradient ascent to remove the influence of the private dataset on the neural network. Inspired by this, we try to use this concept to bridge the gap between the fields of image restoration and security, creating a new research idea. We propose the scene for the All-In-One model (a neural network that restores a wide range of degraded information), where a given dataset such as haze, or rain, is private and needs to be eliminated from the influence of it on the trained model. Notably, we find great challenges in this task to remove the influence of sensitive data while ensuring that the overall model performance remains robust, which is akin to directing a symphony orchestra without specific instruments while keeping the playing soothing. Here we explore a simple but effective approach: Instance-wise Unlearning through the use of adversarial examples and gradient ascent techniques. Our approach is a low-cost solution compared to the strategy of retraining the model from scratch, where the gradient ascent trick forgets the specified data and the performance of the adversarial sample maintenance model is robust. Through extensive experimentation on two popular unified image restoration models, we show that our approach effectively preserves knowledge of remaining data while unlearning a given degradation type.</li>
</ul>

<h3>Title: Curriculum Prompting Foundation Models for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xiuqi Zheng, Yuhang Zhang, Haoran Zhang, Hongrui Liang, Xueqi Bao, Zhuqing Jiang, Qicheng Lao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00695">https://arxiv.org/abs/2409.00695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00695">https://arxiv.org/pdf/2409.00695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00695]] Curriculum Prompting Foundation Models for Medical Image Segmentation(https://arxiv.org/abs/2409.00695)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Adapting large pre-trained foundation models, e.g., SAM, for medical image segmentation remains a significant challenge. A crucial step involves the formulation of a series of specialized prompts that incorporate specific clinical instructions. Past works have been heavily reliant on a singular type of prompt for each instance, necessitating manual input of an ideally correct prompt, which is less efficient. To tackle this issue, we propose to utilize prompts of different granularity, which are sourced from original images to provide a broader scope of clinical insights. However, combining prompts of varying types can pose a challenge due to potential conflicts. In response, we have designed a coarse-to-fine mechanism, referred to as curriculum prompting, that progressively integrates prompts of different types. Through extensive experiments on three public medical datasets across various modalities, we demonstrate the effectiveness of our proposed approach, which not only automates the prompt generation process but also yields superior performance compared to other SAM-based medical image segmentation methods. Code is available at: this https URL.</li>
</ul>

<h3>Title: Polyrating: A Cost-Effective and Bias-Aware Rating System for LLM Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Jasper Dekoninck, Maximilian Baader, Martin Vechev</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00696">https://arxiv.org/abs/2409.00696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00696">https://arxiv.org/pdf/2409.00696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00696]] Polyrating: A Cost-Effective and Bias-Aware Rating System for LLM Evaluation(https://arxiv.org/abs/2409.00696)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Rating-based human evaluation has become an essential tool to accurately evaluate the impressive performance of Large language models (LLMs). However, current rating systems suffer from several critical limitations. Specifically, they fail to account for human biases that significantly influence evaluation results, require large and expensive preference datasets to obtain accurate ratings, and do not facilitate meaningful comparisons of model ratings across different tasks. To address these issues, we introduce Polyrating, an expressive and flexible rating system based on maximum a posteriori estimation that enables a more nuanced and thorough analysis of model performance at lower costs. Polyrating can detect and quantify biases affecting human preferences, ensuring fairer model comparisons. Furthermore, Polyrating can reduce the cost of human evaluations by up to $41\%$ for new models and up to $77\%$ for new tasks by leveraging existing benchmark scores. Lastly, Polyrating enables direct comparisons of ratings across different tasks, providing a comprehensive understanding of an LLMs' strengths, weaknesses, and relative performance across different applications.</li>
</ul>

<h3>Title: ReMOVE: A Reference-free Metric for Object Erasure</h3>
<ul>
<li><strong>Authors: </strong>Aditya Chandrasekar, Goirik Chakrabarty, Jai Bardhan, Ramya Hebbalaguppe, Prathosh AP</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00707">https://arxiv.org/abs/2409.00707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00707">https://arxiv.org/pdf/2409.00707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00707]] ReMOVE: A Reference-free Metric for Object Erasure(https://arxiv.org/abs/2409.00707)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce $\texttt{ReMOVE}$, a novel reference-free metric for assessing object erasure efficacy in diffusion-based image editing models post-generation. Unlike existing measures such as LPIPS and CLIPScore, $\texttt{ReMOVE}$ addresses the challenge of evaluating inpainting without a reference image, common in practical scenarios. It effectively distinguishes between object removal and replacement. This is a key issue in diffusion models due to stochastic nature of image generation. Traditional metrics fail to align with the intuitive definition of inpainting, which aims for (1) seamless object removal within masked regions (2) while preserving the background continuity. $\texttt{ReMOVE}$ not only correlates with state-of-the-art metrics and aligns with human perception but also captures the nuanced aspects of the inpainting process, providing a finer-grained evaluation of the generated outputs.</li>
</ul>

<h3>Title: Unveiling the Bandwidth Nightmare: CDN Compression Format Conversion Attacks</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Lin, Zhiwei Lin, Ximeng Liu, Zuobing Ying, Cheng Chen</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00712">https://arxiv.org/abs/2409.00712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00712">https://arxiv.org/pdf/2409.00712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00712]] Unveiling the Bandwidth Nightmare: CDN Compression Format Conversion Attacks(https://arxiv.org/abs/2409.00712)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>Content Delivery Networks (CDNs) are designed to enhance network performance and protect against web attack traffic for their hosting websites. And the HTTP compression request mechanism primarily aims to reduce unnecessary network transfers. However, we find that the specification failed to consider the security risks introduced when CDNs meet compression requests. In this paper, we present a novel HTTP amplification attack, CDN Compression Format Convert (CDN-Convet) Attacks. It allows attackers to massively exhaust not only the outgoing bandwidth of the origin servers deployed behind CDNs but also the bandwidth of CDN surrogate nodes. We examined the CDN-Convet attacks on 11 popular CDNs to evaluate the feasibility and real-world impacts. Our experimental results show that all these CDNs are affected by the CDN-Convet attacks. We have also disclosed our findings to affected CDN providers and have received constructive feedback.</li>
</ul>

<h3>Title: LPUWF-LDM: Enhanced Latent Diffusion Model for Precise Late-phase UWF-FA Generation on Limited Dataset</h3>
<ul>
<li><strong>Authors: </strong>Zhaojie Fang, Xiao Yu, Guanyu Zhou, Ke Zhuang, Yifei Chen, Ruiquan Ge, Changmiao Wang, Gangyong Jia, Qing Wu, Juan Ye, Maimaiti Nuliqiman, Peifang Xu, Ahmed Elazab</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00726">https://arxiv.org/abs/2409.00726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00726">https://arxiv.org/pdf/2409.00726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00726]] LPUWF-LDM: Enhanced Latent Diffusion Model for Precise Late-phase UWF-FA Generation on Limited Dataset(https://arxiv.org/abs/2409.00726)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Ultra-Wide-Field Fluorescein Angiography (UWF-FA) enables precise identification of ocular diseases using sodium fluorescein, which can be potentially harmful. Existing research has developed methods to generate UWF-FA from Ultra-Wide-Field Scanning Laser Ophthalmoscopy (UWF-SLO) to reduce the adverse reactions associated with injections. However, these methods have been less effective in producing high-quality late-phase UWF-FA, particularly in lesion areas and fine details. Two primary challenges hinder the generation of high-quality late-phase UWF-FA: the scarcity of paired UWF-SLO and early/late-phase UWF-FA datasets, and the need for realistic generation at lesion sites and potential blood leakage regions. This study introduces an improved latent diffusion model framework to generate high-quality late-phase UWF-FA from limited paired UWF images. To address the challenges as mentioned earlier, our approach employs a module utilizing Cross-temporal Regional Difference Loss, which encourages the model to focus on the differences between early and late phases. Additionally, we introduce a low-frequency enhanced noise strategy in the diffusion forward process to improve the realism of medical images. To further enhance the mapping capability of the variational autoencoder module, especially with limited datasets, we implement a Gated Convolutional Encoder to extract additional information from conditional images. Our Latent Diffusion Model for Ultra-Wide-Field Late-Phase Fluorescein Angiography (LPUWF-LDM) effectively reconstructs fine details in late-phase UWF-FA and achieves state-of-the-art results compared to other existing methods when working with limited datasets. Our source code is available at: this https URL.</li>
</ul>

<h3>Title: ContextCite: Attributing Model Generation to Context</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Cohen-Wang, Harshay Shah, Kristian Georgiev, Aleksander Madry</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00729">https://arxiv.org/abs/2409.00729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00729">https://arxiv.org/pdf/2409.00729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00729]] ContextCite: Attributing Model Generation to Context(https://arxiv.org/abs/2409.00729)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>How do language models use information provided as context when generating a response? Can we infer whether a particular generated statement is actually grounded in the context, a misinterpretation, or fabricated? To help answer these questions, we introduce the problem of context attribution: pinpointing the parts of the context (if any) that led a model to generate a particular statement. We then present ContextCite, a simple and scalable method for context attribution that can be applied on top of any existing language model. Finally, we showcase the utility of ContextCite through three applications: (1) helping verify generated statements (2) improving response quality by pruning the context and (3) detecting poisoning attacks. We provide code for ContextCite at this https URL.</li>
</ul>

<h3>Title: Generating Physical Dynamics under Priors</h3>
<ul>
<li><strong>Authors: </strong>Zihan Zhou, Xiaoxue Wang, Tianshu Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00730">https://arxiv.org/abs/2409.00730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00730">https://arxiv.org/pdf/2409.00730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00730]] Generating Physical Dynamics under Priors(https://arxiv.org/abs/2409.00730)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generating physically feasible dynamics in a data-driven context is challenging, especially when adhering to physical priors expressed in specific equations or formulas. Existing methodologies often overlook the integration of physical priors, resulting in violation of basic physical laws and suboptimal performance. In this paper, we introduce a novel framework that seamlessly incorporates physical priors into diffusion-based generative models to address this limitation. Our approach leverages two categories of priors: 1) distributional priors, such as roto-translational invariance, and 2) physical feasibility priors, including energy and momentum conservation laws and PDE constraints. By embedding these priors into the generative process, our method can efficiently generate physically realistic dynamics, encompassing trajectories and flows. Empirical evaluations demonstrate that our method produces high-quality dynamics across a diverse array of physical phenomena with remarkable robustness, underscoring its potential to advance data-driven studies in AI4Physics. Our contributions signify a substantial advancement in the field of generative modeling, offering a robust solution to generate accurate and physically consistent dynamics.</li>
</ul>

<h3>Title: A Critical Analysis on Machine Learning Techniques for Video-based Human Activity Recognition of Surveillance Systems: A Review</h3>
<ul>
<li><strong>Authors: </strong>Shahriar Jahan, Roknuzzaman, Md Robiul Islam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00731">https://arxiv.org/abs/2409.00731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00731">https://arxiv.org/pdf/2409.00731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00731]] A Critical Analysis on Machine Learning Techniques for Video-based Human Activity Recognition of Surveillance Systems: A Review(https://arxiv.org/abs/2409.00731)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Upsurging abnormal activities in crowded locations such as airports, train stations, bus stops, shopping malls, etc., urges the necessity for an intelligent surveillance system. An intelligent surveillance system can differentiate between normal and suspicious activities from real-time video analysis that will enable to take appropriate measures regarding the level of an anomaly instantaneously and efficiently. Video-based human activity recognition has intrigued many researchers with its pressing issues and a variety of applications ranging from simple hand gesture recognition to crucial behavior recognition in a surveillance system. This paper provides a critical survey of video-based Human Activity Recognition (HAR) techniques beginning with an examination of basic approaches for detecting and recognizing suspicious behavior followed by a critical analysis of machine learning and deep learning techniques such as Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), Hidden Markov Model (HMM), K-means Clustering etc. A detailed investigation and comparison are done on these learning techniques on the basis of feature extraction techniques, parameter initialization, and optimization algorithms, accuracy, etc. The purpose of this review is to prioritize positive schemes and to assist researchers with emerging advancements in this field's future endeavors. This paper also pragmatically discusses existing challenges in the field of HAR and examines the prospects in the field.</li>
</ul>

<h3>Title: Benign Overfitting for $\alpha$ Sub-exponential Input</h3>
<ul>
<li><strong>Authors: </strong>Kota Okudo, Kei Kobayashi</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00733">https://arxiv.org/abs/2409.00733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00733">https://arxiv.org/pdf/2409.00733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00733]] Benign Overfitting for $\alpha$ Sub-exponential Input(https://arxiv.org/abs/2409.00733)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper investigates the phenomenon of benign overfitting in binary classification problems with heavy-tailed input distributions. We extend the analysis of maximum margin classifiers to $\alpha$ sub-exponential distributions, where $\alpha \in (0,2]$, generalizing previous work that focused on sub-gaussian inputs. Our main result provides generalization error bounds for linear classifiers trained using gradient descent on unregularized logistic loss in this heavy-tailed setting. We prove that under certain conditions on the dimensionality $p$ and feature vector magnitude $\|\mu\|$, the misclassification error of the maximum margin classifier asymptotically approaches the noise level. This work contributes to the understanding of benign overfitting in more robust distribution settings and demonstrates that the phenomenon persists even with heavier-tailed inputs than previously studied.</li>
</ul>

<h3>Title: VPVet: Vetting Privacy Policies of Virtual Reality Apps</h3>
<ul>
<li><strong>Authors: </strong>Yuxia Zhan, Yan Meng, Lu Zhou, Yichang Xiong, Xiaokuan Zhang, Lichuan Ma, Guoxing Chen, Qingqi Pei, Haojin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00740">https://arxiv.org/abs/2409.00740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00740">https://arxiv.org/pdf/2409.00740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00740]] VPVet: Vetting Privacy Policies of Virtual Reality Apps(https://arxiv.org/abs/2409.00740)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Virtual reality (VR) apps can harvest a wider range of user data than web/mobile apps running on personal computers or smartphones. Existing law and privacy regulations emphasize that VR developers should inform users of what data are collected/used/shared (CUS) through privacy policies. However, privacy policies in the VR ecosystem are still in their early stages, and many developers fail to write appropriate privacy policies that comply with regulations and meet user expectations. In this paper, we propose VPVet to automatically vet privacy policy compliance issues for VR apps. VPVet first analyzes the availability and completeness of a VR privacy policy and then refines its analysis based on three key criteria: granularity, minimization, and consistency of CUS statements. Our study establishes the first and currently largest VR privacy policy dataset named VRPP, consisting of privacy policies of 11,923 different VR apps from 10 mainstream platforms. Our vetting results reveal severe privacy issues within the VR ecosystem, including the limited availability and poor quality of privacy policies, along with their coarse granularity, lack of adaptation to VR traits and the inconsistency between CUS statements in privacy policies and their actual behaviors. We open-source VPVet system along with our findings at repository this https URL, aiming to raise awareness within the VR community and pave the way for further research in this field.</li>
</ul>

<h3>Title: Trust And Balance: Few Trusted Samples Pseudo-Labeling and Temperature Scaled Loss for Effective Source-Free Unsupervised Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Andrea Maracani, Lorenzo Rosasco, Lorenzo Natale</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00741">https://arxiv.org/abs/2409.00741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00741">https://arxiv.org/pdf/2409.00741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00741]] Trust And Balance: Few Trusted Samples Pseudo-Labeling and Temperature Scaled Loss for Effective Source-Free Unsupervised Domain Adaptation(https://arxiv.org/abs/2409.00741)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Deep Neural Networks have significantly impacted many computer vision tasks. However, their effectiveness diminishes when test data distribution (target domain) deviates from the one of training data (source domain). In situations where target labels are unavailable and the access to the labeled source domain is restricted due to data privacy or memory constraints, Source-Free Unsupervised Domain Adaptation (SF-UDA) has emerged as a valuable tool. Recognizing the key role of SF-UDA under these constraints, we introduce a novel approach marked by two key contributions: Few Trusted Samples Pseudo-labeling (FTSP) and Temperature Scaled Adaptive Loss (TSAL). FTSP employs a limited subset of trusted samples from the target data to construct a classifier to infer pseudo-labels for the entire domain, showing simplicity and improved accuracy. Simultaneously, TSAL, designed with a unique dual temperature scheduling, adeptly balance diversity, discriminability, and the incorporation of pseudo-labels in the unsupervised adaptation objective. Our methodology, that we name Trust And Balance (TAB) adaptation, is rigorously evaluated on standard datasets like Office31 and Office-Home, and on less common benchmarks such as ImageCLEF-DA and Adaptiope, employing both ResNet50 and ViT-Large architectures. Our results compare favorably with, and in most cases surpass, contemporary state-of-the-art techniques, underscoring the effectiveness of our methodology in the SF-UDA landscape.</li>
</ul>

<h3>Title: Interpretable Clustering: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Lianyu Hu, Mudi Jiang, Junjie Dong, Xinying Liu, Zengyou He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00743">https://arxiv.org/abs/2409.00743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00743">https://arxiv.org/pdf/2409.00743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00743]] Interpretable Clustering: A Survey(https://arxiv.org/abs/2409.00743)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>In recent years, much of the research on clustering algorithms has primarily focused on enhancing their accuracy and efficiency, frequently at the expense of interpretability. However, as these methods are increasingly being applied in high-stakes domains such as healthcare, finance, and autonomous systems, the need for transparent and interpretable clustering outcomes has become a critical concern. This is not only necessary for gaining user trust but also for satisfying the growing ethical and regulatory demands in these fields. Ensuring that decisions derived from clustering algorithms can be clearly understood and justified is now a fundamental requirement. To address this need, this paper provides a comprehensive and structured review of the current state of explainable clustering algorithms, identifying key criteria to distinguish between various methods. These insights can effectively assist researchers in making informed decisions about the most suitable explainable clustering methods for specific application contexts, while also promoting the development and adoption of clustering algorithms that are both efficient and transparent.</li>
</ul>

<h3>Title: Assessing UHD Image Quality from Aesthetics, Distortions, and Saliency</h3>
<ul>
<li><strong>Authors: </strong>Wei Sun, Weixia Zhang, Yuqin Cao, Linhan Cao, Jun Jia, Zijian Chen, Zicheng Zhang, Xiongkuo Min, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00749">https://arxiv.org/abs/2409.00749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00749">https://arxiv.org/pdf/2409.00749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00749]] Assessing UHD Image Quality from Aesthetics, Distortions, and Saliency(https://arxiv.org/abs/2409.00749)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>UHD images, typically with resolutions equal to or higher than 4K, pose a significant challenge for efficient image quality assessment (IQA) algorithms, as adopting full-resolution images as inputs leads to overwhelming computational complexity and commonly used pre-processing methods like resizing or cropping may cause substantial loss of detail. To address this problem, we design a multi-branch deep neural network (DNN) to assess the quality of UHD images from three perspectives: global aesthetic characteristics, local technical distortions, and salient content perception. Specifically, aesthetic features are extracted from low-resolution images downsampled from the UHD ones, which lose high-frequency texture information but still preserve the global aesthetics characteristics. Technical distortions are measured using a fragment image composed of mini-patches cropped from UHD images based on the grid mini-patch sampling strategy. The salient content of UHD images is detected and cropped to extract quality-aware features from the salient regions. We adopt the Swin Transformer Tiny as the backbone networks to extract features from these three perspectives. The extracted features are concatenated and regressed into quality scores by a two-layer multi-layer perceptron (MLP) network. We employ the mean square error (MSE) loss to optimize prediction accuracy and the fidelity loss to optimize prediction monotonicity. Experimental results show that the proposed model achieves the best performance on the UHD-IQA dataset while maintaining the lowest computational complexity, demonstrating its effectiveness and efficiency. Moreover, the proposed model won first prize in ECCV AIM 2024 UHD-IQA Challenge. The code is available at this https URL.</li>
</ul>

<h3>Title: Self-Supervised Vision Transformers for Writer Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Tim Raven, Arthur Matei, Gernot A. Fink</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00751">https://arxiv.org/abs/2409.00751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00751">https://arxiv.org/pdf/2409.00751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00751]] Self-Supervised Vision Transformers for Writer Retrieval(https://arxiv.org/abs/2409.00751)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While methods based on Vision Transformers (ViT) have achieved state-of-the-art performance in many domains, they have not yet been applied successfully in the domain of writer retrieval. The field is dominated by methods using handcrafted features or features extracted from Convolutional Neural Networks. In this work, we bridge this gap and present a novel method that extracts features from a ViT and aggregates them using VLAD encoding. The model is trained in a self-supervised fashion without any need for labels. We show that extracting local foreground features is superior to using the ViT's class token in the context of writer retrieval. We evaluate our method on two historical document collections. We set a new state-at-of-art performance on the Historical-WI dataset (83.1\% mAP), and the HisIR19 dataset (95.0\% mAP). Additionally, we demonstrate that our ViT feature extractor can be directly applied to modern datasets such as the CVL database (98.6\% mAP) without any fine-tuning.</li>
</ul>

<h3>Title: Generalized Multi-hop Traffic Pressure for Heterogeneous Traffic Perimeter Control</h3>
<ul>
<li><strong>Authors: </strong>Xiaocan Li, Xiaoyu Wang, Ilia Smirnov, Scott Sanner, Baher Abdulhai</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00753">https://arxiv.org/abs/2409.00753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00753">https://arxiv.org/pdf/2409.00753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00753]] Generalized Multi-hop Traffic Pressure for Heterogeneous Traffic Perimeter Control(https://arxiv.org/abs/2409.00753)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Perimeter control prevents loss of traffic network capacity due to congestion in urban areas. Homogeneous perimeter control allows all access points to a protected region to have the same maximal permitted inflow. However, homogeneous perimeter control performs poorly when the congestion in the protected region is heterogeneous (e.g., imbalanced demand) since the homogeneous perimeter control does not consider location-specific traffic conditions around the perimeter. When the protected region has spatially heterogeneous congestion, it can often make sense to modulate the perimeter inflow rate to be higher near low-density regions and vice versa for high-density regions. To assist with this modulation, we can leverage the concept of 1-hop traffic pressure to measure intersection-level traffic congestion. However, as we show, 1-hop pressure turns out to be too spatially myopic for perimeter control and hence we formulate multi-hop generalizations of pressure that look ``deeper'' inside the perimeter beyond the entry intersection. In addition, we formulate a simple heterogeneous perimeter control methodology that can leverage this novel multi-hop pressure to redistribute the total permitted inflow provided by the homogeneous perimeter controller. Experimental results show that our heterogeneous perimeter control policies leveraging multi-hop pressure significantly outperform homogeneous perimeter control in scenarios where the origin-destination flows are highly imbalanced with high spatial heterogeneity.</li>
</ul>

<h3>Title: Trusted Unified Feature-Neighborhood Dynamics for Multi-View Classification</h3>
<ul>
<li><strong>Authors: </strong>Haojian Huang, Chuanyu Qin, Zhe Liu, Kaijing Ma, Jin Chen, Han Fang, Chao Ban, Hao Sun, Zhongjiang He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00755">https://arxiv.org/abs/2409.00755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00755">https://arxiv.org/pdf/2409.00755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00755]] Trusted Unified Feature-Neighborhood Dynamics for Multi-View Classification(https://arxiv.org/abs/2409.00755)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-view classification (MVC) faces inherent challenges due to domain gaps and inconsistencies across different views, often resulting in uncertainties during the fusion process. While Evidential Deep Learning (EDL) has been effective in addressing view uncertainty, existing methods predominantly rely on the Dempster-Shafer combination rule, which is sensitive to conflicting evidence and often neglects the critical role of neighborhood structures within multi-view data. To address these limitations, we propose a Trusted Unified Feature-NEighborhood Dynamics (TUNED) model for robust MVC. This method effectively integrates local and global feature-neighborhood (F-N) structures for robust decision-making. Specifically, we begin by extracting local F-N structures within each view. To further mitigate potential uncertainties and conflicts in multi-view fusion, we employ a selective Markov random field that adaptively manages cross-view neighborhood dependencies. Additionally, we employ a shared parameterized evidence extractor that learns global consensus conditioned on local F-N structures, thereby enhancing the global integration of multi-view features. Experiments on benchmark datasets show that our method improves accuracy and robustness over existing approaches, particularly in scenarios with high uncertainty and conflicting views. The code will be made available at this https URL.</li>
</ul>

<h3>Title: Unbalanced Fingerprint Classification for Hybrid Fingerprint Orientation Maps</h3>
<ul>
<li><strong>Authors: </strong>Ravi Prakash, Sinnu Susan Thomas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00779">https://arxiv.org/abs/2409.00779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00779">https://arxiv.org/pdf/2409.00779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00779]] Unbalanced Fingerprint Classification for Hybrid Fingerprint Orientation Maps(https://arxiv.org/abs/2409.00779)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, biometric</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel fingerprint classification technique based on a multi-layered fuzzy logic classifier. We target the cause of missed detection by identifying the fingerprints at an early stage among dry, standard, and wet. Scanned images are classified based on clarity correlated with the proposed feature points. We also propose a novel adaptive algorithm based on eigenvector space for generating new samples to overcome the multiclass imbalance. Proposed methods improve the performance of ensemble learners. It was also found that the new approach performs better than the neural-network based classification methods. Early-stage improvements give a suitable dataset for fingerprint detection models. Leveraging the novel classifier, the best set of `standard' labelled fingerprints is used to generate a unique hybrid fingerprint orientation map (HFOM). We introduce a novel min-rotate max-flow optimization method inspired by the min-cut max-flow algorithm. The unique properties of HFOM generation introduce a new use case for biometric data protection by using HFOM as a virtual proxy of fingerprints.</li>
</ul>

<h3>Title: Zero-Shot Paragraph-level Handwriting Imitation with Latent Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Martin Mayr, Marcel Dreier, Florian Kordon, Mathias Seuret, Jochen Z√∂llner, Fei Wu, Andreas Maier, Vincent Christlein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00786">https://arxiv.org/abs/2409.00786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00786">https://arxiv.org/pdf/2409.00786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00786]] Zero-Shot Paragraph-level Handwriting Imitation with Latent Diffusion Models(https://arxiv.org/abs/2409.00786)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The imitation of cursive handwriting is mainly limited to generating handwritten words or lines. Multiple synthetic outputs must be stitched together to create paragraphs or whole pages, whereby consistency and layout information are lost. To close this gap, we propose a method for imitating handwriting at the paragraph level that also works for unseen writing styles. Therefore, we introduce a modified latent diffusion model that enriches the encoder-decoder mechanism with specialized loss functions that explicitly preserve the style and content. We enhance the attention mechanism of the diffusion model with adaptive 2D positional encoding and the conditioning mechanism to work with two modalities simultaneously: a style image and the target text. This significantly improves the realism of the generated handwriting. Our approach sets a new benchmark in our comprehensive evaluation. It outperforms all existing imitation methods at both line and paragraph levels, considering combined style and content preservation.</li>
</ul>

<h3>Title: The Dark Side of Human Feedback: Poisoning Large Language Models via User Inputs</h3>
<ul>
<li><strong>Authors: </strong>Bocheng Chen, Hanqing Guo, Guangjing Wang, Yuanda Wang, Qiben Yan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00787">https://arxiv.org/abs/2409.00787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00787">https://arxiv.org/pdf/2409.00787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00787]] The Dark Side of Human Feedback: Poisoning Large Language Models via User Inputs(https://arxiv.org/abs/2409.00787)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated great capabilities in natural language understanding and generation, largely attributed to the intricate alignment process using human feedback. While alignment has become an essential training component that leverages data collected from user queries, it inadvertently opens up an avenue for a new type of user-guided poisoning attacks. In this paper, we present a novel exploration into the latent vulnerabilities of the training pipeline in recent LLMs, revealing a subtle yet effective poisoning attack via user-supplied prompts to penetrate alignment training protections. Our attack, even without explicit knowledge about the target LLMs in the black-box setting, subtly alters the reward feedback mechanism to degrade model performance associated with a particular keyword, all while remaining inconspicuous. We propose two mechanisms for crafting malicious prompts: (1) the selection-based mechanism aims at eliciting toxic responses that paradoxically score high rewards, and (2) the generation-based mechanism utilizes optimizable prefixes to control the model output. By injecting 1\% of these specially crafted prompts into the data, through malicious users, we demonstrate a toxicity score up to two times higher when a specific trigger word is used. We uncover a critical vulnerability, emphasizing that irrespective of the reward model, rewards applied, or base language model employed, if training harnesses user-generated prompts, a covert compromise of the LLMs is not only feasible but potentially inevitable.</li>
</ul>

<h3>Title: Comparing Discrete and Continuous Space LLMs for Speech Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yaoxun Xu, Shi-Xiong Zhang, Jianwei Yu, Zhiyong Wu, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00800">https://arxiv.org/abs/2409.00800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00800">https://arxiv.org/pdf/2409.00800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00800]] Comparing Discrete and Continuous Space LLMs for Speech Recognition(https://arxiv.org/abs/2409.00800)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper investigates discrete and continuous speech representations in Large Language Model (LLM)-based Automatic Speech Recognition (ASR), organizing them by feature continuity and training approach into four categories: supervised and unsupervised for both discrete and continuous types. We further classify LLMs based on their input and autoregressive feedback into continuous and discrete-space models. Using specialized encoders and comparative analysis with a Joint-Training-From-Scratch Language Model (JTFS LM) and pre-trained LLaMA2-7b, we provide a detailed examination of their effectiveness. Our work marks the first extensive comparison of speech representations in LLM-based ASR and explores various modeling techniques. We present an open-sourced achievement of a state-of-the-art Word Error Rate (WER) of 1.69\% on LibriSpeech using a HuBERT encoder, offering valuable insights for advancing ASR and natural language processing (NLP) research.</li>
</ul>

<h3>Title: Diffusion based multi-domain neuroimaging harmonization method with preservation of anatomical details</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Lan, Bino A. Varghese, Nasim Sheikh-Bahaei, Farshid Sepehrband, Arthur W Toga, Jeiran Choupan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00807">https://arxiv.org/abs/2409.00807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00807">https://arxiv.org/pdf/2409.00807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00807]] Diffusion based multi-domain neuroimaging harmonization method with preservation of anatomical details(https://arxiv.org/abs/2409.00807)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Multi-center neuroimaging studies face technical variability due to batch differences across sites, which potentially hinders data aggregation and impacts study reliability.Recent efforts in neuroimaging harmonization have aimed to minimize these technical gaps and reduce technical variability across batches. While Generative Adversarial Networks (GAN) has been a prominent method for addressing image harmonization tasks, GAN-harmonized images suffer from artifacts or anatomical distortions. Given the advancements of denoising diffusion probabilistic model which produces high-fidelity images, we have assessed the efficacy of the diffusion model for neuroimaging harmonization. we have demonstrated the diffusion model's superior capability in harmonizing images from multiple domains, while GAN-based methods are limited to harmonizing images between two domains per model. Our experiments highlight that the learned domain invariant anatomical condition reinforces the model to accurately preserve the anatomical details while differentiating batch differences at each diffusion step. Our proposed method has been tested on two public neuroimaging dataset ADNI1 and ABIDE II, yielding harmonization results with consistent anatomy preservation and superior FID score compared to the GAN-based methods. We have conducted multiple analysis including extensive quantitative and qualitative evaluations against the baseline models, ablation study showcasing the benefits of the learned conditions, and improvements in the consistency of perivascular spaces (PVS) segmentation through harmonization.</li>
</ul>

<h3>Title: A Novel Self-Attention-Enabled Weighted Ensemble-Based Convolutional Neural Network Framework for Distributed Denial of Service Attack Classification</h3>
<ul>
<li><strong>Authors: </strong>Kanthimathi S, Shravan Venkatraman, Jayasankar K S, Pranay Jiljith T, Jashwanth R</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00810">https://arxiv.org/abs/2409.00810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00810">https://arxiv.org/pdf/2409.00810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00810]] A Novel Self-Attention-Enabled Weighted Ensemble-Based Convolutional Neural Network Framework for Distributed Denial of Service Attack Classification(https://arxiv.org/abs/2409.00810)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>Distributed Denial of Service (DDoS) attacks are a major concern in network security, as they overwhelm systems with excessive traffic, compromise sensitive data, and disrupt network services. Accurately detecting these attacks is crucial to protecting network infrastructure. Traditional approaches, such as single Convolutional Neural Networks (CNNs) or conventional Machine Learning (ML) algorithms like Decision Trees (DTs) and Support Vector Machines (SVMs), struggle to extract the diverse features needed for precise classification, resulting in suboptimal performance. This research addresses this gap by introducing a novel approach for DDoS attack detection. The proposed method combines three distinct CNN architectures: SA-Enabled CNN with XGBoost, SA-Enabled CNN with LSTM, and SA-Enabled CNN with Random Forest. Each model extracts features at multiple scales, while self-attention mechanisms enhance feature integration and relevance. The weighted ensemble approach ensures that both prominent and subtle features contribute to the final classification, improving adaptability to evolving attack patterns and novel threats. The proposed method achieves a precision of 98.71%, an F1-score of 98.66%, a recall of 98.63%, and an accuracy of 98.69%, outperforming traditional methods and setting a new benchmark in DDoS attack detection. This innovative approach addresses critical limitations in current models and advances the state of the art in network security.</li>
</ul>

<h3>Title: Comparison of Encryption Algorithms for Wearable Devices in IoT Systems</h3>
<ul>
<li><strong>Authors: </strong>Haobo Yang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00816">https://arxiv.org/abs/2409.00816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00816">https://arxiv.org/pdf/2409.00816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00816]] Comparison of Encryption Algorithms for Wearable Devices in IoT Systems(https://arxiv.org/abs/2409.00816)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>The Internet of Things (IoT) expansion has brought a new era of connected devices, including wearable devices like smartwatches and medical monitors, that are becoming integral parts of our daily lives. These devices not only offer innovative functionalities but also generate and transmit plenty of sensitive data, making their security and privacy the primary concerns. Given the unique challenges posed by wearable devices, such as limited computational resources and the need for real-time data processing, encryption stands as a cornerstone for safeguarding the integrity and confidentiality of the data they handle. Various encryption algorithms, each with its own set of advantages and limitations, are available to meet the diverse security and computational needs of wearable IoT devices. As we move into an age where quantum computing could potentially disrupt traditional encryption methods, choosing a suitable encryption algorithm becomes even more critical. This paper explores and evaluates the suitability of different encryption methods in the context of wearable IoT devices, considering current and future security challenges.</li>
</ul>

<h3>Title: Real-Time Weather Image Classification with SVM</h3>
<ul>
<li><strong>Authors: </strong>Eden Ship, Eitan Spivak, Shubham Agarwal, Raz Birman, Ofer Hadar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00821">https://arxiv.org/abs/2409.00821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00821">https://arxiv.org/pdf/2409.00821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00821]] Real-Time Weather Image Classification with SVM(https://arxiv.org/abs/2409.00821)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate classification of weather conditions in images is essential for enhancing the performance of object detection and classification models under varying weather conditions. This paper presents a comprehensive study on classifying weather conditions in images into four categories: rainy, low light, haze, and clear. The motivation for this work stems from the need to improve the reliability and efficiency of automated systems, such as autonomous vehicles and surveillance, which must operate under diverse weather conditions. Misclassification of weather conditions can lead to significant performance degradation in these systems, making robust weather classification crucial. Utilizing the Support Vector Machine (SVM) algorithm, our approach leverages a robust set of features, including brightness, saturation, noise level, blur metric, edge strength, motion blur, Local Binary Patterns (LBP) mean and variance for radii 1, 2, and 3, edges mean and variance, and color histogram mean and variance for blue, green, and red channels. Our SVM-based method achieved a notable accuracy of 92.8%, surpassing typical benchmarks in the literature, which range from 80% to 90% for classical machine learning methods. While deep learning methods can achieve up to 94% accuracy, our approach offers a competitive advantage in terms of computational efficiency and real-time classification capabilities. Detailed analysis of each feature's contribution highlights the effectiveness of texture, color, and edge-related features in capturing the unique characteristics of different weather conditions. This research advances the state-of-the-art in weather image classification and provides insights into the critical features necessary for accurate weather condition differentiation, underscoring the potential of SVMs in practical applications where accuracy is paramount.</li>
</ul>

<h3>Title: A Physical Layer Analysis of Entropy in Delay-Based PUFs Implemented on FPGAs</h3>
<ul>
<li><strong>Authors: </strong>Jim Plusquellic, Jennifer Howard, Ross MacKinnon, Kristianna Hoffman, Eirini Eleni Tsiropoulou, Calvin Chan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00825">https://arxiv.org/abs/2409.00825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00825">https://arxiv.org/pdf/2409.00825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00825]] A Physical Layer Analysis of Entropy in Delay-Based PUFs Implemented on FPGAs(https://arxiv.org/abs/2409.00825)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Physical Unclonable Functions (PUFs) leverage signal variations that occur within the device as a source of entropy. On-chip instrumentation is utilized by some PUF architectures to measure and digitize these variations, which are then processed into bitstrings and secret keys for use in security functions such as authentication and encryption. In many cases, the variations in the measured signals are introduced by a sequence of components in the circuit structure defined by the PUF architecture. In particular, the Hardware-Embedded deLay PUF (HELP) measures delay variations that occur in combinational logic paths on Field Programmable Gate Arrays (FPGAs), which are composed of a set of interconnecting wires (nodes) and look-up tables (LUTs). Previous investigations of variations in these path delays show that it is possible to derive high quality bitstrings, i.e., those which exhibit high levels of uniqueness and randomness across the device population. However, the underlying source and level of variations associated with the constituent components of the paths remain unknown. In this paper, we apply statistical averaging and differencing techniques to derive estimates for the delay variation associated with an FPGA's basic components, namely LUTs and nodes, as a means of fully characterizing the PUF's source of entropy. The analysis is carried out on a set of 50,015 path delay measurements collected from a set of 20 Xilinx Zynq 7020 SoC-class FPGAs, on which 25 identical instances of a functional unit are instantiated, for a total of 500 instances.</li>
</ul>

<h3>Title: Curvy: A Parametric Cross-section based Surface Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Aradhya N. Mathur, Apoorv Khattar, Ojaswa Sharma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CG, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00829">https://arxiv.org/abs/2409.00829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00829">https://arxiv.org/pdf/2409.00829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00829]] Curvy: A Parametric Cross-section based Surface Reconstruction(https://arxiv.org/abs/2409.00829)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this work, we present a novel approach for reconstructing shape point clouds using planar sparse cross-sections with the help of generative modeling. We present unique challenges pertaining to the representation and reconstruction in this problem setting. Most methods in the classical literature lack the ability to generalize based on object class and employ complex mathematical machinery to reconstruct reliable surfaces. We present a simple learnable approach to generate a large number of points from a small number of input cross-sections over a large dataset. We use a compact parametric polyline representation using adaptive splitting to represent the cross-sections and perform learning using a Graph Neural Network to reconstruct the underlying shape in an adaptive manner reducing the dependence on the number of cross-sections provided.</li>
</ul>

<h3>Title: Entropy Loss: An Interpretability Amplifier of 3D Object Detection Network for Intelligent Driving</h3>
<ul>
<li><strong>Authors: </strong>Haobo Yang, Shiyan Zhang, Zhuoyi Yang, Xinyu Zhang, Li Wang, Yifan Tang, Jilong Guo, Jun Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00839">https://arxiv.org/abs/2409.00839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00839">https://arxiv.org/pdf/2409.00839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00839]] Entropy Loss: An Interpretability Amplifier of 3D Object Detection Network for Intelligent Driving(https://arxiv.org/abs/2409.00839)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>With the increasing complexity of the traffic environment, the significance of safety perception in intelligent driving is intensifying. Traditional methods in the field of intelligent driving perception rely on deep learning, which suffers from limited interpretability, often described as a "black box." This paper introduces a novel type of loss function, termed "Entropy Loss," along with an innovative training strategy. Entropy Loss is formulated based on the functionality of feature compression networks within the perception model. Drawing inspiration from communication systems, the information transmission process in a feature compression network is expected to demonstrate steady changes in information volume and a continuous decrease in information entropy. By modeling network layer outputs as continuous random variables, we construct a probabilistic model that quantifies changes in information volume. Entropy Loss is then derived based on these expectations, guiding the update of network parameters to enhance network interpretability. Our experiments indicate that the Entropy Loss training strategy accelerates the training process. Utilizing the same 60 training epochs, the accuracy of 3D object detection models using Entropy Loss on the KITTI test set improved by up to 4.47\% compared to models without Entropy Loss, underscoring the method's efficacy. The implementation code is available at \url{this https URL}.</li>
</ul>

<h3>Title: Universal Approximation of Operators with Transformers and Neural Integral Operators</h3>
<ul>
<li><strong>Authors: </strong>Emanuele Zappala, Maryam Bagherian</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00841">https://arxiv.org/abs/2409.00841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00841">https://arxiv.org/pdf/2409.00841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00841]] Universal Approximation of Operators with Transformers and Neural Integral Operators(https://arxiv.org/abs/2409.00841)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We study the universal approximation properties of transformers and neural integral operators for operators in Banach spaces. In particular, we show that the transformer architecture is a universal approximator of integral operators between H√∂lder spaces. Moreover, we show that a generalized version of neural integral operators, based on the Gavurin integral, are universal approximators of arbitrary operators between Banach spaces. Lastly, we show that a modified version of transformer, which uses Leray-Schauder mappings, is a universal approximator of operators between arbitrary Banach spaces.</li>
</ul>

<h3>Title: Report Cards: Qualitative Evaluation of Language Models Using Natural Language Summaries</h3>
<ul>
<li><strong>Authors: </strong>Blair Yang, Fuyang Cui, Keiran Paster, Jimmy Ba, Pashootan Vaezipoor, Silviu Pitis, Michael R. Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00844">https://arxiv.org/abs/2409.00844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00844">https://arxiv.org/pdf/2409.00844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00844]] Report Cards: Qualitative Evaluation of Language Models Using Natural Language Summaries(https://arxiv.org/abs/2409.00844)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The rapid development and dynamic nature of large language models (LLMs) make it difficult for conventional quantitative benchmarks to accurately assess their capabilities. We propose report cards, which are human-interpretable, natural language summaries of model behavior for specific skills or topics. We develop a framework to evaluate report cards based on three criteria: specificity (ability to distinguish between models), faithfulness (accurate representation of model capabilities), and interpretability (clarity and relevance to humans). We also propose an iterative algorithm for generating report cards without human supervision and explore its efficacy by ablating various design choices. Through experimentation with popular LLMs, we demonstrate that report cards provide insights beyond traditional benchmarks and can help address the need for a more interpretable and holistic evaluation of LLMs.</li>
</ul>

<h3>Title: Image-to-Lidar Relational Distillation for Autonomous Driving Data</h3>
<ul>
<li><strong>Authors: </strong>Anas Mahmoud, Ali Harakeh, Steven Waslander</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00845">https://arxiv.org/abs/2409.00845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00845">https://arxiv.org/pdf/2409.00845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00845]] Image-to-Lidar Relational Distillation for Autonomous Driving Data(https://arxiv.org/abs/2409.00845)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Pre-trained on extensive and diverse multi-modal datasets, 2D foundation models excel at addressing 2D tasks with little or no downstream supervision, owing to their robust representations. The emergence of 2D-to-3D distillation frameworks has extended these capabilities to 3D models. However, distilling 3D representations for autonomous driving datasets presents challenges like self-similarity, class imbalance, and point cloud sparsity, hindering the effectiveness of contrastive distillation, especially in zero-shot learning contexts. Whereas other methodologies, such as similarity-based distillation, enhance zero-shot performance, they tend to yield less discriminative representations, diminishing few-shot performance. We investigate the gap in structure between the 2D and the 3D representations that result from state-of-the-art distillation frameworks and reveal a significant mismatch between the two. Additionally, we demonstrate that the observed structural gap is negatively correlated with the efficacy of the distilled representations on zero-shot and few-shot 3D semantic segmentation. To bridge this gap, we propose a relational distillation framework enforcing intra-modal and cross-modal constraints, resulting in distilled 3D representations that closely capture the structure of the 2D representation. This alignment significantly enhances 3D representation performance over those learned through contrastive distillation in zero-shot segmentation tasks. Furthermore, our relational loss consistently improves the quality of 3D representations in both in-distribution and out-of-distribution few-shot segmentation tasks, outperforming approaches that rely on the similarity loss.</li>
</ul>

<h3>Title: Federated Aggregation of Mallows Rankings: A Comparative Analysis of Borda and Lehmer Coding</h3>
<ul>
<li><strong>Authors: </strong>Jin Sima, Vishal Rana, Olgica Milenkovic</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00848">https://arxiv.org/abs/2409.00848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00848">https://arxiv.org/pdf/2409.00848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00848]] Federated Aggregation of Mallows Rankings: A Comparative Analysis of Borda and Lehmer Coding(https://arxiv.org/abs/2409.00848)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Rank aggregation combines multiple ranked lists into a consensus ranking. In fields like biomedical data sharing, rankings may be distributed and require privacy. This motivates the need for federated rank aggregation protocols, which support distributed, private, and communication-efficient learning across multiple clients with local data. We present the first known federated rank aggregation methods using Borda scoring and Lehmer codes, focusing on the sample complexity for federated algorithms on Mallows distributions with a known scaling factor $\phi$ and an unknown centroid permutation $\sigma_0$. Federated Borda approach involves local client scoring, nontrivial quantization, and privacy-preserving protocols. We show that for $\phi \in [0,1)$, and arbitrary $\sigma_0$ of length $N$, it suffices for each of the $L$ clients to locally aggregate $\max\{C_1(\phi), C_2(\phi)\frac{1}{L}\log \frac{N}{\delta}\}$ rankings, where $C_1(\phi)$ and $C_2(\phi)$ are constants, quantize the result, and send it to the server who can then recover $\sigma_0$ with probability $\geq 1-\delta$. Communication complexity scales as $NL \log N$. Our results represent the first rigorous analysis of Borda's method in centralized and distributed settings under the Mallows model. Federated Lehmer coding approach creates a local Lehmer code for each client, using a coordinate-majority aggregation approach with specialized quantization methods for efficiency and privacy. We show that for $\phi+\phi^2<1+\phi^N$, and arbitrary $\sigma_0$ of length $N$, it suffices for each of the $L$ clients to locally aggregate $\max\{C_3(\phi), C_4(\phi)\frac{1}{L}\log \frac{N}{\delta}\}$ rankings, where $C_3(\phi)$ and $C_4(\phi)$ are constants. Clients send truncated Lehmer coordinate histograms to the server, which can recover $\sigma_0$ with probability $\geq 1-\delta$. Communication complexity is $\sim O(N\log NL\log L)$.</li>
</ul>

<h3>Title: LanguaShrink: Reducing Token Overhead with Psycholinguistics</h3>
<ul>
<li><strong>Authors: </strong>Xuechen Liang, Meiling Tao, Yinghui Xia, Tianyu Shi, Jun Wang, JingSong Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00855">https://arxiv.org/abs/2409.00855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00855">https://arxiv.org/pdf/2409.00855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00855]] LanguaShrink: Reducing Token Overhead with Psycholinguistics(https://arxiv.org/abs/2409.00855)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) improve their capabilities in handling complex tasks, the issues of computational cost and efficiency due to long prompts are becoming increasingly prominent. To accelerate model inference and reduce costs, we propose an innovative prompt compression framework called LanguaShrink. Inspired by the observation that LLM performance depends on the density and position of key information in the input prompts, LanguaShrink leverages psycholinguistic principles and the Ebbinghaus memory curve to achieve task-agnostic prompt compression. This effectively reduces prompt length while preserving essential information. We referred to the training method of OpenChat.The framework introduces part-of-speech priority compression and data distillation techniques, using smaller models to learn compression targets and employing a KL-regularized reinforcement learning strategy for training.\cite{wang2023openchat} Additionally, we adopt a chunk-based compression algorithm to achieve adjustable compression rates. We evaluate our method on multiple datasets, including LongBench, ZeroScrolls, Arxiv Articles, and a newly constructed novel test set. Experimental results show that LanguaShrink maintains semantic similarity while achieving up to 26 times compression. Compared to existing prompt compression methods, LanguaShrink improves end-to-end latency by 1.43 times.</li>
</ul>

<h3>Title: Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Derian Boer, Fabian Koch, Stefan Kramer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00861">https://arxiv.org/abs/2409.00861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00861">https://arxiv.org/pdf/2409.00861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00861]] Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering(https://arxiv.org/abs/2409.00861)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) frequently lack domain-specific knowledge and even fine-tuned models tend to hallucinate. Hence, more reliable models that can include external knowledge are needed. We present a pipeline, 4StepFocus, and specifically a preprocessing step, that can substantially improve the answers of LLMs. This is achieved by providing guided access to external knowledge making use of the model's ability to capture relational context and conduct rudimentary reasoning by themselves. The method narrows down potentially correct answers by triplets-based searches in a semi-structured knowledge base in a direct, traceable fashion, before switching to latent representations for ranking those candidates based on unstructured data. This distinguishes it from related methods that are purely based on latent representations. 4StepFocus consists of the steps: 1) Triplet generation for extraction of relational data by an LLM, 2) substitution of variables in those triplets to narrow down answer candidates employing a knowledge graph, 3) sorting remaining candidates with a vector similarity search involving associated non-structured data, 4) reranking the best candidates by the LLM with background data provided. Experiments on a medical, a product recommendation, and an academic paper search test set demonstrate that this approach is indeed a powerful augmentation. It not only adds relevant traceable background information from information retrieval, but also improves performance considerably in comparison to state-of-the-art methods. This paper presents a novel, largely unexplored direction and therefore provides a wide range of future work opportunities. Used source code is available at this https URL.</li>
</ul>

<h3>Title: Fisher Information guided Purification against Backdoor Attacks</h3>
<ul>
<li><strong>Authors: </strong>Nazmul Karim, Abdullah Al Arafat, Adnan Siraj Rakin, Zhishan Guo, Nazanin Rahnavard</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00863">https://arxiv.org/abs/2409.00863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00863">https://arxiv.org/pdf/2409.00863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00863]] Fisher Information guided Purification against Backdoor Attacks(https://arxiv.org/abs/2409.00863)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, transformer</a></li>
<li><strong>Abstract: </strong>Studies on backdoor attacks in recent years suggest that an adversary can compromise the integrity of a deep neural network (DNN) by manipulating a small set of training samples. Our analysis shows that such manipulation can make the backdoor model converge to a bad local minima, i.e., sharper minima as compared to a benign model. Intuitively, the backdoor can be purified by re-optimizing the model to smoother minima. However, a na√Øve adoption of any optimization targeting smoother minima can lead to sub-optimal purification techniques hampering the clean test accuracy. Hence, to effectively obtain such re-optimization, inspired by our novel perspective establishing the connection between backdoor removal and loss smoothness, we propose Fisher Information guided Purification (FIP), a novel backdoor purification framework. Proposed FIP consists of a couple of novel regularizers that aid the model in suppressing the backdoor effects and retaining the acquired knowledge of clean data distribution throughout the backdoor removal procedure through exploiting the knowledge of Fisher Information Matrix (FIM). In addition, we introduce an efficient variant of FIP, dubbed as Fast FIP, which reduces the number of tunable parameters significantly and obtains an impressive runtime gain of almost $5\times$. Extensive experiments show that the proposed method achieves state-of-the-art (SOTA) performance on a wide range of backdoor defense benchmarks: 5 different tasks -- Image Recognition, Object Detection, Video Action Recognition, 3D point Cloud, Language Generation; 11 different datasets including ImageNet, PASCAL VOC, UCF101; diverse model architectures spanning both CNN and vision transformer; 14 different backdoor attacks, e.g., Dynamic, WaNet, LIRA, ISSBA, etc.</li>
</ul>

<h3>Title: Self-evolving Agents with reflective and memory-augmented abilities</h3>
<ul>
<li><strong>Authors: </strong>Xuechen Liang, Meiling Tao, Yinghui Xia, Tianyu Shi, Jun Wang, JingSong Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00872">https://arxiv.org/abs/2409.00872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00872">https://arxiv.org/pdf/2409.00872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00872]] Self-evolving Agents with reflective and memory-augmented abilities(https://arxiv.org/abs/2409.00872)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have made significant advances in the field of natural language processing, but they still face challenges such as continuous decision-making. In this research, we propose a novel framework by integrating iterative feedback, reflective mechanisms, and a memory optimization mechanism based on the Ebbinghaus forgetting curve, it significantly enhances the agents' capabilities in handling multi-tasking and long-span information.</li>
</ul>

<h3>Title: Equitable Skin Disease Prediction Using Transfer Learning and Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Sajib Acharjee Dip, Kazi Hasan Ibn Arif, Uddip Acharjee Shuvo, Ishtiaque Ahmed Khan, Na Meng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00873">https://arxiv.org/abs/2409.00873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00873">https://arxiv.org/pdf/2409.00873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00873]] Equitable Skin Disease Prediction Using Transfer Learning and Domain Adaptation(https://arxiv.org/abs/2409.00873)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In the realm of dermatology, the complexity of diagnosing skin conditions manually necessitates the expertise of dermatologists. Accurate identification of various skin ailments, ranging from cancer to inflammatory diseases, is paramount. However, existing artificial intelligence (AI) models in dermatology face challenges, particularly in accurately diagnosing diseases across diverse skin tones, with a notable performance gap in darker skin. Additionally, the scarcity of publicly available, unbiased datasets hampers the development of inclusive AI diagnostic tools. To tackle the challenges in accurately predicting skin conditions across diverse skin tones, we employ a transfer-learning approach that capitalizes on the rich, transferable knowledge from various image domains. Our method integrates multiple pre-trained models from a wide range of sources, including general and specific medical images, to improve the robustness and inclusiveness of the skin condition predictions. We rigorously evaluated the effectiveness of these models using the Diverse Dermatology Images (DDI) dataset, which uniquely encompasses both underrepresented and common skin tones, making it an ideal benchmark for assessing our approach. Among all methods, Med-ViT emerged as the top performer due to its comprehensive feature representation learned from diverse image sources. To further enhance performance, we conducted domain adaptation using additional skin image datasets such as HAM10000. This adaptation significantly improved model performance across all models.</li>
</ul>

<h3>Title: User-Specific Dialogue Generation with User Profile-Aware Pre-Training Model and Parameter-Efficient Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Atsushi Otsuka, Kazuya Matsuo, Ryo Ishii, Narichika Nomoto, Hiroaki Sugiyama</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00887">https://arxiv.org/abs/2409.00887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00887">https://arxiv.org/pdf/2409.00887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00887]] User-Specific Dialogue Generation with User Profile-Aware Pre-Training Model and Parameter-Efficient Fine-Tuning(https://arxiv.org/abs/2409.00887)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper addresses user-specific dialogs. In contrast to previous research on personalized dialogue focused on achieving virtual user dialogue as defined by persona descriptions, user-specific dialogue aims to reproduce real-user dialogue beyond persona-based dialogue. Fine-tuning using the target user's dialogue history is an efficient learning method for a user-specific model. However, it is prone to overfitting and model destruction due to the small amount of data. Therefore, we propose a learning method for user-specific models by combining parameter-efficient fine-tuning with a pre-trained dialogue model that includes user profiles. Parameter-efficient fine-tuning adds a small number of parameters to the entire model, so even small amounts of training data can be trained efficiently and are robust to model destruction. In addition, the pre-trained model, which is learned by adding simple prompts for automatically inferred user profiles, can generate speech with enhanced knowledge of the user's profile, even when there is little training data during fine-tuning. In experiments, we compared the proposed model with large-language-model utterance generation using prompts containing users' personal information. Experiments reproducing real users' utterances revealed that the proposed model can generate utterances with higher reproducibility than the compared methods, even with a small model.</li>
</ul>

<h3>Title: A Noise and Edge extraction-based dual-branch method for Shallowfake and Deepfake Localization</h3>
<ul>
<li><strong>Authors: </strong>Deepak Dagar, Dinesh Kumar Vishwakarma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00896">https://arxiv.org/abs/2409.00896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00896">https://arxiv.org/pdf/2409.00896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00896]] A Noise and Edge extraction-based dual-branch method for Shallowfake and Deepfake Localization(https://arxiv.org/abs/2409.00896)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The trustworthiness of multimedia is being increasingly evaluated by advanced Image Manipulation Localization (IML) techniques, resulting in the emergence of the IML field. An effective manipulation model necessitates the extraction of non-semantic differential features between manipulated and legitimate sections to utilize artifacts. This requires direct comparisons between the two regions.. Current models employ either feature approaches based on handcrafted features, convolutional neural networks (CNNs), or a hybrid approach that combines both. Handcrafted feature approaches presuppose tampering in advance, hence restricting their effectiveness in handling various tampering procedures, but CNNs capture semantic information, which is insufficient for addressing manipulation artifacts. In order to address these constraints, we have developed a dual-branch model that integrates manually designed feature noise with conventional CNN features. This model employs a dual-branch strategy, where one branch integrates noise characteristics and the other branch integrates RGB features using the hierarchical ConvNext Module. In addition, the model utilizes edge supervision loss to acquire boundary manipulation information, resulting in accurate localization at the edges. Furthermore, this architecture utilizes a feature augmentation module to optimize and refine the presentation of attributes. The shallowfakes dataset (CASIA, COVERAGE, COLUMBIA, NIST16) and deepfake dataset Faceforensics++ (FF++) underwent thorough testing to demonstrate their outstanding ability to extract features and their superior performance compared to other baseline models. The AUC score achieved an astounding 99%. The model is superior in comparison and easily outperforms the existing state-of-the-art (SoTA) models.</li>
</ul>

<h3>Title: Multi-scale Temporal Fusion Transformer for Incomplete Vehicle Trajectory Prediction</h3>
<ul>
<li><strong>Authors: </strong>Zhanwen Liu, Chao Li, Yang Wang, Nan Yang, Xing Fan, Jiaqi Ma, Xiangmo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00904">https://arxiv.org/abs/2409.00904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00904">https://arxiv.org/pdf/2409.00904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00904]] Multi-scale Temporal Fusion Transformer for Incomplete Vehicle Trajectory Prediction(https://arxiv.org/abs/2409.00904)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Motion prediction plays an essential role in autonomous driving systems, enabling autonomous vehicles to achieve more accurate local-path planning and driving decisions based on predictions of the surrounding vehicles. However, existing methods neglect the potential missing values caused by object occlusion, perception failures, etc., which inevitably degrades the trajectory prediction performance in real traffic scenarios. To address this limitation, we propose a novel end-to-end framework for incomplete vehicle trajectory prediction, named Multi-scale Temporal Fusion Transformer (MTFT), which consists of the Multi-scale Attention Head (MAH) and the Continuity Representation-guided Multi-scale Fusion (CRMF) module. Specifically, the MAH leverages the multi-head attention mechanism to parallelly capture multi-scale motion representation of trajectory from different temporal granularities, thus mitigating the adverse effect of missing values on prediction. Furthermore, the multi-scale motion representation is input into the CRMF module for multi-scale fusion to obtain the robust temporal feature of the vehicle. During the fusion process, the continuity representation of vehicle motion is first extracted across time steps to guide the fusion, ensuring that the resulting temporal feature incorporates both detailed information and the overall trend of vehicle motion, which facilitates the accurate decoding of future trajectory that is consistent with the vehicle's motion trend. We evaluate the proposed model on four datasets derived from highway and urban traffic scenarios. The experimental results demonstrate its superior performance in the incomplete vehicle trajectory prediction task compared with state-of-the-art models, e.g., a comprehensive performance improvement of more than 39% on the HighD dataset.</li>
</ul>

<h3>Title: Merging Multiple Datasets for Improved Appearance-Based Gaze Estimation</h3>
<ul>
<li><strong>Authors: </strong>Liang Wu, Bertram E. Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00912">https://arxiv.org/abs/2409.00912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00912">https://arxiv.org/pdf/2409.00912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00912]] Merging Multiple Datasets for Improved Appearance-Based Gaze Estimation(https://arxiv.org/abs/2409.00912)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multiple datasets have been created for training and testing appearance-based gaze estimators. Intuitively, more data should lead to better performance. However, combining datasets to train a single esti-mator rarely improves gaze estimation performance. One reason may be differences in the experimental protocols used to obtain the gaze sam-ples, resulting in differences in the distributions of head poses, gaze an-gles, illumination, etc. Another reason may be the inconsistency between methods used to define gaze angles (label mismatch). We propose two innovations to improve the performance of gaze estimation by leveraging multiple datasets, a change in the estimator architecture and the intro-duction of a gaze adaptation module. Most state-of-the-art estimators merge information extracted from images of the two eyes and the entire face either in parallel or combine information from the eyes first then with the face. Our proposed Two-stage Transformer-based Gaze-feature Fusion (TTGF) method uses transformers to merge information from each eye and the face separately and then merge across the two eyes. We argue that this improves head pose invariance since changes in head pose affect left and right eye images in different ways. Our proposed Gaze Adaptation Module (GAM) method handles annotation inconsis-tency by applying a Gaze Adaption Module for each dataset to correct gaze estimates from a single shared estimator. This enables us to combine information across datasets despite differences in labeling. Our experi-ments show that these innovations improve gaze estimation performance over the SOTA both individually and collectively (by 10% - 20%). Our code is available at this https URL.</li>
</ul>

<h3>Title: Large Scale Unsupervised Brain MRI Image</h3>
<ul>
<li><strong>Authors: </strong>Yuxi Zhang, Xiang Chen, Jiazheng Wang, Min Liu, Yaonan Wang, Dongdong Liu, Renjiu Hu, Hang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00917">https://arxiv.org/abs/2409.00917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00917">https://arxiv.org/pdf/2409.00917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00917]] Large Scale Unsupervised Brain MRI Image(https://arxiv.org/abs/2409.00917)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we summarize the methods and experimental results we proposed for Task 2 in the learn2reg 2024 Challenge. This task focuses on unsupervised registration of anatomical structures in brain MRI images between different patients. The difficulty lies in: (1) without segmentation labels, and (2) a large amount of data. To address these challenges, we built an efficient backbone network and explored several schemes to further enhance registration accuracy. Under the guidance of the NCC loss function and smoothness regularization loss function, we obtained a smooth and reasonable deformation field. According to the leaderboard, our method achieved a Dice coefficient of 77.34%, which is 1.4% higher than the TransMorph. Overall, we won second place on the leaderboard for Task 2.</li>
</ul>

<h3>Title: ToolACE: Winning the Points of LLM Function Calling</h3>
<ul>
<li><strong>Authors: </strong>Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, Zezhong Wang, Yuxian Wang, Wu Ning, Yutai Hou, Bin Wang, Chuhan Wu, Xinzhi Wang, Yong Liu, Yasheng Wang, Duyu Tang, Dandan Tu, Lifeng Shang, Xin Jiang, Ruiming Tang, Defu Lian, Qun Liu, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00920">https://arxiv.org/abs/2409.00920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00920">https://arxiv.org/pdf/2409.00920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00920]] ToolACE: Winning the Points of LLM Function Calling(https://arxiv.org/abs/2409.00920)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Function calling significantly extends the application boundary of large language models, where high-quality and diverse training data is critical for unlocking this capability. However, real function-calling data is quite challenging to collect and annotate, while synthetic data generated by existing pipelines tends to lack coverage and accuracy. In this paper, we present ToolACE, an automatic agentic pipeline designed to generate accurate, complex, and diverse tool-learning data. ToolACE leverages a novel self-evolution synthesis process to curate a comprehensive API pool of 26,507 diverse APIs. Dialogs are further generated through the interplay among multiple agents, guided by a formalized thinking process. To ensure data accuracy, we implement a dual-layer verification system combining rule-based and model-based checks. We demonstrate that models trained on our synthesized data, even with only 8B parameters, achieve state-of-the-art performance on the Berkeley Function-Calling Leaderboard, rivaling the latest GPT-4 models. Our model and a subset of the data are publicly available at this https URL.</li>
</ul>

<h3>Title: ProphetFuzz: Fully Automated Prediction and Fuzzing of High-Risk Option Combinations with Only Documentation via Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Dawei Wang, Geng Zhou, Li Chen, Dan Li, Yukai Miao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00922">https://arxiv.org/abs/2409.00922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00922">https://arxiv.org/pdf/2409.00922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00922]] ProphetFuzz: Fully Automated Prediction and Fuzzing of High-Risk Option Combinations with Only Documentation via Large Language Model(https://arxiv.org/abs/2409.00922)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Vulnerabilities related to option combinations pose a significant challenge in software security testing due to their vast search space. Previous research primarily addressed this challenge through mutation or filtering techniques, which inefficiently treated all option combinations as having equal potential for vulnerabilities, thus wasting considerable time on non-vulnerable targets and resulting in low testing efficiency. In this paper, we utilize carefully designed prompt engineering to drive the large language model (LLM) to predict high-risk option combinations (i.e., more likely to contain vulnerabilities) and perform fuzz testing automatically without human intervention. We developed a tool called ProphetFuzz and evaluated it on a dataset comprising 52 programs collected from three related studies. The entire experiment consumed 10.44 CPU years. ProphetFuzz successfully predicted 1748 high-risk option combinations at an average cost of only \$8.69 per program. Results show that after 72 hours of fuzzing, ProphetFuzz discovered 364 unique vulnerabilities associated with 12.30\% of the predicted high-risk option combinations, which was 32.85\% higher than that found by state-of-the-art in the same timeframe. Additionally, using ProphetFuzz, we conducted persistent fuzzing on the latest versions of these programs, uncovering 140 vulnerabilities, with 93 confirmed by developers and 21 awarded CVE numbers.</li>
</ul>

<h3>Title: MedSAM-U: Uncertainty-Guided Auto Multi-Prompt Adaptation for Reliable MedSAM</h3>
<ul>
<li><strong>Authors: </strong>Nan Zhou, Ke Zou, Kai Ren, Mengting Luo, Linchao He, Meng Wang, Yidi Chen, Yi Zhang, Hu Chen, Huazhu Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00924">https://arxiv.org/abs/2409.00924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00924">https://arxiv.org/pdf/2409.00924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00924]] MedSAM-U: Uncertainty-Guided Auto Multi-Prompt Adaptation for Reliable MedSAM(https://arxiv.org/abs/2409.00924)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The Medical Segment Anything Model (MedSAM) has shown remarkable performance in medical image segmentation, drawing significant attention in the field. However, its sensitivity to varying prompt types and locations poses challenges. This paper addresses these challenges by focusing on the development of reliable prompts that enhance MedSAM's accuracy. We introduce MedSAM-U, an uncertainty-guided framework designed to automatically refine multi-prompt inputs for more reliable and precise medical image segmentation. Specifically, we first train a Multi-Prompt Adapter integrated with MedSAM, creating MPA-MedSAM, to adapt to diverse multi-prompt inputs. We then employ uncertainty-guided multi-prompt to effectively estimate the uncertainties associated with the prompts and their initial segmentation results. In particular, a novel uncertainty-guided prompts adaptation technique is then applied automatically to derive reliable prompts and their corresponding segmentation outcomes. We validate MedSAM-U using datasets from multiple modalities to train a universal image segmentation model. Compared to MedSAM, experimental results on five distinct modal datasets demonstrate that the proposed MedSAM-U achieves an average performance improvement of 1.7\% to 20.5\% across uncertainty-guided prompts.</li>
</ul>

<h3>Title: Towards Student Actions in Classroom Scenes: New Dataset and Baseline</h3>
<ul>
<li><strong>Authors: </strong>Zhuolin Tan, Chenqiang Gao, Anyong Qin, Ruixin Chen, Tiecheng Song, Feng Yang, Deyu Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00926">https://arxiv.org/abs/2409.00926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00926">https://arxiv.org/pdf/2409.00926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00926]] Towards Student Actions in Classroom Scenes: New Dataset and Baseline(https://arxiv.org/abs/2409.00926)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Analyzing student actions is an important and challenging task in educational research. Existing efforts have been hampered by the lack of accessible datasets to capture the nuanced action dynamics in classrooms. In this paper, we present a new multi-label student action video (SAV) dataset for complex classroom scenes. The dataset consists of 4,324 carefully trimmed video clips from 758 different classrooms, each labeled with 15 different actions displayed by students in classrooms. Compared to existing behavioral datasets, our dataset stands out by providing a wide range of real classroom scenarios, high-quality video data, and unique challenges, including subtle movement differences, dense object engagement, significant scale differences, varied shooting angles, and visual occlusion. The increased complexity of the dataset brings new opportunities and challenges for benchmarking action detection. Innovatively, we also propose a new baseline method, a visual transformer for enhancing attention to key local details in small and dense object regions. Our method achieves excellent performance with mean Average Precision (mAP) of 67.9\% and 27.4\% on SAV and AVA, respectively. This paper not only provides the dataset but also calls for further research into AI-driven educational tools that may transform teaching methodologies and learning outcomes. The code and dataset will be released at this https URL.</li>
</ul>

<h3>Title: Self-Judge: Selective Instruction Following with Alignment Self-Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Hai Ye, Hwee Tou Ng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00935">https://arxiv.org/abs/2409.00935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00935">https://arxiv.org/pdf/2409.00935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00935]] Self-Judge: Selective Instruction Following with Alignment Self-Evaluation(https://arxiv.org/abs/2409.00935)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Pre-trained large language models (LLMs) can be tailored to adhere to human instructions through instruction tuning. However, due to shifts in the distribution of test-time data, they may not always execute instructions accurately, potentially generating factual errors or misaligned content when acting as chat assistants. To enhance the reliability of LLMs in following instructions, we propose the study of selective instruction following, whereby the system declines to execute instructions if the anticipated response quality is low. We train judge models that can predict numerical quality scores for model responses. To address data scarcity, we introduce Self-J, a novel self-training framework for developing judge models without needing human-annotated quality scores. Our method leverages the model's inherent self-evaluation capability to extract information about response quality from labeled instruction-tuning data. It incorporates a gold reference answer to facilitate self-evaluation and recalibrates by assessing the semantic similarity between the response sample and the gold reference. During the training phase, we implement self-distillation as a regularization technique to enhance the capability of reference-free estimation. To validate alignment evaluation on general instruction-following tasks, we collect large-scale high-quality instructions from Hugging Face for model training and evaluation. Extensive experiments on five open-source models show that our method correlates much more with GPT-4 than strong baselines, e.g., supervised models distilled from GPT-4 and GPT-3.5-turbo. Our analysis shows our model's strong generalization across domains. Additionally, our judge models serve as good reward models, e.g., boosting WizardLM-13B-V1.2 from 89.17 to 92.48 and from 12.03 to 15.90 in version v1 and v2 of AlpacaEval respectively using best-of-32 sampling with our judge models.</li>
</ul>

<h3>Title: Large Language Models for Automatic Detection of Sensitive Topics</h3>
<ul>
<li><strong>Authors: </strong>Ruoyu Wen, Stephanie Elena Crowe, Kunal Gupta, Xinyue Li, Mark Billinghurst, Simon Hoermann, Dwain Allan, Alaeddin Nassani, Thammathip Piumsomboon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00940">https://arxiv.org/abs/2409.00940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00940">https://arxiv.org/pdf/2409.00940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00940]] Large Language Models for Automatic Detection of Sensitive Topics(https://arxiv.org/abs/2409.00940)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Sensitive information detection is crucial in content moderation to maintain safe online communities. Assisting in this traditionally manual process could relieve human moderators from overwhelming and tedious tasks, allowing them to focus solely on flagged content that may pose potential risks. Rapidly advancing large language models (LLMs) are known for their capability to understand and process natural language and so present a potential solution to support this process. This study explores the capabilities of five LLMs for detecting sensitive messages in the mental well-being domain within two online datasets and assesses their performance in terms of accuracy, precision, recall, F1 scores, and consistency. Our findings indicate that LLMs have the potential to be integrated into the moderation workflow as a convenient and precise detection tool. The best-performing model, GPT-4o, achieved an average accuracy of 99.5\% and an F1-score of 0.99. We discuss the advantages and potential challenges of using LLMs in the moderation workflow and suggest that future research should address the ethical considerations of utilising this technology.</li>
</ul>

<h3>Title: XNet v2: Fewer Limitations, Better Results and Greater Universality</h3>
<ul>
<li><strong>Authors: </strong>Yanfeng Zhou, Lingrui Li, Zichen Wang, Guole Liu, Ziwen Liu, Ge Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00947">https://arxiv.org/abs/2409.00947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00947">https://arxiv.org/pdf/2409.00947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00947]] XNet v2: Fewer Limitations, Better Results and Greater Universality(https://arxiv.org/abs/2409.00947)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>XNet introduces a wavelet-based X-shaped unified architecture for fully- and semi-supervised biomedical segmentation. So far, however, XNet still faces the limitations, including performance degradation when images lack high-frequency (HF) information, underutilization of raw images and insufficient fusion. To address these issues, we propose XNet v2, a low- and high-frequency complementary model. XNet v2 performs wavelet-based image-level complementary fusion, using fusion results along with raw images inputs three different sub-networks to construct consistency loss. Furthermore, we introduce a feature-level fusion module to enhance the transfer of low-frequency (LF) information and HF information. XNet v2 achieves state-of-the-art in semi-supervised segmentation while maintaining competitve results in fully-supervised learning. More importantly, XNet v2 excels in scenarios where XNet fails. Compared to XNet, XNet v2 exhibits fewer limitations, better results and greater universality. Extensive experiments on three 2D and two 3D datasets demonstrate the effectiveness of XNet v2. Code is available at this https URL .</li>
</ul>

<h3>Title: Unveiling the Vulnerability of Private Fine-Tuning in Split-Based Frameworks for Large Language Models: A Bidirectionally Enhanced Attack</h3>
<ul>
<li><strong>Authors: </strong>Guanzhong Chen, Zhenhan Qin, Mingxin Yang, Yajie Zhou, Tao Fan, Tianyu Du, Zenglin Xu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00960">https://arxiv.org/abs/2409.00960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00960">https://arxiv.org/pdf/2409.00960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00960]] Unveiling the Vulnerability of Private Fine-Tuning in Split-Based Frameworks for Large Language Models: A Bidirectionally Enhanced Attack(https://arxiv.org/abs/2409.00960)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in pre-trained large language models (LLMs) have significantly influenced various domains. Adapting these models for specific tasks often involves fine-tuning (FT) with private, domain-specific data. However, privacy concerns keep this data undisclosed, and the computational demands for deploying LLMs pose challenges for resource-limited data holders. This has sparked interest in split learning (SL), a Model-as-a-Service (MaaS) paradigm that divides LLMs into smaller segments for distributed training and deployment, transmitting only intermediate activations instead of raw data. SL has garnered substantial interest in both industry and academia as it aims to balance user data privacy, model ownership, and resource challenges in the private fine-tuning of LLMs. Despite its privacy claims, this paper reveals significant vulnerabilities arising from the combination of SL and LLM-FT: the Not-too-far property of fine-tuning and the auto-regressive nature of LLMs. Exploiting these vulnerabilities, we propose Bidirectional Semi-white-box Reconstruction (BiSR), the first data reconstruction attack (DRA) designed to target both the forward and backward propagation processes of SL. BiSR utilizes pre-trained weights as prior knowledge, combining a learning-based attack with a bidirectional optimization-based approach for highly effective data reconstruction. Additionally, it incorporates a Noise-adaptive Mixture of Experts (NaMoE) model to enhance reconstruction performance under perturbation. We conducted systematic experiments on various mainstream LLMs and different setups, empirically demonstrating BiSR's state-of-the-art performance. Furthermore, we thoroughly examined three representative defense mechanisms, showcasing our method's capability to reconstruct private data even in the presence of these defenses.</li>
</ul>

<h3>Title: Interpretable Convolutional SyncNet</h3>
<ul>
<li><strong>Authors: </strong>Sungjoon Park, Jaesub Yun, Donggeon Lee, Minsik Park</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00971">https://arxiv.org/abs/2409.00971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00971">https://arxiv.org/pdf/2409.00971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00971]] Interpretable Convolutional SyncNet(https://arxiv.org/abs/2409.00971)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Because videos in the wild can be out of sync for various reasons, a sync-net is used to bring the video back into sync for tasks that require synchronized videos. Previous state-of-the-art (SOTA) sync-nets use InfoNCE loss, rely on the transformer architecture, or both. Unfortunately, the former makes the model's output difficult to interpret, and the latter is unfriendly with large images, thus limiting the usefulness of sync-nets. In this work, we train a convolutional sync-net using the balanced BCE loss (BBCE), a loss inspired by the binary cross entropy (BCE) and the InfoNCE losses. In contrast to the InfoNCE loss, the BBCE loss does not require complicated sampling schemes. Our model can better handle larger images, and its output can be given a probabilistic interpretation. The probabilistic interpretation allows us to define metrics such as probability at offset and offscreen ratio to evaluate the sync quality of audio-visual (AV) speech datasets. Furthermore, our model achieves SOTA accuracy of $96.5\%$ on the LRS2 dataset and $93.8\%$ on the LRS3 dataset.</li>
</ul>

<h3>Title: IVGF: The Fusion-Guided Infrared and Visible General Framework</h3>
<ul>
<li><strong>Authors: </strong>Fangcen Liu, Chenqiang Gao, Fang Chen, Pengcheng Li, Junjie Guo, Deyu Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00973">https://arxiv.org/abs/2409.00973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00973">https://arxiv.org/pdf/2409.00973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00973]] IVGF: The Fusion-Guided Infrared and Visible General Framework(https://arxiv.org/abs/2409.00973)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Infrared and visible dual-modality tasks such as semantic segmentation and object detection can achieve robust performance even in extreme scenes by fusing complementary information. Most current methods design task-specific frameworks, which are limited in generalization across multiple tasks. In this paper, we propose a fusion-guided infrared and visible general framework, IVGF, which can be easily extended to many high-level vision tasks. Firstly, we adopt the SOTA infrared and visible foundation models to extract the general representations. Then, to enrich the semantics information of these general representations for high-level vision tasks, we design the feature enhancement module and token enhancement module for feature maps and tokens, respectively. Besides, the attention-guided fusion module is proposed for effectively fusing by exploring the complementary information of two modalities. Moreover, we also adopt the cutout&mix augmentation strategy to conduct the data augmentation, which further improves the ability of the model to mine the regional complementary between the two modalities. Extensive experiments show that the IVGF outperforms state-of-the-art dual-modality methods in the semantic segmentation and object detection tasks. The detailed ablation studies demonstrate the effectiveness of each module, and another experiment explores the anti-missing modality ability of the proposed method in the dual-modality semantic segmentation task.</li>
</ul>

<h3>Title: Enhancing Privacy in Federated Learning: Secure Aggregation for Real-World Healthcare Applications</h3>
<ul>
<li><strong>Authors: </strong>Riccardo Taiello, Sergen Cansiz, Marc Vesin, Francesco Cremonesi, Lucia Innocenti, Melek √ñnen, Marco Lorenzi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00974">https://arxiv.org/abs/2409.00974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00974">https://arxiv.org/pdf/2409.00974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00974]] Enhancing Privacy in Federated Learning: Secure Aggregation for Real-World Healthcare Applications(https://arxiv.org/abs/2409.00974)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>Deploying federated learning (FL) in real-world scenarios, particularly in healthcare, poses challenges in communication and security. In particular, with respect to the federated aggregation procedure, researchers have been focusing on the study of secure aggregation (SA) schemes to provide privacy guarantees over the model's parameters transmitted by the clients. Nevertheless, the practical availability of SA in currently available FL frameworks is currently limited, due to computational and communication bottlenecks. To fill this gap, this study explores the implementation of SA within the open-source Fed-BioMed framework. We implement and compare two SA protocols, Joye-Libert (JL) and Low Overhead Masking (LOM), by providing extensive benchmarks in a panel of healthcare data analysis problems. Our theoretical and experimental evaluations on four datasets demonstrate that SA protocols effectively protect privacy while maintaining task accuracy. Computational overhead during training is less than 1% on a CPU and less than 50% on a GPU for large models, with protection phases taking less than 10 seconds. Incorporating SA into Fed-BioMed impacts task accuracy by no more than 2% compared to non-SA scenarios. Overall this study demonstrates the feasibility of SA in real-world healthcare applications and contributes in reducing the gap towards the adoption of privacy-preserving technologies in sensitive applications.</li>
</ul>

<h3>Title: DNN-GDITD: Out-of-distribution detection via Deep Neural Network based Gaussian Descriptor for Imbalanced Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Priyanka Chudasama, Anil Surisetty, Aakarsh Malhotra, Alok Singh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00980">https://arxiv.org/abs/2409.00980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00980">https://arxiv.org/pdf/2409.00980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00980]] DNN-GDITD: Out-of-distribution detection via Deep Neural Network based Gaussian Descriptor for Imbalanced Tabular Data(https://arxiv.org/abs/2409.00980)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Classification tasks present challenges due to class imbalances and evolving data distributions. Addressing these issues requires a robust method to handle imbalances while effectively detecting out-of-distribution (OOD) samples not encountered during training. This study introduces a novel OOD detection algorithm designed for tabular datasets, titled \textit{\textbf{D}eep \textbf{N}eural \textbf{N}etwork-based \textbf{G}aussian \textbf{D}escriptor for \textbf{I}mbalanced \textbf{T}abular \textbf{D}ata} (\textbf{DNN-GDITD}). The DNN-GDITD algorithm can be placed on top of any DNN to facilitate better classification of imbalanced data and OOD detection using spherical decision boundaries. Using a combination of Push, Score-based, and focal losses, DNN-GDITD assigns confidence scores to test data points, categorizing them as known classes or as an OOD sample. Extensive experimentation on tabular datasets demonstrates the effectiveness of DNN-GDITD compared to three OOD algorithms. Evaluation encompasses imbalanced and balanced scenarios on diverse tabular datasets, including a synthetic financial dispute dataset and publicly available tabular datasets like Gas Sensor, Drive Diagnosis, and MNIST, showcasing DNN-GDITD's versatility.</li>
</ul>

<h3>Title: GCCRR: A Short Sequence Gait Cycle Segmentation Method Based on Ear-Worn IMU</h3>
<ul>
<li><strong>Authors: </strong>Zhenye Xu, Yao Guo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00983">https://arxiv.org/abs/2409.00983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00983">https://arxiv.org/pdf/2409.00983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00983]] GCCRR: A Short Sequence Gait Cycle Segmentation Method Based on Ear-Worn IMU(https://arxiv.org/abs/2409.00983)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper addresses the critical task of gait cycle segmentation using short sequences from ear-worn IMUs, a practical and non-invasive approach for home-based monitoring and rehabilitation of patients with impaired motor function. While previous studies have focused on IMUs positioned on the lower limbs, ear-worn IMUs offer a unique advantage in capturing gait dynamics with minimal intrusion. To address the challenges of gait cycle segmentation using short sequences, we introduce the Gait Characteristic Curve Regression and Restoration (GCCRR) method, a novel two-stage approach designed for fine-grained gait phase segmentation. The first stage transforms the segmentation task into a regression task on the Gait Characteristic Curve (GCC), which is a one-dimensional feature sequence incorporating periodic information. The second stage restores the gait cycle using peak detection techniques. Our method employs Bi-LSTM-based deep learning algorithms for regression to ensure reliable segmentation for short gait sequences. Evaluation on the HamlynGait dataset demonstrates that GCCRR achieves over 80\% Accuracy, with a Timestamp Error below one sampling interval. Despite its promising results, the performance lags behind methods using more extensive sensor systems, highlighting the need for larger, more diverse datasets. Future work will focus on data augmentation using motion capture systems and improving algorithmic generalizability.</li>
</ul>

<h3>Title: 3D Priors-Guided Diffusion for Blind Face Restoration</h3>
<ul>
<li><strong>Authors: </strong>Xiaobin Lu, Xiaobin Hu, Jun Luo, Ben Zhu, Yaping Ruan, Wenqi Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00991">https://arxiv.org/abs/2409.00991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00991">https://arxiv.org/pdf/2409.00991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00991]] 3D Priors-Guided Diffusion for Blind Face Restoration(https://arxiv.org/abs/2409.00991)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Blind face restoration endeavors to restore a clear face image from a degraded counterpart. Recent approaches employing Generative Adversarial Networks (GANs) as priors have demonstrated remarkable success in this field. However, these methods encounter challenges in achieving a balance between realism and fidelity, particularly in complex degradation scenarios. To inherit the exceptional realism generative ability of the diffusion model and also constrained by the identity-aware fidelity, we propose a novel diffusion-based framework by embedding the 3D facial priors as structure and identity constraints into a denoising diffusion process. Specifically, in order to obtain more accurate 3D prior representations, the 3D facial image is reconstructed by a 3D Morphable Model (3DMM) using an initial restored face image that has been processed by a pretrained restoration network. A customized multi-level feature extraction method is employed to exploit both structural and identity information of 3D facial images, which are then mapped into the noise estimation process. In order to enhance the fusion of identity information into the noise estimation, we propose a Time-Aware Fusion Block (TAFB). This module offers a more efficient and adaptive fusion of weights for denoising, considering the dynamic nature of the denoising process in the diffusion model, which involves initial structure refinement followed by texture detail enhancement.Extensive experiments demonstrate that our network performs favorably against state-of-the-art algorithms on synthetic and real-world datasets for blind face restoration.</li>
</ul>

<h3>Title: DataSculpt: Crafting Data Landscapes for LLM Post-Training through Multi-objective Partitioning</h3>
<ul>
<li><strong>Authors: </strong>Keer Lu, Zheng Liang, Xiaonan Nie, Da Pan, Shusen Zhang, Keshi Zhao, Weipeng Chen, Zenan Zhou, Guosheng Dong, Wentao Zhang, Bin Cui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.00997">https://arxiv.org/abs/2409.00997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.00997">https://arxiv.org/pdf/2409.00997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.00997]] DataSculpt: Crafting Data Landscapes for LLM Post-Training through Multi-objective Partitioning(https://arxiv.org/abs/2409.00997)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The effectiveness of long-context modeling is important for Large Language Models (LLMs) in various applications. Despite their potential, LLMs' efficacy in processing long context does not consistently meet expectations, posing significant challenges for efficient management of prolonged sequences in training. This difficulty is compounded by the scarcity of comprehensive and diverse training datasets suitable for long sequences, which stems from inherent length biases across different data sources, and the logistical complexities associated with massive data management for training in extended contexts. In this work, we introduce DataSculpt, a data construction framework designed to strategically augment the data architecture for extended-context training. Our thorough evaluations demonstrate DataSculpt's remarkable capacity to boost long-context training performance, achieving improvements including an 18.09% increase in retrieval augmentation, 21.23% in summarization, 21.27% in reading comprehension, and a 3.81% rise in code completion, all while preserving the models' overall proficiency with a 4.88% improvement.</li>
</ul>

<h3>Title: From Bird's-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Xiaojie Xu, Tianshuo Xu, Fulong Ma, Yingcong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01014">https://arxiv.org/abs/2409.01014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01014">https://arxiv.org/pdf/2409.01014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01014]] From Bird's-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model(https://arxiv.org/abs/2409.01014)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>We explore Bird's-Eye View (BEV) generation, converting a BEV map into its corresponding multi-view street images. Valued for its unified spatial representation aiding multi-sensor fusion, BEV is pivotal for various autonomous driving applications. Creating accurate street-view images from BEV maps is essential for portraying complex traffic scenarios and enhancing driving algorithms. Concurrently, diffusion-based conditional image generation models have demonstrated remarkable outcomes, adept at producing diverse, high-quality, and condition-aligned results. Nonetheless, the training of these models demands substantial data and computational resources. Hence, exploring methods to fine-tune these advanced models, like Stable Diffusion, for specific conditional generation tasks emerges as a promising avenue. In this paper, we introduce a practical framework for generating images from a BEV layout. Our approach comprises two main components: the Neural View Transformation and the Street Image Generation. The Neural View Transformation phase converts the BEV map into aligned multi-view semantic segmentation maps by learning the shape correspondence between the BEV and perspective views. Subsequently, the Street Image Generation phase utilizes these segmentations as a condition to guide a fine-tuned latent diffusion model. This finetuning process ensures both view and style consistency. Our model leverages the generative capacity of large pretrained diffusion models within traffic contexts, effectively yielding diverse and condition-coherent street view images.</li>
</ul>

<h3>Title: Fed-MUnet: Multi-modal Federated Unet for Brain Tumor Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ruojun Zhou, Lisha Qu, Lei Zhang, Ziming Li, Hongwei Yu, Bing Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01020">https://arxiv.org/abs/2409.01020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01020">https://arxiv.org/pdf/2409.01020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01020]] Fed-MUnet: Multi-modal Federated Unet for Brain Tumor Segmentation(https://arxiv.org/abs/2409.01020)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, segmentation</a></li>
<li><strong>Abstract: </strong>Deep learning-based techniques have been widely utilized for brain tumor segmentation using both single and multi-modal Magnetic Resonance Imaging (MRI) images. Most current studies focus on centralized training due to the intrinsic challenge of data sharing across clinics. To mitigate privacy concerns, researchers have introduced Federated Learning (FL) methods to brain tumor segmentation tasks. However, currently such methods are focusing on single modal MRI, with limited study on multi-modal MRI. The challenges include complex structure, large-scale parameters, and overfitting issues of the FL based methods using multi-modal MRI. To address the above challenges, we propose a novel multi-modal FL framework for brain tumor segmentation (Fed-MUnet) that is suitable for FL training. We evaluate our approach with the BraTS2022 datasets, which are publicly available. The experimental results demonstrate that our framework achieves FL nature of distributed learning and privacy preserving. For the enhancing tumor, tumor core and whole tumor, the mean of five major metrics were 87.5%, 90.6% and 92.2%, respectively, which were higher than SOTA methods while preserving privacy. In terms of parameters count, quantity of floating-point operations (FLOPs) and inference, Fed-MUnet is Pareto optimal compared with the state-of-the-art segmentation backbone while achieves higher performance and tackles privacy issue. Our codes are open-sourced at this https URL.</li>
</ul>

<h3>Title: SINET: Sparsity-driven Interpretable Neural Network for Underwater Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Gargi Panda, Soumitra Kundu, Saumik Bhattacharya, Aurobinda Routray</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01022">https://arxiv.org/abs/2409.01022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01022">https://arxiv.org/pdf/2409.01022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01022]] SINET: Sparsity-driven Interpretable Neural Network for Underwater Image Enhancement(https://arxiv.org/abs/2409.01022)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Improving the quality of underwater images is essential for advancing marine research and technology. This work introduces a sparsity-driven interpretable neural network (SINET) for the underwater image enhancement (UIE) task. Unlike pure deep learning methods, our network architecture is based on a novel channel-specific convolutional sparse coding (CCSC) model, ensuring good interpretability of the underlying image enhancement process. The key feature of SINET is that it estimates the salient features from the three color channels using three sparse feature estimation blocks (SFEBs). The architecture of SFEB is designed by unrolling an iterative algorithm for solving the $\ell_1$ regulaized convolutional sparse coding (CSC) problem. Our experiments show that SINET surpasses state-of-the-art PSNR value by $1.05$ dB with $3873$ times lower computational complexity.</li>
</ul>

<h3>Title: Variation in prediction accuracy due to randomness in data division and fair evaluation using interval estimation</h3>
<ul>
<li><strong>Authors: </strong>Isao Goto</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01025">https://arxiv.org/abs/2409.01025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01025">https://arxiv.org/pdf/2409.01025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01025]] Variation in prediction accuracy due to randomness in data division and fair evaluation using interval estimation(https://arxiv.org/abs/2409.01025)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>This paper attempts to answer a "simple question" in building predictive models using machine learning algorithms. Although diagnostic and predictive models for various diseases have been proposed using data from large cohort studies and machine learning algorithms, challenges remain in their generalizability. Several causes for this challenge have been pointed out, and partitioning of the dataset with randomness is considered to be one of them. In this study, we constructed 33,600 diabetes diagnosis models with "initial state" dependent randomness using autoML (automatic machine learning framework) and open diabetes data, and evaluated their prediction accuracy. The results showed that the prediction accuracy had an initial state-dependent distribution. Since this distribution could follow a normal distribution, we estimated the expected interval of prediction accuracy using statistical interval estimation in order to fairly compare the accuracy of the prediction models.</li>
</ul>

<h3>Title: Learning to Discover Forgery Cues for Face Forgery Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiahe Tian, Peng Chen, Cai Yu, Xiaomeng Fu, Xi Wang, Jiao Dai, Jizhong Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01030">https://arxiv.org/abs/2409.01030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01030">https://arxiv.org/pdf/2409.01030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01030]] Learning to Discover Forgery Cues for Face Forgery Detection(https://arxiv.org/abs/2409.01030)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Locating manipulation maps, i.e., pixel-level annotation of forgery cues, is crucial for providing interpretable detection results in face forgery detection. Related learning objects have also been widely adopted as auxiliary tasks to improve the classification performance of detectors whereas they require comparisons between paired real and forged faces to obtain manipulation maps as supervision. This requirement restricts their applicability to unpaired faces and contradicts real-world scenarios. Moreover, the used comparison methods annotate all changed pixels, including noise introduced by compression and upsampling. Using such maps as supervision hinders the learning of exploitable cues and makes models prone to overfitting. To address these issues, we introduce a weakly supervised model in this paper, named Forgery Cue Discovery (FoCus), to locate forgery cues in unpaired faces. Unlike some detectors that claim to locate forged regions in attention maps, FoCus is designed to sidestep their shortcomings of capturing partial and inaccurate forgery cues. Specifically, we propose a classification attentive regions proposal module to locate forgery cues during classification and a complementary learning module to facilitate the learning of richer cues. The produced manipulation maps can serve as better supervision to enhance face forgery detectors. Visualization of the manipulation maps of the proposed FoCus exhibits superior interpretability and robustness compared to existing methods. Experiments on five datasets and four multi-task models demonstrate the effectiveness of FoCus in both in-dataset and cross-dataset evaluations.</li>
</ul>

<h3>Title: Unleashing the Power of Task-Specific Directions in Parameter Efficient Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Chongjie Si, Zhiyi Shi, Shifan Zhang, Xiaokang Yang, Hanspeter Pfister, Wei Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01035">https://arxiv.org/abs/2409.01035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01035">https://arxiv.org/pdf/2409.01035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01035]] Unleashing the Power of Task-Specific Directions in Parameter Efficient Fine-tuning(https://arxiv.org/abs/2409.01035)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models demonstrate impressive performance on downstream tasks, yet requiring extensive resource consumption when fully fine-tuning all parameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT) strategies, such as LoRA, have been developed. In this paper, we delve into the concept of task-specific directions--critical for transitioning large models from pre-trained states to task-specific enhancements in PEFT. We propose a framework to clearly define these directions and explore their properties, and practical utilization challenges. We then introduce a novel approach, LoRA-Dash, which aims to maximize the impact of task-specific directions during the fine-tuning process, thereby enhancing model performance on targeted tasks. Extensive experiments have conclusively demonstrated the effectiveness of LoRA-Dash, and in-depth analyses further reveal the underlying mechanisms of LoRA-Dash. The code is available at this https URL.</li>
</ul>

<h3>Title: NYK-MS: A Well-annotated Multi-modal Metaphor and Sarcasm Understanding Benchmark on Cartoon-Caption Dataset</h3>
<ul>
<li><strong>Authors: </strong>Ke Chang, Hao Li, Junzhao Zhang, Yunfang Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01037">https://arxiv.org/abs/2409.01037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01037">https://arxiv.org/pdf/2409.01037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01037]] NYK-MS: A Well-annotated Multi-modal Metaphor and Sarcasm Understanding Benchmark on Cartoon-Caption Dataset(https://arxiv.org/abs/2409.01037)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Metaphor and sarcasm are common figurative expressions in people's communication, especially on the Internet or the memes popular among teenagers. We create a new benchmark named NYK-MS (NewYorKer for Metaphor and Sarcasm), which contains 1,583 samples for metaphor understanding tasks and 1,578 samples for sarcasm understanding tasks. These tasks include whether it contains metaphor/sarcasm, which word or object contains metaphor/sarcasm, what does it satirize and why does it contains metaphor/sarcasm, all of the 7 tasks are well-annotated by at least 3 annotators. We annotate the dataset for several rounds to improve the consistency and quality, and use GUI and GPT-4V to raise our efficiency. Based on the benchmark, we conduct plenty of experiments. In the zero-shot experiments, we show that Large Language Models (LLM) and Large Multi-modal Models (LMM) can't do classification task well, and as the scale increases, the performance on other 5 tasks improves. In the experiments on traditional pre-train models, we show the enhancement with augment and alignment methods, which prove our benchmark is consistent with previous dataset and requires the model to understand both of the two modalities.</li>
</ul>

<h3>Title: A Perspective on Literary Metaphor in the Context of Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Imke van Heerden, Anil Bas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01053">https://arxiv.org/abs/2409.01053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01053">https://arxiv.org/pdf/2409.01053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01053]] A Perspective on Literary Metaphor in the Context of Generative AI(https://arxiv.org/abs/2409.01053)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>At the intersection of creative text generation and literary theory, this study explores the role of literary metaphor and its capacity to generate a range of meanings. In this regard, literary metaphor is vital to the development of any particular language. To investigate whether the inclusion of original figurative language improves textual quality, we trained an LSTM-based language model in Afrikaans. The network produces phrases containing compellingly novel figures of speech. Specifically, the emphasis falls on how AI might be utilised as a defamiliarisation technique, which disrupts expected uses of language to augment poetic expression. Providing a literary perspective on text generation, the paper raises thought-provoking questions on aesthetic value, interpretation and evaluation.</li>
</ul>

<h3>Title: Follow-Your-Canvas: Higher-Resolution Video Outpainting with Extensive Content Generation</h3>
<ul>
<li><strong>Authors: </strong>Qihua Chen, Yue Ma, Hongfa Wang, Junkun Yuan, Wenzhe Zhao, Qi Tian, Hongmei Wang, Shaobo Min, Qifeng Chen, Wei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01055">https://arxiv.org/abs/2409.01055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01055">https://arxiv.org/pdf/2409.01055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01055]] Follow-Your-Canvas: Higher-Resolution Video Outpainting with Extensive Content Generation(https://arxiv.org/abs/2409.01055)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper explores higher-resolution video outpainting with extensive content generation. We point out common issues faced by existing methods when attempting to largely outpaint videos: the generation of low-quality content and limitations imposed by GPU memory. To address these challenges, we propose a diffusion-based method called \textit{Follow-Your-Canvas}. It builds upon two core designs. First, instead of employing the common practice of "single-shot" outpainting, we distribute the task across spatial windows and seamlessly merge them. It allows us to outpaint videos of any size and resolution without being constrained by GPU memory. Second, the source video and its relative positional relation are injected into the generation process of each window. It makes the generated spatial layout within each window harmonize with the source video. Coupling with these two designs enables us to generate higher-resolution outpainting videos with rich content while keeping spatial and temporal consistency. Follow-Your-Canvas excels in large-scale video outpainting, e.g., from 512X512 to 1152X2048 (9X), while producing high-quality and aesthetically pleasing results. It achieves the best quantitative results across various resolution and scale setups. The code is released on this https URL</li>
</ul>

<h3>Title: No Peer, no Cry: Network Application Fuzzing via Fault Injection</h3>
<ul>
<li><strong>Authors: </strong>Nils Bars, Moritz Schloegel, Nico Schiller, Lukas Bernhard, Thorsten Holz</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01059">https://arxiv.org/abs/2409.01059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01059">https://arxiv.org/pdf/2409.01059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01059]] No Peer, no Cry: Network Application Fuzzing via Fault Injection(https://arxiv.org/abs/2409.01059)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, protect, attack</a></li>
<li><strong>Abstract: </strong>Network-facing applications are commonly exposed to all kinds of attacks, especially when connected to the internet. As a result, web servers like Nginx or client applications such as curl make every effort to secure and harden their code to rule out memory safety violations. One would expect this to include regular fuzz testing, as fuzzing has proven to be one of the most successful approaches to uncovering bugs in software. Yet, surprisingly little research has focused on fuzzing network applications. When studying the underlying reasons, we find that the interactive nature of communication, its statefulness, and the protection of exchanged messages render typical fuzzers ineffective. Attempts to replay recorded messages or modify them on the fly only work for specific targets and often lead to early termination of communication. In this paper, we discuss these challenges in detail, highlighting how the focus of existing work on protocol state space promises little relief. We propose a fundamentally different approach that relies on fault injection rather than modifying messages. Effectively, we force one of the communication peers into a weird state where its output no longer matches the expectations of the target peer, potentially uncovering bugs. Importantly, this weird peer can still properly encrypt/sign the protocol message, overcoming a fundamental challenge of current fuzzers. In effect, we leave the communication system intact but introduce small corruptions. Since we can turn either the server or the client into the weird peer, our approach is the first that can effectively test client-side network applications. Evaluating 16 targets, we show that Fuzztruction-Net outperforms other fuzzers in terms of coverage and bugs found. Overall, Fuzztruction-Net uncovered 23 new bugs in well-tested software, such as the web servers Nginx and Apache HTTPd and the OpenSSH client.</li>
</ul>

<h3>Title: Defending against Model Inversion Attacks via Random Erasing</h3>
<ul>
<li><strong>Authors: </strong>Viet-Hung Tran, Ngoc-Bao Nguyen, Son T. Mai, Hans Vandierendonck, Ngai-man Cheung</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01062">https://arxiv.org/abs/2409.01062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01062">https://arxiv.org/pdf/2409.01062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01062]] Defending against Model Inversion Attacks via Random Erasing(https://arxiv.org/abs/2409.01062)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense, attack</a></li>
<li><strong>Abstract: </strong>Model Inversion (MI) is a type of privacy violation that focuses on reconstructing private training data through abusive exploitation of machine learning models. To defend against MI attacks, state-of-the-art (SOTA) MI defense methods rely on regularizations that conflict with the training loss, creating explicit tension between privacy protection and model utility. In this paper, we present a new method to defend against MI attacks. Our method takes a new perspective and focuses on training data. Our idea is based on a novel insight on Random Erasing (RE), which has been applied in the past as a data augmentation technique to improve the model accuracy under occlusion. In our work, we instead focus on applying RE for degrading MI attack accuracy. Our key insight is that MI attacks require significant amount of private training data information encoded inside the model in order to reconstruct high-dimensional private images. Therefore, we propose to apply RE to reduce private information presented to the model during training. We show that this can lead to substantial degradation in MI reconstruction quality and attack accuracy. Meanwhile, natural accuracy of the model is only moderately affected. Our method is very simple to implement and complementary to existing defense methods. Our extensive experiments of 23 setups demonstrate that our method can achieve SOTA performance in balancing privacy and utility of the models. The results consistently demonstrate the superiority of our method over existing defenses across different MI attacks, network architectures, and attack configurations.</li>
</ul>

<h3>Title: Progressive Retinal Image Registration via Global and Local Deformable Transformations</h3>
<ul>
<li><strong>Authors: </strong>Yepeng Liu, Baosheng Yu, Tian Chen, Yuliang Gu, Bo Du, Yongchao Xu, Jun Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01068">https://arxiv.org/abs/2409.01068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01068">https://arxiv.org/pdf/2409.01068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01068]] Progressive Retinal Image Registration via Global and Local Deformable Transformations(https://arxiv.org/abs/2409.01068)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Retinal image registration plays an important role in the ophthalmological diagnosis process. Since there exist variances in viewing angles and anatomical structures across different retinal images, keypoint-based approaches become the mainstream methods for retinal image registration thanks to their robustness and low latency. These methods typically assume the retinal surfaces are planar, and adopt feature matching to obtain the homography matrix that represents the global transformation between images. Yet, such a planar hypothesis inevitably introduces registration errors since retinal surface is approximately curved. This limitation is more prominent when registering image pairs with significant differences in viewing angles. To address this problem, we propose a hybrid registration framework called HybridRetina, which progressively registers retinal images with global and local deformable transformations. For that, we use a keypoint detector and a deformation network called GAMorph to estimate the global transformation and local deformable transformation, respectively. Specifically, we integrate multi-level pixel relation knowledge to guide the training of GAMorph. Additionally, we utilize an edge attention module that includes the geometric priors of the images, ensuring the deformation field focuses more on the vascular regions of clinical interest. Experiments on two widely-used datasets, FIRE and FLoRI21, show that our proposed HybridRetina significantly outperforms some state-of-the-art methods. The code is available at this https URL.</li>
</ul>

<h3>Title: VideoLLaMB: Long-context Video Understanding with Recurrent Memory Bridges</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Wang, Cihang Xie, Yang Liu, Zilong Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01071">https://arxiv.org/abs/2409.01071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01071">https://arxiv.org/pdf/2409.01071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01071]] VideoLLaMB: Long-context Video Understanding with Recurrent Memory Bridges(https://arxiv.org/abs/2409.01071)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advancements in large-scale video-language models have shown significant potential for real-time planning and detailed interactions. However, their high computational demands and the scarcity of annotated datasets limit their practicality for academic researchers. In this work, we introduce VideoLLaMB, a novel framework that utilizes temporal memory tokens within bridge layers to allow for the encoding of entire video sequences alongside historical visual data, effectively preserving semantic continuity and enhancing model performance across various tasks. This approach includes recurrent memory tokens and a SceneTilling algorithm, which segments videos into independent semantic units to preserve semantic integrity. Empirically, VideoLLaMB significantly outstrips existing video-language models, demonstrating a 5.5 points improvement over its competitors across three VideoQA benchmarks, and 2.06 points on egocentric planning. Comprehensive results on the MVBench show that VideoLLaMB-7B achieves markedly better results than previous 7B models of same LLM. Remarkably, it maintains robust performance as PLLaVA even as video length increases up to 8 times. Besides, the frame retrieval results on our specialized Needle in a Video Haystack (NIAVH) benchmark, further validate VideoLLaMB's prowess in accurately identifying specific frames within lengthy videos. Our SceneTilling algorithm also enables the generation of streaming video captions directly, without necessitating additional training. In terms of efficiency, VideoLLaMB, trained on 16 frames, supports up to 320 frames on a single Nvidia A100 GPU with linear GPU memory scaling, ensuring both high performance and cost-effectiveness, thereby setting a new foundation for long-form video-language models in both academic and practical applications.</li>
</ul>

<h3>Title: Towards Robust Online Domain Adaptive Semantic Segmentation under Adverse Weather Conditions</h3>
<ul>
<li><strong>Authors: </strong>Taorong Liu, Jing Xiao, Liang Liao, Chia-Wen Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01072">https://arxiv.org/abs/2409.01072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01072">https://arxiv.org/pdf/2409.01072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01072]] Towards Robust Online Domain Adaptive Semantic Segmentation under Adverse Weather Conditions(https://arxiv.org/abs/2409.01072)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Online Domain Adaptation (OnDA) is designed to handle unforeseeable domain changes at minimal cost that occur during the deployment of the model, lacking clear boundaries between the domain, such as sudden weather events. However, existing OnDA methods that rely solely on the model itself to adapt to the current domain often misidentify ambiguous classes amidst continuous domain shifts and pass on this erroneous knowledge to the next domain. To tackle this, we propose \textbf{RODASS}, a \textbf{R}obust \textbf{O}nline \textbf{D}omain \textbf{A}daptive \textbf{S}emantic \textbf{S}egmentation framework, which dynamically detects domain shifts and adjusts hyper-parameters to minimize training costs and error propagation. Specifically, we introduce the \textbf{D}ynamic \textbf{A}mbiguous \textbf{P}atch \textbf{Mask} (\textbf{DAP Mask}) strategy, which dynamically selects highly disturbed regions and masks these regions, mitigating error accumulation in ambiguous classes and enhancing the model's robustness against external noise in dynamic natural environments. Additionally, we present the \textbf{D}ynamic \textbf{S}ource \textbf{C}lass \textbf{Mix} (\textbf{DSC Mix}), a domain-aware mix method that augments target domain scenes with class-level source buffers, reducing the high uncertainty and noisy labels, thereby accelerating adaptation and offering a more efficient solution for online domain adaptation. Our approach outperforms state-of-the-art methods on widely used OnDA benchmarks while maintaining approximately 40 frames per second (FPS).</li>
</ul>

<h3>Title: SCOPE: Sign Language Contextual Processing with Embedding from LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yuqi Liu, Wenqian Zhang, Sihan Ren, Chengyu Huang, Jingyi Yu, Lan Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01073">https://arxiv.org/abs/2409.01073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01073">https://arxiv.org/pdf/2409.01073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01073]] SCOPE: Sign Language Contextual Processing with Embedding from LLMs(https://arxiv.org/abs/2409.01073)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Sign languages, used by around 70 million Deaf individuals globally, are visual languages that convey visual and contextual information. Current methods in vision-based sign language recognition (SLR) and translation (SLT) struggle with dialogue scenes due to limited dataset diversity and the neglect of contextually relevant information. To address these challenges, we introduce SCOPE (Sign language Contextual Processing with Embedding from LLMs), a novel context-aware vision-based SLR and SLT framework. For SLR, we utilize dialogue contexts through a multi-modal encoder to enhance gloss-level recognition. For subsequent SLT, we further fine-tune a Large Language Model (LLM) by incorporating prior conversational context. We also contribute a new sign language dataset that contains 72 hours of Chinese sign language videos in contextual dialogues across various scenarios. Experimental results demonstrate that our SCOPE framework achieves state-of-the-art performance on multiple datasets, including Phoenix-2014T, CSL-Daily, and our SCOPE dataset. Moreover, surveys conducted with participants from the Deaf community further validate the robustness and effectiveness of our approach in real-world applications. Both our dataset and code will be open-sourced to facilitate further research.</li>
</ul>

<h3>Title: Evidential Transformers for Improved Image Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Danilo Dordevic, Suryansh Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01082">https://arxiv.org/abs/2409.01082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01082">https://arxiv.org/pdf/2409.01082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01082]] Evidential Transformers for Improved Image Retrieval(https://arxiv.org/abs/2409.01082)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>We introduce the Evidential Transformer, an uncertainty-driven transformer model for improved and robust image retrieval. In this paper, we make several contributions to content-based image retrieval (CBIR). We incorporate probabilistic methods into image retrieval, achieving robust and reliable results, with evidential classification surpassing traditional training based on multiclass classification as a baseline for deep metric learning. Furthermore, we improve the state-of-the-art retrieval results on several datasets by leveraging the Global Context Vision Transformer (GC ViT) architecture. Our experimental results consistently demonstrate the reliability of our approach, setting a new benchmark in CBIR in all test settings on the Stanford Online Products (SOP) and CUB-200-2011 datasets.</li>
</ul>

<h3>Title: DPDEdit: Detail-Preserved Diffusion Models for Multimodal Fashion Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Xiaolong Wang, Zhi-Qi Cheng, Jue Wang, Xiaojiang Peng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01086">https://arxiv.org/abs/2409.01086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01086">https://arxiv.org/pdf/2409.01086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01086]] DPDEdit: Detail-Preserved Diffusion Models for Multimodal Fashion Image Editing(https://arxiv.org/abs/2409.01086)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Fashion image editing is a crucial tool for designers to convey their creative ideas by visualizing design concepts interactively. Current fashion image editing techniques, though advanced with multimodal prompts and powerful diffusion models, often struggle to accurately identify editing regions and preserve the desired garment texture detail. To address these challenges, we introduce a new multimodal fashion image editing architecture based on latent diffusion models, called Detail-Preserved Diffusion Models (DPDEdit). DPDEdit guides the fashion image generation of diffusion models by integrating text prompts, region masks, human pose images, and garment texture images. To precisely locate the editing region, we first introduce Grounded-SAM to predict the editing region based on the user's textual description, and then combine it with other conditions to perform local editing. To transfer the detail of the given garment texture into the target fashion image, we propose a texture injection and refinement mechanism. Specifically, this mechanism employs a decoupled cross-attention layer to integrate textual descriptions and texture images, and incorporates an auxiliary U-Net to preserve the high-frequency details of generated garment texture. Additionally, we extend the VITON-HD dataset using a multimodal large language model to generate paired samples with texture images and textual descriptions. Extensive experiments show that our DPDEdit outperforms state-of-the-art methods in terms of image fidelity and coherence with the given multimodal inputs.</li>
</ul>

<h3>Title: Pre-Trained Language Models for Keyphrase Prediction: A Review</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Umair, Tangina Sultana, Young-Koo Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01087">https://arxiv.org/abs/2409.01087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01087">https://arxiv.org/pdf/2409.01087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01087]] Pre-Trained Language Models for Keyphrase Prediction: A Review(https://arxiv.org/abs/2409.01087)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Keyphrase Prediction (KP) is essential for identifying keyphrases in a document that can summarize its content. However, recent Natural Language Processing (NLP) advances have developed more efficient KP models using deep learning techniques. The limitation of a comprehensive exploration jointly both keyphrase extraction and generation using pre-trained language models spotlights a critical gap in the literature, compelling our survey paper to bridge this deficiency and offer a unified and in-depth analysis to address limitations in previous surveys. This paper extensively examines the topic of pre-trained language models for keyphrase prediction (PLM-KP), which are trained on large text corpora via different learning (supervisor, unsupervised, semi-supervised, and self-supervised) techniques, to provide respective insights into these two types of tasks in NLP, precisely, Keyphrase Extraction (KPE) and Keyphrase Generation (KPG). We introduce appropriate taxonomies for PLM-KPE and KPG to highlight these two main tasks of NLP. Moreover, we point out some promising future directions for predicting keyphrases.</li>
</ul>

<h3>Title: CARIn: Constraint-Aware and Responsive Inference on Heterogeneous Devices for Single- and Multi-DNN Workloads</h3>
<ul>
<li><strong>Authors: </strong>Ioannis Panopoulos, Stylianos I. Venieris, Iakovos S. Venieris</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01089">https://arxiv.org/abs/2409.01089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01089">https://arxiv.org/pdf/2409.01089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01089]] CARIn: Constraint-Aware and Responsive Inference on Heterogeneous Devices for Single- and Multi-DNN Workloads(https://arxiv.org/abs/2409.01089)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, fair, transformer</a></li>
<li><strong>Abstract: </strong>The relentless expansion of deep learning applications in recent years has prompted a pivotal shift toward on-device execution, driven by the urgent need for real-time processing, heightened privacy concerns, and reduced latency across diverse domains. This article addresses the challenges inherent in optimising the execution of deep neural networks (DNNs) on mobile devices, with a focus on device heterogeneity, multi-DNN execution, and dynamic runtime adaptation. We introduce CARIn, a novel framework designed for the optimised deployment of both single- and multi-DNN applications under user-defined service-level objectives. Leveraging an expressive multi-objective optimisation framework and a runtime-aware sorting and search algorithm (RASS) as the MOO solver, CARIn facilitates efficient adaptation to dynamic conditions while addressing resource contention issues associated with multi-DNN execution. Notably, RASS generates a set of configurations, anticipating subsequent runtime adaptation, ensuring rapid, low-overhead adjustments in response to environmental fluctuations. Extensive evaluation across diverse tasks, including text classification, scene recognition, and face analysis, showcases the versatility of CARIn across various model architectures, such as Convolutional Neural Networks and Transformers, and realistic use cases. We observe a substantial enhancement in the fair treatment of the problem's objectives, reaching 1.92x when compared to single-model designs and up to 10.69x in contrast to the state-of-the-art OODIn framework. Additionally, we achieve a significant gain of up to 4.06x over hardware-unaware designs in multi-DNN applications. Finally, our framework sustains its performance while effectively eliminating the time overhead associated with identifying the optimal design in response to environmental challenges.</li>
</ul>

<h3>Title: DS MYOLO: A Reliable Object Detector Based on SSMs for Driving Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Yang Li, Jianli Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01093">https://arxiv.org/abs/2409.01093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01093">https://arxiv.org/pdf/2409.01093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01093]] DS MYOLO: A Reliable Object Detector Based on SSMs for Driving Scenarios(https://arxiv.org/abs/2409.01093)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate real-time object detection enhances the safety of advanced driver-assistance systems, making it an essential component in driving scenarios. With the rapid development of deep learning technology, CNN-based YOLO real-time object detectors have gained significant attention. However, the local focus of CNNs results in performance bottlenecks. To further enhance detector performance, researchers have introduced Transformer-based self-attention mechanisms to leverage global receptive fields, but their quadratic complexity incurs substantial computational costs. Recently, Mamba, with its linear complexity, has made significant progress through global selective scanning. Inspired by Mamba's outstanding performance, we propose a novel object detector: DS MYOLO. This detector captures global feature information through a simplified selective scanning fusion block (SimVSS Block) and effectively integrates the network's deep features. Additionally, we introduce an efficient channel attention convolution (ECAConv) that enhances cross-channel feature interaction while maintaining low computational complexity. Extensive experiments on the CCTSDB 2021 and VLD-45 driving scenarios datasets demonstrate that DS MYOLO exhibits significant potential and competitive advantage among similarly scaled YOLO series real-time object detectors.</li>
</ul>

<h3>Title: OCMG-Net: Neural Oriented Normal Refinement for Unstructured Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Yingrui Wu, Mingyang Zhao, Weize Quan, Jian Shi, Xiaohong Jia, Dong-Ming Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01100">https://arxiv.org/abs/2409.01100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01100">https://arxiv.org/pdf/2409.01100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01100]] OCMG-Net: Neural Oriented Normal Refinement for Unstructured Point Clouds(https://arxiv.org/abs/2409.01100)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a robust refinement method for estimating oriented normals from unstructured point clouds. In contrast to previous approaches that either suffer from high computational complexity or fail to achieve desirable accuracy, our novel framework incorporates sign orientation and data augmentation in the feature space to refine the initial oriented normals, striking a balance between efficiency and accuracy. To address the issue of noise-caused direction inconsistency existing in previous approaches, we introduce a new metric called the Chamfer Normal Distance, which faithfully minimizes the estimation error by correcting the annotated normal with the closest point found on the potentially clean point cloud. This metric not only tackles the challenge but also aids in network training and significantly enhances network robustness against noise. Moreover, we propose an innovative dual-parallel architecture that integrates Multi-scale Local Feature Aggregation and Hierarchical Geometric Information Fusion, which enables the network to capture intricate geometric details more effectively and notably reduces ambiguity in scale selection. Extensive experiments demonstrate the superiority and versatility of our method in both unoriented and oriented normal estimation tasks across synthetic and real-world datasets among indoor and outdoor scenarios. The code is available at this https URL.</li>
</ul>

<h3>Title: Poster: Developing an O-RAN Security Test Lab</h3>
<ul>
<li><strong>Authors: </strong>Sotiris Michaelides, David Rupprecht, Katharina Kohls</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01107">https://arxiv.org/abs/2409.01107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01107">https://arxiv.org/pdf/2409.01107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01107]] Poster: Developing an O-RAN Security Test Lab(https://arxiv.org/abs/2409.01107)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Open Radio Access Networks (ORAN) is a new architectural approach, having been proposed only a few years ago, and it is an expansion of the current Next Generation Radio Access Networks (NG-RAN) of 5G. ORAN aims to break this closed RAN market that is controlled by a handful of vendors, by implementing open interfaces between the different Radio Access Networks (RAN) components, and by introducing modern technologies to the RAN like machine learning, virtualization, and disaggregation. However, the architectural design of ORAN was recently causing concerns and debates about its security, which is considered one of its major drawbacks. Several theoretical risk analyses related to ORAN have been conducted, but to the best of our knowledge, not even a single practical one has been performed yet. In this poster, we discuss and propose a way for a minimal, future-proof deployment of an ORAN 5G network, able to accommodate various hands-on security analyses for its different elements.</li>
</ul>

<h3>Title: SOOD-ImageNet: a Large-Scale Dataset for Semantic Out-Of-Distribution Image Classification and Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Alberto Bacchin, Davide Allegro, Stefano Ghidoni, Emanuele Menegatti</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01109">https://arxiv.org/abs/2409.01109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01109">https://arxiv.org/pdf/2409.01109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01109]] SOOD-ImageNet: a Large-Scale Dataset for Semantic Out-Of-Distribution Image Classification and Semantic Segmentation(https://arxiv.org/abs/2409.01109)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Out-of-Distribution (OOD) detection in computer vision is a crucial research area, with related benchmarks playing a vital role in assessing the generalizability of models and their applicability in real-world scenarios. However, existing OOD benchmarks in the literature suffer from two main limitations: (1) they often overlook semantic shift as a potential challenge, and (2) their scale is limited compared to the large datasets used to train modern models. To address these gaps, we introduce SOOD-ImageNet, a novel dataset comprising around 1.6M images across 56 classes, designed for common computer vision tasks such as image classification and semantic segmentation under OOD conditions, with a particular focus on the issue of semantic shift. We ensured the necessary scalability and quality by developing an innovative data engine that leverages the capabilities of modern vision-language models, complemented by accurate human checks. Through extensive training and evaluation of various models on SOOD-ImageNet, we showcase its potential to significantly advance OOD research in computer vision. The project page is available at this https URL.</li>
</ul>

<h3>Title: Diffusion-Driven Data Replay: A Novel Approach to Combat Forgetting in Federated Class Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Jinglin Liang, Jin Zhong, Hanlin Gu, Zhongqi Lu, Xingxing Tang, Gang Dai, Shuangping Huang, Lixin Fan, Qiang Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01128">https://arxiv.org/abs/2409.01128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01128">https://arxiv.org/pdf/2409.01128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01128]] Diffusion-Driven Data Replay: A Novel Approach to Combat Forgetting in Federated Class Continual Learning(https://arxiv.org/abs/2409.01128)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, diffusion, data-free</a></li>
<li><strong>Abstract: </strong>Federated Class Continual Learning (FCCL) merges the challenges of distributed client learning with the need for seamless adaptation to new classes without forgetting old ones. The key challenge in FCCL is catastrophic forgetting, an issue that has been explored to some extent in Continual Learning (CL). However, due to privacy preservation requirements, some conventional methods, such as experience replay, are not directly applicable to FCCL.Existing FCCL methods mitigate forgetting by generating historical data through federated training of GANs or data-free knowledge distillation. However, these approaches often suffer from unstable training of generators or low-quality generated data, limiting their guidance for the this http URL address this challenge, we propose a novel method of data replay based on diffusion models. Instead of training a diffusion model, we employ a pre-trained conditional diffusion model to reverse-engineer each class, searching the corresponding input conditions for each class within the model's input space, significantly reducing computational resources and time consumption while ensuring effective generation. Furthermore, we enhance the classifier's domain generalization ability on generated and real data through contrastive learning, indirectly improving the representational capability of generated data for real data. Comprehensive experiments demonstrate that our method significantly outperforms existing baselines.Code is available at this https URL.</li>
</ul>

<h3>Title: Learning Robust Representations for Communications over Noisy Channels</h3>
<ul>
<li><strong>Authors: </strong>Sudharsan Senthil, Shubham Paul, Nambi Seshadri, R. David Koilpillai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01129">https://arxiv.org/abs/2409.01129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01129">https://arxiv.org/pdf/2409.01129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01129]] Learning Robust Representations for Communications over Noisy Channels(https://arxiv.org/abs/2409.01129)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A deep learning (DL)-based communication system offers advantages over traditional mathematically modelled systems, as the former may be jointly optimized. FCNNs (Fully Connected Neural Networks) are common Deep Learning architectures. Though they are well known to solve optimization problems, existing literature suggests that they fail to learn robust representations for communication models. This work explores the potential of FCNNs to learn an end-to-end communication system without taking any inspiration from existing classical models. The study investigates the impact of imbibing domain knowledge by varying cost functions to generate robust representations of symbols under strict power constraints. Additionally, we introduce a novel encoder structure inspired by the Barlow Twins framework. Finally, we introduce a training strategy that addresses the often-overlooked issue of training Signal to Noise Ratio (SNR) sensitivity, highlighting its importance in communication systems. We demonstrate that such a method leads to more reliable models.</li>
</ul>

<h3>Title: Large Language Models Can Understanding Depth from Monocular Images</h3>
<ul>
<li><strong>Authors: </strong>Zhongyi Xia, Tianzhao Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01133">https://arxiv.org/abs/2409.01133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01133">https://arxiv.org/pdf/2409.01133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01133]] Large Language Models Can Understanding Depth from Monocular Images(https://arxiv.org/abs/2409.01133)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Monocular depth estimation is a critical function in computer vision applications. This paper shows that large language models (LLMs) can effectively interpret depth with minimal supervision, using efficient resource utilization and a consistent neural network architecture. We introduce LLM-MDE, a multimodal framework that deciphers depth through language comprehension. Specifically, LLM-MDE employs two main strategies to enhance the pretrained LLM's capability for depth estimation: cross-modal reprogramming and an adaptive prompt estimation module. These strategies align vision representations with text prototypes and automatically generate prompts based on monocular images, respectively. Comprehensive experiments on real-world MDE datasets confirm the effectiveness and superiority of LLM-MDE, which excels in few-/zero-shot tasks while minimizing resource use. The source code is available.</li>
</ul>

<h3>Title: Generating Synthetic Satellite Imagery for Rare Objects: An Empirical Comparison of Models and Metrics</h3>
<ul>
<li><strong>Authors: </strong>Tuong Vy Nguyen, Johannes Hoster, Alexander Glaser, Kristian Hildebrand, Felix Biessmann</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01138">https://arxiv.org/abs/2409.01138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01138">https://arxiv.org/pdf/2409.01138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01138]] Generating Synthetic Satellite Imagery for Rare Objects: An Empirical Comparison of Models and Metrics(https://arxiv.org/abs/2409.01138)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative deep learning architectures can produce realistic, high-resolution fake imagery -- with potentially drastic societal implications. A key question in this context is: How easy is it to generate realistic imagery, in particular for niche domains. The iterative process required to achieve specific image content is difficult to automate and control. Especially for rare classes, it remains difficult to assess fidelity, meaning whether generative approaches produce realistic imagery and alignment, meaning how (well) the generation can be guided by human input. In this work, we present a large-scale empirical evaluation of generative architectures which we fine-tuned to generate synthetic satellite imagery. We focus on nuclear power plants as an example of a rare object category - as there are only around 400 facilities worldwide, this restriction is exemplary for many other scenarios in which training and test data is limited by the restricted number of occurrences of real-world examples. We generate synthetic imagery by conditioning on two kinds of modalities, textual input and image input obtained from a game engine that allows for detailed specification of the building layout. The generated images are assessed by commonly used metrics for automatic evaluation and then compared with human judgement from our conducted user studies to assess their trustworthiness. Our results demonstrate that even for rare objects, generation of authentic synthetic satellite imagery with textual or detailed building layouts is feasible. In line with previous work, we find that automated metrics are often not aligned with human perception -- in fact, we find strong negative correlations between commonly used image quality metrics and human ratings.</li>
</ul>

<h3>Title: FMRFT: Fusion Mamba and DETR for Query Time Sequence Intersection Fish Tracking</h3>
<ul>
<li><strong>Authors: </strong>Mingyuan Yao, Yukang Huo, Qingbin Tian, Jiayin Zhao, Xiao Liu, Ruifeng Wang, Haihua Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01148">https://arxiv.org/abs/2409.01148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01148">https://arxiv.org/pdf/2409.01148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01148]] FMRFT: Fusion Mamba and DETR for Query Time Sequence Intersection Fish Tracking(https://arxiv.org/abs/2409.01148)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Growth, abnormal behavior, and diseases of fish can be early detected by monitoring fish tracking through the method of image processing, which is of great significance for factory aquaculture. However, underwater reflections and some reasons with fish, such as the high similarity , rapid swimming caused by stimuli and multi-object occlusion bring challenges to multi-target tracking of fish. To address these challenges, this paper establishes a complex multi-scene sturgeon tracking dataset and proposes a real-time end-to-end fish tracking model, FMRFT. In this model, the Mamba In Mamba (MIM) architecture with low memory consumption is introduced into the tracking algorithm to realize multi-frame video timing memory and fast feature extraction, which improves the efficiency of correlation analysis for contiguous frames in multi-fish video. Additionally, the superior feature interaction and a priori frame processing capabilities of RT-DETR are leveraged to provide an effective tracking algorithm. By incorporating the QTSI query interaction processing module, the model effectively handles occluded objects and redundant tracking frames, resulting in more accurate and stable fish tracking. Trained and tested on the dataset, the model achieves an IDF1 score of 90.3% and a MOTA accuracy of 94.3%. Experimental results demonstrate that the proposed FMRFT model effectively addresses the challenges of high similarity and mutual occlusion in fish populations, enabling accurate tracking in factory farming environments.</li>
</ul>

<h3>Title: Understanding Multimodal Hallucination with Parameter-Free Representation Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yueqian Wang, Jianxin Liang, Yuxuan Wang, Huishuai Zhang, Dongyan Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01151">https://arxiv.org/abs/2409.01151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01151">https://arxiv.org/pdf/2409.01151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01151]] Understanding Multimodal Hallucination with Parameter-Free Representation Alignment(https://arxiv.org/abs/2409.01151)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Hallucination is a common issue in Multimodal Large Language Models (MLLMs), yet the underlying principles remain poorly understood. In this paper, we investigate which components of MLLMs contribute to object hallucinations. To analyze image representations while completely avoiding the influence of all other factors other than the image representation itself, we propose a parametric-free representation alignment metric (Pfram) that can measure the similarities between any two representation systems without requiring additional training parameters. Notably, Pfram can also assess the alignment of a neural representation system with the human representation system, represented by ground-truth annotations of images. By evaluating the alignment with object annotations, we demonstrate that this metric shows strong and consistent correlations with object hallucination across a wide range of state-of-the-art MLLMs, spanning various model architectures and sizes. Furthermore, using this metric, we explore other key issues related to image representations in MLLMs, such as the role of different modules, the impact of textual instructions, and potential improvements including the use of alternative visual encoders. Our code is available at: this https URL.</li>
</ul>

<h3>Title: TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Leqi Shen, Tianxiang Hao, Sicheng Zhao, Yifeng Zhang, Pengzhang Liu, Yongjun Bao, Guiguang Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01156">https://arxiv.org/abs/2409.01156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01156">https://arxiv.org/pdf/2409.01156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01156]] TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval(https://arxiv.org/abs/2409.01156)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Most text-video retrieval methods utilize the text-image pre-trained CLIP as a backbone, incorporating complex modules that result in high computational overhead. As a result, many studies focus on efficient fine-tuning. The primary challenge in efficient adaption arises from the inherent differences between image and video modalities. Each sampled video frame must be processed by the image encoder independently, which increases complexity and complicates practical deployment. Although existing efficient methods fine-tune with small trainable parameters, they still incur high inference costs due to the large token number. In this work, we argue that temporal redundancy significantly contributes to the model's high complexity due to the repeated information in consecutive frames. Existing token compression methods for image models fail to solve the unique challenges, as they overlook temporal redundancy across frames. To tackle these problems, we propose Temporal Token Merging (TempMe) to reduce temporal redundancy. Specifically, we introduce a progressive multi-granularity framework. By gradually combining neighboring clips, we merge temporal tokens across different frames and learn video-level features, leading to lower complexity and better performance. Extensive experiments validate the superiority of our TempMe. Compared to previous efficient text-video retrieval methods, TempMe significantly reduces output tokens by 95% and GFLOPs by 51%, while achieving a 1.8X speedup and a 4.4% R-Sum improvement. Additionally, TempMe exhibits robust generalization capabilities by integrating effectively with both efficient and full fine-tuning methods. With full fine-tuning, TempMe achieves a significant 7.9% R-Sum improvement, trains 1.57X faster, and utilizes 75.2% GPU memory usage. Our code will be released.</li>
</ul>

<h3>Title: Balancing Performance and Efficiency: A Multimodal Large Language Model Pruning Method based Image Text Interaction</h3>
<ul>
<li><strong>Authors: </strong>Gaotong Yu, Yi Chen, Jian Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01162">https://arxiv.org/abs/2409.01162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01162">https://arxiv.org/pdf/2409.01162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01162]] Balancing Performance and Efficiency: A Multimodal Large Language Model Pruning Method based Image Text Interaction(https://arxiv.org/abs/2409.01162)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Recently, multimodal large language models (MM-LLMs) have achieved great success in many multimodal tasks, but their high computational costs limit their further promotion and application. In the MM-LLMs framework, the main computational consumption step is the processing of concatenated text and visual tokens at the LLM layer. The length of the input token for LLM directly affects the overall training and inference efficiency. In response to this issue, we further studied the visual tokens of MM-LLMs. We found that the similarity between visual and CLS tokens in the visual encoder follows a long-tail distribution. In other words, only a few visual tokens are highly similar to CLS tokens. Therefore, we designed a dynamic pruning algorithm to address this issue. Firstly, for different input samples, we search for the inflection point of their visual CLS token similarity curve and use it as the corresponding segmentation point to trim the visual markers. This process mainly reduces the output of the visual encoder to accelerate the model. Then, in the LLM layer, the concatenated visual text tokens are pruned for the second time. During this process, due to the interaction between visual and textual features, visual and textual tokens with low text correlation are further filtered, achieving a balance between efficiency and performance. The results on multiple datasets show that our proposed method can achieve performance that competes with the original performance when using an average of 22% of the original token quantity. Our source code will be made publicly available following acceptance.</li>
</ul>

<h3>Title: Logit Scaling for Out-of-Distribution Detection</h3>
<ul>
<li><strong>Authors: </strong>Andrija Djurisic, Rosanne Liu, Mladen Nikolic</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01175">https://arxiv.org/abs/2409.01175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01175">https://arxiv.org/pdf/2409.01175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01175]] Logit Scaling for Out-of-Distribution Detection(https://arxiv.org/abs/2409.01175)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The safe deployment of machine learning and AI models in open-world settings hinges critically on the ability to detect out-of-distribution (OOD) data accurately, data samples that contrast vastly from what the model was trained with. Current approaches to OOD detection often require further training the model, and/or statistics about the training data which may no longer be accessible. Additionally, many existing OOD detection methods struggle to maintain performance when transferred across different architectures. Our research tackles these issues by proposing a simple, post-hoc method that does not require access to the training data distribution, keeps a trained network intact, and holds strong performance across a variety of architectures. Our method, Logit Scaling (LTS), as the name suggests, simply scales the logits in a manner that effectively distinguishes between in-distribution (ID) and OOD samples. We tested our method on benchmarks across various scales, including CIFAR-10, CIFAR-100, ImageNet and OpenOOD. The experiments cover 3 ID and 14 OOD datasets, as well as 9 model architectures. Overall, we demonstrate state-of-the-art performance, robustness and adaptability across different architectures, paving the way towards a universally applicable solution for advanced OOD detection.</li>
</ul>

<h3>Title: Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information</h3>
<ul>
<li><strong>Authors: </strong>Yi Chen, Jian Xu, Xu-Yao Zhang, Wen-Zhuo Liu, Yang-Yang Liu, Cheng-Lin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01179">https://arxiv.org/abs/2409.01179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01179">https://arxiv.org/pdf/2409.01179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01179]] Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information(https://arxiv.org/abs/2409.01179)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>With the advancement of large-scale language modeling techniques, large multimodal models combining visual encoders with large language models have demonstrated exceptional performance in various visual tasks. Most of the current large-scale multimodal models achieve this by mapping visual features obtained from the visual encoder into a large language model and using them as inputs alongside text for downstream tasks. Therefore, the number of visual tokens directly affects the training and inference speed of the model. There has been significant work on token pruning for visual transformers, but for large multimodal models, only relying on visual information for token pruning or compression may lead to significant loss of important information. On the other hand, the textual input in the form of a question may contain valuable information that can aid in answering the question, providing additional knowledge to the model. To address the potential oversimplification and excessive pruning that can occur with most purely visual token pruning methods, we propose a text information-guided dynamic visual token recovery mechanism that does not require training. This mechanism leverages the similarity between the question text and visual tokens to recover visually meaningful tokens with important text information while merging other less important tokens. Experimental results demonstrate that our proposed method achieves comparable performance to the original approach while compressing the visual tokens to an average of 10% of the original quantity. Our source code will be made publicly available following acceptance.</li>
</ul>

<h3>Title: Backdoor Defense through Self-Supervised and Generative Learning</h3>
<ul>
<li><strong>Authors: </strong>Ivan Saboliƒá, Ivan Grubi≈°iƒá, Sini≈°a ≈†egviƒá</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01185">https://arxiv.org/abs/2409.01185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01185">https://arxiv.org/pdf/2409.01185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01185]] Backdoor Defense through Self-Supervised and Generative Learning(https://arxiv.org/abs/2409.01185)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, generative</a></li>
<li><strong>Abstract: </strong>Backdoor attacks change a small portion of training data by introducing hand-crafted triggers and rewiring the corresponding labels towards a desired target class. Training on such data injects a backdoor which causes malicious inference in selected test samples. Most defenses mitigate such attacks through various modifications of the discriminative learning procedure. In contrast, this paper explores an approach based on generative modelling of per-class distributions in a self-supervised representation space. Interestingly, these representations get either preserved or heavily disturbed under recent backdoor attacks. In both cases, we find that per-class generative models allow to detect poisoned data and cleanse the dataset. Experiments show that training on cleansed dataset greatly reduces the attack success rate and retains the accuracy on benign inputs.</li>
</ul>

<h3>Title: CLIBE: Detecting Dynamic Backdoors in Transformer-based NLP Models</h3>
<ul>
<li><strong>Authors: </strong>Rui Zeng, Xi Chen, Yuwen Pu, Xuhong Zhang, Tianyu Du, Shouling Ji</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01193">https://arxiv.org/abs/2409.01193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01193">https://arxiv.org/pdf/2409.01193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01193]] CLIBE: Detecting Dynamic Backdoors in Transformer-based NLP Models(https://arxiv.org/abs/2409.01193)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, steal, transformer</a></li>
<li><strong>Abstract: </strong>Backdoors can be injected into NLP models to induce misbehavior when the input text contains a specific feature, known as a trigger, which the attacker secretly selects. Unlike fixed words, phrases, or sentences used in the static text trigger, NLP dynamic backdoor attacks design triggers associated with abstract and latent text features, making them considerably stealthier than traditional static backdoor attacks. However, existing research on NLP backdoor detection primarily focuses on defending against static backdoor attacks, while detecting dynamic backdoors in NLP models remains largely unexplored. This paper presents CLIBE, the first framework to detect dynamic backdoors in Transformer-based NLP models. CLIBE injects a "few-shot perturbation" into the suspect Transformer model by crafting optimized weight perturbation in the attention layers to make the perturbed model classify a limited number of reference samples as a target label. Subsequently, CLIBE leverages the generalization ability of this few-shot perturbation to determine whether the original model contains a dynamic backdoor. Extensive evaluation on three advanced NLP dynamic backdoor attacks, two widely-used Transformer frameworks, and four real-world classification tasks strongly validates the effectiveness of CLIBE. We also demonstrate the robustness of CLIBE against various adaptive attacks. Furthermore, we employ CLIBE to scrutinize 49 popular Transformer models on Hugging Face and discover one exhibiting a high probability of containing a dynamic backdoor. We have contacted Hugging Face and provided detailed evidence of this model's backdoor behavior. Moreover, we extend CLIBE to detect backdoor text generation models modified to exhibit toxic behavior. To the best of our knowledge, CLIBE is the first framework capable of detecting backdoors in text generation models without access to trigger input test samples.</li>
</ul>

<h3>Title: OD-VAE: An Omni-dimensional Video Compressor for Improving Latent Video Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Liuhan Chen, Zongjian Li, Bin Lin, Bin Zhu, Qian Wang, Shenghai Yuan, Xing Zhou, Xinghua Cheng, Li Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01199">https://arxiv.org/abs/2409.01199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01199">https://arxiv.org/pdf/2409.01199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01199]] OD-VAE: An Omni-dimensional Video Compressor for Improving Latent Video Diffusion Model(https://arxiv.org/abs/2409.01199)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Variational Autoencoder (VAE), compressing videos into latent representations, is a crucial preceding component of Latent Video Diffusion Models (LVDMs). With the same reconstruction quality, the more sufficient the VAE's compression for videos is, the more efficient the LVDMs are. However, most LVDMs utilize 2D image VAE, whose compression for videos is only in the spatial dimension and often ignored in the temporal dimension. How to conduct temporal compression for videos in a VAE to obtain more concise latent representations while promising accurate reconstruction is seldom explored. To fill this gap, we propose an omni-dimension compression VAE, named OD-VAE, which can temporally and spatially compress videos. Although OD-VAE's more sufficient compression brings a great challenge to video reconstruction, it can still achieve high reconstructed accuracy by our fine design. To obtain a better trade-off between video reconstruction quality and compression speed, four variants of OD-VAE are introduced and analyzed. In addition, a novel tail initialization is designed to train OD-VAE more efficiently, and a novel inference strategy is proposed to enable OD-VAE to handle videos of arbitrary length with limited GPU memory. Comprehensive experiments on video reconstruction and LVDM-based video generation demonstrate the effectiveness and efficiency of our proposed methods.</li>
</ul>

<h3>Title: Towards General Industrial Intelligence: A Survey on IIoT-Enhanced Continual Large Models</h3>
<ul>
<li><strong>Authors: </strong>Jiao Chen, Jiayi He, Fangfang Chen, Zuohong Lv, Jianhua Tang, Weihua Li, Zuozhu Liu, Howard H. Yang, Guangjie Han</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01207">https://arxiv.org/abs/2409.01207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01207">https://arxiv.org/pdf/2409.01207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01207]] Towards General Industrial Intelligence: A Survey on IIoT-Enhanced Continual Large Models(https://arxiv.org/abs/2409.01207)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Currently, most applications in the Industrial Internet of Things (IIoT) still rely on CNN-based neural networks. Although Transformer-based large models (LMs), including language, vision, and multimodal models, have demonstrated impressive capabilities in AI-generated content (AIGC), their application in industrial domains, such as detection, planning, and control, remains relatively limited. Deploying pre-trained LMs in industrial environments often encounters the challenge of stability and plasticity due to the complexity of tasks, the diversity of data, and the dynamic nature of user demands. To address these challenges, the pre-training and fine-tuning strategy, coupled with continual learning, has proven to be an effective solution, enabling models to adapt to dynamic demands while continuously optimizing their inference and decision-making capabilities. This paper surveys the integration of LMs into IIoT-enhanced General Industrial Intelligence (GII), focusing on two key areas: LMs for GII and LMs on GII. The former focuses on leveraging LMs to provide optimized solutions for industrial application challenges, while the latter investigates continuous optimization of LMs learning and inference capabilities in collaborative scenarios involving industrial devices, edge computing, and cloud computing. This paper provides insights into the future development of GII, aiming to establish a comprehensive theoretical framework and research direction for GII, thereby advancing GII towards a more general and adaptive future.</li>
</ul>

<h3>Title: SBOM Generation Tools in the Python Ecosystem: an In-Detail Analysis</h3>
<ul>
<li><strong>Authors: </strong>Serena Cofano, Giacomo Benedetti, Matteo Dell'Amico</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01214">https://arxiv.org/abs/2409.01214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01214">https://arxiv.org/pdf/2409.01214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01214]] SBOM Generation Tools in the Python Ecosystem: an In-Detail Analysis(https://arxiv.org/abs/2409.01214)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Software Bills of Material (SBOMs), which improve transparency by listing the components constituting software, are a key countermeasure to the mounting problem of Software Supply Chain attacks. SBOM generation tools take project source files and provide an SBOM as output, interacting with the software ecosystem. While SBOMs are a substantial improvement for security practitioners, providing a complete and correct SBOM is still an open problem. This paper investigates the causes of the issues affecting SBOM completeness and correctness, focusing on the PyPI ecosystem. We analyze four popular SBOM generation tools using the CycloneDX standard. Our analysis highlights issues related to dependency versions, metadata files, remote dependencies, and optional dependencies. Additionally, we identified a systematic issue with the lack of standards for metadata in the PyPI ecosystem. This includes inconsistencies in the presence of metadata files as well as variations in how their content is formatted.</li>
</ul>

<h3>Title: ESP-PCT: Enhanced VR Semantic Performance through Efficient Compression of Temporal and Spatial Redundancies in Point Cloud Transformers</h3>
<ul>
<li><strong>Authors: </strong>Luoyu Mei, Shuai Wang, Yun Cheng, Ruofeng Liu, Zhimeng Yin, Wenchao Jiang, Shuai Wang, Wei Gong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01216">https://arxiv.org/abs/2409.01216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01216">https://arxiv.org/pdf/2409.01216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01216]] ESP-PCT: Enhanced VR Semantic Performance through Efficient Compression of Temporal and Spatial Redundancies in Point Cloud Transformers(https://arxiv.org/abs/2409.01216)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Semantic recognition is pivotal in virtual reality (VR) applications, enabling immersive and interactive experiences. A promising approach is utilizing millimeter-wave (mmWave) signals to generate point clouds. However, the high computational and memory demands of current mmWave point cloud models hinder their efficiency and reliability. To address this limitation, our paper introduces ESP-PCT, a novel Enhanced Semantic Performance Point Cloud Transformer with a two-stage semantic recognition framework tailored for VR applications. ESP-PCT takes advantage of the accuracy of sensory point cloud data and optimizes the semantic recognition process, where the localization and focus stages are trained jointly in an end-to-end manner. We evaluate ESP-PCT on various VR semantic recognition conditions, demonstrating substantial enhancements in recognition efficiency. Notably, ESP-PCT achieves a remarkable accuracy of 93.2% while reducing the computational requirements (FLOPs) by 76.9% and memory usage by 78.2% compared to the existing Point Transformer model simultaneously. These underscore ESP-PCT's potential in VR semantic recognition by achieving high accuracy and reducing redundancy. The code and data of this project are available at \url{this https URL}.</li>
</ul>

<h3>Title: A Review of Image Retrieval Techniques: Data Augmentation and Adversarial Learning Approaches</h3>
<ul>
<li><strong>Authors: </strong>Kim Jinwoo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01219">https://arxiv.org/abs/2409.01219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01219">https://arxiv.org/pdf/2409.01219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01219]] A Review of Image Retrieval Techniques: Data Augmentation and Adversarial Learning Approaches(https://arxiv.org/abs/2409.01219)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Image retrieval is a crucial research topic in computer vision, with broad application prospects ranging from online product searches to security surveillance systems. In recent years, the accuracy and efficiency of image retrieval have significantly improved due to advancements in deep learning. However, existing methods still face numerous challenges, particularly in handling large-scale datasets, cross-domain retrieval, and image perturbations that can arise from real-world conditions such as variations in lighting, occlusion, and viewpoint. Data augmentation techniques and adversarial learning methods have been widely applied in the field of image retrieval to address these challenges. Data augmentation enhances the model's generalization ability and robustness by generating more diverse training samples, simulating real-world variations, and reducing overfitting. Meanwhile, adversarial attacks and defenses introduce perturbations during training to improve the model's robustness against potential attacks, ensuring reliability in practical applications. This review comprehensively summarizes the latest research advancements in image retrieval, with a particular focus on the roles of data augmentation and adversarial learning techniques in enhancing retrieval performance. Future directions and potential challenges are also discussed.</li>
</ul>

<h3>Title: Prompt Compression with Context-Aware Sentence Encoding for Fast and Improved LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Barys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, Shane Luke</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01227">https://arxiv.org/abs/2409.01227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01227">https://arxiv.org/pdf/2409.01227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01227]] Prompt Compression with Context-Aware Sentence Encoding for Fast and Improved LLM Inference(https://arxiv.org/abs/2409.01227)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have triggered a new stream of research focusing on compressing the context length to reduce the computational cost while ensuring the retention of helpful information for LLMs to answer the given question. Token-based removal methods are one of the most prominent approaches in this direction, but risk losing the semantics of the context caused by intermediate token removal, especially under high compression ratios, while also facing challenges in computational efficiency. In this work, we propose context-aware prompt compression (CPC), a sentence-level prompt compression technique where its key innovation is a novel context-aware sentence encoder that provides a relevance score for each sentence for a given question. To train this encoder, we generate a new dataset consisting of questions, positives, and negative pairs where positives are sentences relevant to the question, while negatives are irrelevant context sentences. We train the encoder in a contrastive setup to learn context-aware sentence representations. Our method considerably outperforms prior works on prompt compression on benchmark datasets and is up to 10.93x faster at inference compared to the best token-level compression method. We also find better improvement for shorter length constraints in most benchmarks, showing the effectiveness of our proposed solution in the compression of relevant information in a shorter context. Finally, we release the code and the dataset for quick reproducibility and further development: this https URL.</li>
</ul>

<h3>Title: THInC: A Theory-Driven Framework for Computational Humor Detection</h3>
<ul>
<li><strong>Authors: </strong>Victor De Marez, Thomas Winters, Ayla Rigouts Terryn</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01232">https://arxiv.org/abs/2409.01232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01232">https://arxiv.org/pdf/2409.01232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01232]] THInC: A Theory-Driven Framework for Computational Humor Detection(https://arxiv.org/abs/2409.01232)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Humor is a fundamental aspect of human communication and cognition, as it plays a crucial role in social engagement. Although theories about humor have evolved over centuries, there is still no agreement on a single, comprehensive humor theory. Likewise, computationally recognizing humor remains a significant challenge despite recent advances in large language models. Moreover, most computational approaches to detecting humor are not based on existing humor theories. This paper contributes to bridging this long-standing gap between humor theory research and computational humor detection by creating an interpretable framework for humor classification, grounded in multiple humor theories, called THInC (Theory-driven Humor Interpretation and Classification). THInC ensembles interpretable GA2M classifiers, each representing a different humor theory. We engineered a transparent flow to actively create proxy features that quantitatively reflect different aspects of theories. An implementation of this framework achieves an F1 score of 0.85. The associative interpretability of the framework enables analysis of proxy efficacy, alignment of joke features with theories, and identification of globally contributing features. This paper marks a pioneering effort in creating a humor detection framework that is informed by diverse humor theories and offers a foundation for future advancements in theory-driven humor classification. It also serves as a first step in automatically comparing humor theories in a quantitative manner.</li>
</ul>

<h3>Title: SoK: Security of the Image Processing Pipeline in Autonomous Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Michael K√ºhr, Mohammad Hamad, Pedram MohajerAnsari, Mert D. Pes√©, Sebastian Steinhorst</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01234">https://arxiv.org/abs/2409.01234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01234">https://arxiv.org/pdf/2409.01234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01234]] SoK: Security of the Image Processing Pipeline in Autonomous Vehicles(https://arxiv.org/abs/2409.01234)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Cameras are crucial sensors for autonomous vehicles. They capture images that are essential for many safety-critical tasks, including perception. To process these images, a complex pipeline with multiple layers is used. Security attacks on this pipeline can severely affect passenger safety and system performance. However, many attacks overlook different layers of the pipeline, and their feasibility and impact vary. While there has been research to improve the quality and robustness of the image processing pipeline, these efforts often work in parallel with security research, without much awareness of their potential synergy. In this work, we aim to bridge this gap by combining security and robustness research for the image processing pipeline in autonomous vehicles. We classify the risk of attacks using the automotive security standard ISO 21434, emphasizing the need to consider all layers for overall system security. We also demonstrate how existing robustness research can help mitigate the impact of attacks, addressing the current research gap. Finally, we present an embedded testbed that can influence various parameters across all layers, allowing researchers to analyze the effects of different defense strategies and attack impacts. We demonstrate the importance of such a test environment through a use-case analysis and show how blinding attacks can be mitigated using HDR imaging as an example of robustness-related research.</li>
</ul>

<h3>Title: Adversarial Pruning: A Survey and Benchmark of Pruning Methods for Adversarial Robustness</h3>
<ul>
<li><strong>Authors: </strong>Giorgio Piras, Maura Pintor, Ambra Demontis, Battista Biggio, Giorgio Giacinto, Fabio Roli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01249">https://arxiv.org/abs/2409.01249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01249">https://arxiv.org/pdf/2409.01249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01249]] Adversarial Pruning: A Survey and Benchmark of Pruning Methods for Adversarial Robustness(https://arxiv.org/abs/2409.01249)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Recent work has proposed neural network pruning techniques to reduce the size of a network while preserving robustness against adversarial examples, i.e., well-crafted inputs inducing a misclassification. These methods, which we refer to as adversarial pruning methods, involve complex and articulated designs, making it difficult to analyze the differences and establish a fair and accurate comparison. In this work, we overcome these issues by surveying current adversarial pruning methods and proposing a novel taxonomy to categorize them based on two main dimensions: the pipeline, defining when to prune; and the specifics, defining how to prune. We then highlight the limitations of current empirical analyses and propose a novel, fair evaluation benchmark to address them. We finally conduct an empirical re-evaluation of current adversarial pruning methods and discuss the results, highlighting the shared traits of top-performing adversarial pruning methods, as well as common issues. We welcome contributions in our publicly-available benchmark at this https URL</li>
</ul>

<h3>Title: GAS: Generative Activation-Aided Asynchronous Split Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiarong Yang, Yuan Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01251">https://arxiv.org/abs/2409.01251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01251">https://arxiv.org/pdf/2409.01251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01251]] GAS: Generative Activation-Aided Asynchronous Split Federated Learning(https://arxiv.org/abs/2409.01251)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, generative</a></li>
<li><strong>Abstract: </strong>Split Federated Learning (SFL) splits and collaboratively trains a shared model between clients and server, where clients transmit activations and client-side models to server for updates. Recent SFL studies assume synchronous transmission of activations and client-side models from clients to server. However, due to significant variations in computational and communication capabilities among clients, activations and client-side models arrive at server asynchronously. The delay caused by asynchrony significantly degrades the performance of SFL. To address this issue, we consider an asynchronous SFL framework, where an activation buffer and a model buffer are embedded on the server to manage the asynchronously transmitted activations and client-side models, respectively. Furthermore, as asynchronous activation transmissions cause the buffer to frequently receive activations from resource-rich clients, leading to biased updates of the server-side model, we propose Generative activations-aided Asynchronous SFL (GAS). In GAS, the server maintains an activation distribution for each label based on received activations and generates activations from these distributions according to the degree of bias. These generative activations are then used to assist in updating the server-side model, ensuring more accurate updates. We derive a tighter convergence bound, and our experiments demonstrate the effectiveness of the proposed method.</li>
</ul>

<h3>Title: Path-Consistency: Prefix Enhancement for Efficient Inference in LLM</h3>
<ul>
<li><strong>Authors: </strong>Jiace Zhu, Yingtao Shen, Jie Zhao, An Zou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01281">https://arxiv.org/abs/2409.01281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01281">https://arxiv.org/pdf/2409.01281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01281]] Path-Consistency: Prefix Enhancement for Efficient Inference in LLM(https://arxiv.org/abs/2409.01281)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To enhance the reasoning capabilities of large language models (LLMs), self-consistency has gained significant popularity by combining multiple sampling with majority voting. However, the state-of-the-art self-consistency approaches consume substantial computational resources and lead to significant additional time costs due to the multiple sampling. This prevents its full potential from being realized in scenarios where computational resources are critical. To improve the inference efficiency, this paper introduces \textit{path-consistency}, a method that leverages the confidence of answers generated in earlier branches to identify the prefix of the most promising path. By dynamically guiding the generation of subsequent branches based on this prefix, the \textit{path-consistency} mitigates both the errors and redundancies from random or less useful sampling in self-consistency. As a result, it can significantly accelerate the inference process by reducing the number of tokens generated. Our extensive empirical evaluation shows that the \textit{path-consistency} achieves significant acceleration in inference latency ranging from $7.8\%$ to $40.5\%$, while maintaining or even improving task accuracy across different datasets, including mathematical reasoning, common sense reasoning, symbolic reasoning, and code generation.</li>
</ul>

<h3>Title: One-Index Vector Quantization Based Adversarial Attack on Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Haiju Fan, Xiaona Qin, Shuang Chen, Hubert P. H. Shum, Ming Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01282">https://arxiv.org/abs/2409.01282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01282">https://arxiv.org/pdf/2409.01282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01282]] One-Index Vector Quantization Based Adversarial Attack on Image Classification(https://arxiv.org/abs/2409.01282)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>To improve storage and transmission, images are generally compressed. Vector quantization (VQ) is a popular compression method as it has a high compression ratio that suppresses other compression techniques. Despite this, existing adversarial attack methods on image classification are mostly performed in the pixel domain with few exceptions in the compressed domain, making them less applicable in real-world scenarios. In this paper, we propose a novel one-index attack method in the VQ domain to generate adversarial images by a differential evolution algorithm, successfully resulting in image misclassification in victim models. The one-index attack method modifies a single index in the compressed data stream so that the decompressed image is misclassified. It only needs to modify a single VQ index to realize an attack, which limits the number of perturbed indexes. The proposed method belongs to a semi-black-box attack, which is more in line with the actual attack scenario. We apply our method to attack three popular image classification models, i.e., Resnet, NIN, and VGG16. On average, 55.9% and 77.4% of the images in CIFAR-10 and Fashion MNIST, respectively, are successfully attacked, with a high level of misclassification confidence and a low level of image perturbation.</li>
</ul>

<h3>Title: Topological degree as a discrete diagnostic for disentanglement, with applications to the $\Delta$VAE</h3>
<ul>
<li><strong>Authors: </strong>Mahefa Ratsisetraina Ravelonanosy, Vlado Menkovski, Jacobus W. Portegies</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.AT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01303">https://arxiv.org/abs/2409.01303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01303">https://arxiv.org/pdf/2409.01303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01303]] Topological degree as a discrete diagnostic for disentanglement, with applications to the $\Delta$VAE(https://arxiv.org/abs/2409.01303)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We investigate the ability of Diffusion Variational Autoencoder ($\Delta$VAE) with unit sphere $\mathcal{S}^2$ as latent space to capture topological and geometrical structure and disentangle latent factors in datasets. For this, we introduce a new diagnostic of disentanglement: namely the topological degree of the encoder, which is a map from the data manifold to the latent space. By using tools from homology theory, we derive and implement an algorithm that computes this degree. We use the algorithm to compute the degree of the encoder of models that result from the training procedure. Our experimental results show that the $\Delta$VAE achieves relatively small LSBD scores, and that regardless of the degree after initialization, the degree of the encoder after training becomes $-1$ or $+1$, which implies that the resulting encoder is at least homotopic to a homeomorphism.</li>
</ul>

<h3>Title: Disentangling Mean Embeddings for Better Diagnostics of Image Generators</h3>
<ul>
<li><strong>Authors: </strong>Sebastian G. Gruber, Pascal Tobias Ziegler, Florian Buettner</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01314">https://arxiv.org/abs/2409.01314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01314">https://arxiv.org/pdf/2409.01314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01314]] Disentangling Mean Embeddings for Better Diagnostics of Image Generators(https://arxiv.org/abs/2409.01314)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>The evaluation of image generators remains a challenge due to the limitations of traditional metrics in providing nuanced insights into specific image regions. This is a critical problem as not all regions of an image may be learned with similar ease. In this work, we propose a novel approach to disentangle the cosine similarity of mean embeddings into the product of cosine similarities for individual pixel clusters via central kernel alignment. Consequently, we can quantify the contribution of the cluster-wise performance to the overall image generation performance. We demonstrate how this enhances the explainability and the likelihood of identifying pixel regions of model misbehavior across various real-world use cases.</li>
</ul>

<h3>Title: LoGex: Improved tail detection of extremely rare histopathology classes via guided diffusion</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Mueller, Matthias Hein</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01317">https://arxiv.org/abs/2409.01317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01317">https://arxiv.org/pdf/2409.01317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01317]] LoGex: Improved tail detection of extremely rare histopathology classes via guided diffusion(https://arxiv.org/abs/2409.01317)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In realistic medical settings, the data are often inherently long-tailed, with most samples concentrated in a few classes and a long tail of rare classes, usually containing just a few samples. This distribution presents a significant challenge because rare conditions are critical to detect and difficult to classify due to limited data. In this paper, rather than attempting to classify rare classes, we aim to detect these as out-of-distribution data reliably. We leverage low-rank adaption (LoRA) and diffusion guidance to generate targeted synthetic data for the detection problem. We significantly improve the OOD detection performance on a challenging histopathological task with only ten samples per tail class without losing classification accuracy on the head classes.</li>
</ul>

<h3>Title: Guide-and-Rescale: Self-Guidance Mechanism for Effective Tuning-Free Real Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Vadim Titov, Madina Khalmatova, Alexandra Ivanova, Dmitry Vetrov, Aibek Alanov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01322">https://arxiv.org/abs/2409.01322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01322">https://arxiv.org/pdf/2409.01322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01322]] Guide-and-Rescale: Self-Guidance Mechanism for Effective Tuning-Free Real Image Editing(https://arxiv.org/abs/2409.01322)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Despite recent advances in large-scale text-to-image generative models, manipulating real images with these models remains a challenging problem. The main limitations of existing editing methods are that they either fail to perform with consistent quality on a wide range of image edits or require time-consuming hyperparameter tuning or fine-tuning of the diffusion model to preserve the image-specific appearance of the input image. We propose a novel approach that is built upon a modified diffusion sampling process via the guidance mechanism. In this work, we explore the self-guidance technique to preserve the overall structure of the input image and its local regions appearance that should not be edited. In particular, we explicitly introduce layout-preserving energy functions that are aimed to save local and global structures of the source image. Additionally, we propose a noise rescaling mechanism that allows to preserve noise distribution by balancing the norms of classifier-free guidance and our proposed guiders during generation. Such a guiding approach does not require fine-tuning the diffusion model and exact inversion process. As a result, the proposed method provides a fast and high-quality editing mechanism. In our experiments, we show through human evaluation and quantitative analysis that the proposed method allows to produce desired editing which is more preferable by humans and also achieves a better trade-off between editing quality and preservation of the original image. Our code is available at this https URL.</li>
</ul>

<h3>Title: SPDiffusion: Semantic Protection Diffusion for Multi-concept Text-to-image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhang, Rui Zhang, Xuecheng Nie, Haochen Li, Jikun Chen, Yifan Hao, Xin Zhang, Luoqi Liu, Ling Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01327">https://arxiv.org/abs/2409.01327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01327">https://arxiv.org/pdf/2409.01327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01327]] SPDiffusion: Semantic Protection Diffusion for Multi-concept Text-to-image Generation(https://arxiv.org/abs/2409.01327)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, diffusion</a></li>
<li><strong>Abstract: </strong>Recent text-to-image models have achieved remarkable success in generating high-quality images. However, when tasked with multi-concept generation which creates images containing multiple characters or objects, existing methods often suffer from attribute confusion, resulting in severe text-image inconsistency. We found that attribute confusion occurs when a certain region of the latent features attend to multiple or incorrect prompt tokens. In this work, we propose novel Semantic Protection Diffusion (SPDiffusion) to protect the semantics of regions from the influence of irrelevant tokens, eliminating the confusion of non-corresponding attributes. In the SPDiffusion framework, we design a Semantic Protection Mask (SP-Mask) to represent the relevance of the regions and the tokens, and propose a Semantic Protection Cross-Attention (SP-Attn) to shield the influence of irrelevant tokens on specific regions in the generation process. To evaluate our method, we created a diverse multi-concept benchmark, and SPDiffusion achieves state-of-the-art results on this benchmark, proving its effectiveness. Our method can be combined with many other application methods or backbones, such as ControlNet, Story Diffusion, PhotoMaker and PixArt-alpha to enhance their multi-concept capabilities, demonstrating strong compatibility and scalability.</li>
</ul>

<h3>Title: Assessing the Impact of Image Dataset Features on Privacy-Preserving Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Lucas Lange, Maurice-Maximilian Heykeroth, Erhard Rahm</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01329">https://arxiv.org/abs/2409.01329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01329">https://arxiv.org/pdf/2409.01329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01329]] Assessing the Impact of Image Dataset Features on Privacy-Preserving Machine Learning(https://arxiv.org/abs/2409.01329)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack</a></li>
<li><strong>Abstract: </strong>Machine Learning (ML) is crucial in many sectors, including computer vision. However, ML models trained on sensitive data face security challenges, as they can be attacked and leak information. Privacy-Preserving Machine Learning (PPML) addresses this by using Differential Privacy (DP) to balance utility and privacy. This study identifies image dataset characteristics that affect the utility and vulnerability of private and non-private Convolutional Neural Network (CNN) models. Through analyzing multiple datasets and privacy budgets, we find that imbalanced datasets increase vulnerability in minority classes, but DP mitigates this issue. Datasets with fewer classes improve both model utility and privacy, while high entropy or low Fisher Discriminant Ratio (FDR) datasets deteriorate the utility-privacy trade-off. These insights offer valuable guidance for practitioners and researchers in estimating and optimizing the utility-privacy trade-off in image datasets, helping to inform data and privacy modifications for better outcomes based on dataset characteristics.</li>
</ul>

<h3>Title: Pediatric brain tumor classification using digital histopathology and deep learning: evaluation of SOTA methods on a multi-center Swedish cohort</h3>
<ul>
<li><strong>Authors: </strong>Iulian Emil Tampu, Per Nyman, Christoforos Spyretos, Ida Blystad, Alia Shamikh, Gabriela Prochazka, Teresita D√≠az de St√•hl, Johanna Sandgren, Peter Lundberg, Neda Haj-Hosseini</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01330">https://arxiv.org/abs/2409.01330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01330">https://arxiv.org/pdf/2409.01330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01330]] Pediatric brain tumor classification using digital histopathology and deep learning: evaluation of SOTA methods on a multi-center Swedish cohort(https://arxiv.org/abs/2409.01330)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability</a></li>
<li><strong>Abstract: </strong>Brain tumors are the most common solid tumors in children and young adults, but the scarcity of large histopathology datasets has limited the application of computational pathology in this group. This study implements two weakly supervised multiple-instance learning (MIL) approaches on patch-features obtained from state-of-the-art histology-specific foundation models to classify pediatric brain tumors in hematoxylin and eosin whole slide images (WSIs) from a multi-center Swedish cohort. WSIs from 540 subjects (age 8.5$\pm$4.9 years) diagnosed with brain tumor were gathered from the six Swedish university hospitals. Instance (patch)-level features were obtained from WSIs using three pre-trained feature extractors: ResNet50, UNI and CONCH. Instances were aggregated using attention-based MIL (ABMIL) or clustering-constrained attention MIL (CLAM) for patient-level classification. Models were evaluated on three classification tasks based on the hierarchical classification of pediatric brain tumors: tumor category, family and type. Model generalization was assessed by training on data from two of the centers and testing on data from four other centers. Model interpretability was evaluated through attention-mapping. The highest classification performance was achieved using UNI features and AMBIL aggregation, with Matthew's correlation coefficient of 0.86$\pm$0.04, 0.63$\pm$0.04, and 0.53$\pm$0.05, for tumor category, family and type classification, respectively. When evaluating generalization, models utilizing UNI and CONCH features outperformed those using ResNet50. However, the drop in performance from the in-site to out-of-site testing was similar across feature extractors. These results show the potential of state-of-the-art computational pathology methods in diagnosing pediatric brain tumors at different hierarchical levels with fair generalizability on a multi-center national dataset.</li>
</ul>

<h3>Title: Target-Driven Distillation: Consistency Distillation with Target Timestep Selection and Decoupled Guidance</h3>
<ul>
<li><strong>Authors: </strong>Cunzheng Wang, Ziyuan Guo, Yuxuan Duan, Huaxia Li, Nemo Chen, Xu Tang, Yao Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01347">https://arxiv.org/abs/2409.01347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01347">https://arxiv.org/pdf/2409.01347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01347]] Target-Driven Distillation: Consistency Distillation with Target Timestep Selection and Decoupled Guidance(https://arxiv.org/abs/2409.01347)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Consistency distillation methods have demonstrated significant success in accelerating generative tasks of diffusion models. However, since previous consistency distillation methods use simple and straightforward strategies in selecting target timesteps, they usually struggle with blurs and detail losses in generated images. To address these limitations, we introduce Target-Driven Distillation (TDD), which (1) adopts a delicate selection strategy of target timesteps, increasing the training efficiency; (2) utilizes decoupled guidances during training, making TDD open to post-tuning on guidance scale during inference periods; (3) can be optionally equipped with non-equidistant sampling and x0 clipping, enabling a more flexible and accurate way for image sampling. Experiments verify that TDD achieves state-of-the-art performance in few-step generation, offering a better choice among consistency distillation models.</li>
</ul>

<h3>Title: PatternPaint: Generating Layout Patterns Using Generative AI and Inpainting Techniques</h3>
<ul>
<li><strong>Authors: </strong>Guanglei Zhou, Bhargav Korrapati, Gaurav Rajavendra Reddy, Jiang Hu, Yiran Chen, Dipto G. Thakurta</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CE, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01348">https://arxiv.org/abs/2409.01348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01348">https://arxiv.org/pdf/2409.01348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01348]] PatternPaint: Generating Layout Patterns Using Generative AI and Inpainting Techniques(https://arxiv.org/abs/2409.01348)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generation of VLSI layout patterns is essential for a wide range of Design For Manufacturability (DFM) studies. In this study, we investigate the potential of generative machine learning models for creating design rule legal metal layout patterns. Our results demonstrate that the proposed model can generate legal patterns in complex design rule settings and achieves a high diversity score. The designed system, with its flexible settings, supports both pattern generation with localized changes, and design rule violation correction. Our methodology is validated on Intel 18A Process Design Kit (PDK) and can produce a wide range of DRC-compliant pattern libraries with only 20 starter patterns.</li>
</ul>

<h3>Title: From Pixels to Objects: A Hierarchical Approach for Part and Object Segmentation Using Local and Global Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Yunfei Xie, Cihang Xie, Alan Yuille, Jieru Mei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01353">https://arxiv.org/abs/2409.01353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01353">https://arxiv.org/pdf/2409.01353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01353]] From Pixels to Objects: A Hierarchical Approach for Part and Object Segmentation Using Local and Global Aggregation(https://arxiv.org/abs/2409.01353)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce a hierarchical transformer-based model designed for sophisticated image segmentation tasks, effectively bridging the granularity of part segmentation with the comprehensive scope of object segmentation. At the heart of our approach is a multi-level representation strategy, which systematically advances from individual pixels to superpixels, and ultimately to cohesive group formations. This architecture is underpinned by two pivotal aggregation strategies: local aggregation and global aggregation. Local aggregation is employed to form superpixels, leveraging the inherent redundancy of the image data to produce segments closely aligned with specific parts of the object, guided by object-level supervision. In contrast, global aggregation interlinks these superpixels, organizing them into larger groups that correlate with entire objects and benefit from part-level supervision. This dual aggregation framework ensures a versatile adaptation to varying supervision inputs while maintaining computational efficiency. Our methodology notably improves the balance between adaptability across different supervision modalities and computational manageability, culminating in significant enhancement in segmentation performance. When tested on the PartImageNet dataset, our model achieves a substantial increase, outperforming the previous state-of-the-art by 2.8% and 0.8% in mIoU scores for part and object segmentation, respectively. Similarly, on the Pascal Part dataset, it records performance enhancements of 1.5% and 2.0% for part and object segmentation, respectively.</li>
</ul>

<h3>Title: Explanation Space: A New Perspective into Time Series Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Shahbaz Rezaei, Xin Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01354">https://arxiv.org/abs/2409.01354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01354">https://arxiv.org/pdf/2409.01354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01354]] Explanation Space: A New Perspective into Time Series Interpretability(https://arxiv.org/abs/2409.01354)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Human understandable explanation of deep learning models is necessary for many critical and sensitive applications. Unlike image or tabular data where the importance of each input feature (for the classifier's decision) can be directly projected into the input, time series distinguishable features (e.g. dominant frequency) are often hard to manifest in time domain for a user to easily understand. Moreover, most explanation methods require a baseline value as an indication of the absence of any feature. However, the notion of lack of feature, which is often defined as black pixels for vision tasks or zero/mean values for tabular data, is not well-defined in time series. Despite the adoption of explainable AI methods (XAI) from tabular and vision domain into time series domain, these differences limit the application of these XAI methods in practice. In this paper, we propose a simple yet effective method that allows a model originally trained on time domain to be interpreted in other explanation spaces using existing methods. We suggest four explanation spaces that each can potentially alleviate these issues in certain types of time series. Our method can be readily adopted in existing platforms without any change to trained models or XAI methods.</li>
</ul>

<h3>Title: A Survey and Comparison of Post-quantum and Quantum Blockchains</h3>
<ul>
<li><strong>Authors: </strong>Zebo Yang, Haneen Alfauri, Behrooz Farkiani, Raj Jain, Roberto Di Pietro, Aiman Erbad</a></li>
<li><strong>Subjects: </strong>cs.CR, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01358">https://arxiv.org/abs/2409.01358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01358">https://arxiv.org/pdf/2409.01358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01358]] A Survey and Comparison of Post-quantum and Quantum Blockchains(https://arxiv.org/abs/2409.01358)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack</a></li>
<li><strong>Abstract: </strong>Blockchains have gained substantial attention from academia and industry for their ability to facilitate decentralized trust and communications. However, the rapid progress of quantum computing poses a significant threat to the security of existing blockchain technologies. Notably, the emergence of Shor's and Grover's algorithms raises concerns regarding the compromise of the cryptographic systems underlying blockchains. Consequently, it is essential to develop methods that reinforce blockchain technology against quantum attacks. In response to this challenge, two distinct approaches have been proposed. The first approach involves post-quantum blockchains, which aim to utilize classical cryptographic algorithms resilient to quantum attacks. The second approach explores quantum blockchains, which leverage the power of quantum computers and networks to rebuild the foundations of blockchains. This paper aims to provide a comprehensive overview and comparison of post-quantum and quantum blockchains while exploring open questions and remaining challenges in these domains. It offers an in-depth introduction, examines differences in blockchain structure, security, privacy, and other key factors, and concludes by discussing current research trends.</li>
</ul>

<h3>Title: Correlating Time Series with Interpretable Convolutional Kernels</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Chen, HanQin Cai, Fuqiang Liu, Jinhua Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01362">https://arxiv.org/abs/2409.01362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01362">https://arxiv.org/pdf/2409.01362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01362]] Correlating Time Series with Interpretable Convolutional Kernels(https://arxiv.org/abs/2409.01362)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>This study addresses the problem of convolutional kernel learning in univariate, multivariate, and multidimensional time series data, which is crucial for interpreting temporal patterns in time series and supporting downstream machine learning tasks. First, we propose formulating convolutional kernel learning for univariate time series as a sparse regression problem with a non-negative constraint, leveraging the properties of circular convolution and circulant matrices. Second, to generalize this approach to multivariate and multidimensional time series data, we use tensor computations, reformulating the convolutional kernel learning problem in the form of tensors. This is further converted into a standard sparse regression problem through vectorization and tensor unfolding operations. In the proposed methodology, the optimization problem is addressed using the existing non-negative subspace pursuit method, enabling the convolutional kernel to capture temporal correlations and patterns. To evaluate the proposed model, we apply it to several real-world time series datasets. On the multidimensional rideshare and taxi trip data from New York City and Chicago, the convolutional kernels reveal interpretable local correlations and cyclical patterns, such as weekly seasonality. In the context of multidimensional fluid flow data, both local and nonlocal correlations captured by the convolutional kernels can reinforce tensor factorization, leading to performance improvements in fluid flow reconstruction tasks. Thus, this study lays an insightful foundation for automatically learning convolutional kernels from time series data, with an emphasis on interpretability through sparsity and non-negativity constraints.</li>
</ul>

<h3>Title: CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification</h3>
<ul>
<li><strong>Authors: </strong>Junhui He, Shangyu Wu, Weidong Wen, Chun Jason Xue, Qingan Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01366">https://arxiv.org/abs/2409.01366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01366">https://arxiv.org/pdf/2409.01366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01366]] CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification(https://arxiv.org/abs/2409.01366)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Deploying large language models (LLMs) on edge devices presents significant challenges due to the substantial computational overhead and memory requirements. Activation sparsification can mitigate these challenges by reducing the number of activated neurons during inference. Existing methods typically employ thresholding-based sparsification based on the statistics of activation tensors. However, these methods do not explicitly model the impact of activation sparsification on performance, leading to suboptimal performance degradation. To address this issue, this paper reformulates the activation sparsification problem by introducing a new objective that optimizes the sparsification decisions. Building on this reformulation, we propose CHESS, a general activation sparsification approach via CHannel-wise thrEsholding and Selective Sparsification. First, channel-wise thresholding assigns a unique threshold to each activation channel in the feed-forward network (FFN) layers. Then, selective sparsification involves applying thresholding-based activation sparsification to specific layers within the attention modules. Finally, we detail the implementation of sparse kernels to accelerate LLM inference. Experimental results demonstrate that the proposed CHESS achieves lower performance degradation over 8 downstream tasks while activating fewer parameters compared to existing methods, thus speeding up the LLM inference by up to 1.27x.</li>
</ul>

<h3>Title: Debiasing Graph Representation Learning based on Information Bottleneck</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Zhang, Mingxuan Ouyang, Wanyu Lin, Hao Lan, Lei Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01367">https://arxiv.org/abs/2409.01367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01367">https://arxiv.org/pdf/2409.01367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01367]] Debiasing Graph Representation Learning based on Information Bottleneck(https://arxiv.org/abs/2409.01367)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Graph representation learning has shown superior performance in numerous real-world applications, such as finance and social networks. Nevertheless, most existing works might make discriminatory predictions due to insufficient attention to fairness in their decision-making processes. This oversight has prompted a growing focus on fair representation learning. Among recent explorations on fair representation learning, prior works based on adversarial learning usually induce unstable or counterproductive performance. To achieve fairness in a stable manner, we present the design and implementation of GRAFair, a new framework based on a variational graph auto-encoder. The crux of GRAFair is the Conditional Fairness Bottleneck, where the objective is to capture the trade-off between the utility of representations and sensitive information of interest. By applying variational approximation, we can make the optimization objective tractable. Particularly, GRAFair can be trained to produce informative representations of tasks while containing little sensitive information without adversarial training. Experiments on various real-world datasets demonstrate the effectiveness of our proposed method in terms of fairness, utility, robustness, and stability.</li>
</ul>

<h3>Title: Imitating Language via Scalable Inverse Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Markus Wulfmeier, Michael Bloesch, Nino Vieillard, Arun Ahuja, Jorg Bornschein, Sandy Huang, Artem Sokolov, Matt Barnes, Guillaume Desjardins, Alex Bewley, Sarah Maria Elisabeth Bechtle, Jost Tobias Springenberg, Nikola Momchev, Olivier Bachem, Matthieu Geist, Martin Riedmiller</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01369">https://arxiv.org/abs/2409.01369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01369">https://arxiv.org/pdf/2409.01369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01369]] Imitating Language via Scalable Inverse Reinforcement Learning(https://arxiv.org/abs/2409.01369)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The majority of language model training builds on imitation learning. It covers pretraining, supervised fine-tuning, and affects the starting conditions for reinforcement learning from human feedback (RLHF). The simplicity and scalability of maximum likelihood estimation (MLE) for next token prediction led to its role as predominant paradigm. However, the broader field of imitation learning can more effectively utilize the sequential structure underlying autoregressive generation. We focus on investigating the inverse reinforcement learning (IRL) perspective to imitation, extracting rewards and directly optimizing sequences instead of individual token likelihoods and evaluate its benefits for fine-tuning large language models. We provide a new angle, reformulating inverse soft-Q-learning as a temporal difference regularized extension of MLE. This creates a principled connection between MLE and IRL and allows trading off added complexity with increased performance and diversity of generations in the supervised fine-tuning (SFT) setting. We find clear advantages for IRL-based imitation, in particular for retaining diversity while maximizing task performance, rendering IRL a strong alternative on fixed SFT datasets even without online data generation. Our analysis of IRL-extracted reward functions further indicates benefits for more robust reward functions via tighter integration of supervised and preference-based LLM post-training.</li>
</ul>

<h3>Title: Content, Nudges and Incentives: A Study on the Effectiveness and Perception of Embedded Phishing Training</h3>
<ul>
<li><strong>Authors: </strong>Daniele Lain, Tarek Jost, Sinisa Matetic, Kari Kostiainen, Srdjan Capkun</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01378">https://arxiv.org/abs/2409.01378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01378">https://arxiv.org/pdf/2409.01378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01378]] Content, Nudges and Incentives: A Study on the Effectiveness and Perception of Embedded Phishing Training(https://arxiv.org/abs/2409.01378)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack</a></li>
<li><strong>Abstract: </strong>A common form of phishing training in organizations is the use of simulated phishing emails to test employees' susceptibility to phishing attacks, and the immediate delivery of training material to those who fail the test. This widespread practice is dubbed embedded training; however, its effectiveness in decreasing the likelihood of employees falling for phishing again in the future is questioned by the contradictory findings of several recent field studies. We investigate embedded phishing training in three aspects. First, we observe that the practice incorporates different components -- knowledge gains from its content, nudges and reminders from the test itself, and the deterrent effect of potential consequences -- our goal is to study which ones are more effective, if any. Second, we explore two potential improvements to training, namely its timing and the use of incentives. Third, we analyze employees' reception and perception of the practice. For this, we conducted a large-scale mixed-methods (quantitative and qualitative) study on the employees of a partner company. Our study contributes several novel findings on the training practice: in particular, its effectiveness comes from its nudging effect, i.e., the periodic reminder of the threat rather than from its content, which is rarely consumed by employees due to lack of time and perceived usefulness. Further, delaying training to ease time pressure is as effective as currently established practices, while rewards do not improve secure behavior. Finally, some of our results support previous findings with increased ecological validity, e.g., that phishing is an attention problem, rather than a knowledge one, even for the most susceptible employees, and thus enforcing training does not help.</li>
</ul>

<h3>Title: Membership Inference Attacks Against In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Rui Wen, Zheng Li, Michael Backes, Yang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01380">https://arxiv.org/abs/2409.01380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01380">https://arxiv.org/pdf/2409.01380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01380]] Membership Inference Attacks Against In-Context Learning(https://arxiv.org/abs/2409.01380)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>Adapting Large Language Models (LLMs) to specific tasks introduces concerns about computational efficiency, prompting an exploration of efficient methods such as In-Context Learning (ICL). However, the vulnerability of ICL to privacy attacks under realistic assumptions remains largely unexplored. In this work, we present the first membership inference attack tailored for ICL, relying solely on generated texts without their associated probabilities. We propose four attack strategies tailored to various constrained scenarios and conduct extensive experiments on four popular large language models. Empirical results show that our attacks can accurately determine membership status in most cases, e.g., 95\% accuracy advantage against LLaMA, indicating that the associated risks are much higher than those shown by existing probability-based attacks. Additionally, we propose a hybrid attack that synthesizes the strengths of the aforementioned strategies, achieving an accuracy advantage of over 95\% in most cases. Furthermore, we investigate three potential defenses targeting data, instruction, and output. Results demonstrate combining defenses from orthogonal dimensions significantly reduces privacy leakage and offers enhanced privacy assurances.</li>
</ul>

<h3>Title: Dataset Distillation from First Principles: Integrating Core Information Extraction and Purposeful Learning</h3>
<ul>
<li><strong>Authors: </strong>Vyacheslav Kungurtsev, Yuanfang Peng, Jianyang Gu, Saeed Vahidian, Anthony Quinn, Fadwa Idlahcen, Yiran Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.CO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01410">https://arxiv.org/abs/2409.01410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01410">https://arxiv.org/pdf/2409.01410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01410]] Dataset Distillation from First Principles: Integrating Core Information Extraction and Purposeful Learning(https://arxiv.org/abs/2409.01410)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Dataset distillation (DD) is an increasingly important technique that focuses on constructing a synthetic dataset capable of capturing the core information in training data to achieve comparable performance in models trained on the latter. While DD has a wide range of applications, the theory supporting it is less well evolved. New methods of DD are compared on a common set of benchmarks, rather than oriented towards any particular learning task. In this work, we present a formal model of DD, arguing that a precise characterization of the underlying optimization problem must specify the inference task associated with the application of interest. Without this task-specific focus, the DD problem is under-specified, and the selection of a DD algorithm for a particular task is merely heuristic. Our formalization reveals novel applications of DD across different modeling environments. We analyze existing DD methods through this broader lens, highlighting their strengths and limitations in terms of accuracy and faithfulness to optimal DD operation. Finally, we present numerical results for two case studies important in contemporary settings. Firstly, we address a critical challenge in medical data analysis: merging the knowledge from different datasets composed of intersecting, but not identical, sets of features, in order to construct a larger dataset in what is usually a small sample setting. Secondly, we consider out-of-distribution error across boundary conditions for physics-informed neural networks (PINNs), showing the potential for DD to provide more physically faithful data. By establishing this general formulation of DD, we aim to establish a new research paradigm by which DD can be understood and from which new DD techniques can arise.</li>
</ul>

<h3>Title: Enhancing Sample Efficiency and Exploration in Reinforcement Learning through the Integration of Diffusion Models and Proximal Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Gao Tianci, Dmitriev D. Dmitry, Konstantin A. Neusypin, Yang Bo, Rao Shengren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01427">https://arxiv.org/abs/2409.01427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01427">https://arxiv.org/pdf/2409.01427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01427]] Enhancing Sample Efficiency and Exploration in Reinforcement Learning through the Integration of Diffusion Models and Proximal Policy Optimization(https://arxiv.org/abs/2409.01427)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in reinforcement learning (RL) have been fueled by large-scale data and deep neural networks, particularly for high-dimensional and complex tasks. Online RL methods like Proximal Policy Optimization (PPO) are effective in dynamic scenarios but require substantial real-time data, posing challenges in resource-constrained or slow simulation environments. Offline RL addresses this by pre-learning policies from large datasets, though its success depends on the quality and diversity of the data. This work proposes a framework that enhances PPO algorithms by incorporating a diffusion model to generate high-quality virtual trajectories for offline datasets. This approach improves exploration and sample efficiency, leading to significant gains in cumulative rewards, convergence speed, and strategy stability in complex tasks. Our contributions are threefold: we explore the potential of diffusion models in RL, particularly for offline datasets, extend the application of online RL to offline environments, and experimentally validate the performance improvements of PPO with diffusion models. These findings provide new insights and methods for applying RL to high-dimensional, complex tasks. Finally, we open-source our code at this https URL</li>
</ul>

<h3>Title: Self-Directed Learning of Convex Labelings on Graphs</h3>
<ul>
<li><strong>Authors: </strong>Georgy Sokolov, Maximilian Thiessen, Margarita Akhmejanova, Fabio Vitale, Francesco Orabona</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01428">https://arxiv.org/abs/2409.01428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01428">https://arxiv.org/pdf/2409.01428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01428]] Self-Directed Learning of Convex Labelings on Graphs(https://arxiv.org/abs/2409.01428)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We study the problem of learning the clusters of a given graph in the self-directed learning setup. This learning setting is a variant of online learning, where rather than an adversary determining the sequence in which nodes are presented, the learner autonomously and adaptively selects them. While self-directed learning of Euclidean halfspaces, linear functions, and general abstract multi-class hypothesis classes was recently considered, no results previously existed specifically for self-directed node classification on graphs. In this paper, we address this problem developing efficient algorithms for it. More specifically, we focus on the case of (geodesically) convex clusters, i.e., for every two nodes sharing the same label, all nodes on every shortest path between them also share the same label. In particular, we devise a polynomial-time algorithm that makes only $3(h(G)+1)^4 \ln n$ mistakes on graphs with two convex clusters, where $n$ is the total number of nodes and $h(G)$ is the Hadwiger number, i.e., the size of the largest clique minor of the graph $G$. We also show that our algorithm is robust to the case that clusters are slightly non-convex, still achieving a mistake bound logarithmic in $n$. Finally, for the more standard case of homophilic clusters, where strongly connected nodes tend to belong the same class, we devise a simple and efficient algorithm.</li>
</ul>

<h3>Title: Achieving Byzantine-Resilient Federated Learning via Layer-Adaptive Sparsified Model Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Xu, Zikai Zhang, Rui Hu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01435">https://arxiv.org/abs/2409.01435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01435">https://arxiv.org/pdf/2409.01435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01435]] Achieving Byzantine-Resilient Federated Learning via Layer-Adaptive Sparsified Model Aggregation(https://arxiv.org/abs/2409.01435)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables multiple clients to collaboratively train a model without sharing their local data. Yet the FL system is vulnerable to well-designed Byzantine attacks, which aim to disrupt the model training process by uploading malicious model updates. Existing robust aggregation rule-based defense methods overlook the diversity of magnitude and direction across different layers of the model updates, resulting in limited robustness performance, particularly in non-IID settings. To address these challenges, we propose the Layer-Adaptive Sparsified Model Aggregation (LASA) approach, which combines pre-aggregation sparsification with layer-wise adaptive aggregation to improve robustness. Specifically, LASA includes a pre-aggregation sparsification module that sparsifies updates from each client before aggregation, reducing the impact of malicious parameters and minimizing the interference from less important parameters for the subsequent filtering process. Based on sparsified updates, a layer-wise adaptive filter then adaptively selects benign layers using both magnitude and direction metrics across all clients for aggregation. We provide the detailed theoretical robustness analysis of LASA and the resilience analysis for the FL integrated with LASA. Extensive experiments are conducted on various IID and non-IID datasets. The numerical results demonstrate the effectiveness of LASA. Code is available at \url{this https URL}.</li>
</ul>

<h3>Title: FinePseudo: Improving Pseudo-Labelling through Temporal-Alignablity for Semi-Supervised Fine-Grained Action Recognition</h3>
<ul>
<li><strong>Authors: </strong>Ishan Rajendrakumar Dave, Mamshad Nayeem Rizve, Mubarak Shah</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01448">https://arxiv.org/abs/2409.01448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01448">https://arxiv.org/pdf/2409.01448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01448]] FinePseudo: Improving Pseudo-Labelling through Temporal-Alignablity for Semi-Supervised Fine-Grained Action Recognition(https://arxiv.org/abs/2409.01448)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Real-life applications of action recognition often require a fine-grained understanding of subtle movements, e.g., in sports analytics, user interactions in AR/VR, and surgical videos. Although fine-grained actions are more costly to annotate, existing semi-supervised action recognition has mainly focused on coarse-grained action recognition. Since fine-grained actions are more challenging due to the absence of scene bias, classifying these actions requires an understanding of action-phases. Hence, existing coarse-grained semi-supervised methods do not work effectively. In this work, we for the first time thoroughly investigate semi-supervised fine-grained action recognition (FGAR). We observe that alignment distances like dynamic time warping (DTW) provide a suitable action-phase-aware measure for comparing fine-grained actions, a concept previously unexploited in FGAR. However, since regular DTW distance is pairwise and assumes strict alignment between pairs, it is not directly suitable for classifying fine-grained actions. To utilize such alignment distances in a limited-label setting, we propose an Alignability-Verification-based Metric learning technique to effectively discriminate between fine-grained action pairs. Our learnable alignability score provides a better phase-aware measure, which we use to refine the pseudo-labels of the primary video encoder. Our collaborative pseudo-labeling-based framework `\textit{FinePseudo}' significantly outperforms prior methods on four fine-grained action recognition datasets: Diving48, FineGym99, FineGym288, and FineDiving, and shows improvement on existing coarse-grained datasets: Kinetics400 and Something-SomethingV2. We also demonstrate the robustness of our collaborative pseudo-labeling in handling novel unlabeled classes in open-world semi-supervised setups. Project Page: this https URL.</li>
</ul>

<h3>Title: 3D-LSPTM: An Automatic Framework with 3D-Large-Scale Pretrained Model for Laryngeal Cancer Detection Using Laryngoscopic Videos</h3>
<ul>
<li><strong>Authors: </strong>Meiyu Qiu, Yun Li, Wenjun Huang, Haoyun Zhang, Weiping Zheng, Wenbin Lei, Xiaomao Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01459">https://arxiv.org/abs/2409.01459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01459">https://arxiv.org/pdf/2409.01459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01459]] 3D-LSPTM: An Automatic Framework with 3D-Large-Scale Pretrained Model for Laryngeal Cancer Detection Using Laryngoscopic Videos(https://arxiv.org/abs/2409.01459)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Laryngeal cancer is a malignant disease with a high morality rate in otorhinolaryngology, posing an significant threat to human health. Traditionally larygologists manually visual-inspect laryngeal cancer in laryngoscopic videos, which is quite time-consuming and subjective. In this study, we propose a novel automatic framework via 3D-large-scale pretrained models termed 3D-LSPTM for laryngeal cancer detection. Firstly, we collect 1,109 laryngoscopic videos from the First Affiliated Hospital Sun Yat-sen University with the approval of the Ethics Committee. Then we utilize the 3D-large-scale pretrained models of C3D, TimeSformer, and Video-Swin-Transformer, with the merit of advanced featuring videos, for laryngeal cancer detection with fine-tuning techniques. Extensive experiments show that our proposed 3D-LSPTM can achieve promising performance on the task of laryngeal cancer detection. Particularly, 3D-LSPTM with the backbone of Video-Swin-Transformer can achieve 92.4% accuracy, 95.6% sensitivity, 94.1% precision, and 94.8% F_1.</li>
</ul>

<h3>Title: PoliPrompt: A High-Performance Cost-Effective LLM-Based Text Classification Framework for Political Science</h3>
<ul>
<li><strong>Authors: </strong>Menglin Liu, Ge Shi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01466">https://arxiv.org/abs/2409.01466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01466">https://arxiv.org/pdf/2409.01466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01466]] PoliPrompt: A High-Performance Cost-Effective LLM-Based Text Classification Framework for Political Science(https://arxiv.org/abs/2409.01466)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have opened new avenues for enhancing text classification efficiency in political science, surpassing traditional machine learning methods that often require extensive feature engineering, human labeling, and task-specific training. However, their effectiveness in achieving high classification accuracy remains questionable. This paper introduces a three-stage in-context learning approach that leverages LLMs to improve classification accuracy while minimizing experimental costs. Our method incorporates automatic enhanced prompt generation, adaptive exemplar selection, and a consensus mechanism that resolves discrepancies between two weaker LLMs, refined by an advanced LLM. We validate our approach using datasets from the BBC news reports, Kavanaugh Supreme Court confirmation, and 2018 election campaign ads. The results show significant improvements in classification F1 score (+0.36 for zero-shot classification) with manageable economic costs (-78% compared with human labeling), demonstrating that our method effectively addresses the limitations of traditional machine learning while offering a scalable and reliable solution for text analysis in political science.</li>
</ul>

<h3>Title: Phantom: Untargeted Poisoning Attacks on Semi-Supervised Learning (Full Version)</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Knauer, Phillip Rieger, Hossein Fereidooni, Ahmad-Reza Sadeghi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01470">https://arxiv.org/abs/2409.01470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01470">https://arxiv.org/pdf/2409.01470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01470]] Phantom: Untargeted Poisoning Attacks on Semi-Supervised Learning (Full Version)(https://arxiv.org/abs/2409.01470)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Deep Neural Networks (DNNs) can handle increasingly complex tasks, albeit they require rapidly expanding training datasets. Collecting data from platforms with user-generated content, such as social networks, has significantly eased the acquisition of large datasets for training DNNs. Despite these advancements, the manual labeling process remains a substantial challenge in terms of both time and cost. In response, Semi-Supervised Learning (SSL) approaches have emerged, where only a small fraction of the dataset needs to be labeled, leaving the majority unlabeled. However, leveraging data from untrusted sources like social networks also creates new security risks, as potential attackers can easily inject manipulated samples. Previous research on the security of SSL primarily focused on injecting backdoors into trained models, while less attention was given to the more challenging untargeted poisoning attacks. In this paper, we introduce Phantom, the first untargeted poisoning attack in SSL that disrupts the training process by injecting a small number of manipulated images into the unlabeled dataset. Unlike existing attacks, our approach only requires adding few manipulated samples, such as posting images on social networks, without the need to control the victim. Phantom causes SSL algorithms to overlook the actual images' pixels and to rely only on maliciously crafted patterns that \ourname superimposed on the real images. We show Phantom's effectiveness for 6 different datasets and 3 real-world social-media platforms (Facebook, Instagram, Pinterest). Already small fractions of manipulated samples (e.g., 5\%) reduce the accuracy of the resulting model by 10\%, with higher percentages leading to a performance comparable to a naive classifier. Our findings demonstrate the threat of poisoning user-generated content platforms, rendering them unsuitable for SSL in specific tasks.</li>
</ul>

<h3>Title: Semantic Segmentation from Image Labels by Reconstruction from Structured Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Xuanrui Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01472">https://arxiv.org/abs/2409.01472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01472">https://arxiv.org/pdf/2409.01472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01472]] Semantic Segmentation from Image Labels by Reconstruction from Structured Decomposition(https://arxiv.org/abs/2409.01472)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Weakly supervised image segmentation (WSSS) from image tags remains challenging due to its under-constraint nature. Most mainstream work focus on the extraction of class activation map (CAM) and imposing various additional regularization. Contrary to the mainstream, we propose to frame WSSS as a problem of reconstruction from decomposition of the image using its mask, under which most regularization are embedded implicitly within the framework of the new problem. Our approach has demonstrated promising results on initial experiments, and shown robustness against the problem of background ambiguity. Our code is available at \url{this https URL}.</li>
</ul>

<h3>Title: Masked Mixers for Language Generation and Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Benjamin L. Badger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01482">https://arxiv.org/abs/2409.01482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01482">https://arxiv.org/pdf/2409.01482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01482]] Masked Mixers for Language Generation and Retrieval(https://arxiv.org/abs/2409.01482)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Attention mechanisms that confer selective focus on a strict subset of input elements are nearly ubiquitous in language models today. We posit there to be downside to the use of attention: most information present in the input is necessarily lost. In support of this idea we observe poor input representation accuracy in transformers, but find more accurate representation in what we term masked mixers which replace self-attention with masked convolutions. Applied to TinyStories the masked mixer learns causal language tasks more efficiently than early transformer implementations and somewhat less efficiently than optimized, current implementations. The most efficient learning algorithm observed for this dataset is a transformer-masked mixer hybrid, suggesting that these models learn in an orthogonal manner. We hypothesized that the information loss exhibited by transformers would be much more detrimental to retrieval than generation, and to test this we introduce an efficient training approach for retrieval models based on existing generative model embeddings. With this method, embeddings from masked mixers are found to result in far better summary-to-story retrieval compared to embeddings from transformers.</li>
</ul>

<h3>Title: Revisiting SMoE Language Models by Evaluating Inefficiencies with Task Specific Expert Pruning</h3>
<ul>
<li><strong>Authors: </strong>Soumajyoti Sarkar, Leonard Lausen, Volkan Cevher, Sheng Zha, Thomas Brox, George Karypis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01483">https://arxiv.org/abs/2409.01483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01483">https://arxiv.org/pdf/2409.01483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01483]] Revisiting SMoE Language Models by Evaluating Inefficiencies with Task Specific Expert Pruning(https://arxiv.org/abs/2409.01483)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Sparse Mixture of Expert (SMoE) models have emerged as a scalable alternative to dense models in language modeling. These models use conditionally activated feedforward subnetworks in transformer blocks, allowing for a separation between total model parameters and per-example computation. However, large token-routed SMoE models face a significant challenge: during inference, the entire model must be used for a sequence or a batch, resulting in high latencies in a distributed setting that offsets the advantages of per-token sparse activation. Our research explores task-specific model pruning to inform decisions about designing SMoE architectures, mainly modulating the choice of expert counts in pretraining. We investigate whether such pruned models offer advantages over smaller SMoE models trained from scratch, when evaluating and comparing them individually on tasks. To that end, we introduce an adaptive task-aware pruning technique UNCURL to reduce the number of experts per MoE layer in an offline manner post-training. Our findings reveal a threshold pruning factor for the reduction that depends on the number of experts used in pretraining, above which, the reduction starts to degrade model performance. These insights contribute to our understanding of model design choices when pretraining with SMoE architectures, particularly useful when considering task-specific inference optimization for later stages.</li>
</ul>

<h3>Title: Watermarking of Quantum Circuits</h3>
<ul>
<li><strong>Authors: </strong>Rupshali Roy, Swaroop Ghosh</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01484">https://arxiv.org/abs/2409.01484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01484">https://arxiv.org/pdf/2409.01484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01484]] Watermarking of Quantum Circuits(https://arxiv.org/abs/2409.01484)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, watermark</a></li>
<li><strong>Abstract: </strong>Quantum circuits constitute Intellectual Property (IP) of the quantum developers and users, which needs to be protected from theft by adversarial agents, e.g., the quantum cloud provider or a rogue adversary present in the cloud. This necessitates the exploration of low-overhead techniques applicable to near-term quantum devices, to trace the quantum circuits/algorithms\textquotesingle{} IP and their output. We present two such lightweight watermarking techniques to prove ownership in the event of an adversary cloning the circuit design. For the first technique, a rotation gate is placed on ancilla qubits combined with other gate(s) at the output of the circuit. For the second method, a set of random gates are inserted in the middle of the circuit followed by its inverse, separated from the circuit by a barrier. These models are combined and applied on benchmark circuits, and the circuit depth, 2-qubit gate count, probability of successful trials (PST), and probabilistic proof of authorship (PPA) are compared against the state-of-the-art. The PST is reduced by a minuscule 0.53\% against the non-watermarked benchmarks and is up to 22.69\% higher compared to existing techniques. The circuit depth has been reduced by up to 27.7\% as against the state-of-the-art. The PPA is astronomically smaller than existing watermarks.</li>
</ul>

<h3>Title: EarthGen: Generating the World from Top-Down Views</h3>
<ul>
<li><strong>Authors: </strong>Ansh Sharma, Albert Xiao, Praneet Rathi, Rohit Kundu, Albert Zhai, Yuan Shen, Shenlong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01491">https://arxiv.org/abs/2409.01491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01491">https://arxiv.org/pdf/2409.01491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01491]] EarthGen: Generating the World from Top-Down Views(https://arxiv.org/abs/2409.01491)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In this work, we present a novel method for extensive multi-scale generative terrain modeling. At the core of our model is a cascade of superresolution diffusion models that can be combined to produce consistent images across multiple resolutions. Pairing this concept with a tiled generation method yields a scalable system that can generate thousands of square kilometers of realistic Earth surfaces at high resolution. We evaluate our method on a dataset collected from Bing Maps and show that it outperforms super-resolution baselines on the extreme super-resolution task of 1024x zoom. We also demonstrate its ability to create diverse and coherent scenes via an interactive gigapixel-scale generated map. Finally, we demonstrate how our system can be extended to enable novel content creation applications including controllable world generation and 3D scene generation.</li>
</ul>

<h3>Title: The Compressor-Retriever Architecture for Language Model OS</h3>
<ul>
<li><strong>Authors: </strong>Yuan Yang, Siheng Xiong, Ehsan Shareghi, Faramarz Fekri</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01495">https://arxiv.org/abs/2409.01495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01495">https://arxiv.org/pdf/2409.01495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01495]] The Compressor-Retriever Architecture for Language Model OS(https://arxiv.org/abs/2409.01495)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have significantly enhanced their capacity to aggregate and process information across multiple modalities, enabling them to perform a wide range of tasks such as multimodal data querying, tool usage, web interactions, and handling long documents. These capabilities pave the way for transforming LLMs from mere chatbots into general-purpose agents capable of interacting with the real world. This paper explores the concept of using a language model as the core component of an operating system (OS), effectively acting as a CPU that processes data stored in a context window, which functions as RAM. A key challenge in realizing such an LM OS is managing the life-long context and ensuring statefulness across sessions, a feature limited by the current session-based interaction paradigm due to context window size limit. To address this, we introduce compressor-retriever, a model-agnostic architecture designed for life-long context management. Unlike other long-context solutions such as retrieval-augmented generation, our approach exclusively uses the base model's forward function to compress and retrieve context, ensuring end-to-end differentiability. Preliminary experiments demonstrate the effectiveness of this architecture in in-context learning tasks, marking a step towards the development of a fully stateful LLM OS. Project repo available at: this https URL</li>
</ul>

<h3>Title: DiversityMedQA: Assessing Demographic Biases in Medical Diagnosis using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Rajat Rawat, Hudson McBride, Dhiyaan Nirmal, Rajarshi Ghosh, Jong Moon, Dhruv Alamuri, Sean O'Brien, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01497">https://arxiv.org/abs/2409.01497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01497">https://arxiv.org/pdf/2409.01497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01497]] DiversityMedQA: Assessing Demographic Biases in Medical Diagnosis using Large Language Models(https://arxiv.org/abs/2409.01497)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) gain traction in healthcare, concerns about their susceptibility to demographic biases are growing. We introduce {DiversityMedQA}, a novel benchmark designed to assess LLM responses to medical queries across diverse patient demographics, such as gender and ethnicity. By perturbing questions from the MedQA dataset, which comprises medical board exam questions, we created a benchmark that captures the nuanced differences in medical diagnosis across varying patient profiles. Our findings reveal notable discrepancies in model performance when tested against these demographic variations. Furthermore, to ensure the perturbations were accurate, we also propose a filtering strategy that validates each perturbation. By releasing DiversityMedQA, we provide a resource for evaluating and mitigating demographic bias in LLM medical diagnoses.</li>
</ul>

<h3>Title: Real-Time Multi-Scene Visibility Enhancement for Promoting Navigational Safety of Vessels Under Complex Weather Conditions</h3>
<ul>
<li><strong>Authors: </strong>Ryan Wen Liu, Yuxu Lu, Yuan Gao, Yu Guo, Wenqi Ren, Fenghua Zhu, Fei-Yue Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01500">https://arxiv.org/abs/2409.01500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01500">https://arxiv.org/pdf/2409.01500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01500]] Real-Time Multi-Scene Visibility Enhancement for Promoting Navigational Safety of Vessels Under Complex Weather Conditions(https://arxiv.org/abs/2409.01500)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The visible-light camera, which is capable of environment perception and navigation assistance, has emerged as an essential imaging sensor for marine surface vessels in intelligent waterborne transportation systems (IWTS). However, the visual imaging quality inevitably suffers from several kinds of degradations (e.g., limited visibility, low contrast, color distortion, etc.) under complex weather conditions (e.g., haze, rain, and low-lightness). The degraded visual information will accordingly result in inaccurate environment perception and delayed operations for navigational risk. To promote the navigational safety of vessels, many computational methods have been presented to perform visual quality enhancement under poor weather conditions. However, most of these methods are essentially specific-purpose implementation strategies, only available for one specific weather type. To overcome this limitation, we propose to develop a general-purpose multi-scene visibility enhancement method, i.e., edge reparameterization- and attention-guided neural network (ERANet), to adaptively restore the degraded images captured under different weather conditions. In particular, our ERANet simultaneously exploits the channel attention, spatial attention, and reparameterization technology to enhance the visual quality while maintaining low computational cost. Extensive experiments conducted on standard and IWTS-related datasets have demonstrated that our ERANet could outperform several representative visibility enhancement methods in terms of both imaging quality and computational efficiency. The superior performance of IWTS-related object detection and scene segmentation could also be steadily obtained after ERANet-based visibility enhancement under complex weather conditions.</li>
</ul>

<h3>Title: AMG: Avatar Motion Guided Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhangsihao Yang, Mengyi Shan, Mohammad Farazi, Wenhui Zhu, Yanxi Chen, Xuanzhao Dong, Yalin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01502">https://arxiv.org/abs/2409.01502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01502">https://arxiv.org/pdf/2409.01502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01502]] AMG: Avatar Motion Guided Video Generation(https://arxiv.org/abs/2409.01502)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Human video generation task has gained significant attention with the advancement of deep generative models. Generating realistic videos with human movements is challenging in nature, due to the intricacies of human body topology and sensitivity to visual artifacts. The extensively studied 2D media generation methods take advantage of massive human media datasets, but struggle with 3D-aware control; whereas 3D avatar-based approaches, while offering more freedom in control, lack photorealism and cannot be harmonized seamlessly with background scene. We propose AMG, a method that combines the 2D photorealism and 3D controllability by conditioning video diffusion models on controlled rendering of 3D avatars. We additionally introduce a novel data processing pipeline that reconstructs and renders human avatar movements from dynamic camera videos. AMG is the first method that enables multi-person diffusion video generation with precise control over camera positions, human motions, and background style. We also demonstrate through extensive evaluation that it outperforms existing human video generation methods conditioned on pose sequences or driving videos in terms of realism and adaptability.</li>
</ul>

<h3>Title: From Data to Insights: A Covariate Analysis of the IARPA BRIAR Dataset for Multimodal Biometric Recognition Algorithms at Altitude and Range</h3>
<ul>
<li><strong>Authors: </strong>David S. Bolme, Deniz Aykac, Ryan Shivers, Joel Brogan, Nell Barber, Bob Zhang, Laura Davies, David Cornett III</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01514">https://arxiv.org/abs/2409.01514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01514">https://arxiv.org/pdf/2409.01514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01514]] From Data to Insights: A Covariate Analysis of the IARPA BRIAR Dataset for Multimodal Biometric Recognition Algorithms at Altitude and Range(https://arxiv.org/abs/2409.01514)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, biometric</a></li>
<li><strong>Abstract: </strong>This paper examines covariate effects on fused whole body biometrics performance in the IARPA BRIAR dataset, specifically focusing on UAV platforms, elevated positions, and distances up to 1000 meters. The dataset includes outdoor videos compared with indoor images and controlled gait recordings. Normalized raw fusion scores relate directly to predicted false accept rates (FAR), offering an intuitive means for interpreting model results. A linear model is developed to predict biometric algorithm scores, analyzing their performance to identify the most influential covariates on accuracy at altitude and range. Weather factors like temperature, wind speed, solar loading, and turbulence are also investigated in this analysis. The study found that resolution and camera distance best predicted accuracy and findings can guide future research and development efforts in long-range/elevated/UAV biometrics and support the creation of more reliable and robust systems for national security and other critical domains.</li>
</ul>

<h3>Title: S$^3$c-Math: Spontaneous Step-level Self-correction Makes Large Language Models Better Mathematical Reasoners</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Yan, Jin Jiang, Yang Liu, Yixin Cao, Xin Xu, Mengdi zhang, Xunliang Cai, Jian Shao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01524">https://arxiv.org/abs/2409.01524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01524">https://arxiv.org/pdf/2409.01524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01524]] S$^3$c-Math: Spontaneous Step-level Self-correction Makes Large Language Models Better Mathematical Reasoners(https://arxiv.org/abs/2409.01524)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Self-correction is a novel method that can stimulate the potential reasoning abilities of large language models (LLMs). It involves detecting and correcting errors during the inference process when LLMs solve reasoning problems. However, recent works do not regard self-correction as a spontaneous and intrinsic capability of LLMs. Instead, such correction is achieved through post-hoc generation, external knowledge introduction, multi-model collaboration, and similar techniques. In this paper, we propose a series of mathematical LLMs called S$^3$c-Math, which are able to perform Spontaneous Step-level Self-correction for Mathematical reasoning. This capability helps LLMs to recognize whether their ongoing inference tends to contain errors and simultaneously correct these errors to produce a more reliable response. We proposed a method, which employs a step-level sampling approach to construct step-wise self-correction data for achieving such ability. Additionally, we implement a training strategy that uses above constructed data to equip LLMs with spontaneous step-level self-correction capacities. Our data and methods have been demonstrated to be effective across various foundation LLMs, consistently showing significant progress in evaluations on GSM8K, MATH, and other mathematical benchmarks. To the best of our knowledge, we are the first to introduce the spontaneous step-level self-correction ability of LLMs in mathematical reasoning.</li>
</ul>

<h3>Title: On the Design Space Between Transformers and Recursive Neural Nets</h3>
<ul>
<li><strong>Authors: </strong>Jishnu Ray Chowdhury, Cornelia Caragea</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01531">https://arxiv.org/abs/2409.01531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01531">https://arxiv.org/pdf/2409.01531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01531]] On the Design Space Between Transformers and Recursive Neural Nets(https://arxiv.org/abs/2409.01531)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we study two classes of models, Recursive Neural Networks (RvNNs) and Transformers, and show that a tight connection between them emerges from the recent development of two recent models - Continuous Recursive Neural Networks (CRvNN) and Neural Data Routers (NDR). On one hand, CRvNN pushes the boundaries of traditional RvNN, relaxing its discrete structure-wise composition and ends up with a Transformer-like structure. On the other hand, NDR constrains the original Transformer to induce better structural inductive bias, ending up with a model that is close to CRvNN. Both models, CRvNN and NDR, show strong performance in algorithmic tasks and generalization in which simpler forms of RvNNs and Transformers fail. We explore these "bridge" models in the design space between RvNNs and Transformers, formalize their tight connections, discuss their limitations, and propose ideas for future research.</li>
</ul>

<h3>Title: Improving Robustness of Spectrogram Classifiers with Neural Stochastic Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Joel Brogan, Olivera Kotevska, Anibely Torres, Sumit Jha, Mark Adams</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01532">https://arxiv.org/abs/2409.01532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01532">https://arxiv.org/pdf/2409.01532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01532]] Improving Robustness of Spectrogram Classifiers with Neural Stochastic Differential Equations(https://arxiv.org/abs/2409.01532)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Signal analysis and classification is fraught with high levels of noise and perturbation. Computer-vision-based deep learning models applied to spectrograms have proven useful in the field of signal classification and detection; however, these methods aren't designed to handle the low signal-to-noise ratios inherent within non-vision signal processing tasks. While they are powerful, they are currently not the method of choice in the inherently noisy and dynamic critical infrastructure domain, such as smart-grid sensing, anomaly detection, and non-intrusive load monitoring.</li>
</ul>

<h3>Title: It is Time to Develop an Auditing Framework to Promote Value Aware Chatbots</h3>
<ul>
<li><strong>Authors: </strong>Yanchen Wang, Lisa Singh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01539">https://arxiv.org/abs/2409.01539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01539">https://arxiv.org/pdf/2409.01539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01539]] It is Time to Develop an Auditing Framework to Promote Value Aware Chatbots(https://arxiv.org/abs/2409.01539)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>The launch of ChatGPT in November 2022 marked the beginning of a new era in AI, the availability of generative AI tools for everyone to use. ChatGPT and other similar chatbots boast a wide range of capabilities from answering student homework questions to creating music and art. Given the large amounts of human data chatbots are built on, it is inevitable that they will inherit human errors and biases. These biases have the potential to inflict significant harm or increase inequity on different subpopulations. Because chatbots do not have an inherent understanding of societal values, they may create new content that is contrary to established norms. Examples of concerning generated content includes child pornography, inaccurate facts, and discriminatory posts. In this position paper, we argue that the speed of advancement of this technology requires us, as computer and data scientists, to mobilize and develop a values-based auditing framework containing a community established standard set of measurements to monitor the health of different chatbots and LLMs. To support our argument, we use a simple audit template to share the results of basic audits we conduct that are focused on measuring potential bias in search engine style tasks, code generation, and story generation. We identify responses from GPT 3.5 and GPT 4 that are both consistent and not consistent with values derived from existing law. While the findings come as no surprise, they do underscore the urgency of developing a robust auditing framework for openly sharing results in a consistent way so that mitigation strategies can be developed by the academic community, government agencies, and companies when our values are not being adhered to. We conclude this paper with recommendations for value-based strategies for improving the technologies.</li>
</ul>

<h3>Title: Long-Range Biometric Identification in Real World Scenarios: A Comprehensive Evaluation Framework Based on Missions</h3>
<ul>
<li><strong>Authors: </strong>Deniz Aykac, Joel Brogan, Nell Barber, Ryan Shivers, Bob Zhang, Dallas Sacca, Ryan Tipton, Gavin Jager, Austin Garret, Matthew Love, Jim Goddard, David Cornett III, David S. Bolme</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01540">https://arxiv.org/abs/2409.01540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01540">https://arxiv.org/pdf/2409.01540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01540]] Long-Range Biometric Identification in Real World Scenarios: A Comprehensive Evaluation Framework Based on Missions(https://arxiv.org/abs/2409.01540)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, robust, biometric</a></li>
<li><strong>Abstract: </strong>The considerable body of data available for evaluating biometric recognition systems in Research and Development (R\&D) environments has contributed to the increasingly common problem of target performance mismatch. Biometric algorithms are frequently tested against data that may not reflect the real world applications they target. From a Testing and Evaluation (T\&E) standpoint, this domain mismatch causes difficulty assessing when improvements in State-of-the-Art (SOTA) research actually translate to improved applied outcomes. This problem can be addressed with thoughtful preparation of data and experimental methods to reflect specific use-cases and scenarios. To that end, this paper evaluates research solutions for identifying individuals at ranges and altitudes, which could support various application areas such as counterterrorism, protection of critical infrastructure facilities, military force protection, and border security. We address challenges including image quality issues and reliance on face recognition as the sole biometric modality. By fusing face and body features, we propose developing robust biometric systems for effective long-range identification from both the ground and steep pitch angles. Preliminary results show promising progress in whole-body recognition. This paper presents these early findings and discusses potential future directions for advancing long-range biometric identification systems based on mission-driven metrics.</li>
</ul>

<h3>Title: Purification-Agnostic Proxy Learning for Agentic Copyright Watermarking against Adversarial Evidence Forgery</h3>
<ul>
<li><strong>Authors: </strong>Erjin Bao, Ching-Chun Chang, Hanrui Wang, Isao Echizen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01541">https://arxiv.org/abs/2409.01541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01541">https://arxiv.org/pdf/2409.01541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01541]] Purification-Agnostic Proxy Learning for Agentic Copyright Watermarking against Adversarial Evidence Forgery(https://arxiv.org/abs/2409.01541)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, watermark</a></li>
<li><strong>Abstract: </strong>With the proliferation of AI agents in various domains, protecting the ownership of AI models has become crucial due to the significant investment in their development. Unauthorized use and illegal distribution of these models pose serious threats to intellectual property, necessitating effective copyright protection measures. Model watermarking has emerged as a key technique to address this issue, embedding ownership information within models to assert rightful ownership during copyright disputes. This paper presents several contributions to model watermarking: a self-authenticating black-box watermarking protocol using hash techniques, a study on evidence forgery attacks using adversarial perturbations, a proposed defense involving a purification step to counter adversarial attacks, and a purification-agnostic proxy learning method to enhance watermark reliability and model performance. Experimental results demonstrate the effectiveness of these approaches in improving the security, reliability, and performance of watermarked models.</li>
</ul>

<h3>Title: Self-Instructed Derived Prompt Generation Meets In-Context Learning: Unlocking New Potential of Black-Box LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhuo Li, Yuhao Du, Jinpeng Hu, Xiang Wan, Anningzhe Gao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01552">https://arxiv.org/abs/2409.01552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01552">https://arxiv.org/pdf/2409.01552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01552]] Self-Instructed Derived Prompt Generation Meets In-Context Learning: Unlocking New Potential of Black-Box LLMs(https://arxiv.org/abs/2409.01552)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown success in generating high-quality responses. In order to achieve better alignment with LLMs with human preference, various works are proposed based on specific optimization process, which, however, is not suitable to Black-Box LLMs like GPT-4, due to inaccessible parameters. In Black-Box LLMs case, their performance is highly dependent on the quality of the provided prompts. Existing methods to enhance response quality often involve a prompt refinement model, yet these approaches potentially suffer from semantic inconsistencies between the refined and original prompts, and typically overlook the relationship between them. To address these challenges, we introduce a self-instructed in-context learning framework that empowers LLMs to deliver more effective responses by generating reliable derived prompts to construct informative contextual environments. Our approach incorporates a self-instructed reinforcement learning mechanism, enabling direct interaction with the response model during derived prompt generation for better alignment. We then formulate querying as an in-context learning task, using responses from LLMs combined with the derived prompts to establish a contextual demonstration for the original prompt. This strategy ensures alignment with the original query, reduces discrepancies from refined prompts, and maximizes the LLMs' in-context learning capability. Extensive experiments demonstrate that the proposed method not only generates more reliable derived prompts but also significantly enhances LLMs' ability to deliver more effective responses, including Black-Box models such as GPT-4.</li>
</ul>

<h3>Title: Benchmarking Cognitive Domains for LLMs: Insights from Taiwanese Hakka Culture</h3>
<ul>
<li><strong>Authors: </strong>Chen-Chi Chang, Ching-Yuan Chen, Hung-Shin Lee, Chih-Cheng Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01556">https://arxiv.org/abs/2409.01556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01556">https://arxiv.org/pdf/2409.01556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01556]] Benchmarking Cognitive Domains for LLMs: Insights from Taiwanese Hakka Culture(https://arxiv.org/abs/2409.01556)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This study introduces a comprehensive benchmark designed to evaluate the performance of large language models (LLMs) in understanding and processing cultural knowledge, with a specific focus on Hakka culture as a case study. Leveraging Bloom's Taxonomy, the study develops a multi-dimensional framework that systematically assesses LLMs across six cognitive domains: Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating. This benchmark extends beyond traditional single-dimensional evaluations by providing a deeper analysis of LLMs' abilities to handle culturally specific content, ranging from basic recall of facts to higher-order cognitive tasks such as creative synthesis. Additionally, the study integrates Retrieval-Augmented Generation (RAG) technology to address the challenges of minority cultural knowledge representation in LLMs, demonstrating how RAG enhances the models' performance by dynamically incorporating relevant external information. The results highlight the effectiveness of RAG in improving accuracy across all cognitive domains, particularly in tasks requiring precise retrieval and application of cultural knowledge. However, the findings also reveal the limitations of RAG in creative tasks, underscoring the need for further optimization. This benchmark provides a robust tool for evaluating and comparing LLMs in culturally diverse contexts, offering valuable insights for future research and development in AI-driven cultural knowledge preservation and dissemination.</li>
</ul>

<h3>Title: TASL-Net: Tri-Attention Selective Learning Network for Intelligent Diagnosis of Bimodal Ultrasound Video</h3>
<ul>
<li><strong>Authors: </strong>Chengqian Zhao, Zhao Yao, Zhaoyu Hu, Yuanxin Xie, Yafang Zhang, Yuanyuan Wang, Shuo Li, Jianhua Zhou, Jianqiao Zhou, Yin Wang, Jinhua Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01557">https://arxiv.org/abs/2409.01557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01557">https://arxiv.org/pdf/2409.01557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01557]] TASL-Net: Tri-Attention Selective Learning Network for Intelligent Diagnosis of Bimodal Ultrasound Video(https://arxiv.org/abs/2409.01557)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In the intelligent diagnosis of bimodal (gray-scale and contrast-enhanced) ultrasound videos, medical domain knowledge such as the way sonographers browse videos, the particular areas they emphasize, and the features they pay special attention to, plays a decisive role in facilitating precise diagnosis. Embedding medical knowledge into the deep learning network can not only enhance performance but also boost clinical confidence and reliability of the network. However, it is an intractable challenge to automatically focus on these person- and disease-specific features in videos and to enable networks to encode bimodal information comprehensively and efficiently. This paper proposes a novel Tri-Attention Selective Learning Network (TASL-Net) to tackle this challenge and automatically embed three types of diagnostic attention of sonographers into a mutual transformer framework for intelligent diagnosis of bimodal ultrasound videos. Firstly, a time-intensity-curve-based video selector is designed to mimic the temporal attention of sonographers, thus removing a large amount of redundant information while improving computational efficiency of TASL-Net. Then, to introduce the spatial attention of the sonographers for contrast-enhanced video analysis, we propose the earliest-enhanced position detector based on structural similarity variation, on which the TASL-Net is made to focus on the differences of perfusion variation inside and outside the lesion. Finally, by proposing a mutual encoding strategy that combines convolution and transformer, TASL-Net possesses bimodal attention to structure features on gray-scale videos and to perfusion variations on contrast-enhanced videos. These modules work collaboratively and contribute to superior performance. We conduct a detailed experimental validation of TASL-Net's performance on three datasets, including lung, breast, and liver.</li>
</ul>

<h3>Title: Blocks as Probes: Dissecting Categorization Ability of Large Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Bin Fu, Qiyang Wan, Jialin Li, Ruiping Wang, Xilin Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01560">https://arxiv.org/abs/2409.01560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01560">https://arxiv.org/pdf/2409.01560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01560]] Blocks as Probes: Dissecting Categorization Ability of Large Multimodal Models(https://arxiv.org/abs/2409.01560)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Categorization, a core cognitive ability in humans that organizes objects based on common features, is essential to cognitive science as well as computer vision. To evaluate the categorization ability of visual AI models, various proxy tasks on recognition from datasets to open world scenarios have been proposed. Recent development of Large Multimodal Models (LMMs) has demonstrated impressive results in high-level visual tasks, such as visual question answering, video temporal reasoning, etc., utilizing the advanced architectures and large-scale multimodal instruction tuning. Previous researchers have developed holistic benchmarks to measure the high-level visual capability of LMMs, but there is still a lack of pure and in-depth quantitative evaluation of the most fundamental categorization ability. According to the research on human cognitive process, categorization can be seen as including two parts: category learning and category use. Inspired by this, we propose a novel, challenging, and efficient benchmark based on composite blocks, called ComBo, which provides a disentangled evaluation framework and covers the entire categorization process from learning to use. By analyzing the results of multiple evaluation tasks, we find that although LMMs exhibit acceptable generalization ability in learning new categories, there are still gaps compared to humans in many ways, such as fine-grained perception of spatial relationship and abstract category understanding. Through the study of categorization, we can provide inspiration for the further development of LMMs in terms of interpretability and generalization.</li>
</ul>

<h3>Title: ReSpike: Residual Frames-based Hybrid Spiking Neural Networks for Efficient Action Recognition</h3>
<ul>
<li><strong>Authors: </strong>Shiting Xiao, Yuhang Li, Youngeun Kim, Donghyun Lee, Priyadarshini Panda</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01564">https://arxiv.org/abs/2409.01564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01564">https://arxiv.org/pdf/2409.01564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01564]] ReSpike: Residual Frames-based Hybrid Spiking Neural Networks for Efficient Action Recognition(https://arxiv.org/abs/2409.01564)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Spiking Neural Networks (SNNs) have emerged as a compelling, energy-efficient alternative to traditional Artificial Neural Networks (ANNs) for static image tasks such as image classification and segmentation. However, in the more complex video classification domain, SNN-based methods fall considerably short of ANN-based benchmarks due to the challenges in processing dense frame sequences. To bridge this gap, we propose ReSpike, a hybrid framework that synergizes the strengths of ANNs and SNNs to tackle action recognition tasks with high accuracy and low energy cost. By decomposing film clips into spatial and temporal components, i.e., RGB image Key Frames and event-like Residual Frames, ReSpike leverages ANN for learning spatial information and SNN for learning temporal information. In addition, we propose a multi-scale cross-attention mechanism for effective feature fusion. Compared to state-of-the-art SNN baselines, our ReSpike hybrid architecture demonstrates significant performance improvements (e.g., >30% absolute accuracy improvement on HMDB-51, UCF-101, and Kinetics-400). Furthermore, ReSpike achieves comparable performance with prior ANN approaches while bringing better accuracy-energy tradeoff.</li>
</ul>

<h3>Title: CT-SDM: A Sampling Diffusion Model for Sparse-View CT Reconstruction across All Sampling Rates</h3>
<ul>
<li><strong>Authors: </strong>Liutao Yang, Jiahao Huang, Guang Yang, Daoqiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01571">https://arxiv.org/abs/2409.01571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01571">https://arxiv.org/pdf/2409.01571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01571]] CT-SDM: A Sampling Diffusion Model for Sparse-View CT Reconstruction across All Sampling Rates(https://arxiv.org/abs/2409.01571)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Sparse views X-ray computed tomography has emerged as a contemporary technique to mitigate radiation dose. Because of the reduced number of projection views, traditional reconstruction methods can lead to severe artifacts. Recently, research studies utilizing deep learning methods has made promising progress in removing artifacts for Sparse-View Computed Tomography (SVCT). However, given the limitations on the generalization capability of deep learning models, current methods usually train models on fixed sampling rates, affecting the usability and flexibility of model deployment in real clinical settings. To address this issue, our study proposes a adaptive reconstruction method to achieve high-performance SVCT reconstruction at any sampling rate. Specifically, we design a novel imaging degradation operator in the proposed sampling diffusion model for SVCT (CT-SDM) to simulate the projection process in the sinogram domain. Thus, the CT-SDM can gradually add projection views to highly undersampled measurements to generalize the full-view sinograms. By choosing an appropriate starting point in diffusion inference, the proposed model can recover the full-view sinograms from any sampling rate with only one trained model. Experiments on several datasets have verified the effectiveness and robustness of our approach, demonstrating its superiority in reconstructing high-quality images from sparse-view CT scans across various sampling rates.</li>
</ul>

<h3>Title: LSSF-Net: Lightweight Segmentation with Self-Awareness, Spatial Attention, and Focal Modulation</h3>
<ul>
<li><strong>Authors: </strong>Hamza Farooq, Zuhair Zafar, Ahsan Saadat, Tariq M Khan, Shahzaib Iqbal, Imran Razzak</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01572">https://arxiv.org/abs/2409.01572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01572">https://arxiv.org/pdf/2409.01572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01572]] LSSF-Net: Lightweight Segmentation with Self-Awareness, Spatial Attention, and Focal Modulation(https://arxiv.org/abs/2409.01572)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of skin lesions within dermoscopic images plays a crucial role in the timely identification of skin cancer for computer-aided diagnosis on mobile platforms. However, varying shapes of the lesions, lack of defined edges, and the presence of obstructions such as hair strands and marker colors make this challenge more complex. \textcolor{red}Additionally, skin lesions often exhibit subtle variations in texture and color that are difficult to differentiate from surrounding healthy skin, necessitating models that can capture both fine-grained details and broader contextual information. Currently, melanoma segmentation models are commonly based on fully connected networks and U-Nets. However, these models often struggle with capturing the complex and varied characteristics of skin lesions, such as the presence of indistinct boundaries and diverse lesion appearances, which can lead to suboptimal segmentation this http URL address these challenges, we propose a novel lightweight network specifically designed for skin lesion segmentation utilizing mobile devices, featuring a minimal number of learnable parameters (only 0.8 million). This network comprises an encoder-decoder architecture that incorporates conformer-based focal modulation attention, self-aware local and global spatial attention, and split channel-shuffle. The efficacy of our model has been evaluated on four well-established benchmark datasets for skin lesion segmentation: ISIC 2016, ISIC 2017, ISIC 2018, and PH2. Empirical findings substantiate its state-of-the-art performance, notably reflected in a high Jaccard index.</li>
</ul>

<h3>Title: Improving Apple Object Detection with Occlusion-Enhanced Distillation</h3>
<ul>
<li><strong>Authors: </strong>Liang Geng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01573">https://arxiv.org/abs/2409.01573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01573">https://arxiv.org/pdf/2409.01573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01573]] Improving Apple Object Detection with Occlusion-Enhanced Distillation(https://arxiv.org/abs/2409.01573)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Apples growing in natural environments often face severe visual obstructions from leaves and branches. This significantly increases the risk of false detections in object detection tasks, thereby escalating the challenge. Addressing this issue, we introduce a technique called "Occlusion-Enhanced Distillation" (OED). This approach utilizes occlusion information to regularize the learning of semantically aligned features on occluded datasets and employs Exponential Moving Average (EMA) to enhance training stability. Specifically, we first design an occlusion-enhanced dataset that integrates Grounding DINO and SAM methods to extract occluding elements such as leaves and branches from each sample, creating occlusion examples that reflect the natural growth state of fruits. Additionally, we propose a multi-scale knowledge distillation strategy, where the student network uses images with increased occlusions as inputs, while the teacher network employs images without natural occlusions. Through this setup, the strategy guides the student network to learn from the teacher across scales of semantic and local features alignment, effectively narrowing the feature distance between occluded and non-occluded targets and enhancing the robustness of object detection. Lastly, to improve the stability of the student network, we introduce the EMA strategy, which aids the student network in learning more generalized feature expressions that are less affected by the noise of individual image occlusions. Our method significantly outperforms current state-of-the-art techniques through extensive comparative experiments.</li>
</ul>

<h3>Title: An Implementation of Werewolf Agent That does not Truly Trust LLMs</h3>
<ul>
<li><strong>Authors: </strong>Takehiro Sato, Shintaro Ozaki, Daisaku Yokoyama</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01575">https://arxiv.org/abs/2409.01575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01575">https://arxiv.org/pdf/2409.01575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01575]] An Implementation of Werewolf Agent That does not Truly Trust LLMs(https://arxiv.org/abs/2409.01575)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Werewolf is an incomplete information game, which has several challenges when creating a computer agent as a player given the lack of understanding of the situation and individuality of utterance (e.g., computer agents are not capable of characterful utterance or situational lying). We propose a werewolf agent that solves some of those difficulties by combining a Large Language Model (LLM) and a rule-based algorithm. In particular, our agent uses a rule-based algorithm to select an output either from an LLM or a template prepared beforehand based on the results of analyzing conversation history using an LLM. It allows the agent to refute in specific situations, identify when to end the conversation, and behave with persona. This approach mitigated conversational inconsistencies and facilitated logical utterance as a result. We also conducted a qualitative evaluation, which resulted in our agent being perceived as more human-like compared to an unmodified LLM. The agent is freely available for contributing to advance the research in the field of Werewolf game.</li>
</ul>

<h3>Title: AdaComp: Extractive Context Compression with Adaptive Predictor for Retrieval-Augmented Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qianchi Zhang, Hainan Zhang, Liang Pang, Hongwei Zheng, Zhiming Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01579">https://arxiv.org/abs/2409.01579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01579">https://arxiv.org/pdf/2409.01579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01579]] AdaComp: Extractive Context Compression with Adaptive Predictor for Retrieval-Augmented Large Language Models(https://arxiv.org/abs/2409.01579)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Retrieved documents containing noise will hinder RAG from detecting answer clues and make the inference process slow and expensive. Therefore, context compression is necessary to enhance its accuracy and efficiency. Existing context compression methods use extractive or generative models to retain the most query-relevant sentences or apply the information bottleneck theory to preserve sufficient information. However, these methods may face issues such as over-compression or high computational costs. We observe that the retriever often ranks relevant documents at the top, but the exact number of documents needed to answer the query is uncertain due to the impact of query complexity and retrieval quality: complex queries like multi-hop questions may require retaining more documents than simpler queries, and a low-quality retrieval may need to rely on more documents to generate accurate outputs. Therefore, determining the minimum number of required documents (compression rate) is still a challenge for RAG. In this paper, we introduce AdaComp, a low-cost extractive context compression method that adaptively determines the compression rate based on both query complexity and retrieval quality. Specifically, we first annotate the minimum top-k documents necessary for the RAG system to answer the current query as the compression rate and then construct triplets of the query, retrieved documents, and its compression rate. Then, we use this triplet dataset to train a compression-rate predictor. Experiments on three QA datasets and one conversational Muiti-doc QA dataset show that AdaComp significantly reduces inference costs while maintaining performance nearly identical to uncompressed models, achieving a balance between efficiency and performance.</li>
</ul>

<h3>Title: Buffer-based Gradient Projection for Continual Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Shenghong Dai, Jy-yong Sohn, Yicong Chen, S M Iftekharul Alam, Ravikumar Balakrishnan, Suman Banerjee, Nageen Himayat, Kangwook Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01585">https://arxiv.org/abs/2409.01585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01585">https://arxiv.org/pdf/2409.01585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01585]] Buffer-based Gradient Projection for Continual Federated Learning(https://arxiv.org/abs/2409.01585)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Continual Federated Learning (CFL) is essential for enabling real-world applications where multiple decentralized clients adaptively learn from continuous data streams. A significant challenge in CFL is mitigating catastrophic forgetting, where models lose previously acquired knowledge when learning new information. Existing approaches often face difficulties due to the constraints of device storage capacities and the heterogeneous nature of data distributions among clients. While some CFL algorithms have addressed these challenges, they frequently rely on unrealistic assumptions about the availability of task boundaries (i.e., knowing when new tasks begin). To address these limitations, we introduce Fed-A-GEM, a federated adaptation of the A-GEM method (Chaudhry et al., 2019), which employs a buffer-based gradient projection approach. Fed-A-GEM alleviates catastrophic forgetting by leveraging local buffer samples and aggregated buffer gradients, thus preserving knowledge across multiple clients. Our method is combined with existing CFL techniques, enhancing their performance in the CFL context. Our experiments on standard benchmarks show consistent performance improvements across diverse scenarios. For example, in a task-incremental learning scenario using the CIFAR-100 dataset, our method can increase the accuracy by up to 27%. Our code is available at this https URL.</li>
</ul>

<h3>Title: Booster: Tackling Harmful Fine-tuing for Large Language Models via Attenuating Harmful Perturbation</h3>
<ul>
<li><strong>Authors: </strong>Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01586">https://arxiv.org/abs/2409.01586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01586">https://arxiv.org/pdf/2409.01586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01586]] Booster: Tackling Harmful Fine-tuing for Large Language Models via Attenuating Harmful Perturbation(https://arxiv.org/abs/2409.01586)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, large language model</a></li>
<li><strong>Abstract: </strong>Harmful fine-tuning issue \citep{qi2023fine} poses serious safety concerns for Large language models' fine-tuning-as-a-service. While existing defenses \citep{huang2024vaccine,rosati2024representation} have been proposed to mitigate the issue, their performances are still far away from satisfactory, and the root cause of the problem has not been fully recovered. For the first time in the literature, we in this paper show that \textit{harmful perturbation} over the model weights should be the root cause of alignment-broken of harmful fine-tuning. In order to attenuate the negative impact of harmful perturbation, we propose an alignment-stage solution, dubbed Booster. Technically, along with the original alignment loss, we append a loss regularizer in the alignment stage's optimization. The regularizer ensures that the model's harmful loss reduction before/after simulated harmful perturbation is attenuated, thereby mitigating the subsequent fine-tuning risk. Empirical results show that Booster can effectively reduce the harmful score of the fine-tuned models while maintaining the performance of downstream tasks. Our code is available at \url{this https URL}.</li>
</ul>

<h3>Title: An Array Intermediate Language for Mixed Cryptography</h3>
<ul>
<li><strong>Authors: </strong>Vivian Ding, Co≈üku Acay, Andrew C. Myers</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01587">https://arxiv.org/abs/2409.01587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01587">https://arxiv.org/pdf/2409.01587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01587]] An Array Intermediate Language for Mixed Cryptography(https://arxiv.org/abs/2409.01587)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>We introduce AIRduct, a new array-based intermediate representation designed to support generating efficient code for interactive programs employing multiple cryptographic mechanisms. AIRduct is intended as an IR for the Viaduct compiler, which can synthesize secure, distributed programs with an extensible suite of cryptography. Therefore, AIRduct supports an extensible variety of cryptographic mechanisms, including MPC and ZKP.</li>
</ul>

<h3>Title: Dynamic Motion Synthesis: Masked Audio-Text Conditioned Spatio-Temporal Transformers</h3>
<ul>
<li><strong>Authors: </strong>Sohan Anisetty, James Hays</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01591">https://arxiv.org/abs/2409.01591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01591">https://arxiv.org/pdf/2409.01591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01591]] Dynamic Motion Synthesis: Masked Audio-Text Conditioned Spatio-Temporal Transformers(https://arxiv.org/abs/2409.01591)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Our research presents a novel motion generation framework designed to produce whole-body motion sequences conditioned on multiple modalities simultaneously, specifically text and audio inputs. Leveraging Vector Quantized Variational Autoencoders (VQVAEs) for motion discretization and a bidirectional Masked Language Modeling (MLM) strategy for efficient token prediction, our approach achieves improved processing efficiency and coherence in the generated motions. By integrating spatial attention mechanisms and a token critic we ensure consistency and naturalness in the generated motions. This framework expands the possibilities of motion generation, addressing the limitations of existing approaches and opening avenues for multimodal motion synthesis.</li>
</ul>

<h3>Title: DiVE: DiT-based Video Generation with Enhanced Control</h3>
<ul>
<li><strong>Authors: </strong>Junpeng Jiang, Gangyi Hong, Lijun Zhou, Enhui Ma, Hengtong Hu, Xia Zhou, Jie Xiang, Fan Liu, Kaicheng Yu, Haiyang Sun, Kun Zhan, Peng Jia, Miao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01595">https://arxiv.org/abs/2409.01595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01595">https://arxiv.org/pdf/2409.01595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01595]] DiVE: DiT-based Video Generation with Enhanced Control(https://arxiv.org/abs/2409.01595)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Generating high-fidelity, temporally consistent videos in autonomous driving scenarios faces a significant challenge, e.g. problematic maneuvers in corner cases. Despite recent video generation works are proposed to tackcle the mentioned problem, i.e. models built on top of Diffusion Transformers (DiT), works are still missing which are targeted on exploring the potential for multi-view videos generation scenarios. Noticeably, we propose the first DiT-based framework specifically designed for generating temporally and multi-view consistent videos which precisely match the given bird's-eye view layouts control. Specifically, the proposed framework leverages a parameter-free spatial view-inflated attention mechanism to guarantee the cross-view consistency, where joint cross-attention modules and ControlNet-Transformer are integrated to further improve the precision of control. To demonstrate our advantages, we extensively investigate the qualitative comparisons on nuScenes dataset, particularly in some most challenging corner cases. In summary, the effectiveness of our proposed method in producing long, controllable, and highly consistent videos under difficult conditions is proven to be effective.</li>
</ul>

<h3>Title: A Time-Intensity Aware Pipeline for Generating Late-Stage Breast DCE-MRI using Generative Adversarial Models</h3>
<ul>
<li><strong>Authors: </strong>Ruben D. Fonnegra, Maria Liliana Hern√°ndez, Juan C. Caicedo, Gloria M. D√≠az</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01596">https://arxiv.org/abs/2409.01596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01596">https://arxiv.org/pdf/2409.01596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01596]] A Time-Intensity Aware Pipeline for Generating Late-Stage Breast DCE-MRI using Generative Adversarial Models(https://arxiv.org/abs/2409.01596)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Contrast-enhancement pattern analysis is critical in breast magnetic resonance imaging (MRI) to distinguish benign from probably malignant tumors. However, contrast-enhanced image acquisitions are time-consuming and very expensive. As an alternative to physical acquisition, this paper proposes a comprehensive pipeline for the generation of accurate long-term (late) contrast-enhanced breast MRI from the early counterpart. The proposed strategy focuses on preserving the contrast agent pattern in the enhanced regions while maintaining visual properties in the entire synthesized images. To that end, a novel loss function that leverages the biological behavior of contrast agent (CA) in tissue, given by the Time-Intensity (TI) enhancement curve, is proposed to optimize a pixel-attention based generative model. In addition, unlike traditional normalization and standardization methods, we developed a new normalization strategy that maintains the contrast enhancement pattern across the image sequences at multiple timestamps. This ensures the prevalence of the CA pattern after image preprocessing, unlike conventional approaches. Furthermore, in order to objectively evaluate the clinical quality of the synthesized images, two metrics are also introduced to measure the differences between the TI curves of enhanced regions of the acquired and synthesized images. The experimental results showed that the proposed strategy generates images that significantly outperform diagnostic quality in contrast-enhanced regions while maintaining the spatial features of the entire image. This results suggest a potential use of synthetic late enhanced images generated via deep learning in clinical scenarios.</li>
</ul>

<h3>Title: Data-driven topology design based on principal component analysis for 3D structural design problems</h3>
<ul>
<li><strong>Authors: </strong>Jun Yang, Kentaro Yaji, Shintaro Yamasaki</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01607">https://arxiv.org/abs/2409.01607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01607">https://arxiv.org/pdf/2409.01607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01607]] Data-driven topology design based on principal component analysis for 3D structural design problems(https://arxiv.org/abs/2409.01607)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Topology optimization is a structural design methodology widely utilized to address engineering challenges. However, sensitivity-based topology optimization methods struggle to solve optimization problems characterized by strong non-linearity. Leveraging the sensitivity-free nature and high capacity of deep generative models, data-driven topology design (DDTD) methodology is considered an effective solution to this problem. Despite this, the training effectiveness of deep generative models diminishes when input size exceeds a threshold while maintaining high degrees of freedom is crucial for accurately characterizing complex structures. To resolve the conflict between the both, we propose DDTD based on principal component analysis (PCA). Its core idea is to replace the direct training of deep generative models with material distributions by using a principal component score matrix obtained from PCA computation and to obtain the generated material distributions with new features through the restoration process. We apply the proposed PCA-based DDTD to the problem of minimizing the maximum stress in 3D structural mechanics and demonstrate it can effectively address the current challenges faced by DDTD that fail to handle 3D structural design problems. Various experiments are conducted to demonstrate the effectiveness and practicability of the proposed PCA-based DDTD.</li>
</ul>

<h3>Title: Decompose the model: Mechanistic interpretability in image models with Generalized Integrated Gradients (GIG)</h3>
<ul>
<li><strong>Authors: </strong>Yearim Kim, Sangyu Han, Sangbum Han, Nojun Kwak</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01610">https://arxiv.org/abs/2409.01610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01610">https://arxiv.org/pdf/2409.01610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01610]] Decompose the model: Mechanistic interpretability in image models with Generalized Integrated Gradients (GIG)(https://arxiv.org/abs/2409.01610)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability</a></li>
<li><strong>Abstract: </strong>In the field of eXplainable AI (XAI) in language models, the progression from local explanations of individual decisions to global explanations with high-level concepts has laid the groundwork for mechanistic interpretability, which aims to decode the exact operations. However, this paradigm has not been adequately explored in image models, where existing methods have primarily focused on class-specific interpretations. This paper introduces a novel approach to systematically trace the entire pathway from input through all intermediate layers to the final output within the whole dataset. We utilize Pointwise Feature Vectors (PFVs) and Effective Receptive Fields (ERFs) to decompose model embeddings into interpretable Concept Vectors. Then, we calculate the relevance between concept vectors with our Generalized Integrated Gradients (GIG), enabling a comprehensive, dataset-wide analysis of model behavior. We validate our method of concept extraction and concept attribution in both qualitative and quantitative evaluations. Our approach advances the understanding of semantic significance within image models, offering a holistic view of their operational mechanics.</li>
</ul>

<h3>Title: On-chain Validation of Tracking Data Messages (TDM) Using Distributed Deep Learning on a Proof of Stake (PoS) Blockchain</h3>
<ul>
<li><strong>Authors: </strong>Yasir Latif, Anirban Chowdhury, Samya Bagchi</a></li>
<li><strong>Subjects: </strong>cs.CR, astro-ph.EP, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01614">https://arxiv.org/abs/2409.01614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01614">https://arxiv.org/pdf/2409.01614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01614]] On-chain Validation of Tracking Data Messages (TDM) Using Distributed Deep Learning on a Proof of Stake (PoS) Blockchain(https://arxiv.org/abs/2409.01614)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, transformer</a></li>
<li><strong>Abstract: </strong>Trustless tracking of Resident Space Objects (RSOs) is crucial for Space Situational Awareness (SSA), especially during adverse situations. The importance of transparent SSA cannot be overstated, as it is vital for ensuring space safety and security. In an era where RSO location information can be easily manipulated, the risk of RSOs being used as weapons is a growing concern. The Tracking Data Message (TDM) is a standardized format for broadcasting RSO observations. However, the varying quality of observations from diverse sensors poses challenges to SSA reliability. While many countries operate space assets, relatively few have SSA capabilities, making it crucial to ensure the accuracy and reliability of the data. Current practices assume complete trust in the transmitting party, leaving SSA capabilities vulnerable to adversarial actions such as spoofing TDMs. This work introduces a trustless mechanism for TDM validation and verification using deep learning over blockchain. By leveraging the trustless nature of blockchain, our approach eliminates the need for a central authority, establishing consensus-based truth. We propose a state-of-the-art, transformer-based orbit propagator that outperforms traditional methods like SGP4, enabling cross-validation of multiple observations for a single RSO. This deep learning-based transformer model can be distributed over a blockchain, allowing interested parties to host a node that contains a part of the distributed deep learning model. Our system comprises decentralised observers and validators within a Proof of Stake (PoS) blockchain. Observers contribute TDM data along with a stake to ensure honesty, while validators run the propagation and validation algorithms. The system rewards observers for contributing verified TDMs and penalizes those submitting unverifiable data.</li>
</ul>

<h3>Title: Dynamic Guidance Adversarial Distillation with Enhanced Teacher Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Hyejin Park, Dongbo Min</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01627">https://arxiv.org/abs/2409.01627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01627">https://arxiv.org/pdf/2409.01627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01627]] Dynamic Guidance Adversarial Distillation with Enhanced Teacher Knowledge(https://arxiv.org/abs/2409.01627)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust</a></li>
<li><strong>Abstract: </strong>In the realm of Adversarial Distillation (AD), strategic and precise knowledge transfer from an adversarially robust teacher model to a less robust student model is paramount. Our Dynamic Guidance Adversarial Distillation (DGAD) framework directly tackles the challenge of differential sample importance, with a keen focus on rectifying the teacher model's misclassifications. DGAD employs Misclassification-Aware Partitioning (MAP) to dynamically tailor the distillation focus, optimizing the learning process by steering towards the most reliable teacher predictions. Additionally, our Error-corrective Label Swapping (ELS) corrects misclassifications of the teacher on both clean and adversarially perturbed inputs, refining the quality of knowledge transfer. Further, Predictive Consistency Regularization (PCR) guarantees consistent performance of the student model across both clean and adversarial inputs, significantly enhancing its overall robustness. By integrating these methodologies, DGAD significantly improves upon the accuracy of clean data and fortifies the model's defenses against sophisticated adversarial threats. Our experimental validation on CIFAR10, CIFAR100, and Tiny ImageNet datasets, employing various model architectures, demonstrates the efficacy of DGAD, establishing it as a promising approach for enhancing both the robustness and accuracy of student models in adversarial settings.</li>
</ul>

<h3>Title: CTG-KrEW: Generating Synthetic Structured Contextually Correlated Content by Conditional Tabular GAN with K-Means Clustering and Efficient Word Embedding</h3>
<ul>
<li><strong>Authors: </strong>Riya Samanta, Bidyut Saha, Soumya K. Ghosh, Sajal K. Das</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01628">https://arxiv.org/abs/2409.01628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01628">https://arxiv.org/pdf/2409.01628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01628]] CTG-KrEW: Generating Synthetic Structured Contextually Correlated Content by Conditional Tabular GAN with K-Means Clustering and Efficient Word Embedding(https://arxiv.org/abs/2409.01628)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Conditional Tabular Generative Adversarial Networks (CTGAN) and their various derivatives are attractive for their ability to efficiently and flexibly create synthetic tabular data, showcasing strong performance and adaptability. However, there are certain critical limitations to such models. The first is their inability to preserve the semantic integrity of contextually correlated words or phrases. For instance, skillset in freelancer profiles is one such attribute where individual skills are semantically interconnected and indicative of specific domain interests or qualifications. The second challenge of traditional approaches is that, when applied to generate contextually correlated tabular content, besides generating semantically shallow content, they consume huge memory resources and CPU time during the training stage. To address these problems, we introduce a novel framework, CTGKrEW (Conditional Tabular GAN with KMeans Clustering and Word Embedding), which is adept at generating realistic synthetic tabular data where attributes are collections of semantically and contextually coherent words. CTGKrEW is trained and evaluated using a dataset from Upwork, a realworld freelancing platform. Comprehensive experiments were conducted to analyze the variability, contextual similarity, frequency distribution, and associativity of the generated data, along with testing the framework's system feasibility. CTGKrEW also takes around 99\% less CPU time and 33\% less memory footprints than the conventional approach. Furthermore, we developed KrEW, a web application to facilitate the generation of realistic data containing skill-related information. This application, available at this https URL, is freely accessible to both the general public and the research community.</li>
</ul>

<h3>Title: Unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Kun Zhou, Xinyu Lin, Wenbo Li, Xiaogang Xu, Yuanhao Cai, Zhonghang Liu, Xiaoguang Han, Jiangbo Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01641">https://arxiv.org/abs/2409.01641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01641">https://arxiv.org/pdf/2409.01641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01641]] Unveiling Advanced Frequency Disentanglement Paradigm for Low-Light Image Enhancement(https://arxiv.org/abs/2409.01641)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Previous low-light image enhancement (LLIE) approaches, while employing frequency decomposition techniques to address the intertwined challenges of low frequency (e.g., illumination recovery) and high frequency (e.g., noise reduction), primarily focused on the development of dedicated and complex networks to achieve improved performance. In contrast, we reveal that an advanced disentanglement paradigm is sufficient to consistently enhance state-of-the-art methods with minimal computational overhead. Leveraging the image Laplace decomposition scheme, we propose a novel low-frequency consistency method, facilitating improved frequency disentanglement optimization. Our method, seamlessly integrating with various models such as CNNs, Transformers, and flow-based and diffusion models, demonstrates remarkable adaptability. Noteworthy improvements are showcased across five popular benchmarks, with up to 7.68dB gains on PSNR achieved for six state-of-the-art models. Impressively, our approach maintains efficiency with only 88K extra parameters, setting a new standard in the challenging realm of low-light image enhancement.</li>
</ul>

<h3>Title: From Yes-Men to Truth-Tellers: Addressing Sycophancy in Large Language Models with Pinpoint Tuning</h3>
<ul>
<li><strong>Authors: </strong>Wei Chen, Zhen Huang, Liang Xie, Binbin Lin, Houqiang Li, Le Lu, Xinmei Tian, Deng Cai, Yonggang Zhang, Wenxiao Wan, Xu Shen, Jieping Ye</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01658">https://arxiv.org/abs/2409.01658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01658">https://arxiv.org/pdf/2409.01658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01658]] From Yes-Men to Truth-Tellers: Addressing Sycophancy in Large Language Models with Pinpoint Tuning(https://arxiv.org/abs/2409.01658)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) tend to prioritize adherence to user prompts over providing veracious responses, leading to the sycophancy issue. When challenged by users, LLMs tend to admit mistakes and provide inaccurate responses even if they initially provided the correct answer. Recent works propose to employ supervised fine-tuning (SFT) to mitigate the sycophancy issue, while it typically leads to the degeneration of LLMs' general capability. To address the challenge, we propose a novel supervised pinpoint tuning (SPT), where the region-of-interest modules are tuned for a given objective. Specifically, SPT first reveals and verifies a small percentage (<5%) of the basic modules, which significantly affect a particular behavior of LLMs. i.e., sycophancy. Subsequently, SPT merely fine-tunes these identified modules while freezing the rest. To verify the effectiveness of the proposed SPT, we conduct comprehensive experiments, demonstrating that SPT significantly mitigates the sycophancy issue of LLMs (even better than SFT). Moreover, SPT introduces limited or even no side effects on the general capability of LLMs. Our results shed light on how to precisely, effectively, and efficiently explain and improve the targeted ability of LLMs.</li>
</ul>

<h3>Title: Interpreting and Improving Large Language Models in Arithmetic Calculation</h3>
<ul>
<li><strong>Authors: </strong>Wei Zhang, Chaoqun Wan, Yonggang Zhang, Yiu-ming Cheung, Xinmei Tian, Xu Shen, Jieping Ye</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01659">https://arxiv.org/abs/2409.01659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01659">https://arxiv.org/pdf/2409.01659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01659]] Interpreting and Improving Large Language Models in Arithmetic Calculation(https://arxiv.org/abs/2409.01659)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable potential across numerous applications and have shown an emergent ability to tackle complex reasoning tasks, such as mathematical computations. However, even for the simplest arithmetic calculations, the intrinsic mechanisms behind LLMs remain mysterious, making it challenging to ensure reliability. In this work, we delve into uncovering a specific mechanism by which LLMs execute calculations. Through comprehensive experiments, we find that LLMs frequently involve a small fraction (< 5%) of attention heads, which play a pivotal role in focusing on operands and operators during calculation processes. Subsequently, the information from these operands is processed through multi-layer perceptrons (MLPs), progressively leading to the final solution. These pivotal heads/MLPs, though identified on a specific dataset, exhibit transferability across different datasets and even distinct tasks. This insight prompted us to investigate the potential benefits of selectively fine-tuning these essential heads/MLPs to boost the LLMs' computational performance. We empirically find that such precise tuning can yield notable enhancements on mathematical prowess, without compromising the performance on non-mathematical tasks. Our work serves as a preliminary exploration into the arithmetic calculation abilities inherent in LLMs, laying a solid foundation to reveal more intricate mathematical tasks.</li>
</ul>

<h3>Title: $S^2$NeRF: Privacy-preserving Training Framework for NeRF</h3>
<ul>
<li><strong>Authors: </strong>Bokang Zhang, Yanglin Zhang, Zhikun Zhang, Jinglan Yang, Lingying Huang, Junfeng Wu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01661">https://arxiv.org/abs/2409.01661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01661">https://arxiv.org/pdf/2409.01661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01661]] $S^2$NeRF: Privacy-preserving Training Framework for NeRF(https://arxiv.org/abs/2409.01661)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, defense, attack</a></li>
<li><strong>Abstract: </strong>Neural Radiance Fields (NeRF) have revolutionized 3D computer vision and graphics, facilitating novel view synthesis and influencing sectors like extended reality and e-commerce. However, NeRF's dependence on extensive data collection, including sensitive scene image data, introduces significant privacy risks when users upload this data for model training. To address this concern, we first propose SplitNeRF, a training framework that incorporates split learning (SL) techniques to enable privacy-preserving collaborative model training between clients and servers without sharing local data. Despite its benefits, we identify vulnerabilities in SplitNeRF by developing two attack methods, Surrogate Model Attack and Scene-aided Surrogate Model Attack, which exploit the shared gradient data and a few leaked scene images to reconstruct private scene information. To counter these threats, we introduce $S^2$NeRF, secure SplitNeRF that integrates effective defense mechanisms. By introducing decaying noise related to the gradient norm into the shared gradient information, $S^2$NeRF preserves privacy while maintaining a high utility of the NeRF model. Our extensive evaluations across multiple datasets demonstrate the effectiveness of $S^2$NeRF against privacy breaches, confirming its viability for secure NeRF training in sensitive applications.</li>
</ul>

<h3>Title: Efficiently Expanding Receptive Fields: Local Split Attention and Parallel Aggregation for Enhanced Large-scale Point Cloud Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Haodong Wang, Chongyu Wang, Yinghui Quan, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01662">https://arxiv.org/abs/2409.01662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01662">https://arxiv.org/pdf/2409.01662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01662]] Efficiently Expanding Receptive Fields: Local Split Attention and Parallel Aggregation for Enhanced Large-scale Point Cloud Semantic Segmentation(https://arxiv.org/abs/2409.01662)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Expanding the receptive field in a deep learning model for large-scale 3D point cloud segmentation is an effective technique for capturing rich contextual information, which consequently enhances the network's ability to learn meaningful features. However, this often leads to increased computational complexity and risk of overfitting, challenging the efficiency and effectiveness of the learning paradigm. To address these limitations, we propose the Local Split Attention Pooling (LSAP) mechanism to effectively expand the receptive field through a series of local split operations, thus facilitating the acquisition of broader contextual knowledge. Concurrently, it optimizes the computational workload associated with attention-pooling layers to ensure a more streamlined processing workflow. Based on LSAP, a Parallel Aggregation Enhancement (PAE) module is introduced to enable parallel processing of data using both 2D and 3D neighboring information to further enhance contextual representations within the network. In light of the aforementioned designs, we put forth a novel framework, designated as LSNet, for large-scale point cloud semantic segmentation. Extensive evaluations demonstrated the efficacy of seamlessly integrating the proposed PAE module into existing frameworks, yielding significant improvements in mean intersection over union (mIoU) metrics, with a notable increase of up to 11%. Furthermore, LSNet demonstrated superior performance compared to state-of-the-art semantic segmentation networks on three benchmark datasets, including S3DIS, Toronto3D, and SensatUrban. It is noteworthy that our method achieved a substantial speedup of approximately 38.8% compared to those employing similar-sized receptive fields, which serves to highlight both its computational efficiency and practical utility in real-world large-scale scenes.</li>
</ul>

<h3>Title: In Defense of RAG in the Era of Long-Context Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tan Yu, Anbang Xu, Rama Akkiraju</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01666">https://arxiv.org/abs/2409.01666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01666">https://arxiv.org/pdf/2409.01666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01666]] In Defense of RAG in the Era of Long-Context Language Models(https://arxiv.org/abs/2409.01666)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense</a></li>
<li><strong>Abstract: </strong>Overcoming the limited context limitations in early-generation LLMs, retrieval-augmented generation (RAG) has been a reliable solution for context-based answer generation in the past. Recently, the emergence of long-context LLMs allows the models to incorporate much longer text sequences, making RAG less attractive. Recent studies show that long-context LLMs significantly outperform RAG in long-context applications. Unlike the existing works favoring the long-context LLM over RAG, we argue that the extremely long context in LLMs suffers from a diminished focus on relevant information and leads to potential degradation in answer quality. This paper revisits the RAG in long-context answer generation. We propose an order-preserve retrieval-augmented generation (OP-RAG) mechanism, which significantly improves the performance of RAG for long-context question-answer applications. With OP-RAG, as the number of retrieved chunks increases, the answer quality initially rises, and then declines, forming an inverted U-shaped curve. There exist sweet points where OP-RAG could achieve higher answer quality with much less tokens than long-context LLM taking the whole context as input. Extensive experiments on public benchmark demonstrate the superiority of our OP-RAG.</li>
</ul>

<h3>Title: VProChart: Answering Chart Question through Visual Perception Alignment Agent and Programmatic Solution Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Muye Huang, Lingling Zhang, Lai Han, Wenjun Wu, Xinyu Zhang, Jun Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01667">https://arxiv.org/abs/2409.01667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01667">https://arxiv.org/pdf/2409.01667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01667]] VProChart: Answering Chart Question through Visual Perception Alignment Agent and Programmatic Solution Reasoning(https://arxiv.org/abs/2409.01667)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Charts are widely used for data visualization across various fields, including education, research, and business. Chart Question Answering (CQA) is an emerging task focused on the automatic interpretation and reasoning of data presented in charts. However, chart images are inherently difficult to interpret, and chart-related questions often involve complex logical and numerical reasoning, which hinders the performance of existing models. This paper introduces VProChart, a novel framework designed to address these challenges in CQA by integrating a lightweight Visual Perception Alignment Agent (VPAgent) and a Programmatic Solution Reasoning approach. VPAgent aligns and models chart elements based on principles of human visual perception, enhancing the understanding of chart context. The Programmatic Solution Reasoning approach leverages large language models (LLMs) to transform natural language reasoning questions into structured solution programs, facilitating precise numerical and logical reasoning. Extensive experiments on benchmark datasets such as ChartQA and PlotQA demonstrate that VProChart significantly outperforms existing methods, highlighting its capability in understanding and reasoning with charts.</li>
</ul>

<h3>Title: Enhancing Fine-Grained Visual Recognition in the Low-Data Regime Through Feature Magnitude Regularization</h3>
<ul>
<li><strong>Authors: </strong>Avraham Chapman, Haiming Xu, Lingqiao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01672">https://arxiv.org/abs/2409.01672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01672">https://arxiv.org/pdf/2409.01672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01672]] Enhancing Fine-Grained Visual Recognition in the Low-Data Regime Through Feature Magnitude Regularization(https://arxiv.org/abs/2409.01672)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Training a fine-grained image recognition model with limited data presents a significant challenge, as the subtle differences between categories may not be easily discernible amidst distracting noise patterns. One commonly employed strategy is to leverage pretrained neural networks, which can generate effective feature representations for constructing an image classification model with a restricted dataset. However, these pretrained neural networks are typically trained for different tasks than the fine-grained visual recognition (FGVR) task at hand, which can lead to the extraction of less relevant features. Moreover, in the context of building FGVR models with limited data, these irrelevant features can dominate the training process, overshadowing more useful, generalizable discriminative features. Our research has identified a surprisingly simple solution to this challenge: we introduce a regularization technique to ensure that the magnitudes of the extracted features are evenly distributed. This regularization is achieved by maximizing the uniformity of feature magnitude distribution, measured through the entropy of the normalized features. The motivation behind this regularization is to remove bias in feature magnitudes from pretrained models, where some features may be more prominent and, consequently, more likely to be used for classification. Additionally, we have developed a dynamic weighting mechanism to adjust the strength of this regularization throughout the learning process. Despite its apparent simplicity, our approach has demonstrated significant performance improvements across various fine-grained visual recognition datasets.</li>
</ul>

<h3>Title: Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Wenyang Hu, Gaetan Frusque, Tianyang Wang, Fulei Chu, Olga Fink</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01676">https://arxiv.org/abs/2409.01676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01676">https://arxiv.org/pdf/2409.01676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01676]] Classifier-Free Diffusion-Based Weakly-Supervised Approach for Health Indicator Derivation in Rotating Machines: Advancing Early Fault Detection and Condition Monitoring(https://arxiv.org/abs/2409.01676)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability, diffusion</a></li>
<li><strong>Abstract: </strong>Deriving health indicators of rotating machines is crucial for their maintenance. However, this process is challenging for the prevalent adopted intelligent methods since they may take the whole data distributions, not only introducing noise interference but also lacking the explainability. To address these issues, we propose a diffusion-based weakly-supervised approach for deriving health indicators of rotating machines, enabling early fault detection and continuous monitoring of condition evolution. This approach relies on a classifier-free diffusion model trained using healthy samples and a few anomalies. This model generates healthy samples. and by comparing the differences between the original samples and the generated ones in the envelope spectrum, we construct an anomaly map that clearly identifies faults. Health indicators are then derived, which can explain the fault types and mitigate noise interference. Comparative studies on two cases demonstrate that the proposed method offers superior health monitoring effectiveness and robustness compared to baseline models.</li>
</ul>

<h3>Title: Optimizing Mortality Prediction for ICU Heart Failure Patients: Leveraging XGBoost and Advanced Machine Learning with the MIMIC-III Database</h3>
<ul>
<li><strong>Authors: </strong>Negin Ashrafi, Armin Abdollahi, Jiahong Zhang, Maryam Pishgar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01685">https://arxiv.org/abs/2409.01685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01685">https://arxiv.org/pdf/2409.01685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01685]] Optimizing Mortality Prediction for ICU Heart Failure Patients: Leveraging XGBoost and Advanced Machine Learning with the MIMIC-III Database(https://arxiv.org/abs/2409.01685)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Heart failure affects millions of people worldwide, significantly reducing quality of life and leading to high mortality rates. Despite extensive research, the relationship between heart failure and mortality rates among ICU patients is not fully understood, indicating the need for more accurate prediction models. This study analyzed data from 1,177 patients over 18 years old from the MIMIC-III database, identified using ICD-9 codes. Preprocessing steps included handling missing data, removing duplicates, treating skewness, and using oversampling techniques to address data imbalances. Through rigorous feature selection using Variance Inflation Factor (VIF), expert clinical input, and ablation studies, 46 key features were identified to enhance model performance. Our analysis compared several machine learning models, including Logistic Regression, Support Vector Machine (SVM), Random Forest, LightGBM, and XGBoost. XGBoost emerged as the superior model, achieving a test AUC-ROC of 0.9228 (95\% CI 0.8748 - 0.9613), significantly outperforming our previous work (AUC-ROC of 0.8766) and the best results reported in existing literature (AUC-ROC of 0.824). The improved model's success is attributed to advanced feature selection methods, robust preprocessing techniques, and comprehensive hyperparameter optimization through Grid-Search. SHAP analysis and feature importance evaluations based on XGBoost highlighted key variables like leucocyte count and RDW, providing valuable insights into the clinical factors influencing mortality risk. This framework offers significant support for clinicians, enabling them to identify high-risk ICU heart failure patients and improve patient outcomes through timely and informed interventions.</li>
</ul>

<h3>Title: Frequency-Spatial Entanglement Learning for Camouflaged Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Yanguang Sun, Chunyan Xu, Jian Yang, Hanyu Xuan, Lei Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01686">https://arxiv.org/abs/2409.01686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01686">https://arxiv.org/pdf/2409.01686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01686]] Frequency-Spatial Entanglement Learning for Camouflaged Object Detection(https://arxiv.org/abs/2409.01686)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Camouflaged object detection has attracted a lot of attention in computer vision. The main challenge lies in the high degree of similarity between camouflaged objects and their surroundings in the spatial domain, making identification difficult. Existing methods attempt to reduce the impact of pixel similarity by maximizing the distinguishing ability of spatial features with complicated design, but often ignore the sensitivity and locality of features in the spatial domain, leading to sub-optimal results. In this paper, we propose a new approach to address this issue by jointly exploring the representation in the frequency and spatial domains, introducing the Frequency-Spatial Entanglement Learning (FSEL) method. This method consists of a series of well-designed Entanglement Transformer Blocks (ETB) for representation learning, a Joint Domain Perception Module for semantic enhancement, and a Dual-domain Reverse Parser for feature integration in the frequency and spatial domains. Specifically, the ETB utilizes frequency self-attention to effectively characterize the relationship between different frequency bands, while the entanglement feed-forward network facilitates information interaction between features of different domains through entanglement learning. Our extensive experiments demonstrate the superiority of our FSEL over 21 state-of-the-art methods, through comprehensive quantitative and qualitative comparisons in three widely-used datasets. The source code is available at: this https URL.</li>
</ul>

<h3>Title: Taming CLIP for Fine-grained and Structured Visual Understanding of Museum Exhibits</h3>
<ul>
<li><strong>Authors: </strong>Ada-Astrid Balauca, Danda Pani Paudel, Kristina Toutanova, Luc Van Gool</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01690">https://arxiv.org/abs/2409.01690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01690">https://arxiv.org/pdf/2409.01690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01690]] Taming CLIP for Fine-grained and Structured Visual Understanding of Museum Exhibits(https://arxiv.org/abs/2409.01690)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>CLIP is a powerful and widely used tool for understanding images in the context of natural language descriptions to perform nuanced tasks. However, it does not offer application-specific fine-grained and structured understanding, due to its generic nature. In this work, we aim to adapt CLIP for fine-grained and structured -- in the form of tabular data -- visual understanding of museum exhibits. To facilitate such understanding we (a) collect, curate, and benchmark a dataset of 200K+ image-table pairs, and (b) develop a method that allows predicting tabular outputs for input images. Our dataset is the first of its kind in the public domain. At the same time, the proposed method is novel in leveraging CLIP's powerful representations for fine-grained and tabular understanding. The proposed method (MUZE) learns to map CLIP's image embeddings to the tabular structure by means of a proposed transformer-based parsing network (parseNet). More specifically, parseNet enables prediction of missing attribute values while integrating context from known attribute-value pairs for an input image. We show that this leads to significant improvement in accuracy. Through exhaustive experiments, we show the effectiveness of the proposed method on fine-grained and structured understanding of museum exhibits, by achieving encouraging results in a newly established benchmark. Our dataset and source-code can be found at: this https URL</li>
</ul>

<h3>Title: When 3D Partial Points Meets SAM: Tooth Point Cloud Segmentation with Sparse Labels</h3>
<ul>
<li><strong>Authors: </strong>Yifan Liu, Wuyang Li, Cheng Wang, Hui Chen, Yixuan Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01691">https://arxiv.org/abs/2409.01691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01691">https://arxiv.org/pdf/2409.01691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01691]] When 3D Partial Points Meets SAM: Tooth Point Cloud Segmentation with Sparse Labels(https://arxiv.org/abs/2409.01691)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Tooth point cloud segmentation is a fundamental task in many orthodontic applications. Current research mainly focuses on fully supervised learning which demands expensive and tedious manual point-wise annotation. Although recent weakly-supervised alternatives are proposed to use weak labels for 3D segmentation and achieve promising results, they tend to fail when the labels are extremely sparse. Inspired by the powerful promptable segmentation capability of the Segment Anything Model (SAM), we propose a framework named SAMTooth that leverages such capacity to complement the extremely sparse supervision. To automatically generate appropriate point prompts for SAM, we propose a novel Confidence-aware Prompt Generation strategy, where coarse category predictions are aggregated with confidence-aware filtering. Furthermore, to fully exploit the structural and shape clues in SAM's outputs for assisting the 3D feature learning, we advance a Mask-guided Representation Learning that re-projects the generated tooth masks of SAM into 3D space and constrains these points of different teeth to possess distinguished representations. To demonstrate the effectiveness of the framework, we conduct experiments on the public dataset and surprisingly find with only 0.1\% annotations (one point per tooth), our method can surpass recent weakly supervised methods by a large margin, and the performance is even comparable to the recent fully-supervised methods, showcasing the significant potential of applying SAM to 3D perception tasks with sparse labels. Code is available at this https URL.</li>
</ul>

<h3>Title: On the Vulnerability of Skip Connections to Model Inversion Attacks</h3>
<ul>
<li><strong>Authors: </strong>Jun Hao Koh, Sy-Tuyen Ho, Ngoc-Bao Nguyen, Ngai-man Cheung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01696">https://arxiv.org/abs/2409.01696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01696">https://arxiv.org/pdf/2409.01696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01696]] On the Vulnerability of Skip Connections to Model Inversion Attacks(https://arxiv.org/abs/2409.01696)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Skip connections are fundamental architecture designs for modern deep neural networks (DNNs) such as CNNs and ViTs. While they help improve model performance significantly, we identify a vulnerability associated with skip connections to Model Inversion (MI) attacks, a type of privacy attack that aims to reconstruct private training data through abusive exploitation of a model. In this paper, as a pioneer work to understand how DNN architectures affect MI, we study the impact of skip connections on MI. We make the following discoveries: 1) Skip connections reinforce MI attacks and compromise data privacy. 2) Skip connections in the last stage are the most critical to attack. 3) RepVGG, an approach to remove skip connections in the inference-time architectures, could not mitigate the vulnerability to MI attacks. 4) Based on our findings, we propose MI-resilient architecture designs for the first time. Without bells and whistles, we show in extensive experiments that our MI-resilient architectures can outperform state-of-the-art (SOTA) defense methods in MI robustness. Furthermore, our MI-resilient architectures are complementary to existing MI defense methods. Our project is available at this https URL</li>
</ul>

<h3>Title: ACCESS-FL: Agile Communication and Computation for Efficient Secure Aggregation in Stable Federated Learning Networks</h3>
<ul>
<li><strong>Authors: </strong>Niousha Nazemi, Omid Tavallaie, Shuaijun Chen, Anna Maria Mandalario, Kanchana Thilakarathna, Ralph Holz, Hamed Haddadi, Albert Y. Zomaya</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01722">https://arxiv.org/abs/2409.01722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01722">https://arxiv.org/pdf/2409.01722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01722]] ACCESS-FL: Agile Communication and Computation for Efficient Secure Aggregation in Stable Federated Learning Networks(https://arxiv.org/abs/2409.01722)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a promising distributed learning framework designed for privacy-aware applications. FL trains models on client devices without sharing the client's data and generates a global model on a server by aggregating model updates. Traditional FL approaches risk exposing sensitive client data when plain model updates are transmitted to the server, making them vulnerable to security threats such as model inversion attacks where the server can infer the client's original training data from monitoring the changes of the trained model in different rounds. Google's Secure Aggregation (SecAgg) protocol addresses this threat by employing a double-masking technique, secret sharing, and cryptography computations in honest-but-curious and adversarial scenarios with client dropouts. However, in scenarios without the presence of an active adversary, the computational and communication cost of SecAgg significantly increases by growing the number of clients. To address this issue, in this paper, we propose ACCESS-FL, a communication-and-computation-efficient secure aggregation method designed for honest-but-curious scenarios in stable FL networks with a limited rate of client dropout. ACCESS-FL reduces the computation/communication cost to a constant level (independent of the network size) by generating shared secrets between only two clients and eliminating the need for double masking, secret sharing, and cryptography computations. To evaluate the performance of ACCESS-FL, we conduct experiments using the MNIST, FMNIST, and CIFAR datasets to verify the performance of our proposed method. The evaluation results demonstrate that our proposed method significantly reduces computation and communication overhead compared to state-of-the-art methods, SecAgg and SecAgg+.</li>
</ul>

<h3>Title: Shuffle Mamba: State Space Models with Random Shuffle for Multi-Modal Image Fusion</h3>
<ul>
<li><strong>Authors: </strong>Ke Cao, Xuanhua He, Tao Hu, Chengjun Xie, Jie Zhang, Man Zhou, Danfeng Hong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01728">https://arxiv.org/abs/2409.01728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01728">https://arxiv.org/pdf/2409.01728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01728]] Shuffle Mamba: State Space Models with Random Shuffle for Multi-Modal Image Fusion(https://arxiv.org/abs/2409.01728)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-modal image fusion integrates complementary information from different modalities to produce enhanced and informative images. Although State-Space Models, such as Mamba, are proficient in long-range modeling with linear complexity, most Mamba-based approaches use fixed scanning strategies, which can introduce biased prior information. To mitigate this issue, we propose a novel Bayesian-inspired scanning strategy called Random Shuffle, supplemented by an theoretically-feasible inverse shuffle to maintain information coordination invariance, aiming to eliminate biases associated with fixed sequence scanning. Based on this transformation pair, we customized the Shuffle Mamba Framework, penetrating modality-aware information representation and cross-modality information interaction across spatial and channel axes to ensure robust interaction and an unbiased global receptive field for multi-modal image fusion. Furthermore, we develop a testing methodology based on Monte-Carlo averaging to ensure the model's output aligns more closely with expected results. Extensive experiments across multiple multi-modal image fusion tasks demonstrate the effectiveness of our proposed method, yielding excellent fusion quality over state-of-the-art alternatives. Code will be available upon acceptance.</li>
</ul>

<h3>Title: Federated Prediction-Powered Inference from Decentralized Data</h3>
<ul>
<li><strong>Authors: </strong>Ping Luo, Xiaoge Deng, Ziqing Wen, Tao Sun, Dongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01730">https://arxiv.org/abs/2409.01730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01730">https://arxiv.org/pdf/2409.01730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01730]] Federated Prediction-Powered Inference from Decentralized Data(https://arxiv.org/abs/2409.01730)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>In various domains, the increasing application of machine learning allows researchers to access inexpensive predictive data, which can be utilized as auxiliary data for statistical inference. Although such data are often unreliable compared to gold-standard datasets, Prediction-Powered Inference (PPI) has been proposed to ensure statistical validity despite the unreliability. However, the challenge of `data silos' arises when the private gold-standard datasets are non-shareable for model training, leading to less accurate predictive models and invalid inferences. In this paper, we introduces the Federated Prediction-Powered Inference (Fed-PPI) framework, which addresses this challenge by enabling decentralized experimental data to contribute to statistically valid conclusions without sharing private information. The Fed-PPI framework involves training local models on private data, aggregating them through Federated Learning (FL), and deriving confidence intervals using PPI computation. The proposed framework is evaluated through experiments, demonstrating its effectiveness in producing valid confidence intervals.</li>
</ul>

<h3>Title: State-of-the-art Advances of Deep-learning Linguistic Steganalysis Research</h3>
<ul>
<li><strong>Authors: </strong>Yihao Wang, Ru Zhang, Yifan Tang, Jianyi Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01780">https://arxiv.org/abs/2409.01780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01780">https://arxiv.org/pdf/2409.01780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01780]] State-of-the-art Advances of Deep-learning Linguistic Steganalysis Research(https://arxiv.org/abs/2409.01780)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, generative</a></li>
<li><strong>Abstract: </strong>With the evolution of generative linguistic steganography techniques, conventional steganalysis falls short in robustly quantifying the alterations induced by steganography, thereby complicating detection. Consequently, the research paradigm has pivoted towards deep-learning-based linguistic steganalysis. This study offers a comprehensive review of existing contributions and evaluates prevailing developmental trajectories. Specifically, we first provided a formalized exposition of the general formulas for linguistic steganalysis, while comparing the differences between this field and the domain of text classification. Subsequently, we classified the existing work into two levels based on vector space mapping and feature extraction models, thereby comparing the research motivations, model advantages, and other details. A comparative analysis of the experiments is conducted to assess the performances. Finally, the challenges faced by this field are discussed, and several directions for future development and key issues that urgently need to be addressed are proposed.</li>
</ul>

<h3>Title: Dual Advancement of Representation Learning and Clustering for Sparse and Noisy Images</h3>
<ul>
<li><strong>Authors: </strong>Wenlin Li, Yucheng Xu, Xiaoqing Zheng, Suoya Han, Jun Wang, Xiaobo Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01781">https://arxiv.org/abs/2409.01781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01781">https://arxiv.org/pdf/2409.01781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01781]] Dual Advancement of Representation Learning and Clustering for Sparse and Noisy Images(https://arxiv.org/abs/2409.01781)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Sparse and noisy images (SNIs), like those in spatial gene expression data, pose significant challenges for effective representation learning and clustering, which are essential for thorough data analysis and interpretation. In response to these challenges, we propose Dual Advancement of Representation Learning and Clustering (DARLC), an innovative framework that leverages contrastive learning to enhance the representations derived from masked image modeling. Simultaneously, DARLC integrates cluster assignments in a cohesive, end-to-end approach. This integrated clustering strategy addresses the "class collision problem" inherent in contrastive learning, thus improving the quality of the resulting representations. To generate more plausible positive views for contrastive learning, we employ a graph attention network-based technique that produces denoised images as augmented data. As such, our framework offers a comprehensive approach that improves the learning of representations by enhancing their local perceptibility, distinctiveness, and the understanding of relational semantics. Furthermore, we utilize a Student's t mixture model to achieve more robust and adaptable clustering of SNIs. Extensive experiments, conducted across 12 different types of datasets consisting of SNIs, demonstrate that DARLC surpasses the state-of-the-art methods in both image clustering and generating image representations that accurately capture gene interactions. Code is available at this https URL.</li>
</ul>

<h3>Title: LLM-GAN: Construct Generative Adversarial Network Through Large Language Models For Explainable Fake News Detection</h3>
<ul>
<li><strong>Authors: </strong>Yifeng Wang, Zhouhong Gu, Siwei Zhang, Suhang Zheng, Tao Wang, Tianyu Li, Hongwei Feng, Yanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01787">https://arxiv.org/abs/2409.01787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01787">https://arxiv.org/pdf/2409.01787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01787]] LLM-GAN: Construct Generative Adversarial Network Through Large Language Models For Explainable Fake News Detection(https://arxiv.org/abs/2409.01787)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Explainable fake news detection predicts the authenticity of news items with annotated explanations. Today, Large Language Models (LLMs) are known for their powerful natural language understanding and explanation generation abilities. However, presenting LLMs for explainable fake news detection remains two main challenges. Firstly, fake news appears reasonable and could easily mislead LLMs, leaving them unable to understand the complex news-faking process. Secondly, utilizing LLMs for this task would generate both correct and incorrect explanations, which necessitates abundant labor in the loop. In this paper, we propose LLM-GAN, a novel framework that utilizes prompting mechanisms to enable an LLM to become Generator and Detector and for realistic fake news generation and detection. Our results demonstrate LLM-GAN's effectiveness in both prediction performance and explanation quality. We further showcase the integration of LLM-GAN to a cloud-native AI platform to provide better fake news detection service in the cloud.</li>
</ul>

<h3>Title: DogeFuzz: A Simple Yet Efficient Grey-box Fuzzer for Ethereum Smart Contracts</h3>
<ul>
<li><strong>Authors: </strong>Ismael Medeiros, Fausto Carvalho, Alexandre Ferreira, Rodrigo Bonif√°cio, Fabiano Cavalcanti Fernandes</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01788">https://arxiv.org/abs/2409.01788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01788">https://arxiv.org/pdf/2409.01788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01788]] DogeFuzz: A Simple Yet Efficient Grey-box Fuzzer for Ethereum Smart Contracts(https://arxiv.org/abs/2409.01788)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Ethereum is a distributed, peer-to-peer blockchain infrastructure that has attracted billions of dollars. Perhaps due to its success, Ethereum has become a target for various kinds of attacks, motivating researchers to explore different techniques to identify vulnerabilities in EVM bytecode (the language of the Ethereum Virtual Machine), including formal verification, symbolic execution, and fuzz testing. Although recent studies empirically compare smart contract fuzzers, there is a lack of literature investigating how simpler greybox fuzzers compare to more advanced ones. To fill this gap, in this paper, we present DogeFuzz, an extensible infrastructure for fuzzing Ethereum smart contracts, currently supporting black-box fuzzing and two grey-box fuzzing strategies: coverage-guided grey-box fuzzing (DogeFuzz-G) and directed grey-box fuzzing (DogeFuzz-DG). We conduct a series of experiments using benchmarks already available in the literature and compare the DogeFuzz strategies with state-of-the-art fuzzers for smart contracts. Surprisingly, although DogeFuzz does not leverage advanced techniques for improving input generation (such as symbolic execution or machine learning), DogeFuzz outperforms sFuzz and ILF, two state-of-the-art fuzzers. Nonetheless, the Smartian fuzzer shows higher code coverage and bug-finding capabilities than DogeFuzz.</li>
</ul>

<h3>Title: Training on the Benchmark Is Not All You Need</h3>
<ul>
<li><strong>Authors: </strong>Shiwen Ni, Xiangtao Kong, Chengming Li, Xiping Hu, Ruifeng Xu, Jia Zhu, Min Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01790">https://arxiv.org/abs/2409.01790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01790">https://arxiv.org/pdf/2409.01790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01790]] Training on the Benchmark Is Not All You Need(https://arxiv.org/abs/2409.01790)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The success of Large Language Models (LLMs) relies heavily on the huge amount of pre-training data learned in the pre-training phase. The opacity of the pre-training process and the training data causes the results of many benchmark tests to become unreliable. If any model has been trained on a benchmark test set, it can seriously hinder the health of the field. In order to automate and efficiently test the capabilities of large language models, numerous mainstream benchmarks adopt a multiple-choice format. As the swapping of the contents of multiple-choice options does not affect the meaning of the question itself, we propose a simple and effective data leakage detection method based on this property. Specifically, we shuffle the contents of the options in the data to generate the corresponding derived data sets, and then detect data leakage based on the model's log probability distribution over the derived data sets. If there is a maximum and outlier in the set of log probabilities, it indicates that the data is leaked. Our method is able to work under black-box conditions without access to model training data or weights, effectively identifying data leakage from benchmark test sets in model pre-training data, including both normal scenarios and complex scenarios where options may have been shuffled intentionally or unintentionally. Through experiments based on two LLMs and benchmark designs, we demonstrate the effectiveness of our method. In addition, we evaluate the degree of data leakage of 31 mainstream open-source LLMs on four benchmark datasets and give a ranking of the leaked LLMs for each benchmark, and we find that the Qwen family of LLMs has the highest degree of data leakage.</li>
</ul>

<h3>Title: EPRecon: An Efficient Framework for Real-Time Panoptic 3D Reconstruction from Monocular Video</h3>
<ul>
<li><strong>Authors: </strong>Zhen Zhou, Yunkai Ma, Junfeng Fan, Shaolin Zhang, Fengshui Jing, Min Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01807">https://arxiv.org/abs/2409.01807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01807">https://arxiv.org/pdf/2409.01807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01807]] EPRecon: An Efficient Framework for Real-Time Panoptic 3D Reconstruction from Monocular Video(https://arxiv.org/abs/2409.01807)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Panoptic 3D reconstruction from a monocular video is a fundamental perceptual task in robotic scene understanding. However, existing efforts suffer from inefficiency in terms of inference speed and accuracy, limiting their practical applicability. We present EPRecon, an efficient real-time panoptic 3D reconstruction framework. Current volumetric-based reconstruction methods usually utilize multi-view depth map fusion to obtain scene depth priors, which is time-consuming and poses challenges to real-time scene reconstruction. To end this, we propose a lightweight module to directly estimate scene depth priors in a 3D volume for reconstruction quality improvement by generating occupancy probabilities of all voxels. In addition, to infer richer panoptic features from occupied voxels, EPRecon extracts panoptic features from both voxel features and corresponding image features, obtaining more detailed and comprehensive instance-level semantic information and achieving more accurate segmentation results. Experimental results on the ScanNetV2 dataset demonstrate the superiority of EPRecon over current state-of-the-art methods in terms of both panoptic 3D reconstruction quality and real-time inference. Code is available at this https URL.</li>
</ul>

<h3>Title: Segmenting Object Affordances: Reproducibility and Sensitivity to Scale</h3>
<ul>
<li><strong>Authors: </strong>Tommaso Apicella, Alessio Xompero, Paolo Gastaldo, Andrea Cavallaro</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01814">https://arxiv.org/abs/2409.01814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01814">https://arxiv.org/pdf/2409.01814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01814]] Segmenting Object Affordances: Reproducibility and Sensitivity to Scale(https://arxiv.org/abs/2409.01814)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, segmentation</a></li>
<li><strong>Abstract: </strong>Visual affordance segmentation identifies image regions of an object an agent can interact with. Existing methods re-use and adapt learning-based architectures for semantic segmentation to the affordance segmentation task and evaluate on small-size datasets. However, experimental setups are often not reproducible, thus leading to unfair and inconsistent comparisons. In this work, we benchmark these methods under a reproducible setup on two single objects scenarios, tabletop without occlusions and hand-held containers, to facilitate future comparisons. We include a version of a recent architecture, Mask2Former, re-trained for affordance segmentation and show that this model is the best-performing on most testing sets of both scenarios. Our analysis shows that models are not robust to scale variations when object resolutions differ from those in the training set.</li>
</ul>

<h3>Title: DarthShader: Fuzzing WebGPU Shader Translators & Compilers</h3>
<ul>
<li><strong>Authors: </strong>Lukas Bernhard, Nico Schiller, Moritz Schloegel, Nils Bars, Thorsten Holz</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01824">https://arxiv.org/abs/2409.01824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01824">https://arxiv.org/pdf/2409.01824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01824]] DarthShader: Fuzzing WebGPU Shader Translators & Compilers(https://arxiv.org/abs/2409.01824)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>A recent trend towards running more demanding web applications, such as video games or client-side LLMs, in the browser has led to the adoption of the WebGPU standard that provides a cross-platform API exposing the GPU to websites. This opens up a new attack surface: Untrusted web content is passed through to the GPU stack, which traditionally has been optimized for performance instead of security. Worsening the problem, most of WebGPU cannot be run in the tightly sandboxed process that manages other web content, which eases the attacker's path to compromising the client machine. Contrasting its importance, WebGPU shader processing has received surprisingly little attention from the automated testing community. Part of the reason is that shader translators expect highly structured and statically typed input, which renders typical fuzzing mutations ineffective. Complicating testing further, shader translation consists of a complex multi-step compilation pipeline, each stage presenting unique requirements and challenges. In this paper, we propose DarthShader, the first language fuzzer that combines mutators based on an intermediate representation with those using a more traditional abstract syntax tree. The key idea is that the individual stages of the shader compilation pipeline are susceptible to different classes of faults, requiring entirely different mutation strategies for thorough testing. By fuzzing the full pipeline, we ensure that we maintain a realistic attacker model. In an empirical evaluation, we show that our method outperforms the state-of-the-art fuzzers regarding code coverage. Furthermore, an extensive ablation study validates our key design. DarthShader found a total of 39 software faults in all modern browsers -- Chrome, Firefox, and Safari -- that prior work missed. For 15 of them, the Chrome team assigned a CVE, acknowledging the impact of our results.</li>
</ul>

<h3>Title: AstroMAE: Redshift Prediction Using a Masked Autoencoder with a Novel Fine-Tuning Architecture</h3>
<ul>
<li><strong>Authors: </strong>Amirreza Dolatpour Fathkouhi, Geoffrey Charles Fox</a></li>
<li><strong>Subjects: </strong>cs.CV, astro-ph.GA, astro-ph.IM, cs.CE, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01825">https://arxiv.org/abs/2409.01825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01825">https://arxiv.org/pdf/2409.01825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01825]] AstroMAE: Redshift Prediction Using a Masked Autoencoder with a Novel Fine-Tuning Architecture(https://arxiv.org/abs/2409.01825)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Redshift prediction is a fundamental task in astronomy, essential for understanding the expansion of the universe and determining the distances of astronomical objects. Accurate redshift prediction plays a crucial role in advancing our knowledge of the cosmos. Machine learning (ML) methods, renowned for their precision and speed, offer promising solutions for this complex task. However, traditional ML algorithms heavily depend on labeled data and task-specific feature extraction. To overcome these limitations, we introduce AstroMAE, an innovative approach that pretrains a vision transformer encoder using a masked autoencoder method on Sloan Digital Sky Survey (SDSS) images. This technique enables the encoder to capture the global patterns within the data without relying on labels. To the best of our knowledge, AstroMAE represents the first application of a masked autoencoder to astronomical data. By ignoring labels during the pretraining phase, the encoder gathers a general understanding of the data. The pretrained encoder is subsequently fine-tuned within a specialized architecture tailored for redshift prediction. We evaluate our model against various vision transformer architectures and CNN-based models, demonstrating the superior performance of AstroMAEs pretrained model and fine-tuning architecture.</li>
</ul>

<h3>Title: Towards Generative Class Prompt Learning for Few-shot Visual Recognition</h3>
<ul>
<li><strong>Authors: </strong>Soumitri Chattopadhyay, Sanket Biswas, Emanuele Vivoli, Josep Llad√≥s</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01835">https://arxiv.org/abs/2409.01835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01835">https://arxiv.org/pdf/2409.01835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01835]] Towards Generative Class Prompt Learning for Few-shot Visual Recognition(https://arxiv.org/abs/2409.01835)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Although foundational vision-language models (VLMs) have proven to be very successful for various semantic discrimination tasks, they still struggle to perform faithfully for fine-grained categorization. Moreover, foundational models trained on one domain do not generalize well on a different domain without fine-tuning. We attribute these to the limitations of the VLM's semantic representations and attempt to improve their fine-grained visual awareness using generative modeling. Specifically, we propose two novel methods: Generative Class Prompt Learning (GCPL) and Contrastive Multi-class Prompt Learning (CoMPLe). Utilizing text-to-image diffusion models, GCPL significantly improves the visio-linguistic synergy in class embeddings by conditioning on few-shot exemplars with learnable class prompts. CoMPLe builds on this foundation by introducing a contrastive learning component that encourages inter-class separation during the generative optimization process. Our empirical results demonstrate that such a generative class prompt learning approach substantially outperform existing methods, offering a better alternative to few shot image recognition challenges. The source code will be made available at: this https URL.</li>
</ul>

<h3>Title: AgentRE: An Agent-Based Framework for Navigating Complex Information Landscapes in Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Shi, Guochao Jiang, Tian Qiu, Deqing Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01854">https://arxiv.org/abs/2409.01854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01854">https://arxiv.org/pdf/2409.01854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01854]] AgentRE: An Agent-Based Framework for Navigating Complex Information Landscapes in Relation Extraction(https://arxiv.org/abs/2409.01854)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>The relation extraction (RE) in complex scenarios faces challenges such as diverse relation types and ambiguous relations between entities within a single sentence, leading to the poor performance of pure "text-in, text-out" language models (LMs). To address these challenges, in this paper, we propose an agent-based RE framework, namely AgentRE, which fully leverages the potential of large language models (LLMs) including memory, retrieval and reflection, to achieve RE in complex scenarios. Specifically, three major modules are built in AgentRE serving as the tools to help the agent acquire and process various useful information, thereby obtaining improved RE performance. Our extensive experimental results upon two datasets in English and Chinese demonstrate our AgentRE's superior performance, especially in low-resource scenarios. Additionally, the trajectories generated by AgentRE can be refined to construct a high-quality training dataset incorporating different reasoning methods, which can be used to fine-tune smaller models. Code is available at this https URL.</li>
</ul>

<h3>Title: Graph-based Modeling and Simulation of Emergency Services Communication Systems</h3>
<ul>
<li><strong>Authors: </strong>Jardi Martinez Jordan, Michael Stiber</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01855">https://arxiv.org/abs/2409.01855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01855">https://arxiv.org/pdf/2409.01855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01855]] Graph-based Modeling and Simulation of Emergency Services Communication Systems(https://arxiv.org/abs/2409.01855)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Emergency Services Communication Systems (ESCS) are evolving into Internet Protocol based communication networks, promising enhancements to their function, availability, and resilience. This increase in complexity and cyber-attack surface demands better understanding of these systems' breakdown dynamics under extreme circumstances. Existing ESCS research largely overlooks simulation and the little work that exists focuses primarily on cybersecurity threats and neglects critical factors such as non-stationarity of call arrivals. This paper introduces a robust, adaptable graph-based simulation framework and essential mathematical models for ESCS simulation. The framework uses a representation of ESCSes where each vertex is a communicating finite-state machine that exchanges messages along edges and whose behavior is governed by a discrete event queuing model. Call arrival burstiness and its connection to emergency incidents is modeled through a cluster point process. Model applicability is demonstrated through simulations of the Seattle Police Department ESCS. Ongoing work is developing GPU implementation and exploring use in cybersecurity tabletop exercises.</li>
</ul>

<h3>Title: Explicit Second-order LiDAR Bundle Adjustment Algorithm Using Mean Squared Group Metric</h3>
<ul>
<li><strong>Authors: </strong>Tingchen Ma, Yongsheng Ou, Sheng Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01856">https://arxiv.org/abs/2409.01856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01856">https://arxiv.org/pdf/2409.01856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01856]] Explicit Second-order LiDAR Bundle Adjustment Algorithm Using Mean Squared Group Metric(https://arxiv.org/abs/2409.01856)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>The Bundle Adjustment (BA) algorithm is a widely used nonlinear optimization technique in the backend of Simultaneous Localization and Mapping (SLAM) systems. By leveraging the co-view relationships of landmarks from multiple perspectives, it constructs a joint estimation model for both poses and landmarks, enabling the system to generate refined maps and reduce front-end localization errors. However, applying BA to LiDAR data presents unique challenges due to the large volume of 3D points typically present in point clouds, making robust and accurate model solving more complex. In this work, we propose a novel mean square group metric (MSGM). This metric applies mean square transformation to uniformly process the measurement of plane landmarks from a single perspective. The transformed metric ensures scale interpretability while avoiding the time-consuming point-by-point calculations. By integrating a robust kernel function, the metrics involved in the BA model are reweighted, enhancing the robustness of the solution process. On the basis of the proposed robust LiDAR BA model, we derived an explicit second-order estimator (RSO-BA). This estimator employs analytical formulas for Hessian and gradient calculations, ensuring the precision of the BA solution. We evaluated the proposed RSO-BA estimator against existing implicit second-order and explicit approximate second-order estimators using the publicly available datasets. The experimental results demonstrate that the RSO-BA estimator outperforms its counterparts regarding registration accuracy and robustness, particularly in large-scale or complex unstructured environments.</li>
</ul>

<h3>Title: Real-Time Indoor Object Detection based on hybrid CNN-Transformer Approach</h3>
<ul>
<li><strong>Authors: </strong>Salah Eddine Laidoudi, Madjid Maidi, Samir Otmane</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01871">https://arxiv.org/abs/2409.01871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01871">https://arxiv.org/pdf/2409.01871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01871]] Real-Time Indoor Object Detection based on hybrid CNN-Transformer Approach(https://arxiv.org/abs/2409.01871)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Real-time object detection in indoor settings is a challenging area of computer vision, faced with unique obstacles such as variable lighting and complex backgrounds. This field holds significant potential to revolutionize applications like augmented and mixed realities by enabling more seamless interactions between digital content and the physical world. However, the scarcity of research specifically fitted to the intricacies of indoor environments has highlighted a clear gap in the literature. To address this, our study delves into the evaluation of existing datasets and computational models, leading to the creation of a refined dataset. This new dataset is derived from OpenImages v7, focusing exclusively on 32 indoor categories selected for their relevance to real-world applications. Alongside this, we present an adaptation of a CNN detection model, incorporating an attention mechanism to enhance the model's ability to discern and prioritize critical features within cluttered indoor scenes. Our findings demonstrate that this approach is not just competitive with existing state-of-the-art models in accuracy and speed but also opens new avenues for research and application in the field of real-time indoor object detection.</li>
</ul>

<h3>Title: CyberHost: Taming Audio-driven Avatar Diffusion Model with Region Codebook Attention</h3>
<ul>
<li><strong>Authors: </strong>Gaojie Lin, Jianwen Jiang, Chao Liang, Tianyun Zhong, Jiaqi Yang, Yanbo Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01876">https://arxiv.org/abs/2409.01876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01876">https://arxiv.org/pdf/2409.01876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01876]] CyberHost: Taming Audio-driven Avatar Diffusion Model with Region Codebook Attention(https://arxiv.org/abs/2409.01876)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based video generation technology has advanced significantly, catalyzing a proliferation of research in human animation. However, the majority of these studies are confined to same-modality driving settings, with cross-modality human body animation remaining relatively underexplored. In this paper, we introduce, an end-to-end audio-driven human animation framework that ensures hand integrity, identity consistency, and natural motion. The key design of CyberHost is the Region Codebook Attention mechanism, which improves the generation quality of facial and hand animations by integrating fine-grained local features with learned motion pattern priors. Furthermore, we have developed a suite of human-prior-guided training strategies, including body movement map, hand clarity score, pose-aligned reference feature, and local enhancement supervision, to improve synthesis results. To our knowledge, CyberHost is the first end-to-end audio-driven human diffusion model capable of facilitating zero-shot video generation within the scope of human body. Extensive experiments demonstrate that CyberHost surpasses previous works in both quantitative and qualitative aspects.</li>
</ul>

<h3>Title: SPiKE: 3D Human Pose from Point Cloud Sequences</h3>
<ul>
<li><strong>Authors: </strong>Irene Ballester, Ond≈ôej Peterka, Martin Kampel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01879">https://arxiv.org/abs/2409.01879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01879">https://arxiv.org/pdf/2409.01879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01879]] SPiKE: 3D Human Pose from Point Cloud Sequences(https://arxiv.org/abs/2409.01879)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>3D Human Pose Estimation (HPE) is the task of locating keypoints of the human body in 3D space from 2D or 3D representations such as RGB images, depth maps or point clouds. Current HPE methods from depth and point clouds predominantly rely on single-frame estimation and do not exploit temporal information from sequences. This paper presents SPiKE, a novel approach to 3D HPE using point cloud sequences. Unlike existing methods that process frames of a sequence independently, SPiKE leverages temporal context by adopting a Transformer architecture to encode spatio-temporal relationships between points across the sequence. By partitioning the point cloud into local volumes and using spatial feature extraction via point spatial convolution, SPiKE ensures efficient processing by the Transformer while preserving spatial integrity per timestamp. Experiments on the ITOP benchmark for 3D HPE show that SPiKE reaches 89.19% mAP, achieving state-of-the-art performance with significantly lower inference times. Extensive ablations further validate the effectiveness of sequence exploitation and our algorithmic choices. Code and models are available at: this https URL</li>
</ul>

<h3>Title: The Impact of Run-Time Variability on Side-Channel Attacks Targeting FPGAs</h3>
<ul>
<li><strong>Authors: </strong>Davide Galli, Adriano Guarisco, William Fornaciari, Matteo Matteucci, Davide Zoni</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01881">https://arxiv.org/abs/2409.01881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01881">https://arxiv.org/pdf/2409.01881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01881]] The Impact of Run-Time Variability on Side-Channel Attacks Targeting FPGAs(https://arxiv.org/abs/2409.01881)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust</a></li>
<li><strong>Abstract: </strong>To defeat side-channel attacks, many recent countermeasures work by enforcing random run-time variability to the target computing platform in terms of clock jitters, frequency and voltage scaling, and phase shift, also combining the contributions from different actuators to maximize the side-channel resistance of the target. However, the robustness of such solutions seems strongly influenced by several hyper-parameters for which an in-depth analysis is still missing. This work proposes a fine-grained dynamic voltage and frequency scaling actuator to investigate the effectiveness of recent desynchronization countermeasures with the goal of highlighting the link between the enforced run-time variability and the vulnerability to side-channel attacks of cryptographic implementations targeting FPGAs. The analysis of the results collected from real hardware allowed for a comprehensive understanding of the protection offered by run-time variability countermeasures against side-channel attacks.</li>
</ul>

<h3>Title: Investigating Expert-in-the-Loop LLM Discourse Patterns for Ancient Intertextual Analysis</h3>
<ul>
<li><strong>Authors: </strong>Ray Umphrey, Jesse Roberts, Lindsey Roberts</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01882">https://arxiv.org/abs/2409.01882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01882">https://arxiv.org/pdf/2409.01882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01882]] Investigating Expert-in-the-Loop LLM Discourse Patterns for Ancient Intertextual Analysis(https://arxiv.org/abs/2409.01882)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study explores the potential of large language models (LLMs) for identifying and examining intertextual relationships within biblical, Koine Greek texts. By evaluating the performance of LLMs on various intertextuality scenarios the study demonstrates that these models can detect direct quotations, allusions, and echoes between texts. The LLM's ability to generate novel intertextual observations and connections highlights its potential to uncover new insights. However, the model also struggles with long query passages and the inclusion of false intertextual dependences, emphasizing the importance of expert evaluation. The expert-in-the-loop methodology presented offers a scalable approach for intertextual research into the complex web of intertextuality within and beyond the biblical corpus.</li>
</ul>

<h3>Title: Detecting and Measuring Security Implications of Entangled Domain Verification in CDN</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Lin, Zhiwei Lin, Run Guo, Jianjun Chen, Mingming Zhang, Ximeng Liu, Tianhao Yang, Zhuoran Cao, Robert H. Deng</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01887">https://arxiv.org/abs/2409.01887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01887">https://arxiv.org/pdf/2409.01887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01887]] Detecting and Measuring Security Implications of Entangled Domain Verification in CDN(https://arxiv.org/abs/2409.01887)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>Content Delivery Networks (CDNs) offer a protection layer for enhancing the security of websites. However, a significant security flaw named Absence of Domain Verification (DVA) has become emerging recently. Although this threat is recognized, the current practices and security flaws of domain verification strategies in CDNs have not been thoroughly investigated. In this paper, we present DVAHunter, an automated system for detecting DVA vulnerabilities that can lead to domain abuse in CDNs. Our evaluation of 45 major CDN providers reveals the prevalence of DVA: most (39/45) providers do not perform any verification, and even those that do remain exploitable. Additionally, we used DVAHunter to conduct a large-scale measurement of 89M subdomains from Tranco's Top 1M sites hosted on the 45 CDNs under evaluation. Our focus was on two primary DVA exploitation scenarios: covert communication and domain hijacking. We identified over 332K subdomains vulnerable to domain abuse. This tool provides deeper insights into DVA exploitation and allows us to propose viable mitigation practices for CDN providers. To date, we have received vulnerability confirmations from 12 providers; 6 (e.g., Edgio, Kuocai) have implemented fixes, and 1 (ChinaNetCenter) are actively working on solutions based on our recommendations.</li>
</ul>

<h3>Title: What are the Essential Factors in Crafting Effective Long Context Multi-Hop Instruction Datasets? Insights and Best Practices</h3>
<ul>
<li><strong>Authors: </strong>Zhi Chen, Qiguang Chen, Libo Qin, Qipeng Guo, Haijun Lv, Yicheng Zou, Wanxiang Che, Hang Yan, Kai Chen, Dahua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01893">https://arxiv.org/abs/2409.01893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01893">https://arxiv.org/pdf/2409.01893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01893]] What are the Essential Factors in Crafting Effective Long Context Multi-Hop Instruction Datasets? Insights and Best Practices(https://arxiv.org/abs/2409.01893)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) with extended context windows have significantly improved tasks such as information extraction, question answering, and complex planning scenarios. In order to achieve success in long context tasks, a large amount of work has been done to enhance the long context capabilities of the model through synthetic data. Existing methods typically utilize the Self-Instruct framework to generate instruction tuning data for better long context capability improvement. However, our preliminary experiments indicate that less than 35% of generated samples are multi-hop, and more than 40% exhibit poor quality, limiting comprehensive understanding and further research. To improve the quality of synthetic data, we propose the Multi-agent Interactive Multi-hop Generation (MIMG) framework, incorporating a Quality Verification Agent, a Single-hop Question Generation Agent, a Multiple Question Sampling Strategy, and a Multi-hop Question Merger Agent. This framework improves the data quality, with the proportion of high-quality, multi-hop, and diverse data exceeding 85%. Furthermore, we systematically investigate strategies for document selection, question merging, and validation techniques through extensive experiments across various models. Our findings show that our synthetic high-quality long-context instruction data significantly enhances model performance, even surpassing models trained on larger amounts of human-annotated data. Our code is available at: this https URL.</li>
</ul>

<h3>Title: Privacy-Preserving and Post-Quantum Counter Denial of Service Framework for Wireless Networks</h3>
<ul>
<li><strong>Authors: </strong>Saleh Darzi, Attila Altay Yavuz</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01924">https://arxiv.org/abs/2409.01924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01924">https://arxiv.org/pdf/2409.01924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01924]] Privacy-Preserving and Post-Quantum Counter Denial of Service Framework for Wireless Networks(https://arxiv.org/abs/2409.01924)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, defense, attack</a></li>
<li><strong>Abstract: </strong>As network services progress and mobile and IoT environments expand, numerous security concerns have surfaced for spectrum access systems. The omnipresent risk of Denial-of-Service (DoS) attacks and raising concerns about user privacy (e.g., location privacy, anonymity) are among such cyber threats. These security and privacy risks increase due to the threat of quantum computers that can compromise long-term security by circumventing conventional cryptosystems and increasing the cost of countermeasures. While some defense mechanisms exist against these threats in isolation, there is a significant gap in the state of the art on a holistic solution against DoS attacks with privacy and anonymity for spectrum management systems, especially when post-quantum (PQ) security is in mind. In this paper, we propose a new cybersecurity framework PACDoSQ, which is (to the best of our knowledge) the first to offer location privacy and anonymity for spectrum management with counter DoS and PQ security simultaneously. Our solution introduces the private spectrum bastion (database) concept to exploit existing architectural features of spectrum management systems and then synergizes them with multi-server private information retrieval and PQ-secure Tor to guarantee a location-private and anonymous acquisition of spectrum information together with hash-based client-server puzzles for counter DoS. We prove that PACDoSQ achieves its security objectives, and show its feasibility via a comprehensive performance evaluation.</li>
</ul>

<h3>Title: Comprehensive Equity Index (CEI): Definition and Application to Bias Evaluation in Biometrics</h3>
<ul>
<li><strong>Authors: </strong>Imanol Solano, Alejandro Pe√±a, Aythami Morales, Julian Fierrez, Ruben Tolosana, Francisco Zamora-Martinez, Javier San Agustin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01928">https://arxiv.org/abs/2409.01928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01928">https://arxiv.org/pdf/2409.01928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01928]] Comprehensive Equity Index (CEI): Definition and Application to Bias Evaluation in Biometrics(https://arxiv.org/abs/2409.01928)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric, fair</a></li>
<li><strong>Abstract: </strong>We present a novel metric designed, among other applications, to quantify biased behaviors of machine learning models. As its core, the metric consists of a new similarity metric between score distributions that balances both their general shapes and tails' probabilities. In that sense, our proposed metric may be useful in many application areas. Here we focus on and apply it to the operational evaluation of face recognition systems, with special attention to quantifying demographic biases; an application where our metric is especially useful. The topic of demographic bias and fairness in biometric recognition systems has gained major attention in recent years. The usage of these systems has spread in society, raising concerns about the extent to which these systems treat different population groups. A relevant step to prevent and mitigate demographic biases is first to detect and quantify them. Traditionally, two approaches have been studied to quantify differences between population groups in machine learning literature: 1) measuring differences in error rates, and 2) measuring differences in recognition score distributions. Our proposed Comprehensive Equity Index (CEI) trade-offs both approaches combining both errors from distribution tails and general distribution shapes. This new metric is well suited to real-world scenarios, as measured on NIST FRVT evaluations, involving high-performance systems and realistic face databases including a wide range of covariates and demographic groups. We first show the limitations of existing metrics to correctly assess the presence of biases in realistic setups and then propose our new metric to tackle these limitations. We tested the proposed metric with two state-of-the-art models and four widely used databases, showing its capacity to overcome the main flaws of previous bias metrics.</li>
</ul>

<h3>Title: Map-Assisted Remote-Sensing Image Compression at Extremely Low Bitrates</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Ye, Ce Wang, Wanjie Sun, Zhenzhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01935">https://arxiv.org/abs/2409.01935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01935">https://arxiv.org/pdf/2409.01935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01935]] Map-Assisted Remote-Sensing Image Compression at Extremely Low Bitrates(https://arxiv.org/abs/2409.01935)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Remote-sensing (RS) image compression at extremely low bitrates has always been a challenging task in practical scenarios like edge device storage and narrow bandwidth transmission. Generative models including VAEs and GANs have been explored to compress RS images into extremely low-bitrate streams. However, these generative models struggle to reconstruct visually plausible images due to the highly ill-posed nature of extremely low-bitrate image compression. To this end, we propose an image compression framework that utilizes a pre-trained diffusion model with powerful natural image priors to achieve high-realism reconstructions. However, diffusion models tend to hallucinate small structures and textures due to the significant information loss at limited bitrates. Thus, we introduce vector maps as semantic and structural guidance and propose a novel image compression approach named Map-Assisted Generative Compression (MAGC). MAGC employs a two-stage pipeline to compress and decompress RS images at extremely low bitrates. The first stage maps an image into a latent representation, which is then further compressed in a VAE architecture to save bitrates and serves as implicit guidance in the subsequent diffusion process. The second stage conducts a conditional diffusion model to generate a visually pleasing and semantically accurate result using implicit guidance and explicit semantic guidance. Quantitative and qualitative comparisons show that our method outperforms standard codecs and other learning-based methods in terms of perceptual quality and semantic accuracy. The dataset and code will be publicly available at this https URL.</li>
</ul>

<h3>Title: Optimizing CLIP Models for Image Retrieval with Maintained Joint-Embedding Alignment</h3>
<ul>
<li><strong>Authors: </strong>Konstantin Schall, Kai Uwe Barthel, Nico Hezel, Klaus Jung</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01936">https://arxiv.org/abs/2409.01936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01936">https://arxiv.org/pdf/2409.01936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01936]] Optimizing CLIP Models for Image Retrieval with Maintained Joint-Embedding Alignment(https://arxiv.org/abs/2409.01936)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Contrastive Language and Image Pairing (CLIP), a transformative method in multimedia retrieval, typically trains two neural networks concurrently to generate joint embeddings for text and image pairs. However, when applied directly, these models often struggle to differentiate between visually distinct images that have similar captions, resulting in suboptimal performance for image-based similarity searches. This paper addresses the challenge of optimizing CLIP models for various image-based similarity search scenarios, while maintaining their effectiveness in text-based search tasks such as text-to-image retrieval and zero-shot classification. We propose and evaluate two novel methods aimed at refining the retrieval capabilities of CLIP without compromising the alignment between text and image embeddings. The first method involves a sequential fine-tuning process: initially optimizing the image encoder for more precise image retrieval and subsequently realigning the text encoder to these optimized image embeddings. The second approach integrates pseudo-captions during the retrieval-optimization phase to foster direct alignment within the embedding space. Through comprehensive experiments, we demonstrate that these methods enhance CLIP's performance on various benchmarks, including image retrieval, k-NN classification, and zero-shot text-based classification, while maintaining robustness in text-to-image retrieval. Our optimized models permit maintaining a single embedding per image, significantly simplifying the infrastructure needed for large-scale multi-modal similarity search systems.</li>
</ul>

<h3>Title: Towards Leveraging Large Language Models for Automated Medical Q&A Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Jack Krolik, Herprit Mahal, Feroz Ahmad, Gaurav Trivedi, Bahador Saket</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01941">https://arxiv.org/abs/2409.01941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01941">https://arxiv.org/pdf/2409.01941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01941]] Towards Leveraging Large Language Models for Automated Medical Q&A Evaluation(https://arxiv.org/abs/2409.01941)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper explores the potential of using Large Language Models (LLMs) to automate the evaluation of responses in medical Question and Answer (Q\&A) systems, a crucial form of Natural Language Processing. Traditionally, human evaluation has been indispensable for assessing the quality of these responses. However, manual evaluation by medical professionals is time-consuming and costly. Our study examines whether LLMs can reliably replicate human evaluations by using questions derived from patient data, thereby saving valuable time for medical experts. While the findings suggest promising results, further research is needed to address more specific or complex questions that were beyond the scope of this initial investigation.</li>
</ul>

<h3>Title: FuzzCoder: Byte-level Fuzzing Test via Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Liqun Yang, Jian Yang, Chaoren Wei, Guanglin Niu, Ge Zhang, Yunli Wang, Linzheng ChaI, Wanxu Xia, Hongcheng Guo, Shun Zhang, Jiaheng Liu, Yuwei Yin, Junran Peng, Jiaxin Ma, Liang Sun, Zhoujun Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01944">https://arxiv.org/abs/2409.01944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01944">https://arxiv.org/pdf/2409.01944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01944]] FuzzCoder: Byte-level Fuzzing Test via Large Language Model(https://arxiv.org/abs/2409.01944)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Fuzzing is an important dynamic program analysis technique designed for finding vulnerabilities in complex software. Fuzzing involves presenting a target program with crafted malicious input to cause crashes, buffer overflows, memory errors, and exceptions. Crafting malicious inputs in an efficient manner is a difficult open problem and the best approaches often apply uniform random mutations to pre-existing valid inputs. In this work, we propose to adopt fine-tuned large language models (FuzzCoder) to learn patterns in the input files from successful attacks to guide future fuzzing explorations. Specifically, we develop a framework to leverage the code LLMs to guide the mutation process of inputs in fuzzing. The mutation process is formulated as the sequence-to-sequence modeling, where LLM receives a sequence of bytes and then outputs the mutated byte sequence. FuzzCoder is fine-tuned on the created instruction dataset (Fuzz-Instruct), where the successful fuzzing history is collected from the heuristic fuzzing tool. FuzzCoder can predict mutation locations and strategies locations in input files to trigger abnormal behaviors of the program. Experimental results show that FuzzCoder based on AFL (American Fuzzy Lop) gain significant improvements in terms of effective proportion of mutation (EPM) and number of crashes (NC) for various input formats including ELF, JPG, MP3, and XML.</li>
</ul>

<h3>Title: Exploiting the Vulnerability of Large Language Models via Defense-Aware Architectural Backdoor</h3>
<ul>
<li><strong>Authors: </strong>Abdullah Arafat Miah, Yu Bi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01952">https://arxiv.org/abs/2409.01952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01952">https://arxiv.org/pdf/2409.01952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01952]] Exploiting the Vulnerability of Large Language Models via Defense-Aware Architectural Backdoor(https://arxiv.org/abs/2409.01952)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) have long been recognized as vulnerable to backdoor attacks. By providing poisoned training data in the fine-tuning process, the attacker can implant a backdoor into the victim model. This enables input samples meeting specific textual trigger patterns to be classified as target labels of the attacker's choice. While such black-box attacks have been well explored in both computer vision and natural language processing (NLP), backdoor attacks relying on white-box attack philosophy have hardly been thoroughly investigated. In this paper, we take the first step to introduce a new type of backdoor attack that conceals itself within the underlying model architecture. Specifically, we pcricKet1996!ropose to design separate backdoor modules consisting of two functions: trigger detection and noise injection. The add-on modules of model architecture layers can detect the presence of input trigger tokens and modify layer weights using Gaussian noise to disturb the feature distribution of the baseline model. We conduct extensive experiments to evaluate our attack methods using two model architecture settings on five different large language datasets. We demonstrate that the training-free architectural backdoor on a large language model poses a genuine threat. Unlike the-state-of-art work, it can survive the rigorous fine-tuning and retraining process, as well as evade output probability-based defense methods (i.e. BDDR). All the code and data is available this https URL.</li>
</ul>

<h3>Title: Private Electronic Payments with Self-Custody and Zero-Knowledge Verified Reissuance</h3>
<ul>
<li><strong>Authors: </strong>Daniele Friolo, Geoffrey Goodell, Dann Toliver, Hazem Danny Nakib</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01958">https://arxiv.org/abs/2409.01958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01958">https://arxiv.org/pdf/2409.01958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01958]] Private Electronic Payments with Self-Custody and Zero-Knowledge Verified Reissuance(https://arxiv.org/abs/2409.01958)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect</a></li>
<li><strong>Abstract: </strong>This article builds upon the protocol for digital transfers described by Goodell, Toliver, and Nakib, which combines privacy by design for consumers with strong compliance enforcement for recipients of payments and self-validating assets that carry their own verifiable provenance information. We extend the protocol to allow for the verification that reissued assets were created in accordance with rules prohibiting the creation of new assets by anyone but the issuer, without exposing information about the circumstances in which the assets were created that could be used to identify the payer. The modified protocol combines an audit log with zero-knowledge proofs, so that a consumer spending an asset can demonstrate that there exists a valid entry on the audit log that is associated with the asset, without specifying which entry it is. This property is important as a means to allow money to be reissued within the system without the involvement of system operators within the zone of control of the original issuer. Additionally, we identify a key property of privacy-respecting electronic payments, wherein the payer is not required to retain secrets arising from one transaction until the following transaction, and argue that this property is essential to framing security requirements for storage of digital assets and the risk of blackmail or coercion as a way to exfiltrate information about payment history. We claim that the design of our protocol strongly protects the anonymity of payers with respect to their payment transactions, while preventing the creation of assets by any party other than the original issuer without destroying assets of equal value.</li>
</ul>

<h3>Title: MetaFood3D: Large 3D Food Object Dataset with Nutrition Values</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Chen, Jiangpeng He, Chris Czarnecki, Gautham Vinod, Talha Ibn Mahmud, Siddeshwar Raghavan, Jinge Ma, Dayou Mao, Saeejith Nair, Pengcheng Xi, Alexander Wong, Edward Delp, Fengqing Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01966">https://arxiv.org/abs/2409.01966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01966">https://arxiv.org/pdf/2409.01966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01966]] MetaFood3D: Large 3D Food Object Dataset with Nutrition Values(https://arxiv.org/abs/2409.01966)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Food computing is both important and challenging in computer vision (CV). It significantly contributes to the development of CV algorithms due to its frequent presence in datasets across various applications, ranging from classification and instance segmentation to 3D reconstruction. The polymorphic shapes and textures of food, coupled with high variation in forms and vast multimodal information, including language descriptions and nutritional data, make food computing a complex and demanding task for modern CV algorithms. 3D food modeling is a new frontier for addressing food-related problems, due to its inherent capability to deal with random camera views and its straightforward representation for calculating food portion size. However, the primary hurdle in the development of algorithms for food object analysis is the lack of nutrition values in existing 3D datasets. Moreover, in the broader field of 3D research, there is a critical need for domain-specific test datasets. To bridge the gap between general 3D vision and food computing research, we propose MetaFood3D. This dataset consists of 637 meticulously labeled 3D food objects across 108 categories, featuring detailed nutrition information, weight, and food codes linked to a comprehensive nutrition database. The dataset emphasizes intra-class diversity and includes rich modalities such as textured mesh files, RGB-D videos, and segmentation masks. Experimental results demonstrate our dataset's significant potential for improving algorithm performance, highlight the challenging gap between video captures and 3D scanned data, and show the strength of the MetaFood3D dataset in high-quality data generation, simulation, and augmentation.</li>
</ul>

<h3>Title: Snapshot: Towards Application-centered Models for Pedestrian Trajectory Prediction in Urban Traffic Environments</h3>
<ul>
<li><strong>Authors: </strong>Nico Uhlemann, Yipeng Zhou, Tobias Mohr, Markus Lienkamp</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01971">https://arxiv.org/abs/2409.01971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01971">https://arxiv.org/pdf/2409.01971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01971]] Snapshot: Towards Application-centered Models for Pedestrian Trajectory Prediction in Urban Traffic Environments(https://arxiv.org/abs/2409.01971)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper explores pedestrian trajectory prediction in urban traffic while focusing on both model accuracy and real-world applicability. While promising approaches exist, they are often not publicly available, revolve around pedestrian datasets excluding traffic-related information, or resemble architectures that are either not real-time capable or robust. To address these limitations, we first introduce a dedicated benchmark based on Argoverse 2, specifically targeting pedestrians in urban settings. Following this, we present Snapshot, a modular, feed-forward neural network that outperforms the current state of the art while utilizing significantly less information. Despite its agent-centric encoding scheme, Snapshot demonstrates scalability, real-time performance, and robustness to varying motion histories. Moreover, by integrating Snapshot into a modular autonomous driving software stack, we showcase its real-world applicability</li>
</ul>

<h3>Title: 1DCNNTrans: BISINDO Sign Language Interpreters in Improving the Inclusiveness of Public Services</h3>
<ul>
<li><strong>Authors: </strong>Muchammad Daniyal Kautsar, Ridwan Akmal, Afra Majida Hariono</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01975">https://arxiv.org/abs/2409.01975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01975">https://arxiv.org/pdf/2409.01975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01975]] 1DCNNTrans: BISINDO Sign Language Interpreters in Improving the Inclusiveness of Public Services(https://arxiv.org/abs/2409.01975)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Indonesia ranks fourth globally in the number of deaf cases. Individuals with hearing impairments often find communication challenging, necessitating the use of sign language. However, there are limited public services that offer such inclusivity. On the other hand, advancements in artificial intelligence (AI) present promising solutions to overcome communication barriers faced by the deaf. This study aims to explore the application of AI in developing models for a simplified sign language translation app and dictionary, designed for integration into public service facilities, to facilitate communication for individuals with hearing impairments, thereby enhancing inclusivity in public services. The researchers compared the performance of LSTM and 1D CNN + Transformer (1DCNNTrans) models for sign language recognition. Through rigorous testing and validation, it was found that the LSTM model achieved an accuracy of 94.67%, while the 1DCNNTrans model achieved an accuracy of 96.12%. Model performance evaluation indicated that although the LSTM exhibited lower inference latency, it showed weaknesses in classifying classes with similar keypoints. In contrast, the 1DCNNTrans model demonstrated greater stability and higher F1 scores for classes with varying levels of complexity compared to the LSTM model. Both models showed excellent performance, exceeding 90% validation accuracy and demonstrating rapid classification of 50 sign language gestures.</li>
</ul>

<h3>Title: Benchmarking ZK-Friendly Hash Functions and SNARK Proving Systems for EVM-compatible Blockchains</h3>
<ul>
<li><strong>Authors: </strong>Hanze Guo, Yebo Feng, Cong Wu, Zengpeng Li, Jiahua Xu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01976">https://arxiv.org/abs/2409.01976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01976">https://arxiv.org/pdf/2409.01976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01976]] Benchmarking ZK-Friendly Hash Functions and SNARK Proving Systems for EVM-compatible Blockchains(https://arxiv.org/abs/2409.01976)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>With the rapid development of Zero-Knowledge Proofs (ZKPs), particularly Succinct Non-Interactive Arguments of Knowledge (SNARKs), benchmarking various ZK tools has become a valuable task. ZK-friendly hash functions, as key algorithms in blockchain, have garnered significant attention. Therefore, comprehensive benchmarking and evaluations of these evolving algorithms in ZK circuits present both promising opportunities and challenges. Additionally, we focus on a popular ZKP application, privacy-preserving transaction protocols, aiming to leverage SNARKs' cost-efficiency through "batch processing" to address high on-chain costs and compliance issues. To this end, we benchmarked three SNARK proving systems and five ZK-friendly hash functions, including our self-developed circuit templates for Poseidon2, Neptune, and GMiMC, on the bn254 curve within the circom-snarkjs framework. We also introduced the role of "sequencer" in our SNARK-based privacy-preserving transaction scheme to enhance efficiency and enable flexible auditing. We conducted privacy and security analyses, as well as implementation and evaluation on Ethereum Virtual Machine (EVM)-compatible chains. The results indicate that Poseidon and Poseidon2 demonstrate superior memory usage and runtime during proof generation under Groth16. Moreover, compared to the baseline, Poseidon2 not only generates proofs faster but also reduces on-chain costs by 73% on EVM chains and nearly 26% on Hedera. Our work provides a benchmark for ZK-friendly hash functions and ZK tools, while also exploring cost efficiency and compliance in ZKP-based privacy-preserving transaction protocols.</li>
</ul>

<h3>Title: Counterfactual Fairness by Combining Factual and Counterfactual Predictions</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Zhou, Tianci Liu, Ruqi Bai, Jing Gao, Murat Kocaoglu, David I. Inouye</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01977">https://arxiv.org/abs/2409.01977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01977">https://arxiv.org/pdf/2409.01977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01977]] Counterfactual Fairness by Combining Factual and Counterfactual Predictions(https://arxiv.org/abs/2409.01977)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>In high-stake domains such as healthcare and hiring, the role of machine learning (ML) in decision-making raises significant fairness concerns. This work focuses on Counterfactual Fairness (CF), which posits that an ML model's outcome on any individual should remain unchanged if they had belonged to a different demographic group. Previous works have proposed methods that guarantee CF. Notwithstanding, their effects on the model's predictive performance remains largely unclear. To fill in this gap, we provide a theoretical study on the inherent trade-off between CF and predictive performance in a model-agnostic manner. We first propose a simple but effective method to cast an optimal but potentially unfair predictor into a fair one without losing the optimality. By analyzing its excess risk in order to achieve CF, we quantify this inherent trade-off. Further analysis on our method's performance with access to only incomplete causal knowledge is also conducted. Built upon it, we propose a performant algorithm that can be applied in such scenarios. Experiments on both synthetic and semi-synthetic datasets demonstrate the validity of our analysis and methods.</li>
</ul>

<h3>Title: Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Ruiyao Xu, Kaize Ding</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01980">https://arxiv.org/abs/2409.01980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01980">https://arxiv.org/pdf/2409.01980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01980]] Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey(https://arxiv.org/abs/2409.01980)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Detecting anomalies or out-of-distribution (OOD) samples is critical for maintaining the reliability and trustworthiness of machine learning systems. Recently, Large Language Models (LLMs) have demonstrated their effectiveness not only in natural language processing but also in broader applications due to their advanced comprehension and generative capabilities. The integration of LLMs into anomaly and OOD detection marks a significant shift from the traditional paradigm in the field. This survey focuses on the problem of anomaly and OOD detection under the context of LLMs. We propose a new taxonomy to categorize existing approaches into three classes based on the role played by LLMs. Following our proposed taxonomy, we further discuss the related work under each of the categories and finally discuss potential challenges and directions for future research in this field. We also provide an up-to-date reading list of relevant papers.</li>
</ul>

<h3>Title: QueryCheetah: Fast Automated Discovery of Attribute Inference Attacks Against Query-Based Systems</h3>
<ul>
<li><strong>Authors: </strong>Bozhidar Stevanoski, Ana-Maria Cretu, Yves-Alexandre de Montjoye</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.01992">https://arxiv.org/abs/2409.01992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.01992">https://arxiv.org/pdf/2409.01992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.01992]] QueryCheetah: Fast Automated Discovery of Attribute Inference Attacks Against Query-Based Systems(https://arxiv.org/abs/2409.01992)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense, attack</a></li>
<li><strong>Abstract: </strong>Query-based systems (QBSs) are one of the key approaches for sharing data. QBSs allow analysts to request aggregate information from a private protected dataset. Attacks are a crucial part of ensuring QBSs are truly privacy-preserving. The development and testing of attacks is however very labor-intensive and unable to cope with the increasing complexity of systems. Automated approaches have been shown to be promising but are currently extremely computationally intensive, limiting their applicability in practice. We here propose QueryCheetah, a fast and effective method for automated discovery of privacy attacks against QBSs. We instantiate QueryCheetah on attribute inference attacks and show it to discover stronger attacks than previous methods while being 18 times faster than the state-of-the-art automated approach. We then show how QueryCheetah allows system developers to thoroughly evaluate the privacy risk, including for various attacker strengths and target individuals. We finally show how QueryCheetah can be used out-of-the-box to find attacks in larger syntaxes and workarounds around ad-hoc defenses.</li>
</ul>

<h3>Title: Robust Fitting on a Gate Quantum Computer</h3>
<ul>
<li><strong>Authors: </strong>Frances Fengyi Yang, Michele Sasdelli, Tat-Jun Chin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02006">https://arxiv.org/abs/2409.02006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02006">https://arxiv.org/pdf/2409.02006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02006]] Robust Fitting on a Gate Quantum Computer(https://arxiv.org/abs/2409.02006)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Gate quantum computers generate significant interest due to their potential to solve certain difficult problems such as prime factorization in polynomial time. Computer vision researchers have long been attracted to the power of quantum computers. Robust fitting, which is fundamentally important to many computer vision pipelines, has recently been shown to be amenable to gate quantum computing. The previous proposed solution was to compute Boolean influence as a measure of outlyingness using the Bernstein-Vazirani quantum circuit. However, the method assumed a quantum implementation of an $\ell_\infty$ feasibility test, which has not been demonstrated. In this paper, we take a big stride towards quantum robust fitting: we propose a quantum circuit to solve the $\ell_\infty$ feasibility test in the 1D case, which allows to demonstrate for the first time quantum robust fitting on a real gate quantum computer, the IonQ Aria. We also show how 1D Boolean influences can be accumulated to compute Boolean influences for higher-dimensional non-linear models, which we experimentally validate on real benchmark datasets.</li>
</ul>

<h3>Title: PMT-MAE: Dual-Branch Self-Supervised Learning with Distillation for Efficient Point Cloud Classification</h3>
<ul>
<li><strong>Authors: </strong>Qiang Zheng, Chao Zhang, Jian Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02007">https://arxiv.org/abs/2409.02007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02007">https://arxiv.org/pdf/2409.02007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02007]] PMT-MAE: Dual-Branch Self-Supervised Learning with Distillation for Efficient Point Cloud Classification(https://arxiv.org/abs/2409.02007)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Advances in self-supervised learning are essential for enhancing feature extraction and understanding in point cloud processing. This paper introduces PMT-MAE (Point MLP-Transformer Masked Autoencoder), a novel self-supervised learning framework for point cloud classification. PMT-MAE features a dual-branch architecture that integrates Transformer and MLP components to capture rich features. The Transformer branch leverages global self-attention for intricate feature interactions, while the parallel MLP branch processes tokens through shared fully connected layers, offering a complementary feature transformation pathway. A fusion mechanism then combines these features, enhancing the model's capacity to learn comprehensive 3D representations. Guided by the sophisticated teacher model Point-M2AE, PMT-MAE employs a distillation strategy that includes feature distillation during pre-training and logit distillation during fine-tuning, ensuring effective knowledge transfer. On the ModelNet40 classification task, achieving an accuracy of 93.6\% without employing voting strategy, PMT-MAE surpasses the baseline Point-MAE (93.2\%) and the teacher Point-M2AE (93.4\%), underscoring its ability to learn discriminative 3D point cloud representations. Additionally, this framework demonstrates high efficiency, requiring only 40 epochs for both pre-training and fine-tuning. PMT-MAE's effectiveness and efficiency render it well-suited for scenarios with limited computational resources, positioning it as a promising solution for practical point cloud analysis.</li>
</ul>

<h3>Title: Deep learning for objective estimation of Parkinsonian tremor severity</h3>
<ul>
<li><strong>Authors: </strong>Felipe Duque-Quiceno, Grzegorz Sarapata, Yuriy Dushin, Miles Allen, Jonathan O'Keeffe</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02011">https://arxiv.org/abs/2409.02011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02011">https://arxiv.org/pdf/2409.02011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02011]] Deep learning for objective estimation of Parkinsonian tremor severity(https://arxiv.org/abs/2409.02011)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate assessment of Parkinsonian tremor is vital for monitoring disease progression and evaluating treatment efficacy. We introduce a pixel-based deep learning model designed to analyse postural tremor in Parkinson's disease (PD) from video data, overcoming the limitations of traditional pose estimation techniques. Trained on 2,742 assessments from five specialised movement disorder centres across two continents, the model demonstrated robust concordance with clinical evaluations. It effectively predicted treatment effects for levodopa and deep brain stimulation (DBS), detected lateral asymmetry of symptoms, and differentiated between different tremor severities. Feature space analysis revealed a non-linear, structured distribution of tremor severity, with low-severity scores occupying a larger portion of the feature space. The model also effectively identified outlier videos, suggesting its potential for adaptive learning and quality control in clinical settings. Our approach offers a scalable and objective method for tremor scoring, with potential integration into other MDS-UPDRS motor assessments, including bradykinesia and gait. The system's adaptability and performance underscore its promise for high-frequency, longitudinal monitoring of PD symptoms, complementing clinical expertise and enhancing decision-making in patient management. Future work will extend this pixel-based methodology to other cardinal symptoms of PD, aiming to develop a comprehensive, multi-symptom model for automated Parkinson's disease severity assessment.</li>
</ul>

<h3>Title: TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Bobby Azad, Pourya Adibfar, Kaiqun Fu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02018">https://arxiv.org/abs/2409.02018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02018">https://arxiv.org/pdf/2409.02018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02018]] TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation(https://arxiv.org/abs/2409.02018)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>In healthcare, medical image segmentation is crucial for accurate disease diagnosis and the development of effective treatment strategies. Early detection can significantly aid in managing diseases and potentially prevent their progression. Machine learning, particularly deep convolutional neural networks, has emerged as a promising approach to addressing segmentation challenges. Traditional methods like U-Net use encoding blocks for local representation modeling and decoding blocks to uncover semantic relationships. However, these models often struggle with multi-scale objects exhibiting significant variations in texture and shape, and they frequently fail to capture long-range dependencies in the input data. Transformers designed for sequence-to-sequence predictions have been proposed as alternatives, utilizing global self-attention mechanisms. Yet, they can sometimes lack precise localization due to insufficient granular details. To overcome these limitations, we introduce TransDAE: a novel approach that reimagines the self-attention mechanism to include both spatial and channel-wise associations across the entire feature space, while maintaining computational efficiency. Additionally, TransDAE enhances the skip connection pathway with an inter-scale interaction module, promoting feature reuse and improving localization accuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on the Synaps multi-organ dataset, even without relying on pre-trained weights.</li>
</ul>

<h3>Title: Foundations of Large Language Model Compression -- Part 1: Weight Quantization</h3>
<ul>
<li><strong>Authors: </strong>Sean I. Young</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02026">https://arxiv.org/abs/2409.02026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02026">https://arxiv.org/pdf/2409.02026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02026]] Foundations of Large Language Model Compression -- Part 1: Weight Quantization(https://arxiv.org/abs/2409.02026)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, compression of large language models (LLMs) has emerged as an important problem to allow language model deployment on resource-constrained devices, reduce computational costs, and mitigate the environmental footprint of large-scale AI infrastructure. In this paper, we present the foundations of LLM quantization from a convex optimization perspective and propose a quantization method that builds on these foundations and outperforms previous methods. Our quantization framework, CVXQ, scales to models containing hundreds of billions of weight parameters and provides users with the flexibility to compress models to any specified model size, post-training. A reference implementation of CVXQ can be obtained from this https URL.</li>
</ul>

<h3>Title: AllWeatherNet:Unified Image enhancement for autonomous driving under adverse weather and lowlight-conditions</h3>
<ul>
<li><strong>Authors: </strong>Chenghao Qian, Mahdi Rezaei, Saeed Anwar, Wenjing Li, Tanveer Hussain, Mohsen Azarmi, Wei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02045">https://arxiv.org/abs/2409.02045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02045">https://arxiv.org/pdf/2409.02045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02045]] AllWeatherNet:Unified Image enhancement for autonomous driving under adverse weather and lowlight-conditions(https://arxiv.org/abs/2409.02045)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Adverse conditions like snow, rain, nighttime, and fog, pose challenges for autonomous driving perception systems. Existing methods have limited effectiveness in improving essential computer vision tasks, such as semantic segmentation, and often focus on only one specific condition, such as removing rain or translating nighttime images into daytime ones. To address these limitations, we propose a method to improve the visual quality and clarity degraded by such adverse conditions. Our method, AllWeather-Net, utilizes a novel hierarchical architecture to enhance images across all adverse conditions. This architecture incorporates information at three semantic levels: scene, object, and texture, by discriminating patches at each level. Furthermore, we introduce a Scaled Illumination-aware Attention Mechanism (SIAM) that guides the learning towards road elements critical for autonomous driving perception. SIAM exhibits robustness, remaining unaffected by changes in weather conditions or environmental scenes. AllWeather-Net effectively transforms images into normal weather and daytime scenes, demonstrating superior image enhancement results and subsequently enhancing the performance of semantic segmentation, with up to a 5.3% improvement in mIoU in the trained domain. We also show our model's generalization ability by applying it to unseen domains without re-training, achieving up to 3.9% mIoU improvement. Code can be accessed at: this https URL.</li>
</ul>

<h3>Title: ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, Yonghong Tian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02048">https://arxiv.org/abs/2409.02048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02048">https://arxiv.org/pdf/2409.02048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02048]] ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis(https://arxiv.org/abs/2409.02048)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite recent advancements in neural 3D reconstruction, the dependence on dense multi-view captures restricts their broader applicability. In this work, we propose \textbf{ViewCrafter}, a novel method for synthesizing high-fidelity novel views of generic scenes from single or sparse images with the prior of video diffusion model. Our method takes advantage of the powerful generation capabilities of video diffusion model and the coarse 3D clues offered by point-based representation to generate high-quality video frames with precise camera pose control. To further enlarge the generation range of novel views, we tailored an iterative view synthesis strategy together with a camera trajectory planning algorithm to progressively extend the 3D clues and the areas covered by the novel views. With ViewCrafter, we can facilitate various applications, such as immersive experiences with real-time rendering by efficiently optimizing a 3D-GS representation using the reconstructed 3D points and the generated novel views, and scene-level text-to-3D generation for more imaginative content creation. Extensive experiments on diverse datasets demonstrate the strong generalization capability and superior performance of our method in synthesizing high-fidelity and consistent novel views.</li>
</ul>

<h3>Title: Enhancing Code-Switching Speech Recognition with LID-Based Collaborative Mixture of Experts Model</h3>
<ul>
<li><strong>Authors: </strong>Hukai Huang, Jiayan Lin, Kaidi Wang, Yishuang Li, Wenhao Guan, Qingyang Hong, Lin Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02050">https://arxiv.org/abs/2409.02050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02050">https://arxiv.org/pdf/2409.02050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02050]] Enhancing Code-Switching Speech Recognition with LID-Based Collaborative Mixture of Experts Model(https://arxiv.org/abs/2409.02050)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Due to the inherent difficulty in modeling phonetic similarities across different languages, code-switching speech recognition presents a formidable challenge. This study proposes a Collaborative-MoE, a Mixture of Experts (MoE) model that leverages a collaborative mechanism among expert groups. Initially, a preceding routing network explicitly learns Language Identification (LID) tasks and selects experts based on acquired LID weights. This process ensures robust routing information to the MoE layer, mitigating interference from diverse language domains on expert network parameter updates. The LID weights are also employed to facilitate inter-group collaboration, enabling the integration of language-specific representations. Furthermore, within each language expert group, a gating network operates unsupervised to foster collaboration on attributes beyond language. Extensive experiments demonstrate the efficacy of our approach, achieving significant performance enhancements compared to alternative methods. Importantly, our method preserves the efficient inference capabilities characteristic of MoE models without necessitating additional pre-training.</li>
</ul>

<h3>Title: Robust Fourier Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Halyun Jeong, Jihun Han</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02052">https://arxiv.org/abs/2409.02052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02052">https://arxiv.org/pdf/2409.02052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02052]] Robust Fourier Neural Networks(https://arxiv.org/abs/2409.02052)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Fourier embedding has shown great promise in removing spectral bias during neural network training. However, it can still suffer from high generalization errors, especially when the labels or measurements are noisy. We demonstrate that introducing a simple diagonal layer after the Fourier embedding layer makes the network more robust to measurement noise, effectively prompting it to learn sparse Fourier features. We provide theoretical justifications for this Fourier feature learning, leveraging recent developments in diagonal networks and implicit regularization in neural networks. Under certain conditions, our proposed approach can also learn functions that are noisy mixtures of nonlinear functions of Fourier features. Numerical experiments validate the effectiveness of our proposed architecture, supporting our theory.</li>
</ul>

<h3>Title: F2former: When Fractional Fourier Meets Deep Wiener Deconvolution and Selective Frequency Transformer for Image Deblurring</h3>
<ul>
<li><strong>Authors: </strong>Subhajit Paul, Sahil Kumawat, Ashutosh Gupta, Deepak Mishra</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02056">https://arxiv.org/abs/2409.02056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02056">https://arxiv.org/pdf/2409.02056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02056]] F2former: When Fractional Fourier Meets Deep Wiener Deconvolution and Selective Frequency Transformer for Image Deblurring(https://arxiv.org/abs/2409.02056)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent progress in image deblurring techniques focuses mainly on operating in both frequency and spatial domains using the Fourier transform (FT) properties. However, their performance is limited due to the dependency of FT on stationary signals and its lack of capability to extract spatial-frequency properties. In this paper, we propose a novel approach based on the Fractional Fourier Transform (FRFT), a unified spatial-frequency representation leveraging both spatial and frequency components simultaneously, making it ideal for processing non-stationary signals like images. Specifically, we introduce a Fractional Fourier Transformer (F2former), where we combine the classical fractional Fourier based Wiener deconvolution (F2WD) as well as a multi-branch encoder-decoder transformer based on a new fractional frequency aware transformer block (F2TB). We design F2TB consisting of a fractional frequency aware self-attention (F2SA) to estimate element-wise product attention based on important frequency components and a novel feed-forward network based on frequency division multiplexing (FM-FFN) to refine high and low frequency features separately for efficient latent clear image restoration. Experimental results for the cases of both motion deblurring as well as defocus deblurring show that the performance of our proposed method is superior to other state-of-the-art (SOTA) approaches.</li>
</ul>

<h3>Title: Personalized Federated Learning via Active Sampling</h3>
<ul>
<li><strong>Authors: </strong>Alexander Jung, Yasmin SarcheshmehPour, Amirhossein Mohammadi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02064">https://arxiv.org/abs/2409.02064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02064">https://arxiv.org/pdf/2409.02064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02064]] Personalized Federated Learning via Active Sampling(https://arxiv.org/abs/2409.02064)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Consider a collection of data generators which could represent, e.g., humans equipped with a smart-phone or wearables. We want to train a personalized (or tailored) model for each data generator even if they provide only small local datasets. The available local datasets might fail to provide sufficient statistical power to train high-dimensional models (such as deep neural networks) effectively. One possible solution is to identify similar data generators and pool their local datasets to obtain a sufficiently large training set. This paper proposes a novel method for sequentially identifying similar (or relevant) data generators. Our method is similar in spirit to active sampling methods but does not require exchange of raw data. Indeed, our method evaluates the relevance of a data generator by evaluating the effect of a gradient step using its local dataset. This evaluation can be performed in a privacy-friendly fashion without sharing raw data. We extend this method to non-parametric models by a suitable generalization of the gradient step to update a hypothesis using the local dataset provided by a data generator.</li>
</ul>

<h3>Title: Robust Clustering on High-Dimensional Data with Stochastic Quantization</h3>
<ul>
<li><strong>Authors: </strong>Vladimir Norkin, Anton Kozyriev</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02066">https://arxiv.org/abs/2409.02066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02066">https://arxiv.org/pdf/2409.02066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02066]] Robust Clustering on High-Dimensional Data with Stochastic Quantization(https://arxiv.org/abs/2409.02066)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper addresses the limitations of traditional vector quantization (clustering) algorithms, particularly K-Means and its variant K-Means++, and explores the Stochastic Quantization (SQ) algorithm as a scalable alternative for high-dimensional unsupervised and semi-supervised learning problems. Some traditional clustering algorithms suffer from inefficient memory utilization during computation, necessitating the loading of all data samples into memory, which becomes impractical for large-scale datasets. While variants such as Mini-Batch K-Means partially mitigate this issue by reducing memory usage, they lack robust theoretical convergence guarantees due to the non-convex nature of clustering problems. In contrast, the Stochastic Quantization algorithm provides strong theoretical convergence guarantees, making it a robust alternative for clustering tasks. We demonstrate the computational efficiency and rapid convergence of the algorithm on an image classification problem with partially labeled data, comparing model accuracy across various ratios of labeled to unlabeled data. To address the challenge of high dimensionality, we trained Triplet Network to encode images into low-dimensional representations in a latent space, which serve as a basis for comparing the efficiency of both the Stochastic Quantization algorithm and traditional quantization algorithms. Furthermore, we enhance the algorithm's convergence speed by introducing modifications with an adaptive learning rate.</li>
</ul>

<h3>Title: RACONTEUR: A Knowledgeable, Insightful, and Portable LLM-Powered Shell Command Explainer</h3>
<ul>
<li><strong>Authors: </strong>Jiangyi Deng (1), Xinfeng Li (1), Yanjiao Chen (1), Yijie Bai (1), Haiqin Weng (2), Yan Liu (2), Tao Wei (2), Wenyuan Xu (1) ((1) Zhejiang University, (2) Ant Group)</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.HC, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02074">https://arxiv.org/abs/2409.02074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02074">https://arxiv.org/pdf/2409.02074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02074]] RACONTEUR: A Knowledgeable, Insightful, and Portable LLM-Powered Shell Command Explainer(https://arxiv.org/abs/2409.02074)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Malicious shell commands are linchpins to many cyber-attacks, but may not be easy to understand by security analysts due to complicated and often disguised code structures. Advances in large language models (LLMs) have unlocked the possibility of generating understandable explanations for shell commands. However, existing general-purpose LLMs suffer from a lack of expert knowledge and a tendency to hallucinate in the task of shell command explanation. In this paper, we present Raconteur, a knowledgeable, expressive and portable shell command explainer powered by LLM. Raconteur is infused with professional knowledge to provide comprehensive explanations on shell commands, including not only what the command does (i.e., behavior) but also why the command does it (i.e., purpose). To shed light on the high-level intent of the command, we also translate the natural-language-based explanation into standard technique & tactic defined by MITRE ATT&CK, the worldwide knowledge base of cybersecurity. To enable Raconteur to explain unseen private commands, we further develop a documentation retriever to obtain relevant information from complementary documentations to assist the explanation process. We have created a large-scale dataset for training and conducted extensive experiments to evaluate the capability of Raconteur in shell command explanation. The experiments verify that Raconteur is able to provide high-quality explanations and in-depth insight of the intent of the command.</li>
</ul>

<h3>Title: Political DEBATE: Efficient Zero-shot and Few-shot Classifiers for Political Text</h3>
<ul>
<li><strong>Authors: </strong>Michael Burnham, Kayla Kahn, Ryan Yank Wang, Rachel X. Peng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02078">https://arxiv.org/abs/2409.02078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02078">https://arxiv.org/pdf/2409.02078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02078]] Political DEBATE: Efficient Zero-shot and Few-shot Classifiers for Political Text(https://arxiv.org/abs/2409.02078)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Social scientists quickly adopted large language models due to their ability to annotate documents without supervised training, an ability known as zero-shot learning. However, due to their compute demands, cost, and often proprietary nature, these models are often at odds with replication and open science standards. This paper introduces the Political DEBATE (DeBERTa Algorithm for Textual Entailment) language models for zero-shot and few-shot classification of political documents. These models are not only as good, or better than, state-of-the art large language models at zero and few-shot classification, but are orders of magnitude more efficient and completely open source. By training the models on a simple random sample of 10-25 documents, they can outperform supervised classifiers trained on hundreds or thousands of documents and state-of-the-art generative models with complex, engineered prompts. Additionally, we release the PolNLI dataset used to train these models -- a corpus of over 200,000 political documents with highly accurate labels across over 800 classification tasks.</li>
</ul>

<h3>Title: Physical Rule-Guided Convolutional Neural Network</h3>
<ul>
<li><strong>Authors: </strong>Kishor Datta Gupta, Marufa Kamal, Rakib Hossain Rifat, Mohd Ariful Haque, Roy George</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02081">https://arxiv.org/abs/2409.02081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02081">https://arxiv.org/pdf/2409.02081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02081]] Physical Rule-Guided Convolutional Neural Network(https://arxiv.org/abs/2409.02081)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The black-box nature of Convolutional Neural Networks (CNNs) and their reliance on large datasets limit their use in complex domains with limited labeled data. Physics-Guided Neural Networks (PGNNs) have emerged to address these limitations by integrating scientific principles and real-world knowledge, enhancing model interpretability and efficiency. This paper proposes a novel Physics-Guided CNN (PGCNN) architecture that incorporates dynamic, trainable, and automated LLM-generated, widely recognized rules integrated into the model as custom layers to address challenges like limited data and low confidence scores. The PGCNN is evaluated on multiple datasets, demonstrating superior performance compared to a baseline CNN model. Key improvements include a significant reduction in false positives and enhanced confidence scores for true detection. The results highlight the potential of PGCNNs to improve CNN performance for broader application areas.</li>
</ul>

<h3>Title: DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos</h3>
<ul>
<li><strong>Authors: </strong>Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, Ying Shan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02095">https://arxiv.org/abs/2409.02095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02095">https://arxiv.org/pdf/2409.02095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02095]] DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos(https://arxiv.org/abs/2409.02095)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite significant advancements in monocular depth estimation for static images, estimating video depth in the open world remains challenging, since open-world videos are extremely diverse in content, motion, camera movement, and length. We present DepthCrafter, an innovative method for generating temporally consistent long depth sequences with intricate details for open-world videos, without requiring any supplementary information such as camera poses or optical flow. DepthCrafter achieves generalization ability to open-world videos by training a video-to-depth model from a pre-trained image-to-video diffusion model, through our meticulously designed three-stage training strategy with the compiled paired video-depth datasets. Our training approach enables the model to generate depth sequences with variable lengths at one time, up to 110 frames, and harvest both precise depth details and rich content diversity from realistic and synthetic datasets. We also propose an inference strategy that processes extremely long videos through segment-wise estimation and seamless stitching. Comprehensive evaluations on multiple datasets reveal that DepthCrafter achieves state-of-the-art performance in open-world video depth estimation under zero-shot settings. Furthermore, DepthCrafter facilitates various downstream applications, including depth-based visual effects and conditional video generation.</li>
</ul>

<h3>Title: LinFusion: 1 GPU, 1 Minute, 16K Image</h3>
<ul>
<li><strong>Authors: </strong>Songhua Liu, Weihao Yu, Zhenxiong Tan, Xinchao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02097">https://arxiv.org/abs/2409.02097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02097">https://arxiv.org/pdf/2409.02097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02097]] LinFusion: 1 GPU, 1 Minute, 16K Image(https://arxiv.org/abs/2409.02097)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Modern diffusion models, particularly those utilizing a Transformer-based UNet for denoising, rely heavily on self-attention operations to manage complex spatial relationships, thus achieving impressive generation performance. However, this existing paradigm faces significant challenges in generating high-resolution visual content due to its quadratic time and memory complexity with respect to the number of spatial tokens. To address this limitation, we aim at a novel linear attention mechanism as an alternative in this paper. Specifically, we begin our exploration from recently introduced models with linear complexity, e.g., Mamba, Mamba2, and Gated Linear Attention, and identify two key features-attention normalization and non-causal inference-that enhance high-resolution visual generation performance. Building on these insights, we introduce a generalized linear attention paradigm, which serves as a low-rank approximation of a wide spectrum of popular linear token mixers. To save the training cost and better leverage pre-trained models, we initialize our models and distill the knowledge from pre-trained StableDiffusion (SD). We find that the distilled model, termed LinFusion, achieves performance on par with or superior to the original SD after only modest training, while significantly reducing time and memory complexity. Extensive experiments on SD-v1.5, SD-v2.1, and SD-XL demonstrate that LinFusion delivers satisfactory zero-shot cross-resolution generation performance, generating high-resolution images like 16K resolution. Moreover, it is highly compatible with pre-trained SD components, such as ControlNet and IP-Adapter, requiring no adaptation efforts. Codes are available at this https URL.</li>
</ul>

<h3>Title: CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Ingo Ziegler, Abdullatif K√∂ksal, Desmond Elliott, Hinrich Sch√ºtze</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02098">https://arxiv.org/abs/2409.02098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02098">https://arxiv.org/pdf/2409.02098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02098]] CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation(https://arxiv.org/abs/2409.02098)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Building high-quality datasets for specialized tasks is a time-consuming and resource-intensive process that often requires specialized domain knowledge. We propose Corpus Retrieval and Augmentation for Fine-Tuning (CRAFT), a method for generating synthetic datasets, given a small number of user-written few-shots that demonstrate the task to be performed. Given the few-shot examples, we use large-scale public web-crawled corpora and similarity-based document retrieval to find other relevant human-written documents. Lastly, instruction-tuned large language models (LLMs) augment the retrieved documents into custom-formatted task samples, which then can be used for fine-tuning. We demonstrate that CRAFT can efficiently generate large-scale task-specific training datasets for four diverse tasks: biology question-answering (QA), medicine QA and commonsense QA as well as summarization. Our experiments show that CRAFT-based models outperform or achieve comparable performance to general LLMs for QA tasks, while CRAFT-based summarization models outperform models trained on human-curated data by 46 preference points.</li>
</ul>

<h3>Title: DynOMo: Online Point Tracking by Dynamic Online Monocular Gaussian Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Jenny Seidenschwarz, Qunjie Zhou, Bardienus Duisterhof, Deva Ramanan, Laura Leal-Taix√©</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02104">https://arxiv.org/abs/2409.02104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02104">https://arxiv.org/pdf/2409.02104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02104]] DynOMo: Online Point Tracking by Dynamic Online Monocular Gaussian Reconstruction(https://arxiv.org/abs/2409.02104)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reconstructing scenes and tracking motion are two sides of the same coin. Tracking points allow for geometric reconstruction [14], while geometric reconstruction of (dynamic) scenes allows for 3D tracking of points over time [24, 39]. The latter was recently also exploited for 2D point tracking to overcome occlusion ambiguities by lifting tracking directly into 3D [38]. However, above approaches either require offline processing or multi-view camera setups both unrealistic for real-world applications like robot navigation or mixed reality. We target the challenge of online 2D and 3D point tracking from unposed monocular camera input introducing Dynamic Online Monocular Reconstruction (DynOMo). We leverage 3D Gaussian splatting to reconstruct dynamic scenes in an online fashion. Our approach extends 3D Gaussians to capture new content and object motions while estimating camera movements from a single RGB frame. DynOMo stands out by enabling emergence of point trajectories through robust image feature reconstruction and a novel similarity-enhanced regularization term, without requiring any correspondence-level supervision. It sets the first baseline for online point tracking with monocular unposed cameras, achieving performance on par with existing methods. We aim to inspire the community to advance online point tracking and reconstruction, expanding the applicability to diverse real-world scenarios.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
