<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: Autonomous and Collaborative Smart Home Security System (ACSHSS). (arXiv:2309.02899v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02899">http://arxiv.org/abs/2309.02899</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02899]] Autonomous and Collaborative Smart Home Security System (ACSHSS)(http://arxiv.org/abs/2309.02899)</code></li>
<li>Summary: <p>Firstly, the proposed solution provides remotely accessible integrated IoT
resources for the safety and security of the building. By using Sha ort
Messaging System (SMS), the age is sent to the user by the Global System for
Mobile (GSM) system. An SMS alert is sent to the user in case any sensor
detects an abnormality in their operation. Secondly, an authentication
mechanism is deployed to enable only authorized users to access resources.
Thirdly, in case of a malicious approach in accessing IoT resources, a timely
alert should be received by the owner. A Network Intrusion Detection System
(NIDS) is deployed to detect and real-time information in case of any
suspicious activity while accessing the Internet of Things network.
</p></li>
</ul>

<h3>Title: Demystifying RCE Vulnerabilities in LLM-Integrated Apps. (arXiv:2309.02926v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02926">http://arxiv.org/abs/2309.02926</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02926]] Demystifying RCE Vulnerabilities in LLM-Integrated Apps(http://arxiv.org/abs/2309.02926)</code></li>
<li>Summary: <p>In recent years, Large Language Models (LLMs) have demonstrated remarkable
potential across various downstream tasks. LLM-integrated frameworks, which
serve as the essential infrastructure, have given rise to many LLM-integrated
web apps. However, some of these frameworks suffer from Remote Code Execution
(RCE) vulnerabilities, allowing attackers to execute arbitrary code on apps'
servers remotely via prompt injections. Despite the severity of these
vulnerabilities, no existing work has been conducted for a systematic
investigation of them. This leaves a great challenge on how to detect
vulnerabilities in frameworks as well as LLM-integrated apps in real-world
scenarios.
</p>
<p>To fill this gap, we present two novel strategies, including 1) a static
analysis-based tool called LLMSmith to scan the source code of the framework to
detect potential RCE vulnerabilities and 2) a prompt-based automated testing
approach to verify the vulnerability in LLM-integrated web apps. We discovered
13 vulnerabilities in 6 frameworks, including 12 RCE vulnerabilities and 1
arbitrary file read/write vulnerability. 11 of them are confirmed by the
framework developers, resulting in the assignment of 7 CVE IDs. After testing
51 apps, we found vulnerabilities in 17 apps, 16 of which are vulnerable to RCE
and 1 to SQL injection. We responsibly reported all 17 issues to the
corresponding developers and received acknowledgments. Furthermore, we amplify
the attack impact beyond achieving RCE by allowing attackers to exploit other
app users (e.g. app responses hijacking, user API key leakage) without direct
interaction between the attacker and the victim. Lastly, we propose some
mitigating strategies for improving the security awareness of both framework
and app developers, helping them to mitigate these risks effectively.
</p></li>
</ul>

<h3>Title: Fuzz on the Beach: Fuzzing Solana Smart Contracts. (arXiv:2309.03006v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03006">http://arxiv.org/abs/2309.03006</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03006]] Fuzz on the Beach: Fuzzing Solana Smart Contracts(http://arxiv.org/abs/2309.03006)</code></li>
<li>Summary: <p>Solana has quickly emerged as a popular platform for building decentralized
applications (DApps), such as marketplaces for non-fungible tokens (NFTs). A
key reason for its success are Solana's low transaction fees and high
performance, which is achieved in part due to its stateless programming model.
Although the literature features extensive tooling support for smart contract
security, current solutions are largely tailored for the Ethereum Virtual
Machine. Unfortunately, the very stateless nature of Solana's execution
environment introduces novel attack patterns specific to Solana requiring a
rethinking for building vulnerability analysis methods.
</p>
<p>In this paper, we address this gap and propose FuzzDelSol, the first
binary-only coverage-guided fuzzing architecture for Solana smart contracts.
FuzzDelSol faithfully models runtime specifics such as smart contract
interactions. Moreover, since source code is not available for the large
majority of Solana contracts, FuzzDelSol operates on the contract's binary
code. Hence, due to the lack of semantic information, we carefully extracted
low-level program and state information to develop a diverse set of bug oracles
covering all major bug classes in Solana. Our extensive evaluation on 6049
smart contracts shows that FuzzDelSol's bug oracles find bugs with a high
precision and recall. To the best of our knowledge, this is the largest
evaluation of the security landscape on the Solana mainnet.
</p></li>
</ul>

<h3>Title: Automated CVE Analysis for Threat Prioritization and Impact Prediction. (arXiv:2309.03040v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03040">http://arxiv.org/abs/2309.03040</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03040]] Automated CVE Analysis for Threat Prioritization and Impact Prediction(http://arxiv.org/abs/2309.03040)</code></li>
<li>Summary: <p>The Common Vulnerabilities and Exposures (CVE) are pivotal information for
proactive cybersecurity measures, including service patching, security
hardening, and more. However, CVEs typically offer low-level, product-oriented
descriptions of publicly disclosed cybersecurity vulnerabilities, often lacking
the essential attack semantic information required for comprehensive weakness
characterization and threat impact estimation. This critical insight is
essential for CVE prioritization and the identification of potential
countermeasures, particularly when dealing with a large number of CVEs. Current
industry practices involve manual evaluation of CVEs to assess their attack
severities using the Common Vulnerability Scoring System (CVSS) and mapping
them to Common Weakness Enumeration (CWE) for potential mitigation
identification. Unfortunately, this manual analysis presents a major bottleneck
in the vulnerability analysis process, leading to slowdowns in proactive
cybersecurity efforts and the potential for inaccuracies due to human errors.
In this research, we introduce our novel predictive model and tool (called
CVEDrill) which revolutionizes CVE analysis and threat prioritization. CVEDrill
accurately estimates the CVSS vector for precise threat mitigation and priority
ranking and seamlessly automates the classification of CVEs into the
appropriate CWE hierarchy classes. By harnessing CVEDrill, organizations can
now implement cybersecurity countermeasure mitigation with unparalleled
accuracy and timeliness, surpassing in this domain the capabilities of
state-of-the-art tools like ChaptGPT.
</p></li>
</ul>

<h3>Title: Provably Unlinkable Smart Card-based Payments. (arXiv:2309.03128v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03128">http://arxiv.org/abs/2309.03128</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03128]] Provably Unlinkable Smart Card-based Payments(http://arxiv.org/abs/2309.03128)</code></li>
<li>Summary: <p>The most prevalent smart card-based payment method, EMV, currently offers no
privacy to its users. Transaction details and the card number are sent in
cleartext, enabling the profiling and tracking of cardholders. Since public
awareness of privacy issues is growing and legislation, such as GDPR, is
emerging, we believe it is necessary to investigate the possibility of making
payments anonymous and unlinkable without compromising essential security
guarantees and functional properties of EMV. This paper draws attention to
trade-offs between functional and privacy requirements in the design of such a
protocol. We present the UTX protocol - an enhanced payment protocol satisfying
such requirements, and we formally certify key security and privacy properties
using techniques based on the applied pi-calculus.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: SlAction: Non-intrusive, Lightweight Obstructive Sleep Apnea Detection using Infrared Video. (arXiv:2309.02713v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02713">http://arxiv.org/abs/2309.02713</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02713]] SlAction: Non-intrusive, Lightweight Obstructive Sleep Apnea Detection using Infrared Video(http://arxiv.org/abs/2309.02713)</code></li>
<li>Summary: <p>Obstructive sleep apnea (OSA) is a prevalent sleep disorder affecting
approximately one billion people world-wide. The current gold standard for
diagnosing OSA, Polysomnography (PSG), involves an overnight hospital stay with
multiple attached sensors, leading to potential inaccuracies due to the
first-night effect. To address this, we present SlAction, a non-intrusive OSA
detection system for daily sleep environments using infrared videos.
Recognizing that sleep videos exhibit minimal motion, this work investigates
the fundamental question: "Are respiratory events adequately reflected in human
motions during sleep?" Analyzing the largest sleep video dataset of 5,098
hours, we establish correlations between OSA events and human motions during
sleep. Our approach uses a low frame rate (2.5 FPS), a large size (60 seconds)
and step (30 seconds) for sliding window analysis to capture slow and long-term
motions related to OSA. Furthermore, we utilize a lightweight deep neural
network for resource-constrained devices, ensuring all video streams are
processed locally without compromising privacy. Evaluations show that SlAction
achieves an average F1 score of 87.6% in detecting OSA across various
environments. Implementing SlAction on NVIDIA Jetson Nano enables real-time
inference (~3 seconds for a 60-second video clip), highlighting its potential
for early detection and personalized treatment of OSA.
</p></li>
</ul>

<h3>Title: Exploring Semantic Consistency in Unpaired Image Translation to Generate Data for Surgical Applications. (arXiv:2309.03048v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03048">http://arxiv.org/abs/2309.03048</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03048]] Exploring Semantic Consistency in Unpaired Image Translation to Generate Data for Surgical Applications(http://arxiv.org/abs/2309.03048)</code></li>
<li>Summary: <p>In surgical computer vision applications, obtaining labeled training data is
challenging due to data-privacy concerns and the need for expert annotation.
Unpaired image-to-image translation techniques have been explored to
automatically generate large annotated datasets by translating synthetic images
to the realistic domain. However, preserving the structure and semantic
consistency between the input and translated images presents significant
challenges, mainly when there is a distributional mismatch in the semantic
characteristics of the domains. This study empirically investigates unpaired
image translation methods for generating suitable data in surgical
applications, explicitly focusing on semantic consistency. We extensively
evaluate various state-of-the-art image translation models on two challenging
surgical datasets and downstream semantic segmentation tasks. We find that a
simple combination of structural-similarity loss and contrastive learning
yields the most promising results. Quantitatively, we show that the data
generated with this approach yields higher semantic consistency and can be used
more effectively as training data.
</p></li>
</ul>

<h3>Title: Geometry of Sensitivity: Twice Sampling and Hybrid Clipping in Differential Privacy with Optimal Gaussian Noise and Application to Deep Learning. (arXiv:2309.02672v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02672">http://arxiv.org/abs/2309.02672</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02672]] Geometry of Sensitivity: Twice Sampling and Hybrid Clipping in Differential Privacy with Optimal Gaussian Noise and Application to Deep Learning(http://arxiv.org/abs/2309.02672)</code></li>
<li>Summary: <p>We study the fundamental problem of the construction of optimal randomization
in Differential Privacy. Depending on the clipping strategy or additional
properties of the processing function, the corresponding sensitivity set
theoretically determines the necessary randomization to produce the required
security parameters. Towards the optimal utility-privacy tradeoff, finding the
minimal perturbation for properly-selected sensitivity sets stands as a central
problem in DP research. In practice, l_2/l_1-norm clippings with
Gaussian/Laplace noise mechanisms are among the most common setups. However,
they also suffer from the curse of dimensionality. For more generic clipping
strategies, the understanding of the optimal noise for a high-dimensional
sensitivity set remains limited.
</p>
<p>In this paper, we revisit the geometry of high-dimensional sensitivity sets
and present a series of results to characterize the non-asymptotically optimal
Gaussian noise for R\'enyi DP (RDP). Our results are both negative and
positive: on one hand, we show the curse of dimensionality is tight for a broad
class of sensitivity sets satisfying certain symmetry properties; but if,
fortunately, the representation of the sensitivity set is asymmetric on some
group of orthogonal bases, we show the optimal noise bounds need not be
explicitly dependent on either dimension or rank. We also revisit sampling in
the high-dimensional scenario, which is the key for both privacy amplification
and computation efficiency in large-scale data processing. We propose a novel
method, termed twice sampling, which implements both sample-wise and
coordinate-wise sampling, to enable Gaussian noises to fit the sensitivity
geometry more closely. With closed-form RDP analysis, we prove twice sampling
produces asymptotic improvement of the privacy amplification given an
additional infinity-norm restriction, especially for small sampling rate.
</p></li>
</ul>

<h3>Title: Roulette: A Semantic Privacy-Preserving Device-Edge Collaborative Inference Framework for Deep Learning Classification Tasks. (arXiv:2309.02820v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02820">http://arxiv.org/abs/2309.02820</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02820]] Roulette: A Semantic Privacy-Preserving Device-Edge Collaborative Inference Framework for Deep Learning Classification Tasks(http://arxiv.org/abs/2309.02820)</code></li>
<li>Summary: <p>Deep learning classifiers are crucial in the age of artificial intelligence.
The device-edge-based collaborative inference has been widely adopted as an
efficient framework for promoting its applications in IoT and 5G/6G networks.
However, it suffers from accuracy degradation under non-i.i.d. data
distribution and privacy disclosure. For accuracy degradation, direct use of
transfer learning and split learning is high cost and privacy issues remain.
For privacy disclosure, cryptography-based approaches lead to a huge overhead.
Other lightweight methods assume that the ground truth is non-sensitive and can
be exposed. But for many applications, the ground truth is the user's crucial
privacy-sensitive information. In this paper, we propose a framework of
Roulette, which is a task-oriented semantic privacy-preserving collaborative
inference framework for deep learning classifiers. More than input data, we
treat the ground truth of the data as private information. We develop a novel
paradigm of split learning where the back-end DNN is frozen and the front-end
DNN is retrained to be both a feature extractor and an encryptor. Moreover, we
provide a differential privacy guarantee and analyze the hardness of ground
truth inference attacks. To validate the proposed Roulette, we conduct
extensive performance evaluations using realistic datasets, which demonstrate
that Roulette can effectively defend against various attacks and meanwhile
achieve good model accuracy. In a situation where the non-i.i.d. is very
severe, Roulette improves the inference accuracy by 21\% averaged over
benchmarks, while making the accuracy of discrimination attacks almost
equivalent to random guessing.
</p></li>
</ul>

<h3>Title: Hide and Seek (HaS): A Lightweight Framework for Prompt Privacy Protection. (arXiv:2309.03057v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03057">http://arxiv.org/abs/2309.03057</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03057]] Hide and Seek (HaS): A Lightweight Framework for Prompt Privacy Protection(http://arxiv.org/abs/2309.03057)</code></li>
<li>Summary: <p>Numerous companies have started offering services based on large language
models (LLM), such as ChatGPT, which inevitably raises privacy concerns as
users' prompts are exposed to the model provider. Previous research on secure
reasoning using multi-party computation (MPC) has proven to be impractical for
LLM applications due to its time-consuming and communication-intensive nature.
While lightweight anonymization techniques can protect private information in
prompts through substitution or masking, they fail to recover sensitive data
replaced in the LLM-generated results. In this paper, we expand the application
scenarios of anonymization techniques by training a small local model to
de-anonymize the LLM's returned results with minimal computational overhead. We
introduce the HaS framework, where "H(ide)" and "S(eek)" represent its two core
processes: hiding private entities for anonymization and seeking private
entities for de-anonymization, respectively. To quantitatively assess HaS's
privacy protection performance, we propose both black-box and white-box
adversarial models. Furthermore, we conduct experiments to evaluate HaS's
usability in translation and classification tasks. The experimental findings
demonstrate that the HaS framework achieves an optimal balance between privacy
protection and utility.
</p></li>
</ul>

<h3>Title: Blink: Link Local Differential Privacy in Graph Neural Networks via Bayesian Estimation. (arXiv:2309.03190v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03190">http://arxiv.org/abs/2309.03190</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03190]] Blink: Link Local Differential Privacy in Graph Neural Networks via Bayesian Estimation(http://arxiv.org/abs/2309.03190)</code></li>
<li>Summary: <p>Graph neural networks (GNNs) have gained an increasing amount of popularity
due to their superior capability in learning node embeddings for various graph
inference tasks, but training them can raise privacy concerns. To address this,
we propose using link local differential privacy over decentralized nodes,
enabling collaboration with an untrusted server to train GNNs without revealing
the existence of any link. Our approach spends the privacy budget separately on
links and degrees of the graph for the server to better denoise the graph
topology using Bayesian estimation, alleviating the negative impact of LDP on
the accuracy of the trained GNNs. We bound the mean absolute error of the
inferred link probabilities against the ground truth graph topology. We then
propose two variants of our LDP mechanism complementing each other in different
privacy settings, one of which estimates fewer links under lower privacy
budgets to avoid false positive link estimates when the uncertainty is high,
while the other utilizes more information and performs better given relatively
higher privacy budgets. Furthermore, we propose a hybrid variant that combines
both strategies and is able to perform better across different privacy budgets.
Extensive experiments show that our approach outperforms existing methods in
terms of accuracy under varying privacy budgets.
</p></li>
</ul>

<h3>Title: Dynamic Encoding and Decoding of Information for Split Learning in Mobile-Edge Computing: Leveraging Information Bottleneck Theory. (arXiv:2309.02787v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02787">http://arxiv.org/abs/2309.02787</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02787]] Dynamic Encoding and Decoding of Information for Split Learning in Mobile-Edge Computing: Leveraging Information Bottleneck Theory(http://arxiv.org/abs/2309.02787)</code></li>
<li>Summary: <p>Split learning is a privacy-preserving distributed learning paradigm in which
an ML model (e.g., a neural network) is split into two parts (i.e., an encoder
and a decoder). The encoder shares so-called latent representation, rather than
raw data, for model training. In mobile-edge computing, network functions (such
as traffic forecasting) can be trained via split learning where an encoder
resides in a user equipment (UE) and a decoder resides in the edge network.
Based on the data processing inequality and the information bottleneck (IB)
theory, we present a new framework and training mechanism to enable a dynamic
balancing of the transmission resource consumption with the informativeness of
the shared latent representations, which directly impacts the predictive
performance. The proposed training mechanism offers an encoder-decoder neural
network architecture featuring multiple modes of complexity-relevance
tradeoffs, enabling tunable performance. The adaptability can accommodate
varying real-time network conditions and application requirements, potentially
reducing operational expenditure and enhancing network agility. As a proof of
concept, we apply the training mechanism to a millimeter-wave (mmWave)-enabled
throughput prediction problem. We also offer new insights and highlight some
challenges related to recurrent neural networks from the perspective of the IB
theory. Interestingly, we find a compression phenomenon across the temporal
domain of the sequential model, in addition to the compression phase that
occurs with the number of training epochs.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: My Art My Choice: Adversarial Protection Against Unruly AI. (arXiv:2309.03198v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03198">http://arxiv.org/abs/2309.03198</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03198]] My Art My Choice: Adversarial Protection Against Unruly AI(http://arxiv.org/abs/2309.03198)</code></li>
<li>Summary: <p>Generative AI is on the rise, enabling everyone to produce realistic content
via publicly available interfaces. Especially for guided image generation,
diffusion models are changing the creator economy by producing high quality low
cost content. In parallel, artists are rising against unruly AI, since their
artwork are leveraged, distributed, and dissimulated by large generative
models. Our approach, My Art My Choice (MAMC), aims to empower content owners
by protecting their copyrighted materials from being utilized by diffusion
models in an adversarial fashion. MAMC learns to generate adversarially
perturbed "protected" versions of images which can in turn "break" diffusion
models. The perturbation amount is decided by the artist to balance distortion
vs. protection of the content. MAMC is designed with a simple UNet-based
generator, attacking black box diffusion models, combining several losses to
create adversarial twins of the original artwork. We experiment on three
datasets for various image-to-image tasks, with different user control values.
Both protected image and diffusion output results are evaluated in visual,
noise, structure, pixel, and generative spaces to validate our claims. We
believe that MAMC is a crucial step for preserving ownership information for AI
generated content in a flawless, based-on-need, and human-centric way.
</p></li>
</ul>

<h3>Title: ORL-AUDITOR: Dataset Auditing in Offline Deep Reinforcement Learning. (arXiv:2309.03081v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03081">http://arxiv.org/abs/2309.03081</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03081]] ORL-AUDITOR: Dataset Auditing in Offline Deep Reinforcement Learning(http://arxiv.org/abs/2309.03081)</code></li>
<li>Summary: <p>Data is a critical asset in AI, as high-quality datasets can significantly
improve the performance of machine learning models. In safety-critical domains
such as autonomous vehicles, offline deep reinforcement learning (offline DRL)
is frequently used to train models on pre-collected datasets, as opposed to
training these models by interacting with the real-world environment as the
online DRL. To support the development of these models, many institutions make
datasets publicly available with opensource licenses, but these datasets are at
risk of potential misuse or infringement. Injecting watermarks to the dataset
may protect the intellectual property of the data, but it cannot handle
datasets that have already been published and is infeasible to be altered
afterward. Other existing solutions, such as dataset inference and membership
inference, do not work well in the offline DRL scenario due to the diverse
model behavior characteristics and offline setting constraints. In this paper,
we advocate a new paradigm by leveraging the fact that cumulative rewards can
act as a unique identifier that distinguishes DRL models trained on a specific
dataset. To this end, we propose ORL-AUDITOR, which is the first
trajectory-level dataset auditing mechanism for offline RL scenarios. Our
experiments on multiple offline DRL models and tasks reveal the efficacy of
ORL-AUDITOR, with auditing accuracy over 95% and false positive rates less than
2.88%. We also provide valuable insights into the practical implementation of
ORL-AUDITOR by studying various parameter settings. Furthermore, we demonstrate
the auditing capability of ORL-AUDITOR on open-source datasets from Google and
DeepMind, highlighting its effectiveness in auditing published datasets.
ORL-AUDITOR is open-sourced at https://github.com/link-zju/ORL-Auditor.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Certifying LLM Safety against Adversarial Prompting. (arXiv:2309.02705v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02705">http://arxiv.org/abs/2309.02705</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02705]] Certifying LLM Safety against Adversarial Prompting(http://arxiv.org/abs/2309.02705)</code></li>
<li>Summary: <p>Large language models (LLMs) released for public use incorporate guardrails
to ensure their output is safe, often referred to as "model alignment." An
aligned language model should decline a user's request to produce harmful
content. However, such safety measures are vulnerable to adversarial prompts,
which contain maliciously designed token sequences to circumvent the model's
safety guards and cause it to produce harmful content. In this work, we
introduce erase-and-check, the first framework to defend against adversarial
prompts with verifiable safety guarantees. We erase tokens individually and
inspect the resulting subsequences using a safety filter. Our procedure labels
the input prompt as harmful if any subsequences or the input prompt are
detected as harmful by the filter. This guarantees that any adversarial
modification of a harmful prompt up to a certain size is also labeled harmful.
We defend against three attack modes: i) adversarial suffix, which appends an
adversarial sequence at the end of the prompt; ii) adversarial insertion, where
the adversarial sequence is inserted anywhere in the middle of the prompt; and
iii) adversarial infusion, where adversarial tokens are inserted at arbitrary
positions in the prompt, not necessarily as a contiguous block. Empirical
results demonstrate that our technique obtains strong certified safety
guarantees on harmful prompts while maintaining good performance on safe
prompts. For example, against adversarial suffixes of length 20, it certifiably
detects 93% of the harmful prompts and labels 94% of the safe prompts as safe
using the open source language model Llama 2 as the safety filter.
</p></li>
</ul>

<h3>Title: Adaptive Adversarial Training Does Not Increase Recourse Costs. (arXiv:2309.02528v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02528">http://arxiv.org/abs/2309.02528</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02528]] Adaptive Adversarial Training Does Not Increase Recourse Costs(http://arxiv.org/abs/2309.02528)</code></li>
<li>Summary: <p>Recent work has connected adversarial attack methods and algorithmic recourse
methods: both seek minimal changes to an input instance which alter a model's
classification decision. It has been shown that traditional adversarial
training, which seeks to minimize a classifier's susceptibility to malicious
perturbations, increases the cost of generated recourse; with larger
adversarial training radii correlating with higher recourse costs. From the
perspective of algorithmic recourse, however, the appropriate adversarial
training radius has always been unknown. Another recent line of work has
motivated adversarial training with adaptive training radii to address the
issue of instance-wise variable adversarial vulnerability, showing success in
domains with unknown attack radii. This work studies the effects of adaptive
adversarial training on algorithmic recourse costs. We establish that the
improvements in model robustness induced by adaptive adversarial training show
little effect on algorithmic recourse costs, providing a potential avenue for
affordable robustness in domains where recoursability is critical.
</p></li>
</ul>

<h3>Title: Mayhem: Targeted Corruption of Register and Stack Variables. (arXiv:2309.02545v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02545">http://arxiv.org/abs/2309.02545</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02545]] Mayhem: Targeted Corruption of Register and Stack Variables(http://arxiv.org/abs/2309.02545)</code></li>
<li>Summary: <p>In the past decade, many vulnerabilities were discovered in
microarchitectures which yielded attack vectors and motivated the study of
countermeasures. Further, architectural and physical imperfections in DRAMs led
to the discovery of Rowhammer attacks which give an adversary power to
introduce bit flips in a victim's memory space. Numerous studies analyzed
Rowhammer and proposed techniques to prevent it altogether or to mitigate its
effects.
</p>
<p>In this work, we push the boundary and show how Rowhammer can be further
exploited to inject faults into stack variables and even register values in a
victim's process. We achieve this by targeting the register value that is
stored in the process's stack, which subsequently is flushed out into the
memory, where it becomes vulnerable to Rowhammer. When the faulty value is
restored into the register, it will end up used in subsequent iterations. The
register value can be stored in the stack via latent function calls in the
source or by actively triggering signal handlers. We demonstrate the power of
the findings by applying the techniques to bypass SUDO and SSH authentication.
We further outline how MySQL and other cryptographic libraries can be targeted
with the new attack vector. There are a number of challenges this work
overcomes with extensive experimentation before coming together to yield an
end-to-end attack on an OpenSSL digital signature: achieving co-location with
stack and register variables, with synchronization provided via a blocking
window. We show that stack and registers are no longer safe from the Rowhammer
attack.
</p></li>
</ul>

<h3>Title: Malicious Package Detection in NPM and PyPI using a Single Model of Malicious Behavior Sequence. (arXiv:2309.02637v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02637">http://arxiv.org/abs/2309.02637</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02637]] Malicious Package Detection in NPM and PyPI using a Single Model of Malicious Behavior Sequence(http://arxiv.org/abs/2309.02637)</code></li>
<li>Summary: <p>Open-source software (OSS) supply chain enlarges the attack surface, which
makes package registries attractive targets for attacks. Recently, package
registries NPM and PyPI have been flooded with malicious packages. The
effectiveness of existing malicious NPM and PyPI package detection approaches
is hindered by two challenges. The first challenge is how to leverage the
knowledge of malicious packages from different ecosystems in a unified way such
that multi-lingual malicious package detection can be feasible. The second
challenge is how to model malicious behavior in a sequential way such that
maliciousness can be precisely captured. To address the two challenges, we
propose and implement Cerebro to detect malicious packages in NPM and PyPI. We
curate a feature set based on a high-level abstraction of malicious behavior to
enable multi-lingual knowledge fusing. We organize extracted features into a
behavior sequence to model sequential malicious behavior. We fine-tune the BERT
model to understand the semantics of malicious behavior. Extensive evaluation
has demonstrated the effectiveness of Cerebro over the state-of-the-art as well
as the practically acceptable efficiency. Cerebro has successfully detected 306
and 196 new malicious packages in PyPI and NPM, and received 385 thank letters
from the official PyPI and NPM teams.
</p></li>
</ul>

<h3>Title: SWAP: Exploiting Second-Ranked Logits for Adversarial Attacks on Time Series. (arXiv:2309.02752v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02752">http://arxiv.org/abs/2309.02752</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02752]] SWAP: Exploiting Second-Ranked Logits for Adversarial Attacks on Time Series(http://arxiv.org/abs/2309.02752)</code></li>
<li>Summary: <p>Time series classification (TSC) has emerged as a critical task in various
domains, and deep neural models have shown superior performance in TSC tasks.
However, these models are vulnerable to adversarial attacks, where subtle
perturbations can significantly impact the prediction results. Existing
adversarial methods often suffer from over-parameterization or random logit
perturbation, hindering their effectiveness. Additionally, increasing the
attack success rate (ASR) typically involves generating more noise, making the
attack more easily detectable. To address these limitations, we propose SWAP, a
novel attacking method for TSC models. SWAP focuses on enhancing the confidence
of the second-ranked logits while minimizing the manipulation of other logits.
This is achieved by minimizing the Kullback-Leibler divergence between the
target logit distribution and the predictive logit distribution. Experimental
results demonstrate that SWAP achieves state-of-the-art performance, with an
ASR exceeding 50% and an 18% increase compared to existing methods.
</p></li>
</ul>

<h3>Title: CVE-driven Attack Technique Prediction with Semantic Information Extraction and a Domain-specific Language Model. (arXiv:2309.02785v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02785">http://arxiv.org/abs/2309.02785</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02785]] CVE-driven Attack Technique Prediction with Semantic Information Extraction and a Domain-specific Language Model(http://arxiv.org/abs/2309.02785)</code></li>
<li>Summary: <p>This paper addresses a critical challenge in cybersecurity: the gap between
vulnerability information represented by Common Vulnerabilities and Exposures
(CVEs) and the resulting cyberattack actions. CVEs provide insights into
vulnerabilities, but often lack details on potential threat actions (tactics,
techniques, and procedures, or TTPs) within the ATT&amp;CK framework. This gap
hinders accurate CVE categorization and proactive countermeasure initiation.
The paper introduces the TTPpredictor tool, which uses innovative techniques to
analyze CVE descriptions and infer plausible TTP attacks resulting from CVE
exploitation. TTPpredictor overcomes challenges posed by limited labeled data
and semantic disparities between CVE and TTP descriptions. It initially
extracts threat actions from unstructured cyber threat reports using Semantic
Role Labeling (SRL) techniques. These actions, along with their contextual
attributes, are correlated with MITRE's attack functionality classes. This
automated correlation facilitates the creation of labeled data, essential for
categorizing novel threat actions into threat functionality classes and TTPs.
The paper presents an empirical assessment, demonstrating TTPpredictor's
effectiveness with accuracy rates of approximately 98% and F1-scores ranging
from 95% to 98% in precise CVE classification to ATT&amp;CK techniques.
TTPpredictor outperforms state-of-the-art language model tools like ChatGPT.
Overall, this paper offers a robust solution for linking CVEs to potential
attack techniques, enhancing cybersecurity practitioners' ability to
proactively identify and mitigate threats.
</p></li>
</ul>

<h3>Title: Disarming Steganography Attacks Inside Neural Network Models. (arXiv:2309.03071v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03071">http://arxiv.org/abs/2309.03071</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03071]] Disarming Steganography Attacks Inside Neural Network Models(http://arxiv.org/abs/2309.03071)</code></li>
<li>Summary: <p>Similar to the revolution of open source code sharing, Artificial
Intelligence (AI) model sharing is gaining increased popularity. However, the
fast adaptation in the industry, lack of awareness, and ability to exploit the
models make them significant attack vectors. By embedding malware in neurons,
the malware can be delivered covertly, with minor or no impact on the neural
network's performance. The covert attack will use the Least Significant Bits
(LSB) weight attack since LSB has a minimal effect on the model accuracy, and
as a result, the user will not notice it. Since there are endless ways to hide
the attacks, we focus on a zero-trust prevention strategy based on AI model
attack disarm and reconstruction. We proposed three types of model
steganography weight disarm defense mechanisms. The first two are based on
random bit substitution noise, and the other on model weight quantization. We
demonstrate a 100\% prevention rate while the methods introduce a minimal
decrease in model accuracy based on Qint8 and K-LRBP methods, which is an
essential factor for improving AI security.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: MAD: Modality Agnostic Distance Measure for Image Registration. (arXiv:2309.02875v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02875">http://arxiv.org/abs/2309.02875</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02875]] MAD: Modality Agnostic Distance Measure for Image Registration(http://arxiv.org/abs/2309.02875)</code></li>
<li>Summary: <p>Multi-modal image registration is a crucial pre-processing step in many
medical applications. However, it is a challenging task due to the complex
intensity relationships between different imaging modalities, which can result
in large discrepancy in image appearance. The success of multi-modal image
registration, whether it is conventional or learning based, is predicated upon
the choice of an appropriate distance (or similarity) measure. Particularly,
deep learning registration algorithms lack in accuracy or even fail completely
when attempting to register data from an "unseen" modality. In this work, we
present Modality Agnostic Distance (MAD), a deep image distance}] measure that
utilises random convolutions to learn the inherent geometry of the images while
being robust to large appearance changes. Random convolutions are
geometry-preserving modules which we use to simulate an infinite number of
synthetic modalities alleviating the need for aligned paired data during
training. We can therefore train MAD on a mono-modal dataset and successfully
apply it to a multi-modal dataset. We demonstrate that not only can MAD
affinely register multi-modal images successfully, but it has also a larger
capture range than traditional measures such as Mutual Information and
Normalised Gradient Fields.
</p></li>
</ul>

<h3>Title: Patched Line Segment Learning for Vector Road Mapping. (arXiv:2309.02923v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02923">http://arxiv.org/abs/2309.02923</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02923]] Patched Line Segment Learning for Vector Road Mapping(http://arxiv.org/abs/2309.02923)</code></li>
<li>Summary: <p>This paper presents a novel approach to computing vector road maps from
satellite remotely sensed images, building upon a well-defined Patched Line
Segment (PaLiS) representation for road graphs that holds geometric
significance. Unlike prevailing methods that derive road vector representations
from satellite images using binary masks or keypoints, our method employs line
segments. These segments not only convey road locations but also capture their
orientations, making them a robust choice for representation. More precisely,
given an input image, we divide it into non-overlapping patches and predict a
suitable line segment within each patch. This strategy enables us to capture
spatial and structural cues from these patch-based line segments, simplifying
the process of constructing the road network graph without the necessity of
additional neural networks for connectivity. In our experiments, we demonstrate
how an effective representation of a road graph significantly enhances the
performance of vector road mapping on established benchmarks, without requiring
extensive modifications to the neural network architecture. Furthermore, our
method achieves state-of-the-art performance with just 6 GPU hours of training,
leading to a substantial 32-fold reduction in training costs in terms of GPU
hours.
</p></li>
</ul>

<h3>Title: M3D-NCA: Robust 3D Segmentation with Built-in Quality Control. (arXiv:2309.02954v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02954">http://arxiv.org/abs/2309.02954</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02954]] M3D-NCA: Robust 3D Segmentation with Built-in Quality Control(http://arxiv.org/abs/2309.02954)</code></li>
<li>Summary: <p>Medical image segmentation relies heavily on large-scale deep learning
models, such as UNet-based architectures. However, the real-world utility of
such models is limited by their high computational requirements, which makes
them impractical for resource-constrained environments such as primary care
facilities and conflict zones. Furthermore, shifts in the imaging domain can
render these models ineffective and even compromise patient safety if such
errors go undetected. To address these challenges, we propose M3D-NCA, a novel
methodology that leverages Neural Cellular Automata (NCA) segmentation for 3D
medical images using n-level patchification. Moreover, we exploit the variance
in M3D-NCA to develop a novel quality metric which can automatically detect
errors in the segmentation process of NCAs. M3D-NCA outperforms the two
magnitudes larger UNet models in hippocampus and prostate segmentation by 2%
Dice and can be run on a Raspberry Pi 4 Model B (2GB RAM). This highlights the
potential of M3D-NCA as an effective and efficient alternative for medical
image segmentation in resource-constrained environments.
</p></li>
</ul>

<h3>Title: FishMOT: A Simple and Effective Method for Fish Tracking Based on IoU Matching. (arXiv:2309.02975v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02975">http://arxiv.org/abs/2309.02975</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02975]] FishMOT: A Simple and Effective Method for Fish Tracking Based on IoU Matching(http://arxiv.org/abs/2309.02975)</code></li>
<li>Summary: <p>The tracking of various fish species plays a profoundly significant role in
understanding the behavior of individual fish and their groups. Present
tracking methods suffer from issues of low accuracy or poor robustness. In
order to address these concerns, this paper proposes a novel tracking approach,
named FishMOT (Fish Multiple Object Tracking). This method combines object
detection techniques with the IoU matching algorithm, thereby achieving
efficient, precise, and robust fish detection and tracking. Diverging from
other approaches, this method eliminates the need for multiple feature
extractions and identity assignments for each individual, instead directly
utilizing the output results of the detector for tracking, thereby
significantly reducing computational time and storage space. Furthermore, this
method imposes minimal requirements on factors such as video quality and
variations in individual appearance. As long as the detector can accurately
locate and identify fish, effective tracking can be achieved. This approach
enhances robustness and generalizability. Moreover, the algorithm employed in
this method addresses the issue of missed detections without relying on complex
feature matching or graph optimization algorithms. This contributes to improved
accuracy and reliability. Experimental trials were conducted in the open-source
video dataset provided by idtracker.ai, and comparisons were made with
state-of-the-art detector-based multi-object tracking methods. Additionally,
comparisons were made with idtracker.ai and TRex, two tools that demonstrate
exceptional performance in the field of animal tracking. The experimental
results demonstrate that the proposed method outperforms other approaches in
various evaluation metrics, exhibiting faster speed and lower memory
requirements. The source codes and pre-trained models are available at:
https://github.com/gakkistar/FishMOT
</p></li>
</ul>

<h3>Title: Epi-Curriculum: Episodic Curriculum Learning for Low-Resource Domain Adaptation in Neural Machine Translation. (arXiv:2309.02640v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02640">http://arxiv.org/abs/2309.02640</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02640]] Epi-Curriculum: Episodic Curriculum Learning for Low-Resource Domain Adaptation in Neural Machine Translation(http://arxiv.org/abs/2309.02640)</code></li>
<li>Summary: <p>Neural Machine Translation (NMT) models have become successful, but their
performance remains poor when translating on new domains with a limited number
of data. In this paper, we present a novel approach Epi-Curriculum to address
low-resource domain adaptation (DA), which contains a new episodic training
framework along with denoised curriculum learning. Our episodic training
framework enhances the model's robustness to domain shift by episodically
exposing the encoder/decoder to an inexperienced decoder/encoder. The denoised
curriculum learning filters the noised data and further improves the model's
adaptability by gradually guiding the learning process from easy to more
difficult tasks. Experiments on English-German and English-Romanian translation
show that: (i) Epi-Curriculum improves both model's robustness and adaptability
in seen and unseen domains; (ii) Our episodic training framework enhances the
encoder and decoder's robustness to domain shift.
</p></li>
</ul>

<h3>Title: J-Guard: Journalism Guided Adversarially Robust Detection of AI-generated News. (arXiv:2309.03164v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03164">http://arxiv.org/abs/2309.03164</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03164]] J-Guard: Journalism Guided Adversarially Robust Detection of AI-generated News(http://arxiv.org/abs/2309.03164)</code></li>
<li>Summary: <p>The rapid proliferation of AI-generated text online is profoundly reshaping
the information landscape. Among various types of AI-generated text,
AI-generated news presents a significant threat as it can be a prominent source
of misinformation online. While several recent efforts have focused on
detecting AI-generated text in general, these methods require enhanced
reliability, given concerns about their vulnerability to simple adversarial
attacks. Furthermore, due to the eccentricities of news writing, applying these
detection methods for AI-generated news can produce false positives,
potentially damaging the reputation of news organizations. To address these
challenges, we leverage the expertise of an interdisciplinary team to develop a
framework, J-Guard, capable of steering existing supervised AI text detectors
for detecting AI-generated news while boosting adversarial robustness. By
incorporating stylistic cues inspired by the unique journalistic attributes,
J-Guard effectively distinguishes between real-world journalism and
AI-generated news articles. Our experiments on news articles generated by a
vast array of AI models, including ChatGPT (GPT3.5), demonstrate the
effectiveness of J-Guard in enhancing detection capabilities while maintaining
an average performance decrease of as low as 7% when faced with adversarial
attacks.
</p></li>
</ul>

<h3>Title: T-SaS: Toward Shift-aware Dynamic Adaptation for Streaming Data. (arXiv:2309.02610v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02610">http://arxiv.org/abs/2309.02610</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02610]] T-SaS: Toward Shift-aware Dynamic Adaptation for Streaming Data(http://arxiv.org/abs/2309.02610)</code></li>
<li>Summary: <p>In many real-world scenarios, distribution shifts exist in the streaming data
across time steps. Many complex sequential data can be effectively divided into
distinct regimes that exhibit persistent dynamics. Discovering the shifted
behaviors and the evolving patterns underlying the streaming data are important
to understand the dynamic system. Existing methods typically train one robust
model to work for the evolving data of distinct distributions or sequentially
adapt the model utilizing explicitly given regime boundaries. However, there
are two challenges: (1) shifts in data streams could happen drastically and
abruptly without precursors. Boundaries of distribution shifts are usually
unavailable, and (2) training a shared model for all domains could fail to
capture varying patterns. This paper aims to solve the problem of sequential
data modeling in the presence of sudden distribution shifts that occur without
any precursors. Specifically, we design a Bayesian framework, dubbed as T-SaS,
with a discrete distribution-modeling variable to capture abrupt shifts of
data. Then, we design a model that enable adaptation with dynamic network
selection conditioned on that discrete variable. The proposed method learns
specific model parameters for each distribution by learning which neurons
should be activated in the full network. A dynamic masking strategy is adopted
here to support inter-distribution transfer through the overlapping of a set of
sparse networks. Extensive experiments show that our proposed method is
superior in both accurately detecting shift boundaries to get segments of
varying distributions and effectively adapting to downstream forecast or
classification tasks.
</p></li>
</ul>

<h3>Title: Deep Reinforcement Learning from Hierarchical Weak Preference Feedback. (arXiv:2309.02632v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02632">http://arxiv.org/abs/2309.02632</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02632]] Deep Reinforcement Learning from Hierarchical Weak Preference Feedback(http://arxiv.org/abs/2309.02632)</code></li>
<li>Summary: <p>Reward design is a fundamental, yet challenging aspect of practical
reinforcement learning (RL). For simple tasks, researchers typically handcraft
the reward function, e.g., using a linear combination of several reward
factors. However, such reward engineering is subject to approximation bias,
incurs large tuning cost, and often cannot provide the granularity required for
complex tasks. To avoid these difficulties, researchers have turned to
reinforcement learning from human feedback (RLHF), which learns a reward
function from human preferences between pairs of trajectory sequences. By
leveraging preference-based reward modeling, RLHF learns complex rewards that
are well aligned with human preferences, allowing RL to tackle increasingly
difficult problems. Unfortunately, the applicability of RLHF is limited due to
the high cost and difficulty of obtaining human preference data. In light of
this cost, we investigate learning reward functions for complex tasks with less
human effort; simply by ranking the importance of the reward factors. More
specifically, we propose a new RL framework -- HERON, which compares
trajectories using a hierarchical decision tree induced by the given ranking.
These comparisons are used to train a preference-based reward model, which is
then used for policy learning. We find that our framework can not only train
high performing agents on a variety of difficult tasks, but also provide
additional benefits such as improved sample efficiency and robustness. Our code
is available at https://github.com/abukharin3/HERON.
</p></li>
</ul>

<h3>Title: Improved Outlier Robust Seeding for k-means. (arXiv:2309.02710v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02710">http://arxiv.org/abs/2309.02710</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02710]] Improved Outlier Robust Seeding for k-means(http://arxiv.org/abs/2309.02710)</code></li>
<li>Summary: <p>The $k$-means is a popular clustering objective, although it is inherently
non-robust and sensitive to outliers. Its popular seeding or initialization
called $k$-means++ uses $D^{2}$ sampling and comes with a provable $O(\log k)$
approximation guarantee \cite{AV2007}. However, in the presence of adversarial
noise or outliers, $D^{2}$ sampling is more likely to pick centers from distant
outliers instead of inlier clusters, and therefore its approximation guarantees
\textit{w.r.t.} $k$-means solution on inliers, does not hold.
</p>
<p>Assuming that the outliers constitute a constant fraction of the given data,
we propose a simple variant in the $D^2$ sampling distribution, which makes it
robust to the outliers. Our algorithm runs in $O(ndk)$ time, outputs $O(k)$
clusters, discards marginally more points than the optimal number of outliers,
and comes with a provable $O(1)$ approximation guarantee.
</p>
<p>Our algorithm can also be modified to output exactly $k$ clusters instead of
$O(k)$ clusters, while keeping its running time linear in $n$ and $d$. This is
an improvement over previous results for robust $k$-means based on LP
relaxation and rounding \cite{Charikar}, \cite{KrishnaswamyLS18} and
\textit{robust $k$-means++} \cite{DeshpandeKP20}. Our empirical results show
the advantage of our algorithm over $k$-means++~\cite{AV2007}, uniform random
seeding, greedy sampling for $k$ means~\cite{tkmeanspp}, and robust
$k$-means++~\cite{DeshpandeKP20}, on standard real-world and synthetic data
sets used in previous work. Our proposal is easily amenable to scalable,
faster, parallel implementations of $k$-means++ \cite{Bahmani,BachemL017} and
is of independent interest for coreset constructions in the presence of
outliers \cite{feldman2007ptas,langberg2010universal,feldman2011unified}.
</p></li>
</ul>

<h3>Title: DECODE: Data-driven Energy Consumption Prediction leveraging Historical Data and Environmental Factors in Buildings. (arXiv:2309.02908v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02908">http://arxiv.org/abs/2309.02908</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02908]] DECODE: Data-driven Energy Consumption Prediction leveraging Historical Data and Environmental Factors in Buildings(http://arxiv.org/abs/2309.02908)</code></li>
<li>Summary: <p>Energy prediction in buildings plays a crucial role in effective energy
management. Precise predictions are essential for achieving optimal energy
consumption and distribution within the grid. This paper introduces a Long
Short-Term Memory (LSTM) model designed to forecast building energy consumption
using historical energy data, occupancy patterns, and weather conditions. The
LSTM model provides accurate short, medium, and long-term energy predictions
for residential and commercial buildings compared to existing prediction
models. We compare our LSTM model with established prediction methods,
including linear regression, decision trees, and random forest. Encouragingly,
the proposed LSTM model emerges as the superior performer across all metrics.
It demonstrates exceptional prediction accuracy, boasting the highest R2 score
of 0.97 and the most favorable mean absolute error (MAE) of 0.007. An
additional advantage of our developed model is its capacity to achieve
efficient energy consumption forecasts even when trained on a limited dataset.
We address concerns about overfitting (variance) and underfitting (bias)
through rigorous training and evaluation on real-world data. In summary, our
research contributes to energy prediction by offering a robust LSTM model that
outperforms alternative methods and operates with remarkable efficiency,
generalizability, and reliability.
</p></li>
</ul>

<h3>Title: Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness. (arXiv:2309.03004v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03004">http://arxiv.org/abs/2309.03004</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03004]] Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness(http://arxiv.org/abs/2309.03004)</code></li>
<li>Summary: <p>A recent empirical observation of activation sparsity in MLP layers offers an
opportunity to drastically reduce computation costs for free. Despite several
works attributing it to training dynamics, the theoretical explanation of
activation sparsity's emergence is restricted to shallow networks, small
training steps well as modified training, even though the sparsity has been
found in deep models trained by vanilla protocols for large steps. To fill the
three gaps, we propose the notion of gradient sparsity as the source of
activation sparsity and a theoretical explanation based on it that explains
gradient sparsity and then activation sparsity as necessary steps to
adversarial robustness w.r.t. hidden features and parameters, which is
approximately the flatness of minima for well-learned models. The theory
applies to standardly trained LayerNorm-ed pure MLPs, and further to
Transformers or other architectures if noises are added to weights during
training. To eliminate other sources of flatness when arguing sparsities'
necessity, we discover the phenomenon of spectral concentration, i.e., the
ratio between the largest and the smallest non-zero singular values of weight
matrices is small. We utilize random matrix theory (RMT) as a powerful
theoretical tool to analyze stochastic gradient noises and discuss the
emergence of spectral concentration. With these insights, we propose two
plug-and-play modules for both training from scratch and sparsity finetuning,
as well as one radical modification that only applies to from-scratch training.
Another under-testing module for both sparsity and flatness is also immediate
from our theories. Validational experiments are conducted to verify our
explanation. Experiments for productivity demonstrate modifications'
improvement in sparsity, indicating further theoretical cost reduction in both
training and inference.
</p></li>
</ul>

<h3>Title: Deep Learning for Polycystic Kidney Disease: Utilizing Neural Networks for Accurate and Early Detection through Gene Expression Analysis. (arXiv:2309.03033v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03033">http://arxiv.org/abs/2309.03033</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03033]] Deep Learning for Polycystic Kidney Disease: Utilizing Neural Networks for Accurate and Early Detection through Gene Expression Analysis(http://arxiv.org/abs/2309.03033)</code></li>
<li>Summary: <p>With Polycystic Kidney Disease (PKD) potentially leading to fatal
complications in patients due to the formation of cysts in the kidneys, early
detection of PKD is crucial for effective management of the condition. However,
the various patient-specific factors that play a role in the diagnosis make it
an intricate puzzle for clinicians to solve. Therefore, in this study, we aim
to utilize a deep learning-based approach for early disease detection. The
devised neural network can achieve accurate and robust predictions for possible
PKD in patients by analyzing patient gene expressions.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Gene-induced Multimodal Pre-training for Image-omic Classification. (arXiv:2309.02702v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02702">http://arxiv.org/abs/2309.02702</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02702]] Gene-induced Multimodal Pre-training for Image-omic Classification(http://arxiv.org/abs/2309.02702)</code></li>
<li>Summary: <p>Histology analysis of the tumor micro-environment integrated with genomic
assays is the gold standard for most cancers in modern medicine. This paper
proposes a Gene-induced Multimodal Pre-training (GiMP) framework, which jointly
incorporates genomics and Whole Slide Images (WSIs) for classification tasks.
Our work aims at dealing with the main challenges of multi-modality image-omic
classification w.r.t. (1) the patient-level feature extraction difficulties
from gigapixel WSIs and tens of thousands of genes, and (2) effective fusion
considering high-order relevance modeling. Concretely, we first propose a group
multi-head self-attention gene encoder to capture global structured features in
gene expression cohorts. We design a masked patch modeling paradigm (MPM) to
capture the latent pathological characteristics of different tissues. The mask
strategy is randomly masking a fixed-length contiguous subsequence of patch
embeddings of a WSI. Finally, we combine the classification tokens of paired
modalities and propose a triplet learning module to learn high-order relevance
and discriminative patient-level information.After pre-training, a simple
fine-tuning can be adopted to obtain the classification results. Experimental
results on the TCGA dataset show the superiority of our network architectures
and our pre-training framework, achieving 99.47% in accuracy for image-omic
classification. The code is publicly available at
https://github.com/huangwudiduan/GIMP.
</p></li>
</ul>

<h3>Title: FArMARe: a Furniture-Aware Multi-task methodology for Recommending Apartments based on the user interests. (arXiv:2309.03100v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03100">http://arxiv.org/abs/2309.03100</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03100]] FArMARe: a Furniture-Aware Multi-task methodology for Recommending Apartments based on the user interests(http://arxiv.org/abs/2309.03100)</code></li>
<li>Summary: <p>Nowadays, many people frequently have to search for new accommodation
options. Searching for a suitable apartment is a time-consuming process,
especially because visiting them is often mandatory to assess the truthfulness
of the advertisements found on the Web. While this process could be alleviated
by visiting the apartments in the metaverse, the Web-based recommendation
platforms are not suitable for the task. To address this shortcoming, in this
paper, we define a new problem called text-to-apartment recommendation, which
requires ranking the apartments based on their relevance to a textual query
expressing the user's interests. To tackle this problem, we introduce FArMARe,
a multi-task approach that supports cross-modal contrastive training with a
furniture-aware objective. Since public datasets related to indoor scenes do
not contain detailed descriptions of the furniture, we collect and annotate a
dataset comprising more than 6000 apartments. A thorough experimentation with
three different methods and two raw feature extraction procedures reveals the
effectiveness of FArMARe in dealing with the problem at hand.
</p></li>
</ul>

<h3>Title: Leave no Place Behind: Improved Geolocation in Humanitarian Documents. (arXiv:2309.02914v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02914">http://arxiv.org/abs/2309.02914</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02914]] Leave no Place Behind: Improved Geolocation in Humanitarian Documents(http://arxiv.org/abs/2309.02914)</code></li>
<li>Summary: <p>Geographical location is a crucial element of humanitarian response,
outlining vulnerable populations, ongoing events, and available resources.
Latest developments in Natural Language Processing may help in extracting vital
information from the deluge of reports and documents produced by the
humanitarian sector. However, the performance and biases of existing
state-of-the-art information extraction tools are unknown. In this work, we
develop annotated resources to fine-tune the popular Named Entity Recognition
(NER) tools Spacy and roBERTa to perform geotagging of humanitarian texts. We
then propose a geocoding method FeatureRank which links the candidate locations
to the GeoNames database. We find that not only does the humanitarian-domain
data improves the performance of the classifiers (up to F1 = 0.92), but it also
alleviates some of the bias of the existing tools, which erroneously favor
locations in the Western countries. Thus, we conclude that more resources from
non-Western documents are necessary to ensure that off-the-shelf NER systems
are suitable for the deployment in the humanitarian sector.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h2>fair</h2>
<h3>Title: Developing A Fair Individualized Polysocial Risk Score (iPsRS) for Identifying Increased Social Risk of Hospitalizations in Patients with Type 2 Diabetes (T2D). (arXiv:2309.02467v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02467">http://arxiv.org/abs/2309.02467</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02467]] Developing A Fair Individualized Polysocial Risk Score (iPsRS) for Identifying Increased Social Risk of Hospitalizations in Patients with Type 2 Diabetes (T2D)(http://arxiv.org/abs/2309.02467)</code></li>
<li>Summary: <p>Background: Racial and ethnic minority groups and individuals facing social
disadvantages, which often stem from their social determinants of health
(SDoH), bear a disproportionate burden of type 2 diabetes (T2D) and its
complications. It is therefore crucial to implement effective social risk
management strategies at the point of care. Objective: To develop an EHR-based
machine learning (ML) analytical pipeline to identify the unmet social needs
associated with hospitalization risk in patients with T2D. Methods: We
identified 10,192 T2D patients from the EHR data (from 2012 to 2022) from the
University of Florida Health Integrated Data Repository, including contextual
SDoH (e.g., neighborhood deprivation) and individual-level SDoH (e.g., housing
stability). We developed an electronic health records (EHR)-based machine
learning (ML) analytic pipeline, namely individualized polysocial risk score
(iPsRS), to identify high social risk associated with hospitalizations in T2D
patients, along with explainable AI (XAI) techniques and fairness assessment
and optimization. Results: Our iPsRS achieved a C statistic of 0.72 in
predicting 1-year hospitalization after fairness optimization across
racial-ethnic groups. The iPsRS showed excellent utility for capturing
individuals at high hospitalization risk; the actual 1-year hospitalization
rate in the top 5% of iPsRS was ~13 times as high as the bottom decile.
Conclusion: Our ML pipeline iPsRS can fairly and accurately screen for patients
who have increased social risk leading to hospitalization in T2D patients.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: PDiscoNet: Semantically consistent part discovery for fine-grained recognition. (arXiv:2309.03173v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03173">http://arxiv.org/abs/2309.03173</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03173]] PDiscoNet: Semantically consistent part discovery for fine-grained recognition(http://arxiv.org/abs/2309.03173)</code></li>
<li>Summary: <p>Fine-grained classification often requires recognizing specific object parts,
such as beak shape and wing patterns for birds. Encouraging a fine-grained
classification model to first detect such parts and then using them to infer
the class could help us gauge whether the model is indeed looking at the right
details better than with interpretability methods that provide a single
attribution map. We propose PDiscoNet to discover object parts by using only
image-level class labels along with priors encouraging the parts to be:
discriminative, compact, distinct from each other, equivariant to rigid
transforms, and active in at least some of the images. In addition to using the
appropriate losses to encode these priors, we propose to use part-dropout,
where full part feature vectors are dropped at once to prevent a single part
from dominating in the classification, and part feature vector modulation,
which makes the information coming from each part distinct from the perspective
of the classifier. Our results on CUB, CelebA, and PartImageNet show that the
proposed method provides substantially better part discovery performance than
previous methods while not requiring any additional hyper-parameter tuning and
without penalizing the classification performance. The code is available at
https://github.com/robertdvdk/part_detection.
</p></li>
</ul>

<h3>Title: RLSynC: Offline-Online Reinforcement Learning for Synthon Completion. (arXiv:2309.02671v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02671">http://arxiv.org/abs/2309.02671</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02671]] RLSynC: Offline-Online Reinforcement Learning for Synthon Completion(http://arxiv.org/abs/2309.02671)</code></li>
<li>Summary: <p>Retrosynthesis is the process of determining the set of reactant molecules
that can react to form a desired product. Semi-template-based retrosynthesis
methods, which imitate the reverse logic of synthesis reactions, first predict
the reaction centers in the products, and then complete the resulting synthons
back into reactants. These methods enable necessary interpretability and high
practical utility to inform synthesis planning. We develop a new offline-online
reinforcement learning method RLSynC for synthon completion in
semi-template-based methods. RLSynC assigns one agent to each synthon, all of
which complete the synthons by conducting actions step by step in a
synchronized fashion. RLSynC learns the policy from both offline training
episodes and online interactions which allow RLSynC to explore new reaction
spaces. RLSynC uses a forward synthesis model to evaluate the likelihood of the
predicted reactants in synthesizing a product, and thus guides the action
search. We compare RLSynC with the state-of-the-art retrosynthesis methods. Our
experimental results demonstrate that RLSynC can outperform these methods with
improvement as high as 14.9% on synthon completion, and 14.0% on
retrosynthesis, highlighting its potential in synthesis planning.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Anatomy-Driven Pathology Detection on Chest X-rays. (arXiv:2309.02578v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02578">http://arxiv.org/abs/2309.02578</a></li>
<li>Code URL: https://github.com/philip-mueller/adpd</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02578]] Anatomy-Driven Pathology Detection on Chest X-rays(http://arxiv.org/abs/2309.02578)</code></li>
<li>Summary: <p>Pathology detection and delineation enables the automatic interpretation of
medical scans such as chest X-rays while providing a high level of
explainability to support radiologists in making informed decisions. However,
annotating pathology bounding boxes is a time-consuming task such that large
public datasets for this purpose are scarce. Current approaches thus use weakly
supervised object detection to learn the (rough) localization of pathologies
from image-level annotations, which is however limited in performance due to
the lack of bounding box supervision. We therefore propose anatomy-driven
pathology detection (ADPD), which uses easy-to-annotate bounding boxes of
anatomical regions as proxies for pathologies. We study two training
approaches: supervised training using anatomy-level pathology labels and
multiple instance learning (MIL) with image-level pathology labels. Our results
show that our anatomy-level training approach outperforms weakly supervised
methods and fully supervised detection with limited training samples, and our
MIL approach is competitive with both baseline approaches, therefore
demonstrating the potential of our approach.
</p></li>
</ul>

<h3>Title: Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs. (arXiv:2309.03118v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03118">http://arxiv.org/abs/2309.03118</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03118]] Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs(http://arxiv.org/abs/2309.03118)</code></li>
<li>Summary: <p>Large language models (LLMs), such as ChatGPT and GPT-4, are versatile and
can solve different tasks due to their emergent ability and generalizability.
However, LLMs sometimes lack domain-specific knowledge to perform tasks, which
would also cause hallucination during inference. In some previous works,
additional modules like graph neural networks (GNNs) are trained on retrieved
knowledge from external knowledge bases, aiming to mitigate the problem of
lacking domain-specific knowledge. However, incorporating additional modules:
1) would need retraining additional modules when encountering novel domains; 2)
would become a bottleneck since LLMs' strong abilities are not fully utilized
for retrieval. In this paper, we propose a paradigm, termed Knowledge Solver
(KSL), to teach LLMs to search for essential knowledge from external knowledge
bases by harnessing their own strong generalizability. Specifically, we design
a simple yet effective prompt to transform retrieval into a multi-hop decision
sequence, which empowers LLMs with searching knowledge ability in zero-shot
manner. Additionally, KSL is able to provide complete retrieval paths and
therefore increase explainability of LLMs' reasoning processes. We conduct
experiments on three datasets: CommonsenseQA, OpenbookQA, and MedQA-USMLE, and
found that our approach improves LLM baseline performance by a relatively large
margin.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: RSDiff: Remote Sensing Image Generation from Text Using Diffusion Model. (arXiv:2309.02455v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02455">http://arxiv.org/abs/2309.02455</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02455]] RSDiff: Remote Sensing Image Generation from Text Using Diffusion Model(http://arxiv.org/abs/2309.02455)</code></li>
<li>Summary: <p>Satellite imagery generation and super-resolution are pivotal tasks in remote
sensing, demanding high-quality, detailed images for accurate analysis and
decision-making. In this paper, we propose an innovative and lightweight
approach that employs two-stage diffusion models to gradually generate
high-resolution Satellite images purely based on text prompts. Our innovative
pipeline comprises two interconnected diffusion models: a Low-Resolution
Generation Diffusion Model (LR-GDM) that generates low-resolution images from
text and a Super-Resolution Diffusion Model (SRDM) conditionally produced. The
LR-GDM effectively synthesizes low-resolution by (computing the correlations of
the text embedding and the image embedding in a shared latent space), capturing
the essential content and layout of the desired scenes. Subsequently, the SRDM
takes the generated low-resolution image and its corresponding text prompts and
efficiently produces the high-resolution counterparts, infusing fine-grained
spatial details and enhancing visual fidelity. Experiments are conducted on the
commonly used dataset, Remote Sensing Image Captioning Dataset (RSICD). Our
results demonstrate that our approach outperforms existing state-of-the-art
(SoTA) models in generating satellite images with realistic geographical
features, weather conditions, and land structures while achieving remarkable
super-resolution results for increased spatial precision.
</p></li>
</ul>

<h3>Title: Diffusion Model is Secretly a Training-free Open Vocabulary Semantic Segmenter. (arXiv:2309.02773v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02773">http://arxiv.org/abs/2309.02773</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02773]] Diffusion Model is Secretly a Training-free Open Vocabulary Semantic Segmenter(http://arxiv.org/abs/2309.02773)</code></li>
<li>Summary: <p>Recent research has explored the utilization of pre-trained text-image
discriminative models, such as CLIP, to tackle the challenges associated with
open-vocabulary semantic segmentation. However, it is worth noting that the
alignment process based on contrastive learning employed by these models may
unintentionally result in the loss of crucial localization information and
object completeness, which are essential for achieving accurate semantic
segmentation. More recently, there has been an emerging interest in extending
the application of diffusion models beyond text-to-image generation tasks,
particularly in the domain of semantic segmentation. These approaches utilize
diffusion models either for generating annotated data or for extracting
features to facilitate semantic segmentation. This typically involves training
segmentation models by generating a considerable amount of synthetic data or
incorporating additional mask annotations. To this end, we uncover the
potential of generative text-to-image conditional diffusion models as highly
efficient open-vocabulary semantic segmenters, and introduce a novel
training-free approach named DiffSegmenter. Specifically, by feeding an input
image and candidate classes into an off-the-shelf pre-trained conditional
latent diffusion model, the cross-attention maps produced by the denoising
U-Net are directly used as segmentation scores, which are further refined and
completed by the followed self-attention maps. Additionally, we carefully
design effective textual prompts and a category filtering mechanism to further
enhance the segmentation results. Extensive experiments on three benchmark
datasets show that the proposed DiffSegmenter achieves impressive results for
open-vocabulary semantic segmentation.
</p></li>
</ul>

<h3>Title: MCM: Multi-condition Motion Synthesis Framework for Multi-scenario. (arXiv:2309.03031v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03031">http://arxiv.org/abs/2309.03031</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03031]] MCM: Multi-condition Motion Synthesis Framework for Multi-scenario(http://arxiv.org/abs/2309.03031)</code></li>
<li>Summary: <p>The objective of the multi-condition human motion synthesis task is to
incorporate diverse conditional inputs, encompassing various forms like text,
music, speech, and more. This endows the task with the capability to adapt
across multiple scenarios, ranging from text-to-motion and music-to-dance,
among others. While existing research has primarily focused on single
conditions, the multi-condition human motion generation remains underexplored.
In this paper, we address these challenges by introducing MCM, a novel paradigm
for motion synthesis that spans multiple scenarios under diverse conditions.
The MCM framework is able to integrate with any DDPM-like diffusion model to
accommodate multi-conditional information input while preserving its generative
capabilities. Specifically, MCM employs two-branch architecture consisting of a
main branch and a control branch. The control branch shares the same structure
as the main branch and is initialized with the parameters of the main branch,
effectively maintaining the generation ability of the main branch and
supporting multi-condition input. We also introduce a Transformer-based
diffusion model MWNet (DDPM-like) as our main branch that can capture the
spatial complexity and inter-joint correlations in motion sequences through a
channel-dimension self-attention module. Quantitative comparisons demonstrate
that our approach achieves SoTA results in both text-to-motion and competitive
results in music-to-dance tasks, comparable to task-specific methods.
Furthermore, the qualitative evaluation shows that MCM not only streamlines the
adaptation of methodologies originally designed for text-to-motion tasks to
domains like music-to-dance and speech-to-gesture, eliminating the need for
extensive network re-configurations but also enables effective multi-condition
modal control, realizing "once trained is motion need".
</p></li>
</ul>

<h3>Title: SLiMe: Segment Like Me. (arXiv:2309.03179v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03179">http://arxiv.org/abs/2309.03179</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03179]] SLiMe: Segment Like Me(http://arxiv.org/abs/2309.03179)</code></li>
<li>Summary: <p>Significant strides have been made using large vision-language models, like
Stable Diffusion (SD), for a variety of downstream tasks, including image
editing, image correspondence, and 3D shape generation. Inspired by these
advancements, we explore leveraging these extensive vision-language models for
segmenting images at any desired granularity using as few as one annotated
sample by proposing SLiMe. SLiMe frames this problem as an optimization task.
Specifically, given a single training image and its segmentation mask, we first
extract attention maps, including our novel "weighted accumulated
self-attention map" from the SD prior. Then, using the extracted attention
maps, the text embeddings of Stable Diffusion are optimized such that, each of
them, learn about a single segmented region from the training image. These
learned embeddings then highlight the segmented region in the attention maps,
which in turn can then be used to derive the segmentation map. This enables
SLiMe to segment any real-world image during inference with the granularity of
the segmented region in the training image, using just one example. Moreover,
leveraging additional training data when available, i.e. few-shot, improves the
performance of SLiMe. We carried out a knowledge-rich set of experiments
examining various design factors and showed that SLiMe outperforms other
existing one-shot and few-shot segmentation methods.
</p></li>
</ul>

<h3>Title: Diffusion on the Probability Simplex. (arXiv:2309.02530v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02530">http://arxiv.org/abs/2309.02530</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02530]] Diffusion on the Probability Simplex(http://arxiv.org/abs/2309.02530)</code></li>
<li>Summary: <p>Diffusion models learn to reverse the progressive noising of a data
distribution to create a generative model. However, the desired continuous
nature of the noising process can be at odds with discrete data. To deal with
this tension between continuous and discrete objects, we propose a method of
performing diffusion on the probability simplex. Using the probability simplex
naturally creates an interpretation where points correspond to categorical
probability distributions. Our method uses the softmax function applied to an
Ornstein-Unlenbeck Process, a well-known stochastic differential equation. We
find that our methodology also naturally extends to include diffusion on the
unit cube which has applications for bounded image generation.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Self-Supervised Video Transformers for Isolated Sign Language Recognition. (arXiv:2309.02450v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02450">http://arxiv.org/abs/2309.02450</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02450]] Self-Supervised Video Transformers for Isolated Sign Language Recognition(http://arxiv.org/abs/2309.02450)</code></li>
<li>Summary: <p>This paper presents an in-depth analysis of various self-supervision methods
for isolated sign language recognition (ISLR). We consider four recently
introduced transformer-based approaches to self-supervised learning from
videos, and four pre-training data regimes, and study all the combinations on
the WLASL2000 dataset. Our findings reveal that MaskFeat achieves performance
superior to pose-based and supervised video models, with a top-1 accuracy of
79.02% on gloss-based WLASL2000. Furthermore, we analyze these models' ability
to produce representations of ASL signs using linear probing on diverse
phonological features. This study underscores the value of architecture and
pre-training task choices in ISLR. Specifically, our results on WLASL2000
highlight the power of masked reconstruction pre-training, and our linear
probing results demonstrate the importance of hierarchical vision transformers
for sign language representation.
</p></li>
</ul>

<h3>Title: Domain Adaptation for Efficiently Fine-tuning Vision Transformer with Encrypted Images. (arXiv:2309.02556v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02556">http://arxiv.org/abs/2309.02556</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02556]] Domain Adaptation for Efficiently Fine-tuning Vision Transformer with Encrypted Images(http://arxiv.org/abs/2309.02556)</code></li>
<li>Summary: <p>In recent years, deep neural networks (DNNs) trained with transformed data
have been applied to various applications such as privacy-preserving learning,
access control, and adversarial defenses. However, the use of transformed data
decreases the performance of models. Accordingly, in this paper, we propose a
novel method for fine-tuning models with transformed images under the use of
the vision transformer (ViT). The proposed domain adaptation method does not
cause the accuracy degradation of models, and it is carried out on the basis of
the embedding structure of ViT. In experiments, we confirmed that the proposed
method prevents accuracy degradation even when using encrypted images with the
CIFAR-10 and CIFAR-100 datasets.
</p></li>
</ul>

<h3>Title: Compressing Vision Transformers for Low-Resource Visual Learning. (arXiv:2309.02617v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02617">http://arxiv.org/abs/2309.02617</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02617]] Compressing Vision Transformers for Low-Resource Visual Learning(http://arxiv.org/abs/2309.02617)</code></li>
<li>Summary: <p>Vision transformer (ViT) and its variants have swept through visual learning
leaderboards and offer state-of-the-art accuracy in tasks such as image
classification, object detection, and semantic segmentation by attending to
different parts of the visual input and capturing long-range spatial
dependencies. However, these models are large and computation-heavy. For
instance, the recently proposed ViT-B model has 86M parameters making it
impractical for deployment on resource-constrained devices. As a result, their
deployment on mobile and edge scenarios is limited. In our work, we aim to take
a step toward bringing vision transformers to the edge by utilizing popular
model compression techniques such as distillation, pruning, and quantization.
</p>
<p>Our chosen application environment is an unmanned aerial vehicle (UAV) that
is battery-powered and memory-constrained, carrying a single-board computer on
the scale of an NVIDIA Jetson Nano with 4GB of RAM. On the other hand, the UAV
requires high accuracy close to that of state-of-the-art ViTs to ensure safe
object avoidance in autonomous navigation, or correct localization of humans in
search-and-rescue. Inference latency should also be minimized given the
application requirements. Hence, our target is to enable rapid inference of a
vision transformer on an NVIDIA Jetson Nano (4GB) with minimal accuracy loss.
This allows us to deploy ViTs on resource-constrained devices, opening up new
possibilities in surveillance, environmental monitoring, etc. Our
implementation is made available at https://github.com/chensy7/efficient-vit.
</p></li>
</ul>

<h3>Title: Efficient Training for Visual Tracking with Deformable Transformer. (arXiv:2309.02676v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02676">http://arxiv.org/abs/2309.02676</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02676]] Efficient Training for Visual Tracking with Deformable Transformer(http://arxiv.org/abs/2309.02676)</code></li>
<li>Summary: <p>Recent Transformer-based visual tracking models have showcased superior
performance. Nevertheless, prior works have been resource-intensive, requiring
prolonged GPU training hours and incurring high GFLOPs during inference due to
inefficient training methods and convolution-based target heads. This intensive
resource use renders them unsuitable for real-world applications. In this
paper, we present DETRack, a streamlined end-to-end visual object tracking
framework. Our framework utilizes an efficient encoder-decoder structure where
the deformable transformer decoder acting as a target head, achieves higher
sparsity than traditional convolution heads, resulting in decreased GFLOPs. For
training, we introduce a novel one-to-many label assignment and an auxiliary
denoising technique, significantly accelerating model's convergence.
Comprehensive experiments affirm the effectiveness and efficiency of our
proposed method. For instance, DETRack achieves 72.9% AO on challenging GOT-10k
benchmarks using only 20% of the training epochs required by the baseline, and
runs with lower GFLOPs than all the transformer-based trackers.
</p></li>
</ul>

<h3>Title: Vote2Cap-DETR++: Decoupling Localization and Describing for End-to-End 3D Dense Captioning. (arXiv:2309.02999v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02999">http://arxiv.org/abs/2309.02999</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02999]] Vote2Cap-DETR++: Decoupling Localization and Describing for End-to-End 3D Dense Captioning(http://arxiv.org/abs/2309.02999)</code></li>
<li>Summary: <p>3D dense captioning requires a model to translate its understanding of an
input 3D scene into several captions associated with different object regions.
Existing methods adopt a sophisticated "detect-then-describe" pipeline, which
builds explicit relation modules upon a 3D detector with numerous hand-crafted
components. While these methods have achieved initial success, the cascade
pipeline tends to accumulate errors because of duplicated and inaccurate box
estimations and messy 3D scenes. In this paper, we first propose Vote2Cap-DETR,
a simple-yet-effective transformer framework that decouples the decoding
process of caption generation and object localization through parallel
decoding. Moreover, we argue that object localization and description
generation require different levels of scene understanding, which could be
challenging for a shared set of queries to capture. To this end, we propose an
advanced version, Vote2Cap-DETR++, which decouples the queries into
localization and caption queries to capture task-specific features.
Additionally, we introduce the iterative spatial refinement strategy to vote
queries for faster convergence and better localization performance. We also
insert additional spatial information to the caption head for more accurate
descriptions. Without bells and whistles, extensive experiments on two commonly
used datasets, ScanRefer and Nr3D, demonstrate Vote2Cap-DETR and
Vote2Cap-DETR++ surpass conventional "detect-then-describe" methods by a large
margin. Codes will be made available at
https://github.com/ch3cook-fdu/Vote2Cap-DETR.
</p></li>
</ul>

<h3>Title: Combining pre-trained Vision Transformers and CIDER for Out Of Domain Detection. (arXiv:2309.03047v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03047">http://arxiv.org/abs/2309.03047</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03047]] Combining pre-trained Vision Transformers and CIDER for Out Of Domain Detection(http://arxiv.org/abs/2309.03047)</code></li>
<li>Summary: <p>Out-of-domain (OOD) detection is a crucial component in industrial
applications as it helps identify when a model encounters inputs that are
outside the training distribution. Most industrial pipelines rely on
pre-trained models for downstream tasks such as CNN or Vision Transformers.
This paper investigates the performance of those models on the task of
out-of-domain detection. Our experiments demonstrate that pre-trained
transformers models achieve higher detection performance out of the box.
Furthermore, we show that pre-trained ViT and CNNs can be combined with
refinement methods such as CIDER to improve their OOD detection performance
even more. Our results suggest that transformers are a promising approach for
OOD detection and set a stronger baseline for this task in many contexts
</p></li>
</ul>

<h3>Title: Prompt-based All-in-One Image Restoration using CNNs and Transformer. (arXiv:2309.03063v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03063">http://arxiv.org/abs/2309.03063</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03063]] Prompt-based All-in-One Image Restoration using CNNs and Transformer(http://arxiv.org/abs/2309.03063)</code></li>
<li>Summary: <p>Image restoration aims to recover the high-quality images from their degraded
observations. Since most existing methods have been dedicated into single
degradation removal, they may not yield optimal results on other types of
degradations, which do not satisfy the applications in real world scenarios. In
this paper, we propose a novel data ingredient-oriented approach that leverages
prompt-based learning to enable a single model to efficiently tackle multiple
image degradation tasks. Specifically, we utilize a encoder to capture features
and introduce prompts with degradation-specific information to guide the
decoder in adaptively recovering images affected by various degradations. In
order to model the local invariant properties and non-local information for
high-quality image restoration, we combined CNNs operations and Transformers.
Simultaneously, we made several key designs in the Transformer blocks
(multi-head rearranged attention with prompts and simple-gate feed-forward
network) to reduce computational requirements and selectively determines what
information should be persevered to facilitate efficient recovery of
potentially sharp images. Furthermore, we incorporate a feature fusion
mechanism further explores the multi-scale information to improve the
aggregated features. The resulting tightly interlinked hierarchy architecture,
named as CAPTNet, despite being designed to handle different types of
degradations, extensive experiments demonstrate that our method performs
competitively to the task-specific algorithms.
</p></li>
</ul>

<h3>Title: Character Queries: A Transformer-based Approach to On-Line Handwritten Character Segmentation. (arXiv:2309.03072v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03072">http://arxiv.org/abs/2309.03072</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03072]] Character Queries: A Transformer-based Approach to On-Line Handwritten Character Segmentation(http://arxiv.org/abs/2309.03072)</code></li>
<li>Summary: <p>On-line handwritten character segmentation is often associated with
handwriting recognition and even though recognition models include mechanisms
to locate relevant positions during the recognition process, it is typically
insufficient to produce a precise segmentation. Decoupling the segmentation
from the recognition unlocks the potential to further utilize the result of the
recognition. We specifically focus on the scenario where the transcription is
known beforehand, in which case the character segmentation becomes an
assignment problem between sampling points of the stylus trajectory and
characters in the text. Inspired by the $k$-means clustering algorithm, we view
it from the perspective of cluster assignment and present a Transformer-based
architecture where each cluster is formed based on a learned character query in
the Transformer decoder block. In order to assess the quality of our approach,
we create character segmentation ground truths for two popular on-line
handwriting datasets, IAM-OnDB and HANDS-VNOnDB, and evaluate multiple methods
on them, demonstrating that our approach achieves the overall best results.
</p></li>
</ul>

<h3>Title: Representation Learning for Sequential Volumetric Design Tasks. (arXiv:2309.02583v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02583">http://arxiv.org/abs/2309.02583</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02583]] Representation Learning for Sequential Volumetric Design Tasks(http://arxiv.org/abs/2309.02583)</code></li>
<li>Summary: <p>Volumetric design, also called massing design, is the first and critical step
in professional building design which is sequential in nature. As the
volumetric design process is complex, the underlying sequential design process
encodes valuable information for designers. Many efforts have been made to
automatically generate reasonable volumetric designs, but the quality of the
generated design solutions varies, and evaluating a design solution requires
either a prohibitively comprehensive set of metrics or expensive human
expertise. While previous approaches focused on learning only the final design
instead of sequential design tasks, we propose to encode the design knowledge
from a collection of expert or high-performing design sequences and extract
useful representations using transformer-based models. Later we propose to
utilize the learned representations for crucial downstream applications such as
design preference evaluation and procedural design generation. We develop the
preference model by estimating the density of the learned representations
whereas we train an autoregressive transformer model for sequential design
generation. We demonstrate our ideas by leveraging a novel dataset of thousands
of sequential volumetric designs. Our preference model can compare two
arbitrarily given design sequences and is almost 90% accurate in evaluation
against random design sequences. Our autoregressive model is also capable of
autocompleting a volumetric design sequence from a partial design sequence.
</p></li>
</ul>

<h3>Title: TFBEST: Dual-Aspect Transformer with Learnable Positional Encoding for Failure Prediction. (arXiv:2309.02641v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02641">http://arxiv.org/abs/2309.02641</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02641]] TFBEST: Dual-Aspect Transformer with Learnable Positional Encoding for Failure Prediction(http://arxiv.org/abs/2309.02641)</code></li>
<li>Summary: <p>Hard Disk Drive (HDD) failures in datacenters are costly - from catastrophic
data loss to a question of goodwill, stakeholders want to avoid it like the
plague. An important tool in proactively monitoring against HDD failure is
timely estimation of the Remaining Useful Life (RUL). To this end, the
Self-Monitoring, Analysis and Reporting Technology employed within HDDs
(S.M.A.R.T.) provide critical logs for long-term maintenance of the security
and dependability of these essential data storage devices. Data-driven
predictive models in the past have used these S.M.A.R.T. logs and CNN/RNN based
architectures heavily. However, they have suffered significantly in providing a
confidence interval around the predicted RUL values as well as in processing
very long sequences of logs. In addition, some of these approaches, such as
those based on LSTMs, are inherently slow to train and have tedious feature
engineering overheads. To overcome these challenges, in this work we propose a
novel transformer architecture - a Temporal-fusion Bi-encoder Self-attention
Transformer (TFBEST) for predicting failures in hard-drives. It is an
encoder-decoder based deep learning technique that enhances the context gained
from understanding health statistics sequences and predicts a sequence of the
number of days remaining before a disk potentially fails. In this paper, we
also provide a novel confidence margin statistic that can help manufacturers
replace a hard-drive within a time frame. Experiments on Seagate HDD data show
that our method significantly outperforms the state-of-the-art RUL prediction
methods during testing over the exhaustive 10-year data from Backblaze
(2013-present). Although validated on HDD failure prediction, the TFBEST
architecture is well-suited for other prognostics applications and may be
adapted for allied regression problems.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Hierarchical-level rain image generative model based on GAN. (arXiv:2309.02964v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02964">http://arxiv.org/abs/2309.02964</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02964]] Hierarchical-level rain image generative model based on GAN(http://arxiv.org/abs/2309.02964)</code></li>
<li>Summary: <p>Autonomous vehicles are exposed to various weather during operation, which is
likely to trigger the performance limitations of the perception system, leading
to the safety of the intended functionality (SOTIF) problems. To efficiently
generate data for testing the performance of visual perception algorithms under
various weather conditions, a hierarchical-level rain image generative model,
rain conditional CycleGAN (RCCycleGAN), is constructed. RCCycleGAN is based on
the generative adversarial network (GAN) and can generate images of light,
medium, and heavy rain. Different rain intensities are introduced as labels in
conditional GAN (CGAN). Meanwhile, the model structure is optimized and the
training strategy is adjusted to alleviate the problem of mode collapse. In
addition, natural rain images of different intensities are collected and
processed for model training and validation. Compared with the two baseline
models, CycleGAN and DerainCycleGAN, the peak signal-to-noise ratio (PSNR) of
RCCycleGAN on the test dataset is improved by 2.58 dB and 0.74 dB, and the
structural similarity (SSIM) is improved by 18% and 8%, respectively. The
ablation experiments are also carried out to validate the effectiveness of the
model tuning.
</p></li>
</ul>

<h3>Title: Persona-aware Generative Model for Code-mixed Language. (arXiv:2309.02915v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02915">http://arxiv.org/abs/2309.02915</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02915]] Persona-aware Generative Model for Code-mixed Language(http://arxiv.org/abs/2309.02915)</code></li>
<li>Summary: <p>Code-mixing and script-mixing are prevalent across online social networks and
multilingual societies. However, a user's preference toward code-mixing depends
on the socioeconomic status, demographics of the user, and the local context,
which existing generative models mostly ignore while generating code-mixed
texts. In this work, we make a pioneering attempt to develop a persona-aware
generative model to generate texts resembling real-life code-mixed texts of
individuals. We propose a Persona-aware Generative Model for Code-mixed
Generation, PARADOX, a novel Transformer-based encoder-decoder model that
encodes an utterance conditioned on a user's persona and generates code-mixed
texts without monolingual reference data. We propose an alignment module that
re-calibrates the generated sequence to resemble real-life code-mixed texts.
PARADOX generates code-mixed texts that are semantically more meaningful and
linguistically more valid. To evaluate the personification capabilities of
PARADOX, we propose four new metrics -- CM BLEU, CM Rouge-1, CM Rouge-L and CM
KS. On average, PARADOX achieves 1.6 points better CM BLEU, 47% better
perplexity and 32% better semantic coherence than the non-persona-based
counterparts.
</p></li>
</ul>

<h3>Title: Enhancing Semantic Communication with Deep Generative Models -- An ICASSP Special Session Overview. (arXiv:2309.02478v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02478">http://arxiv.org/abs/2309.02478</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02478]] Enhancing Semantic Communication with Deep Generative Models -- An ICASSP Special Session Overview(http://arxiv.org/abs/2309.02478)</code></li>
<li>Summary: <p>Semantic communication is poised to play a pivotal role in shaping the
landscape of future AI-driven communication systems. Its challenge of
extracting semantic information from the original complex content and
regenerating semantically consistent data at the receiver, possibly being
robust to channel corruptions, can be addressed with deep generative models.
This ICASSP special session overview paper discloses the semantic communication
challenges from the machine learning perspective and unveils how deep
generative models will significantly enhance semantic communication frameworks
in dealing with real-world complex data, extracting and exploiting semantic
information, and being robust to channel corruptions. Alongside establishing
this emerging field, this paper charts novel research pathways for the next
generative semantic communication frameworks.
</p></li>
</ul>

<h3>Title: Utilizing Generative Adversarial Networks for Stable Structure Generation in Angry Birds. (arXiv:2309.02614v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02614">http://arxiv.org/abs/2309.02614</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02614]] Utilizing Generative Adversarial Networks for Stable Structure Generation in Angry Birds(http://arxiv.org/abs/2309.02614)</code></li>
<li>Summary: <p>This paper investigates the suitability of using Generative Adversarial
Networks (GANs) to generate stable structures for the physics-based puzzle game
Angry Birds. While previous applications of GANs for level generation have been
mostly limited to tile-based representations, this paper explores their
suitability for creating stable structures made from multiple smaller blocks.
This includes a detailed encoding/decoding process for converting between Angry
Birds level descriptions and a suitable grid-based representation, as well as
utilizing state-of-the-art GAN architectures and training methods to produce
new structure designs. Our results show that GANs can be successfully applied
to generate a varied range of complex and stable Angry Birds structures.
</p></li>
</ul>

<h3>Title: Generative Algorithms for Fusion of Physics-Based Wildfire Spread Models with Satellite Data for Initializing Wildfire Forecasts. (arXiv:2309.02615v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02615">http://arxiv.org/abs/2309.02615</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02615]] Generative Algorithms for Fusion of Physics-Based Wildfire Spread Models with Satellite Data for Initializing Wildfire Forecasts(http://arxiv.org/abs/2309.02615)</code></li>
<li>Summary: <p>Increases in wildfire activity and the resulting impacts have prompted the
development of high-resolution wildfire behavior models for forecasting fire
spread. Recent progress in using satellites to detect fire locations further
provides the opportunity to use measurements to improve fire spread forecasts
from numerical models through data assimilation. This work develops a method
for inferring the history of a wildfire from satellite measurements, providing
the necessary information to initialize coupled atmosphere-wildfire models from
a measured wildfire state in a physics-informed approach. The fire arrival
time, which is the time the fire reaches a given spatial location, acts as a
succinct representation of the history of a wildfire. In this work, a
conditional Wasserstein Generative Adversarial Network (cWGAN), trained with
WRF-SFIRE simulations, is used to infer the fire arrival time from satellite
active fire data. The cWGAN is used to produce samples of likely fire arrival
times from the conditional distribution of arrival times given satellite active
fire detections. Samples produced by the cWGAN are further used to assess the
uncertainty of predictions. The cWGAN is tested on four California wildfires
occurring between 2020 and 2022, and predictions for fire extent are compared
against high resolution airborne infrared measurements. Further, the predicted
ignition times are compared with reported ignition times. An average Sorensen's
coefficient of 0.81 for the fire perimeters and an average ignition time error
of 32 minutes suggest that the method is highly accurate.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Automating Behavioral Testing in Machine Translation. (arXiv:2309.02553v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02553">http://arxiv.org/abs/2309.02553</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02553]] Automating Behavioral Testing in Machine Translation(http://arxiv.org/abs/2309.02553)</code></li>
<li>Summary: <p>Behavioral testing in NLP allows fine-grained evaluation of systems by
examining their linguistic capabilities through the analysis of input-output
behavior. Unfortunately, existing work on behavioral testing in Machine
Translation (MT) is currently restricted to largely handcrafted tests covering
a limited range of capabilities and languages. To address this limitation, we
propose to use Large Language Models (LLMs) to generate a diverse set of source
sentences tailored to test the behavior of MT models in a range of situations.
We can then verify whether the MT model exhibits the expected behavior through
matching candidate sets that are also generated using LLMs. Our approach aims
to make behavioral testing of MT systems practical while requiring only minimal
human effort. In our experiments, we apply our proposed evaluation framework to
assess multiple available MT systems, revealing that while in general
pass-rates follow the trends observable from traditional accuracy-based
metrics, our method was able to uncover several important differences and
potential bugs that go unnoticed when relying only on accuracy.
</p></li>
</ul>

<h3>Title: Zero-Resource Hallucination Prevention for Large Language Models. (arXiv:2309.02654v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02654">http://arxiv.org/abs/2309.02654</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02654]] Zero-Resource Hallucination Prevention for Large Language Models(http://arxiv.org/abs/2309.02654)</code></li>
<li>Summary: <p>The prevalent use of large language models (LLMs) in various domains has
drawn attention to the issue of "hallucination," which refers to instances
where LLMs generate factually inaccurate or ungrounded information. Existing
techniques for hallucination detection in language assistants rely on intricate
fuzzy, specific free-language-based chain of thought (CoT) techniques or
parameter-based methods that suffer from interpretability issues. Additionally,
the methods that identify hallucinations post-generation could not prevent
their occurrence and suffer from inconsistent performance due to the influence
of the instruction format and model style. In this paper, we introduce a novel
pre-detection self-evaluation technique, referred to as {\method}, which
focuses on evaluating the model's familiarity with the concepts present in the
input instruction and withholding the generation of response in case of
unfamiliar concepts. This approach emulates the human ability to refrain from
responding to unfamiliar topics, thus reducing hallucinations. We validate
{\method} across four different large language models, demonstrating
consistently superior performance compared to existing techniques. Our findings
propose a significant shift towards preemptive strategies for hallucination
mitigation in LLM assistants, promising improvements in reliability,
applicability, and interpretability.
</p></li>
</ul>

<h3>Title: HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models. (arXiv:2309.02706v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02706">http://arxiv.org/abs/2309.02706</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02706]] HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models(http://arxiv.org/abs/2309.02706)</code></li>
<li>Summary: <p>Large Language Models (LLMs) pretrained on massive corpora exhibit remarkable
capabilities across a wide range of tasks, however, the attention given to
non-English languages has been limited in this field of research. To address
this gap and assess the proficiency of language models in the Korean language
and culture, we present HAE-RAE Bench, covering 6 tasks including vocabulary,
history, and general knowledge. Our evaluation of language models on this
benchmark highlights the potential advantages of employing Large
Language-Specific Models(LLSMs) over a comprehensive, universal model like
GPT-3.5. Remarkably, our study reveals that models approximately 13 times
smaller than GPT-3.5 can exhibit similar performance levels in terms of
language-specific knowledge retrieval. This observation underscores the
importance of homogeneous corpora for training professional-level
language-specific models. On the contrary, we also observe a perplexing
performance dip in these smaller LMs when they are tasked to generate
structured answers.
</p></li>
</ul>

<h3>Title: Large Language Models for Automated Open-domain Scientific Hypotheses Discovery. (arXiv:2309.02726v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02726">http://arxiv.org/abs/2309.02726</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02726]] Large Language Models for Automated Open-domain Scientific Hypotheses Discovery(http://arxiv.org/abs/2309.02726)</code></li>
<li>Summary: <p>Hypothetical induction is recognized as the main reasoning type when
scientists make observations about the world and try to propose hypotheses to
explain those observations. Past research on hypothetical induction has a
limited setting that (1) the observation annotations of the dataset are not raw
web corpus but are manually selected sentences (resulting in a close-domain
setting); and (2) the ground truth hypotheses annotations are mostly
commonsense knowledge, making the task less challenging. In this work, we
propose the first NLP dataset for social science academic hypotheses discovery,
consisting of 50 recent papers published in top social science journals. Raw
web corpora that are necessary for developing hypotheses in the published
papers are also collected in the dataset, with the final goal of creating a
system that automatically generates valid, novel, and helpful (to human
researchers) hypotheses, given only a pile of raw web corpora. The new dataset
can tackle the previous problems because it requires to (1) use raw web corpora
as observations; and (2) propose hypotheses even new to humanity. A
multi-module framework is developed for the task, as well as three different
feedback mechanisms that empirically show performance gain over the base
framework. Finally, our framework exhibits high performance in terms of both
GPT-4 based evaluation and social science expert evaluation.
</p></li>
</ul>

<h3>Title: Norm Tweaking: High-performance Low-bit Quantization of Large Language Models. (arXiv:2309.02784v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02784">http://arxiv.org/abs/2309.02784</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02784]] Norm Tweaking: High-performance Low-bit Quantization of Large Language Models(http://arxiv.org/abs/2309.02784)</code></li>
<li>Summary: <p>As the size of large language models (LLMs) continues to grow, model
compression without sacrificing accuracy has become a crucial challenge for
deployment. While some quantization methods, such as GPTQ, have made progress
in achieving acceptable 4-bit weight-only quantization, attempts at lower bit
quantization often result in severe performance degradation. In this paper, we
introduce a technique called norm tweaking, which can be used as a plugin in
current PTQ methods to achieve high precision while being cost-efficient. Our
approach is inspired by the observation that rectifying the quantized
activation distribution to match its float counterpart can readily restore
accuracy for LLMs. To achieve this, we carefully design a tweaking strategy
that includes calibration data generation and channel-wise distance constraint
to update the weights of normalization layers for better generalization. We
conduct extensive experiments on various datasets using several open-sourced
LLMs. Our method demonstrates significant improvements in both weight-only
quantization and joint quantization of weights and activations, surpassing
existing PTQ methods. On GLM-130B and OPT-66B, our method even achieves the
same level of accuracy at 2-bit quantization as their float ones. Our simple
and effective approach makes it more practical for real-world applications.
</p></li>
</ul>

<h3>Title: Aligning Large Language Models for Clinical Tasks. (arXiv:2309.02884v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02884">http://arxiv.org/abs/2309.02884</a></li>
<li>Code URL: https://github.com/ssm123ssm/medGPT</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02884]] Aligning Large Language Models for Clinical Tasks(http://arxiv.org/abs/2309.02884)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have demonstrated remarkable adaptability,
showcasing their capacity to excel in tasks for which they were not explicitly
trained. However, despite their impressive natural language processing (NLP)
capabilities, effective alignment of LLMs remains a crucial challenge when
deploying them for specific clinical applications. The ability to generate
responses with factually accurate content and to engage in non-trivial
reasoning steps are crucial for the LLMs to be eligible for applications in
clinical medicine. Employing a combination of techniques including
instruction-tuning and in-prompt strategies like few-shot and chain of thought
prompting has significantly enhanced the performance of LLMs. Our proposed
alignment strategy for medical question-answering, known as
'expand-guess-refine', offers a parameter and data-efficient solution. A
preliminary analysis of this method demonstrated outstanding performance,
achieving a score of 70.63% on a subset of questions sourced from the USMLE
dataset.
</p></li>
</ul>

<h3>Title: Everyone Deserves A Reward: Learning Customized Human Preferences. (arXiv:2309.03126v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03126">http://arxiv.org/abs/2309.03126</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03126]] Everyone Deserves A Reward: Learning Customized Human Preferences(http://arxiv.org/abs/2309.03126)</code></li>
<li>Summary: <p>Reward models (RMs) are crucial in aligning large language models (LLMs) with
human preferences for improving interaction quality. However, the real world is
pluralistic, which leads to diversified human preferences based on different
religions, politics, cultures, etc. Moreover, each individual can have their
own unique preferences on various topics. Neglecting the diversity of human
preferences, current LLM training processes only use a general reward model,
which is below satisfaction for customized or personalized application
scenarios. To explore customized preference learning, we collect a
domain-specific preference (DSP) dataset, which collects preferred responses to
each given query from four practical domains. Besides, from the perspective of
data efficiency, we proposed a three-stage customized RM learning scheme, whose
effectiveness is empirically verified on both general preference datasets and
our DSP set. Furthermore, we test multiple training and data strategies on the
three learning stages, and have found several ways to better preserve the
general preferring ability while training the customized RMs, especially
general preference enrichment and customized preference imitation learning. The
DSP dataset and code are available at https://github.com/Linear95/DSP.
</p></li>
</ul>

<h3>Title: Gender-specific Machine Translation with Large Language Models. (arXiv:2309.03175v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03175">http://arxiv.org/abs/2309.03175</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03175]] Gender-specific Machine Translation with Large Language Models(http://arxiv.org/abs/2309.03175)</code></li>
<li>Summary: <p>Decoder-only Large Language Models (LLMs) have demonstrated potential in
machine translation (MT), albeit with performance slightly lagging behind
traditional encoder-decoder Neural Machine Translation (NMT) systems. However,
LLMs offer a unique advantage: the ability to control the properties of the
output through prompts. In this study, we harness this flexibility to explore
LLaMa's capability to produce gender-specific translations for languages with
grammatical gender. Our results indicate that LLaMa can generate
gender-specific translations with competitive accuracy and gender bias
mitigation when compared to NLLB, a state-of-the-art multilingual NMT system.
Furthermore, our experiments reveal that LLaMa's translations are robust,
showing significant performance drops when evaluated against opposite-gender
references in gender-ambiguous datasets but maintaining consistency in less
ambiguous contexts. This research provides insights into the potential and
challenges of using LLMs for gender-specific translations and highlights the
importance of in-context learning to elicit new tasks in LLMs.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: A skeletonization algorithm for gradient-based optimization. (arXiv:2309.02527v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02527">http://arxiv.org/abs/2309.02527</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02527]] A skeletonization algorithm for gradient-based optimization(http://arxiv.org/abs/2309.02527)</code></li>
<li>Summary: <p>The skeleton of a digital image is a compact representation of its topology,
geometry, and scale. It has utility in many computer vision applications, such
as image description, segmentation, and registration. However, skeletonization
has only seen limited use in contemporary deep learning solutions. Most
existing skeletonization algorithms are not differentiable, making it
impossible to integrate them with gradient-based optimization. Compatible
algorithms based on morphological operations and neural networks have been
proposed, but their results often deviate from the geometry and topology of the
true medial axis. This work introduces the first three-dimensional
skeletonization algorithm that is both compatible with gradient-based
optimization and preserves an object's topology. Our method is exclusively
based on matrix additions and multiplications, convolutional operations, basic
non-linear functions, and sampling from a uniform probability distribution,
allowing it to be easily implemented in any major deep learning library. In
benchmarking experiments, we prove the advantages of our skeletonization
algorithm compared to non-differentiable, morphological, and
neural-network-based baselines. Finally, we demonstrate the utility of our
algorithm by integrating it with two medical image processing applications that
use gradient-based optimization: deep-learning-based blood vessel segmentation,
and multimodal registration of the mandible in computed tomography and magnetic
resonance images.
</p></li>
</ul>

<h3>Title: A Survey of the Impact of Self-Supervised Pretraining for Diagnostic Tasks with Radiological Images. (arXiv:2309.02555v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02555">http://arxiv.org/abs/2309.02555</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02555]] A Survey of the Impact of Self-Supervised Pretraining for Diagnostic Tasks with Radiological Images(http://arxiv.org/abs/2309.02555)</code></li>
<li>Summary: <p>Self-supervised pretraining has been observed to be effective at improving
feature representations for transfer learning, leveraging large amounts of
unlabelled data. This review summarizes recent research into its usage in
X-ray, computed tomography, magnetic resonance, and ultrasound imaging,
concentrating on studies that compare self-supervised pretraining to fully
supervised learning for diagnostic tasks such as classification and
segmentation. The most pertinent finding is that self-supervised pretraining
generally improves downstream task performance compared to full supervision,
most prominently when unlabelled examples greatly outnumber labelled examples.
Based on the aggregate evidence, recommendations are provided for practitioners
considering using self-supervised learning. Motivated by limitations identified
in current research, directions and practices for future study are suggested,
such as integrating clinical knowledge with theoretically justified
self-supervised learning methods, evaluating on public datasets, growing the
modest body of evidence for ultrasound, and characterizing the impact of
self-supervised pretraining on generalization.
</p></li>
</ul>

<h3>Title: Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning. (arXiv:2309.02591v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02591">http://arxiv.org/abs/2309.02591</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02591]] Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning(http://arxiv.org/abs/2309.02591)</code></li>
<li>Summary: <p>We present CM3Leon (pronounced "Chameleon"), a retrieval-augmented,
token-based, decoder-only multi-modal language model capable of generating and
infilling both text and images. CM3Leon uses the CM3 multi-modal architecture
but additionally shows the extreme benefits of scaling up and tuning on more
diverse instruction-style data. It is the first multi-modal model trained with
a recipe adapted from text-only language models, including a large-scale
retrieval-augmented pre-training stage and a second multi-task supervised
fine-tuning (SFT) stage. It is also a general-purpose model that can do both
text-to-image and image-to-text generation, allowing us to introduce
self-contained contrastive decoding methods that produce high-quality outputs.
Extensive experiments demonstrate that this recipe is highly effective for
multi-modal models. CM3Leon achieves state-of-the-art performance in
text-to-image generation with 5x less training compute than comparable methods
(zero-shot MS-COCO FID of 4.88). After SFT, CM3Leon can also demonstrate
unprecedented levels of controllability in tasks ranging from language-guided
image editing to image-controlled generation and segmentation.
</p></li>
</ul>

<h3>Title: MLN-net: A multi-source medical image segmentation method for clustered microcalcifications using multiple layer normalization. (arXiv:2309.02742v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.02742">http://arxiv.org/abs/2309.02742</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.02742]] MLN-net: A multi-source medical image segmentation method for clustered microcalcifications using multiple layer normalization(http://arxiv.org/abs/2309.02742)</code></li>
<li>Summary: <p>Accurate segmentation of clustered microcalcifications in mammography is
crucial for the diagnosis and treatment of breast cancer. Despite exhibiting
expert-level accuracy, recent deep learning advancements in medical image
segmentation provide insufficient contribution to practical applications, due
to the domain shift resulting from differences in patient postures, individual
gland density, and imaging modalities of mammography etc. In this paper, a
novel framework named MLN-net, which can accurately segment multi-source images
using only single source images, is proposed for clustered microcalcification
segmentation. We first propose a source domain image augmentation method to
generate multi-source images, leading to improved generalization. And a
structure of multiple layer normalization (LN) layers is used to construct the
segmentation network, which can be found efficient for clustered
microcalcification segmentation in different domains. Additionally, a branch
selection strategy is designed for measuring the similarity of the source
domain data and the target domain data. To validate the proposed MLN-net,
extensive analyses including ablation experiments are performed, comparison of
12 baseline methods. Extensive experiments validate the effectiveness of
MLN-net in segmenting clustered microcalcifications from different domains and
the its segmentation accuracy surpasses state-of-the-art methods. Code will be
available at https://github.com/yezanting/MLN-NET-VERSON1.
</p></li>
</ul>

<h3>Title: Sparse 3D Reconstruction via Object-Centric Ray Sampling. (arXiv:2309.03008v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03008">http://arxiv.org/abs/2309.03008</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03008]] Sparse 3D Reconstruction via Object-Centric Ray Sampling(http://arxiv.org/abs/2309.03008)</code></li>
<li>Summary: <p>We propose a novel method for 3D object reconstruction from a sparse set of
views captured from a 360-degree calibrated camera rig. We represent the object
surface through a hybrid model that uses both an MLP-based neural
representation and a triangle mesh. A key contribution in our work is a novel
object-centric sampling scheme of the neural representation, where rays are
shared among all views. This efficiently concentrates and reduces the number of
samples used to update the neural model at each iteration. This sampling scheme
relies on the mesh representation to ensure also that samples are
well-distributed along its normals. The rendering is then performed efficiently
by a differentiable renderer. We demonstrate that this sampling scheme results
in a more effective training of the neural representation, does not require the
additional supervision of segmentation masks, yields state of the art 3D
reconstructions, and works with sparse views on the Google's Scanned Objects,
Tank and Temples and MVMC Car datasets.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
