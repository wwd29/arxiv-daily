<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-09</h1>
<h3>Title: Towards Log Analysis with AI Agents: Cowrie Case Study</h3>
<ul>
<li><strong>Authors: </strong>Enis Karaarslan, Esin Güler, Efe Emir Yüce, Cagatay Coban</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05306">https://arxiv.org/abs/2509.05306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05306">https://arxiv.org/pdf/2509.05306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05306]] Towards Log Analysis with AI Agents: Cowrie Case Study(https://arxiv.org/abs/2509.05306)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>The scarcity of real-world attack data significantly hinders progress in cybersecurity research and education. Although honeypots like Cowrie effectively collect live threat intelligence, they generate overwhelming volumes of unstructured and heterogeneous logs, rendering manual analysis impractical. As a first step in our project on secure and efficient AI automation, this study explores the use of AI agents for automated log analysis. We present a lightweight and automated approach to process Cowrie honeypot logs. Our approach leverages AI agents to intelligently parse, summarize, and extract insights from raw data, while also considering the security implications of deploying such an autonomous system. Preliminary results demonstrate the pipeline's effectiveness in reducing manual effort and identifying attack patterns, paving the way for more advanced autonomous cybersecurity analysis in future work.</li>
</ul>

<h3>Title: Large Language Model Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations</h3>
<ul>
<li><strong>Authors: </strong>Konur Tholl, François Rivest, Mariam El Mezouar, Ranwa Al Mallah</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05311">https://arxiv.org/abs/2509.05311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05311">https://arxiv.org/pdf/2509.05311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05311]] Large Language Model Integration with Reinforcement Learning to Augment Decision-Making in Autonomous Cyber Operations(https://arxiv.org/abs/2509.05311)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has shown great potential for autonomous decision-making in the cybersecurity domain, enabling agents to learn through direct environment interaction. However, RL agents in Autonomous Cyber Operations (ACO) typically learn from scratch, requiring them to execute undesirable actions to learn their consequences. In this study, we integrate external knowledge in the form of a Large Language Model (LLM) pretrained on cybersecurity data that our RL agent can directly leverage to make informed decisions. By guiding initial training with an LLM, we improve baseline performance and reduce the need for exploratory actions with obviously negative outcomes. We evaluate our LLM-integrated approach in a simulated cybersecurity environment, and demonstrate that our guided agent achieves over 2x higher rewards during early training and converges to a favorable policy approximately 4,500 episodes faster than the baseline.</li>
</ul>

<h3>Title: Standard vs. Modular Sampling: Best Practices for Reliable LLM Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Praveen Bushipaka, Lucia Passaro, Tommaso Cucinotta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05316">https://arxiv.org/abs/2509.05316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05316">https://arxiv.org/pdf/2509.05316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05316]] Standard vs. Modular Sampling: Best Practices for Reliable LLM Unlearning(https://arxiv.org/abs/2509.05316)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>A conventional LLM Unlearning setting consists of two subsets -"forget" and "retain", with the objectives of removing the undesired knowledge from the forget set while preserving the remaining knowledge from the retain. In privacy-focused unlearning research, a retain set is often further divided into neighbor sets, containing either directly or indirectly connected to the forget targets; and augmented by a general-knowledge set. A common practice in existing benchmarks is to employ only a single neighbor set, with general knowledge which fails to reflect the real-world data complexities and relationships. LLM Unlearning typically involves 1:1 sampling or cyclic iteration sampling. However, the efficacy and stability of these de facto standards have not been critically examined. In this study, we systematically evaluate these common practices. Our findings reveal that relying on a single neighbor set is suboptimal and that a standard sampling approach can obscure performance trade-offs. Based on this analysis, we propose and validate an initial set of best practices: (1) Incorporation of diverse neighbor sets to balance forget efficacy and model utility, (2) Standard 1:1 sampling methods are inefficient and yield poor results, (3) Our proposed Modular Entity-Level Unlearning (MELU) strategy as an alternative to cyclic sampling. We demonstrate that this modular approach, combined with robust algorithms, provides a clear and stable path towards effective unlearning.</li>
</ul>

<h3>Title: Backdoor Samples Detection Based on Perturbation Discrepancy Consistency in Pre-trained Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zuquan Peng, Jianming Fu, Lixin Zou, Li Zheng, Yanzhen Ren, Guojun Peng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05318">https://arxiv.org/abs/2509.05318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05318">https://arxiv.org/pdf/2509.05318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05318]] Backdoor Samples Detection Based on Perturbation Discrepancy Consistency in Pre-trained Language Models(https://arxiv.org/abs/2509.05318)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>The use of unvetted third-party and internet data renders pre-trained models susceptible to backdoor attacks. Detecting backdoor samples is critical to prevent backdoor activation during inference or injection during training. However, existing detection methods often require the defender to have access to the poisoned models, extra clean samples, or significant computational resources to detect backdoor samples, limiting their practicality. To address this limitation, we propose a backdoor sample detection method based on perturbatio\textbf{N} discr\textbf{E}pancy consis\textbf{T}ency \textbf{E}valuation (\NETE). This is a novel detection method that can be used both pre-training and post-training phases. In the detection process, it only requires an off-the-shelf pre-trained model to compute the log probability of samples and an automated function based on a mask-filling strategy to generate perturbations. Our method is based on the interesting phenomenon that the change in perturbation discrepancy for backdoor samples is smaller than that for clean samples. Based on this phenomenon, we use curvature to measure the discrepancy in log probabilities between different perturbed samples and input samples, thereby evaluating the consistency of the perturbation discrepancy to determine whether the input sample is a backdoor sample. Experiments conducted on four typical backdoor attacks and five types of large language model backdoor attacks demonstrate that our detection strategy outperforms existing zero-shot black-box detection methods.</li>
</ul>

<h3>Title: Privacy-Preserving Offloading for Large Language Models in 6G Vehicular Networks</h3>
<ul>
<li><strong>Authors: </strong>Ikhlasse Badidi, Nouhaila El Khiyaoui, Aya Riany, Badr Ben Elallid, Amine Abouaomar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05320">https://arxiv.org/abs/2509.05320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05320">https://arxiv.org/pdf/2509.05320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05320]] Privacy-Preserving Offloading for Large Language Models in 6G Vehicular Networks(https://arxiv.org/abs/2509.05320)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, federate, large language model</a></li>
<li><strong>Abstract: </strong>The integration of Large Language Models (LLMs) in 6G vehicular networks promises unprecedented advancements in intelligent transportation systems. However, offloading LLM computations from vehicles to edge infrastructure poses significant privacy risks, potentially exposing sensitive user data. This paper presents a novel privacy-preserving offloading framework for LLM-integrated vehicular networks. We introduce a hybrid approach combining federated learning (FL) and differential privacy (DP) techniques to protect user data while maintaining LLM performance. Our framework includes a privacy-aware task partitioning algorithm that optimizes the trade-off between local and edge computation, considering both privacy constraints and system efficiency. We also propose a secure communication protocol for transmitting model updates and aggregating results across the network. Experimental results demonstrate that our approach achieves 75\% global accuracy with only a 2-3\% reduction compared to non-privacy-preserving methods, while maintaining DP guarantees with an optimal privacy budget of $\varepsilon = 0.8$. The framework shows stable communication overhead of approximately 2.1MB per round with computation comprising over 90\% of total processing time, validating its efficiency for resource-constrained vehicular environments.</li>
</ul>

<h3>Title: A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD</h3>
<ul>
<li><strong>Authors: </strong>Yunfei Guo, Tao Zhang, Wu Huang, Yao Song</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05321">https://arxiv.org/abs/2509.05321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05321">https://arxiv.org/pdf/2509.05321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05321]] A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD(https://arxiv.org/abs/2509.05321)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion, that leverages the SEED-VD dataset to generate a multimodal dataset of EEG signals conditioned on video stimuli. Additionally, we disclose an engineering pipeline for aligning video and EEG data pairs, facilitating the training of multimodal large models with EEG alignment capabilities. Personalized EEG signals are generated using a self-play graph network (SPGN) integrated with a diffusion model. As a major contribution, we release a new dataset comprising over 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG signals at 200 Hz and emotion labels, enabling video-EEG alignment and advancing multimodal research. This framework offers novel tools for emotion analysis, data augmentation, and brain-computer interface applications, with substantial research and engineering significance.</li>
</ul>

<h3>Title: Zero-Knowledge Proofs in Sublinear Space</h3>
<ul>
<li><strong>Authors: </strong>Logan Nye</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05326">https://arxiv.org/abs/2509.05326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05326">https://arxiv.org/pdf/2509.05326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05326]] Zero-Knowledge Proofs in Sublinear Space(https://arxiv.org/abs/2509.05326)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Modern zero-knowledge proof (ZKP) systems, essential for privacy and verifiable computation, suffer from a fundamental limitation: the prover typically uses memory that scales linearly with the computation's trace length T, making them impractical for resource-constrained devices and prohibitively expensive for large-scale tasks. This paper overcomes this barrier by constructing, to our knowledge, the first sublinear-space ZKP prover. Our core contribution is an equivalence that reframes proof generation as an instance of the classic Tree Evaluation problem. Leveraging a recent space-efficient tree-evaluation algorithm, we design a streaming prover that assembles the proof without ever materializing the full execution trace. The approach reduces prover memory from linear in T to O(sqrt(T)) (up to O(log T) lower-order terms) while preserving proof size, verifier time, and the transcript/security guarantees of the underlying system. This enables a shift from specialized, server-bound proving to on-device proving, opening applications in decentralized systems, on-device machine learning, and privacy-preserving technologies.</li>
</ul>

<h3>Title: Feed Two Birds with One Scone: Exploiting Function-Space Regularization for Both OOD Robustness and ID Fine-Tuning Performance</h3>
<ul>
<li><strong>Authors: </strong>Xiang Yuan, Jun Shu, Deyu meng, Zongben Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05328">https://arxiv.org/abs/2509.05328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05328">https://arxiv.org/pdf/2509.05328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05328]] Feed Two Birds with One Scone: Exploiting Function-Space Regularization for Both OOD Robustness and ID Fine-Tuning Performance(https://arxiv.org/abs/2509.05328)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Robust fine-tuning aims to achieve competitive in-distribution (ID) performance while maintaining the out-of-distribution (OOD) robustness of a pre-trained model when transferring it to a downstream task. To remedy this, most robust fine-tuning methods aim to preserve the pretrained weights, features, or logits. However, we find that these methods cannot always improve OOD robustness for different model architectures. This is due to the OOD robustness requiring the model function to produce stable prediction for input information of downstream tasks, while existing methods might serve as a poor proxy for the optimization in the function space. Based on this finding, we propose a novel regularization that constrains the distance of fine-tuning and pre-trained model in the function space with the simulated OOD samples, aiming to preserve the OOD robustness of the pre-trained model. Besides, to further enhance the OOD robustness capability of the fine-tuning model, we introduce an additional consistency regularization to promote stable predictions of perturbed samples. Extensive experiments demonstrate our approach could consistently improve both downstream task ID fine-tuning performance and OOD robustness across a variety of CLIP backbones, outperforming existing regularization-based robust fine-tuning methods.</li>
</ul>

<h3>Title: ForensicsData: A Digital Forensics Dataset for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Youssef Chakir, Iyad Lahsen-Cherif</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05331">https://arxiv.org/abs/2509.05331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05331">https://arxiv.org/pdf/2509.05331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05331]] ForensicsData: A Digital Forensics Dataset for Large Language Models(https://arxiv.org/abs/2509.05331)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>The growing complexity of cyber incidents presents significant challenges for digital forensic investigators, especially in evidence collection and analysis. Public resources are still limited because of ethical, legal, and privacy concerns, even though realistic datasets are necessary to support research and tool developments. To address this gap, we introduce ForensicsData, an extensive Question-Context-Answer (Q-C-A) dataset sourced from actual malware analysis reports. It consists of more than 5,000 Q-C-A triplets. A unique workflow was used to create the dataset, which extracts structured data, uses large language models (LLMs) to transform it into Q-C-A format, and then uses a specialized evaluation process to confirm its quality. Among the models evaluated, Gemini 2 Flash demonstrated the best performance in aligning generated content with forensic terminology. ForensicsData aims to advance digital forensics by enabling reproducible experiments and fostering collaboration within the research community.</li>
</ul>

<h3>Title: Integrated Simulation Framework for Adversarial Attacks on Autonomous Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Christos Anagnostopoulos, Ioulia Kapsali, Alexandros Gkillas, Nikos Piperigkos, Aris S. Lalos</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05332">https://arxiv.org/abs/2509.05332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05332">https://arxiv.org/pdf/2509.05332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05332]] Integrated Simulation Framework for Adversarial Attacks on Autonomous Vehicles(https://arxiv.org/abs/2509.05332)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Autonomous vehicles (AVs) rely on complex perception and communication systems, making them vulnerable to adversarial attacks that can compromise safety. While simulation offers a scalable and safe environment for robustness testing, existing frameworks typically lack comprehensive supportfor modeling multi-domain adversarial scenarios. This paper introduces a novel, open-source integrated simulation framework designed to generate adversarial attacks targeting both perception and communication layers of AVs. The framework provides high-fidelity modeling of physical environments, traffic dynamics, and V2X networking, orchestrating these components through a unified core that synchronizes multiple simulators based on a single configuration file. Our implementation supports diverse perception-level attacks on LiDAR sensor data, along with communication-level threats such as V2X message manipulation and GPS spoofing. Furthermore, ROS 2 integration ensures seamless compatibility with third-party AV software stacks. We demonstrate the framework's effectiveness by evaluating the impact of generated adversarial scenarios on a state-of-the-art 3D object detector, revealing significant performance degradation under realistic conditions.</li>
</ul>

<h3>Title: RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness</h3>
<ul>
<li><strong>Authors: </strong>Junghyun Park, Tuan Anh Nguyen, Dugki Min</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05333">https://arxiv.org/abs/2509.05333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05333">https://arxiv.org/pdf/2509.05333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05333]] RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness(https://arxiv.org/abs/2509.05333)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Real world deployments often expose modern object recognition models to domain shifts that precipitate a severe drop in accuracy. Such shifts encompass (i) variations in low level image statistics, (ii) changes in object pose and viewpoint, (iii) partial occlusion, and (iv) visual confusion across adjacent classes. To mitigate this degradation, we introduce the Re-Thinking Vision Language Model (RT-VLM) framework. The foundation of this framework is a unique synthetic dataset generation pipeline that produces images annotated with "4-Clues": precise bounding boxes, class names, detailed object-level captions, and a comprehensive context-level caption for the entire scene. We then perform parameter efficient supervised tuning of Llama 3.2 11B Vision Instruct on this resource. At inference time, a two stage Re-Thinking scheme is executed: the model first emits its own four clues, then re examines these responses as evidence and iteratively corrects them. Across robustness benchmarks that isolate individual domain shifts, RT-VLM consistently surpasses strong baselines. These findings indicate that the integration of structured multimodal evidence with an explicit self critique loop constitutes a promising route toward reliable and transferable visual understanding.</li>
</ul>

<h3>Title: A Real-Time, Vision-Based System for Badminton Smash Speed Estimation on Mobile Devices</h3>
<ul>
<li><strong>Authors: </strong>Diwen Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05334">https://arxiv.org/abs/2509.05334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05334">https://arxiv.org/pdf/2509.05334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05334]] A Real-Time, Vision-Based System for Badminton Smash Speed Estimation on Mobile Devices(https://arxiv.org/abs/2509.05334)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Performance metrics in sports, such as shot speed and angle, provide crucial feedback for athlete development. However, the technology to capture these metrics has historically been expensive, complex, and largely inaccessible to amateur and recreational players. This paper addresses this gap in the context of badminton, one of the world's most popular sports, by introducing a novel, cost-effective, and user-friendly system for measuring smash speed using ubiquitous smartphone technology. Our approach leverages a custom-trained YOLOv5 model for shuttlecock detection, combined with a Kalman filter for robust trajectory tracking. By implementing a video-based kinematic speed estimation method with spatiotemporal scaling, the system automatically calculates the shuttlecock's velocity from a standard video recording. The entire process is packaged into an intuitive mobile application, democratizing access to high-level performance analytics and empowering players at all levels to analyze and improve their game.</li>
</ul>

<h3>Title: Comparative Evaluation of Hard and Soft Clustering for Precise Brain Tumor Segmentation in MR Imaging</h3>
<ul>
<li><strong>Authors: </strong>Dibya Jyoti Bora, Mrinal Kanti Mishra</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05340">https://arxiv.org/abs/2509.05340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05340">https://arxiv.org/pdf/2509.05340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05340]] Comparative Evaluation of Hard and Soft Clustering for Precise Brain Tumor Segmentation in MR Imaging(https://arxiv.org/abs/2509.05340)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Segmentation of brain tumors from Magnetic Resonance Imaging (MRI) remains a pivotal challenge in medical image analysis due to the heterogeneous nature of tumor morphology and intensity distributions. Accurate delineation of tumor boundaries is critical for clinical decision-making, radiotherapy planning, and longitudinal disease monitoring. In this study, we perform a comprehensive comparative analysis of two major clustering paradigms applied in MRI tumor segmentation: hard clustering, exemplified by the K-Means algorithm, and soft clustering, represented by Fuzzy C-Means (FCM). While K-Means assigns each pixel strictly to a single cluster, FCM introduces partial memberships, meaning each pixel can belong to multiple clusters with varying degrees of association. Experimental validation was performed using the BraTS2020 dataset, incorporating pre-processing through Gaussian filtering and Contrast Limited Adaptive Histogram Equalization (CLAHE). Evaluation metrics included the Dice Similarity Coefficient (DSC) and processing time, which collectively demonstrated that K-Means achieved superior speed with an average runtime of 0.3s per image, whereas FCM attained higher segmentation accuracy with an average DSC of 0.67 compared to 0.43 for K-Means, albeit at a higher computational cost (1.3s per image). These results highlight the inherent trade-off between computational efficiency and boundary precision.</li>
</ul>

<h3>Title: Handling imbalance and few-sample size in ML based Onion disease classification</h3>
<ul>
<li><strong>Authors: </strong>Abhijeet Manoj Pal, Rajbabu Velmurugan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05341">https://arxiv.org/abs/2509.05341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05341">https://arxiv.org/pdf/2509.05341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05341]] Handling imbalance and few-sample size in ML based Onion disease classification(https://arxiv.org/abs/2509.05341)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate classification of pests and diseases plays a vital role in precision agriculture, enabling efficient identification, targeted interventions, and preventing their further spread. However, current methods primarily focus on binary classification, which limits their practical applications, especially in scenarios where accurately identifying the specific type of disease or pest is essential. We propose a robust deep learning based model for multi-class classification of onion crop diseases and pests. We enhance a pre-trained Convolutional Neural Network (CNN) model by integrating attention based modules and employing comprehensive data augmentation pipeline to mitigate class imbalance. We propose a model which gives 96.90% overall accuracy and 0.96 F1 score on real-world field image dataset. This model gives better results than other approaches using the same datasets.</li>
</ul>

<h3>Title: Delta Velocity Rectified Flow for Text-to-Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Gaspard Beaudouin, Minghan Li, Jaeyeon Kim, Sunghoon Yoon, Mengyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05342">https://arxiv.org/abs/2509.05342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05342">https://arxiv.org/pdf/2509.05342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05342]] Delta Velocity Rectified Flow for Text-to-Image Editing(https://arxiv.org/abs/2509.05342)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose Delta Velocity Rectified Flow (DVRF), a novel inversion-free, path-aware editing framework within rectified flow models for text-to-image editing. DVRF is a distillation-based method that explicitly models the discrepancy between the source and target velocity fields in order to mitigate over-smoothing artifacts rampant in prior distillation sampling approaches. We further introduce a time-dependent shift term to push noisy latents closer to the target trajectory, enhancing the alignment with the target distribution. We theoretically demonstrate that when this shift is disabled, DVRF reduces to Delta Denoising Score, thereby bridging score-based diffusion optimization and velocity-based rectified-flow optimization. Moreover, when the shift term follows a linear schedule under rectified-flow dynamics, DVRF generalizes the Inversion-free method FlowEdit and provides a principled theoretical interpretation for it. Experimental results indicate that DVRF achieves superior editing quality, fidelity, and controllability while requiring no architectural modifications, making it efficient and broadly applicable to text-to-image editing tasks. Code is available at this https URL.</li>
</ul>

<h3>Title: Systematic Integration of Attention Modules into CNNs for Accurate and Generalizable Medical Image Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Zahid Ullah, Minki Hong, Tahir Mahmood, Jihie Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05343">https://arxiv.org/abs/2509.05343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05343">https://arxiv.org/pdf/2509.05343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05343]] Systematic Integration of Attention Modules into CNNs for Accurate and Generalizable Medical Image Diagnosis(https://arxiv.org/abs/2509.05343)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning has become a powerful tool for medical image analysis; however, conventional Convolutional Neural Networks (CNNs) often fail to capture the fine-grained and complex features critical for accurate diagnosis. To address this limitation, we systematically integrate attention mechanisms into five widely adopted CNN architectures, namely, VGG16, ResNet18, InceptionV3, DenseNet121, and EfficientNetB5, to enhance their ability to focus on salient regions and improve discriminative performance. Specifically, each baseline model is augmented with either a Squeeze and Excitation block or a hybrid Convolutional Block Attention Module, allowing adaptive recalibration of channel and spatial feature representations. The proposed models are evaluated on two distinct medical imaging datasets, a brain tumor MRI dataset comprising multiple tumor subtypes, and a Products of Conception histopathological dataset containing four tissue categories. Experimental results demonstrate that attention augmented CNNs consistently outperform baseline architectures across all metrics. In particular, EfficientNetB5 with hybrid attention achieves the highest overall performance, delivering substantial gains on both datasets. Beyond improved classification accuracy, attention mechanisms enhance feature localization, leading to better generalization across heterogeneous imaging modalities. This work contributes a systematic comparative framework for embedding attention modules in diverse CNN architectures and rigorously assesses their impact across multiple medical imaging tasks. The findings provide practical insights for the development of robust, interpretable, and clinically applicable deep learning based decision support systems.</li>
</ul>

<h3>Title: Vision-Based Object Detection for UAV Solar Panel Inspection Using an Enhanced Defects Dataset</h3>
<ul>
<li><strong>Authors: </strong>Ashen Rodrigo, Isuru Munasinghe, Asanka Perera</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05348">https://arxiv.org/abs/2509.05348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05348">https://arxiv.org/pdf/2509.05348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05348]] Vision-Based Object Detection for UAV Solar Panel Inspection Using an Enhanced Defects Dataset(https://arxiv.org/abs/2509.05348)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Timely and accurate detection of defects and contaminants in solar panels is critical for maintaining the efficiency and reliability of photovoltaic systems. This study presents a comprehensive evaluation of five state-of-the-art object detection models: YOLOv3, Faster R-CNN, RetinaNet, EfficientDet, and Swin Transformer, for identifying physical and electrical defects as well as surface contaminants such as dust, dirt, and bird droppings on solar panels. A custom dataset, annotated in the COCO format and specifically designed for solar panel defect and contamination detection, was developed alongside a user interface to train and evaluate the models. The performance of each model is assessed and compared based on mean Average Precision (mAP), precision, recall, and inference speed. The results demonstrate the trade-offs between detection accuracy and computational efficiency, highlighting the relative strengths and limitations of each model. These findings provide valuable guidance for selecting appropriate detection approaches in practical solar panel monitoring and maintenance scenarios. The dataset will be publicly available at this https URL.</li>
</ul>

<h3>Title: Ensembling Membership Inference Attacks Against Tabular Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Joshua Ward, Yuxuan Yang, Chi-Hua Wang, Guang Cheng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05350">https://arxiv.org/abs/2509.05350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05350">https://arxiv.org/pdf/2509.05350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05350]] Ensembling Membership Inference Attacks Against Tabular Generative Models(https://arxiv.org/abs/2509.05350)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, membership infer, generative</a></li>
<li><strong>Abstract: </strong>Membership Inference Attacks (MIAs) have emerged as a principled framework for auditing the privacy of synthetic data generated by tabular generative models, where many diverse methods have been proposed that each exploit different privacy leakage signals. However, in realistic threat scenarios, an adversary must choose a single method without a priori guarantee that it will be the empirically highest performing option. We study this challenge as a decision theoretic problem under uncertainty and conduct the largest synthetic data privacy benchmark to date. Here, we find that no MIA constitutes a strictly dominant strategy across a wide variety of model architectures and dataset domains under our threat model. Motivated by these findings, we propose ensemble MIAs and show that unsupervised ensembles built on individual attacks offer empirically more robust, regret-minimizing strategies than individual attacks.</li>
</ul>

<h3>Title: Unsupervised Instance Segmentation with Superpixels</h3>
<ul>
<li><strong>Authors: </strong>Cuong Manh Hoang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05352">https://arxiv.org/abs/2509.05352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05352">https://arxiv.org/pdf/2509.05352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05352]] Unsupervised Instance Segmentation with Superpixels(https://arxiv.org/abs/2509.05352)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Instance segmentation is essential for numerous computer vision applications, including robotics, human-computer interaction, and autonomous driving. Currently, popular models bring impressive performance in instance segmentation by training with a large number of human annotations, which are costly to collect. For this reason, we present a new framework that efficiently and effectively segments objects without the need for human annotations. Firstly, a MultiCut algorithm is applied to self-supervised features for coarse mask segmentation. Then, a mask filter is employed to obtain high-quality coarse masks. To train the segmentation network, we compute a novel superpixel-guided mask loss, comprising hard loss and soft loss, with high-quality coarse masks and superpixels segmented from low-level image features. Lastly, a self-training process with a new adaptive loss is proposed to improve the quality of predicted masks. We conduct experiments on public datasets in instance segmentation and object detection to demonstrate the effectiveness of the proposed framework. The results show that the proposed framework outperforms previous state-of-the-art methods.</li>
</ul>

<h3>Title: An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Yanis Labrak, Richard Dufour, Mickaël Rouvier</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05359">https://arxiv.org/abs/2509.05359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05359">https://arxiv.org/pdf/2509.05359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05359]] An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training(https://arxiv.org/abs/2509.05359)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper investigates discrete unit representations in Speech Language Models (SLMs), focusing on optimizing speech modeling during continual pre-training. In this paper, we systematically examine how model architecture, data representation, and training robustness influence the pre-training stage in which we adapt existing pre-trained language models to the speech modality. Our experiments highlight the role of speech encoders and clustering granularity across different model scales, showing how optimal discretization strategies vary with model capacity. By examining cluster distribution and phonemic alignments, we investigate the effective use of discrete vocabulary, uncovering both linguistic and paralinguistic patterns. Additionally, we explore the impact of clustering data selection on model robustness, highlighting the importance of domain matching between discretization training and target applications.</li>
</ul>

<h3>Title: Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection</h3>
<ul>
<li><strong>Authors: </strong>Jerry Li, Evangelos Papalexakis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05360">https://arxiv.org/abs/2509.05360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05360">https://arxiv.org/pdf/2509.05360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05360]] Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection(https://arxiv.org/abs/2509.05360)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated effectiveness across a wide variety of tasks involving natural language, however, a fundamental problem of hallucinations still plagues these models, limiting their trustworthiness in generating consistent, truthful information. Detecting hallucinations has quickly become an important topic, with various methods such as uncertainty estimation, LLM Judges, retrieval augmented generation (RAG), and consistency checks showing promise. Many of these methods build upon foundational metrics, such as ROUGE, BERTScore, or Perplexity, which often lack the semantic depth necessary to detect hallucinations effectively. In this work, we propose a novel approach inspired by ROUGE that constructs an N-Gram frequency tensor from LLM-generated text. This tensor captures richer semantic structure by encoding co-occurrence patterns, enabling better differentiation between factual and hallucinated content. We demonstrate this by applying tensor decomposition methods to extract singular values from each mode and use these as input features to train a multi-layer perceptron (MLP) binary classifier for hallucinations. Our method is evaluated on the HaluEval dataset and demonstrates significant improvements over traditional baselines, as well as competitive performance against state-of-the-art LLM judges.</li>
</ul>

<h3>Title: AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and Conversational Scambaiting by Leveraging LLMs and Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Ismail Hossain, Sai Puppala, Sajedul Talukder, Md Jahangir Alam</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05362">https://arxiv.org/abs/2509.05362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05362">https://arxiv.org/pdf/2509.05362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05362]] AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and Conversational Scambaiting by Leveraging LLMs and Federated Learning(https://arxiv.org/abs/2509.05362)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense, robust, federate</a></li>
<li><strong>Abstract: </strong>Scams exploiting real-time social engineering -- such as phishing, impersonation, and phone fraud -- remain a persistent and evolving threat across digital platforms. Existing defenses are largely reactive, offering limited protection during active interactions. We propose a privacy-preserving, AI-in-the-loop framework that proactively detects and disrupts scam conversations in real time. The system combines instruction-tuned artificial intelligence with a safety-aware utility function that balances engagement with harm minimization, and employs federated learning to enable continual model updates without raw data sharing. Experimental evaluations show that the system produces fluent and engaging responses (perplexity as low as 22.3, engagement $\approx$0.80), while human studies confirm significant gains in realism, safety, and effectiveness over strong baselines. In federated settings, models trained with FedAvg sustain up to 30 rounds while preserving high engagement ($\approx$0.80), strong relevance ($\approx$0.74), and low PII leakage ($\leq$0.0085). Even with differential privacy, novelty and safety remain stable, indicating that robust privacy can be achieved without sacrificing performance. The evaluation of guard models (LlamaGuard, LlamaGuard2/3, MD-Judge) shows a straightforward pattern: stricter moderation settings reduce the chance of exposing personal information, but they also limit how much the model engages in conversation. In contrast, more relaxed settings allow longer and richer interactions, which improve scam detection, but at the cost of higher privacy risk. To our knowledge, this is the first framework to unify real-time scam-baiting, federated privacy preservation, and calibrated safety moderation into a proactive defense paradigm.</li>
</ul>

<h3>Title: A Framework for Detection and Classification of Attacks on Surveillance Cameras under IoT Networks</h3>
<ul>
<li><strong>Authors: </strong>Umair Amjid, M. Umar Khan, S. A. Manan Kirmani</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05366">https://arxiv.org/abs/2509.05366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05366">https://arxiv.org/pdf/2509.05366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05366]] A Framework for Detection and Classification of Attacks on Surveillance Cameras under IoT Networks(https://arxiv.org/abs/2509.05366)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>The increasing use of Internet of Things (IoT) devices has led to a rise in security related concerns regarding IoT Networks. The surveillance cameras in IoT networks are vulnerable to security threats such as brute force and zero-day attacks which can lead to unauthorized access by hackers and potential spying on the users activities. Moreover, these cameras can be targeted by Denial of Service (DOS) attacks, which will make it unavailable for the user. The proposed AI based framework will leverage machine learning algorithms to analyze network traffic and detect anomalous behavior, allowing for quick detection and response to potential intrusions. The framework will be trained and evaluated using real-world datasets to learn from past security incidents and improve its ability to detect potential intrusion.</li>
</ul>

<h3>Title: Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shei Pern Chua, Thai Zhen Leng, Teh Kai Jun, Xiao Li, Xiaolin Hu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05367">https://arxiv.org/abs/2509.05367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05367">https://arxiv.org/pdf/2509.05367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05367]] Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs(https://arxiv.org/abs/2509.05367)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have undergone safety alignment efforts to mitigate harmful outputs. However, as LLMs become more sophisticated in reasoning, their intelligence may introduce new security risks. While traditional jailbreak attacks relied on singlestep attacks, multi-turn jailbreak strategies that adapt dynamically to context remain underexplored. In this work, we introduce TRIAL (Trolley-problem Reasoning for Interactive Attack Logic), a framework that leverages LLMs ethical reasoning to bypass their safeguards. TRIAL embeds adversarial goals within ethical dilemmas modeled on the trolley problem. TRIAL demonstrates high jailbreak success rates towards both open and close-source models. Our findings underscore a fundamental limitation in AI safety: as models gain advanced reasoning abilities, the nature of their alignment may inadvertently allow for more covert security vulnerabilities to be exploited. TRIAL raises an urgent need in reevaluating safety alignment oversight strategies, as current safeguards may prove insufficient against context-aware adversarial attack.</li>
</ul>

<h3>Title: Quantum AI Algorithm Development for Enhanced Cybersecurity: A Hybrid Approach to Malware Detection</h3>
<ul>
<li><strong>Authors: </strong>Tanya Joshi, Krishnendu Guha</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05370">https://arxiv.org/abs/2509.05370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05370">https://arxiv.org/pdf/2509.05370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05370]] Quantum AI Algorithm Development for Enhanced Cybersecurity: A Hybrid Approach to Malware Detection(https://arxiv.org/abs/2509.05370)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, extraction</a></li>
<li><strong>Abstract: </strong>This study explores the application of quantum machine learning (QML) algorithms to enhance cybersecurity threat detection, particularly in the classification of malware and intrusion detection within high-dimensional datasets. Classical machine learning approaches encounter limitations when dealing with intricate, obfuscated malware patterns and extensive network intrusion data. To address these challenges, we implement and evaluate various QML algorithms, including Quantum Neural Networks (QNN), Quantum Support Vector Machines (QSVM), and hybrid Quantum Convolutional Neural Networks (QCNN) for malware detection tasks. Our experimental analysis utilized two datasets: the Intrusion dataset, comprising 150 samples with 56 memory-based features derived from Volatility framework analysis, and the ObfuscatedMalMem2022 dataset, containing 58,596 samples with 57 features representing benign and malicious software. Remarkably, our QML methods demonstrated superior performance compared to classical approaches, achieving accuracies of 95% for QNN and 94% for QSVM. These quantum-enhanced methods leveraged quantum superposition and entanglement principles to accurately identify complex patterns within highly obfuscated malware samples that were imperceptible to classical methods. To further advance malware analysis, we propose a novel real-time malware analysis framework that incorporates Quantum Feature Extraction using Quantum Fourier Transform, Quantum Feature Maps, and Classification using Variational Quantum Circuits. This system integrates explainable AI methods, including GradCAM++ and ScoreCAM algorithms, to provide interpretable insights into the quantum decision-making processes.</li>
</ul>

<h3>Title: Privacy Preservation and Identity Tracing Prevention in AI-Driven Eye Tracking for Interactive Learning Environments</h3>
<ul>
<li><strong>Authors: </strong>Abdul Rehman, Are Dæhlen, Ilona Heldal, Jerry Chun-wei Lin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05376">https://arxiv.org/abs/2509.05376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05376">https://arxiv.org/pdf/2509.05376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05376]] Privacy Preservation and Identity Tracing Prevention in AI-Driven Eye Tracking for Interactive Learning Environments(https://arxiv.org/abs/2509.05376)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, federate</a></li>
<li><strong>Abstract: </strong>Eye-tracking technology can aid in understanding neurodevelopmental disorders and tracing a person's identity. However, this technology poses a significant risk to privacy, as it captures sensitive information about individuals and increases the likelihood that data can be traced back to them. This paper proposes a human-centered framework designed to prevent identity backtracking while preserving the pedagogical benefits of AI-powered eye tracking in interactive learning environments. We explore how real-time data anonymization, ethical design principles, and regulatory compliance (such as GDPR) can be integrated to build trust and transparency. We first demonstrate the potential for backtracking student IDs and diagnoses in various scenarios using serious game-based eye-tracking data. We then provide a two-stage privacy-preserving framework that prevents participants from being tracked while still enabling diagnostic classification. The first phase covers four scenarios: I) Predicting disorder diagnoses based on different game levels. II) Predicting student IDs based on different game levels. III) Predicting student IDs based on randomized data. IV) Utilizing K-Means for out-of-sample data. In the second phase, we present a two-stage framework that preserves privacy. We also employ Federated Learning (FL) across multiple clients, incorporating a secure identity management system with dummy IDs and administrator-only access controls. In the first phase, the proposed framework achieved 99.3% accuracy for scenario 1, 63% accuracy for scenario 2, and 99.7% accuracy for scenario 3, successfully identifying and assigning a new student ID in scenario 4. In phase 2, we effectively prevented backtracking and established a secure identity management system with dummy IDs and administrator-only access controls, achieving an overall accuracy of 99.40%.</li>
</ul>

<h3>Title: ThreatGPT: An Agentic AI Framework for Enhancing Public Safety through Threat Modeling</h3>
<ul>
<li><strong>Authors: </strong>Sharif Noor Zisad, Ragib Hasan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05379">https://arxiv.org/abs/2509.05379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05379">https://arxiv.org/pdf/2509.05379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05379]] ThreatGPT: An Agentic AI Framework for Enhancing Public Safety through Threat Modeling(https://arxiv.org/abs/2509.05379)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>As our cities and communities become smarter, the systems that keep us safe, such as traffic control centers, emergency response networks, and public transportation, also become more complex. With this complexity comes a greater risk of security threats that can affect not just machines but real people's lives. To address this challenge, we present ThreatGPT, an agentic Artificial Intelligence (AI) assistant built to help people whether they are engineers, safety officers, or policy makers to understand and analyze threats in public safety systems. Instead of requiring deep cybersecurity expertise, it allows users to simply describe the components of a system they are concerned about, such as login systems, data storage, or communication networks. Then, with the click of a button, users can choose how they want the system to be analyzed by using popular frameworks such as STRIDE, MITRE ATT&CK, CVE reports, NIST, or CISA. ThreatGPT is unique because it does not just provide threat information, but rather it acts like a knowledgeable partner. Using few-shot learning, the AI learns from examples and generates relevant smart threat models. It can highlight what might go wrong, how attackers could take advantage, and what can be done to prevent harm. Whether securing a city's infrastructure or a local health service, this tool adapts to users' needs. In simple terms, ThreatGPT brings together AI and human judgment to make our public systems safer. It is designed not just to analyze threats, but to empower people to understand and act on them, faster, smarter, and with more confidence.</li>
</ul>

<h3>Title: A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Wei, Faguo Wu, Xiao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05385">https://arxiv.org/abs/2509.05385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05385">https://arxiv.org/pdf/2509.05385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05385]] A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs(https://arxiv.org/abs/2509.05385)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models are unable to continuously adapt and learn from new data during reasoning at inference time. To address this limitation, we propose that complex reasoning tasks be decomposed into atomic subtasks and introduce SAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive updates during reasoning at inference time. SAGE consists of three key components: (1) a Trigger module that detects reasoning failures through multiple evaluation metrics in real time; (2) a Trigger Buffer module that clusters anomaly samples using a streaming clustering process with HDBSCAN, followed by stability checks and similarity-based merging; and (3) a Lora Store module that dynamically optimizes parameter updates with an adapter pool for knowledge retention. Evaluation results show that SAGE demonstrates excellent accuracy, robustness, and stability on the atomic reasoning subtask through dynamic knowledge updating during test time.</li>
</ul>

<h3>Title: Safeguarding Graph Neural Networks against Topology Inference Attacks</h3>
<ul>
<li><strong>Authors: </strong>Jie Fu, Hong Yuan, Zhili Chen, Wendy Hui Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05429">https://arxiv.org/abs/2509.05429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05429">https://arxiv.org/pdf/2509.05429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05429]] Safeguarding Graph Neural Networks against Topology Inference Attacks(https://arxiv.org/abs/2509.05429)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense, attack</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have emerged as powerful models for learning from graph-structured data. However, their widespread adoption has raised serious privacy concerns. While prior research has primarily focused on edge-level privacy, a critical yet underexplored threat lies in topology privacy - the confidentiality of the graph's overall structure. In this work, we present a comprehensive study on topology privacy risks in GNNs, revealing their vulnerability to graph-level inference attacks. To this end, we propose a suite of Topology Inference Attacks (TIAs) that can reconstruct the structure of a target training graph using only black-box access to a GNN model. Our findings show that GNNs are highly susceptible to these attacks, and that existing edge-level differential privacy mechanisms are insufficient as they either fail to mitigate the risk or severely compromise model accuracy. To address this challenge, we introduce Private Graph Reconstruction (PGR), a novel defense framework designed to protect topology privacy while maintaining model accuracy. PGR is formulated as a bi-level optimization problem, where a synthetic training graph is iteratively generated using meta-gradients, and the GNN model is concurrently updated based on the evolving graph. Extensive experiments demonstrate that PGR significantly reduces topology leakage with minimal impact on model accuracy. Our code is anonymously available at this https URL.</li>
</ul>

<h3>Title: Advanced Brain Tumor Segmentation Using EMCAD: Efficient Multi-scale Convolutional Attention Decoding</h3>
<ul>
<li><strong>Authors: </strong>GodsGift Uzor, Tania-Amanda Nkoyo Fredrick Eneye, Chukwuebuka Ijezue</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05431">https://arxiv.org/abs/2509.05431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05431">https://arxiv.org/pdf/2509.05431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05431]] Advanced Brain Tumor Segmentation Using EMCAD: Efficient Multi-scale Convolutional Attention Decoding(https://arxiv.org/abs/2509.05431)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Brain tumor segmentation is a critical pre-processing step in the medical image analysis pipeline that involves precise delineation of tumor regions from healthy brain tissue in medical imaging data, particularly MRI scans. An efficient and effective decoding mechanism is crucial in brain tumor segmentation especially in scenarios with limited computational resources. However these decoding mechanisms usually come with high computational costs. To address this concern EMCAD a new efficient multi-scale convolutional attention decoder designed was utilized to optimize both performance and computational efficiency for brain tumor segmentation on the BraTs2020 dataset consisting of MRI scans from 369 brain tumor patients. The preliminary result obtained by the model achieved a best Dice score of 0.31 and maintained a stable mean Dice score of 0.285 plus/minus 0.015 throughout the training process which is moderate. The initial model maintained consistent performance across the validation set without showing signs of over-fitting.</li>
</ul>

<h3>Title: FAVAE-Effective Frequency Aware Latent Tokenizer</h3>
<ul>
<li><strong>Authors: </strong>Tejaswini Medi, Hsien-Yi Wang, Arianna Rampini, Margret Keuper</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05441">https://arxiv.org/abs/2509.05441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05441">https://arxiv.org/pdf/2509.05441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05441]] FAVAE-Effective Frequency Aware Latent Tokenizer(https://arxiv.org/abs/2509.05441)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Latent generative models have shown remarkable progress in high-fidelity image synthesis, typically using a two-stage training process that involves compressing images into latent embeddings via learned tokenizers in the first stage. The quality of generation strongly depends on how expressive and well-optimized these latent embeddings are. While various methods have been proposed to learn effective latent representations, the reconstructed images often lack realism, particularly in textured regions with sharp transitions, due to loss of fine details governed by high frequencies. We conduct a detailed frequency decomposition of existing state-of-the-art (SOTA) latent tokenizers and show that conventional objectives inherently prioritize low-frequency reconstruction, often at the expense of high-frequency fidelity. Our analysis reveals these latent tokenizers exhibit a bias toward low-frequency information, when jointly optimized, leading to over-smoothed outputs and visual artifacts that diminish perceptual quality. To address this, we propose a wavelet-based, frequency-aware variational autoencoder (FA-VAE) framework that explicitly decouples the optimization of low- and high-frequency components. This decoupling enables improved reconstruction of fine textures while preserving global structure. Our approach bridges the fidelity gap in current latent tokenizers and emphasizes the importance of frequency-aware optimization for realistic image representation, with broader implications for applications in content creation, neural rendering, and medical imaging.</li>
</ul>

<h3>Title: Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis</h3>
<ul>
<li><strong>Authors: </strong>Disha Makhija, Manoj Ghuhan Arivazhagan, Vinayshekhar Bannihatti Kumar, Rashmi Gangadharaiah</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05449">https://arxiv.org/abs/2509.05449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05449">https://arxiv.org/pdf/2509.05449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05449]] Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis(https://arxiv.org/abs/2509.05449)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, robust, membership infer, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Membership inference attacks (MIAs) reveal whether specific data was used to train machine learning models, serving as important tools for privacy auditing and compliance assessment. Recent studies have reported that MIAs perform only marginally better than random guessing against large language models, suggesting that modern pre-training approaches with massive datasets may be free from privacy leakage risks. Our work offers a complementary perspective to these findings by exploring how examining LLMs' internal representations, rather than just their outputs, may provide additional insights into potential membership inference signals. Our framework, \emph{memTrace}, follows what we call \enquote{neural breadcrumbs} extracting informative signals from transformer hidden states and attention patterns as they process candidate sequences. By analyzing layer-wise representation dynamics, attention distribution characteristics, and cross-layer transition patterns, we detect potential memorization fingerprints that traditional loss-based approaches may not capture. This approach yields strong membership detection across several model families achieving average AUC scores of 0.85 on popular MIA benchmarks. Our findings suggest that internal model behaviors can reveal aspects of training data exposure even when output-based signals appear protected, highlighting the need for further research into membership privacy and the development of more robust privacy-preserving training techniques for large language models.</li>
</ul>

<h3>Title: Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Youjia Zheng, Mohammad Zandsalimy, Shanu Sushmita</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05471">https://arxiv.org/abs/2509.05471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05471">https://arxiv.org/pdf/2509.05471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05471]] Behind the Mask: Benchmarking Camouflaged Jailbreaks in Large Language Models(https://arxiv.org/abs/2509.05471)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly vulnerable to a sophisticated form of adversarial prompting known as camouflaged jailbreaking. This method embeds malicious intent within seemingly benign language to evade existing safety mechanisms. Unlike overt attacks, these subtle prompts exploit contextual ambiguity and the flexible nature of language, posing significant challenges to current defense systems. This paper investigates the construction and impact of camouflaged jailbreak prompts, emphasizing their deceptive characteristics and the limitations of traditional keyword-based detection methods. We introduce a novel benchmark dataset, Camouflaged Jailbreak Prompts, containing 500 curated examples (400 harmful and 100 benign prompts) designed to rigorously stress-test LLM safety protocols. In addition, we propose a multi-faceted evaluation framework that measures harmfulness across seven dimensions: Safety Awareness, Technical Feasibility, Implementation Safeguards, Harmful Potential, Educational Value, Content Quality, and Compliance Score. Our findings reveal a stark contrast in LLM behavior: while models demonstrate high safety and content quality with benign inputs, they exhibit a significant decline in performance and safety when confronted with camouflaged jailbreak attempts. This disparity underscores a pervasive vulnerability, highlighting the urgent need for more nuanced and adaptive security strategies to ensure the responsible and robust deployment of LLMs in real-world applications.</li>
</ul>

<h3>Title: Veriserum: A dual-plane fluoroscopic dataset with knee implant phantoms for deep learning in medical imaging</h3>
<ul>
<li><strong>Authors: </strong>Jinhao Wang, Florian Vogl, Pascal Schütz, Saša Ćuković, William R. Taylor</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05483">https://arxiv.org/abs/2509.05483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05483">https://arxiv.org/pdf/2509.05483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05483]] Veriserum: A dual-plane fluoroscopic dataset with knee implant phantoms for deep learning in medical imaging(https://arxiv.org/abs/2509.05483)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Veriserum is an open-source dataset designed to support the training of deep learning registration for dual-plane fluoroscopic analysis. It comprises approximately 110,000 X-ray images of 10 knee implant pair combinations (2 femur and 5 tibia implants) captured during 1,600 trials, incorporating poses associated with daily activities such as level gait and ramp descent. Each image is annotated with an automatically registered ground-truth pose, while 200 images include manually registered poses for benchmarking. Key features of Veriserum include dual-plane images and calibration tools. The dataset aims to support the development of applications such as 2D/3D image registration, image segmentation, X-ray distortion correction, and 3D reconstruction. Freely accessible, Veriserum aims to advance computer vision and medical imaging research by providing a reproducible benchmark for algorithm development and evaluation. The Veriserum dataset used in this study is publicly available via this https URL, with the data stored at ETH Zürich Research Collections: this https URL.</li>
</ul>

<h3>Title: From Staff Messages to Actionable Insights: A Multi-Stage LLM Classification Framework for Healthcare Analytics</h3>
<ul>
<li><strong>Authors: </strong>Hajar Sakai, Yi-En Tseng, Mohammadsadegh Mikaeili, Joshua Bosire, Franziska Jovin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05484">https://arxiv.org/abs/2509.05484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05484">https://arxiv.org/pdf/2509.05484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05484]] From Staff Messages to Actionable Insights: A Multi-Stage LLM Classification Framework for Healthcare Analytics(https://arxiv.org/abs/2509.05484)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Hospital call centers serve as the primary contact point for patients within a hospital system. They also generate substantial volumes of staff messages as navigators process patient requests and communicate with the hospital offices following the established protocol restrictions and guidelines. This continuously accumulated large amount of text data can be mined and processed to retrieve insights; however, traditional supervised learning approaches require annotated data, extensive training, and model tuning. Large Language Models (LLMs) offer a paradigm shift toward more computationally efficient methodologies for healthcare analytics. This paper presents a multi-stage LLM-based framework that identifies staff message topics and classifies messages by their reasons in a multi-class fashion. In the process, multiple LLM types, including reasoning, general-purpose, and lightweight models, were evaluated. The best-performing model was o3, achieving 78.4% weighted F1-score and 79.2% accuracy, followed closely by gpt-5 (75.3% Weighted F1-score and 76.2% accuracy). The proposed methodology incorporates data security measures and HIPAA compliance requirements essential for healthcare environments. The processed LLM outputs are integrated into a visualization decision support tool that transforms the staff messages into actionable insights accessible to healthcare professionals. This approach enables more efficient utilization of the collected staff messaging data, identifies navigator training opportunities, and supports improved patient experience and care quality.</li>
</ul>

<h3>Title: Prior Distribution and Model Confidence</h3>
<ul>
<li><strong>Authors: </strong>Maksim Kazanskii, Artem Kasianov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05485">https://arxiv.org/abs/2509.05485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05485">https://arxiv.org/pdf/2509.05485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05485]] Prior Distribution and Model Confidence(https://arxiv.org/abs/2509.05485)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper investigates the impact of training data distribution on the performance of image classification models. By analyzing the embeddings of the training set, we propose a framework to understand the confidence of model predictions on unseen data without the need for retraining. Our approach filters out low-confidence predictions based on their distance from the training distribution in the embedding space, significantly improving classification accuracy. We demonstrate this on the example of several classification models, showing consistent performance gains across architectures. Furthermore, we show that using multiple embedding models to represent the training data enables a more robust estimation of confidence, as different embeddings capture complementary aspects of the data. Combining these embeddings allows for better detection and exclusion of out-of-distribution samples, resulting in further accuracy improvements. The proposed method is model-agnostic and generalizable, with potential applications beyond computer vision, including domains such as Natural Language Processing where prediction reliability is critical.</li>
</ul>

<h3>Title: The Token Tax: Systematic Bias in Multilingual Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Jessica M. Lundin, Ada Zhang, Nihal Karim, Hamza Louzan, Victor Wei, David Adelani, Cody Carroll</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05486">https://arxiv.org/abs/2509.05486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05486">https://arxiv.org/pdf/2509.05486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05486]] The Token Tax: Systematic Bias in Multilingual Tokenization(https://arxiv.org/abs/2509.05486)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Tokenization inefficiency imposes structural disadvantages on morphologically complex, low-resource languages, inflating compute resources and depressing accuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA items; 5 subjects; 16 African languages) and show that fertility (tokens/word) reliably predicts accuracy. Higher fertility consistently predicts lower accuracy across all models and subjects. We further find that reasoning models (DeepSeek, o1) consistently outperform non-reasoning peers across high and low resource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in prior generations. Finally, translating token inflation to economics, a doubling in tokens results in quadrupled training cost and time, underscoring the token tax faced by many languages. These results motivate morphologically aware tokenization, fair pricing, and multilingual benchmarks for equitable natural language processing (NLP).</li>
</ul>

<h3>Title: Self-Aligned Reward: Towards Effective and Efficient Reasoners</h3>
<ul>
<li><strong>Authors: </strong>Peixuan Han, Adit Krishnan, Gerald Friedland, Jiaxuan You, Chris Kong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05489">https://arxiv.org/abs/2509.05489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05489">https://arxiv.org/pdf/2509.05489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05489]] Self-Aligned Reward: Towards Effective and Efficient Reasoners(https://arxiv.org/abs/2509.05489)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning with verifiable rewards has significantly advanced reasoning in large language models (LLMs), but such signals remain coarse, offering only binary correctness feedback. This limitation often results in inefficiencies, including overly verbose reasoning and high computational cost, while existing solutions often compromise accuracy. To address this, we introduce self-aligned reward (SAR), a self-guided signal that complements verifiable rewards to encourage both reasoning accuracy and efficiency. SAR is defined as the relative perplexity difference between an answer conditioned on the query and the standalone answer, thereby favoring responses that are concise and query-specific. Quantitative analysis reveals that SAR reliably distinguishes answer quality: concise, correct answers score higher than redundant ones, and partially correct answers score higher than entirely incorrect ones. Evaluation on 4 models across 7 benchmarks shows that integrating SAR with prevalent RL algorithms like PPO and GRPO improves accuracy by 4%, while reducing inference cost by 30%. Further analysis demonstrates that SAR achieves a Pareto-optimal trade-off between correctness and efficiency compared to reward signals based on length or self-confidence. We also show that SAR shortens responses while preserving advanced reasoning behaviors, demonstrating its ability to suppress unnecessary elaboration without losing critical reasoning. These results highlight the promise of self-aligned reward as a fine-grained complement to verifiable rewards, paving the way for more efficient and effective LLM training.</li>
</ul>

<h3>Title: What is Cybersecurity in Space?</h3>
<ul>
<li><strong>Authors: </strong>Charbel Mattar, Jacques Bou Abdo, Abdallah Makhoul, Benoit Piranda, Jacques Demerjian</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05496">https://arxiv.org/abs/2509.05496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05496">https://arxiv.org/pdf/2509.05496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05496]] What is Cybersecurity in Space?(https://arxiv.org/abs/2509.05496)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense</a></li>
<li><strong>Abstract: </strong>Satellites, drones, and 5G space links now support critical services such as air traffic, finance, and weather. Yet most were not built to resist modern cyber threats. Ground stations can be breached, GPS jammed, and supply chains compromised, while no shared list of vulnerabilities or safe testing range exists. This paper maps eleven research gaps, including secure routing, onboard intrusion detection, recovery methods, trusted supply chains, post-quantum encryption, zero-trust architectures, and real-time impact monitoring. For each, we outline the challenge, why it matters, and a guiding research question. We also highlight an agentic (multi-agent) AI approach where small, task-specific agents share defense tasks onboard instead of one large model. Finally, we propose a five-year roadmap: post-quantum and QKD flight trials, open cyber-ranges, clearer vulnerability shar ing, and early multi-agent deployments. These steps move space cybersecurity from reactive patching toward proactive resilience.</li>
</ul>

<h3>Title: Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)</h3>
<ul>
<li><strong>Authors: </strong>Mansi Garg, Lee-Chi Wang, Bhavesh Ghanchi, Sanjana Dumpala, Shreyash Kakde, Yen Chih Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05505">https://arxiv.org/abs/2509.05505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05505">https://arxiv.org/pdf/2509.05505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05505]] Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)(https://arxiv.org/abs/2509.05505)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>This work presents a Biomedical Literature Question Answering (Q&A) system based on a Retrieval-Augmented Generation (RAG) architecture, designed to improve access to accurate, evidence-based medical information. Addressing the shortcomings of conventional health search engines and the lag in public access to biomedical research, the system integrates diverse sources, including PubMed articles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant information and generate concise, context-aware responses. The retrieval pipeline uses MiniLM-based semantic embeddings and FAISS vector search, while answer generation is performed by a fine-tuned Mistral-7B-v0.3 language model optimized using QLoRA for efficient, low-resource training. The system supports both general medical queries and domain-specific tasks, with a focused evaluation on breast cancer literature demonstrating the value of domain-aligned retrieval. Empirical results, measured using BERTScore (F1), show substantial improvements in factual consistency and semantic relevance compared to baseline models. The findings underscore the potential of RAG-enhanced language models to bridge the gap between complex biomedical literature and accessible public health knowledge, paving the way for future work on multilingual adaptation, privacy-preserving inference, and personalized medical AI systems.</li>
</ul>

<h3>Title: Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation in 3D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Sen Wang, Kunyi Li, Siyun Liang, Elena Alegret, Jing Ma, Nassir Navab, Stefano Gasperini</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05515">https://arxiv.org/abs/2509.05515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05515">https://arxiv.org/pdf/2509.05515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05515]] Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation in 3D Gaussian Splatting(https://arxiv.org/abs/2509.05515)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Recently, distilling open-vocabulary language features from 2D images into 3D Gaussians has attracted significant attention. Although existing methods achieve impressive language-based interactions of 3D scenes, we observe two fundamental issues: background Gaussians contributing negligibly to a rendered pixel get the same feature as the dominant foreground ones, and multi-view inconsistencies due to view-specific noise in language embeddings. We introduce Visibility-Aware Language Aggregation (VALA), a lightweight yet effective method that computes marginal contributions for each ray and applies a visibility-aware gate to retain only visible Gaussians. Moreover, we propose a streaming weighted geometric median in cosine space to merge noisy multi-view features. Our method yields a robust, view-consistent language feature embedding in a fast and memory-efficient manner. VALA improves open-vocabulary localization and segmentation across reference datasets, consistently surpassing existing works.</li>
</ul>

<h3>Title: DuoCLR: Dual-Surrogate Contrastive Learning for Skeleton-based Human Action Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Haitao Tian, Pierre Payeur</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05543">https://arxiv.org/abs/2509.05543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05543">https://arxiv.org/pdf/2509.05543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05543]] DuoCLR: Dual-Surrogate Contrastive Learning for Skeleton-based Human Action Segmentation(https://arxiv.org/abs/2509.05543)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, a contrastive representation learning framework is proposed to enhance human action segmentation via pre-training using trimmed (single action) skeleton sequences. Unlike previous representation learning works that are tailored for action recognition and that build upon isolated sequence-wise representations, the proposed framework focuses on exploiting multi-scale representations in conjunction with cross-sequence variations. More specifically, it proposes a novel data augmentation strategy, 'Shuffle and Warp', which exploits diverse multi-action permutations. The latter effectively assists two surrogate tasks that are introduced in contrastive learning: Cross Permutation Contrasting (CPC) and Relative Order Reasoning (ROR). In optimization, CPC learns intra-class similarities by contrasting representations of the same action class across different permutations, while ROR reasons about inter-class contexts by predicting relative mapping between two permutations. Together, these tasks enable a Dual-Surrogate Contrastive Learning (DuoCLR) network to learn multi-scale feature representations optimized for action segmentation. In experiments, DuoCLR is pre-trained on a trimmed skeleton dataset and evaluated on an untrimmed dataset where it demonstrates a significant boost over state-the-art comparatives in both multi-class and multi-label action segmentation tasks. Lastly, ablation studies are conducted to evaluate the effectiveness of each component of the proposed approach.</li>
</ul>

<h3>Title: Secure and Efficient $L^p$-Norm Computation for Two-Party Learning Applications</h3>
<ul>
<li><strong>Authors: </strong>Ali Arastehfard, Weiran Liu, Joshua Lee, Bingyu Liu, Xuegang Ban, Yuan Hong</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05552">https://arxiv.org/abs/2509.05552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05552">https://arxiv.org/pdf/2509.05552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05552]] Secure and Efficient $L^p$-Norm Computation for Two-Party Learning Applications(https://arxiv.org/abs/2509.05552)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>Secure norm computation is becoming increasingly important in many real-world learning applications. However, existing cryptographic systems often lack a general framework for securely computing the $L^p$-norm over private inputs held by different parties. These systems often treat secure norm computation as a black-box process, neglecting to design tailored cryptographic protocols that optimize performance. Moreover, they predominantly focus on the $L^2$-norm, paying little attention to other popular $L^p$-norms, such as $L^1$ and $L^\infty$, which are commonly used in practice, such as machine learning tasks and location-based services. To our best knowledge, we propose the first comprehensive framework for secure two-party $L^p$-norm computations ($L^1$, $L^2$, and $L^\infty$), denoted as \mbox{Crypto-$L^p$}, designed to be versatile across various applications. We have designed, implemented, and thoroughly evaluated our framework across a wide range of benchmarking applications, state-of-the-art (SOTA) cryptographic protocols, and real-world datasets to validate its effectiveness and practical applicability. In summary, \mbox{Crypto-$L^p$} outperforms prior works on secure $L^p$-norm computation, achieving $82\times$, $271\times$, and $42\times$ improvements in runtime while reducing communication overhead by $36\times$, $4\times$, and $21\times$ for $p=1$, $2$, and $\infty$, respectively. Furthermore, we take the first step in adapting our Crypto-$L^p$ framework for secure machine learning inference, reducing communication costs by $3\times$ compared to SOTA systems while maintaining comparable runtime and accuracy.</li>
</ul>

<h3>Title: Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study</h3>
<ul>
<li><strong>Authors: </strong>Serge Lionel Nikiema, Jordan Samhi, Micheline Bénédicte Moumoula, Albérick Euraste Djiré, Abdoul Kader Kaboré, Jacques Klein, Tegawendé F. Bissyandé</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05553">https://arxiv.org/abs/2509.05553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05553">https://arxiv.org/pdf/2509.05553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05553]] Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study(https://arxiv.org/abs/2509.05553)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This research addresses a fundamental question in AI: whether large language models truly understand concepts or simply recognize patterns. The authors propose bidirectional reasoning,the ability to apply transformations in both directions without being explicitly trained on the reverse direction, as a test for genuine understanding. They argue that true comprehension should naturally allow reversibility. For example, a model that can change a variable name like userIndex to i should also be able to infer that i represents a user index without reverse training. The researchers tested current language models and discovered what they term cognitive specialization: when models are fine-tuned on forward tasks, their performance on those tasks improves, but their ability to reason bidirectionally becomes significantly worse. To address this issue, they developed Contrastive Fine-Tuning (CFT), which trains models using three types of examples: positive examples that maintain semantic meaning, negative examples with different semantics, and forward-direction obfuscation examples. This approach aims to develop deeper understanding rather than surface-level pattern recognition and allows reverse capabilities to develop naturally without explicit reverse training. Their experiments demonstrated that CFT successfully achieved bidirectional reasoning, enabling strong reverse performance while maintaining forward task capabilities. The authors conclude that bidirectional reasoning serves both as a theoretical framework for assessing genuine understanding and as a practical training approach for developing more capable AI systems.</li>
</ul>

<h3>Title: RED: Robust Event-Guided Motion Deblurring with Modality-Specific Disentangled Representation</h3>
<ul>
<li><strong>Authors: </strong>Yihong Leng, Siming Zheng, Jinwei Chen, Bo Li, Jiaojiao Li, Peng-Tao Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05554">https://arxiv.org/abs/2509.05554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05554">https://arxiv.org/pdf/2509.05554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05554]] RED: Robust Event-Guided Motion Deblurring with Modality-Specific Disentangled Representation(https://arxiv.org/abs/2509.05554)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Event cameras provide sparse yet temporally high-temporal-resolution motion information, demonstrating great potential for motion deblurring. Existing methods focus on cross-modal interaction, overlooking the inherent incompleteness of event streams, which arises from the trade-off between sensitivity and noise introduced by the thresholding mechanism of Dynamic Vision Sensors (DVS). Such degradation compromises the integrity of motion priors and limits the effectiveness of event-guided deblurring. To tackle these challenges, we propose a Robust Event-guided Deblurring (RED) network with modality-specific disentangled representation. First, we introduce a Robustness-Oriented Perturbation Strategy (RPS) that applies random masking to events, which exposes RED to incomplete patterns and then foster robustness against various unknown scenario this http URL, a disentangled OmniAttention is presented to explicitly model intra-motion, inter-motion, and cross-modality correlations from two inherently distinct but complementary sources: blurry images and partially disrupted events. Building on these reliable features, two interactive modules are designed to enhance motion-sensitive areas in blurry images and inject semantic context into incomplete event representations. Extensive experiments on synthetic and real-world datasets demonstrate RED consistently achieves state-of-the-art performance in both accuracy and robustness.</li>
</ul>

<h3>Title: Ad hoc conventions generalize to new referents</h3>
<ul>
<li><strong>Authors: </strong>Anya Ji, Claire Augusta Bergey, Ron Eliav, Yoav Artzi, Robert D. Hawkins</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05566">https://arxiv.org/abs/2509.05566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05566">https://arxiv.org/pdf/2509.05566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05566]] Ad hoc conventions generalize to new referents(https://arxiv.org/abs/2509.05566)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>How do people talk about things they've never talked about before? One view suggests that a new shared naming system establishes an arbitrary link to a specific target, like proper names that cannot extend beyond their bearers. An alternative view proposes that forming a shared way of describing objects involves broader conceptual alignment, reshaping each individual's semantic space in ways that should generalize to new referents. We test these competing accounts in a dyadic communication study (N=302) leveraging the recently-released KiloGram dataset containing over 1,000 abstract tangram images. After pairs of participants coordinated on referential conventions for one set of images through repeated communication, we measured the extent to which their descriptions aligned for undiscussed images. We found strong evidence for generalization: partners showed increased alignment relative to their pre-test labels. Generalization also decayed nonlinearly with visual similarity (consistent with Shepard's law) and was robust across levels of the images' nameability. These findings suggest that ad hoc conventions are not arbitrary labels but reflect genuine conceptual coordination, with implications for theories of reference and the design of more adaptive language agents.</li>
</ul>

<h3>Title: ProfilingAgent: Profiling-Guided Agentic Reasoning for Adaptive Model Optimization</h3>
<ul>
<li><strong>Authors: </strong>Sadegh Jafari, Aishwarya Sarkar, Mohiuddin Bilwal, Ali Jannesari</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05584">https://arxiv.org/abs/2509.05584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05584">https://arxiv.org/pdf/2509.05584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05584]] ProfilingAgent: Profiling-Guided Agentic Reasoning for Adaptive Model Optimization(https://arxiv.org/abs/2509.05584)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Foundation models face growing compute and memory bottlenecks, hindering deployment on resource-limited platforms. While compression techniques such as pruning and quantization are widely used, most rely on uniform heuristics that ignore architectural and runtime heterogeneity. Profiling tools expose per-layer latency, memory, and compute cost, yet are rarely integrated into automated pipelines. We propose ProfilingAgent, a profiling-guided, agentic approach that uses large language models (LLMs) to automate compression via structured pruning and post-training dynamic quantization. Our modular multi-agent system reasons over static metrics (MACs, parameter counts) and dynamic signals (latency, memory) to design architecture-specific strategies. Unlike heuristic baselines, ProfilingAgent tailors layer-wise decisions to bottlenecks. Experiments on ImageNet-1K, CIFAR-10, and CIFAR-100 with ResNet-101, ViT-B/16, Swin-B, and DeiT-B/16 show pruning maintains competitive or improved accuracy (about 1% drop on ImageNet-1K, +2% gains for ViT-B/16 on smaller datasets), while quantization achieves up to 74% memory savings with <0.5% accuracy loss. Our quantization also yields consistent inference speedups of up to 1.74 times faster. Comparative studies with GPT-4o and GPT-4-Turbo highlight the importance of LLM reasoning quality for iterative pruning. These results establish agentic systems as scalable solutions for profiling-guided model optimization.</li>
</ul>

<h3>Title: MFFI: Multi-Dimensional Face Forgery Image Dataset for Real-World Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Changtao Miao, Yi Zhang, Man Luo, Weiwei Feng, Kaiyuan Zheng, Qi Chu, Tao Gong, Jianshu Li, Yunfeng Diao, Wei Zhou, Joey Tianyi Zhou, Xiaoshuai Hao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05592">https://arxiv.org/abs/2509.05592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05592">https://arxiv.org/pdf/2509.05592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05592]] MFFI: Multi-Dimensional Face Forgery Image Dataset for Real-World Scenarios(https://arxiv.org/abs/2509.05592)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Rapid advances in Artificial Intelligence Generated Content (AIGC) have enabled increasingly sophisticated face forgeries, posing a significant threat to social security. However, current Deepfake detection methods are limited by constraints in existing datasets, which lack the diversity necessary in real-world scenarios. Specifically, these data sets fall short in four key areas: unknown of advanced forgery techniques, variability of facial scenes, richness of real data, and degradation of real-world propagation. To address these challenges, we propose the Multi-dimensional Face Forgery Image (\textbf{MFFI}) dataset, tailored for real-world scenarios. MFFI enhances realism based on four strategic dimensions: 1) Wider Forgery Methods; 2) Varied Facial Scenes; 3) Diversified Authentic Data; 4) Multi-level Degradation Operations. MFFI integrates $50$ different forgery methods and contains $1024K$ image samples. Benchmark evaluations show that MFFI outperforms existing public datasets in terms of scene complexity, cross-domain generalization capability, and detection difficulty gradients. These results validate the technical advance and practical utility of MFFI in simulating real-world conditions. The dataset and additional details are publicly available at {this https URL}.</li>
</ul>

<h3>Title: Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation</h3>
<ul>
<li><strong>Authors: </strong>Hongyan Xie, Yitong Yao, Yikun Ban, Zixuan Huang, Deqing Wang, Zhenhe Wu, Haoxiang Su, Chao Wang, Shuangyong Song, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05602">https://arxiv.org/abs/2509.05602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05602">https://arxiv.org/pdf/2509.05602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05602]] Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation(https://arxiv.org/abs/2509.05602)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel at reasoning tasks but are expensive to deploy. Thus small language models (SLMs) are fine-tuned on CoT data generated by LLMs to copy LLMs' abilities. However, these CoT data may include noisy rationales that either fail to substantiate the answers or contribute no additional information to support answer prediction, which leads SLMs to capture spurious correlations between questions and answers and compromise the quality of reasoning. In this work, we propose Chain-of-Thought Correctness Perception Distillation (CoPeD), which aims to improve the reasoning quality of the student model from the perspectives of task setting and data utilization. Firstly, we introduce a correctness-aware task setting that encourages the student model to predict answers based on correct rationales and revise them when they are incorrect. This setting improves the faithfulness of reasoning and allows the model to learn from its mistakes. Then, we propose a Correctness-Aware Weighted loss, which dynamically adjusts the contribution of each training instance based on the combined loss of the rationale and the answer. This strategy encourages the model to focus more on samples where the rationale offers stronger support for the correct answer. Experiments have shown that CoPeD is effective on both in-distribution (IND) and out-of-distribution (OOD) benchmark reasoning datasets.</li>
</ul>

<h3>Title: Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation</h3>
<ul>
<li><strong>Authors: </strong>Qiyuan Chen, Hongsen Huang, Qian Shao, Jiahe Chen, Jintai Chen, Hongxia Xu, Renjie Hua, Ren Chuan, Jian Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05605">https://arxiv.org/abs/2509.05605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05605">https://arxiv.org/pdf/2509.05605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05605]] Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation(https://arxiv.org/abs/2509.05605)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) require high quality preference datasets to align with human preferences. However, conventional methods for constructing such datasets face significant challenges: reliance on pre-collected instructions often leads to distribution mismatches with target models, while the need for sampling multiple stochastic responses introduces substantial computational overhead. In this work, we explore a paradigm shift by leveraging inherent regulation of LLMs' representation space for efficient and tailored preference dataset construction, named Icon$^{2}$. Specifically, it first extracts layer-wise direction vectors to encode sophisticated human preferences and then uses these vectors to filter self-synthesized instructions based on their inherent consistency. During decoding, bidirectional inherent control is applied to steer token representations, enabling the precise generation of response pairs with clear alignment distinctions. Experimental results demonstrate significant improvements in both alignment and efficiency. Llama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on AlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by up to 48.1%.</li>
</ul>

<h3>Title: Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents</h3>
<ul>
<li><strong>Authors: </strong>Qiyuan Chen, Jiahe Chen, Hongsen Huang, Qian Shao, Jintai Chen, Renjie Hua, Hongxia Xu, Ruijia Wu, Ren Chuan, Jian Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05607">https://arxiv.org/abs/2509.05607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05607">https://arxiv.org/pdf/2509.05607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05607]] Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents(https://arxiv.org/abs/2509.05607)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The paradigm shift from traditional ranked-based search to Generative Search Engines has rendered conventional SEO metrics obsolete, creating an urgent need to understand, measure, and optimize for content influence on synthesized answers. This paper introduces a comprehensive, end-to-end framework for Generative Search Engine Optimization (GSEO) to address this challenge. We make two primary contributions. First, we construct CC-GSEO-Bench, a large-scale, content-centric benchmark, and propose a multi-dimensional evaluation framework that systematically quantifies influence, moving beyond surface-level attribution to assess substantive semantic impact. Second, we design a novel multi-agent system that operationalizes this framework, automating the strategic refinement of content through a collaborative analyze-revise-evaluate workflow. Our empirical analysis using this framework reveals novel insights into the dynamics of content influence, offering actionable strategies for creators and establishing a principled foundation for future GSEO research.</li>
</ul>

<h3>Title: Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints</h3>
<ul>
<li><strong>Authors: </strong>Waris Gill, Natalie Isak, Matthew Dressman</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05608">https://arxiv.org/abs/2509.05608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05608">https://arxiv.org/pdf/2509.05608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05608]] Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints(https://arxiv.org/abs/2509.05608)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack</a></li>
<li><strong>Abstract: </strong>The widespread deployment of LLMs across enterprise services has created a critical security blind spot. Organizations operate multiple LLM services handling billions of queries daily, yet regulatory compliance boundaries prevent these services from sharing threat intelligence about prompt injection attacks, the top security risk for LLMs. When an attack is detected in one service, the same threat may persist undetected in others for months, as privacy regulations prohibit sharing user prompts across compliance boundaries. We present BinaryShield, the first privacy-preserving threat intelligence system that enables secure sharing of attack fingerprints across compliance boundaries. BinaryShield transforms suspicious prompts through a unique pipeline combining PII redaction, semantic embedding, binary quantization, and randomized response mechanism to potentially generate non-invertible fingerprints that preserve attack patterns while providing privacy. Our evaluations demonstrate that BinaryShield achieves an F1-score of 0.94, significantly outperforming SimHash (0.77), the privacy-preserving baseline, while achieving 64x storage reduction and 38x faster similarity search compared to dense embeddings.</li>
</ul>

<h3>Title: From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics</h3>
<ul>
<li><strong>Authors: </strong>Shay Dahary, Avi Edana, Alexander Apartsin, Yehudit Aperstein</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05617">https://arxiv.org/abs/2509.05617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05617">https://arxiv.org/pdf/2509.05617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05617]] From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics(https://arxiv.org/abs/2509.05617)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The emotional content of song lyrics plays a pivotal role in shaping listener experiences and influencing musical preferences. This paper investigates the task of multi-label emotional attribution of song lyrics by predicting six emotional intensity scores corresponding to six fundamental emotions. A manually labeled dataset is constructed using a mean opinion score (MOS) approach, which aggregates annotations from multiple human raters to ensure reliable ground-truth labels. Leveraging this dataset, we conduct a comprehensive evaluation of several publicly available large language models (LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model specifically for predicting multi-label emotion scores. Experimental results reveal the relative strengths and limitations of zero-shot and fine-tuned models in capturing the nuanced emotional content of lyrics. Our findings highlight the potential of LLMs for emotion recognition in creative texts, providing insights into model selection strategies for emotion-based music information retrieval applications. The labeled dataset is available at this https URL.</li>
</ul>

<h3>Title: SuMa: A Subspace Mapping Approach for Robust and Effective Concept Erasure in Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kien Nguyen, Anh Tran, Cuong Pham</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05625">https://arxiv.org/abs/2509.05625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05625">https://arxiv.org/pdf/2509.05625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05625]] SuMa: A Subspace Mapping Approach for Robust and Effective Concept Erasure in Text-to-Image Diffusion Models(https://arxiv.org/abs/2509.05625)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>The rapid growth of text-to-image diffusion models has raised concerns about their potential misuse in generating harmful or unauthorized contents. To address these issues, several Concept Erasure methods have been proposed. However, most of them fail to achieve both robustness, i.e., the ability to robustly remove the target concept., and effectiveness, i.e., maintaining image quality. While few recent techniques successfully achieve these goals for NSFW concepts, none could handle narrow concepts such as copyrighted characters or celebrities. Erasing these narrow concepts is critical in addressing copyright and legal concerns. However, erasing them is challenging due to their close distances to non-target neighboring concepts, requiring finer-grained manipulation. In this paper, we introduce Subspace Mapping (SuMa), a novel method specifically designed to achieve both robustness and effectiveness in easing these narrow concepts. SuMa first derives a target subspace representing the concept to be erased and then neutralizes it by mapping it to a reference subspace that minimizes the distance between the two. This mapping ensures the target concept is robustly erased while preserving image quality. We conduct extensive experiments with SuMa across four tasks: subclass erasure, celebrity erasure, artistic style erasure, and instance erasure and compare the results with current state-of-the-art methods. Our method achieves image quality comparable to approaches focused on effectiveness, while also yielding results that are on par with methods targeting completeness.</li>
</ul>

<h3>Title: FuzzBox: Blending Fuzzing into Emulation for Binary-Only Embedded Targets</h3>
<ul>
<li><strong>Authors: </strong>Carmine Cesarano, Roberto Natella</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05643">https://arxiv.org/abs/2509.05643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05643">https://arxiv.org/pdf/2509.05643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05643]] FuzzBox: Blending Fuzzing into Emulation for Binary-Only Embedded Targets(https://arxiv.org/abs/2509.05643)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Coverage-guided fuzzing has been widely applied to address zero-day vulnerabilities in general-purpose software and operating systems. This approach relies on instrumenting the target code at compile time. However, applying it to industrial systems remains challenging, due to proprietary and closed-source compiler toolchains and lack of access to source code. FuzzBox addresses these limitations by integrating emulation with fuzzing: it dynamically instruments code during execution in a virtualized environment, for the injection of fuzz inputs, failure detection, and coverage analysis, without requiring source code recompilation and hardware-specific dependencies. We show the effectiveness of FuzzBox through experiments in the context of a proprietary MILS (Multiple Independent Levels of Security) hypervisor for industrial applications. Additionally, we analyze the applicability of FuzzBox across commercial IoT firmware, showcasing its broad portability.</li>
</ul>

<h3>Title: Evaluating YOLO Architectures: Implications for Real-Time Vehicle Detection in Urban Environments of Bangladesh</h3>
<ul>
<li><strong>Authors: </strong>Ha Meem Hossain, Pritam Nath, Mahitun Nesa Mahi, Imtiaz Uddin, Ishrat Jahan Eiste, Syed Nasibur Rahman Ratul, Md Naim Uddin Mozumdar, Asif Mohammed Saad</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05652">https://arxiv.org/abs/2509.05652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05652">https://arxiv.org/pdf/2509.05652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05652]] Evaluating YOLO Architectures: Implications for Real-Time Vehicle Detection in Urban Environments of Bangladesh(https://arxiv.org/abs/2509.05652)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vehicle detection systems trained on Non-Bangladeshi datasets struggle to accurately identify local vehicle types in Bangladesh's unique road environments, creating critical gaps in autonomous driving technology for developing regions. This study evaluates six YOLO model variants on a custom dataset featuring 29 distinct vehicle classes, including region-specific vehicles such as ``Desi Nosimon'', ``Leguna'', ``Battery Rickshaw'', and ``CNG''. The dataset comprises high-resolution images (1920x1080) captured across various Bangladeshi roads using mobile phone cameras and manually annotated using LabelImg with YOLO format bounding boxes. Performance evaluation revealed YOLOv11x as the top performer, achieving 63.7\% mAP@0.5, 43.8\% mAP@0.5:0.95, 61.4\% recall, and 61.6\% F1-score, though requiring 45.8 milliseconds per image for inference. Medium variants (YOLOv8m, YOLOv11m) struck an optimal balance, delivering robust detection performance with mAP@0.5 values of 62.5\% and 61.8\% respectively, while maintaining moderate inference times around 14-15 milliseconds. The study identified significant detection challenges for rare vehicle classes, with Construction Vehicles and Desi Nosimons showing near-zero accuracy due to dataset imbalances and insufficient training samples. Confusion matrices revealed frequent misclassifications between visually similar vehicles, particularly Mini Trucks versus Mini Covered Vans. This research provides a foundation for developing robust object detection systems specifically adapted to Bangladesh traffic conditions, addressing critical needs in autonomous vehicle technology advancement for developing regions where conventional generic-trained models fail to perform adequately.</li>
</ul>

<h3>Title: LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Hu, Jihao Liu, Ke Wang, Jinliang Zhen, Weikang Shi, Manyuan Zhang, Qi Dou, Rui Liu, Aojun Zhou, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05657">https://arxiv.org/abs/2509.05657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05657">https://arxiv.org/pdf/2509.05657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05657]] LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding(https://arxiv.org/abs/2509.05657)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Recent progress in Large Language Models (LLMs) has opened new avenues for solving complex optimization problems, including Neural Architecture Search (NAS). However, existing LLM-driven NAS approaches rely heavily on prompt engineering and domain-specific tuning, limiting their practicality and scalability across diverse tasks. In this work, we propose LM-Searcher, a novel framework that leverages LLMs for cross-domain neural architecture optimization without the need for extensive domain-specific adaptation. Central to our approach is NCode, a universal numerical string representation for neural architectures, which enables cross-domain architecture encoding and search. We also reformulate the NAS problem as a ranking task, training LLMs to select high-performing architectures from candidate pools using instruction-tuning samples derived from a novel pruning-based subspace sampling strategy. Our curated dataset, encompassing a wide range of architecture-performance pairs, encourages robust and transferable learning. Comprehensive experiments demonstrate that LM-Searcher achieves competitive performance in both in-domain (e.g., CNNs for image classification) and out-of-domain (e.g., LoRA configurations for segmentation and generation) tasks, establishing a new paradigm for flexible and generalizable LLM-based architecture search. The datasets and models will be released at this https URL.</li>
</ul>

<h3>Title: EditIDv2: Editable ID Customization with Data-Lubricated ID Feature Integration for Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Guandong Li, Zhaobin Chu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05659">https://arxiv.org/abs/2509.05659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05659">https://arxiv.org/pdf/2509.05659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05659]] EditIDv2: Editable ID Customization with Data-Lubricated ID Feature Integration for Text-to-Image Generation(https://arxiv.org/abs/2509.05659)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose EditIDv2, a tuning-free solution specifically designed for high-complexity narrative scenes and long text inputs. Existing character editing methods perform well under simple prompts, but often suffer from degraded editing capabilities, semantic understanding biases, and identity consistency breakdowns when faced with long text narratives containing multiple semantic layers, temporal logic, and complex contextual relationships. In EditID, we analyzed the impact of the ID integration module on editability. In EditIDv2, we further explore and address the influence of the ID feature integration module. The core of EditIDv2 is to discuss the issue of editability injection under minimal data lubrication. Through a sophisticated decomposition of PerceiverAttention, the introduction of ID loss and joint dynamic training with the diffusion model, as well as an offline fusion strategy for the integration module, we achieve deep, multi-level semantic editing while maintaining identity consistency in complex narrative environments using only a small amount of data lubrication. This meets the demands of long prompts and high-quality image generation, and achieves excellent results in the IBench evaluation.</li>
</ul>

<h3>Title: Cross-Question Method Reuse in Large Language Models: From Word-Level Prediction to Rational Logical-Layer Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Hong Su</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05660">https://arxiv.org/abs/2509.05660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05660">https://arxiv.org/pdf/2509.05660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05660]] Cross-Question Method Reuse in Large Language Models: From Word-Level Prediction to Rational Logical-Layer Reasoning(https://arxiv.org/abs/2509.05660)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been widely applied to assist in finding solutions for diverse questions. Prior work has proposed representing a method as a pair of a question and its corresponding solution, enabling method reuse. However, existing approaches typically require the questions to be highly similar. In this paper, we extend the scope of method reuse to address questions with low similarity or with hidden similarities that are not explicitly observable. For questions that are similar in a general-specific sense (i.e., broader or narrower in scope), we propose to first separate the question and solution, rather than directly feeding the pair to the LLM. The LLM is then guided to adapt the solution to new but related questions, allowing it to focus on solution transfer rather than question recognition. Furthermore, we extend this approach to cases where questions only share partial features or hidden characteristics. This enables cross-question method reuse beyond conventional similarity constraints. Experimental verification shows that our scope-extension approach increases the probability of filtering out reusable solutions, thereby improving the effectiveness of cross-question method reuse.</li>
</ul>

<h3>Title: OOTSM: A Decoupled Linguistic Framework for Effective Scene Graph Anticipation</h3>
<ul>
<li><strong>Authors: </strong>Xiaomeng Zhu, Changwei Wang, Haozhe Wang, Xinyu Liu, Fangzhen Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05661">https://arxiv.org/abs/2509.05661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05661">https://arxiv.org/pdf/2509.05661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05661]] OOTSM: A Decoupled Linguistic Framework for Effective Scene Graph Anticipation(https://arxiv.org/abs/2509.05661)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>A scene graph is a structured represention of objects and their relationships in a scene. Scene Graph Anticipation (SGA) involves predicting future scene graphs from video clips, enabling applications as intelligent surveillance and human-machine collaboration. Existing SGA approaches primarily leverage visual cues, often struggling to integrate valuable commonsense knowledge, thereby limiting long-term prediction robustness. To explicitly leverage such commonsense knowledge, we propose a new approach to better understand the objects, concepts, and relationships in a scene graph. Our approach decouples the SGA task in two steps: first a scene graph capturing model is used to convert a video clip into a sequence of scene graphs, then a pure text-based model is used to predict scene graphs in future frames. Our focus in this work is on the second step, and we call it Linguistic Scene Graph Anticipation (LSGA) and believes it should have independent interest beyond the use in SGA discussed here. For LSGA, we introduce an Object-Oriented Two-Staged Method (OOTSM) where an Large Language Model (LLM) first forecasts object appearances and disappearances before generating detailed human-object relations. We conduct extensive experiments to evaluate OOTSM in two settings. For LSGA, we evaluate our fine-tuned open-sourced LLMs against zero-shot APIs (i.e., GPT-4o, GPT-4o-mini, and DeepSeek-V3) on a benchmark constructed from Action Genome annotations. For SGA, we combine our OOTSM with STTran++ from, and our experiments demonstrate effective state-of-the-art performance: short-term mean-Recall (@10) increases by 3.4% while long-term mean-Recall (@50) improves dramatically by 21.9%. Code is available at this https URL.</li>
</ul>

<h3>Title: WIPUNet: A Physics-inspired Network with Weighted Inductive Biases for Image Denoising</h3>
<ul>
<li><strong>Authors: </strong>Wasikul Islam</a></li>
<li><strong>Subjects: </strong>cs.CV, hep-ex</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05662">https://arxiv.org/abs/2509.05662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05662">https://arxiv.org/pdf/2509.05662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05662]] WIPUNet: A Physics-inspired Network with Weighted Inductive Biases for Image Denoising(https://arxiv.org/abs/2509.05662)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In high-energy particle physics, collider measurements are contaminated by "pileup", overlapping soft interactions that obscure the hard-scatter signal of interest. Dedicated subtraction strategies exploit physical priors such as conservation, locality, and isolation. Inspired by this analogy, we investigate how such principles can inform image denoising by embedding physics-guided inductive biases into neural architectures. This paper is a proof of concept: rather than targeting state-of-the-art (SOTA) benchmarks, we ask whether physics-inspired priors improve robustness under strong corruption. We introduce a hierarchy of PU-inspired denoisers: a residual CNN with conservation constraints, its Gaussian-noise variants, and the Weighted Inductive Pileup-physics-inspired U-Network for Denoising (WIPUNet), which integrates these ideas into a UNet backbone. On CIFAR-10 with Gaussian noise at $\sigma\in\{15,25,50,75,100\}$, PU-inspired CNNs are competitive with standard baselines, while WIPUNet shows a \emph{widening margin} at higher noise. Complementary BSD500 experiments show the same trend, suggesting physics-inspired priors provide stability where purely data-driven models degrade. Our contributions are: (i) translating pileup-mitigation principles into modular inductive biases; (ii) integrating them into UNet; and (iii) demonstrating robustness gains at high noise without relying on heavy SOTA machinery.</li>
</ul>

<h3>Title: DQS: A Low-Budget Query Strategy for Enhancing Unsupervised Data-driven Anomaly Detection Approaches</h3>
<ul>
<li><strong>Authors: </strong>Lucas Correia, Jan-Christoph Goos, Thomas Bäck, Anna V. Kononova</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05663">https://arxiv.org/abs/2509.05663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05663">https://arxiv.org/pdf/2509.05663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05663]] DQS: A Low-Budget Query Strategy for Enhancing Unsupervised Data-driven Anomaly Detection Approaches(https://arxiv.org/abs/2509.05663)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Truly unsupervised approaches for time series anomaly detection are rare in the literature. Those that exist suffer from a poorly set threshold, which hampers detection performance, while others, despite claiming to be unsupervised, need to be calibrated using a labelled data subset, which is often not available in the real world. This work integrates active learning with an existing unsupervised anomaly detection method by selectively querying the labels of multivariate time series, which are then used to refine the threshold selection process. To achieve this, we introduce a novel query strategy called the dissimilarity-based query strategy (DQS). DQS aims to maximise the diversity of queried samples by evaluating the similarity between anomaly scores using dynamic time warping. We assess the detection performance of DQS in comparison to other query strategies and explore the impact of mislabelling, a topic that is underexplored in the literature. Our findings indicate that DQS performs best in small-budget scenarios, though the others appear to be more robust when faced with mislabelling. Therefore, in the real world, the choice of query strategy depends on the expertise of the oracle and the number of samples they are willing to label. Regardless, all query strategies outperform the unsupervised threshold even in the presence of mislabelling. Thus, whenever it is feasible to query an oracle, employing an active learning-based threshold is recommended.</li>
</ul>

<h3>Title: Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian</h3>
<ul>
<li><strong>Authors: </strong>Michael Hoffmann, Jophin John, Stefan Schweter, Gokul Ramakrishnan, Hoi-Fong Mak, Alice Zhang, Dmitry Gaynullin, Nicolay J. Hammer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05668">https://arxiv.org/abs/2509.05668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05668">https://arxiv.org/pdf/2509.05668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05668]] Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian(https://arxiv.org/abs/2509.05668)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present Llama-GENBA-10B, a trilingual foundation model addressing English-centric bias in large language models. Built on Llama 3.1-8B and scaled to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens (82B English, 82B German, and 80M Bavarian), balancing resources while preventing English dominance. Targeted at the German NLP community, the model also promotes Bavarian as a low-resource language. Development tackled four challenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2) creating a unified tokenizer for English, German, and Bavarian, (3) optimizing architecture and language-ratio hyperparameters for cross-lingual transfer, and (4) establishing the first standardized trilingual evaluation suite by translating German benchmarks into Bavarian. Evaluations show that Llama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing itself as the best model in its class for this language, while also outperforming EuroLLM in English and matching its results in German. Training on the Cerebras CS-2 demonstrated efficient large-scale multilingual pretraining with documented energy use, offering a blueprint for inclusive foundation models that integrate low-resource languages.</li>
</ul>

<h3>Title: Context-Aware Multi-Turn Visual-Textual Reasoning in LVLMs via Dynamic Memory and Adaptive Visual Guidance</h3>
<ul>
<li><strong>Authors: </strong>Weijie Shen, Xinrui Wang, Yuanqi Nie, Apiradee Boonmee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05669">https://arxiv.org/abs/2509.05669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05669">https://arxiv.org/pdf/2509.05669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05669]] Context-Aware Multi-Turn Visual-Textual Reasoning in LVLMs via Dynamic Memory and Adaptive Visual Guidance(https://arxiv.org/abs/2509.05669)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Current Large Language Models (LLMs) and Vision-Language Large Models (LVLMs) excel in single-turn tasks but face significant challenges in multi-turn interactions requiring deep contextual understanding and complex visual reasoning, often leading to fragmented reasoning, context loss, and hallucinations. To address these limitations, we propose Context-Aware Multi-Turn Visual Reasoning (CAMVR), a novel framework designed to empower LVLMs with robust and coherent multi-turn visual-textual inference capabilities. CAMVR introduces two key innovations: a Visual-Textual Context Memory Unit (VCMU), a dynamic read-write memory network that stores and manages critical visual features, textual semantic representations, and their cross-modal correspondences from each interaction turn; and an Adaptive Visual Focus Guidance (AVFG) mechanism, which leverages the VCMU's context to dynamically adjust the visual encoder's attention to contextually relevant image regions. Our multi-level reasoning integration strategy ensures that response generation is deeply coherent with both current inputs and accumulated historical context. Extensive experiments on challenging datasets, including VisDial, an adapted A-OKVQA, and our novel Multi-Turn Instruction Following (MTIF) dataset, demonstrate that CAMVR consistently achieves state-of-the-art performance.</li>
</ul>

<h3>Title: MeshMetrics: A Precise Implementation of Distance-Based Image Segmentation Metrics</h3>
<ul>
<li><strong>Authors: </strong>Gašper Podobnik, Tomaž Vrtovec</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05670">https://arxiv.org/abs/2509.05670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05670">https://arxiv.org/pdf/2509.05670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05670]] MeshMetrics: A Precise Implementation of Distance-Based Image Segmentation Metrics(https://arxiv.org/abs/2509.05670)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The surge of research in image segmentation has yielded remarkable performance gains but also exposed a reproducibility crisis. A major contributor is performance evaluation, where both selection and implementation of metrics play critical roles. While recent efforts have improved the former, the reliability of metric implementation has received far less attention. Pitfalls in distance-based metric implementation can lead to considerable discrepancies between common open-source tools, for instance, exceeding 100 mm for the Hausdorff distance and 30%pt for the normalized surface distance for the same pair of segmentations. To address these pitfalls, we introduce MeshMetrics, a mesh-based framework that provides a more precise computation of distance-based metrics than conventional grid-based approaches. Through theoretical analysis and empirical validation, we demonstrate that MeshMetrics achieves higher accuracy and precision than established tools, and is substantially less affected by discretization artifacts, such as distance quantization. We release MeshMetrics as an open-source Python package, available at this https URL.</li>
</ul>

<h3>Title: GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR</h3>
<ul>
<li><strong>Authors: </strong>Labani Halder, Tanmay Sen, Sarbani Palit</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05671">https://arxiv.org/abs/2509.05671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05671">https://arxiv.org/pdf/2509.05671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05671]] GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR(https://arxiv.org/abs/2509.05671)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Human Activity Recognition (HAR) using multimodal sensor data remains challenging due to noisy or incomplete measurements, scarcity of labeled examples, and privacy concerns. Traditional centralized deep learning approaches are often constrained by infrastructure availability, network latency, and data sharing restrictions. While federated learning (FL) addresses privacy by training models locally and sharing only model parameters, it still has to tackle issues arising from the use of heterogeneous multimodal data and differential privacy requirements. In this article, a Graph-based Multimodal Federated Learning framework, GraMFedDHAR, is proposed for HAR tasks. Diverse sensor streams such as a pressure mat, depth camera, and multiple accelerometers are modeled as modality-specific graphs, processed through residual Graph Convolutional Neural Networks (GCNs), and fused via attention-based weighting rather than simple concatenation. The fused embeddings enable robust activity classification, while differential privacy safeguards data during federated aggregation. Experimental results show that the proposed MultiModalGCN model outperforms the baseline MultiModalFFN, with up to 2 percent higher accuracy in non-DP settings in both centralized and federated paradigms. More importantly, significant improvements are observed under differential privacy constraints: MultiModalGCN consistently surpasses MultiModalFFN, with performance gaps ranging from 7 to 13 percent depending on the privacy budget and setting. These results highlight the robustness of graph-based modeling in multimodal learning, where GNNs prove more resilient to the performance degradation introduced by DP noise.</li>
</ul>

<h3>Title: SEASONED: Semantic-Enhanced Self-Counterfactual Explainable Detection of Adversarial Exploiter Contracts</h3>
<ul>
<li><strong>Authors: </strong>Xng Ai, Shudan Lin, Zecheng Li, Kai Zhou, Bixin Li, Bin Xiao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05681">https://arxiv.org/abs/2509.05681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05681">https://arxiv.org/pdf/2509.05681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05681]] SEASONED: Semantic-Enhanced Self-Counterfactual Explainable Detection of Adversarial Exploiter Contracts(https://arxiv.org/abs/2509.05681)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, interpretability</a></li>
<li><strong>Abstract: </strong>Decentralized Finance (DeFi) attacks have resulted in significant losses, often orchestrated through Adversarial Exploiter Contracts (AECs) that exploit vulnerabilities in victim smart contracts. To proactively identify such threats, this paper targets the explainable detection of AECs. Existing detection methods struggle to capture semantic dependencies and lack interpretability, limiting their effectiveness and leaving critical knowledge gaps in AEC analysis. To address these challenges, we introduce SEASONED, an effective, self-explanatory, and robust framework for AEC detection. SEASONED extracts semantic information from contract bytecode to construct a semantic relation graph (SRG), and employs a self-counterfactual explainable detector (SCFED) to classify SRGs and generate explanations that highlight the core attack logic. SCFED further enhances robustness, generalizability, and data efficiency by extracting representative information from these explanations. Both theoretical analysis and experimental results demonstrate the effectiveness of SEASONED, which showcases outstanding detection performance, robustness, generalizability, and data efficiency learning ability. To support further research, we also release a new dataset of 359 AECs.</li>
</ul>

<h3>Title: Leveraging Vision-Language Large Models for Interpretable Video Action Recognition with Semantic Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Jingwei Peng, Zhixuan Qiu, Boyu Jin, Surasakdi Siripong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05695">https://arxiv.org/abs/2509.05695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05695">https://arxiv.org/pdf/2509.05695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05695]] Leveraging Vision-Language Large Models for Interpretable Video Action Recognition with Semantic Tokenization(https://arxiv.org/abs/2509.05695)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Human action recognition often struggles with deep semantic understanding, complex contextual information, and fine-grained distinction, limitations that traditional methods frequently encounter when dealing with diverse video data. Inspired by the remarkable capabilities of large language models, this paper introduces LVLM-VAR, a novel framework that pioneers the application of pre-trained Vision-Language Large Models (LVLMs) to video action recognition, emphasizing enhanced accuracy and interpretability. Our method features a Video-to-Semantic-Tokens (VST) Module, which innovatively transforms raw video sequences into discrete, semantically and temporally consistent "semantic action tokens," effectively crafting an "action narrative" that is comprehensible to an LVLM. These tokens, combined with natural language instructions, are then processed by a LoRA-fine-tuned LVLM (e.g., LLaVA-13B) for robust action classification and semantic reasoning. LVLM-VAR not only achieves state-of-the-art or highly competitive performance on challenging benchmarks such as NTU RGB+D and NTU RGB+D 120, demonstrating significant improvements (e.g., 94.1% on NTU RGB+D X-Sub and 90.0% on NTU RGB+D 120 X-Set), but also substantially boosts model interpretability by generating natural language explanations for its predictions.</li>
</ul>

<h3>Title: JRN-Geo: A Joint Perception Network based on RGB and Normal images for Cross-view Geo-localization</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Zhou, Yunzhou Zhang, Tingsong Huang, Fawei Ge, Man Qi, Xichen Zhang, Yizhong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05696">https://arxiv.org/abs/2509.05696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05696">https://arxiv.org/pdf/2509.05696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05696]] JRN-Geo: A Joint Perception Network based on RGB and Normal images for Cross-view Geo-localization(https://arxiv.org/abs/2509.05696)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Cross-view geo-localization plays a critical role in Unmanned Aerial Vehicle (UAV) localization and navigation. However, significant challenges arise from the drastic viewpoint differences and appearance variations between images. Existing methods predominantly rely on semantic features from RGB images, often neglecting the importance of spatial structural information in capturing viewpoint-invariant features. To address this issue, we incorporate geometric structural information from normal images and introduce a Joint perception network to integrate RGB and Normal images (JRN-Geo). Our approach utilizes a dual-branch feature extraction framework, leveraging a Difference-Aware Fusion Module (DAFM) and Joint-Constrained Interaction Aggregation (JCIA) strategy to enable deep fusion and joint-constrained semantic and structural information representation. Furthermore, we propose a 3D geographic augmentation technique to generate potential viewpoint variation samples, enhancing the network's ability to learn viewpoint-invariant features. Extensive experiments on the University-1652 and SUES-200 datasets validate the robustness of our method against complex viewpoint ariations, achieving state-of-the-art performance.</li>
</ul>

<h3>Title: KnowHow: Automatically Applying High-Level CTI Knowledge for Interpretable and Accurate Provenance Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Meng, Shaofei Li, Jiaping Gui, Peng Jiang, Ding Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05698">https://arxiv.org/abs/2509.05698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05698">https://arxiv.org/pdf/2509.05698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05698]] KnowHow: Automatically Applying High-Level CTI Knowledge for Interpretable and Accurate Provenance Analysis(https://arxiv.org/abs/2509.05698)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>High-level natural language knowledge in CTI reports, such as the ATT&CK framework, is beneficial to counter APT attacks. However, how to automatically apply the high-level knowledge in CTI reports in realistic attack detection systems, such as provenance analysis systems, is still an open problem. The challenge stems from the semantic gap between the knowledge and the low-level security logs: while the knowledge in CTI reports is written in natural language, attack detection systems can only process low-level system events like file accesses or network IP manipulations. Manual approaches can be labor-intensive and error-prone. In this paper, we propose KnowHow, a CTI-knowledge-driven online provenance analysis approach that can automatically apply high-level attack knowledge from CTI reports written in natural languages to detect low-level system events. The core of KnowHow is a novel attack knowledge representation, gIoC, that represents the subject, object, and actions of attacks. By lifting system identifiers, such as file paths, in system events to natural language terms, KnowHow can match system events to gIoC and further match them to techniques described in natural languages. Finally, based on the techniques matched to system events, KnowHow reasons about the temporal logic of attack steps and detects potential APT attacks in system events. Our evaluation shows that KnowHow can accurately detect all 16 APT campaigns in the open-source and industrial datasets, while existing approaches all introduce large numbers of false positives. Meanwhile, our evaluation also shows that KnowHow reduces at most 90% of node-level false positives while having a higher node-level recall and is robust against several unknown attacks and mimicry attacks.</li>
</ul>

<h3>Title: Larger-scale Nakamoto-style Blockchains Offer Better Security</h3>
<ul>
<li><strong>Authors: </strong>Junjie Hu, Na Ruan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05708">https://arxiv.org/abs/2509.05708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05708">https://arxiv.org/pdf/2509.05708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05708]] Larger-scale Nakamoto-style Blockchains Offer Better Security(https://arxiv.org/abs/2509.05708)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Traditional security models for Nakamoto-style blockchains overestimate adversarial coordination by assuming instantaneous synchronization among malicious nodes, neglecting the critical impact of internal communication delays on security. This paper introduces a dual-delay framework to revisit security analysis, addressing this oversight through two key innovations. First, the static delay model quantifies how adversarial communication delays (\(\Delta_a\)) constrain the effective growth rate of private chains, derived via an M/D/1 queuing model as \(\lambda_{eff} = \lambda_a / (1 + \lambda_a \Delta_a)\). This model reveals that the security threshold (\(\beta^*\)), the maximum adversarial power the system tolerates, increases with \(\Delta_a\), even exceeding the classic 51\% boundary when \(\Delta_a \textgreater \Delta\) (honest nodes' delay), breaking the long-standing 50\% assumption. Second, the dynamic delay model integrates probabilistic corruption and scale-dependent delays to characterize the total adversarial delay window (\(\Delta_{total} = \Delta(n) e^{-k\beta} + c \log(1 + \beta n)\)), where \(\Delta(n) \in \Theta(\log n)\) captures honest nodes' logarithmic delay growth. Asymptotic analysis shows adversarial power decays linearly with network scale, ensuring the probability of \(\beta \leq \beta^*\) approaches 1 as \(n \to \infty\). By exposing the interplay between network scale, communication delays, and power dilution, we provide a theoretical foundation for optimizing consensus protocols and assessing robustness in large-scale Nakamoto-style blockchains.</li>
</ul>

<h3>Title: A Survey of the State-of-the-Art in Conversational Question Answering Systems</h3>
<ul>
<li><strong>Authors: </strong>Manoj Madushanka Perera, Adnan Mahmood, Kasun Eranda Wijethilake, Fahmida Islam, Maryam Tahermazandarani, Quan Z. Sheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05716">https://arxiv.org/abs/2509.05716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05716">https://arxiv.org/pdf/2509.05716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05716]] A Survey of the State-of-the-Art in Conversational Question Answering Systems(https://arxiv.org/abs/2509.05716)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Conversational Question Answering (ConvQA) systems have emerged as a pivotal area within Natural Language Processing (NLP) by driving advancements that enable machines to engage in dynamic and context-aware conversations. These capabilities are increasingly being applied across various domains, i.e., customer support, education, legal, and healthcare where maintaining a coherent and relevant conversation is essential. Building on recent advancements, this survey provides a comprehensive analysis of the state-of-the-art in ConvQA. This survey begins by examining the core components of ConvQA systems, i.e., history selection, question understanding, and answer prediction, highlighting their interplay in ensuring coherence and relevance in multi-turn conversations. It further investigates the use of advanced machine learning techniques, including but not limited to, reinforcement learning, contrastive learning, and transfer learning to improve ConvQA accuracy and efficiency. The pivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash, Mistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact through data scalability and architectural advancements. Additionally, this survey presents a comprehensive analysis of key ConvQA datasets and concludes by outlining open research directions. Overall, this work offers a comprehensive overview of the ConvQA landscape and provides valuable insights to guide future advancements in the field.</li>
</ul>

<h3>Title: LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Niels Balemans, Ali Anwar, Jan Steckel, Siegfried Mercelis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05728">https://arxiv.org/abs/2509.05728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05728">https://arxiv.org/pdf/2509.05728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05728]] LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction(https://arxiv.org/abs/2509.05728)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper extends LiDAR-BIND, a modular multi-modal fusion framework that binds heterogeneous sensors (radar, sonar) to a LiDAR-defined latent space, with mechanisms that explicitly enforce temporal consistency. We introduce three contributions: (i) temporal embedding similarity that aligns consecutive latents, (ii) a motion-aligned transformation loss that matches displacement between predictions and ground truth LiDAR, and (iii) windows temporal fusion using a specialised temporal module. We further update the model architecture to better preserve spatial structure. Evaluations on radar/sonar-to-LiDAR translation demonstrate improved temporal and spatial coherence, yielding lower absolute trajectory error and better occupancy map accuracy in Cartographer-based SLAM (Simultaneous Localisation and Mapping). We propose different metrics based on the Fréchet Video Motion Distance (FVMD) and a correlation-peak distance metric providing practical temporal quality indicators to evaluate SLAM performance. The proposed temporal LiDAR-BIND, or LiDAR-BIND-T, maintains plug-and-play modality fusion while substantially enhancing temporal stability, resulting in improved robustness and performance for downstream SLAM.</li>
</ul>

<h3>Title: Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Chen, Ji Shi, Cansu Sancaktar, Jonas Frey, Georg Martius</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05735">https://arxiv.org/abs/2509.05735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05735">https://arxiv.org/pdf/2509.05735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05735]] Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies(https://arxiv.org/abs/2509.05735)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Data collection is crucial for learning robust world models in model-based reinforcement learning. The most prevalent strategies are to actively collect trajectories by interacting with the environment during online training or training on offline datasets. At first glance, the nature of learning task-agnostic environment dynamics makes world models a good candidate for effective offline training. However, the effects of online vs. offline data on world models and thus on the resulting task performance have not been thoroughly studied in the literature. In this work, we investigate both paradigms in model-based settings, conducting experiments on 31 different environments. First, we showcase that online agents outperform their offline counterparts. We identify a key challenge behind performance degradation of offline agents: encountering Out-Of-Distribution states at test time. This issue arises because, without the self-correction mechanism in online agents, offline datasets with limited state space coverage induce a mismatch between the agent's imagination and real rollouts, compromising policy training. We demonstrate that this issue can be mitigated by allowing for additional online interactions in a fixed or adaptive schedule, restoring the performance of online training with limited interaction data. We also showcase that incorporating exploration data helps mitigate the performance degradation of offline agents. Based on our insights, we recommend adding exploration data when collecting large datasets, as current efforts predominantly focus on expert data alone.</li>
</ul>

<h3>Title: Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated</h3>
<ul>
<li><strong>Authors: </strong>Hanna Foerster, Ilia Shumailov, Yiren Zhao, Harsh Chaudhari, Jamie Hayes, Robert Mullins, Yarin Gal</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05739">https://arxiv.org/abs/2509.05739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05739">https://arxiv.org/pdf/2509.05739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05739]] Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated(https://arxiv.org/abs/2509.05739)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, steal, large language model</a></li>
<li><strong>Abstract: </strong>Early research into data poisoning attacks against Large Language Models (LLMs) demonstrated the ease with which backdoors could be injected. More recent LLMs add step-by-step reasoning, expanding the attack surface to include the intermediate chain-of-thought (CoT) and its inherent trait of decomposing problems into subproblems. Using these vectors for more stealthy poisoning, we introduce ``decomposed reasoning poison'', in which the attacker modifies only the reasoning path, leaving prompts and final answers clean, and splits the trigger across multiple, individually harmless components. Fascinatingly, while it remains possible to inject these decomposed poisons, reliably activating them to change final answers (rather than just the CoT) is surprisingly difficult. This difficulty arises because the models can often recover from backdoors that are activated within their thought processes. Ultimately, it appears that an emergent form of backdoor robustness is originating from the reasoning capabilities of these advanced LLMs, as well as from the architectural separation between reasoning and final answer generation.</li>
</ul>

<h3>Title: Multi-LVI-SAM: A Robust LiDAR-Visual-Inertial Odometry for Multiple Fisheye Cameras</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Zhang, Kai Huang, Junqiao Zhao, Zihan Yuan, Tiantian Feng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05740">https://arxiv.org/abs/2509.05740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05740">https://arxiv.org/pdf/2509.05740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05740]] Multi-LVI-SAM: A Robust LiDAR-Visual-Inertial Odometry for Multiple Fisheye Cameras(https://arxiv.org/abs/2509.05740)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose a multi-camera LiDAR-visual-inertial odometry framework, Multi-LVI-SAM, which fuses data from multiple fisheye cameras, LiDAR and inertial sensors for highly accurate and robust state estimation. To enable efficient and consistent integration of visual information from multiple fisheye cameras, we introduce a panoramic visual feature model that unifies multi-camera observations into a single representation. The panoramic model serves as a global geometric optimization framework that consolidates multi-view constraints, enabling seamless loop closure and global pose optimization, while simplifying system design by avoiding redundant handling of individual cameras. To address the triangulation inconsistency caused by the misalignment between each camera's frame and the panoramic model's frame, we propose an extrinsic compensation method. This method improves feature consistency across views and significantly reduces triangulation and optimization errors, leading to more accurate pose estimation. We integrate the panoramic visual feature model into a tightly coupled LiDAR-visual-inertial system based on a factor graph. Extensive experiments on public datasets demonstrate that the panoramic visual feature model enhances the quality and consistency of multi-camera constraints, resulting in higher accuracy and robustness than existing multi-camera LiDAR-visual-inertial systems.</li>
</ul>

<h3>Title: Enhancing Factual Accuracy and Citation Generation in LLMs via Multi-Stage Self-Verification</h3>
<ul>
<li><strong>Authors: </strong>Fernando Gabriela García, Qiyang Shi, Zilin Feng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05741">https://arxiv.org/abs/2509.05741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05741">https://arxiv.org/pdf/2509.05741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05741]] Enhancing Factual Accuracy and Citation Generation in LLMs via Multi-Stage Self-Verification(https://arxiv.org/abs/2509.05741)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This research introduces VeriFact-CoT (Verified Factual Chain-of-Thought), a novel method designed to address the pervasive issues of hallucination and the absence of credible citation sources in Large Language Models (LLMs) when generating complex, fact-sensitive content. By incorporating a multi-stage mechanism of 'fact verification-reflection-citation integration,' VeriFact-CoT empowers LLMs to critically self-examine and revise their intermediate reasoning steps and final answers. This process significantly enhances the objective accuracy, trustworthiness, and traceability of the generated outputs, making LLMs more reliable for applications demanding high fidelity such as scientific research, news reporting, and legal consultation.</li>
</ul>

<h3>Title: InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Leo Ho, Yinghao Huang, Dafei Qin, Mingyi Shi, Wangpok Tse, Wei Liu, Junichi Yamagishi, Taku Komura</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.MA, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05747">https://arxiv.org/abs/2509.05747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05747">https://arxiv.org/pdf/2509.05747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05747]] InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios(https://arxiv.org/abs/2509.05747)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We address the problem of accurate capture of interactive behaviors between two people in daily scenarios. Most previous works either only consider one person or solely focus on conversational gestures of two people, assuming the body orientation and/or position of each actor are constant or barely change over each interaction. In contrast, we propose to simultaneously model two people's activities, and target objective-driven, dynamic, and semantically consistent interactions which often span longer duration and cover bigger space. To this end, we capture a new multi-modal dataset dubbed InterAct, which is composed of 241 motion sequences where two people perform a realistic and coherent scenario for one minute or longer over a complete interaction. For each sequence, two actors are assigned different roles and emotion labels, and collaborate to finish one task or conduct a common interaction activity. The audios, body motions, and facial expressions of both persons are captured. InterAct contains diverse and complex motions of individuals and interesting and relatively long-term interaction patterns barely seen before. We also demonstrate a simple yet effective diffusion-based method that estimates interactive face expressions and body motions of two people from speech inputs. Our method regresses the body motions in a hierarchical manner, and we also propose a novel fine-tuning mechanism to improve the lip accuracy of facial expressions. To facilitate further research, the data and code is made available at this https URL .</li>
</ul>

<h3>Title: Unleashing Hierarchical Reasoning: An LLM-Driven Framework for Training-Free Referring Video Object Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Bingrui Zhao, Lin Yuanbo Wu, Xiangtian Fan, Deyin Liu, Lu Zhang, Ruyi He, Jialie Shen, Ximing Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05751">https://arxiv.org/abs/2509.05751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05751">https://arxiv.org/pdf/2509.05751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05751]] Unleashing Hierarchical Reasoning: An LLM-Driven Framework for Training-Free Referring Video Object Segmentation(https://arxiv.org/abs/2509.05751)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Referring Video Object Segmentation (RVOS) aims to segment an object of interest throughout a video based on a language description. The prominent challenge lies in aligning static text with dynamic visual content, particularly when objects exhibiting similar appearances with inconsistent motion and poses. However, current methods often rely on a holistic visual-language fusion that struggles with complex, compositional descriptions. In this paper, we propose \textbf{PARSE-VOS}, a novel, training-free framework powered by Large Language Models (LLMs), for a hierarchical, coarse-to-fine reasoning across text and video domains. Our approach begins by parsing the natural language query into structured semantic commands. Next, we introduce a spatio-temporal grounding module that generates all candidate trajectories for all potential target objects, guided by the parsed semantics. Finally, a hierarchical identification module select the correct target through a two-stage reasoning process: it first performs coarse-grained motion reasoning with an LLM to narrow down candidates; if ambiguity remains, a fine-grained pose verification stage is conditionally triggered to disambiguate. The final output is an accurate segmentation mask for the target object. \textbf{PARSE-VOS} achieved state-of-the-art performance on three major benchmarks: Ref-YouTube-VOS, Ref-DAVIS17, and MeViS.</li>
</ul>

<h3>Title: Tell-Tale Watermarks for Explanatory Reasoning in Synthetic Media Forensics</h3>
<ul>
<li><strong>Authors: </strong>Ching-Chun Chang, Isao Echizen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05753">https://arxiv.org/abs/2509.05753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05753">https://arxiv.org/pdf/2509.05753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05753]] Tell-Tale Watermarks for Explanatory Reasoning in Synthetic Media Forensics(https://arxiv.org/abs/2509.05753)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark</a></li>
<li><strong>Abstract: </strong>The rise of synthetic media has blurred the boundary between reality and fabrication under the evolving power of artificial intelligence, fueling an infodemic that erodes public trust in cyberspace. For digital imagery, a multitude of editing applications further complicates the forensic analysis, including semantic edits that alter content, photometric adjustments that recalibrate colour characteristics, and geometric projections that reshape viewpoints. Collectively, these transformations manipulate and control perceptual interpretation of digital imagery. This susceptibility calls for forensic enquiry into reconstructing the chain of events, thereby revealing deeper evidential insight into the presence or absence of criminal intent. This study seeks to address an inverse problem of tracing the underlying generation chain that gives rise to the observed synthetic media. A tell-tale watermarking system is developed for explanatory reasoning over the nature and extent of transformations across the lifecycle of synthetic media. Tell-tale watermarks are tailored to different classes of transformations, responding in a manner that is neither strictly robust nor fragile but instead interpretable. These watermarks function as reference clues that evolve under the same transformation dynamics as the carrier media, leaving interpretable traces when subjected to transformations. Explanatory reasoning is then performed to infer the most plausible account across the combinatorial parameter space of composite transformations. Experimental evaluations demonstrate the validity of tell-tale watermarking with respect to fidelity, synchronicity and traceability.</li>
</ul>

<h3>Title: Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based Agentic System</h3>
<ul>
<li><strong>Authors: </strong>Yu Liu, Yuchong Xie, Mingyu Luo, Zesen Liu, Zhixiang Zhang, Kaikai Zhang, Zongjie Li, Ping Chen, Shuai Wang, Dongdong She</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05755">https://arxiv.org/abs/2509.05755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05755">https://arxiv.org/pdf/2509.05755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05755]] Exploit Tool Invocation Prompt for Tool Behavior Hijacking in LLM-Based Agentic System(https://arxiv.org/abs/2509.05755)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>LLM-based agentic systems leverage large language models to handle user queries, make decisions, and execute external tools for complex tasks across domains like chatbots, customer service, and software engineering. A critical component of these systems is the Tool Invocation Prompt (TIP), which defines tool interaction protocols and guides LLMs to ensure the security and correctness of tool usage. Despite its importance, TIP security has been largely overlooked. This work investigates TIP-related security risks, revealing that major LLM-based systems like Cursor, Claude Code, and others are vulnerable to attacks such as remote code execution (RCE) and denial of service (DoS). Through a systematic TIP exploitation workflow (TEW), we demonstrate external tool behavior hijacking via manipulated tool invocations. We also propose defense mechanisms to enhance TIP security in LLM-based agentic systems.</li>
</ul>

<h3>Title: Ensemble of Precision-Recall Curve (PRC) Classification Trees with Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Jiaju Miao, Wei Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05766">https://arxiv.org/abs/2509.05766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05766">https://arxiv.org/pdf/2509.05766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05766]] Ensemble of Precision-Recall Curve (PRC) Classification Trees with Autoencoders(https://arxiv.org/abs/2509.05766)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, interpretability</a></li>
<li><strong>Abstract: </strong>Anomaly detection underpins critical applications from network security and intrusion detection to fraud prevention, where recognizing aberrant patterns rapidly is indispensable. Progress in this area is routinely impeded by two obstacles: extreme class imbalance and the curse of dimensionality. To combat the former, we previously introduced Precision-Recall Curve (PRC) classification trees and their ensemble extension, the PRC Random Forest (PRC-RF). Building on that foundation, we now propose a hybrid framework that integrates PRC-RF with autoencoders, unsupervised machine learning methods that learn compact latent representations, to confront both challenges simultaneously. Extensive experiments across diverse benchmark datasets demonstrate that the resulting Autoencoder-PRC-RF model achieves superior accuracy, scalability, and interpretability relative to prior methods, affirming its potential for high-stakes anomaly-detection tasks.</li>
</ul>

<h3>Title: Real-E: A Foundation Benchmark for Advancing Robust and Generalizable Electricity Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Chen Shao, Yue Wang, Zhenyi Zhu, Zhanbo Huang, Sebastian Pütz, Benjamin Schäfer, Tobais Käfer, Michael Färber</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05768">https://arxiv.org/abs/2509.05768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05768">https://arxiv.org/pdf/2509.05768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05768]] Real-E: A Foundation Benchmark for Advancing Robust and Generalizable Electricity Forecasting(https://arxiv.org/abs/2509.05768)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Energy forecasting is vital for grid reliability and operational efficiency. Although recent advances in time series forecasting have led to progress, existing benchmarks remain limited in spatial and temporal scope and lack multi-energy features. This raises concerns about their reliability and applicability in real-world deployment. To address this, we present the Real-E dataset, covering over 74 power stations across 30+ European countries over a 10-year span with rich metadata. Using Real- E, we conduct an extensive data analysis and benchmark over 20 baselines across various model types. We introduce a new metric to quantify shifts in correlation structures and show that existing methods struggle on our dataset, which exhibits more complex and non-stationary correlation dynamics. Our findings highlight key limitations of current methods and offer a strong empirical basis for building more robust forecasting models</li>
</ul>

<h3>Title: DCV-ROOD Evaluation Framework: Dual Cross-Validation for Robust Out-of-Distribution Detection</h3>
<ul>
<li><strong>Authors: </strong>Arantxa Urrea-Castaño, Nicolás Segura-Kunsagi, Juan Luis Suárez-Díaz, Rosana Montes, Francisco Herrera</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05778">https://arxiv.org/abs/2509.05778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05778">https://arxiv.org/pdf/2509.05778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05778]] DCV-ROOD Evaluation Framework: Dual Cross-Validation for Robust Out-of-Distribution Detection(https://arxiv.org/abs/2509.05778)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Out-of-distribution (OOD) detection plays a key role in enhancing the robustness of artificial intelligence systems by identifying inputs that differ significantly from the training distribution, thereby preventing unreliable predictions and enabling appropriate fallback mechanisms. Developing reliable OOD detection methods is a significant challenge, and rigorous evaluation of these techniques is essential for ensuring their effectiveness, as it allows researchers to assess their performance under diverse conditions and to identify potential limitations or failure modes. Cross-validation (CV) has proven to be a highly effective tool for providing a reasonable estimate of the performance of a learning algorithm. Although OOD scenarios exhibit particular characteristics, an appropriate adaptation of CV can lead to a suitable evaluation framework for this setting. This work proposes a dual CV framework for robust evaluation of OOD detection models, aimed at improving the reliability of their assessment. The proposed evaluation framework aims to effectively integrate in-distribution (ID) and OOD data while accounting for their differing characteristics. To achieve this, ID data are partitioned using a conventional approach, whereas OOD data are divided by grouping samples based on their classes. Furthermore, we analyze the context of data with class hierarchy to propose a data splitting that considers the entire class hierarchy to obtain fair ID-OOD partitions to apply the proposed evaluation framework. This framework is called Dual Cross-Validation for Robust Out-of-Distribution Detection (DCV-ROOD). To test the validity of the evaluation framework, we selected a set of state-of-the-art OOD detection methods, both with and without outlier exposure. The results show that the method achieves very fast convergence to the true performance.</li>
</ul>

<h3>Title: Select, then Balance: A Plug-and-Play Framework for Exogenous-Aware Spatio-Temporal Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Wei Chen, Yuqian Wu, Yuanshao Zhu, Xixuan Hao, Shiyu Wang, Yuxuan Liang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05779">https://arxiv.org/abs/2509.05779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05779">https://arxiv.org/pdf/2509.05779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05779]] Select, then Balance: A Plug-and-Play Framework for Exogenous-Aware Spatio-Temporal Forecasting(https://arxiv.org/abs/2509.05779)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Spatio-temporal forecasting aims to predict the future state of dynamic systems and plays an important role in multiple fields. However, existing solutions only focus on modeling using a limited number of observed target variables. In real-world scenarios, exogenous variables can be integrated into the model as additional input features and associated with the target signal to promote forecast accuracy. Although promising, this still encounters two challenges: the inconsistent effects of different exogenous variables to the target system, and the imbalance effects between historical variables and future variables. To address these challenges, this paper introduces \model, a novel framework for modeling \underline{exo}genous variables in \underline{s}patio-\underline{t}emporal forecasting, which follows a ``select, then balance'' paradigm. Specifically, we first construct a latent space gated expert module, where fused exogenous information is projected into a latent space to dynamically select and recompose salient signals via specialized sub-experts. Furthermore, we design a siamese network architecture in which recomposed representations of past and future exogenous variables are fed into dual-branch spatio-temporal backbones to capture dynamic patterns. The outputs are integrated through a context-aware weighting mechanism to achieve dynamic balance during the modeling process. Extensive experiments on real-world datasets demonstrate the effectiveness, generality, robustness, and efficiency of our proposed framework.</li>
</ul>

<h3>Title: CRAB: Camera-Radar Fusion for Reducing Depth Ambiguity in Backward Projection based View Transformation</h3>
<ul>
<li><strong>Authors: </strong>In-Jae Lee, Sihwan Hwang, Youngseok Kim, Wonjune Kim, Sanmin Kim, Dongsuk Kum</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05785">https://arxiv.org/abs/2509.05785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05785">https://arxiv.org/pdf/2509.05785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05785]] CRAB: Camera-Radar Fusion for Reducing Depth Ambiguity in Backward Projection based View Transformation(https://arxiv.org/abs/2509.05785)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recently, camera-radar fusion-based 3D object detection methods in bird's eye view (BEV) have gained attention due to the complementary characteristics and cost-effectiveness of these sensors. Previous approaches using forward projection struggle with sparse BEV feature generation, while those employing backward projection overlook depth ambiguity, leading to false positives. In this paper, to address the aforementioned limitations, we propose a novel camera-radar fusion-based 3D object detection and segmentation model named CRAB (Camera-Radar fusion for reducing depth Ambiguity in Backward projection-based view transformation), using a backward projection that leverages radar to mitigate depth ambiguity. During the view transformation, CRAB aggregates perspective view image context features into BEV queries. It improves depth distinction among queries along the same ray by combining the dense but unreliable depth distribution from images with the sparse yet precise depth information from radar occupancy. We further introduce spatial cross-attention with a feature map containing radar context information to enhance the comprehension of the 3D scene. When evaluated on the nuScenes open dataset, our proposed approach achieves a state-of-the-art performance among backward projection-based camera-radar fusion methods with 62.4\% NDS and 54.0\% mAP in 3D object detection.</li>
</ul>

<h3>Title: Secure and Trustful Cross-domain Communication with Decentralized Identifiers in 5G and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Hai Dinh-Tuan, Sandro Rodriguez Garzon, Jianeng Fu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05797">https://arxiv.org/abs/2509.05797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05797">https://arxiv.org/pdf/2509.05797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05797]] Secure and Trustful Cross-domain Communication with Decentralized Identifiers in 5G and Beyond(https://arxiv.org/abs/2509.05797)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>In the evolving landscape of future mobile networks, there is a critical need for secure and trustful communication modalities to support dynamic interactions among core network components of different network domains. This paper proposes the application of W3C-endorsed Decentralized Identifiers (DIDs) to establish secure and trustful communication channels among network functions in 5G and subsequent generations. A new communication agent is introduced that integrates seamlessly with 5G-standardized network functions and utilizes a DID-based application layer transport protocol to ensure confidentiality, integrity, and authenticity for cross-domain interactions. A comparative analysis of the two different versions of the DID-based communication protocol for inter network function communication reveals compatibility advantages of the latest protocol iteration. Furthermore, a comprehensive evaluation of the communication overhead caused by both protocol iterations compared to traditional TCP/TLS shows the benefits of using DIDs to improve communication security, albeit with performance loses compared to TCP/TLS. These results uncover the potential of DID-based communication for future mobile networks but also point out areas for optimization.</li>
</ul>

<h3>Title: time2time: Causal Intervention in Hidden States to Simulate Rare Events in Time Series Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Debdeep Sanyal, Aaryan Nagpal, Dhruv Kumar, Murari Mandal, Saurabh Deshpande</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05801">https://arxiv.org/abs/2509.05801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05801">https://arxiv.org/pdf/2509.05801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05801]] time2time: Causal Intervention in Hidden States to Simulate Rare Events in Time Series Foundation Models(https://arxiv.org/abs/2509.05801)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>While transformer-based foundation models excel at forecasting routine patterns, two questions remain: do they internalize semantic concepts such as market regimes, or merely fit curves? And can their internal representations be leveraged to simulate rare, high-stakes events such as market crashes? To investigate this, we introduce activation transplantation, a causal intervention that manipulates hidden states by imposing the statistical moments of one event (e.g., a historical crash) onto another (e.g., a calm period) during the forward pass. This procedure deterministically steers forecasts: injecting crash semantics induces downturn predictions, while injecting calm semantics suppresses crashes and restores stability. Beyond binary control, we find that models encode a graded notion of event severity, with the latent vector norm directly correlating with the magnitude of systemic shocks. Validated across two architecturally distinct TSFMs, Toto (decoder only) and Chronos (encoder-decoder), our results demonstrate that steerable, semantically grounded representations are a robust property of large time series transformers. Our findings provide evidence for a latent concept space that governs model predictions, shifting interpretability from post-hoc attribution to direct causal intervention, and enabling semantic "what-if" analysis for strategic stress-testing.</li>
</ul>

<h3>Title: A Probabilistic Segment Anything Model for Ambiguity-Aware Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Tyler Ward, Abdullah Imran</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05809">https://arxiv.org/abs/2509.05809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05809">https://arxiv.org/pdf/2509.05809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05809]] A Probabilistic Segment Anything Model for Ambiguity-Aware Medical Image Segmentation(https://arxiv.org/abs/2509.05809)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recent advances in promptable segmentation, such as the Segment Anything Model (SAM), have enabled flexible, high-quality mask generation across a wide range of visual domains. However, SAM and similar models remain fundamentally deterministic, producing a single segmentation per object per prompt, and fail to capture the inherent ambiguity present in many real-world tasks. This limitation is particularly troublesome in medical imaging, where multiple plausible segmentations may exist due to annotation uncertainty or inter-expert variability. In this paper, we introduce Probabilistic SAM, a probabilistic extension of SAM that models a distribution over segmentations conditioned on both the input image and prompt. By incorporating a latent variable space and training with a variational objective, our model learns to generate diverse and plausible segmentation masks reflecting the variability in human annotations. The architecture integrates a prior and posterior network into the SAM framework, allowing latent codes to modulate the prompt embeddings during inference. The latent space allows for efficient sampling during inference, enabling uncertainty-aware outputs with minimal overhead. We evaluate Probabilistic SAM on the public LIDC-IDRI lung nodule dataset and demonstrate its ability to produce diverse outputs that align with expert disagreement, outperforming existing probabilistic baselines on uncertainty-aware metrics. Our code is available at: this https URL.</li>
</ul>

<h3>Title: Finetuning LLMs for Human Behavior Prediction in Social Science Experiments</h3>
<ul>
<li><strong>Authors: </strong>Akaash Kolluri, Shengguang Wu, Joon Sung Park, Michael S. Bernstein</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05830">https://arxiv.org/abs/2509.05830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05830">https://arxiv.org/pdf/2509.05830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05830]] Finetuning LLMs for Human Behavior Prediction in Social Science Experiments(https://arxiv.org/abs/2509.05830)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) offer a powerful opportunity to simulate the results of social science experiments. In this work, we demonstrate that finetuning LLMs directly on individual-level responses from past experiments meaningfully improves the accuracy of such simulations across diverse social science domains. We construct SocSci210 via an automatic pipeline, a dataset comprising 2.9 million responses from 400,491 participants in 210 open-source social science experiments. Through finetuning, we achieve multiple levels of generalization. In completely unseen studies, our strongest model, Socrates-Qwen-14B, produces predictions that are 26% more aligned with distributions of human responses to diverse outcome questions under varying conditions relative to its base model (Qwen2.5-14B), outperforming GPT-4o by 13%. By finetuning on a subset of conditions in a study, generalization to new unseen conditions is particularly robust, improving by 71%. Since SocSci210 contains rich demographic information, we reduce demographic parity, a measure of bias, by 10.6% through finetuning. Because social sciences routinely generate rich, topic-specific datasets, our findings indicate that finetuning on such data could enable more accurate simulations for experimental hypothesis screening. We release our data, models and finetuning code at this http URL.</li>
</ul>

<h3>Title: Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization</h3>
<ul>
<li><strong>Authors: </strong>Ishaan Verma</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05831">https://arxiv.org/abs/2509.05831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05831">https://arxiv.org/pdf/2509.05831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05831]] Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in Web Summarization(https://arxiv.org/abs/2509.05831)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly integrated into web-based systems for content summarization, yet their susceptibility to prompt injection attacks remains a pressing concern. In this study, we explore how non-visible HTML elements such as <meta>, aria-label, and alt attributes can be exploited to embed adversarial instructions without altering the visible content of a webpage. We introduce a novel dataset comprising 280 static web pages, evenly divided between clean and adversarial injected versions, crafted using diverse HTML-based strategies. These pages are processed through a browser automation pipeline to extract both raw HTML and rendered text, closely mimicking real-world LLM deployment scenarios. We evaluate two state-of-the-art open-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their ability to summarize this content. Using both lexical (ROUGE-L) and semantic (SBERT cosine similarity) metrics, along with manual annotations, we assess the impact of these covert injections. Our findings reveal that over 29% of injected samples led to noticeable changes in the Llama 4 Scout summaries, while Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These results highlight a critical and largely overlooked vulnerability in LLM driven web pipelines, where hidden adversarial content can subtly manipulate model outputs. Our work offers a reproducible framework and benchmark for evaluating HTML-based prompt injection and underscores the urgent need for robust mitigation strategies in LLM applications involving web content.</li>
</ul>

<h3>Title: Benchmarking Robust Aggregation in Decentralized Gradient Marketplaces</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Song, Sainyam Galhotra, Shagufta Mehnaz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05833">https://arxiv.org/abs/2509.05833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05833">https://arxiv.org/pdf/2509.05833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05833]] Benchmarking Robust Aggregation in Decentralized Gradient Marketplaces(https://arxiv.org/abs/2509.05833)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, federate, fair</a></li>
<li><strong>Abstract: </strong>The rise of distributed and privacy-preserving machine learning has sparked interest in decentralized gradient marketplaces, where participants trade intermediate artifacts like gradients. However, existing Federated Learning (FL) benchmarks overlook critical economic and systemic factors unique to such marketplaces-cost-effectiveness, fairness to sellers, and market stability-especially when a buyer relies on a private baseline dataset for evaluation. We introduce a comprehensive benchmark framework to holistically evaluate robust gradient aggregation methods within these buyer-baseline-reliant marketplaces. Our contributions include: (1) a simulation environment modeling marketplace dynamics with a variable buyer baseline and diverse seller distributions; (2) an evaluation methodology augmenting standard FL metrics with marketplace-centric dimensions such as Economic Efficiency, Fairness, and Selection Dynamics; (3) an in-depth empirical analysis of the existing Distributed Gradient Marketplace framework, MartFL, including the integration and comparative evaluation of adapted FLTrust and SkyMask as alternative aggregation strategies within it. This benchmark spans diverse datasets, local attacks, and Sybil attacks targeting the marketplace selection process; and (4) actionable insights into the trade-offs between model performance, robustness, cost, fairness, and stability. This benchmark equips the community with essential tools and empirical evidence to evaluate and design more robust, equitable, and economically viable decentralized gradient marketplaces.</li>
</ul>

<h3>Title: Yours or Mine? Overwriting Attacks against Neural Audio Watermarking</h3>
<ul>
<li><strong>Authors: </strong>Lingfeng Yao, Chenpei Huang, Shengyao Wang, Junpei Xue, Hanqing Guo, Jiang Liu, Phone Lin, Tomoaki Ohtsuki, Miao Pan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05835">https://arxiv.org/abs/2509.05835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05835">https://arxiv.org/pdf/2509.05835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05835]] Yours or Mine? Overwriting Attacks against Neural Audio Watermarking(https://arxiv.org/abs/2509.05835)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, robust, watermark, generative</a></li>
<li><strong>Abstract: </strong>As generative audio models are rapidly evolving, AI-generated audios increasingly raise concerns about copyright infringement and misinformation spread. Audio watermarking, as a proactive defense, can embed secret messages into audio for copyright protection and source verification. However, current neural audio watermarking methods focus primarily on the imperceptibility and robustness of watermarking, while ignoring its vulnerability to security attacks. In this paper, we develop a simple yet powerful attack: the overwriting attack that overwrites the legitimate audio watermark with a forged one and makes the original legitimate watermark undetectable. Based on the audio watermarking information that the adversary has, we propose three categories of overwriting attacks, i.e., white-box, gray-box, and black-box attacks. We also thoroughly evaluate the proposed attacks on state-of-the-art neural audio watermarking methods. Experimental results demonstrate that the proposed overwriting attacks can effectively compromise existing watermarking schemes across various settings and achieve a nearly 100% attack success rate. The practicality and effectiveness of the proposed overwriting attacks expose security flaws in existing neural audio watermarking systems, underscoring the need to enhance security in future audio watermarking designs.</li>
</ul>

<h3>Title: Data-Driven Stochastic Modeling Using Autoregressive Sequence Models: Translating Event Tables to Queueing Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Daksh Mittal, Shunri Zheng, Jing Dong, Hongseok Namkoong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05839">https://arxiv.org/abs/2509.05839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05839">https://arxiv.org/pdf/2509.05839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05839]] Data-Driven Stochastic Modeling Using Autoregressive Sequence Models: Translating Event Tables to Queueing Dynamics(https://arxiv.org/abs/2509.05839)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While queueing network models are powerful tools for analyzing service systems, they traditionally require substantial human effort and domain expertise to construct. To make this modeling approach more scalable and accessible, we propose a data-driven framework for queueing network modeling and simulation based on autoregressive sequence models trained on event-stream data. Instead of explicitly specifying arrival processes, service mechanisms, or routing logic, our approach learns the conditional distributions of event types and event times, recasting the modeling task as a problem of sequence distribution learning. We show that Transformer-style architectures can effectively parameterize these distributions, enabling automated construction of high-fidelity simulators. As a proof of concept, we validate our framework on event tables generated from diverse queueing networks, showcasing its utility in simulation, uncertainty quantification, and counterfactual evaluation. Leveraging advances in artificial intelligence and the growing availability of data, our framework takes a step toward more automated, data-driven modeling pipelines to support broader adoption of queueing network models across service domains.</li>
</ul>

<h3>Title: LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Luis Felipe Chary, Miguel Arjona Ramirez</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05863">https://arxiv.org/abs/2509.05863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05863">https://arxiv.org/pdf/2509.05863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05863]] LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization(https://arxiv.org/abs/2509.05863)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present LatinX, a multilingual text-to-speech (TTS) model for cascaded speech-to-speech translation that preserves the source speaker's identity across languages. LatinX is a 12-layer decoder-only Transformer trained in three stages: (i) pre-training for text-to-audio mapping, (ii) supervised fine-tuning for zero-shot voice cloning, and (iii) alignment with Direct Preference Optimization (DPO) using automatically labeled pairs based on Word Error Rate (WER) and speaker-similarity metrics. Trained on English and Romance languages with emphasis on Portuguese, LatinX with DPO consistently reduces WER and improves objective similarity over the fine-tuned baseline. Human evaluations further indicate stronger perceived speaker similarity than a strong baseline (XTTSv2), revealing gaps between objective and subjective measures. We provide cross-lingual analyses and discuss balanced preference signals and lower-latency architectures as future work.</li>
</ul>

<h3>Title: The Measure of Deception: An Analysis of Data Forging in Machine Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Rishabh Dixit, Yuan Hui, Rayan Saab</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05865">https://arxiv.org/abs/2509.05865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05865">https://arxiv.org/pdf/2509.05865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05865]] The Measure of Deception: An Analysis of Data Forging in Machine Unlearning(https://arxiv.org/abs/2509.05865)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Motivated by privacy regulations and the need to mitigate the effects of harmful data, machine unlearning seeks to modify trained models so that they effectively ``forget'' designated data. A key challenge in verifying unlearning is forging -- adversarially crafting data that mimics the gradient of a target point, thereby creating the appearance of unlearning without actually removing information. To capture this phenomenon, we consider the collection of data points whose gradients approximate a target gradient within tolerance $\epsilon$ -- which we call an $\epsilon$-forging set -- and develop a framework for its analysis. For linear regression and one-layer neural networks, we show that the Lebesgue measure of this set is small. It scales on the order of $\epsilon$, and when $\epsilon$ is small enough, $\epsilon^d$. More generally, under mild regularity assumptions, we prove that the forging set measure decays as $\epsilon^{(d-r)/2}$, where $d$ is the data dimension and $r<d$ is the nullity of a variation matrix defined by the model gradients. Extensions to batch SGD and almost-everywhere smooth loss functions yield the same asymptotic scaling. In addition, we establish probability bounds showing that, under non-degenerate data distributions, the likelihood of randomly sampling a forging point is vanishingly small. These results provide evidence that adversarial forging is fundamentally limited and that false unlearning claims can, in principle, be detected.</li>
</ul>

<h3>Title: ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula</h3>
<ul>
<li><strong>Authors: </strong>ZiXuan Zhang, Bowen Hao, Yingjie Li, Hongzhi Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05867">https://arxiv.org/abs/2509.05867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05867">https://arxiv.org/pdf/2509.05867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05867]] ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula(https://arxiv.org/abs/2509.05867)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Traditional Chinese Medicine (TCM) formulas play a significant role in treating epidemics and complex diseases. Existing models for TCM utilize traditional algorithms or deep learning techniques to analyze formula relationships, yet lack comprehensive results, such as complete formula compositions and detailed explanations. Although recent efforts have used TCM instruction datasets to fine-tune Large Language Models (LLMs) for explainable formula generation, existing datasets lack sufficient details, such as the roles of the formula's sovereign, minister, assistant, courier; efficacy; contraindications; tongue and pulse diagnosis-limiting the depth of model outputs. To address these challenges, we propose ZhiFangDanTai, a framework combining Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM fine-tuning. ZhiFangDanTai uses GraphRAG to retrieve and synthesize structured TCM knowledge into concise summaries, while also constructing an enhanced instruction dataset to improve LLMs' ability to integrate retrieved information. Furthermore, we provide novel theoretical proofs demonstrating that integrating GraphRAG with fine-tuning techniques can reduce generalization error and hallucination rates in the TCM formula task. Experimental results on both collected and clinical datasets demonstrate that ZhiFangDanTai achieves significant improvements over state-of-the-art models. Our model is open-sourced at this https URL.</li>
</ul>

<h3>Title: MedFactEval and MedAgentBrief: A Framework and Workflow for Generating and Evaluating Factual Clinical Summaries</h3>
<ul>
<li><strong>Authors: </strong>François Grolleau, Emily Alsentzer, Timothy Keyes, Philip Chung, Akshay Swaminathan, Asad Aali, Jason Hom, Tridu Huynh, Thomas Lew, April S. Liang, Weihan Chu, Natasha Z. Steele, Christina F. Lin, Jingkun Yang, Kameron C. Black, Stephen P. Ma, Fateme N. Haredasht, Nigam H. Shah, Kevin Schulman, Jonathan H. Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05878">https://arxiv.org/abs/2509.05878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05878">https://arxiv.org/pdf/2509.05878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05878]] MedFactEval and MedAgentBrief: A Framework and Workflow for Generating and Evaluating Factual Clinical Summaries(https://arxiv.org/abs/2509.05878)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Evaluating factual accuracy in Large Language Model (LLM)-generated clinical text is a critical barrier to adoption, as expert review is unscalable for the continuous quality assurance these systems require. We address this challenge with two complementary contributions. First, we introduce MedFactEval, a framework for scalable, fact-grounded evaluation where clinicians define high-salience key facts and an "LLM Jury"--a multi-LLM majority vote--assesses their inclusion in generated summaries. Second, we present MedAgentBrief, a model-agnostic, multi-step workflow designed to generate high-quality, factual discharge summaries. To validate our evaluation framework, we established a gold-standard reference using a seven-physician majority vote on clinician-defined key facts from inpatient cases. The MedFactEval LLM Jury achieved almost perfect agreement with this panel (Cohen's kappa=81%), a performance statistically non-inferior to that of a single human expert (kappa=67%, P < 0.001). Our work provides both a robust evaluation framework (MedFactEval) and a high-performing generation workflow (MedAgentBrief), offering a comprehensive approach to advance the responsible deployment of generative AI in clinical workflows.</li>
</ul>

<h3>Title: Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Abhijnan Nath, Carine Graff, Nikhil Krishnaswamy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05882">https://arxiv.org/abs/2509.05882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05882">https://arxiv.org/pdf/2509.05882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05882]] Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues(https://arxiv.org/abs/2509.05882)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) integrate into diverse workflows, they are increasingly being considered "collaborators" with humans. If such AI collaborators are to be reliable, their behavior over multiturn interactions must be predictable, validated and verified before deployment. Common alignment techniques are typically developed under simplified single-user settings and do not account for the dynamics of long-horizon multiparty interactions. This paper examines how different alignment methods affect LLM agents' effectiveness as partners in multiturn, multiparty collaborations. We study this question through the lens of friction agents that intervene in group dialogues to encourage the collaborative group to slow down and reflect upon their reasoning for deliberative decision-making. Using a roleplay methodology, we evaluate interventions from differently-trained friction agents in collaborative task conversations. We propose a novel counterfactual evaluation framework that quantifies how friction interventions change the trajectory of group collaboration and belief alignment. Our results show that a friction-aware approach significantly outperforms common alignment baselines in helping both convergence to a common ground, or agreed-upon task-relevant propositions, and correctness of task outcomes.</li>
</ul>

<h3>Title: Multimodal Prompt Injection Attacks: Risks and Defenses for Modern LLMs</h3>
<ul>
<li><strong>Authors: </strong>Andrew Yeo, Daeseon Choi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05883">https://arxiv.org/abs/2509.05883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05883">https://arxiv.org/pdf/2509.05883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05883]] Multimodal Prompt Injection Attacks: Risks and Defenses for Modern LLMs(https://arxiv.org/abs/2509.05883)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have seen rapid adoption in recent years, with industries increasingly relying on them to maintain a competitive advantage. These models excel at interpreting user instructions and generating human-like responses, leading to their integration across diverse domains, including consulting and information retrieval. However, their widespread deployment also introduces substantial security risks, most notably in the form of prompt injection and jailbreak attacks. To systematically evaluate LLM vulnerabilities -- particularly to external prompt injection -- we conducted a series of experiments on eight commercial models. Each model was tested without supplementary sanitization, relying solely on its built-in safeguards. The results exposed exploitable weaknesses and emphasized the need for stronger security measures. Four categories of attacks were examined: direct injection, indirect (external) injection, image-based injection, and prompt leakage. Comparative analysis indicated that Claude 3 demonstrated relatively greater robustness; nevertheless, empirical findings confirm that additional defenses, such as input normalization, remain necessary to achieve reliable protection.</li>
</ul>

<h3>Title: MemTraceDB: Reconstructing MySQL User Activity Using ActiviTimeTrace Algorithm</h3>
<ul>
<li><strong>Authors: </strong>Mahfuzul I. Nissan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05891">https://arxiv.org/abs/2509.05891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05891">https://arxiv.org/pdf/2509.05891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05891]] MemTraceDB: Reconstructing MySQL User Activity Using ActiviTimeTrace Algorithm(https://arxiv.org/abs/2509.05891)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Database audit and transaction logs are fundamental to forensic investigations, but they are vulnerable to tampering by privileged attackers. Malicious insiders or external threats with administrative access can alter, purge, or temporarily disable logging mechanisms, creating significant blind spots and rendering disk-based records unreliable. Memory analysis offers a vital alternative, providing investigators direct access to volatile artifacts that represent a ground-truth source of recent user activity, even when log files have been compromised. This paper introduces MemTraceDB, a tool that reconstructs user activity timelines by analyzing raw memory snapshots from the MySQL database process. MemTraceDB utilizes a novel algorithm, ActiviTimeTrace, to systematically extract and correlate forensic artifacts such as user connections and executed queries. Through a series of experiments, I demonstrate MemTraceDB's effectiveness and reveal a critical empirical finding: the MySQL query stack has a finite operational capacity of approximately 9,997 queries. This discovery allows me to establish a practical, data-driven formula for determining the optimal frequency for memory snapshot collection, providing a clear, actionable guideline for investigators. The result is a forensically-sound reconstruction of user activity, independent of compromised disk-based logs.</li>
</ul>

<h3>Title: Challenges in Deep Learning-Based Small Organ Segmentation: A Benchmarking Perspective for Medical Research with Limited Datasets</h3>
<ul>
<li><strong>Authors: </strong>Phongsakon Mark Konrad, Andrei-Alexandru Popa, Yaser Sabzehmeidani, Liang Zhong, Elisa A. Liehn, Serkan Ayvaz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05892">https://arxiv.org/abs/2509.05892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05892">https://arxiv.org/pdf/2509.05892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05892]] Challenges in Deep Learning-Based Small Organ Segmentation: A Benchmarking Perspective for Medical Research with Limited Datasets(https://arxiv.org/abs/2509.05892)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of carotid artery structures in histopathological images is vital for advancing cardiovascular disease research and diagnosis. However, deep learning model development in this domain is constrained by the scarcity of annotated cardiovascular histopathological data. This study investigates a systematic evaluation of state-of-the-art deep learning segmentation models, including convolutional neural networks (U-Net, DeepLabV3+), a Vision Transformer (SegFormer), and recent foundation models (SAM, MedSAM, MedSAM+UNet), on a limited dataset of cardiovascular histology images. Despite employing an extensive hyperparameter optimization strategy with Bayesian search, our findings reveal that model performance is highly sensitive to data splits, with minor differences driven more by statistical noise than by true algorithmic superiority. This instability exposes the limitations of standard benchmarking practices in low-data clinical settings and challenges the assumption that performance rankings reflect meaningful clinical utility.</li>
</ul>

<h3>Title: Wrangling Entropy: Next-Generation Multi-Factor Key Derivation, Credential Hashing, and Credential Generation Functions</h3>
<ul>
<li><strong>Authors: </strong>Colin Roberts, Vivek Nair, Dawn Song</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05893">https://arxiv.org/abs/2509.05893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05893">https://arxiv.org/pdf/2509.05893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05893]] Wrangling Entropy: Next-Generation Multi-Factor Key Derivation, Credential Hashing, and Credential Generation Functions(https://arxiv.org/abs/2509.05893)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>The Multi-Factor Key Derivation Function (MFKDF) offered a novel solution to the classic problem of usable client-side key management by incorporating multiple popular authentication factors into a key derivation process, but was later shown to be vulnerable to cryptanalysis that degraded its security over multiple invocations. In this paper, we present the Entropy State Transition Modeling Framework (ESTMF), a novel cryptanalytic technique designed to reveal pernicious leaks of entropy across multiple invocations of a cryptographic key derivation or hash function, and show that it can be used to correctly identify each of the known vulnerabilities in the original MFKDF construction. We then use these findings to propose a new construction for ``MFKDF2,'' a next-generation multi-factor key derivation function that can be proven to be end-to-end secure using the ESTMF. Finally, we discuss how MFKDF2 can be extended to support more authentication factors and usability features than the previous MFKDF construction, and derive several generalizable best-practices for the construction of new KDFs in the future.</li>
</ul>

<h3>Title: BTCChat: Advancing Remote Sensing Bi-temporal Change Captioning with Multimodal Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yujie Li, Wenjia Xu, Yuanben Zhang, Zhiwei Wei, Mugen Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05895">https://arxiv.org/abs/2509.05895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05895">https://arxiv.org/pdf/2509.05895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05895]] BTCChat: Advancing Remote Sensing Bi-temporal Change Captioning with Multimodal Large Language Model(https://arxiv.org/abs/2509.05895)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Bi-temporal satellite imagery supports critical applications such as urban development monitoring and disaster assessment. Although powerful multimodal large language models (MLLMs) have been applied in bi-temporal change analysis, previous methods process image pairs through direct concatenation, inadequately modeling temporal correlations and spatial semantic changes. This deficiency hampers visual-semantic alignment in change understanding, thereby constraining the overall effectiveness of current approaches. To address this gap, we propose BTCChat, a multi-temporal MLLM with advanced bi-temporal change understanding capability. BTCChat supports bi-temporal change captioning and retains single-image interpretation capability. To better capture temporal features and spatial semantic changes in image pairs, we design a Change Extraction module. Moreover, to enhance the model's attention to spatial details, we introduce a Prompt Augmentation mechanism, which incorporates contextual clues into the prompt to enhance model performance. Experimental results demonstrate that BTCChat achieves state-of-the-art performance on change captioning and visual question answering tasks.</li>
</ul>

<h3>Title: X-SQL: Expert Schema Linking and Understanding of Text-to-SQL with Multi-LLMs</h3>
<ul>
<li><strong>Authors: </strong>Dazhi Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05899">https://arxiv.org/abs/2509.05899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05899">https://arxiv.org/pdf/2509.05899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05899]] X-SQL: Expert Schema Linking and Understanding of Text-to-SQL with Multi-LLMs(https://arxiv.org/abs/2509.05899)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With Large Language Models' (LLMs) emergent abilities on code generation tasks, Text-to-SQL has become one of the most popular downstream applications. Despite the strong results of multiple recent LLM-based Text-to-SQL frameworks, the research community often overlooks the importance of database schema information for generating high-quality SQL queries. We find that such schema information plays a significant or even dominant role in the Text-to-SQL task. To tackle this challenge, we propose a novel database schema expert with two components. We first introduce X-Linking, an LLM Supervised Finetuning (SFT)-based method that achieves superior Schema Linking results compared to existing open-source Text-to-SQL methods. In addition, we innovatively propose an X-Admin component that focuses on Schema Understanding by bridging the gap between abstract schema information and the user's natural language question. Aside from better learning with schema information, we experiment with Multi-LLMs for different components within the system to further boost its performance. By incorporating these techniques into our end-to-end framework, X-SQL, we have achieved Execution Accuracies of 84.9% on the Spider-Dev dataset and 82.5% on the Spider-Test dataset. This outstanding performance establishes X-SQL as the leading Text-to-SQL framework based on open-source models.</li>
</ul>

<h3>Title: Enhancing the Robustness of Contextual ASR to Varying Biasing Information Volumes Through Purified Semantic Correlation Joint Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yue Gu, Zhihao Du, Ying Shi, Shiliang Zhang, Qian Chen, Jiqing Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05908">https://arxiv.org/abs/2509.05908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05908">https://arxiv.org/pdf/2509.05908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05908]] Enhancing the Robustness of Contextual ASR to Varying Biasing Information Volumes Through Purified Semantic Correlation Joint Modeling(https://arxiv.org/abs/2509.05908)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recently, cross-attention-based contextual automatic speech recognition (ASR) models have made notable advancements in recognizing personalized biasing phrases. However, the effectiveness of cross-attention is affected by variations in biasing information volume, especially when the length of the biasing list increases significantly. We find that, regardless of the length of the biasing list, only a limited amount of biasing information is most relevant to a specific ASR intermediate representation. Therefore, by identifying and integrating the most relevant biasing information rather than the entire biasing list, we can alleviate the effects of variations in biasing information volume for contextual ASR. To this end, we propose a purified semantic correlation joint modeling (PSC-Joint) approach. In PSC-Joint, we define and calculate three semantic correlations between the ASR intermediate representations and biasing information from coarse to fine: list-level, phrase-level, and token-level. Then, the three correlations are jointly modeled to produce their intersection, so that the most relevant biasing information across various granularities is highlighted and integrated for contextual recognition. In addition, to reduce the computational cost introduced by the joint modeling of three semantic correlations, we also propose a purification mechanism based on a grouped-and-competitive strategy to filter out irrelevant biasing phrases. Compared with baselines, our PSC-Joint approach achieves average relative F1 score improvements of up to 21.34% on AISHELL-1 and 28.46% on KeSpeech, across biasing lists of varying lengths.</li>
</ul>

<h3>Title: A Fine-Grained Attention and Geometric Correspondence Model for Musculoskeletal Risk Classification in Athletes Using Multimodal Visual and Skeletal Features</h3>
<ul>
<li><strong>Authors: </strong>Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Tamanna Shermin, Md Rafiqul Islam, Mukhtar Hussain, Sami Azam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05913">https://arxiv.org/abs/2509.05913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05913">https://arxiv.org/pdf/2509.05913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05913]] A Fine-Grained Attention and Geometric Correspondence Model for Musculoskeletal Risk Classification in Athletes Using Multimodal Visual and Skeletal Features(https://arxiv.org/abs/2509.05913)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Musculoskeletal disorders pose significant risks to athletes, and assessing risk early is important for prevention. However, most existing methods are designed for controlled settings and fail to reliably assess risk in complex environments due to their reliance on a single type of data. This research proposes ViSK-GAT (Visual-Skeletal Geometric Attention Transformer), a novel multimodal deep learning framework designed to classify musculoskeletal risk using visual and skeletal coordinate-based features. In addition, a custom multimodal dataset is constructed by combining visual data and skeletal coordinates for risk assessment. Each sample is labeled into eight risk categories based on the Rapid Entire Body Assessment system. ViSK-GAT combines a Residual Block with a Lightweight Transformer Block to learn spatial and temporal dependencies jointly. It incorporates two novel modules: the Fine-Grained Attention Module (FGAM), which enables precise inter-modal feature refinement through cross-attention between visual and skeletal inputs, and the Multimodal Geometric Correspondence Module (MGCM), which enhances cross-modal coherence by aligning image features with coordinate-based representations. ViSK-GAT achieved strong performance with validation and test accuracies of 93.55\% and 93.89\%, respectively; a precision of 93.86\%; an F1 score of 93.85\%; and Cohen's Kappa and Matthews Correlation Coefficient of 93\%. The regression results also indicated a low Root Mean Square Error of the predicted probability distribution of 0.1205 and a corresponding Mean Absolute Error of 0.0156. Compared to nine popular transfer learning backbones, ViSK-GAT consistently outperformed previous methods. The ViSK-GAT model advances artificial intelligence implementation and application, transforming musculoskeletal risk classification and enabling impactful early interventions in sports.</li>
</ul>

<h3>Title: Accelerating Large Language Model Inference via Early-Exiting Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Sangmin Bae</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05915">https://arxiv.org/abs/2509.05915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05915">https://arxiv.org/pdf/2509.05915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05915]] Accelerating Large Language Model Inference via Early-Exiting Algorithms(https://arxiv.org/abs/2509.05915)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models have achieved remarkable capabilities, but their practical deployment is hindered by significant computational costs. While adaptive computation methods like early-exiting promise to reduce these costs, they introduce a fundamental conflict: the per-token dynamism intended to save computation often creates system-level bottlenecks that can paradoxically reduce throughput in batched inference. This dissertation resolves this conflict by co-designing adaptive algorithms and model architectures to strike an optimal balance between dynamism and efficiency. To this end, our work first addresses critical sources of overhead in conventional early-exiting by proposing an efficient parallel decoding mechanism. We then show that deep parameter sharing provides an architectural foundation that not only yields compact, parameter-efficient models but also inherently mitigates the critical synchronization issues affecting dynamic inference. Finally, this work presents a unified framework where lightweight routers are pretrained to dynamically assign an optimal recursion depth for each token. This approach establishes a new Pareto frontier between efficiency and performance by effectively optimizing for both adaptive computation and parameter efficiency within a single model.</li>
</ul>

<h3>Title: Dataset Ownership in the Era of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kun Li, Cheng Wang, Minghui Xu, Yue Zhang, Xiuzhen Cheng</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05921">https://arxiv.org/abs/2509.05921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05921">https://arxiv.org/pdf/2509.05921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05921]] Dataset Ownership in the Era of Large Language Models(https://arxiv.org/abs/2509.05921)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, large language model</a></li>
<li><strong>Abstract: </strong>As datasets become critical assets in modern machine learning systems, ensuring robust copyright protection has emerged as an urgent challenge. Traditional legal mechanisms often fail to address the technical complexities of digital data replication and unauthorized use, particularly in opaque or decentralized environments. This survey provides a comprehensive review of technical approaches for dataset copyright protection, systematically categorizing them into three main classes: non-intrusive methods, which detect unauthorized use without modifying data; minimally-intrusive methods, which embed lightweight, reversible changes to enable ownership verification; and maximally-intrusive methods, which apply aggressive data alterations, such as reversible adversarial examples, to enforce usage restrictions. We synthesize key techniques, analyze their strengths and limitations, and highlight open research challenges. This work offers an organized perspective on the current landscape and suggests future directions for developing unified, scalable, and ethically sound solutions to protect datasets in increasingly complex machine learning ecosystems.</li>
</ul>

<h3>Title: Compression Beyond Pixels: Semantic Compression with Multimodal Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Ruiqi Shen, Haotian Wu, Wenjing Zhang, Jiangjing Hu, Deniz Gunduz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05925">https://arxiv.org/abs/2509.05925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05925">https://arxiv.org/pdf/2509.05925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05925]] Compression Beyond Pixels: Semantic Compression with Multimodal Foundation Models(https://arxiv.org/abs/2509.05925)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent deep learning-based methods for lossy image compression achieve competitive rate-distortion performance through extensive end-to-end training and advanced architectures. However, emerging applications increasingly prioritize semantic preservation over pixel-level reconstruction and demand robust performance across diverse data distributions and downstream tasks. These challenges call for advanced semantic compression paradigms. Motivated by the zero-shot and representational capabilities of multimodal foundation models, we propose a novel semantic compression method based on the contrastive language-image pretraining (CLIP) model. Rather than compressing images for reconstruction, we propose compressing the CLIP feature embeddings into minimal bits while preserving semantic information across different tasks. Experiments show that our method maintains semantic integrity across benchmark datasets, achieving an average bit rate of approximately 2-3* 10(-3) bits per pixel. This is less than 5% of the bitrate required by mainstream image compression approaches for comparable performance. Remarkably, even under extreme compression, the proposed approach exhibits zero-shot robustness across diverse data distributions and downstream tasks.</li>
</ul>

<h3>Title: Smoothed Online Optimization for Target Tracking: Robust and Learning-Augmented Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Ali Zeynali, Mahsa Sahebdel, Qingsong Liu, Mohammad Hajiesmaili, Ramesh K. Sitaraman</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05930">https://arxiv.org/abs/2509.05930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05930">https://arxiv.org/pdf/2509.05930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05930]] Smoothed Online Optimization for Target Tracking: Robust and Learning-Augmented Algorithms(https://arxiv.org/abs/2509.05930)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce the Smoothed Online Optimization for Target Tracking (SOOTT) problem, a new framework that integrates three key objectives in online decision-making under uncertainty: (1) tracking cost for following a dynamically moving target, (2) adversarial perturbation cost for withstanding unpredictable disturbances, and (3) switching cost for penalizing abrupt changes in decisions. This formulation captures real-world scenarios such as elastic and inelastic workload scheduling in AI clusters, where operators must balance long-term service-level agreements (e.g., LLM training) against sudden demand spikes (e.g., real-time inference). We first present BEST, a robust algorithm with provable competitive guarantees for SOOTT. To enhance practical performance, we introduce CoRT, a learning-augmented variant that incorporates untrusted black-box predictions (e.g., from ML models) into its decision process. Our theoretical analysis shows that CoRT strictly improves over BEST when predictions are accurate, while maintaining robustness under arbitrary prediction errors. We validate our approach through a case study on workload scheduling, demonstrating that both algorithms effectively balance trajectory tracking, decision smoothness, and resilience to external disturbances.</li>
</ul>

<h3>Title: Coefficients-Preserving Sampling for Reinforcement Learning with Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Feng Wang, Zihao Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05952">https://arxiv.org/abs/2509.05952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05952">https://arxiv.org/pdf/2509.05952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05952]] Coefficients-Preserving Sampling for Reinforcement Learning with Flow Matching(https://arxiv.org/abs/2509.05952)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) has recently emerged as a powerful technique for improving image and video generation in Diffusion and Flow Matching models, specifically for enhancing output quality and alignment with prompts. A critical step for applying online RL methods on Flow Matching is the introduction of stochasticity into the deterministic framework, commonly realized by Stochastic Differential Equation (SDE). Our investigation reveals a significant drawback to this approach: SDE-based sampling introduces pronounced noise artifacts in the generated images, which we found to be detrimental to the reward learning process. A rigorous theoretical analysis traces the origin of this noise to an excess of stochasticity injected during inference. To address this, we draw inspiration from Denoising Diffusion Implicit Models (DDIM) to reformulate the sampling process. Our proposed method, Coefficients-Preserving Sampling (CPS), eliminates these noise artifacts. This leads to more accurate reward modeling, ultimately enabling faster and more stable convergence for reinforcement learning-based optimizers like Flow-GRPO and Dance-GRPO. Code will be released at this https URL</li>
</ul>

<h3>Title: Dual Interaction Network with Cross-Image Attention for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jeonghyun Noh, Wangsu Jeon, Jinsun Park</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05953">https://arxiv.org/abs/2509.05953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05953">https://arxiv.org/pdf/2509.05953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05953]] Dual Interaction Network with Cross-Image Attention for Medical Image Segmentation(https://arxiv.org/abs/2509.05953)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation is a crucial method for assisting professionals in diagnosing various diseases through medical imaging. However, various factors such as noise, blurriness, and low contrast often hinder the accurate diagnosis of diseases. While numerous image enhancement techniques can mitigate these issues, they may also alter crucial information needed for accurate diagnosis in the original image. Conventional image fusion strategies, such as feature concatenation can address this challenge. However, they struggle to fully leverage the advantages of both original and enhanced images while suppressing the side effects of the enhancements. To overcome the problem, we propose a dual interactive fusion module (DIFM) that effectively exploits mutual complementary information from the original and enhanced images. DIFM employs cross-attention bidirectionally to simultaneously attend to corresponding spatial information across different images, subsequently refining the complementary features via global spatial attention. This interaction leverages low- to high-level features implicitly associated with diverse structural attributes like edges, blobs, and object shapes, resulting in enhanced features that embody important spatial characteristics. In addition, we introduce a multi-scale boundary loss based on gradient extraction to improve segmentation accuracy at object boundaries. Experimental results on the ACDC and Synapse datasets demonstrate the superiority of the proposed method quantitatively and qualitatively. Code available at: this https URL</li>
</ul>

<h3>Title: Spatial-Aware Self-Supervision for Medical 3D Imaging with Multi-Granularity Observable Tasks</h3>
<ul>
<li><strong>Authors: </strong>Yiqin Zhang, Meiling Chen, Zhengjie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05967">https://arxiv.org/abs/2509.05967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05967">https://arxiv.org/pdf/2509.05967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05967]] Spatial-Aware Self-Supervision for Medical 3D Imaging with Multi-Granularity Observable Tasks(https://arxiv.org/abs/2509.05967)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The application of self-supervised techniques has become increasingly prevalent within medical visualization tasks, primarily due to its capacity to mitigate the data scarcity prevalent in the healthcare sector. The majority of current works are influenced by designs originating in the generic 2D visual domain, which lack the intuitive demonstration of the model's learning process regarding 3D spatial knowledge. Consequently, these methods often fall short in terms of medical interpretability. We propose a method consisting of three sub-tasks to capture the spatially relevant semantics in medical 3D imaging. Their design adheres to observable principles to ensure interpretability, and minimize the performance loss caused thereby as much as possible. By leveraging the enhanced semantic depth offered by the extra dimension in 3D imaging, this approach incorporates multi-granularity spatial relationship modeling to maintain training stability. Experimental findings suggest that our approach is capable of delivering performance that is on par with current methodologies, while facilitating an intuitive understanding of the self-supervised learning process.</li>
</ul>

<h3>Title: ConstStyle: Robust Domain Generalization with Unified Style Transformation</h3>
<ul>
<li><strong>Authors: </strong>Nam Duong Tran, Nam Nguyen Phuong, Hieu H. Pham, Phi Le Nguyen, My T. Thai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05975">https://arxiv.org/abs/2509.05975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05975">https://arxiv.org/pdf/2509.05975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05975]] ConstStyle: Robust Domain Generalization with Unified Style Transformation(https://arxiv.org/abs/2509.05975)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks often suffer performance drops when test data distribution differs from training data. Domain Generalization (DG) aims to address this by focusing on domain-invariant features or augmenting data for greater diversity. However, these methods often struggle with limited training domains or significant gaps between seen (training) and unseen (test) domains. To enhance DG robustness, we hypothesize that it is essential for the model to be trained on data from domains that closely resemble unseen test domains-an inherently difficult task due to the absence of prior knowledge about the unseen domains. Accordingly, we propose ConstStyle, a novel approach that leverages a unified domain to capture domain-invariant features and bridge the domain gap with theoretical analysis. During training, all samples are mapped onto this unified domain, optimized for seen domains. During testing, unseen domain samples are projected similarly before predictions. By aligning both training and testing data within this unified domain, ConstStyle effectively reduces the impact of domain shifts, even with large domain gaps or few seen domains. Extensive experiments demonstrate that ConstStyle consistently outperforms existing methods across diverse scenarios. Notably, when only a limited number of seen domains are available, ConstStyle can boost accuracy up to 19.82\% compared to the next best approach.</li>
</ul>

<h3>Title: Multi-Strategy Guided Diffusion via Sparse Masking Temporal Reweighting Distribution Correction</h3>
<ul>
<li><strong>Authors: </strong>Zekun Zhou, Yanru Gong, Liu Shi, Qiegen Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05992">https://arxiv.org/abs/2509.05992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05992">https://arxiv.org/pdf/2509.05992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05992]] Multi-Strategy Guided Diffusion via Sparse Masking Temporal Reweighting Distribution Correction(https://arxiv.org/abs/2509.05992)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated remarkable generative capabilities in image processing tasks. We propose a Sparse condition Temporal Rewighted Integrated Distribution Estimation guided diffusion model (STRIDE) for sparse-view CT reconstruction. Specifically, we design a joint training mechanism guided by sparse conditional probabilities to facilitate the model effective learning of missing projection view completion and global information modeling. Based on systematic theoretical analysis, we propose a temporally varying sparse condition reweighting guidance strategy to dynamically adjusts weights during the progressive denoising process from pure noise to the real image, enabling the model to progressively perceive sparse-view information. The linear regression is employed to correct distributional shifts between known and generated data, mitigating inconsistencies arising during the guidance process. Furthermore, we construct a dual-network parallel architecture to perform global correction and optimization across multiple sub-frequency components, thereby effectively improving the model capability in both detail restoration and structural preservation, ultimately achieving high-quality image reconstruction. Experimental results on both public and real datasets demonstrate that the proposed method achieves the best improvement of 2.58 dB in PSNR, increase of 2.37\% in SSIM, and reduction of 0.236 in MSE compared to the best-performing baseline methods. The reconstructed images exhibit excellent generalization and robustness in terms of structural consistency, detail restoration, and artifact suppression.</li>
</ul>

<h3>Title: S-LAM3D: Segmentation-Guided Monocular 3D Object Detection via Feature Space Fusion</h3>
<ul>
<li><strong>Authors: </strong>Diana-Alexandra Sas, Florin Oniga</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05999">https://arxiv.org/abs/2509.05999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05999">https://arxiv.org/pdf/2509.05999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05999]] S-LAM3D: Segmentation-Guided Monocular 3D Object Detection via Feature Space Fusion(https://arxiv.org/abs/2509.05999)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Monocular 3D Object Detection represents a challenging Computer Vision task due to the nature of the input used, which is a single 2D image, lacking in any depth cues and placing the depth estimation problem as an ill-posed one. Existing solutions leverage the information extracted from the input by using Convolutional Neural Networks or Transformer architectures as feature extraction backbones, followed by specific detection heads for 3D parameters prediction. In this paper, we introduce a decoupled strategy based on injecting precomputed segmentation information priors and fusing them directly into the feature space for guiding the detection, without expanding the detection model or jointly learning the priors. The focus is on evaluating the impact of additional segmentation information on existing detection pipelines without adding additional prediction branches. The proposed method is evaluated on the KITTI 3D Object Detection Benchmark, outperforming the equivalent architecture that relies only on RGB image features for small objects in the scene: pedestrians and cyclists, and proving that understanding the input data can balance the need for additional sensors or training data.</li>
</ul>

<h3>Title: Motion Aware ViT-based Framework for Monocular 6-DoF Spacecraft Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Jose Sosa, Dan Pineau, Arunkumar Rathinam, Abdelrahman Shabayek, Djamila Aouada</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06000">https://arxiv.org/abs/2509.06000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06000">https://arxiv.org/pdf/2509.06000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06000]] Motion Aware ViT-based Framework for Monocular 6-DoF Spacecraft Pose Estimation(https://arxiv.org/abs/2509.06000)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Monocular 6-DoF pose estimation plays an important role in multiple spacecraft missions. Most existing pose estimation approaches rely on single images with static keypoint localisation, failing to exploit valuable temporal information inherent to space operations. In this work, we adapt a deep learning framework from human pose estimation to the spacecraft pose estimation domain that integrates motion-aware heatmaps and optical flow to capture motion dynamics. Our approach combines image features from a Vision Transformer (ViT) encoder with motion cues from a pre-trained optical flow model to localise 2D keypoints. Using the estimates, a Perspective-n-Point (PnP) solver recovers 6-DoF poses from known 2D-3D correspondences. We train and evaluate our method on the SPADES-RGB dataset and further assess its generalisation on real and synthetic data from the SPARK-2024 dataset. Overall, our approach demonstrates improved performance over single-image baselines in both 2D keypoint localisation and 6-DoF pose estimation. Furthermore, it shows promising generalisation capabilities when testing on different data distributions.</li>
</ul>

<h3>Title: Khana: A Comprehensive Indian Cuisine Dataset</h3>
<ul>
<li><strong>Authors: </strong>Omkar Prabhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06006">https://arxiv.org/abs/2509.06006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06006">https://arxiv.org/pdf/2509.06006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06006]] Khana: A Comprehensive Indian Cuisine Dataset(https://arxiv.org/abs/2509.06006)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>As global interest in diverse culinary experiences grows, food image models are essential for improving food-related applications by enabling accurate food recognition, recipe suggestions, dietary tracking, and automated meal planning. Despite the abundance of food datasets, a noticeable gap remains in capturing the nuances of Indian cuisine due to its vast regional diversity, complex preparations, and the lack of comprehensive labeled datasets that cover its full breadth. Through this exploration, we uncover Khana, a new benchmark dataset for food image classification, segmentation, and retrieval of dishes from Indian cuisine. Khana fills the gap by establishing a taxonomy of Indian cuisine and offering around 131K images in the dataset spread across 80 labels, each with a resolution of 500x500 pixels. This paper describes the dataset creation process and evaluates state-of-the-art models on classification, segmentation, and retrieval as baselines. Khana bridges the gap between research and development by providing a comprehensive and challenging benchmark for researchers while also serving as a valuable resource for developers creating real-world applications that leverage the rich tapestry of Indian cuisine. Webpage: this https URL</li>
</ul>

<h3>Title: BLaVe-CoT: Consistency-Aware Visual Question Answering for Blind and Low Vision Users</h3>
<ul>
<li><strong>Authors: </strong>Wanyin Cheng, Zanxi Ruan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06010">https://arxiv.org/abs/2509.06010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06010">https://arxiv.org/pdf/2509.06010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06010]] BLaVe-CoT: Consistency-Aware Visual Question Answering for Blind and Low Vision Users(https://arxiv.org/abs/2509.06010)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Visual Question Answering (VQA) holds great potential for assisting Blind and Low Vision (BLV) users, yet real-world usage remains challenging. Due to visual impairments, BLV users often take blurry or poorly framed photos and face difficulty in articulating specific questions about what they cannot fully see. As a result, their visual questions are frequently ambiguous, and different users may interpret them in diverse ways. This leads to multiple valid answers, each grounded in different image regions-posing a mismatch with conventional VQA systems that assume a single answer and region. To bridge this gap, we present BLaVe-CoT, a VQA framework designed to reason about answer consistency in the face of ambiguity. Our method proposes diverse candidate answers using a LoRA-tuned BLIP-2 model, then grounds each answer spatially using PolyFormer, and finally applies a chain-of-thought reasoning module to assess whether the answers refer to the same or different regions. Evaluated on the VQA-AnswerTherapy benchmark, BLaVe-CoT outperforms previous methods and proves more robust to the ambiguity and visual noise common in assistive settings. This work highlights the need for VQA systems that can adapt to real human uncertainty and provide inclusive support for BLV users. To foster further research and accessibility applications, we have made the code publicly available at this https URL.</li>
</ul>

<h3>Title: Micro-Expression Recognition via Fine-Grained Dynamic Perception</h3>
<ul>
<li><strong>Authors: </strong>Zhiwen Shao, Yifan Cheng, Fan Zhang, Xuehuai Shi, Canlin Li, Lizhuang Ma, Dit-yan Yeung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06015">https://arxiv.org/abs/2509.06015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06015">https://arxiv.org/pdf/2509.06015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06015]] Micro-Expression Recognition via Fine-Grained Dynamic Perception(https://arxiv.org/abs/2509.06015)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Facial micro-expression recognition (MER) is a challenging task, due to the transience, subtlety, and dynamics of micro-expressions (MEs). Most existing methods resort to hand-crafted features or deep networks, in which the former often additionally requires key frames, and the latter suffers from small-scale and low-diversity training data. In this paper, we develop a novel fine-grained dynamic perception (FDP) framework for MER. We propose to rank frame-level features of a sequence of raw frames in chronological order, in which the rank process encodes the dynamic information of both ME appearances and motions. Specifically, a novel local-global feature-aware transformer is proposed for frame representation learning. A rank scorer is further adopted to calculate rank scores of each frame-level feature. Afterwards, the rank features from rank scorer are pooled in temporal dimension to capture dynamic representation. Finally, the dynamic representation is shared by a MER module and a dynamic image construction module, in which the former predicts the ME category, and the latter uses an encoder-decoder structure to construct the dynamic image. The design of dynamic image construction task is beneficial for capturing facial subtle actions associated with MEs and alleviating the data scarcity issue. Extensive experiments show that our method (i) significantly outperforms the state-of-the-art MER methods, and (ii) works well for dynamic image construction. Particularly, our FDP improves by 4.05%, 2.50%, 7.71%, and 2.11% over the previous best results in terms of F1-score on the CASME II, SAMM, CAS(ME)^2, and CAS(ME)^3 datasets, respectively. The code is available at this https URL.</li>
</ul>

<h3>Title: DVLO4D: Deep Visual-Lidar Odometry with Sparse Spatial-temporal Fusion</h3>
<ul>
<li><strong>Authors: </strong>Mengmeng Liu, Michael Ying Yang, Jiuming Liu, Yunpeng Zhang, Jiangtao Li, Sander Oude Elberink, George Vosselman, Hao Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06023">https://arxiv.org/abs/2509.06023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06023">https://arxiv.org/pdf/2509.06023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06023]] DVLO4D: Deep Visual-Lidar Odometry with Sparse Spatial-temporal Fusion(https://arxiv.org/abs/2509.06023)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Visual-LiDAR odometry is a critical component for autonomous system localization, yet achieving high accuracy and strong robustness remains a challenge. Traditional approaches commonly struggle with sensor misalignment, fail to fully leverage temporal information, and require extensive manual tuning to handle diverse sensor configurations. To address these problems, we introduce DVLO4D, a novel visual-LiDAR odometry framework that leverages sparse spatial-temporal fusion to enhance accuracy and robustness. Our approach proposes three key innovations: (1) Sparse Query Fusion, which utilizes sparse LiDAR queries for effective multi-modal data fusion; (2) a Temporal Interaction and Update module that integrates temporally-predicted positions with current frame data, providing better initialization values for pose estimation and enhancing model's robustness against accumulative errors; and (3) a Temporal Clip Training strategy combined with a Collective Average Loss mechanism that aggregates losses across multiple frames, enabling global optimization and reducing the scale drift over long sequences. Extensive experiments on the KITTI and Argoverse Odometry dataset demonstrate the superiority of our proposed DVLO4D, which achieves state-of-the-art performance in terms of both pose accuracy and robustness. Additionally, our method has high efficiency, with an inference time of 82 ms, possessing the potential for the real-time deployment.</li>
</ul>

<h3>Title: DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Gao, Xiangtao Meng, Yingkai Dong, Zheng Li, Shanqing Guo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06026">https://arxiv.org/abs/2509.06026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06026">https://arxiv.org/pdf/2509.06026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06026]] DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation(https://arxiv.org/abs/2509.06026)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, membership infer</a></li>
<li><strong>Abstract: </strong>While Retrieval-Augmented Generation (RAG) effectively reduces hallucinations by integrating external knowledge bases, it introduces vulnerabilities to membership inference attacks (MIAs), particularly in systems handling sensitive data. Existing MIAs targeting RAG's external databases often rely on model responses but ignore the interference of non-member-retrieved documents on RAG outputs, limiting their effectiveness. To address this, we propose DCMI, a differential calibration MIA that mitigates the negative impact of non-member-retrieved documents. Specifically, DCMI leverages the sensitivity gap between member and non-member retrieved documents under query perturbation. It generates perturbed queries for calibration to isolate the contribution of member-retrieved documents while minimizing the interference from non-member-retrieved documents. Experiments under progressively relaxed assumptions show that DCMI consistently outperforms baselines--for example, achieving 97.42% AUC and 94.35% Accuracy against the RAG system with Flan-T5, exceeding the MBA baseline by over 40%. Furthermore, on real-world RAG platforms such as Dify and MaxKB, DCMI maintains a 10%-20% advantage over the baseline. These results highlight significant privacy risks in RAG systems and emphasize the need for stronger protection mechanisms. We appeal to the community's consideration of deeper investigations, like ours, against the data leakage risks in rapidly evolving RAG systems. Our code is available at this https URL.</li>
</ul>

<h3>Title: TinyDef-DETR:An Enhanced DETR Detector for UAV Power Line Defect Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Cui</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06035">https://arxiv.org/abs/2509.06035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06035">https://arxiv.org/pdf/2509.06035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06035]] TinyDef-DETR:An Enhanced DETR Detector for UAV Power Line Defect Detection(https://arxiv.org/abs/2509.06035)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Automated inspection of transmission lines using UAVs is hindered by the difficulty of detecting small and ambiguous defects against complex backgrounds. Conventional detectors often suffer from detail loss due to strided downsampling, weak boundary sensitivity in lightweight backbones, and insufficient integration of global context with local cues. To address these challenges, we propose TinyDef-DETR, a DETR-based framework designed for small-defect detection. The method introduces a stride-free space-to-depth module for lossless downsampling, an edge-enhanced convolution for boundary-aware feature extraction, a cross-stage dual-domain multi-scale attention module to jointly capture global and local information, and a Focaler-Wise-SIoU regression loss to improve localization of small objects. Experiments conducted on the CSG-ADCD dataset demonstrate that TinyDef-DETR achieves substantial improvements in both precision and recall compared to competitive baselines, with particularly notable gains on small-object subsets, while incurring only modest computational overhead. Further validation on the VisDrone benchmark confirms the generalization capability of the proposed approach. Overall, the results indicate that integrating detail-preserving downsampling, edge-sensitive representations, dual-domain attention, and difficulty-adaptive regression provides a practical and efficient solution for UAV-based small-defect inspection in power grids.</li>
</ul>

<h3>Title: BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yuming Li, Yikai Wang, Yuying Zhu, Zhongyu Zhao, Ming Lu, Qi She, Shanghang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06040">https://arxiv.org/abs/2509.06040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06040">https://arxiv.org/pdf/2509.06040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06040]] BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models(https://arxiv.org/abs/2509.06040)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in aligning image and video generative models via GRPO have achieved remarkable gains in enhancing human preference alignment. However, these methods still face high computational costs from on-policy rollouts and excessive SDE sampling steps, as well as training instability due to sparse rewards. In this paper, we propose BranchGRPO, a novel method that introduces a branch sampling policy updating the SDE sampling process. By sharing computation across common prefixes and pruning low-reward paths and redundant depths, BranchGRPO substantially lowers the per-update compute cost while maintaining or improving exploration diversity. This work makes three main contributions: (1) a branch sampling scheme that reduces rollout and training cost; (2) a tree-based advantage estimator incorporating dense process-level rewards; and (3) pruning strategies exploiting path and depth redundancy to accelerate convergence and boost performance. Experiments on image and video preference alignment show that BranchGRPO improves alignment scores by 16% over strong baselines, while cutting training time by 50%.</li>
</ul>

<h3>Title: PolicyEvolve: Evolving Programmatic Policies by LLMs for multi-player games via Population-Based Training</h3>
<ul>
<li><strong>Authors: </strong>Mingrui Lv, Hangzhi Liu, Zhi Luo, Hongjie Zhang, Jie Ou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06053">https://arxiv.org/abs/2509.06053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06053">https://arxiv.org/pdf/2509.06053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06053]] PolicyEvolve: Evolving Programmatic Policies by LLMs for multi-player games via Population-Based Training(https://arxiv.org/abs/2509.06053)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Multi-agent reinforcement learning (MARL) has achieved significant progress in solving complex multi-player games through self-play. However, training effective adversarial policies requires millions of experience samples and substantial computational resources. Moreover, these policies lack interpretability, hindering their practical deployment. Recently, researchers have successfully leveraged Large Language Models (LLMs) to generate programmatic policies for single-agent tasks, transforming neural network-based policies into interpretable rule-based code with high execution efficiency. Inspired by this, we propose PolicyEvolve, a general framework for generating programmatic policies in multi-player games. PolicyEvolve significantly reduces reliance on manually crafted policy code, achieving high-performance policies with minimal environmental interactions. The framework comprises four modules: Global Pool, Local Pool, Policy Planner, and Trajectory Critic. The Global Pool preserves elite policies accumulated during iterative training. The Local Pool stores temporary policies for the current iteration; only sufficiently high-performing policies from this pool are promoted to the Global Pool. The Policy Planner serves as the core policy generation module. It samples the top three policies from the Global Pool, generates an initial policy for the current iteration based on environmental information, and refines this policy using feedback from the Trajectory Critic. Refined policies are then deposited into the Local Pool. This iterative process continues until the policy achieves a sufficiently high average win rate against the Global Pool, at which point it is integrated into the Global Pool. The Trajectory Critic analyzes interaction data from the current policy, identifies vulnerabilities, and proposes directional improvements to guide the Policy Planner</li>
</ul>

<h3>Title: KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino</h3>
<ul>
<li><strong>Authors: </strong>Lorenzo Alfred Nery, Ronald Dawson Catignas, Thomas James Tiam-Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06065">https://arxiv.org/abs/2509.06065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06065">https://arxiv.org/pdf/2509.06065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06065]] KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino(https://arxiv.org/abs/2509.06065)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) achieve remarkable performance across various tasks, but their tendency to produce hallucinations limits reliable adoption. Benchmarks such as TruthfulQA have been developed to measure truthfulness, yet they are primarily available in English, leaving a gap in evaluating LLMs in low-resource languages. To address this, we present KatotohananQA, a Filipino translation of the TruthfulQA benchmark. Seven free-tier proprietary models were assessed using a binary-choice framework. Findings show a significant performance gap between English and Filipino truthfulness, with newer OpenAI models (GPT-5 and GPT-5 mini) demonstrating strong multilingual robustness. Results also reveal disparities across question characteristics, suggesting that some question types, categories, and topics are less robust to multilingual transfer which highlight the need for broader multilingual evaluation to ensure fairness and reliability in LLM usage.</li>
</ul>

<h3>Title: Home-made Diffusion Model from Scratch to Hatch</h3>
<ul>
<li><strong>Authors: </strong>Shih-Ying Yeh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06068">https://arxiv.org/abs/2509.06068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06068">https://arxiv.org/pdf/2509.06068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06068]] Home-made Diffusion Model from Scratch to Hatch(https://arxiv.org/abs/2509.06068)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We introduce Home-made Diffusion Model (HDM), an efficient yet powerful text-to-image diffusion model optimized for training (and inferring) on consumer-grade hardware. HDM achieves competitive 1024x1024 generation quality while maintaining a remarkably low training cost of $535-620 using four RTX5090 GPUs, representing a significant reduction in computational requirements compared to traditional approaches. Our key contributions include: (1) Cross-U-Transformer (XUT), a novel U-shape transformer, Cross-U-Transformer (XUT), that employs cross-attention for skip connections, providing superior feature integration that leads to remarkable compositional consistency; (2) a comprehensive training recipe that incorporates TREAD acceleration, a novel shifted square crop strategy for efficient arbitrary aspect-ratio training, and progressive resolution scaling; and (3) an empirical demonstration that smaller models (343M parameters) with carefully crafted architectures can achieve high-quality results and emergent capabilities, such as intuitive camera control. Our work provides an alternative paradigm of scaling, demonstrating a viable path toward democratizing high-quality text-to-image generation for individual researchers and smaller organizations with limited computational resources.</li>
</ul>

<h3>Title: Asymmetry Vulnerability and Physical Attacks on Online Map Construction for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Yang Lou, Haibo Hu, Qun Song, Qian Xu, Yi Zhu, Rui Tan, Wei-Bin Lee, Jianping Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06071">https://arxiv.org/abs/2509.06071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06071">https://arxiv.org/pdf/2509.06071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06071]] Asymmetry Vulnerability and Physical Attacks on Online Map Construction for Autonomous Driving(https://arxiv.org/abs/2509.06071)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>High-definition maps provide precise environmental information essential for prediction and planning in autonomous driving systems. Due to the high cost of labeling and maintenance, recent research has turned to online HD map construction using onboard sensor data, offering wider coverage and more timely updates for autonomous vehicles. However, the robustness of online map construction under adversarial conditions remains underexplored. In this paper, we present a systematic vulnerability analysis of online map construction models, which reveals that these models exhibit an inherent bias toward predicting symmetric road structures. In asymmetric scenes like forks or merges, this bias often causes the model to mistakenly predict a straight boundary that mirrors the opposite side. We demonstrate that this vulnerability persists in the real-world and can be reliably triggered by obstruction or targeted interference. Leveraging this vulnerability, we propose a novel two-stage attack framework capable of manipulating online constructed maps. First, our method identifies vulnerable asymmetric scenes along the victim AV's potential route. Then, we optimize the location and pattern of camera-blinding attacks and adversarial patch attacks. Evaluations on a public AD dataset demonstrate that our attacks can degrade mapping accuracy by up to 9.9%, render up to 44% of targeted routes unreachable, and increase unsafe planned trajectory rates, colliding with real-world road boundaries, by up to 27%. These attacks are also validated on a real-world testbed vehicle. We further analyze root causes of the symmetry bias, attributing them to training data imbalance, model architecture, and map element representation. To the best of our knowledge, this study presents the first vulnerability assessment of online map construction models and introduces the first digital and physical attack against them.</li>
</ul>

<h3>Title: Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge</h3>
<ul>
<li><strong>Authors: </strong>Hao Liang, Ruitao Wu, Bohan Zeng, Junbo Niu, Wentao Zhang, Bin Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06079">https://arxiv.org/abs/2509.06079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06079">https://arxiv.org/pdf/2509.06079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06079]] Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge(https://arxiv.org/abs/2509.06079)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal reasoning remains a fundamental challenge in artificial intelligence. Despite substantial advances in text-based reasoning, even state-of-the-art models such as GPT-o3 struggle to maintain strong performance in multimodal scenarios. To address this gap, we introduce a caption-assisted reasoning framework that effectively bridges visual and textual modalities. Our approach achieved 1st place in the ICML 2025 AI for Math Workshop \& Challenge 2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we validate its generalization on the MathVerse benchmark for geometric reasoning, demonstrating the versatility of our method. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: MedSeqFT: Sequential Fine-tuning Foundation Models for 3D Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yiwen Ye, Yicheng Wu, Xiangde Luo, He Zhang, Ziyang Chen, Ting Dang, Yanning Zhang, Yong Xia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06096">https://arxiv.org/abs/2509.06096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06096">https://arxiv.org/pdf/2509.06096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06096]] MedSeqFT: Sequential Fine-tuning Foundation Models for 3D Medical Image Segmentation(https://arxiv.org/abs/2509.06096)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Foundation models have become a promising paradigm for advancing medical image analysis, particularly for segmentation tasks where downstream applications often emerge sequentially. Existing fine-tuning strategies, however, remain limited: parallel fine-tuning isolates tasks and fails to exploit shared knowledge, while multi-task fine-tuning requires simultaneous access to all datasets and struggles with incremental task integration. To address these challenges, we propose MedSeqFT, a sequential fine-tuning framework that progressively adapts pre-trained models to new tasks while refining their representational capacity. MedSeqFT introduces two core components: (1) Maximum Data Similarity (MDS) selection, which identifies downstream samples most representative of the original pre-training distribution to preserve general knowledge, and (2) Knowledge and Generalization Retention Fine-Tuning (K&G RFT), a LoRA-based knowledge distillation scheme that balances task-specific adaptation with the retention of pre-trained knowledge. Extensive experiments on two multi-task datasets covering ten 3D segmentation tasks demonstrate that MedSeqFT consistently outperforms state-of-the-art fine-tuning strategies, yielding substantial performance gains (e.g., an average Dice improvement of 3.0%). Furthermore, evaluations on two unseen tasks (COVID-19-20 and Kidney) verify that MedSeqFT enhances transferability, particularly for tumor segmentation. Visual analyses of loss landscapes and parameter variations further highlight the robustness of MedSeqFT. These results establish sequential fine-tuning as an effective, knowledge-retentive paradigm for adapting foundation models to evolving clinical tasks. Code will be released.</li>
</ul>

<h3>Title: Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kefan Cao, Shuaicheng Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06100">https://arxiv.org/abs/2509.06100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06100">https://arxiv.org/pdf/2509.06100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06100]] Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of Large Language Models(https://arxiv.org/abs/2509.06100)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are prone to catastrophic forgetting in sequential multi-task settings. Parameter regularization methods such as O-LoRA and N-LoRA alleviate task interference by enforcing low-rank subspace orthogonality, but they overlook the fact that conventional additive fine-tuning disrupts the intrinsic geometric structure of LLM parameters, limiting performance. Our key insight is that the parameter space of LLMs possesses a geometric structure, which must be preserved in addition to enforcing orthogonality. Based on this, we propose Orthogonal Low-rank Adaptation in Lie Groups (OLieRA), which introduces Lie group theory into LLM fine-tuning: leveraging multiplicative updates to preserve parameter geometry while applying orthogonality constraints to task subspaces. Experiments demonstrate that OLieRA achieves state-of-the-art results on the Standard CL benchmark and remains among the top-performing methods in the Large Number of Tasks setting.</li>
</ul>

<h3>Title: Towards Reliable Service Provisioning for Dynamic UAV Clusters in Low-Altitude Economy Networks</h3>
<ul>
<li><strong>Authors: </strong>Yanwei Gong, Ruichen Zhang, Xiaoqing Wang, Xiaolin Chang, Bo Ai, Junchao Fan, Bocheng Ju, Dusit Niyato</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06112">https://arxiv.org/abs/2509.06112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06112">https://arxiv.org/pdf/2509.06112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06112]] Towards Reliable Service Provisioning for Dynamic UAV Clusters in Low-Altitude Economy Networks(https://arxiv.org/abs/2509.06112)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, robust</a></li>
<li><strong>Abstract: </strong>Unmanned Aerial Vehicle (UAV) cluster services are crucial for promoting the low-altitude economy by enabling scalable, flexible, and adaptive aerial networks. To meet diverse service demands, clusters must dynamically incorporate a New UAVs (NUAVs) or an Existing UAV (EUAV). However, achieving sustained service reliability remains challenging due to the need for efficient and scalable NUAV authentication, privacy-preserving cross-cluster authentication for EUAVs, and robust protection of the cluster session key, including both forward and backward secrecy. To address these challenges, we propose a Lightweight and Privacy-Preserving Cluster Authentication and Session Key Update (LP2-CASKU) scheme tailored for dynamic UAV clusters in low-altitude economy networks. LP2-CASKU integrates an efficient batch authentication mechanism that simultaneously authenticates multiple NUAVs with minimal communication overhead. It further introduces a lightweight cross-cluster authentication mechanism that ensures EUAV anonymity and unlinkability. Additionally, a secure session key update mechanism is incorporated to maintain key confidentiality over time, thereby preserving both forward and backward secrecy. We provide a comprehensive security analysis and evaluate LP2-CASKU performance through both theoretical analysis and OMNeT++ simulations. Experimental results demonstrate that, compared to the baseline, LP2-CASKU achieves a latency reduction of 82.8%-90.8% by across different UAV swarm configurations and network bitrates, demonstrating strong adaptability to dynamic communication environments. Besides, under varying UAV swarm configurations, LP2-CASKU reduces the energy consumption by approximately 37.6-72.6%, while effectively supporting privacy-preserving authentication in highly dynamic UAV cluster environments.</li>
</ul>

<h3>Title: If generative AI is the answer, what is the question?</h3>
<ul>
<li><strong>Authors: </strong>Ambuj Tewari</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06120">https://arxiv.org/abs/2509.06120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06120">https://arxiv.org/pdf/2509.06120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06120]] If generative AI is the answer, what is the question?(https://arxiv.org/abs/2509.06120)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Beginning with text and images, generative AI has expanded to audio, video, computer code, and molecules. Yet, if generative AI is the answer, what is the question? We explore the foundations of generation as a distinct machine learning task with connections to prediction, compression, and decision-making. We survey five major generative model families: autoregressive models, variational autoencoders, normalizing flows, generative adversarial networks, and diffusion models. We then introduce a probabilistic framework that emphasizes the distinction between density estimation and generation. We review a game-theoretic framework with a two-player adversary-learner setup to study generation. We discuss post-training modifications that prepare generative models for deployment. We end by highlighting some important topics in socially responsible generation such as privacy, detection of AI-generated content, and copyright and IP. We adopt a task-first framing of generation, focusing on what generation is as a machine learning problem, rather than only on how models implement it.</li>
</ul>

<h3>Title: SpecSwin3D: Generating Hyperspectral Imagery from Multispectral Data via Transformer Networks</h3>
<ul>
<li><strong>Authors: </strong>Tang Sui, Songxi Yang, Qunying Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06122">https://arxiv.org/abs/2509.06122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06122">https://arxiv.org/pdf/2509.06122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06122]] SpecSwin3D: Generating Hyperspectral Imagery from Multispectral Data via Transformer Networks(https://arxiv.org/abs/2509.06122)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Multispectral and hyperspectral imagery are widely used in agriculture, environmental monitoring, and urban planning due to their complementary spatial and spectral characteristics. A fundamental trade-off persists: multispectral imagery offers high spatial but limited spectral resolution, while hyperspectral imagery provides rich spectra at lower spatial resolution. Prior hyperspectral generation approaches (e.g., pan-sharpening variants, matrix factorization, CNNs) often struggle to jointly preserve spatial detail and spectral fidelity. In response, we propose SpecSwin3D, a transformer-based model that generates hyperspectral imagery from multispectral inputs while preserving both spatial and spectral quality. Specifically, SpecSwin3D takes five multispectral bands as input and reconstructs 224 hyperspectral bands at the same spatial resolution. In addition, we observe that reconstruction errors grow for hyperspectral bands spectrally distant from the input bands. To address this, we introduce a cascade training strategy that progressively expands the spectral range to stabilize learning and improve fidelity. Moreover, we design an optimized band sequence that strategically repeats and orders the five selected multispectral bands to better capture pairwise relations within a 3D shifted-window transformer framework. Quantitatively, our model achieves a PSNR of 35.82 dB, SAM of 2.40°, and SSIM of 0.96, outperforming the baseline MHF-Net by +5.6 dB in PSNR and reducing ERGAS by more than half. Beyond reconstruction, we further demonstrate the practical value of SpecSwin3D on two downstream tasks, including land use classification and burnt area segmentation.</li>
</ul>

<h3>Title: CSI-IBBS: Identity-Based Blind Signature using CSIDH</h3>
<ul>
<li><strong>Authors: </strong>Soumya Bhoumik, Sarbari Mitra, Rohit Raj Sharma, Kuldeep Namdeo</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06127">https://arxiv.org/abs/2509.06127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06127">https://arxiv.org/pdf/2509.06127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06127]] CSI-IBBS: Identity-Based Blind Signature using CSIDH(https://arxiv.org/abs/2509.06127)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect</a></li>
<li><strong>Abstract: </strong>Identity-based cryptography (IBC), proposed by Adi Shamir, revolutionized public key authentication by eliminating the need for certificates, enabling a more efficient and scalable approach to cryptographic systems. Meanwhile, in \cite{Katsumata2024group}, Katsumata et al. were the first to present the blind signature protocol based on the hardness assumption of isogeny with provable security, which resembles the Schnorr blind signature. Building upon these foundational concepts, we propose an Identity-Based Blind Signature Scheme with an Honest Zero-Knowledge Verifier utilizing the CSIDH framework. This scheme combines blind signatures for privacy preservation with zero-knowledge proofs to ensure the verifier's honesty without revealing any additional information. Leveraging the quantum-resistant properties of CSIDH, a post-quantum secure scheme based on supersingular isogenies, our scheme offers strong protection against quantum adversaries while maintaining computational efficiency. We analyze the security of the introduced protocol in the standard cryptographic model and demonstrate its effectiveness in safeguarding privacy and verifier honesty. Furthermore, we present a performance evaluation, confirming the practical viability of this quantum-resistant cryptographic solution for privacy-preserving applications. This work advances the creation of secure, and scalable cryptographic systems for the post-quantum era.</li>
</ul>

<h3>Title: VehiclePassport: A GAIA-X-Aligned, Blockchain-Anchored Privacy-Preserving, Zero-Knowledge Digital Passport for Smart Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Pradyumna Kaushal</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC, cs.SE, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06133">https://arxiv.org/abs/2509.06133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06133">https://arxiv.org/pdf/2509.06133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06133]] VehiclePassport: A GAIA-X-Aligned, Blockchain-Anchored Privacy-Preserving, Zero-Knowledge Digital Passport for Smart Vehicles(https://arxiv.org/abs/2509.06133)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Modern vehicles accumulate fragmented lifecycle records across OEMs, owners, and service centers that are difficult to verify and prone to fraud. We propose VehiclePassport, a GAIA-X-aligned digital passport anchored on blockchain with zero-knowledge proofs (ZKPs) for privacy-preserving verification. VehiclePassport immutably commits to manufacturing, telemetry, and service events while enabling selective disclosure via short-lived JWTs and Groth16 proofs. Our open-source reference stack anchors hashes on Polygon zkEVM at <$0.02 per event, validates proofs in <10 ms, and scales to millions of vehicles. This architecture eliminates paper-based KYC, ensures GDPR-compliant traceability, and establishes a trustless foundation for insurance, resale, and regulatory applications in global mobility data markets.</li>
</ul>

<h3>Title: Measuring the Vulnerability Disclosure Policies of AI Vendors</h3>
<ul>
<li><strong>Authors: </strong>Yangheran Piao (1), Jingjie Li (1), Daniel W. Woods (1) ((1) University of Edinburgh)</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06136">https://arxiv.org/abs/2509.06136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06136">https://arxiv.org/pdf/2509.06136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06136]] Measuring the Vulnerability Disclosure Policies of AI Vendors(https://arxiv.org/abs/2509.06136)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>As AI is increasingly integrated into products and critical systems, researchers are paying greater attention to identifying related vulnerabilities. Effective remediation depends on whether vendors are willing to accept and respond to AI vulnerability reports. In this paper, we examine the disclosure policies of 264 AI vendors. Using a mixed-methods approach, our quantitative analysis finds that 36% of vendors provide no disclosure channel, and only 18% explicitly mention AI-related risks. Vulnerabilities involving data access, authorization, and model extraction are generally considered in-scope, while jailbreaking and hallucination are frequently excluded. Through qualitative analysis, we further identify three vendor postures toward AI vulnerabilities - proactive clarification (n = 46, include active supporters, AI integrationists, and back channels), silence (n = 115, include self-hosted and hosted vendors), and restrictive (n = 103). Finally, by comparing vendor policies against 1,130 AI incidents and 359 academic publications, we show that bug bounty policy evolution has lagged behind both academic research and real-world events.</li>
</ul>

<h3>Title: RetinaGuard: Obfuscating Retinal Age in Fundus Images for Biometric Privacy Preserving</h3>
<ul>
<li><strong>Authors: </strong>Zhengquan Luo (1), Chi Liu (1), Dongfu Xiao (1), Zhen Yu (2), Yueye Wang (3), Tianqing Zhu (1) ((1) City University of Macau, (2) Monash University, (3) Hong Kong Polytechnic University)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06142">https://arxiv.org/abs/2509.06142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06142">https://arxiv.org/pdf/2509.06142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06142]] RetinaGuard: Obfuscating Retinal Age in Fundus Images for Biometric Privacy Preserving(https://arxiv.org/abs/2509.06142)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, biometric, extraction, generative</a></li>
<li><strong>Abstract: </strong>The integration of AI with medical images enables the extraction of implicit image-derived biomarkers for a precise health assessment. Recently, retinal age, a biomarker predicted from fundus images, is a proven predictor of systemic disease risks, behavioral patterns, aging trajectory and even mortality. However, the capability to infer such sensitive biometric data raises significant privacy risks, where unauthorized use of fundus images could lead to bioinformation leakage, breaching individual privacy. In response, we formulate a new research problem of biometric privacy associated with medical images and propose RetinaGuard, a novel privacy-enhancing framework that employs a feature-level generative adversarial masking mechanism to obscure retinal age while preserving image visual quality and disease diagnostic utility. The framework further utilizes a novel multiple-to-one knowledge distillation strategy incorporating a retinal foundation model and diverse surrogate age encoders to enable a universal defense against black-box age prediction models. Comprehensive evaluations confirm that RetinaGuard successfully obfuscates retinal age prediction with minimal impact on image quality and pathological feature representation. RetinaGuard is also flexible for extension to other medical image derived biomarkers. RetinaGuard is also flexible for extension to other medical image biomarkers.</li>
</ul>

<h3>Title: Benchmarking Gender and Political Bias in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jinrui Yang, Xudong Han, Timothy Baldwin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06164">https://arxiv.org/abs/2509.06164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06164">https://arxiv.org/pdf/2509.06164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06164]] Benchmarking Gender and Political Bias in Large Language Models(https://arxiv.org/abs/2509.06164)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>We introduce EuroParlVote, a novel benchmark for evaluating large language models (LLMs) in politically sensitive contexts. It links European Parliament debate speeches to roll-call vote outcomes and includes rich demographic metadata for each Member of the European Parliament (MEP), such as gender, age, country, and political group. Using EuroParlVote, we evaluate state-of-the-art LLMs on two tasks -- gender classification and vote prediction -- revealing consistent patterns of bias. We find that LLMs frequently misclassify female MEPs as male and demonstrate reduced accuracy when simulating votes for female speakers. Politically, LLMs tend to favor centrist groups while underperforming on both far-left and far-right ones. Proprietary models like GPT-4o outperform open-weight alternatives in terms of both robustness and fairness. We release the EuroParlVote dataset, code, and demo to support future research on fairness and accountability in NLP within political contexts.</li>
</ul>

<h3>Title: UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Huy Le, Nhat Chung, Tung Kieu, Jingkang Yang, Ngan Le</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06165">https://arxiv.org/abs/2509.06165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06165">https://arxiv.org/pdf/2509.06165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06165]] UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning(https://arxiv.org/abs/2509.06165)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Video Scene Graph Generation (VidSGG) aims to represent dynamic visual content by detecting objects and modeling their temporal interactions as structured graphs. Prior studies typically target either coarse-grained box-level or fine-grained panoptic pixel-level VidSGG, often requiring task-specific architectures and multi-stage training pipelines. In this paper, we present UNO (UNified Object-centric VidSGG), a single-stage, unified framework that jointly addresses both tasks within an end-to-end architecture. UNO is designed to minimize task-specific modifications and maximize parameter sharing, enabling generalization across different levels of visual granularity. The core of UNO is an extended slot attention mechanism that decomposes visual features into object and relation slots. To ensure robust temporal modeling, we introduce object temporal consistency learning, which enforces consistent object representations across frames without relying on explicit tracking modules. Additionally, a dynamic triplet prediction module links relation slots to corresponding object pairs, capturing evolving interactions over time. We evaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results demonstrate that UNO not only achieves competitive performance across both tasks but also offers improved efficiency through a unified, object-centric design.</li>
</ul>

<h3>Title: Understanding the Influence of Synthetic Data for Text Embedders</h3>
<ul>
<li><strong>Authors: </strong>Jacob Mitchell Springer, Vaibhav Adlakha, Siva Reddy, Aditi Raghunathan, Marius Mosbach</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06184">https://arxiv.org/abs/2509.06184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06184">https://arxiv.org/pdf/2509.06184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06184]] Understanding the Influence of Synthetic Data for Text Embedders(https://arxiv.org/abs/2509.06184)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent progress in developing general purpose text embedders has been driven by training on ever-growing corpora of synthetic LLM-generated data. Nonetheless, no publicly available synthetic dataset exists, posing a barrier to studying its role for generalization. To address this issue, we first reproduce and publicly release the synthetic data proposed by Wang et al. (Mistral-E5). Our synthetic data is high quality and leads to consistent improvements in performance. Next, we critically examine where exactly synthetic data improves model generalization. Our analysis reveals that benefits from synthetic data are sparse and highly localized to individual datasets. Moreover, we observe trade-offs between the performance on different categories and data that benefits one task, degrades performance on another. Our findings highlight the limitations of current synthetic data approaches for building general-purpose embedders and challenge the notion that training on synthetic data leads to more robust embedding models across tasks.</li>
</ul>

<h3>Title: Augmented Fine-Tuned LLMs for Enhanced Recruitment Automation</h3>
<ul>
<li><strong>Authors: </strong>Mohamed T. Younes, Omar Walid, Khaled Shaban, Ali Hamdi, Mai Hassan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06196">https://arxiv.org/abs/2509.06196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06196">https://arxiv.org/pdf/2509.06196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06196]] Augmented Fine-Tuned LLMs for Enhanced Recruitment Automation(https://arxiv.org/abs/2509.06196)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a novel approach to recruitment automation. Large Language Models (LLMs) were fine-tuned to improve accuracy and efficiency. Building upon our previous work on the Multilayer Large Language Model-Based Robotic Process Automation Applicant Tracking (MLAR) system . This work introduces a novel methodology. Training fine-tuned LLMs specifically tuned for recruitment tasks. The proposed framework addresses the limitations of generic LLMs by creating a synthetic dataset that uses a standardized JSON format. This helps ensure consistency and scalability. In addition to the synthetic data set, the resumes were parsed using DeepSeek, a high-parameter LLM. The resumes were parsed into the same structured JSON format and placed in the training set. This will help improve data diversity and realism. Through experimentation, we demonstrate significant improvements in performance metrics, such as exact match, F1 score, BLEU score, ROUGE score, and overall similarity compared to base models and other state-of-the-art LLMs. In particular, the fine-tuned Phi-4 model achieved the highest F1 score of 90.62%, indicating exceptional precision and recall in recruitment tasks. This study highlights the potential of fine-tuned LLMs. Furthermore, it will revolutionize recruitment workflows by providing more accurate candidate-job matching.</li>
</ul>

<h3>Title: MSLEF: Multi-Segment LLM Ensemble Finetuning in Recruitment</h3>
<ul>
<li><strong>Authors: </strong>Omar Walid, Mohamed T. Younes, Khaled Shaban, Mai Hassan, Ali Hamdi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06200">https://arxiv.org/abs/2509.06200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06200">https://arxiv.org/pdf/2509.06200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06200]] MSLEF: Multi-Segment LLM Ensemble Finetuning in Recruitment(https://arxiv.org/abs/2509.06200)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents MSLEF, a multi-segment ensemble framework that employs LLM fine-tuning to enhance resume parsing in recruitment automation. It integrates fine-tuned Large Language Models (LLMs) using weighted voting, with each model specializing in a specific resume segment to boost accuracy. Building on MLAR , MSLEF introduces a segment-aware architecture that leverages field-specific weighting tailored to each resume part, effectively overcoming the limitations of single-model systems by adapting to diverse formats and structures. The framework incorporates Gemini-2.5-Flash LLM as a high-level aggregator for complex sections and utilizes Gemma 9B, LLaMA 3.1 8B, and Phi-4 14B. MSLEF achieves significant improvements in Exact Match (EM), F1 score, BLEU, ROUGE, and Recruitment Similarity (RS) metrics, outperforming the best single model by up to +7% in RS. Its segment-aware design enhances generalization across varied resume layouts, making it highly adaptable to real-world hiring scenarios while ensuring precise and reliable candidate representation.</li>
</ul>

<h3>Title: Lightweight Intrusion Detection System Using a Hybrid CNN and ConvNeXt-Tiny Model for Internet of Things Networks</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Roshanzadeh, Hamid Barati, Ali Barati</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06202">https://arxiv.org/abs/2509.06202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06202">https://arxiv.org/pdf/2509.06202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06202]] Lightweight Intrusion Detection System Using a Hybrid CNN and ConvNeXt-Tiny Model for Internet of Things Networks(https://arxiv.org/abs/2509.06202)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>The rapid expansion of Internet of Things (IoT) systems across various domains such as industry, smart cities, healthcare, manufacturing, and government services has led to a significant increase in security risks, threatening data integrity, confidentiality, and availability. Consequently, ensuring the security and resilience of IoT systems has become a critical requirement. In this paper, we propose a lightweight and efficient intrusion detection system (IDS) for IoT environments, leveraging a hybrid model of CNN and ConvNeXt-Tiny. The proposed method is designed to detect and classify different types of network attacks, particularly botnet and malicious traffic, while the lightweight ConvNeXt-Tiny architecture enables effective deployment in resource-constrained devices and networks. A real-world dataset comprising both benign and malicious network packets collected from practical IoT scenarios was employed in the experiments. The results demonstrate that the proposed method achieves high accuracy while significantly reducing training and inference time compared to more complex models. Specifically, the system attained 99.63% accuracy in the testing phase, 99.67% accuracy in the training phase, and an error rate of 0.0107 across eight classes, while maintaining short response times and low resource consumption. These findings highlight the effectiveness of the proposed method in detecting and classifying attacks in real-world IoT environments, indicating that the lightweight architecture can serve as a practical alternative to complex and resource-intensive approaches in IoT network security.</li>
</ul>

<h3>Title: Toward a Metrology for Artificial Intelligence: Hidden-Rule Environments and Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Christo Mathew, Wentian Wang, Lazaros Gallos, Paul Kantor, Vladimir Menkov, Hao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06213">https://arxiv.org/abs/2509.06213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06213">https://arxiv.org/pdf/2509.06213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06213]] Toward a Metrology for Artificial Intelligence: Hidden-Rule Environments and Reinforcement Learning(https://arxiv.org/abs/2509.06213)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We investigate reinforcement learning in the Game Of Hidden Rules (GOHR) environment, a complex puzzle in which an agent must infer and execute hidden rules to clear a 6$\times$6 board by placing game pieces into buckets. We explore two state representation strategies, namely Feature-Centric (FC) and Object-Centric (OC), and employ a Transformer-based Advantage Actor-Critic (A2C) algorithm for training. The agent has access only to partial observations and must simultaneously infer the governing rule and learn the optimal policy through experience. We evaluate our models across multiple rule-based and trial-list-based experimental setups, analyzing transfer effects and the impact of representation on learning efficiency.</li>
</ul>

<h3>Title: Metric Embedding Initialization-Based Differentially Private and Explainable Graph Clustering</h3>
<ul>
<li><strong>Authors: </strong>Haochen You, Baojing Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06214">https://arxiv.org/abs/2509.06214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06214">https://arxiv.org/pdf/2509.06214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06214]] Metric Embedding Initialization-Based Differentially Private and Explainable Graph Clustering(https://arxiv.org/abs/2509.06214)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, interpretability</a></li>
<li><strong>Abstract: </strong>Graph clustering under the framework of differential privacy, which aims to process graph-structured data while protecting individual privacy, has been receiving increasing attention. Despite significant achievements in current research, challenges such as high noise, low efficiency and poor interpretability continue to severely constrain the development of this field. In this paper, we construct a differentially private and interpretable graph clustering approach based on metric embedding initialization. Specifically, we construct an SDP optimization, extract the key set and provide a well-initialized clustering configuration using an HST-based initialization method. Subsequently, we apply an established k-median clustering strategy to derive the cluster results and offer comparative explanations for the query set through differences from the cluster centers. Extensive experiments on public datasets demonstrate that our proposed framework outperforms existing methods in various clustering metrics while strictly ensuring privacy.</li>
</ul>

<h3>Title: AI-Based Applied Innovation for Fracture Detection in X-rays Using Custom CNN and Transfer Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Amna Hassan, Ilsa Afzaal, Nouman Muneeb, Aneeqa Batool, Hamail Noor</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06228">https://arxiv.org/abs/2509.06228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06228">https://arxiv.org/pdf/2509.06228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06228]] AI-Based Applied Innovation for Fracture Detection in X-rays Using Custom CNN and Transfer Learning Models(https://arxiv.org/abs/2509.06228)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Bone fractures present a major global health challenge, often resulting in pain, reduced mobility, and productivity loss, particularly in low-resource settings where access to expert radiology services is limited. Conventional imaging methods suffer from high costs, radiation exposure, and dependency on specialized interpretation. To address this, we developed an AI-based solution for automated fracture detection from X-ray images using a custom Convolutional Neural Network (CNN) and benchmarked it against transfer learning models including EfficientNetB0, MobileNetV2, and ResNet50. Training was conducted on the publicly available FracAtlas dataset, comprising 4,083 anonymized musculoskeletal radiographs. The custom CNN achieved 95.96% accuracy, 0.94 precision, 0.88 recall, and an F1-score of 0.91 on the FracAtlas dataset. Although transfer learning models (EfficientNetB0, MobileNetV2, ResNet50) performed poorly in this specific setup, these results should be interpreted in light of class imbalance and data set limitations. This work highlights the promise of lightweight CNNs for detecting fractures in X-rays and underscores the importance of fair benchmarking, diverse datasets, and external validation for clinical translation</li>
</ul>

<h3>Title: PLRV-O: Advancing Differentially Private Deep Learning via Privacy Loss Random Variable Optimization</h3>
<ul>
<li><strong>Authors: </strong>Qin Yang, Nicholas Stout, Meisam Mohammady, Han Wang, Ayesha Samreen, Christopher J Quinn, Yan Yan, Ashish Kundu, Yuan Hong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06264">https://arxiv.org/abs/2509.06264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06264">https://arxiv.org/pdf/2509.06264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06264]] PLRV-O: Advancing Differentially Private Deep Learning via Privacy Loss Random Variable Optimization(https://arxiv.org/abs/2509.06264)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Differentially Private Stochastic Gradient Descent (DP-SGD) is a standard method for enforcing privacy in deep learning, typically using the Gaussian mechanism to perturb gradient updates. However, conventional mechanisms such as Gaussian and Laplacian noise are parameterized only by variance or scale. This single degree of freedom ties the magnitude of noise directly to both privacy loss and utility degradation, preventing independent control of these two factors. The problem becomes more pronounced when the number of composition rounds T and batch size B vary across tasks, as these variations induce task-dependent shifts in the privacy-utility trade-off, where small changes in noise parameters can disproportionately affect model accuracy. To address this limitation, we introduce PLRV-O, a framework that defines a broad search space of parameterized DP-SGD noise distributions, where privacy loss moments are tightly characterized yet can be optimized more independently with respect to utility loss. This formulation enables systematic adaptation of noise to task-specific requirements, including (i) model size, (ii) training duration, (iii) batch sampling strategies, and (iv) clipping thresholds under both training and fine-tuning settings. Empirical results demonstrate that PLRV-O substantially improves utility under strict privacy constraints. On CIFAR-10, a fine-tuned ViT achieves 94.03% accuracy at epsilon approximately 0.5, compared to 83.93% with Gaussian noise. On SST-2, RoBERTa-large reaches 92.20% accuracy at epsilon approximately 0.2, versus 50.25% with Gaussian.</li>
</ul>

<h3>Title: No Encore: Unlearning as Opt-Out in Music Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinju Kim, Taehan Kim, Abdul Waheed, Rita Singh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06277">https://arxiv.org/abs/2509.06277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06277">https://arxiv.org/pdf/2509.06277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06277]] No Encore: Unlearning as Opt-Out in Music Generation(https://arxiv.org/abs/2509.06277)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>AI music generation is rapidly emerging in the creative industries, enabling intuitive music generation from textual descriptions. However, these systems pose risks in exploitation of copyrighted creations, raising ethical and legal concerns. In this paper, we present preliminary results on the first application of machine unlearning techniques from an ongoing research to prevent inadvertent usage of creative content. Particularly, we explore existing methods in machine unlearning to a pre-trained Text-to-Music (TTM) baseline and analyze their efficacy in unlearning pre-trained datasets without harming model performance. Through our experiments, we provide insights into the challenges of applying unlearning in music generation, offering a foundational analysis for future works on the application of unlearning for music generative models.</li>
</ul>

<h3>Title: AI-driven Remote Facial Skin Hydration and TEWL Assessment from Selfie Images: A Systematic Solution</h3>
<ul>
<li><strong>Authors: </strong>Cecelia Soh, Rizhao Cai, Monalisha Paul, Dennis Sng, Alex Kot</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06282">https://arxiv.org/abs/2509.06282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06282">https://arxiv.org/pdf/2509.06282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06282]] AI-driven Remote Facial Skin Hydration and TEWL Assessment from Selfie Images: A Systematic Solution(https://arxiv.org/abs/2509.06282)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, transformer</a></li>
<li><strong>Abstract: </strong>Skin health and disease resistance are closely linked to the skin barrier function, which protects against environmental factors and water loss. Two key physiological indicators can quantitatively represent this barrier function: skin hydration (SH) and trans-epidermal water loss (TEWL). Measurement of SH and TEWL is valuable for the public to monitor skin conditions regularly, diagnose dermatological issues, and personalize their skincare regimens. However, these measurements are not easily accessible to general users unless they visit a dermatology clinic with specialized instruments. To tackle this problem, we propose a systematic solution to estimate SH and TEWL from selfie facial images remotely with smartphones. Our solution encompasses multiple stages, including SH/TEWL data collection, data preprocessing, and formulating a novel Skin-Prior Adaptive Vision Transformer model for SH/TEWL regression. Through experiments, we identified the annotation imbalance of the SH/TEWL data and proposed a symmetric-based contrastive regularization to reduce the model bias due to the imbalance effectively. This work is the first study to explore skin assessment from selfie facial images without physical measurements. It bridges the gap between computer vision and skin care research, enabling AI-driven accessible skin analysis for broader real-world applications.</li>
</ul>

<h3>Title: Prototype-Aware Multimodal Alignment for Open-Vocabulary Visual Grounding</h3>
<ul>
<li><strong>Authors: </strong>Jiangnan Xie, Xiaolong Zheng, Liang Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06291">https://arxiv.org/abs/2509.06291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06291">https://arxiv.org/pdf/2509.06291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06291]] Prototype-Aware Multimodal Alignment for Open-Vocabulary Visual Grounding(https://arxiv.org/abs/2509.06291)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Visual Grounding (VG) aims to utilize given natural language queries to locate specific target objects within images. While current transformer-based approaches demonstrate strong localization performance in standard scene (i.e, scenarios without any novel objects), they exhibit notable limitations in open-vocabulary scene (i.e, both familiar and novel object categories during testing). These limitations primarily stem from three key factors: (1) imperfect alignment between visual and linguistic modalities, (2) insufficient cross-modal feature fusion, and (3) ineffective utilization of semantic prototype information. To overcome these challenges, we present Prototype-Aware Multimodal Learning (PAML), an innovative framework that systematically addresses these issues through several key components: First, we leverage ALBEF to establish robust cross-modal alignment during initial feature encoding. Subsequently, our Visual Discriminative Feature Encoder selectively enhances salient object representations while suppressing irrelevant visual context. The framework then incorporates a novel prototype discovering and inheriting mechanism that extracts and aggregates multi-neighbor semantic prototypes to facilitate open-vocabulary recognition. These enriched features undergo comprehensive multimodal integration through our Multi-stage Decoder before final bounding box regression. Extensive experiments across five benchmark datasets validate our approach, showing competitive performance in standard scene while achieving state-of-the-art results in open-vocabulary scene. Our code is available at this https URL.</li>
</ul>

<h3>Title: WindFM: An Open-Source Foundation Model for Zero-Shot Wind Power Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Hang Fan, Yu Shi, Zongliang Fu, Shuo Chen, Wei Wei, Wei Xu, Jian Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06311">https://arxiv.org/abs/2509.06311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06311">https://arxiv.org/pdf/2509.06311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06311]] WindFM: An Open-Source Foundation Model for Zero-Shot Wind Power Forecasting(https://arxiv.org/abs/2509.06311)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>High-quality wind power forecasting is crucial for the operation of modern power grids. However, prevailing data-driven paradigms either train a site-specific model which cannot generalize to other locations or rely on fine-tuning of general-purpose time series foundation models which are difficult to incorporate domain-specific data in the energy sector. This paper introduces WindFM, a lightweight and generative Foundation Model designed specifically for probabilistic wind power forecasting. WindFM employs a discretize-and-generate framework. A specialized time-series tokenizer first converts continuous multivariate observations into discrete, hierarchical tokens. Subsequently, a decoder-only Transformer learns a universal representation of wind generation dynamics by autoregressively pre-training on these token sequences. Using the comprehensive WIND Toolkit dataset comprising approximately 150 billion time steps from more than 126,000 sites, WindFM develops a foundational understanding of the complex interplay between atmospheric conditions and power output. Extensive experiments demonstrate that our compact 8.1M parameter model achieves state-of-the-art zero-shot performance on both deterministic and probabilistic tasks, outperforming specialized models and larger foundation models without any fine-tuning. In particular, WindFM exhibits strong adaptiveness under out-of-distribution data from a different continent, demonstrating the robustness and transferability of its learned representations. Our pre-trained model is publicly available at this https URL.</li>
</ul>

<h3>Title: Evaluating the Efficiency of Latent Spaces via the Coupling-Matrix</h3>
<ul>
<li><strong>Authors: </strong>Mehmet Can Yavuz, Berrin Yanikoglu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06314">https://arxiv.org/abs/2509.06314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06314">https://arxiv.org/pdf/2509.06314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06314]] Evaluating the Efficiency of Latent Spaces via the Coupling-Matrix(https://arxiv.org/abs/2509.06314)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A central challenge in representation learning is constructing latent embeddings that are both expressive and efficient. In practice, deep networks often produce redundant latent spaces where multiple coordinates encode overlapping information, reducing effective capacity and hindering generalization. Standard metrics such as accuracy or reconstruction loss provide only indirect evidence of such redundancy and cannot isolate it as a failure mode. We introduce a redundancy index, denoted rho(C), that directly quantifies inter-dimensional dependencies by analyzing coupling matrices derived from latent representations and comparing their off-diagonal statistics against a normal distribution via energy distance. The result is a compact, interpretable, and statistically grounded measure of representational quality. We validate rho(C) across discriminative and generative settings on MNIST variants, Fashion-MNIST, CIFAR-10, and CIFAR-100, spanning multiple architectures and hyperparameter optimization strategies. Empirically, low rho(C) reliably predicts high classification accuracy or low reconstruction error, while elevated redundancy is associated with performance collapse. Estimator reliability grows with latent dimension, yielding natural lower bounds for reliable analysis. We further show that Tree-structured Parzen Estimators (TPE) preferentially explore low-rho regions, suggesting that rho(C) can guide neural architecture search and serve as a redundancy-aware regularization target. By exposing redundancy as a universal bottleneck across models and tasks, rho(C) offers both a theoretical lens and a practical tool for evaluating and improving the efficiency of learned representations.</li>
</ul>

<h3>Title: Text4Seg++: Advancing Image Segmentation via Generative Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Mengcheng Lan, Chaofeng Chen, Jiaxing Xu, Zongrui Li, Yiping Ke, Xudong Jiang, Yingchen Yu, Yunqing Zhao, Song Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06321">https://arxiv.org/abs/2509.06321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06321">https://arxiv.org/pdf/2509.06321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06321]] Text4Seg++: Advancing Image Segmentation via Generative Language Modeling(https://arxiv.org/abs/2509.06321)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have shown exceptional capabilities in vision-language tasks. However, effectively integrating image segmentation into these models remains a significant challenge. In this work, we propose a novel text-as-mask paradigm that casts image segmentation as a text generation problem, eliminating the need for additional decoders and significantly simplifying the segmentation process. Our key innovation is semantic descriptors, a new textual representation of segmentation masks where each image patch is mapped to its corresponding text label. We first introduce image-wise semantic descriptors, a patch-aligned textual representation of segmentation masks that integrates naturally into the language modeling pipeline. To enhance efficiency, we introduce the Row-wise Run-Length Encoding (R-RLE), which compresses redundant text sequences, reducing the length of semantic descriptors by 74% and accelerating inference by $3\times$, without compromising performance. Building upon this, our initial framework Text4Seg achieves strong segmentation performance across a wide range of vision tasks. To further improve granularity and compactness, we propose box-wise semantic descriptors, which localizes regions of interest using bounding boxes and represents region masks via structured mask tokens called semantic bricks. This leads to our refined model, Text4Seg++, which formulates segmentation as a next-brick prediction task, combining precision, scalability, and generative efficiency. Comprehensive experiments on natural and remote sensing datasets show that Text4Seg++ consistently outperforms state-of-the-art models across diverse benchmarks without any task-specific fine-tuning, while remaining compatible with existing MLLM backbones. Our work highlights the effectiveness, scalability, and generalizability of text-driven image segmentation within the MLLM framework.</li>
</ul>

<h3>Title: Text-Trained LLMs Can Zero-Shot Extrapolate PDE Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Bao, Nicolas Boullé, Toni J.B. Liu, Raphaël Sarfati, Christopher J. Earls</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06322">https://arxiv.org/abs/2509.06322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06322">https://arxiv.org/pdf/2509.06322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06322]] Text-Trained LLMs Can Zero-Shot Extrapolate PDE Dynamics(https://arxiv.org/abs/2509.06322)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated emergent in-context learning (ICL) capabilities across a range of tasks, including zero-shot time-series forecasting. We show that text-trained foundation models can accurately extrapolate spatiotemporal dynamics from discretized partial differential equation (PDE) solutions without fine-tuning or natural language prompting. Predictive accuracy improves with longer temporal contexts but degrades at finer spatial discretizations. In multi-step rollouts, where the model recursively predicts future spatial states over multiple time steps, errors grow algebraically with the time horizon, reminiscent of global error accumulation in classical finite-difference solvers. We interpret these trends as in-context neural scaling laws, where prediction quality varies predictably with both context length and output length. To better understand how LLMs are able to internally process PDE solutions so as to accurately roll them out, we analyze token-level output distributions and uncover a consistent ICL progression: beginning with syntactic pattern imitation, transitioning through an exploratory high-entropy phase, and culminating in confident, numerically grounded predictions.</li>
</ul>

<h3>Title: AttestLLM: Efficient Attestation Framework for Billion-scale On-device LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ruisi Zhang, Yifei Zhao, Neusha Javidnia, Mengxin Zheng, Farinaz Koushanfar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06326">https://arxiv.org/abs/2509.06326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06326">https://arxiv.org/pdf/2509.06326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06326]] AttestLLM: Efficient Attestation Framework for Billion-scale On-device LLMs(https://arxiv.org/abs/2509.06326)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>As on-device LLMs(e.g., Apple on-device Intelligence) are widely adopted to reduce network dependency, improve privacy, and enhance responsiveness, verifying the legitimacy of models running on local devices becomes critical. Existing attestation techniques are not suitable for billion-parameter Large Language Models (LLMs), struggling to remain both time- and memory-efficient while addressing emerging threats in the LLM era. In this paper, we present AttestLLM, the first-of-its-kind attestation framework to protect the hardware-level intellectual property (IP) of device vendors by ensuring that only authorized LLMs can execute on target platforms. AttestLLM leverages an algorithm/software/hardware co-design approach to embed robust watermarking signatures onto the activation distributions of LLM building blocks. It also optimizes the attestation protocol within the Trusted Execution Environment (TEE), providing efficient verification without compromising inference throughput. Extensive proof-of-concept evaluations on LLMs from Llama, Qwen, and Phi families for on-device use cases demonstrate AttestLLM's attestation reliability, fidelity, and efficiency. Furthermore, AttestLLM enforces model legitimacy and exhibits resilience against model replacement and forgery attacks.</li>
</ul>

<h3>Title: Towards scalable organ level 3D plant segmentation: Bridging the data algorithm computing gap</h3>
<ul>
<li><strong>Authors: </strong>Ruiming Du, Guangxun Zhai, Tian Qiu, Yu Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06329">https://arxiv.org/abs/2509.06329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06329">https://arxiv.org/pdf/2509.06329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06329]] Towards scalable organ level 3D plant segmentation: Bridging the data algorithm computing gap(https://arxiv.org/abs/2509.06329)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The precise characterization of plant morphology provides valuable insights into plant environment interactions and genetic evolution. A key technology for extracting this information is 3D segmentation, which delineates individual plant organs from complex point clouds. Despite significant progress in general 3D computer vision domains, the adoption of 3D segmentation for plant phenotyping remains limited by three major challenges: i) the scarcity of large-scale annotated datasets, ii) technical difficulties in adapting advanced deep neural networks to plant point clouds, and iii) the lack of standardized benchmarks and evaluation protocols tailored to plant science. This review systematically addresses these barriers by: i) providing an overview of existing 3D plant datasets in the context of general 3D segmentation domains, ii) systematically summarizing deep learning-based methods for point cloud semantic and instance segmentation, iii) introducing Plant Segmentation Studio (PSS), an open-source framework for reproducible benchmarking, and iv) conducting extensive quantitative experiments to evaluate representative networks and sim-to-real learning strategies. Our findings highlight the efficacy of sparse convolutional backbones and transformer-based instance segmentation, while also emphasizing the complementary role of modeling-based and augmentation-based synthetic data generation for sim-to-real learning in reducing annotation demands. In general, this study bridges the gap between algorithmic advances and practical deployment, providing immediate tools for researchers and a roadmap for developing data-efficient and generalizable deep learning solutions in 3D plant phenotyping. Data and code are available at this https URL.</li>
</ul>

<h3>Title: A Fragile Number Sense: Probing the Elemental Limits of Numerical Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Roussel Rahman, Aashwin Ananda Mishra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06332">https://arxiv.org/abs/2509.06332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06332">https://arxiv.org/pdf/2509.06332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06332]] A Fragile Number Sense: Probing the Elemental Limits of Numerical Reasoning in LLMs(https://arxiv.org/abs/2509.06332)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable emergent capabilities, yet the robustness of their numerical reasoning remains an open question. While standard benchmarks evaluate LLM reasoning on complex problem sets using aggregated metrics, they often obscure foundational weaknesses. In this work, we probe LLM mathematical numeracy by evaluating performance on problems of escalating complexity, from constituent operations to combinatorial puzzles. We test several state-of-the-art LLM-based agents on a 100-problem challenge comprising four categories: (1) basic arithmetic, (2) advanced operations, (3) primality checking, and (4) the Game of 24 number puzzle. Our results show that while the agents achieved high accuracy on the first three categories, which require deterministic algorithmic execution, they consistently failed at the number puzzle, underlining its demand for a heuristic search over a large combinatorial space to be a significant bottleneck. These findings reveal that the agents' proficiency is largely confined to recalling and executing known algorithms, rather than performing generative problem-solving. This suggests their apparent numerical reasoning is more akin to sophisticated pattern-matching than flexible, analytical thought, limiting their potential for tasks that require novel or creative numerical insights.</li>
</ul>

<h3>Title: Multi-Modal Camera-Based Detection of Vulnerable Road Users</h3>
<ul>
<li><strong>Authors: </strong>Penelope Brown, Julie Stephany Berrio Perez, Mao Shan, Stewart Worrall</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06333">https://arxiv.org/abs/2509.06333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06333">https://arxiv.org/pdf/2509.06333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06333]] Multi-Modal Camera-Based Detection of Vulnerable Road Users(https://arxiv.org/abs/2509.06333)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vulnerable road users (VRUs) such as pedestrians, cyclists, and motorcyclists represent more than half of global traffic deaths, yet their detection remains challenging in poor lighting, adverse weather, and unbalanced data sets. This paper presents a multimodal detection framework that integrates RGB and thermal infrared imaging with a fine-tuned YOLOv8 model. Training leveraged KITTI, BDD100K, and Teledyne FLIR datasets, with class re-weighting and light augmentations to improve minority-class performance and robustness, experiments show that 640-pixel resolution and partial backbone freezing optimise accuracy and efficiency, while class-weighted losses enhance recall for rare VRUs. Results highlight that thermal models achieve the highest precision, and RGB-to-thermal augmentation boosts recall, demonstrating the potential of multimodal detection to improve VRU safety at intersections.</li>
</ul>

<h3>Title: Harnessing Object Grounding for Time-Sensitive Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Tz-Ying Wu, Sharath Nittur Sridhar, Subarna Tripathi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06335">https://arxiv.org/abs/2509.06335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06335">https://arxiv.org/pdf/2509.06335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06335]] Harnessing Object Grounding for Time-Sensitive Video Understanding(https://arxiv.org/abs/2509.06335)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose to improve the time-sensitive video understanding (TSV) capability of video large language models (Video-LLMs) with grounded objects (GO). We hypothesize that TSV tasks can benefit from GO within frames, which is supported by our preliminary experiments on LITA, a state-of-the-art Video-LLM for reasoning temporal localization. While augmenting prompts with textual description of these object annotations improves the performance of LITA, it also introduces extra token length and susceptibility to the noise in object level information. To address this, we propose GO-Tokenizer, a lightweight add-on module for Video-LLMs leveraging off-the-shelf object detectors to encode compact object information on the fly. Experimental results demonstrate that pretraining with GO-Tokenizer outperforms the vanilla Video-LLM and its counterpart utilizing textual description of objects in the prompt. The gain generalizes across different models, datasets and video understanding tasks such as reasoning temporal localization and dense captioning.</li>
</ul>

<h3>Title: Multi View Slot Attention Using Paraphrased Texts For Face Anti-Spoofing</h3>
<ul>
<li><strong>Authors: </strong>Jeongmin Yu, Susang Kim, Kisu Lee, Taekyoung Kwon, Won-Yong Shin, Ha Young Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06336">https://arxiv.org/abs/2509.06336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06336">https://arxiv.org/pdf/2509.06336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06336]] Multi View Slot Attention Using Paraphrased Texts For Face Anti-Spoofing(https://arxiv.org/abs/2509.06336)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent face anti-spoofing (FAS) methods have shown remarkable cross-domain performance by employing vision-language models like CLIP. However, existing CLIP-based FAS models do not fully exploit CLIP's patch embedding tokens, failing to detect critical spoofing clues. Moreover, these models rely on a single text prompt per class (e.g., 'live' or 'fake'), which limits generalization. To address these issues, we propose MVP-FAS, a novel framework incorporating two key modules: Multi-View Slot attention (MVS) and Multi-Text Patch Alignment (MTPA). Both modules utilize multiple paraphrased texts to generate generalized features and reduce dependence on domain-specific text. MVS extracts local detailed spatial features and global context from patch embeddings by leveraging diverse texts with multiple perspectives. MTPA aligns patches with multiple text representations to improve semantic robustness. Extensive experiments demonstrate that MVP-FAS achieves superior generalization performance, outperforming previous state-of-the-art methods on cross-domain datasets. Code: this https URL.</li>
</ul>

<h3>Title: Embedding Poisoning: Bypassing Safety Alignment via Embedding Semantic Shift</h3>
<ul>
<li><strong>Authors: </strong>Shuai Yuan, Zhibo Zhang, Yuxi Li, Guangdong Bai, Wang Kailong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06338">https://arxiv.org/abs/2509.06338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06338">https://arxiv.org/pdf/2509.06338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06338]] Embedding Poisoning: Bypassing Safety Alignment via Embedding Semantic Shift(https://arxiv.org/abs/2509.06338)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>The widespread distribution of Large Language Models (LLMs) through public platforms like Hugging Face introduces significant security challenges. While these platforms perform basic security scans, they often fail to detect subtle manipulations within the embedding layer. This work identifies a novel class of deployment phase attacks that exploit this vulnerability by injecting imperceptible perturbations directly into the embedding layer outputs without modifying model weights or input text. These perturbations, though statistically benign, systematically bypass safety alignment mechanisms and induce harmful behaviors during inference. We propose Search based Embedding Poisoning(SEP), a practical, model agnostic framework that introduces carefully optimized perturbations into embeddings associated with high risk tokens. SEP leverages a predictable linear transition in model responses, from refusal to harmful output to semantic deviation to identify a narrow perturbation window that evades alignment safeguards. Evaluated across six aligned LLMs, SEP achieves an average attack success rate of 96.43% while preserving benign task performance and evading conventional detection mechanisms. Our findings reveal a critical oversight in deployment security and emphasize the urgent need for embedding level integrity checks in future LLM defense strategies.</li>
</ul>

<h3>Title: Ban&Pick: Achieving Free Performance Gains and Inference Speedup via Smarter Routing in MoE-LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yuanteng Chen, Peisong Wang, Yuantian Shao, Jian Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06346">https://arxiv.org/abs/2509.06346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06346">https://arxiv.org/pdf/2509.06346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06346]] Ban&Pick: Achieving Free Performance Gains and Inference Speedup via Smarter Routing in MoE-LLMs(https://arxiv.org/abs/2509.06346)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Sparse Mixture-of-Experts (MoE) has become a key architecture for scaling large language models (LLMs) efficiently. Recent fine-grained MoE designs introduce hundreds of experts per layer, with multiple experts activated per token, enabling stronger specialization. However, during pre-training, routers are optimized mainly for stability and robustness: they converge prematurely and enforce balanced usage, limiting the full potential of model performance and efficiency. In this work, we uncover two overlooked issues: (i) a few highly influential experts are underutilized due to premature and balanced routing decisions; and (ii) enforcing a fixed number of active experts per token introduces substantial redundancy. Instead of retraining models or redesigning MoE architectures, we introduce Ban&Pick, a post-training, plug-and-play strategy for smarter MoE routing. Pick discovers and reinforces key experts-a small group with outsized impact on performance-leading to notable accuracy gains across domains. Ban complements this by dynamically pruning redundant experts based on layer and token sensitivity, delivering faster inference with minimal accuracy loss. Experiments on fine-grained MoE-LLMs (DeepSeek, Qwen3) across math, code, and general reasoning benchmarks demonstrate that Ban&Pick delivers free performance gains and inference acceleration without retraining or architectural changes. For instance, on Qwen3-30B-A3B, it improves accuracy from 80.67 to 84.66 on AIME2024 and from 65.66 to 68.18 on GPQA-Diamond, while accelerating inference by 1.25x under the vLLM.</li>
</ul>

<h3>Title: Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?</h3>
<ul>
<li><strong>Authors: </strong>Junjie Mu, Zonghao Ying, Zhekui Fan, Zonglei Jing, Yaoyuan Zhang, Zhengmin Yu, Wenxin Zhang, Quanchen Zou, Xiangzheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06350">https://arxiv.org/abs/2509.06350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06350">https://arxiv.org/pdf/2509.06350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06350]] Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?(https://arxiv.org/abs/2509.06350)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Jailbreak attacks on Large Language Models (LLMs) have demonstrated various successful methods whereby attackers manipulate models into generating harmful responses that they are designed to avoid. Among these, Greedy Coordinate Gradient (GCG) has emerged as a general and effective approach that optimizes the tokens in a suffix to generate jailbreakable prompts. While several improved variants of GCG have been proposed, they all rely on fixed-length suffixes. However, the potential redundancy within these suffixes remains unexplored. In this work, we propose Mask-GCG, a plug-and-play method that employs learnable token masking to identify impactful tokens within the suffix. Our approach increases the update probability for tokens at high-impact positions while pruning those at low-impact positions. This pruning not only reduces redundancy but also decreases the size of the gradient space, thereby lowering computational overhead and shortening the time required to achieve successful attacks compared to GCG. We evaluate Mask-GCG by applying it to the original GCG and several improved variants. Experimental results show that most tokens in the suffix contribute significantly to attack success, and pruning a minority of low-impact tokens does not affect the loss values or compromise the attack success rate (ASR), thereby revealing token redundancy in LLM prompts. Our findings provide insights for developing efficient and interpretable LLMs from the perspective of jailbreak attacks.</li>
</ul>

<h3>Title: A Multi-Modal Deep Learning Framework for Colorectal Pathology Diagnosis: Integrating Histological and Colonoscopy Data in a Pilot Study</h3>
<ul>
<li><strong>Authors: </strong>Krithik Ramesh, Ritvik Koneru</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06351">https://arxiv.org/abs/2509.06351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06351">https://arxiv.org/pdf/2509.06351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06351]] A Multi-Modal Deep Learning Framework for Colorectal Pathology Diagnosis: Integrating Histological and Colonoscopy Data in a Pilot Study(https://arxiv.org/abs/2509.06351)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Colorectal diseases, including inflammatory conditions and neoplasms, require quick, accurate care to be effectively treated. Traditional diagnostic pipelines require extensive preparation and rely on separate, individual evaluations on histological images and colonoscopy footage, introducing possible variability and inefficiencies. This pilot study proposes a unified deep learning network that uses convolutional neural networks (CN N s) to classify both histopathological slides and colonoscopy video frames in one pipeline. The pipeline integrates class-balancing learning, robust augmentation, and calibration methods to ensure accurate results. Static colon histology images were taken from the PathMNIST dataset, and the lower gastrointestinal (colonoscopy) videos were drawn from the HyperKvasir dataset. The CNN architecture used was ResNet-50. This study demonstrates an interpretable and reproducible diagnostic pipeline that unifies multiple diagnostic modalities to advance and ease the detection of colorectal diseases.</li>
</ul>

<h3>Title: MRD-LiNet: A Novel Lightweight Hybrid CNN with Gradient-Guided Unlearning for Improved Drought Stress Identification</h3>
<ul>
<li><strong>Authors: </strong>Aswini Kumar Patra, Lingaraj Sahoo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06367">https://arxiv.org/abs/2509.06367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06367">https://arxiv.org/pdf/2509.06367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06367]] MRD-LiNet: A Novel Lightweight Hybrid CNN with Gradient-Guided Unlearning for Improved Drought Stress Identification(https://arxiv.org/abs/2509.06367)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Drought stress is a major threat to global crop productivity, making its early and precise detection essential for sustainable agricultural management. Traditional approaches, though useful, are often time-consuming and labor-intensive, which has motivated the adoption of deep learning methods. In recent years, Convolutional Neural Network (CNN) and Vision Transformer architectures have been widely explored for drought stress identification; however, these models generally rely on a large number of trainable parameters, restricting their use in resource-limited and real-time agricultural settings. To address this challenge, we propose a novel lightweight hybrid CNN framework inspired by ResNet, DenseNet, and MobileNet architectures. The framework achieves a remarkable 15-fold reduction in trainable parameters compared to conventional CNN and Vision Transformer models, while maintaining competitive accuracy. In addition, we introduce a machine unlearning mechanism based on a gradient norm-based influence function, which enables targeted removal of specific training data influence, thereby improving model adaptability. The method was evaluated on an aerial image dataset of potato fields with expert-annotated healthy and drought-stressed regions. Experimental results show that our framework achieves high accuracy while substantially lowering computational costs. These findings highlight its potential as a practical, scalable, and adaptive solution for drought stress monitoring in precision agriculture, particularly under resource-constrained conditions.</li>
</ul>

<h3>Title: From Perception to Protection: A Developer-Centered Study of Security and Privacy Threats in Extended Reality (XR)</h3>
<ul>
<li><strong>Authors: </strong>Kunlin Cai, Jinghuai Zhang, Ying Li, Zhiyuan Wang, Xun Chen, Tianshi Li, Yuan Tian</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06368">https://arxiv.org/abs/2509.06368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06368">https://arxiv.org/pdf/2509.06368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06368]] From Perception to Protection: A Developer-Centered Study of Security and Privacy Threats in Extended Reality (XR)(https://arxiv.org/abs/2509.06368)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect</a></li>
<li><strong>Abstract: </strong>The immersive nature of XR introduces a fundamentally different set of security and privacy (S&P) challenges due to the unprecedented user interactions and data collection that traditional paradigms struggle to mitigate. As the primary architects of XR applications, developers play a critical role in addressing novel threats. However, to effectively support developers, we must first understand how they perceive and respond to different threats. Despite the growing importance of this issue, there is a lack of in-depth, threat-aware studies that examine XR S&P from the developers' perspective. To fill this gap, we interviewed 23 professional XR developers with a focus on emerging threats in XR. Our study addresses two research questions aiming to uncover existing problems in XR development and identify actionable paths forward. By examining developers' perceptions of S&P threats, we found that: (1) XR development decisions (e.g., rich sensor data collection, user-generated content interfaces) are closely tied to and can amplify S&P threats, yet developers are often unaware of these risks, resulting in cognitive biases in threat perception; and (2) limitations in existing mitigation methods, combined with insufficient strategic, technical, and communication support, undermine developers' motivation, awareness, and ability to effectively address these threats. Based on these findings, we propose actionable and stakeholder-aware recommendations to improve XR S&P throughout the XR development process. This work represents the first effort to undertake a threat-aware, developer-centered study in the XR domain -- an area where the immersive, data-rich nature of the XR technology introduces distinctive challenges.</li>
</ul>

<h3>Title: Breaking SafetyCore: Exploring the Risks of On-Device AI Deployment</h3>
<ul>
<li><strong>Authors: </strong>Victor Guyomard, Mathis Mauvisseau, Marie Paindavoine</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06371">https://arxiv.org/abs/2509.06371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06371">https://arxiv.org/pdf/2509.06371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06371]] Breaking SafetyCore: Exploring the Risks of On-Device AI Deployment(https://arxiv.org/abs/2509.06371)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect</a></li>
<li><strong>Abstract: </strong>Due to hardware and software improvements, an increasing number of AI models are deployed on-device. This shift enhances privacy and reduces latency, but also introduces security risks distinct from traditional software. In this article, we examine these risks through the real-world case study of SafetyCore, an Android system service incorporating sensitive image content detection. We demonstrate how the on-device AI model can be extracted and manipulated to bypass detection, effectively rendering the protection ineffective. Our analysis exposes vulnerabilities of on-device AI models and provides a practical demonstration of how adversaries can exploit them.</li>
</ul>

<h3>Title: Variational Garrote for Statistical Physics-based Sparse and Robust Variable Selection</h3>
<ul>
<li><strong>Authors: </strong>Hyungjoon Soh, Dongha Lee, Vipul Periwal, Junghyo Jo</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06383">https://arxiv.org/abs/2509.06383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06383">https://arxiv.org/pdf/2509.06383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06383]] Variational Garrote for Statistical Physics-based Sparse and Robust Variable Selection(https://arxiv.org/abs/2509.06383)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>Selecting key variables from high-dimensional data is increasingly important in the era of big data. Sparse regression serves as a powerful tool for this purpose by promoting model simplicity and explainability. In this work, we revisit a valuable yet underutilized method, the statistical physics-based Variational Garrote (VG), which introduces explicit feature selection spin variables and leverages variational inference to derive a tractable loss function. We enhance VG by incorporating modern automatic differentiation techniques, enabling scalable and efficient optimization. We evaluate VG on both fully controllable synthetic datasets and complex real-world datasets. Our results demonstrate that VG performs especially well in highly sparse regimes, offering more consistent and robust variable selection than Ridge and LASSO regression across varying levels of sparsity. We also uncover a sharp transition: as superfluous variables are admitted, generalization degrades abruptly and the uncertainty of the selection variables increases. This transition point provides a practical signal for estimating the correct number of relevant variables, an insight we successfully apply to identify key predictors in real-world data. We expect that VG offers strong potential for sparse modeling across a wide range of applications, including compressed sensing and model pruning in machine learning.</li>
</ul>

<h3>Title: Your Super Resolution Model is not Enough for Tackling Real-World Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Dongsik Yoon, Jongeun Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06387">https://arxiv.org/abs/2509.06387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06387">https://arxiv.org/pdf/2509.06387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06387]] Your Super Resolution Model is not Enough for Tackling Real-World Scenarios(https://arxiv.org/abs/2509.06387)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Despite remarkable progress in Single Image Super-Resolution (SISR), traditional models often struggle to generalize across varying scale factors, limiting their real-world applicability. To address this, we propose a plug-in Scale-Aware Attention Module (SAAM) designed to retrofit modern fixed-scale SR models with the ability to perform arbitrary-scale SR. SAAM employs lightweight, scale-adaptive feature extraction and upsampling, incorporating the Simple parameter-free Attention Module (SimAM) for efficient guidance and gradient variance loss to enhance sharpness in image details. Our method integrates seamlessly into multiple state-of-the-art SR backbones (e.g., SCNet, HiT-SR, OverNet), delivering competitive or superior performance across a wide range of integer and non-integer scale factors. Extensive experiments on benchmark datasets demonstrate that our approach enables robust multi-scale upscaling with minimal computational overhead, offering a practical solution for real-world scenarios.</li>
</ul>

<h3>Title: Graph Neural Networks for Resource Allocation in Interference-limited Multi-Channel Wireless Networks with QoS Constraints</h3>
<ul>
<li><strong>Authors: </strong>Lili Chen, Changyang She, Jingge Zhu, Jamie Evans</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06395">https://arxiv.org/abs/2509.06395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06395">https://arxiv.org/pdf/2509.06395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06395]] Graph Neural Networks for Resource Allocation in Interference-limited Multi-Channel Wireless Networks with QoS Constraints(https://arxiv.org/abs/2509.06395)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Meeting minimum data rate constraints is a significant challenge in wireless communication systems, particularly as network complexity grows. Traditional deep learning approaches often address these constraints by incorporating penalty terms into the loss function and tuning hyperparameters empirically. However, this heuristic treatment offers no theoretical convergence guarantees and frequently fails to satisfy QoS requirements in practical scenarios. Building upon the structure of the WMMSE algorithm, we first extend it to a multi-channel setting with QoS constraints, resulting in the enhanced WMMSE (eWMMSE) algorithm, which is provably convergent to a locally optimal solution when the problem is feasible. To further reduce computational complexity and improve scalability, we develop a GNN-based algorithm, JCPGNN-M, capable of supporting simultaneous multi-channel allocation per user. To overcome the limitations of traditional deep learning methods, we propose a principled framework that integrates GNN with a Lagrangian-based primal-dual optimization method. By training the GNN within the Lagrangian framework, we ensure satisfaction of QoS constraints and convergence to a stationary point. Extensive simulations demonstrate that JCPGNN-M matches the performance of eWMMSE while offering significant gains in inference speed, generalization to larger networks, and robustness under imperfect channel state information. This work presents a scalable and theoretically grounded solution for constrained resource allocation in future wireless networks.</li>
</ul>

<h3>Title: AI-based response assessment and prediction in longitudinal imaging for brain metastases treated with stereotactic radiosurgery</h3>
<ul>
<li><strong>Authors: </strong>Lorenz Achim Kuhn, Daniel Abler, Jonas Richiardi, Andreas F. Hottinger, Luis Schiappacasse, Vincent Dunet, Adrien Depeursinge, Vincent Andrearczyk</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06396">https://arxiv.org/abs/2509.06396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06396">https://arxiv.org/pdf/2509.06396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06396]] AI-based response assessment and prediction in longitudinal imaging for brain metastases treated with stereotactic radiosurgery(https://arxiv.org/abs/2509.06396)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Brain Metastases (BM) are a large contributor to mortality of patients with cancer. They are treated with Stereotactic Radiosurgery (SRS) and monitored with Magnetic Resonance Imaging (MRI) at regular follow-up intervals according to treatment guidelines. Analyzing and quantifying this longitudinal imaging represents an intractable workload for clinicians. As a result, follow-up images are not annotated and merely assessed by observation. Response to treatment in longitudinal imaging is being studied, to better understand growth trajectories and ultimately predict treatment success or toxicity as early as possible. In this study, we implement an automated pipeline to curate a large longitudinal dataset of SRS treatment data, resulting in a cohort of 896 BMs in 177 patients who were monitored for >360 days at approximately two-month intervals at Lausanne University Hospital (CHUV). We use a data-driven clustering to identify characteristic trajectories. In addition, we predict 12 months lesion-level response using classical as well as graph machine learning Graph Machine Learning (GML). Clustering revealed 5 dominant growth trajectories with distinct final response categories. Response prediction reaches up to 0.90 AUC (CI95%=0.88-0.92) using only pre-treatment and first follow-up MRI with gradient boosting. Similarly, robust predictive performance of up to 0.88 AUC (CI95%=0.86-0.90) was obtained using GML, offering more flexibility with a single model for multiple input time-points configurations. Our results suggest potential automation and increased precision for the comprehensive assessment and prediction of BM response to SRS in longitudinal MRI. The proposed pipeline facilitates scalable data curation for the investigation of BM growth patterns, and lays the foundation for clinical decision support systems aiming at optimizing personalized care.</li>
</ul>

<h3>Title: Do LLMs exhibit the same commonsense capabilities across languages?</h3>
<ul>
<li><strong>Authors: </strong>Ivan Martínez-Murillo, Elena Lloret, Paloma Moreda, Albert Gatt</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06401">https://arxiv.org/abs/2509.06401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06401">https://arxiv.org/pdf/2509.06401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06401]] Do LLMs exhibit the same commonsense capabilities across languages?(https://arxiv.org/abs/2509.06401)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper explores the multilingual commonsense generation abilities of Large Language Models (LLMs). To facilitate this investigation, we introduce MULTICOM, a novel benchmark that extends the COCOTEROS dataset to four languages: English, Spanish, Dutch, and Valencian. The task involves generating a commonsensical sentence that includes a given triplet of words. We evaluate a range of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and Salamandra, on this benchmark. Our evaluation combines automatic metrics, LLM-as-a-judge approaches (using Prometheus and JudgeLM), and human annotations. Results consistently show superior performance in English, with significantly lower performance in less-resourced languages. While contextual support yields mixed results, it tends to benefit underrepresented languages. These findings underscore the current limitations of LLMs in multilingual commonsense generation. The dataset is publicly available at this https URL.</li>
</ul>

<h3>Title: VQualA 2025 Challenge on Image Super-Resolution Generated Content Quality Assessment: Methods and Results</h3>
<ul>
<li><strong>Authors: </strong>Yixiao Li, Xin Li, Chris Wei Zhou, Shuo Xing, Hadi Amirpour, Xiaoshuai Hao, Guanghui Yue, Baoquan Zhao, Weide Liu, Xiaoyuan Yang, Zhengzhong Tu, Xinyu Li, Chuanbiao Song, Chenqi Zhang, Jun Lan, Huijia Zhu, Weiqiang Wang, Xiaoyan Sun, Shishun Tian, Dongyang Yan, Weixia Zhang, Junlin Chen, Wei Sun, Zhihua Wang, Zhuohang Shi, Zhizun Luo, Hang Ouyang, Tianxin Xiao, Fan Yang, Zhaowang Wu, Kaixin Deng</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06413">https://arxiv.org/abs/2509.06413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06413">https://arxiv.org/pdf/2509.06413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06413]] VQualA 2025 Challenge on Image Super-Resolution Generated Content Quality Assessment: Methods and Results(https://arxiv.org/abs/2509.06413)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper presents the ISRGC-Q Challenge, built upon the Image Super-Resolution Generated Content Quality Assessment (ISRGen-QA) dataset, and organized as part of the Visual Quality Assessment (VQualA) Competition at the ICCV 2025 Workshops. Unlike existing Super-Resolution Image Quality Assessment (SR-IQA) datasets, ISRGen-QA places a greater emphasis on SR images generated by the latest generative approaches, including Generative Adversarial Networks (GANs) and diffusion models. The primary goal of this challenge is to analyze the unique artifacts introduced by modern super-resolution techniques and to evaluate their perceptual quality effectively. A total of 108 participants registered for the challenge, with 4 teams submitting valid solutions and fact sheets for the final testing phase. These submissions demonstrated state-of-the-art (SOTA) performance on the ISRGen-QA dataset. The project is publicly available at: this https URL.</li>
</ul>

<h3>Title: CAPMix: Robust Time Series Anomaly Detection Based on Abnormal Assumptions with Dual-Space Mixup</h3>
<ul>
<li><strong>Authors: </strong>Xudong Mou, Rui Wang, Tiejun Wang, Renyu Yang, Shiru Chen, Jie Sun, Tianyu Wo, Xudong Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06419">https://arxiv.org/abs/2509.06419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06419">https://arxiv.org/pdf/2509.06419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06419]] CAPMix: Robust Time Series Anomaly Detection Based on Abnormal Assumptions with Dual-Space Mixup(https://arxiv.org/abs/2509.06419)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection (TSAD) is a vital yet challenging task, particularly in scenarios where labeled anomalies are scarce and temporal dependencies are complex. Recent anomaly assumption (AA) approaches alleviate the lack of anomalies by injecting synthetic samples and training discriminative models. Despite promising results, these methods often suffer from two fundamental limitations: patchy generation, where scattered anomaly knowledge leads to overly simplistic or incoherent anomaly injection, and Anomaly Shift, where synthetic anomalies either resemble normal data too closely or diverge unrealistically from real anomalies, thereby distorting classification boundaries. In this paper, we propose CAPMix, a controllable anomaly augmentation framework that addresses both issues. First, we design a CutAddPaste mechanism to inject diverse and complex anomalies in a targeted manner, avoiding patchy generation. Second, we introduce a label revision strategy to adaptively refine anomaly labels, reducing the risk of anomaly shift. Finally, we employ dual-space mixup within a temporal convolutional network to enforce smoother and more robust decision boundaries. Extensive experiments on five benchmark datasets, including AIOps, UCR, SWaT, WADI, and ESA, demonstrate that CAPMix achieves significant improvements over state-of-the-art baselines, with enhanced robustness against contaminated training data. The code is available at this https URL.</li>
</ul>

<h3>Title: Phantom-Insight: Adaptive Multi-cue Fusion for Video Camouflaged Object Detection with Multimodal LLM</h3>
<ul>
<li><strong>Authors: </strong>Hua Zhang, Changjiang Luo, Ruoyu Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06422">https://arxiv.org/abs/2509.06422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06422">https://arxiv.org/pdf/2509.06422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06422]] Phantom-Insight: Adaptive Multi-cue Fusion for Video Camouflaged Object Detection with Multimodal LLM(https://arxiv.org/abs/2509.06422)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video camouflaged object detection (VCOD) is challenging due to dynamic environments. Existing methods face two main issues: (1) SAM-based methods struggle to separate camouflaged object edges due to model freezing, and (2) MLLM-based methods suffer from poor object separability as large language models merge foreground and background. To address these issues, we propose a novel VCOD method based on SAM and MLLM, called Phantom-Insight. To enhance the separability of object edge details, we represent video sequences with temporal and spatial clues and perform feature fusion via LLM to increase information density. Next, multiple cues are generated through the dynamic foreground visual token scoring module and the prompt network to adaptively guide and fine-tune the SAM model, enabling it to adapt to subtle textures. To enhance the separability of objects and background, we propose a decoupled foreground-background learning strategy. By generating foreground and background cues separately and performing decoupled training, the visual token can effectively integrate foreground and background information independently, enabling SAM to more accurately segment camouflaged objects in the video. Experiments on the MoCA-Mask dataset show that Phantom-Insight achieves state-of-the-art performance across various metrics. Additionally, its ability to detect unseen camouflaged objects on the CAD2016 dataset highlights its strong generalization ability.</li>
</ul>

<h3>Title: When Language Model Guides Vision: Grounding DINO for Cattle Muzzle Detection</h3>
<ul>
<li><strong>Authors: </strong>Rabin Dulal, Lihong Zheng, Muhammad Ashad Kabir</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06427">https://arxiv.org/abs/2509.06427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06427">https://arxiv.org/pdf/2509.06427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06427]] When Language Model Guides Vision: Grounding DINO for Cattle Muzzle Detection(https://arxiv.org/abs/2509.06427)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>Muzzle patterns are among the most effective biometric traits for cattle identification. Fast and accurate detection of the muzzle region as the region of interest is critical to automatic visual cattle identification.. Earlier approaches relied on manual detection, which is labor-intensive and inconsistent. Recently, automated methods using supervised models like YOLO have become popular for muzzle detection. Although effective, these methods require extensive annotated datasets and tend to be trained data-dependent, limiting their performance on new or unseen cattle. To address these limitations, this study proposes a zero-shot muzzle detection framework based on Grounding DINO, a vision-language model capable of detecting muzzles without any task-specific training or annotated data. This approach leverages natural language prompts to guide detection, enabling scalable and flexible muzzle localization across diverse breeds and environments. Our model achieves a mean Average Precision (mAP)@0.5 of 76.8\%, demonstrating promising performance without requiring annotated data. To our knowledge, this is the first research to provide a real-world, industry-oriented, and annotation-free solution for cattle muzzle detection. The framework offers a practical alternative to supervised methods, promising improved adaptability and ease of deployment in livestock monitoring applications.</li>
</ul>

<h3>Title: Cross3DReg: Towards a Large-scale Real-world Cross-source Point Cloud Registration Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Zongyi Xu, Zhongpeng Lang, Yilong Chen, Shanshan Zhao, Xiaoshui Huang, Yifan Zuo, Yan Zhang, Qianni Zhang, Xinbo Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06456">https://arxiv.org/abs/2509.06456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06456">https://arxiv.org/pdf/2509.06456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06456]] Cross3DReg: Towards a Large-scale Real-world Cross-source Point Cloud Registration Benchmark(https://arxiv.org/abs/2509.06456)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Cross-source point cloud registration, which aims to align point cloud data from different sensors, is a fundamental task in 3D vision. However, compared to the same-source point cloud registration, cross-source registration faces two core challenges: the lack of publicly available large-scale real-world datasets for training the deep registration models, and the inherent differences in point clouds captured by multiple sensors. The diverse patterns induced by the sensors pose great challenges in robust and accurate point cloud feature extraction and matching, which negatively influence the registration accuracy. To advance research in this field, we construct Cross3DReg, the currently largest and real-world multi-modal cross-source point cloud registration dataset, which is collected by a rotating mechanical lidar and a hybrid semi-solid-state lidar, respectively. Moreover, we design an overlap-based cross-source registration framework, which utilizes unaligned images to predict the overlapping region between source and target point clouds, effectively filtering out redundant points in the irrelevant regions and significantly mitigating the interference caused by noise in non-overlapping areas. Then, a visual-geometric attention guided matching module is proposed to enhance the consistency of cross-source point cloud features by fusing image and geometric information to establish reliable correspondences and ultimately achieve accurate and robust registration. Extensive experiments show that our method achieves state-of-the-art registration performance. Our framework reduces the relative rotation error (RRE) and relative translation error (RTE) by $63.2\%$ and $40.2\%$, respectively, and improves the registration recall (RR) by $5.4\%$, which validates its effectiveness in achieving accurate cross-source registration.</li>
</ul>

<h3>Title: IGAff: Benchmarking Adversarial Iterative and Genetic Affine Algorithms on Deep Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Sebastian-Vasile Echim, Andrei-Alexandru Preda, Dumitru-Clementin Cercel, Florin Pop</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06459">https://arxiv.org/abs/2509.06459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06459">https://arxiv.org/pdf/2509.06459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06459]] IGAff: Benchmarking Adversarial Iterative and Genetic Affine Algorithms on Deep Neural Networks(https://arxiv.org/abs/2509.06459)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>Deep neural networks currently dominate many fields of the artificial intelligence landscape, achieving state-of-the-art results on numerous tasks while remaining hard to understand and exhibiting surprising weaknesses. An active area of research focuses on adversarial attacks, which aim to generate inputs that uncover these weaknesses. However, this proves challenging, especially in the black-box scenario where model details are inaccessible. This paper explores in detail the impact of such adversarial algorithms on ResNet-18, DenseNet-121, Swin Transformer V2, and Vision Transformer network architectures. Leveraging the Tiny ImageNet, Caltech-256, and Food-101 datasets, we benchmark two novel black-box iterative adversarial algorithms based on affine transformations and genetic algorithms: 1) Affine Transformation Attack (ATA), an iterative algorithm maximizing our attack score function using random affine transformations, and 2) Affine Genetic Attack (AGA), a genetic algorithm that involves random noise and affine transformations. We evaluate the performance of the models in the algorithm parameter variation, data augmentation, and global and targeted attack configurations. We also compare our algorithms with two black-box adversarial algorithms, Pixle and Square Attack. Our experiments yield better results on the image classification task than similar methods in the literature, achieving an accuracy improvement of up to 8.82%. We provide noteworthy insights into successful adversarial defenses and attacks at both global and targeted levels, and demonstrate adversarial robustness through algorithm parameter variation.</li>
</ul>

<h3>Title: Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yuyao Ge, Shenghua Liu, Yiwei Wang, Lingrui Mei, Baolong Bi, Xuanshan Zhou, Jiayu Yao, Jiafeng Guo, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06461">https://arxiv.org/abs/2509.06461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06461">https://arxiv.org/pdf/2509.06461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06461]] Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning(https://arxiv.org/abs/2509.06461)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have demonstrated remarkable success across diverse visual tasks, yet their performance degrades in complex visual environments. While existing enhancement approaches require additional training, rely on external segmentation tools, or operate at coarse-grained levels, they overlook the innate ability within VLMs. To bridge this gap, we investigate VLMs' attention patterns and discover that: (1) visual complexity strongly correlates with attention entropy, negatively impacting reasoning performance; (2) attention progressively refines from global scanning in shallow layers to focused convergence in deeper layers, with convergence degree determined by visual complexity. (3) Theoretically, we prove that the contrast of attention maps between general queries and task-specific queries enables the decomposition of visual signal into semantic signals and visual noise components. Building on these insights, we propose Contrastive Attention Refinement for Visual Enhancement (CARVE), a training-free method that extracts task-relevant visual signals through attention contrasting at the pixel level. Extensive experiments demonstrate that CARVE consistently enhances performance, achieving up to 75% improvement on open-source models. Our work provides critical insights into the interplay between visual complexity and attention mechanisms, offering an efficient pathway for improving visual reasoning with contrasting attention.</li>
</ul>

<h3>Title: A Statistical 3D Stomach Shape Model for Anatomical Analysis</h3>
<ul>
<li><strong>Authors: </strong>Erez Posner, Ore Shtalrid, Oded Erell, Daniel Noy, Moshe Bouhnik</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06464">https://arxiv.org/abs/2509.06464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06464">https://arxiv.org/pdf/2509.06464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06464]] A Statistical 3D Stomach Shape Model for Anatomical Analysis(https://arxiv.org/abs/2509.06464)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Realistic and parameterized 3D models of human anatomy have become invaluable in research, diagnostics, and surgical planning. However, the development of detailed models for internal organs, such as the stomach, has been limited by data availability and methodological challenges. In this paper, we propose a novel pipeline for the generation of synthetic 3D stomach models, enabling the creation of anatomically diverse morphologies informed by established studies on stomach shape variability. Using this pipeline, we construct a dataset of synthetic stomachs. Building on this dataset, we develop a 3D statistical shape model of the stomach, trained to capture natural anatomical variability in a low-dimensional shape space. The model is further refined using CT meshes derived from publicly available datasets through a semi-supervised alignment process, enhancing its ability to generalize to unseen anatomical variations. We evaluated the model on a held-out test set of real stomach CT scans, demonstrating robust generalization and fit accuracy. We make the statistical shape model along with the synthetic dataset publicly available on GitLab: this https URL to facilitate further research. This work introduces the first statistical 3D shape model of the stomach, with applications ranging from surgical simulation and pre-operative planning to medical education and computational modeling. By combining synthetic data generation, parametric modeling, and real-world validation, our approach represents a significant advancement in organ modeling and opens new possibilities for personalized healthcare solutions.</li>
</ul>

<h3>Title: CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction</h3>
<ul>
<li><strong>Authors: </strong>Hongzong Li, Jiahao Ma, Zhanpeng Shi, Fanming Jin, Ye-Fan Hu, Jian-Dong Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06465">https://arxiv.org/abs/2509.06465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06465">https://arxiv.org/pdf/2509.06465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06465]] CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction(https://arxiv.org/abs/2509.06465)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Antibody binding site prediction plays a pivotal role in computational immunology and therapeutic antibody design. Existing sequence or structure methods rely on single-view features and fail to identify antibody-specific binding sites on the antigens-a dual limitation in representation and prediction. In this paper, we propose CAME-AB, a novel Cross-modality Attention framework with a Mixture-of-Experts (MoE) backbone for robust antibody binding site prediction. CAME-AB integrates five biologically grounded modalities, including raw amino acid encodings, BLOSUM substitution profiles, pretrained language model embeddings, structure-aware features, and GCN-refined biochemical graphs-into a unified multimodal representation. To enhance adaptive cross-modal reasoning, we propose an adaptive modality fusion module that learns to dynamically weight each modality based on its global relevance and input-specific contribution. A Transformer encoder combined with an MoE module further promotes feature specialization and capacity expansion. We additionally incorporate a supervised contrastive learning objective to explicitly shape the latent space geometry, encouraging intra-class compactness and inter-class separability. To improve optimization stability and generalization, we apply stochastic weight averaging during training. Extensive experiments on benchmark antibody-antigen datasets demonstrate that CAME-AB consistently outperforms strong baselines on multiple metrics, including Precision, Recall, F1-score, AUC-ROC, and MCC. Ablation studies further validate the effectiveness of each architectural component and the benefit of multimodal feature integration. The model implementation details and the codes are available on this https URL</li>
</ul>

<h3>Title: Does DINOv3 Set a New Medical Vision Standard?</h3>
<ul>
<li><strong>Authors: </strong>Che Liu, Yinda Chen, Haoyuan Shi, Jinpeng Lu, Bailiang Jian, Jiazhen Pan, Linghan Cai, Jiayi Wang, Yundi Zhang, Jun Li, Cosmin I. Bercea, Cheng Ouyang, Chen Chen, Zhiwei Xiong, Benedikt Wiestler, Christian Wachinger, Daniel Rueckert, Wenjia Bai, Rossella Arcucci</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06467">https://arxiv.org/abs/2509.06467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06467">https://arxiv.org/pdf/2509.06467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06467]] Does DINOv3 Set a New Medical Vision Standard?(https://arxiv.org/abs/2509.06467)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The advent of large-scale vision foundation models, pre-trained on diverse natural images, has marked a paradigm shift in computer vision. However, how the frontier vision foundation models' efficacies transfer to specialized domains remains such as medical imaging remains an open question. This report investigates whether DINOv3, a state-of-the-art self-supervised vision transformer (ViT) that features strong capability in dense prediction tasks, can directly serve as a powerful, unified encoder for medical vision tasks without domain-specific pre-training. To answer this, we benchmark DINOv3 across common medical vision tasks, including 2D/3D classification and segmentation on a wide range of medical imaging modalities. We systematically analyze its scalability by varying model sizes and input image resolutions. Our findings reveal that DINOv3 shows impressive performance and establishes a formidable new baseline. Remarkably, it can even outperform medical-specific foundation models like BiomedCLIP and CT-Net on several tasks, despite being trained solely on natural images. However, we identify clear limitations: The model's features degrade in scenarios requiring deep domain specialization, such as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM), and Positron Emission Tomography (PET). Furthermore, we observe that DINOv3 does not consistently obey scaling law in the medical domain; performance does not reliably increase with larger models or finer feature resolutions, showing diverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3 as a strong baseline, whose powerful visual features can serve as a robust prior for multiple complex medical tasks. This opens promising future directions, such as leveraging its features to enforce multiview consistency in 3D reconstruction.</li>
</ul>

<h3>Title: DyC-STG: Dynamic Causal Spatio-Temporal Graph Network for Real-time Data Credibility Analysis in IoT</h3>
<ul>
<li><strong>Authors: </strong>Guanjie Cheng, Boyi Li, Peihan Wu, Feiyi Chen, Xinkui Zhao, Mengying Zhu, Shuiguang Deng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06483">https://arxiv.org/abs/2509.06483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06483">https://arxiv.org/pdf/2509.06483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06483]] DyC-STG: Dynamic Causal Spatio-Temporal Graph Network for Real-time Data Credibility Analysis in IoT(https://arxiv.org/abs/2509.06483)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The wide spreading of Internet of Things (IoT) sensors generates vast spatio-temporal data streams, but ensuring data credibility is a critical yet unsolved challenge for applications like smart homes. While spatio-temporal graph (STG) models are a leading paradigm for such data, they often fall short in dynamic, human-centric environments due to two fundamental limitations: (1) their reliance on static graph topologies, which fail to capture physical, event-driven dynamics, and (2) their tendency to confuse spurious correlations with true causality, undermining robustness in human-centric environments. To address these gaps, we propose the Dynamic Causal Spatio-Temporal Graph Network (DyC-STG), a novel framework designed for real-time data credibility analysis in IoT. Our framework features two synergistic contributions: an event-driven dynamic graph module that adapts the graph topology in real-time to reflect physical state changes, and a causal reasoning module to distill causally-aware representations by strictly enforcing temporal precedence. To facilitate the research in this domain we release two new real-world datasets. Comprehensive experiments show that DyC-STG establishes a new state-of-the-art, outperforming the strongest baselines by 1.4 percentage points and achieving an F1-Score of up to 0.930.</li>
</ul>

<h3>Title: A machine-learned expression for the excess Gibbs energy</h3>
<ul>
<li><strong>Authors: </strong>Marco Hoffmann, Thomas Specht, Quirin Göttl, Jakob Burger, Stephan Mandt, Hans Hasse, Fabian Jirasek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06484">https://arxiv.org/abs/2509.06484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06484">https://arxiv.org/pdf/2509.06484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06484]] A machine-learned expression for the excess Gibbs energy(https://arxiv.org/abs/2509.06484)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The excess Gibbs energy plays a central role in chemical engineering and chemistry, providing a basis for modeling the thermodynamic properties of liquid mixtures. Predicting the excess Gibbs energy of multi-component mixtures solely from the molecular structures of their components is a long-standing challenge. In this work, we address this challenge by integrating physical laws as hard constraints within a flexible neural network. The resulting model, HANNA, was trained end-to-end on an extensive experimental dataset for binary mixtures from the Dortmund Data Bank, guaranteeing thermodynamically consistent predictions. A novel surrogate solver developed in this work enabled the inclusion of liquid-liquid equilibrium data in the training process. Furthermore, a geometric projection method was applied to enable robust extrapolations to multi-component mixtures, without requiring additional parameters. We demonstrate that HANNA delivers excellent predictions, clearly outperforming state-of-the-art benchmark methods in accuracy and scope. The trained model and corresponding code are openly available, and an interactive interface is provided on our website, MLPROP.</li>
</ul>

<h3>Title: WS$^2$: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting</h3>
<ul>
<li><strong>Authors: </strong>Andrea Marelli, Alberto Foresti, Leonardo Pesce, Giacomo Boracchi, Mario Grosso</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06485">https://arxiv.org/abs/2509.06485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06485">https://arxiv.org/pdf/2509.06485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06485]] WS$^2$: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting(https://arxiv.org/abs/2509.06485)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In industrial quality control, to visually recognize unwanted items within a moving heterogeneous stream, human operators are often still indispensable. Waste-sorting stands as a significant example, where operators on multiple conveyor belts manually remove unwanted objects to select specific materials. To automate this recognition problem, computer vision systems offer great potential in accurately identifying and segmenting unwanted items in such settings. Unfortunately, considering the multitude and the variety of sorting tasks, fully supervised approaches are not a viable option to address this challange, as they require extensive labeling efforts. Surprisingly, weakly supervised alternatives that leverage the implicit supervision naturally provided by the operator in his removal action are relatively unexplored. In this paper, we define the concept of Before-After Supervision, illustrating how to train a segmentation network by leveraging only the visual differences between images acquired \textit{before} and \textit{after} the operator. To promote research in this direction, we introduce WS$^2$ (Weakly Supervised segmentation for Waste-Sorting), the first multiview dataset consisting of more than 11 000 high-resolution video frames captured on top of a conveyor belt, including "before" and "after" images. We also present a robust end-to-end pipeline, used to benchmark several state-of-the-art weakly supervised segmentation methods on WS$^2$.</li>
</ul>

<h3>Title: TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Jibai Lin, Bo Ma, Yating Yang, Rong Ma, Turghun Osman, Ahtamjan Ahmat, Rui Dong, Lei Wang, Xi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06499">https://arxiv.org/abs/2509.06499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06499">https://arxiv.org/pdf/2509.06499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06499]] TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement(https://arxiv.org/abs/2509.06499)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Subject-driven image generation (SDIG) aims to manipulate specific subjects within images while adhering to textual instructions, a task crucial for advancing text-to-image diffusion models. SDIG requires reconciling the tension between maintaining subject identity and complying with dynamic edit instructions, a challenge inadequately addressed by existing methods. In this paper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework, which resolves this tension through target supervision and preference learning without test-time fine-tuning. TIDE pioneers target-supervised triplet alignment, modelling subject adaptation dynamics using a (reference image, instruction, target images) triplet. This approach leverages the Direct Subject Diffusion (DSD) objective, training the model with paired "winning" (balanced preservation-compliance) and "losing" (distorted) targets, systematically generated and evaluated via quantitative metrics. This enables implicit reward modelling for optimal preservation-compliance balance. Experimental results on standard benchmarks demonstrate TIDE's superior performance in generating subject-faithful outputs while maintaining instruction compliance, outperforming baseline methods across multiple quantitative metrics. TIDE's versatility is further evidenced by its successful application to diverse tasks, including structural-conditioned generation, image-to-image generation, and text-image interpolation. Our code is available at this https URL.</li>
</ul>

<h3>Title: WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents</h3>
<ul>
<li><strong>Authors: </strong>Junteng Liu, Yunji Li, Chi Zhang, Jingyang Li, Aili Chen, Ke Ji, Weiyu Cheng, Zijia Wu, Chengyu Du, Qidi Xu, Jiayuan Song, Zhengmao Zhu, Wenhu Chen, Pengyu Zhao, Junxian He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06501">https://arxiv.org/abs/2509.06501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06501">https://arxiv.org/pdf/2509.06501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06501]] WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents(https://arxiv.org/abs/2509.06501)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The paradigm of Large Language Models (LLMs) has increasingly shifted toward agentic applications, where web browsing capabilities are fundamental for retrieving information from diverse online sources. However, existing open-source web agents either demonstrate limited information-seeking abilities on complex tasks or lack transparent implementations. In this work, we identify that the key challenge lies in the scarcity of challenging data for information seeking. To address this limitation, we introduce WebExplorer: a systematic data generation approach using model-based exploration and iterative, long-to-short query evolution. This method creates challenging query-answer pairs that require multi-step reasoning and complex web navigation. By leveraging our curated high-quality dataset, we successfully develop advanced web agent WebExplorer-8B through supervised fine-tuning followed by reinforcement learning. Our model supports 128K context length and up to 100 tool calling turns, enabling long-horizon problem solving. Across diverse information-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art performance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able to effectively search over an average of 16 turns after RL training, achieving higher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best performance among models up to 100B parameters on WebWalkerQA and FRAMES. Beyond these information-seeking tasks, our model also achieves strong generalization on the HLE benchmark even though it is only trained on knowledge-intensive QA data. These results highlight our approach as a practical path toward long-horizon web agents.</li>
</ul>

<h3>Title: When Code Crosses Borders: A Security-Centric Evaluation of LLM-based Code Translation</h3>
<ul>
<li><strong>Authors: </strong>Hailong Chang, Guozhu Meng, Shuhui Xiao, Kai Chen, Kun Sun, Yilin Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06504">https://arxiv.org/abs/2509.06504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06504">https://arxiv.org/pdf/2509.06504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06504]] When Code Crosses Borders: A Security-Centric Evaluation of LLM-based Code Translation(https://arxiv.org/abs/2509.06504)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>With the growing demand for cross-language codebase migration, evaluating LLMs' security implications in translation tasks has become critical. Existing evaluations primarily focus on syntactic or functional correctness at the function level, neglecting the critical dimension of security. To enable security evaluation, we construct STED (Security-centric Translation Evaluation Dataset), the first dataset specifically designed for evaluating the security implications of LLM-based code translation. It comprises 720 security-related code samples across five programming languages and nine high-impact CWE categories, sourced from CVE/NVD and manually verified for translation tasks. Our evaluation framework consists of two independent assessment modules: (1) rigorous evaluation by security researchers, and (2) automated analysis via LLM-as-a-judge. Together they evaluate three critical aspects: functional correctness, vulnerability preservation, and vulnerability introduction rates. Our large-scale evaluation of five state-of-the-art LLMs across 6,000 translation instances reveals significant security degradation, with 28.6-45% of translations introducing new vulnerabilities--particularly for web-related flaws like input validation, where LLMs show consistent weaknesses. Furthermore, we develop a Retrieval-Augmented Generation (RAG)-based mitigation strategy that reduces translation-induced vulnerabilities by 32.8%, showing the potential of knowledge-enhanced prompting.</li>
</ul>

<h3>Title: On optimal solutions of classical and sliced Wasserstein GANs with non-Gaussian data</h3>
<ul>
<li><strong>Authors: </strong>Yu-Jui Huang, Hsin-Hua Shen, Yu-Chih Huang, Wan-Yi Lin, Shih-Chun Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06505">https://arxiv.org/abs/2509.06505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06505">https://arxiv.org/pdf/2509.06505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06505]] On optimal solutions of classical and sliced Wasserstein GANs with non-Gaussian data(https://arxiv.org/abs/2509.06505)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The generative adversarial network (GAN) aims to approximate an unknown distribution via a parameterized neural network (NN). While GANs have been widely applied in reinforcement and semisupervised learning as well as computer vision tasks, selecting their parameters often needs an exhaustive search and only a few selection methods can be proved to be theoretically optimal. One of the most promising GAN variants is the Wasserstein GAN (WGAN). Prior work on optimal parameters for WGAN is limited to the linear-quadratic-Gaussian (LQG) setting, where the NN is linear and the data is Gaussian. In this paper, we focus on the characterization of optimal WGAN parameters beyond the LQG setting. We derive closed-form optimal parameters for one-dimensional WGANs when the NN has non-linear activation functions and the data is non-Gaussian. To extend this to high-dimensional WGANs, we adopt the sliced Wasserstein framework and replace the constraint on marginal distributions of the randomly projected data by a constraint on the joint distribution of the original (unprojected) data. We show that the linear generator can be asymptotically optimal for sliced WGAN with non-Gaussian data. Empirical studies show that our closed-form WGAN parameters have good convergence behavior with data under both Gaussian and Laplace distributions. Also, compared to the r principal component analysis (r-PCA) solution, our proposed solution for sliced WGAN can achieve the same performance while requiring less computational resources.</li>
</ul>

<h3>Title: Synthesis of Sound and Precise Leakage Contracts for Open-Source RISC-V Processors</h3>
<ul>
<li><strong>Authors: </strong>Zilong Wang, Gideon Mohr, Klaus von Gleissenthall, Jan Reineke, Marco Guarnieri</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06509">https://arxiv.org/abs/2509.06509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06509">https://arxiv.org/pdf/2509.06509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06509]] Synthesis of Sound and Precise Leakage Contracts for Open-Source RISC-V Processors(https://arxiv.org/abs/2509.06509)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Leakage contracts have been proposed as a new security abstraction at the instruction set architecture level. Leakage contracts aim to capture the information that processors may leak via microarchitectural side channels. Recently, the first tools have emerged to verify whether a processor satisfies a given contract. However, coming up with a contract that is both sound and precise for a given processor is challenging, time-consuming, and error-prone, as it requires in-depth knowledge of the timing side channels introduced by microarchitectural optimizations. In this paper, we address this challenge by proposing LeaSyn, the first tool for automatically synthesizing leakage contracts that are both sound and precise for processor designs at register-transfer level. Starting from a user-provided contract template that captures the space of possible contracts, LeaSyn automatically constructs a contract, alternating between contract synthesis, which ensures precision based on an empirical characterization of the processor's leaks, and contract verification, which ensures soundness. Using LeaSyn, we automatically synthesize contracts for six open-source RISC-V CPUs for a variety of contract templates. Our experiments indicate that LeaSyn's contracts are sound and more precise (i.e., represent the actual leaks in the target processor more faithfully) than contracts constructed by existing approaches.</li>
</ul>

<h3>Title: Predicting Brain Tumor Response to Therapy using a Hybrid Deep Learning and Radiomics Approach</h3>
<ul>
<li><strong>Authors: </strong>Daniil Tikhonov, Matheus Scatolin, Mohor Banerjee, Qiankun Ji, Ahmed Jaheen, Mostafa Salem, Abdelrahman Elsayed, Hu Wang, Sarim Hashmi, Mohammad Yaqub</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06511">https://arxiv.org/abs/2509.06511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06511">https://arxiv.org/pdf/2509.06511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06511]] Predicting Brain Tumor Response to Therapy using a Hybrid Deep Learning and Radiomics Approach(https://arxiv.org/abs/2509.06511)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Accurate evaluation of the response of glioblastoma to therapy is crucial for clinical decision-making and patient management. The Response Assessment in Neuro-Oncology (RANO) criteria provide a standardized framework to assess patients' clinical response, but their application can be complex and subject to observer variability. This paper presents an automated method for classifying the intervention response from longitudinal MRI scans, developed to predict tumor response during therapy as part of the BraTS 2025 challenge. We propose a novel hybrid framework that combines deep learning derived feature extraction and an extensive set of radiomics and clinically chosen features. Our approach utilizes a fine-tuned ResNet-18 model to extract features from 2D regions of interest across four MRI modalities. These deep features are then fused with a rich set of more than 4800 radiomic and clinically driven features, including 3D radiomics of tumor growth and shrinkage masks, volumetric changes relative to the nadir, and tumor centroid shift. Using the fused feature set, a CatBoost classifier achieves a mean ROC AUC of 0.81 and a Macro F1 score of 0.50 in the 4-class response prediction task (Complete Response, Partial Response, Stable Disease, Progressive Disease). Our results highlight that synergizing learned image representations with domain-targeted radiomic features provides a robust and effective solution for automated treatment response assessment in neuro-oncology.</li>
</ul>

<h3>Title: QualityFM: a Multimodal Physiological Signal Foundation Model with Self-Distillation for Signal Quality Challenges in Critically Ill Patients</h3>
<ul>
<li><strong>Authors: </strong>Zongheng Guo, Tao Chen, Manuela Ferrario</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06516">https://arxiv.org/abs/2509.06516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06516">https://arxiv.org/pdf/2509.06516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06516]] QualityFM: a Multimodal Physiological Signal Foundation Model with Self-Distillation for Signal Quality Challenges in Critically Ill Patients(https://arxiv.org/abs/2509.06516)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Photoplethysmogram (PPG) and electrocardiogram (ECG) are commonly recorded in intesive care unit (ICU) and operating room (OR). However, the high incidence of poor, incomplete, and inconsistent signal quality, can lead to false alarms or diagnostic inaccuracies. The methods explored so far suffer from limited generalizability, reliance on extensive labeled data, and poor cross-task transferability. To overcome these challenges, we introduce QualityFM, a novel multimodal foundation model for these physiological signals, designed to acquire a general-purpose understanding of signal quality. Our model is pre-trained on an large-scale dataset comprising over 21 million 30-second waveforms and 179,757 hours of data. Our approach involves a dual-track architecture that processes paired physiological signals of differing quality, leveraging a self-distillation strategy where an encoder for high-quality signals is used to guide the training of an encoder for low-quality signals. To efficiently handle long sequential signals and capture essential local quasi-periodic patterns, we integrate a windowed sparse attention mechanism within our Transformer-based model. Furthermore, a composite loss function, which combines direct distillation loss on encoder outputs with indirect reconstruction loss based on power and phase spectra, ensures the preservation of frequency-domain characteristics of the signals. We pre-train three models with varying parameter counts (9.6 M to 319 M) and demonstrate their efficacy and practical value through transfer learning on three distinct clinical tasks: false alarm of ventricular tachycardia detection, the identification of atrial fibrillation and the estimation of arterial blood pressure (ABP) from PPG and ECG signals.</li>
</ul>

<h3>Title: Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Andrei Baroian, Kasper Notebomer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06518">https://arxiv.org/abs/2509.06518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06518">https://arxiv.org/pdf/2509.06518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06518]] Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training(https://arxiv.org/abs/2509.06518)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based language models traditionally use uniform (isotropic) layer sizes, yet they ignore the diverse functional roles that different depths can play and their computational capacity needs. Building on Layer-Wise Scaling (LWS) and pruning literature, we introduce three new LWS variants - Framed, Reverse, and Crown - that redistribute FFN widths and attention heads via two or three-point linear interpolation in the pre-training stage. We present the first systematic ablation of LWS and its variants, on a fixed budget of 180M parameters, trained on 5B tokens. All models converge to similar losses and achieve better performance compared to an equal-cost isotropic baseline, without a substantial decrease in training throughput. This work represents an initial step into the design space of layer-wise architectures for pre-training, but future work should scale experiments to orders of magnitude more tokens and parameters to fully assess their potential.</li>
</ul>

<h3>Title: LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection</h3>
<ul>
<li><strong>Authors: </strong>Jian Wu, Hang Yu, Bingchang Liu, Wenjie Yang, Peng Di, Jianguo Li, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06524">https://arxiv.org/abs/2509.06524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06524">https://arxiv.org/pdf/2509.06524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06524]] LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection(https://arxiv.org/abs/2509.06524)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Adapting large language models (LLMs) to specific domains often faces a critical bottleneck: the scarcity of high-quality, human-curated data. While large volumes of unchecked data are readily available, indiscriminately using them for fine-tuning risks introducing noise and degrading performance. Strategic data selection is thus crucial, requiring a method that is both accurate and efficient. Existing approaches, categorized as similarity-based and direct optimization methods, struggle to simultaneously achieve these goals. In this paper, we introduce LAMDAS (LLM As an iMplicit classifier for domain-specific DAta Selection), a novel approach that leverages the pre-trained LLM itself as an implicit classifier, thereby bypassing explicit feature engineering and computationally intensive optimization process. LAMDAS reframes data selection as a one-class classification problem, identifying candidate data that "belongs" to the target domain defined by a small reference dataset. Extensive experimental results demonstrate that LAMDAS not only exceeds the performance of full-data training using a fraction of the data but also outperforms nine state-of-the-art (SOTA) baselines under various scenarios. Furthermore, LAMDAS achieves the most compelling balance between performance gains and computational efficiency compared to all evaluated baselines.</li>
</ul>

<h3>Title: Lane Change Intention Prediction of two distinct Populations using a Transformer</h3>
<ul>
<li><strong>Authors: </strong>Francesco De Cristofaro, Cornelia Lex, Jia Hu, Arno Eichberger</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06529">https://arxiv.org/abs/2509.06529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06529">https://arxiv.org/pdf/2509.06529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06529]] Lane Change Intention Prediction of two distinct Populations using a Transformer(https://arxiv.org/abs/2509.06529)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>As a result of the growing importance of lane change intention prediction for a safe and efficient driving experience in complex driving scenarios, researchers have in recent years started to train novel machine learning algorithms on available datasets with promising results. A shortcoming of this recent research effort, though, is that the vast majority of the proposed algorithms are trained on a single datasets. In doing so, researchers failed to test if their algorithm would be as effective if tested on a different dataset and, by extension, on a different population with respect to the one on which they were trained. In this article we test a transformer designed for lane change intention prediction on two datasets collected by LevelX in Germany and Hong Kong. We found that the transformer's accuracy plummeted when tested on a population different to the one it was trained on with accuracy values as low as 39.43%, but that when trained on both populations simultaneously it could achieve an accuracy as high as 86.71%. - This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.</li>
</ul>

<h3>Title: SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion</h3>
<ul>
<li><strong>Authors: </strong>Mengxue Yang, Chun Yang, Jiaqi Zhu, Jiafan Li, Jingqi Zhang, Yuyang Li, Ying Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06531">https://arxiv.org/abs/2509.06531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06531">https://arxiv.org/pdf/2509.06531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06531]] SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion(https://arxiv.org/abs/2509.06531)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Link prediction in knowledge graphs requires integrating structural information and semantic context to infer missing entities. While large language models offer strong generative reasoning capabilities, their limited exploitation of structural signals often results in structural sparsity and semantic ambiguity, especially under incomplete or zero-shot settings. To address these challenges, we propose SLiNT (Structure-aware Language model with Injection and coNtrastive Training), a modular framework that injects knowledge-graph-derived structural context into a frozen LLM backbone with lightweight LoRA-based adaptation for robust link prediction. Specifically, Structure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to enrich sparse entities and mitigate missing context; Dynamic Hard Contrastive Learning (DHCL) introduces fine-grained supervision by interpolating hard positives and negatives to resolve entity-level ambiguity; and Gradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware intervention while preserving the core LLM parameters. Experiments on WN18RR and FB15k-237 show that SLiNT achieves superior or competitive performance compared with both embedding-based and generation-based baselines, demonstrating the effectiveness of structure-aware representation learning for scalable knowledge graph completion.</li>
</ul>

<h3>Title: On the Reproducibility of "FairCLIP: Harnessing Fairness in Vision-Language Learning''</h3>
<ul>
<li><strong>Authors: </strong>Hua Chang Bakker, Stan Fris, Angela Madelon Bernardy, Stan Deutekom</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06535">https://arxiv.org/abs/2509.06535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06535">https://arxiv.org/pdf/2509.06535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06535]] On the Reproducibility of "FairCLIP: Harnessing Fairness in Vision-Language Learning''(https://arxiv.org/abs/2509.06535)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>We investigated the reproducibility of FairCLIP, proposed by Luo et al. (2024), for improving the group fairness of CLIP (Radford et al., 2021) by minimizing image-text similarity score disparities across sensitive groups using the Sinkhorn distance. The experimental setup of Luo et al. (2024) was reproduced to primarily investigate the research findings for FairCLIP. The model description by Luo et al. (2024) was found to differ from the original implementation. Therefore, a new implementation, A-FairCLIP, is introduced to examine specific design choices. Furthermore, FairCLIP+ is proposed to extend the FairCLIP objective to include multiple attributes. Additionally, the impact of the distance minimization on FairCLIP's fairness and performance was explored. In alignment with the original authors, CLIP was found to be biased towards certain demographics when applied to zero-shot glaucoma classification using medical scans and clinical notes from the Harvard-FairVLMed dataset. However, the experimental results on two datasets do not support their claim that FairCLIP improves the performance and fairness of CLIP. Although the regularization objective reduces Sinkhorn distances, both the official implementation and the aligned implementation, A-FairCLIP, were not found to improve performance nor fairness in zero-shot glaucoma classification.</li>
</ul>

<h3>Title: Learning Optimal Defender Strategies for CAGE-2 using a POMDP Model</h3>
<ul>
<li><strong>Authors: </strong>Duc Huy Le, Rolf Stadler</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06539">https://arxiv.org/abs/2509.06539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06539">https://arxiv.org/pdf/2509.06539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06539]] Learning Optimal Defender Strategies for CAGE-2 using a POMDP Model(https://arxiv.org/abs/2509.06539)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack</a></li>
<li><strong>Abstract: </strong>CAGE-2 is an accepted benchmark for learning and evaluating defender strategies against cyberattacks. It reflects a scenario where a defender agent protects an IT infrastructure against various attacks. Many defender methods for CAGE-2 have been proposed in the literature. In this paper, we construct a formal model for CAGE-2 using the framework of Partially Observable Markov Decision Process (POMDP). Based on this model, we define an optimal defender strategy for CAGE-2 and introduce a method to efficiently learn this strategy. Our method, called BF-PPO, is based on PPO, and it uses particle filter to mitigate the computational complexity due to the large state space of the CAGE-2 model. We evaluate our method in the CAGE-2 CybORG environment and compare its performance with that of CARDIFF, the highest ranked method on the CAGE-2 leaderboard. We find that our method outperforms CARDIFF regarding the learned defender strategy and the required training time.</li>
</ul>

<h3>Title: Predicting Fetal Outcomes from Cardiotocography Signals Using a Supervised Variational Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>John Tolladay, Beth Albert, Gabriel Davis Jones</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06540">https://arxiv.org/abs/2509.06540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06540">https://arxiv.org/pdf/2509.06540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06540]] Predicting Fetal Outcomes from Cardiotocography Signals Using a Supervised Variational Autoencoder(https://arxiv.org/abs/2509.06540)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Objective: To develop and interpret a supervised variational autoencoder (VAE) model for classifying cardiotocography (CTG) signals based on pregnancy outcomes, addressing interpretability limits of current deep learning approaches. Methods: The OxMat CTG dataset was used to train a VAE on five-minute fetal heart rate (FHR) segments, labeled with postnatal outcomes. The model was optimised for signal reconstruction and outcome prediction, incorporating Kullback-Leibler divergence and total correlation (TC) constraints to structure the latent space. Performance was evaluated using area under the receiver operating characteristic curve (AUROC) and mean squared error (MSE). Interpretability was assessed using coefficient of determination, latent traversals and unsupervised component analyses. Results: The model achieved an AUROC of 0.752 at the segment level and 0.779 at the CTG level, where predicted scores were aggregated. Relaxing TC constraints improved both reconstruction and classification. Latent analysis showed that baseline-related features (e.g., FHR baseline, baseline shift) were well represented and aligned with model scores, while metrics like short- and long-term variability were less strongly encoded. Traversals revealed clear signal changes for baseline features, while other properties were entangled or subtle. Unsupervised decompositions corroborated these patterns. Findings: This work demonstrates that supervised VAEs can achieve competitive fetal outcome prediction while partially encoding clinically meaningful CTG features. The irregular, multi-timescale nature of FHR signals poses challenges for disentangling physiological components, distinguishing CTG from more periodic signals such as ECG. Although full interpretability was not achieved, the model supports clinically useful outcome prediction and provides a basis for future interpretable, generative models.</li>
</ul>

<h3>Title: Signal-Based Malware Classification Using 1D CNNs</h3>
<ul>
<li><strong>Authors: </strong>Jack Wilkie, Hanan Hindy, Ivan Andonovic, Christos Tachtatzis, Robert Atkinson</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06548">https://arxiv.org/abs/2509.06548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06548">https://arxiv.org/pdf/2509.06548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06548]] Signal-Based Malware Classification Using 1D CNNs(https://arxiv.org/abs/2509.06548)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Malware classification is a contemporary and ongoing challenge in cyber-security: modern obfuscation techniques are able to evade traditional static analysis, while dynamic analysis is too resource intensive to be deployed at a large scale. One prominent line of research addresses these limitations by converting malware binaries into 2D images by heuristically reshaping them into a 2D grid before resizing using Lanczos resampling. These images can then be classified based on their textural information using computer vision approaches. While this approach can detect obfuscated malware more effectively than static analysis, the process of converting files into 2D images results in significant information loss due to both quantisation noise, caused by rounding to integer pixel values, and the introduction of 2D dependencies which do not exist in the original data. This loss of signal limits the classification performance of the downstream model. This work addresses these weaknesses by instead resizing the files into 1D signals which avoids the need for heuristic reshaping, and additionally these signals do not suffer from quantisation noise due to being stored in a floating-point format. It is shown that existing 2D CNN architectures can be readily adapted to classify these 1D signals for improved performance. Furthermore, a bespoke 1D convolutional neural network, based on the ResNet architecture and squeeze-and-excitation layers, was developed to classify these signals and evaluated on the MalNet dataset. It was found to achieve state-of-the-art performance on binary, type, and family level classification with F1 scores of 0.874, 0.503, and 0.507, respectively, paving the way for future models to operate on the proposed signal modality.</li>
</ul>

<h3>Title: Super-Quadratic Quantum Speed-ups and Guessing Many Likely Keys</h3>
<ul>
<li><strong>Authors: </strong>Timo Glaser, Alexander May, Julian Nowakowski</a></li>
<li><strong>Subjects: </strong>cs.CR, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06549">https://arxiv.org/abs/2509.06549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06549">https://arxiv.org/pdf/2509.06549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06549]] Super-Quadratic Quantum Speed-ups and Guessing Many Likely Keys(https://arxiv.org/abs/2509.06549)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>We study the fundamental problem of guessing cryptographic keys, drawn from some non-uniform probability distribution $D$, as e.g. in LPN, LWE or for passwords. The optimal classical algorithm enumerates keys in decreasing order of likelihood. The optimal quantum algorithm, due to Montanaro (2011), is a sophisticated Grover search. We give the first tight analysis for Montanaro's algorithm, showing that its runtime is $2^{H_{2/3}(D)/2}$, where $H_{\alpha}(\cdot)$ denotes Renyi entropy with parameter $\alpha$. Interestingly, this is a direct consequence of an information theoretic result called Arikan's Inequality (1996) -- which has so far been missed in the cryptographic community -- that tightly bounds the runtime of classical key guessing by $2^{H_{1/2}(D)}$. Since $H_{2/3}(D) < H_{1/2}(D)$ for every non-uniform distribution $D$, we thus obtain a super-quadratic quantum speed-up $s>2$ over classical key guessing. As another main result, we provide the first thorough analysis of guessing in a multi-key setting. Specifically, we consider the task of attacking many keys sampled independently from some distribution $D$, and aim to guess a fraction of them. For product distributions $D = \chi^n$, we show that any constant fraction of keys can be guessed within $2^{H(D)}$ classically and $2 ^{H(D)/2}$ quantumly per key, where $H(\chi)$ denotes Shannon entropy. In contrast, Arikan's Inequality implies that guessing a single key costs $2^{H_{1/2}(D)}$ classically and $2^{H_{2/3}(D)/2}$ quantumly. Since $H(D) < H_{2/3}(D) < H_{1/2}(D)$, this shows that in a multi-key setting the guessing cost per key is substantially smaller than in a single-key setting, both classically and quantumly.</li>
</ul>

<h3>Title: Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs</h3>
<ul>
<li><strong>Authors: </strong>Jack Wilkie, Hanan Hindy, Christos Tachtatzis, Robert Atkinson</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06550">https://arxiv.org/abs/2509.06550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06550">https://arxiv.org/pdf/2509.06550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06550]] Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs(https://arxiv.org/abs/2509.06550)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Network intrusion detection remains a critical challenge in cybersecurity. While supervised machine learning models achieve state-of-the-art performance, their reliance on large labelled datasets makes them impractical for many real-world applications. Anomaly detection methods, which train exclusively on benign traffic to identify malicious activity, suffer from high false positive rates, limiting their usability. Recently, self-supervised learning techniques have demonstrated improved performance with lower false positive rates by learning discriminative latent representations of benign traffic. In particular, contrastive self-supervised models achieve this by minimizing the distance between similar (positive) views of benign traffic while maximizing it between dissimilar (negative) views. Existing approaches generate positive views through data augmentation and treat other samples as negative. In contrast, this work introduces Contrastive Learning using Augmented Negative pairs (CLAN), a novel paradigm for network intrusion detection where augmented samples are treated as negative views - representing potentially malicious distributions - while other benign samples serve as positive views. This approach enhances both classification accuracy and inference efficiency after pretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset demonstrates that the proposed method surpasses existing self-supervised and anomaly detection techniques in a binary classification task. Furthermore, when fine-tuned on a limited labelled dataset, the proposed approach achieves superior multi-class classification performance compared to existing self-supervised models.</li>
</ul>

<h3>Title: Back To The Drawing Board: Rethinking Scene-Level Sketch-Based Image Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Emil Demić, Luka Čehovin Zajc</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06566">https://arxiv.org/abs/2509.06566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06566">https://arxiv.org/pdf/2509.06566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06566]] Back To The Drawing Board: Rethinking Scene-Level Sketch-Based Image Retrieval(https://arxiv.org/abs/2509.06566)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The goal of Scene-level Sketch-Based Image Retrieval is to retrieve natural images matching the overall semantics and spatial layout of a free-hand sketch. Unlike prior work focused on architectural augmentations of retrieval models, we emphasize the inherent ambiguity and noise present in real-world sketches. This insight motivates a training objective that is explicitly designed to be robust to sketch variability. We show that with an appropriate combination of pre-training, encoder architecture, and loss formulation, it is possible to achieve state-of-the-art performance without the introduction of additional complexity. Extensive experiments on a challenging FS-COCO and widely-used SketchyCOCO datasets confirm the effectiveness of our approach and underline the critical role of training design in cross-modal retrieval tasks, as well as the need to improve the evaluation scenarios of scene-level SBIR.</li>
</ul>

<h3>Title: A Simple Data Exfiltration Game</h3>
<ul>
<li><strong>Authors: </strong>Tristan Caulfield</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06571">https://arxiv.org/abs/2509.06571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06571">https://arxiv.org/pdf/2509.06571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06571]] A Simple Data Exfiltration Game(https://arxiv.org/abs/2509.06571)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Data exfiltration is a growing problem for business who face costs related to the loss of confidential data as well as potential extortion. This work presents a simple game theoretic model of network data exfiltration. In the model, the attacker chooses the exfiltration route and speed, and the defender selects monitoring thresholds to detect unusual activity. The attacker is rewarded for exfiltrating data, and the defender tries to minimize the costs of data loss and of responding to alerts.</li>
</ul>

<h3>Title: Mind Your Server: A Systematic Study of Parasitic Toolchain Attacks on the MCP Ecosystem</h3>
<ul>
<li><strong>Authors: </strong>Shuli Zhao, Qinsheng Hou, Zihan Zhan, Yanhao Wang, Yuchong Xie, Yu Guo, Libo Chen, Shenghong Li, Zhi Xue</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06572">https://arxiv.org/abs/2509.06572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06572">https://arxiv.org/pdf/2509.06572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06572]] Mind Your Server: A Systematic Study of Parasitic Toolchain Attacks on the MCP Ecosystem(https://arxiv.org/abs/2509.06572)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, defense, attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly integrated with external systems through the Model Context Protocol (MCP), which standardizes tool invocation and has rapidly become a backbone for LLM-powered applications. While this paradigm enhances functionality, it also introduces a fundamental security shift: LLMs transition from passive information processors to autonomous orchestrators of task-oriented toolchains, expanding the attack surface, elevating adversarial goals from manipulating single outputs to hijacking entire execution flows. In this paper, we reveal a new class of attacks, Parasitic Toolchain Attacks, instantiated as MCP Unintended Privacy Disclosure (MCP-UPD). These attacks require no direct victim interaction; instead, adversaries embed malicious instructions into external data sources that LLMs access during legitimate tasks. The malicious logic infiltrates the toolchain and unfolds in three phases: Parasitic Ingestion, Privacy Collection, and Privacy Disclosure, culminating in stealthy exfiltration of private data. Our root cause analysis reveals that MCP lacks both context-tool isolation and least-privilege enforcement, enabling adversarial instructions to propagate unchecked into sensitive tool invocations. To assess the severity, we design MCP-SEC and conduct the first large-scale security census of the MCP ecosystem, analyzing 12,230 tools across 1,360 servers. Our findings show that the MCP ecosystem is rife with exploitable gadgets and diverse attack methods, underscoring systemic risks in MCP platforms and the urgent need for defense mechanisms in LLM-integrated environments.</li>
</ul>

<h3>Title: CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Xin Kong, Daniel Watson, Yannick Strümpler, Michael Niemeyer, Federico Tombari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06579">https://arxiv.org/abs/2509.06579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06579">https://arxiv.org/pdf/2509.06579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06579]] CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis(https://arxiv.org/abs/2509.06579)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Multi-view diffusion models have shown promise in 3D novel view synthesis, but most existing methods adopt a non-autoregressive formulation. This limits their applicability in world modeling, as they only support a fixed number of views and suffer from slow inference due to denoising all frames simultaneously. To address these limitations, we propose CausNVS, a multi-view diffusion model in an autoregressive setting, which supports arbitrary input-output view configurations and generates views sequentially. We train CausNVS with causal masking and per-frame noise, using pairwise-relative camera pose encodings (CaPE) for precise camera control. At inference time, we combine a spatially-aware sliding-window with key-value caching and noise conditioning augmentation to mitigate drift. Our experiments demonstrate that CausNVS supports a broad range of camera trajectories, enables flexible autoregressive novel view synthesis, and achieves consistently strong visual quality across diverse settings. Project page: this https URL.</li>
</ul>

<h3>Title: LLMs in Cybersecurity: Friend or Foe in the Human Decision Loop?</h3>
<ul>
<li><strong>Authors: </strong>Irdin Pekaric, Philipp Zech, Tom Mattson</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06595">https://arxiv.org/abs/2509.06595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06595">https://arxiv.org/pdf/2509.06595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06595]] LLMs in Cybersecurity: Friend or Foe in the Human Decision Loop?(https://arxiv.org/abs/2509.06595)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are transforming human decision-making by acting as cognitive collaborators. Yet, this promise comes with a paradox: while LLMs can improve accuracy, they may also erode independent reasoning, promote over-reliance and homogenize decisions. In this paper, we investigate how LLMs shape human judgment in security-critical contexts. Through two exploratory focus groups (unaided and LLM-supported), we assess decision accuracy, behavioral resilience and reliance dynamics. Our findings reveal that while LLMs enhance accuracy and consistency in routine decisions, they can inadvertently reduce cognitive diversity and improve automation bias, which is especially the case among users with lower resilience. In contrast, high-resilience individuals leverage LLMs more effectively, suggesting that cognitive traits mediate AI benefit.</li>
</ul>

<h3>Title: HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xin Tong, Zhi Lin, Jingya Wang, Bo Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06596">https://arxiv.org/abs/2509.06596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06596">https://arxiv.org/pdf/2509.06596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06596]] HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models(https://arxiv.org/abs/2509.06596)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often produce hallucinations in retrieval-augmented or long-context generation, even when relevant evidence is present. This stems from two issues: head importance is treated as input-agnostic, and raw attention weights poorly reflect each token's true contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a parameter-free decoding framework that directly addresses both challenges. HAVE introduces head-adaptive gating, which performs instance-level soft reweighing of attention heads, and value calibration, which augments attention with the magnitude of value vectors to approximate write-back contribution. Together, these modules construct token-level evidence aligned with model updates and fuse it with the LM distribution through a lightweight uncertainty-scaled policy. HAVE requires no finetuning and operates in a single forward pass, making it efficient and broadly applicable. Experiments across multiple QA benchmarks and LLM families demonstrate that HAVE consistently reduces hallucinations and outperforms strong baselines, including DAGCD, with modest overhead. The framework is transparent, reproducible, and readily integrates with off-the-shelf LLMs, advancing trustworthy generation in real-world settings.</li>
</ul>

<h3>Title: Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in Molecular Tumor Boards</h3>
<ul>
<li><strong>Authors: </strong>Noel Codella, Sam Preston, Hao Qiu, Leonardo Schettini, Wen-wai Yim, Mert Öz, Shrey Jain, Matthew P. Lungren, Thomas Osborne</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06602">https://arxiv.org/abs/2509.06602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06602">https://arxiv.org/pdf/2509.06602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06602]] Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in Molecular Tumor Boards(https://arxiv.org/abs/2509.06602)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, data-free, large language model</a></li>
<li><strong>Abstract: </strong>Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology specialists collaboratively assess complex patient cases to determine optimal treatment strategies. A central element of this process is the patient summary, typically compiled by a medical oncologist, radiation oncologist, or surgeon, or their trained medical assistant, who distills heterogeneous medical records into a concise narrative to facilitate discussion. This manual approach is often labor-intensive, subjective, and prone to omissions of critical information. To address these limitations, we introduce the Healthcare Agent Orchestrator (HAO), a Large Language Model (LLM)-driven AI agent that coordinates a multi-agent clinical workflow to generate accurate and comprehensive patient summaries for MTBs. Evaluating predicted patient summaries against ground truth presents additional challenges due to stylistic variation, ordering, synonym usage, and phrasing differences, which complicate the measurement of both succinctness and completeness. To overcome these evaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework designed to assess the comprehensiveness and succinctness of generated summaries. Using a benchmark dataset derived from de-identified tumor board discussions, we applied TBFact to evaluate our Patient History agent. Results show that the agent captured 94% of high-importance information (including partial entailments) and achieved a TBFact recall of 0.84 under strict entailment criteria. We further demonstrate that TBFact enables a data-free evaluation framework that institutions can deploy locally without sharing sensitive clinical data. Together, HAO and TBFact establish a robust foundation for delivering reliable and scalable support to MTBs.</li>
</ul>

<h3>Title: Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors</h3>
<ul>
<li><strong>Authors: </strong>Viacheslav Sinii, Nikita Balagansky, Yaroslav Aksenov, Vadim Kurochkin, Daniil Laptev, Gleb Gerasimov, Alexey Gorbatovski, Boris Shaposhnikov, Daniil Gavrilov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06608">https://arxiv.org/abs/2509.06608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06608">https://arxiv.org/pdf/2509.06608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06608]] Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors(https://arxiv.org/abs/2509.06608)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The mechanisms by which reasoning training reshapes language-model computations remain poorly understood. We study lightweight steering vectors inserted into the base model's residual stream and trained with a reinforcement-learning objective, which can match full fine-tuning performance while retaining the interpretability of small, additive interventions. Using logit-lens readouts, path patching, and circuit analyses, we analyze two models and find: (i) the last-layer steering vector behaves like a token-substitution bias concentrated on the first generated token, consistently boosting tokens such as "To" and "Step"; and (ii) the penultimate-layer steering vector leaves attention patterns largely unchanged and instead acts through the MLP and unembedding, preferentially up-weighting process words and structure symbols. These results establish a principled framework for interpreting the behavioral changes induced by reasoning training.</li>
</ul>

<h3>Title: A Secure Sequencer and Data Availability Committee for Rollups (Extended Version)</h3>
<ul>
<li><strong>Authors: </strong>Margarita Capretto, Martín Ceresa, Antonio Fernández Anta, Pedro Moreno Sánchez, César Sánchez</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06614">https://arxiv.org/abs/2509.06614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06614">https://arxiv.org/pdf/2509.06614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06614]] A Secure Sequencer and Data Availability Committee for Rollups (Extended Version)(https://arxiv.org/abs/2509.06614)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Blockchains face a scalability limitation, partly due to the throughput limitations of consensus protocols, especially when aiming to obtain a high degree of decentralization. Layer 2 Rollups (L2s) are a faster alternative to conventional blockchains. L2s perform most computations offchain using minimally blockchains (L1) under-the-hood to guarantee correctness. A sequencer is a service that receives offchain L2 transaction requests, batches these transactions, and commits compressed or hashed batches to L1. Using hashing needs less L1 space, which is beneficial for gas cost, but requires a data availability committee (DAC) service to translate hashes into their corresponding batches of transaction requests. The behavior of sequencers and DACs influence the evolution of the L2 blockchain, presenting a potential security threat and delaying L2 adoption. We propose in this paper fraud-proof mechanisms, arbitrated by L1 contracts, to detect and generate evidence of dishonest behavior of the sequencer and DAC. We study how these fraud-proofs limit the power of adversaries that control different number of sequencer and DACs members, and provide incentives for their honest behavior. We designed these fraud-proof mechanisms as two player games. Unlike the generic fraud-proofs in current L2s (designed to guarantee the correct execution of transactions), our fraud-proofs are over pred-etermined algorithms that verify the properties that determine the correctness of the DAC. Arbitrating over concrete algorithms makes our fraud-proofs more efficient, easier to understand, and simpler to prove correct. We provide as an artifact a mechanization in LEAN4 of our fraud-proof games, including (1) the verified strategies that honest players should play to win all games as well as (2) mechanisms to detect dishonest claims.</li>
</ul>

<h3>Title: BEAM: Brainwave Empathy Assessment Model for Early Childhood</h3>
<ul>
<li><strong>Authors: </strong>Chen Xie, Gaofeng Wu, Kaidong Wang, Zihao Zhu, Xiaoshu Luo, Yan Liang, Feiyu Quan, Ruoxi Wu, Xianghui Huang, Han Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06620">https://arxiv.org/abs/2509.06620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06620">https://arxiv.org/pdf/2509.06620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06620]] BEAM: Brainwave Empathy Assessment Model for Early Childhood(https://arxiv.org/abs/2509.06620)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Empathy in young children is crucial for their social and emotional development, yet predicting it remains challenging. Traditional methods often only rely on self-reports or observer-based labeling, which are susceptible to bias and fail to objectively capture the process of empathy formation. EEG offers an objective alternative; however, current approaches primarily extract static patterns, neglecting temporal dynamics. To overcome these limitations, we propose a novel deep learning framework, the Brainwave Empathy Assessment Model (BEAM), to predict empathy levels in children aged 4-6 years. BEAM leverages multi-view EEG signals to capture both cognitive and emotional dimensions of empathy. The framework comprises three key components: 1) a LaBraM-based encoder for effective spatio-temporal feature extraction, 2) a feature fusion module to integrate complementary information from multi-view signals, and 3) a contrastive learning module to enhance class separation. Validated on the CBCP dataset, BEAM outperforms state-of-the-art methods across multiple metrics, demonstrating its potential for objective empathy assessment and providing a preliminary insight into early interventions in children's prosocial development.</li>
</ul>

<h3>Title: Improved Classification of Nitrogen Stress Severity in Plants Under Combined Stress Conditions Using Spatio-Temporal Deep Learning Framework</h3>
<ul>
<li><strong>Authors: </strong>Aswini Kumar Patra</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06625">https://arxiv.org/abs/2509.06625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06625">https://arxiv.org/pdf/2509.06625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06625]] Improved Classification of Nitrogen Stress Severity in Plants Under Combined Stress Conditions Using Spatio-Temporal Deep Learning Framework(https://arxiv.org/abs/2509.06625)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust</a></li>
<li><strong>Abstract: </strong>Plants in their natural habitats endure an array of interacting stresses, both biotic and abiotic, that rarely occur in isolation. Nutrient stress-particularly nitrogen deficiency-becomes even more critical when compounded with drought and weed competition, making it increasingly difficult to distinguish and address its effects. Early detection of nitrogen stress is therefore crucial for protecting plant health and implementing effective management strategies. This study proposes a novel deep learning framework to accurately classify nitrogen stress severity in a combined stress environment. Our model uses a unique blend of four imaging modalities-RGB, multispectral, and two infrared wavelengths-to capture a wide range of physiological plant responses from canopy images. These images, provided as time-series data, document plant health across three levels of nitrogen availability (low, medium, and high) under varying water stress and weed pressures. The core of our approach is a spatio-temporal deep learning pipeline that merges a Convolutional Neural Network (CNN) for extracting spatial features from images with a Long Short-Term Memory (LSTM) network to capture temporal dependencies. We also devised and evaluated a spatial-only CNN pipeline for comparison. Our CNN-LSTM pipeline achieved an impressive accuracy of 98%, impressively surpassing the spatial-only model's 80.45% and other previously reported machine learning method's 76%. These results bring actionable insights based on the power of our CNN-LSTM approach in effectively capturing the subtle and complex interactions between nitrogen deficiency, water stress, and weed pressure. This robust platform offers a promising tool for the timely and proactive identification of nitrogen stress severity, enabling better crop management and improved plant health.</li>
</ul>

<h3>Title: Network-level Censorship Attacks in the InterPlanetary File System</h3>
<ul>
<li><strong>Authors: </strong>Jan Matter, Muoi Tran</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06626">https://arxiv.org/abs/2509.06626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06626">https://arxiv.org/pdf/2509.06626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06626]] Network-level Censorship Attacks in the InterPlanetary File System(https://arxiv.org/abs/2509.06626)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>The InterPlanetary File System (IPFS) has been successfully established as the de facto standard for decentralized data storage in the emerging Web3. Despite its decentralized nature, IPFS nodes, as well as IPFS content providers, have converged to centralization in large public clouds. Centralization introduces BGP routing-based attacks, such as passive interception and BGP hijacking, as potential threats. Although this attack vector has been investigated for many other Web3 protocols, such as Bitcoin and Ethereum, to the best of our knowledge, it has not been analyzed for the IPFS network. In our work, we bridge this gap and demonstrate that BGP routing attacks can be effectively leveraged to censor content in IPFS. For the analysis, we collected 3,000 content blocks called CIDs and conducted a simulation of BGP hijacking and passive interception against them. We find that a single malicious AS can censor 75% of the IPFS content for more than 57% of all requester nodes. Furthermore, we show that even with a small set of only 62 hijacked prefixes, 70% of the full attack effectiveness can already be reached. We further propose and validate countermeasures based on global collaborative content replication among all nodes in the IPFS network, together with additional robust backup content provider nodes that are well-hardened against BGP hijacking. We hope this work raises awareness about the threat BGP routing-based attacks pose to IPFS and triggers further efforts to harden the live IPFS network against them.</li>
</ul>

<h3>Title: Guided Decoding and Its Critical Role in Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Özgür Uğur, Musa Yılmaz, Esra Şavirdi, Özay Ezerceli, Mahmut El Huseyni, Selva Taş, Reyhan Bayraktar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06631">https://arxiv.org/abs/2509.06631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06631">https://arxiv.org/pdf/2509.06631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06631]] Guided Decoding and Its Critical Role in Retrieval-Augmented Generation(https://arxiv.org/abs/2509.06631)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The integration of Large Language Models (LLMs) into various applications has driven the need for structured and reliable responses. A key challenge in Retrieval-Augmented Generation (RAG) systems is ensuring that outputs align with expected formats while minimizing hallucinations. This study examines the role of guided decoding in RAG systems, comparing three methods, Outlines, XGrammar, and LM Format Enforcer, across different multi-turn prompting setups (0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates, and output quality, we provide insights into their performance and applicability. Our findings reveal how multi-turn interactions influence guided decoding, uncovering unexpected performance variations that can inform method selection for specific use cases. This work advances the understanding of structured output generation in RAG systems, offering both theoretical insights and practical guidance for LLM deployment.</li>
</ul>

<h3>Title: Knowledge-Guided Machine Learning for Stabilizing Near-Shortest Path Routing</h3>
<ul>
<li><strong>Authors: </strong>Yung-Fu Chen, Sen Lin, Anish Arora</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06640">https://arxiv.org/abs/2509.06640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06640">https://arxiv.org/pdf/2509.06640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06640]] Knowledge-Guided Machine Learning for Stabilizing Near-Shortest Path Routing(https://arxiv.org/abs/2509.06640)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>We propose a simple algorithm that needs only a few data samples from a single graph for learning local routing policies that generalize across a rich class of geometric random graphs in Euclidean metric spaces. We thus solve the all-pairs near-shortest path problem by training deep neural networks (DNNs) that let each graph node efficiently and scalably route (i.e., forward) packets by considering only the node's state and the state of the neighboring nodes. Our algorithm design exploits network domain knowledge in the selection of input features and design of the policy function for learning an approximately optimal policy. Domain knowledge also provides theoretical assurance that the choice of a ``seed graph'' and its node data sampling suffices for generalizable learning. Remarkably, one of these DNNs we train -- using distance-to-destination as the only input feature -- learns a policy that exactly matches the well-known Greedy Forwarding policy, which forwards packets to the neighbor with the shortest distance to the destination. We also learn a new policy, which we call GreedyTensile routing -- using both distance-to-destination and node stretch as the input features -- that almost always outperforms greedy forwarding. We demonstrate the explainability and ultra-low latency run-time operation of Greedy Tensile routing by symbolically interpreting its DNN in low-complexity terms of two linear actions.</li>
</ul>

<h3>Title: Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Hao Lin, Peitong Xie, Jingxue Chen, Jie Lin, Qingkun Tang, Qianchun Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06650">https://arxiv.org/abs/2509.06650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06650">https://arxiv.org/pdf/2509.06650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06650]] Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval(https://arxiv.org/abs/2509.06650)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval stage, particularly the coarse-ranking process. Existing coarse-ranking optimization approaches often struggle to balance domain-specific knowledge learning with query enhencement, resulting in suboptimal retrieval performance. To address this challenge, we propose MoLER, a domain-aware RAG method that uses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a two-stage pipeline: a continual pre-training (CPT) phase using a Mixture of Losses (MoL) to balance domain-specific knowledge with general language capabilities, and a reinforcement learning (RL) phase leveraging Group Relative Policy Optimization (GRPO) to optimize query and passage generation for maximizing document recall. A key innovation is our Multi-query Single-passage Late Fusion (MSLF) strategy, which reduces computational overhead during RL training while maintaining scalable inference via Multi-query Multi-passage Late Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER achieves state-of-the-art performance, significantly outperforming baseline methods. MoLER bridges the knowledge gap in RAG systems, enabling robust and scalable retrieval in specialized domains.</li>
</ul>

<h3>Title: IntrEx: A Dataset for Modeling Engagement in Educational Conversations</h3>
<ul>
<li><strong>Authors: </strong>Xingwei Tan, Mahathi Parvatham, Chiara Gambi, Gabriele Pergola</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06652">https://arxiv.org/abs/2509.06652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06652">https://arxiv.org/pdf/2509.06652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06652]] IntrEx: A Dataset for Modeling Engagement in Educational Conversations(https://arxiv.org/abs/2509.06652)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Engagement and motivation are crucial for second-language acquisition, yet maintaining learner interest in educational conversations remains a challenge. While prior research has explored what makes educational texts interesting, still little is known about the linguistic features that drive engagement in conversations. To address this gap, we introduce IntrEx, the first large dataset annotated for interestingness and expected interestingness in teacher-student interactions. Built upon the Teacher-Student Chatroom Corpus (TSCC), IntrEx extends prior work by incorporating sequence-level annotations, allowing for the study of engagement beyond isolated turns to capture how interest evolves over extended dialogues. We employ a rigorous annotation process with over 100 second-language learners, using a comparison-based rating approach inspired by reinforcement learning from human feedback (RLHF) to improve agreement. We investigate whether large language models (LLMs) can predict human interestingness judgments. We find that LLMs (7B/8B parameters) fine-tuned on interestingness ratings outperform larger proprietary models like GPT-4o, demonstrating the potential for specialised datasets to model engagement in educational settings. Finally, we analyze how linguistic and cognitive factors, such as concreteness, comprehensibility (readability), and uptake, influence engagement in educational dialogues.</li>
</ul>

<h3>Title: Group Effect Enhanced Generative Adversarial Imitation Learning for Individual Travel Behavior Modeling under Incentives</h3>
<ul>
<li><strong>Authors: </strong>Yuanyuan Wu, Zhenlin Qin, Leizhen Wang, Xiaolei Ma, Zhenliang Ma</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06656">https://arxiv.org/abs/2509.06656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06656">https://arxiv.org/pdf/2509.06656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06656]] Group Effect Enhanced Generative Adversarial Imitation Learning for Individual Travel Behavior Modeling under Incentives(https://arxiv.org/abs/2509.06656)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Understanding and modeling individual travel behavior responses is crucial for urban mobility regulation and policy evaluation. The Markov decision process (MDP) provides a structured framework for dynamic travel behavior modeling at the individual level. However, solving an MDP in this context is highly data-intensive and faces challenges of data quantity, spatial-temporal coverage, and situational diversity. To address these, we propose a group-effect-enhanced generative adversarial imitation learning (gcGAIL) model that improves the individual behavior modeling efficiency by leveraging shared behavioral patterns among passenger groups. We validate the gcGAIL model using a public transport fare-discount case study and compare against state-of-the-art benchmarks, including adversarial inverse reinforcement learning (AIRL), baseline GAIL, and conditional GAIL. Experimental results demonstrate that gcGAIL outperforms these methods in learning individual travel behavior responses to incentives over time in terms of accuracy, generalization, and pattern demonstration efficiency. Notably, gcGAIL is robust to spatial variation, data sparsity, and behavioral diversity, maintaining strong performance even with partial expert demonstrations and underrepresented passenger groups. The gcGAIL model predicts the individual behavior response at any time, providing the basis for personalized incentives to induce sustainable behavior changes (better timing of incentive injections).</li>
</ul>

<h3>Title: Investigating Location-Regularised Self-Supervised Feature Learning for Seafloor Visual Imagery</h3>
<ul>
<li><strong>Authors: </strong>Cailei Liang, Adrian Bodenmann, Emma J Curtis, Samuel Simmons, Kazunori Nagano, Stan Brown, Adam Riese, Blair Thornton</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06660">https://arxiv.org/abs/2509.06660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06660">https://arxiv.org/pdf/2509.06660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06660]] Investigating Location-Regularised Self-Supervised Feature Learning for Seafloor Visual Imagery(https://arxiv.org/abs/2509.06660)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>High-throughput interpretation of robotically gathered seafloor visual imagery can increase the efficiency of marine monitoring and exploration. Although recent research has suggested that location metadata can enhance self-supervised feature learning (SSL), its benefits across different SSL strategies, models and seafloor image datasets are underexplored. This study evaluates the impact of location-based regularisation on six state-of-the-art SSL frameworks, which include Convolutional Neural Network (CNN) and Vision Transformer (ViT) models with varying latent-space dimensionality. Evaluation across three diverse seafloor image datasets finds that location-regularisation consistently improves downstream classification performance over standard SSL, with average F1-score gains of $4.9 \pm 4.0%$ for CNNs and $6.3 \pm 8.9%$ for ViTs, respectively. While CNNs pretrained on generic datasets benefit from high-dimensional latent representations, dataset-optimised SSL achieves similar performance across the high (512) and low (128) dimensional latent representations. Location-regularised SSL improves CNN performance over pre-trained models by $2.7 \pm 2.7%$ and $10.1 \pm 9.4%$ for high and low-dimensional latent representations, respectively. For ViTs, high-dimensionality benefits both pre-trained and dataset-optimised SSL. Although location-regularisation improves SSL performance compared to standard SSL methods, pre-trained ViTs show strong generalisation, matching the best-performing location-regularised SSL with F1-scores of $0.795 \pm 0.075$ and $0.795 \pm 0.077$, respectively. The findings highlight the value of location metadata for SSL regularisation, particularly when using low-dimensional latent representations, and demonstrate strong generalisation of high-dimensional ViTs for seafloor image analysis.</li>
</ul>

<h3>Title: Online Clustering of Seafloor Imagery for Interpretation during Long-Term AUV Operations</h3>
<ul>
<li><strong>Authors: </strong>Cailei Liang, Adrian Bodenmann, Sam Fenton, Blair Thornton</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06678">https://arxiv.org/abs/2509.06678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06678">https://arxiv.org/pdf/2509.06678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06678]] Online Clustering of Seafloor Imagery for Interpretation during Long-Term AUV Operations(https://arxiv.org/abs/2509.06678)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As long-endurance and seafloor-resident AUVs become more capable, there is an increasing need for extended, real-time interpretation of seafloor imagery to enable adaptive missions and optimise communication efficiency. Although offline image analysis methods are well established, they rely on access to complete datasets and human-labelled examples to manage the strong influence of environmental and operational conditions on seafloor image appearance-requirements that cannot be met in real-time settings. To address this, we introduce an online clustering framework (OCF) capable of interpreting seafloor imagery without supervision, which is designed to operate in real-time on continuous data streams in a scalable, adaptive, and self-consistent manner. The method enables the efficient review and consolidation of common patterns across the entire data history in constant time by identifying and maintaining a set of representative samples that capture the evolving feature distribution, supporting dynamic cluster merging and splitting without reprocessing the full image history. We evaluate the framework on three diverse seafloor image datasets, analysing the impact of different representative sampling strategies on both clustering accuracy and computational cost. The OCF achieves the highest average F1 score of 0.68 across the three datasets among all comparative online clustering approaches, with a standard deviation of 3% across three distinct survey trajectories, demonstrating its superior clustering capability and robustness to trajectory variation. In addition, it maintains consistently lower and bounded computational time as the data volume increases. These properties are beneficial for generating survey data summaries and supporting informative path planning in long-term, persistent autonomous marine exploration.</li>
</ul>

<h3>Title: BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Usman Haider, Lukasz Szemet, Daniel Kelly, Vasileios Sergis, Andrew C. Daly, Karl Mason</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06690">https://arxiv.org/abs/2509.06690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06690">https://arxiv.org/pdf/2509.06690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06690]] BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring(https://arxiv.org/abs/2509.06690)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Bioprinting is a rapidly advancing field that offers a transformative approach to fabricating tissue and organ models through the precise deposition of cell-laden bioinks. Ensuring the fidelity and consistency of printed structures in real-time remains a core challenge, particularly under constraints imposed by limited imaging data and resource-constrained embedded hardware. Semantic segmentation of the extrusion process, differentiating between nozzle, extruded bioink, and surrounding background, enables in situ monitoring critical to maintaining print quality and biological viability. In this work, we introduce a lightweight semantic segmentation framework tailored for real-time bioprinting applications. We present a novel, manually annotated dataset comprising 787 RGB images captured during the bioprinting process, labeled across three classes: nozzle, bioink, and background. To achieve fast and efficient inference suitable for integration with bioprinting systems, we propose a BioLite U-Net architecture that leverages depthwise separable convolutions to drastically reduce computational load without compromising accuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based segmentation baselines using mean Intersection over Union (mIoU), Dice score, and pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess real-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85% and a Dice score of 96.17%, while being over 1300x smaller than MobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame, demonstrating near real-time capability. Compared to MobileNet baselines, BioLite U-Net offers a superior tradeoff between segmentation accuracy, efficiency, and deployability, making it highly suitable for intelligent, closed-loop bioprinting systems.</li>
</ul>

<h3>Title: STAGE: Segmentation-oriented Industrial Anomaly Synthesis via Graded Diffusion with Explicit Mask Alignment</h3>
<ul>
<li><strong>Authors: </strong>Xichen Xu, Yanshu Wang, Jinbao Wang, Qunyi Zhang, Xiaoning Lei, Guoyang Xie, Guannan Jiang, Zhichao Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06693">https://arxiv.org/abs/2509.06693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06693">https://arxiv.org/pdf/2509.06693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06693]] STAGE: Segmentation-oriented Industrial Anomaly Synthesis via Graded Diffusion with Explicit Mask Alignment(https://arxiv.org/abs/2509.06693)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Segmentation-oriented Industrial Anomaly Synthesis (SIAS) plays a pivotal role in enhancing the performance of downstream anomaly segmentation, as it provides an effective means of expanding abnormal data. However, existing SIAS methods face several critical limitations: (i) the synthesized anomalies often lack intricate texture details and fail to align precisely with the surrounding background, and (ii) they struggle to generate fine-grained, pixel-level anomalies. To address these challenges, we propose Segmentation-oriented Anomaly synthesis via Graded diffusion with Explicit mask alignment, termed STAGE. STAGE introduces a novel anomaly inference strategy that incorporates clean background information as a prior to guide the denoising distribution, enabling the model to more effectively distinguish and highlight abnormal foregrounds. Furthermore, it employs a graded diffusion framework with an anomaly-only branch to explicitly record local anomalies during both the forward and reverse processes, ensuring that subtle anomalies are not overlooked. Finally, STAGE incorporates the explicit mask alignment (EMA) strategy to progressively align the synthesized anomalies with the background, resulting in context-consistent and structurally coherent generations. Extensive experiments on the MVTec and BTAD datasets demonstrate that STAGE achieves state-of-the-art performance in SIAS, which in turn enhances downstream anomaly segmentation.</li>
</ul>

<h3>Title: Nested Optimal Transport Distances</h3>
<ul>
<li><strong>Authors: </strong>Ruben Bontorno, Songyan Hou</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.CP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06702">https://arxiv.org/abs/2509.06702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06702">https://arxiv.org/pdf/2509.06702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06702]] Nested Optimal Transport Distances(https://arxiv.org/abs/2509.06702)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Simulating realistic financial time series is essential for stress testing, scenario generation, and decision-making under uncertainty. Despite advances in deep generative models, there is no consensus metric for their evaluation. We focus on generative AI for financial time series in decision-making applications and employ the nested optimal transport distance, a time-causal variant of optimal transport distance, which is robust to tasks such as hedging, optimal stopping, and reinforcement learning. Moreover, we propose a statistically consistent, naturally parallelizable algorithm for its computation, achieving substantial speedups over existing approaches.</li>
</ul>

<h3>Title: When Secure Isn't: Assessing the Security of Machine Learning Model Sharing</h3>
<ul>
<li><strong>Authors: </strong>Gabriele Digregorio, Marco Di Gennaro, Stefano Zanero, Stefano Longari, Michele Carminati</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06703">https://arxiv.org/abs/2509.06703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06703">https://arxiv.org/pdf/2509.06703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06703]] When Secure Isn't: Assessing the Security of Machine Learning Model Sharing(https://arxiv.org/abs/2509.06703)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect</a></li>
<li><strong>Abstract: </strong>The rise of model-sharing through frameworks and dedicated hubs makes Machine Learning significantly more accessible. Despite their benefits, these tools expose users to underexplored security risks, while security awareness remains limited among both practitioners and developers. To enable a more security-conscious culture in Machine Learning model sharing, in this paper we evaluate the security posture of frameworks and hubs, assess whether security-oriented mechanisms offer real protection, and survey how users perceive the security narratives surrounding model sharing. Our evaluation shows that most frameworks and hubs address security risks partially at best, often by shifting responsibility to the user. More concerningly, our analysis of frameworks advertising security-oriented settings and complete model sharing uncovered six 0-day vulnerabilities enabling arbitrary code execution. Through this analysis, we debunk the misconceptions that the model-sharing problem is largely solved and that its security can be guaranteed by the file format used for sharing. As expected, our survey shows that the surrounding security narrative leads users to consider security-oriented settings as trustworthy, despite the weaknesses shown in this work. From this, we derive takeaways and suggestions to strengthen the security of model-sharing ecosystems.</li>
</ul>

<h3>Title: MRI-Based Brain Tumor Detection through an Explainable EfficientNetV2 and MLP-Mixer-Attention Architecture</h3>
<ul>
<li><strong>Authors: </strong>Mustafa Yurdakul, Şakir Taşdemir</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06713">https://arxiv.org/abs/2509.06713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06713">https://arxiv.org/pdf/2509.06713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06713]] MRI-Based Brain Tumor Detection through an Explainable EfficientNetV2 and MLP-Mixer-Attention Architecture(https://arxiv.org/abs/2509.06713)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Brain tumors are serious health problems that require early diagnosis due to their high mortality rates. Diagnosing tumors by examining Magnetic Resonance Imaging (MRI) images is a process that requires expertise and is prone to error. Therefore, the need for automated diagnosis systems is increasing day by day. In this context, a robust and explainable Deep Learning (DL) model for the classification of brain tumors is proposed. In this study, a publicly available Figshare dataset containing 3,064 T1-weighted contrast-enhanced brain MRI images of three tumor types was used. First, the classification performance of nine well-known CNN architectures was evaluated to determine the most effective backbone. Among these, EfficientNetV2 demonstrated the best performance and was selected as the backbone for further development. Subsequently, an attention-based MLP-Mixer architecture was integrated into EfficientNetV2 to enhance its classification capability. The performance of the final model was comprehensively compared with basic CNNs and the methods in the literature. Additionally, Grad-CAM visualization was used to interpret and validate the decision-making process of the proposed model. The proposed model's performance was evaluated using the five-fold cross-validation method. The proposed model demonstrated superior performance with 99.50% accuracy, 99.47% precision, 99.52% recall and 99.49% F1 score. The results obtained show that the model outperforms the studies in the literature. Moreover, Grad-CAM visualizations demonstrate that the model effectively focuses on relevant regions of MRI images, thus improving interpretability and clinical reliability. A robust deep learning model for clinical decision support systems has been obtained by combining EfficientNetV2 and attention-based MLP-Mixer, providing high accuracy and interpretability in brain tumor classification.</li>
</ul>

<h3>Title: Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via Test-Time Training</h3>
<ul>
<li><strong>Authors: </strong>Ruicheng Zhang, Jun Zhou, Zunnan Xu, Zihao Liu, Jiehui Huang, Mingyang Zhang, Yu Sun, Xiu Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06723">https://arxiv.org/abs/2509.06723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06723">https://arxiv.org/pdf/2509.06723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06723]] Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via Test-Time Training(https://arxiv.org/abs/2509.06723)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos that adhere to user-specified motion instructions. Existing methods typically rely on computationally expensive fine-tuning on scarce annotated datasets. Although some zero-shot methods attempt to trajectory control in the latent space, they may yield unrealistic motion by neglecting 3D perspective and creating a misalignment between the manipulated latents and the network's noise predictions. To address these challenges, we introduce Zo3T, a novel zero-shot test-time-training framework for trajectory-guided generation with three core innovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging inferring scene depth to derive perspective-correct affine transformations for target regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a mechanism that dynamically injects and optimizes ephemeral LoRA adapters into the denoising network alongside the latent state. Driven by a regional feature consistency loss, this co-adaptation effectively enforces motion constraints while allowing the pre-trained model to locally adapt its internal representations to the manipulated latent, thereby ensuring generative fidelity and on-manifold adherence. Finally, we develop Guidance Field Rectification, which refines the denoising evolutionary path by optimizing the conditional guidance field through a one-step lookahead strategy, ensuring efficient generative progression towards the target trajectory. Zo3T significantly enhances 3D realism and motion accuracy in trajectory-controlled I2V generation, demonstrating superior performance over existing training-based and zero-shot approaches.</li>
</ul>

<h3>Title: Co-Seg: Mutual Prompt-Guided Collaborative Learning for Tissue and Nuclei Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Qing Xu, Wenting Duan, Zhen Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06740">https://arxiv.org/abs/2509.06740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06740">https://arxiv.org/pdf/2509.06740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06740]] Co-Seg: Mutual Prompt-Guided Collaborative Learning for Tissue and Nuclei Segmentation(https://arxiv.org/abs/2509.06740)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Histopathology image analysis is critical yet challenged by the demand of segmenting tissue regions and nuclei instances for tumor microenvironment and cellular morphology analysis. Existing studies focused on tissue semantic segmentation or nuclei instance segmentation separately, but ignored the inherent relationship between these two tasks, resulting in insufficient histopathology understanding. To address this issue, we propose a Co-Seg framework for collaborative tissue and nuclei segmentation. Specifically, we introduce a novel co-segmentation paradigm, allowing tissue and nuclei segmentation tasks to mutually enhance each other. To this end, we first devise a region-aware prompt encoder (RP-Encoder) to provide high-quality semantic and instance region prompts as prior constraints. Moreover, we design a mutual prompt mask decoder (MP-Decoder) that leverages cross-guidance to strengthen the contextual consistency of both tasks, collaboratively computing semantic and instance segmentation masks. Extensive experiments on the PUMA dataset demonstrate that the proposed Co-Seg surpasses state-of-the-arts in the semantic, instance and panoptic segmentation of tumor tissues and nuclei instances. The source code is available at this https URL.</li>
</ul>

<h3>Title: Event Spectroscopy: Event-based Multispectral and Depth Sensing using Structured Light</h3>
<ul>
<li><strong>Authors: </strong>Christian Geckeler, Niklas Neugebauer, Manasi Muglikar, Davide Scaramuzza, Stefano Mintchev</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06741">https://arxiv.org/abs/2509.06741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06741">https://arxiv.org/pdf/2509.06741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06741]] Event Spectroscopy: Event-based Multispectral and Depth Sensing using Structured Light(https://arxiv.org/abs/2509.06741)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Uncrewed aerial vehicles (UAVs) are increasingly deployed in forest environments for tasks such as environmental monitoring and search and rescue, which require safe navigation through dense foliage and precise data collection. Traditional sensing approaches, including passive multispectral and RGB imaging, suffer from latency, poor depth resolution, and strong dependence on ambient light - especially under forest canopies. In this work, we present a novel event spectroscopy system that simultaneously enables high-resolution, low-latency depth reconstruction and multispectral imaging using a single sensor. Depth is reconstructed using structured light, and by modulating the wavelength of the projected structured light, our system captures spectral information in controlled bands between 650 nm and 850 nm. We demonstrate up to $60\%$ improvement in RMSE over commercial depth sensors and validate the spectral accuracy against a reference spectrometer and commercial multispectral cameras, demonstrating comparable performance. A portable version limited to RGB (3 wavelengths) is used to collect real-world depth and spectral data from a Masoala Rainforest. We demonstrate the use of this prototype for color image reconstruction and material differentiation between leaves and branches using spectral and depth data. Our results show that adding depth (available at no extra effort with our setup) to material differentiation improves the accuracy by over $30\%$ compared to color-only method. Our system, tested in both lab and real-world rainforest environments, shows strong performance in depth estimation, RGB reconstruction, and material differentiation - paving the way for lightweight, integrated, and robust UAV perception and data collection in complex natural environments.</li>
</ul>

<h3>Title: Pothole Detection and Recognition based on Transfer Learning</h3>
<ul>
<li><strong>Authors: </strong>Mang Hu, Qianqian Xia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06750">https://arxiv.org/abs/2509.06750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06750">https://arxiv.org/pdf/2509.06750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06750]] Pothole Detection and Recognition based on Transfer Learning(https://arxiv.org/abs/2509.06750)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>With the rapid development of computer vision and machine learning, automated methods for pothole detection and recognition based on image and video data have received significant attention. It is of great significance for social development to conduct an in-depth analysis of road images through feature extraction, thereby achieving automatic identification of the pothole condition in new images. Consequently, this is the main issue addressed in this study. Based on preprocessing techniques such as standardization, normalization, and data augmentation applied to the collected raw dataset, we continuously improved the network model based on experimental results. Ultimately, we constructed a deep learning feature extraction network ResNet50-EfficientNet-RegNet model based on transfer learning. This model exhibits high classification accuracy and computational efficiency. In terms of model evaluation, this study employed a comparative evaluation approach by comparing the performance of the proposed transfer learning model with other models, including Random Forest, MLP, SVM, and LightGBM. The comparison analysis was conducted based on metrics such as Accuracy, Recall, Precision, F1-score, and FPS, to assess the classification performance of the transfer learning model proposed in this paper. The results demonstrate that our model exhibits high performance in terms of recognition speed and accuracy, surpassing the performance of other models. Through careful parameter selection and model optimization, our transfer learning model achieved a classification accuracy of 97.78% (88/90) on the initial set of 90 test samples and 98.89% (890/900) on the expanded test set.</li>
</ul>

<h3>Title: Image Encryption Scheme Based on Hyper-Chaotic Map and Self-Adaptive Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yiqi Tang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06754">https://arxiv.org/abs/2509.06754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06754">https://arxiv.org/pdf/2509.06754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06754]] Image Encryption Scheme Based on Hyper-Chaotic Map and Self-Adaptive Diffusion(https://arxiv.org/abs/2509.06754)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In the digital age, image encryption technology acts as a safeguard, preventing unauthorized access to images. This paper proposes an innovative image encryption scheme that integrates a novel 2D hyper-chaotic map with a newly developed self-adaptive diffusion method. The 2D hyper-chaotic map, namely the 2D-RA map, is designed by hybridizing the Rastrigin and Ackley functions. The chaotic performance of the 2D-RA map is validated through a series of measurements, including the Bifurcation Diagram, Lyapunov Exponent (LE), Initial Value Sensitivity, 0 - 1 Test, Correlation Dimension (CD), and Kolmogorov Entropy (KE). The results demonstrate that the chaotic performance of the 2D-RA map surpasses that of existing advanced chaotic functions. Additionally, the self-adaptive diffusion method is employed to enhance the uniformity of grayscale distribution. The performance of the image encryption scheme is evaluated using a series of indicators. The results show that the proposed image encryption scheme significantly outperforms current state-of-the-art image encryption techniques.</li>
</ul>

<h3>Title: Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Thanh Thi Nguyen, Campbell Wilson, Janis Dalins</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06759">https://arxiv.org/abs/2509.06759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06759">https://arxiv.org/pdf/2509.06759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06759]] Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization(https://arxiv.org/abs/2509.06759)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) or multimodal large language models represent a significant advancement in artificial intelligence, enabling systems to understand and generate content across both visual and textual modalities. While large-scale pretraining has driven substantial progress, fine-tuning these models for aligning with human values or engaging in specific tasks or behaviors remains a critical challenge. Deep Reinforcement Learning (DRL) and Direct Preference Optimization (DPO) offer promising frameworks for this aligning process. While DRL enables models to optimize actions using reward signals instead of relying solely on supervised preference data, DPO directly aligns the policy with preferences, eliminating the need for an explicit reward model. This overview explores paradigms for fine-tuning LVLMs, highlighting how DRL and DPO techniques can be used to align models with human preferences and values, improve task performance, and enable adaptive multimodal interaction. We categorize key approaches, examine sources of preference data, reward signals, and discuss open challenges such as scalability, sample efficiency, continual learning, generalization, and safety. The goal is to provide a clear understanding of how DRL and DPO contribute to the evolution of robust and human-aligned LVLMs.</li>
</ul>

<h3>Title: D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Sai Kartheek Reddy Kasu, Mohammad Zia Ur Rehman, Shahid Shafi Dar, Rishi Bharat Junghare, Dhanvin Sanjay Namboodiri, Nagendra Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06771">https://arxiv.org/abs/2509.06771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06771">https://arxiv.org/pdf/2509.06771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06771]] D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning(https://arxiv.org/abs/2509.06771)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Dark humor in online memes poses unique challenges due to its reliance on implicit, sensitive, and culturally contextual cues. To address the lack of resources and methods for detecting dark humor in multimodal content, we introduce a novel dataset of 4,379 Reddit memes annotated for dark humor, target category (gender, mental health, violence, race, disability, and other), and a three-level intensity rating (mild, moderate, severe). Building on this resource, we propose a reasoning-augmented framework that first generates structured explanations for each meme using a Large Vision-Language Model (VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective to iteratively refine its explanations, ensuring completeness and alignment. We then extract textual features from both the OCR transcript and the self-refined reasoning via a text encoder, while visual features are obtained using a vision transformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three streams, text, image, and reasoning, via pairwise attention mechanisms, producing a unified representation for classification. Experimental results demonstrate that our approach outperforms strong baselines across three tasks: dark humor detection, target identification, and intensity prediction. The dataset, annotations, and code are released to facilitate further research in multimodal humor understanding and content moderation. Code and Dataset are available at: this https URL</li>
</ul>

<h3>Title: UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Shahbaz, Shaurya Agarwal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06781">https://arxiv.org/abs/2509.06781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06781">https://arxiv.org/pdf/2509.06781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06781]] UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets(https://arxiv.org/abs/2509.06781)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This article presents UrbanTwin datasets - high-fidelity, realistic replicas of three public roadside lidar datasets: LUMPI, V2X-Real-IC, and TUMTraf-I. Each UrbanTwin dataset contains 10K annotated frames corresponding to one of the public datasets. Annotations include 3D bounding boxes, instance segmentation labels, and tracking IDs for six object classes, along with semantic segmentation labels for nine classes. These datasets are synthesized using emulated lidar sensors within realistic digital twins, modeled based on surrounding geometry, road alignment at lane level, and the lane topology and vehicle movement patterns at intersections of the actual locations corresponding to each real dataset. Due to the precise digital twin modeling, the synthetic datasets are well aligned with their real counterparts, offering strong standalone and augmentative value for training deep learning models on tasks such as 3D object detection, tracking, and semantic and instance segmentation. We evaluate the alignment of the synthetic replicas through statistical and structural similarity analysis with real data, and further demonstrate their utility by training 3D object detection models solely on synthetic data and testing them on real, unseen data. The high similarity scores and improved detection performance, compared to the models trained on real data, indicate that the UrbanTwin datasets effectively enhance existing benchmark datasets by increasing sample size and scene diversity. In addition, the digital twins can be adapted to test custom scenarios by modifying the design and dynamics of the simulations. To our knowledge, these are the first digitally synthesized datasets that can replace in-domain real-world datasets for lidar perception tasks. UrbanTwin datasets are publicly available at this https URL.</li>
</ul>

<h3>Title: P3-SAM: Native 3D Part Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Changfeng Ma, Yang Li, Xinhao Yan, Jiachen Xu, Yunhan Yang, Chunshi Wang, Zibo Zhao, Yanwen Guo, Zhuo Chen, Chunchao Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06784">https://arxiv.org/abs/2509.06784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06784">https://arxiv.org/pdf/2509.06784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06784]] P3-SAM: Native 3D Part Segmentation(https://arxiv.org/abs/2509.06784)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Segmenting 3D assets into their constituent parts is crucial for enhancing 3D understanding, facilitating model reuse, and supporting various applications such as part generation. However, current methods face limitations such as poor robustness when dealing with complex objects and cannot fully automate the process. In this paper, we propose a native 3D point-promptable part segmentation model termed P3-SAM, designed to fully automate the segmentation of any 3D objects into components. Inspired by SAM, P3-SAM consists of a feature extractor, multiple segmentation heads, and an IoU predictor, enabling interactive segmentation for users. We also propose an algorithm to automatically select and merge masks predicted by our model for part instance segmentation. Our model is trained on a newly built dataset containing nearly 3.7 million models with reasonable segmentation labels. Comparisons show that our method achieves precise segmentation results and strong robustness on any complex objects, attaining state-of-the-art performance. Our code will be released soon.</li>
</ul>

<h3>Title: Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via Projection Constraint</h3>
<ul>
<li><strong>Authors: </strong>Yanrui Du, Fenglei Fan, Sendong Zhao, Jiawei Cao, Qika Lin, Kai He, Ting Liu, Bing Qin, Mengling Feng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06795">https://arxiv.org/abs/2509.06795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06795">https://arxiv.org/pdf/2509.06795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06795]] Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via Projection Constraint(https://arxiv.org/abs/2509.06795)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Instruction Fine-Tuning (IFT) has been widely adopted as an effective post-training strategy to enhance various abilities of Large Language Models (LLMs). However, prior studies have shown that IFT can significantly compromise LLMs' safety, particularly their ability to refuse malicious instructions, raising significant concerns. Recent research into the internal mechanisms of LLMs has identified the refusal direction (r-direction) in the hidden states, which plays a pivotal role in governing refusal behavior. Building on this insight, our study reveals that the r-direction tends to drift during training, which we identify as one of the causes of the associated safety risks. To mitigate such drift, our proposed ProCon method introduces a projection-constrained loss term that regularizes the projection magnitude of each training sample's hidden state onto the r-direction. Our initial analysis shows that applying an appropriate constraint can effectively mitigate the refusal direction drift and associated safety risks, but remains limited by overall performance barriers. To overcome this barrier, informed by our observation of early-stage sharp drift and a data-driven perspective, we introduce a warm-up strategy that emphasizes early-stage strong constraints and broaden the data distribution to strengthen constraint signals, leading to an enhanced ProCon method. Experimental results under various datasets, scenarios, and LLMs demonstrate that our method can significantly mitigate safety risks posed by IFT while preserving task performance gains. Even compared with strong baselines, our method consistently delivers superior overall performance. Crucially, our analysis indicates that ProCon can contribute to stabilizing the r-direction during training, while such an interpretability-driven exploration of LLMs' internal mechanisms lays a solid foundation for future safety research.</li>
</ul>

<h3>Title: Imitative Membership Inference Attack</h3>
<ul>
<li><strong>Authors: </strong>Yuntao Du, Yuetian Chen, Hanshen Xiao, Bruno Ribeiro, Ninghui Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06796">https://arxiv.org/abs/2509.06796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06796">https://arxiv.org/pdf/2509.06796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06796]] Imitative Membership Inference Attack(https://arxiv.org/abs/2509.06796)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, membership infer</a></li>
<li><strong>Abstract: </strong>A Membership Inference Attack (MIA) assesses how much a target machine learning model reveals about its training data by determining whether specific query instances were part of the training set. State-of-the-art MIAs rely on training hundreds of shadow models that are independent of the target model, leading to significant computational overhead. In this paper, we introduce Imitative Membership Inference Attack (IMIA), which employs a novel imitative training technique to strategically construct a small number of target-informed imitative models that closely replicate the target model's behavior for inference. Extensive experimental results demonstrate that IMIA substantially outperforms existing MIAs in various attack settings while only requiring less than 5% of the computational cost of state-of-the-art approaches.</li>
</ul>

<h3>Title: SynthDrive: Scalable Real2Sim2Real Sensor Simulation Pipeline for High-Fidelity Asset Generation and Driving Data Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Zhengqing Chen, Ruohong Mei, Xiaoyang Guo, Qingjie Wang, Yubin Hu, Wei Yin, Weiqiang Ren, Qian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06798">https://arxiv.org/abs/2509.06798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06798">https://arxiv.org/pdf/2509.06798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06798]] SynthDrive: Scalable Real2Sim2Real Sensor Simulation Pipeline for High-Fidelity Asset Generation and Driving Data Synthesis(https://arxiv.org/abs/2509.06798)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In the field of autonomous driving, sensor simulation is essential for generating rare and diverse scenarios that are difficult to capture in real-world environments. Current solutions fall into two categories: 1) CG-based methods, such as CARLA, which lack diversity and struggle to scale to the vast array of rare cases required for robust perception training; and 2) learning-based approaches, such as NeuSim, which are limited to specific object categories (vehicles) and require extensive multi-sensor data, hindering their applicability to generic objects. To address these limitations, we propose a scalable real2sim2real system that leverages 3D generation to automate asset mining, generation, and rare-case data synthesis.</li>
</ul>

<h3>Title: MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Dong, Pengkun Zhang, Mingzhe Lu, Yanzhen Shen, Guolin Ke</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06806">https://arxiv.org/abs/2509.06806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06806">https://arxiv.org/pdf/2509.06806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06806]] MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML(https://arxiv.org/abs/2509.06806)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. We introduce MachineLearningLM, a portable continued-pretraining framework that equips a general-purpose LLM with robust in-context ML capability while preserving its general knowledge and reasoning for broader chat workflows. Our pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. We begin with a random-forest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with a token-efficient prompt, enabling 3x to 6x more examples per context window and delivering up to 50x amortized throughput via batch inference. Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of about 15% on out-of-distribution tabular classification across finance, physics, biology, and healthcare domains. It exhibits a striking many-shot scaling law: accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forest-level accuracy across hundreds of shots. General chat capabilities, including knowledge and reasoning, are preserved: it achieves 75.4% on MMLU.</li>
</ul>

<h3>Title: MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and Security</h3>
<ul>
<li><strong>Authors: </strong>Yanrui Du, Fenglei Fan, Sendong Zhao, Jiawei Cao, Ting Liu, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06807">https://arxiv.org/abs/2509.06807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06807">https://arxiv.org/pdf/2509.06807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06807]] MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and Security(https://arxiv.org/abs/2509.06807)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) increasingly permeate human life, their security has emerged as a critical concern, particularly their ability to maintain harmless responses to malicious instructions. Although extensive methods have improved LLMs' security, they often lead to conservative, rejection-oriented responses that compromise practical usability. This presents a key challenge: how to advance the Pareto frontier between LLMs' usability and security, rather than necessitate a trade-off between them. To address this, we propose the MoGU framework, in which the intra-layer router dynamically allocates weights by sensing hidden states, thereby balancing the contributions of security-optimized and usability-optimized variants. Despite its initial potential, the MoGU framework faces limitations such as parameter redundancy and performance bottlenecks. To overcome these, we further propose an improved MoGU_v2 framework that establishes a tighter coupling between the routers and hidden states. In MoGU_v2, routers are embedded only in layers encoding highly classifiable security features, and backbone modules are activated during router optimization to enable bidirectional adaptation. MoGU_V2 exhibits strong adaptability and stable improvements across various series of LLMs, including mainstream LLMs serving as brains in various applications, on-device LLMs optimized for resource-constrained scenarios, and reasoning LLMs tailored for user interpretability. Meanwhile, even facing risks introduced by Instruction Fine-tuning, MoGU_v2 can easily restore security without compromising the task performance gains via a simple data-mix strategy. These comprehensive improvements highlight MoGU_V2 as a robust and versatile solution for mitigating security risks in real-world applications.</li>
</ul>

<h3>Title: Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem</h3>
<ul>
<li><strong>Authors: </strong>Valentin Quesnel, Damien Sileo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06809">https://arxiv.org/abs/2509.06809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06809">https://arxiv.org/pdf/2509.06809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06809]] Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem(https://arxiv.org/abs/2509.06809)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The scarcity of high-quality, logically sound data is a critical bottleneck for advancing the mathematical reasoning of Large Language Models (LLMs). Our work confronts this challenge by turning decades of automated theorem proving research into a scalable data engine. Rather than relying on error-prone LLMs or complex proof-assistant syntax like Lean and Isabelle, our framework leverages E-prover's saturation capabilities on the vast TPTP axiom library to derive a massive, guaranteed-valid corpus of theorems. Our pipeline is principled and simple: saturate axioms, filter for "interesting" theorems, and generate tasks. With no LLMs in the loop, we eliminate factual errors by construction. This purely symbolic data is then transformed into three difficulty-controlled challenges: entailment verification, premise selection, and proof reconstruction. Our zero-shot experiments on frontier models reveal a clear weakness: performance collapses on tasks requiring deep, structural reasoning. Our framework provides both the diagnostic tool to measure this gap and a scalable source of symbolic training data to address it. We make the code and data publicly available. this https URL this https URL</li>
</ul>

<h3>Title: A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs</h3>
<ul>
<li><strong>Authors: </strong>Max Malyi, Jonathan Shek, Alasdair McDonald, Andre Biscaya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06813">https://arxiv.org/abs/2509.06813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06813">https://arxiv.org/pdf/2509.06813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06813]] A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs(https://arxiv.org/abs/2509.06813)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Effective Operation and Maintenance (O&M) is critical to reducing the Levelised Cost of Energy (LCOE) from wind power, yet the unstructured, free-text nature of turbine maintenance logs presents a significant barrier to automated analysis. Our paper addresses this by presenting a novel and reproducible framework for benchmarking Large Language Models (LLMs) on the task of classifying these complex industrial records. To promote transparency and encourage further research, this framework has been made publicly available as an open-source tool. We systematically evaluate a diverse suite of state-of-the-art proprietary and open-source LLMs, providing a foundational assessment of their trade-offs in reliability, operational efficiency, and model calibration. Our results quantify a clear performance hierarchy, identifying top models that exhibit high alignment with a benchmark standard and trustworthy, well-calibrated confidence scores. We also demonstrate that classification performance is highly dependent on the task's semantic ambiguity, with all models showing higher consensus on objective component identification than on interpretive maintenance actions. Given that no model achieves perfect accuracy and that calibration varies dramatically, we conclude that the most effective and responsible near-term application is a Human-in-the-Loop system, where LLMs act as a powerful assistant to accelerate and standardise data labelling for human experts, thereby enhancing O&M data quality and downstream reliability analysis.</li>
</ul>

<h3>Title: UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward</h3>
<ul>
<li><strong>Authors: </strong>Yufeng Cheng, Wenxu Wu, Shaojin Wu, Mengqi Huang, Fei Ding, Qian He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06818">https://arxiv.org/abs/2509.06818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06818">https://arxiv.org/pdf/2509.06818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06818]] UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward(https://arxiv.org/abs/2509.06818)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in image customization exhibit a wide range of application prospects due to stronger customization capabilities. However, since we humans are more sensitive to faces, a significant challenge remains in preserving consistent identity while avoiding identity confusion with multi-reference images, limiting the identity scalability of customization models. To address this, we present UMO, a Unified Multi-identity Optimization framework, designed to maintain high-fidelity identity preservation and alleviate identity confusion with scalability. With "multi-to-multi matching" paradigm, UMO reformulates multi-identity generation as a global assignment optimization problem and unleashes multi-identity consistency for existing image customization methods generally through reinforcement learning on diffusion models. To facilitate the training of UMO, we develop a scalable customization dataset with multi-reference images, consisting of both synthesised and real parts. Additionally, we propose a new metric to measure identity confusion. Extensive experiments demonstrate that UMO not only improves identity consistency significantly, but also reduces identity confusion on several image customization methods, setting a new state-of-the-art among open-source methods along the dimension of identity preserving. Code and model: this https URL</li>
</ul>

<h3>Title: Video-Based MPAA Rating Prediction: An Attention-Driven Hybrid Architecture Using Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Dipta Neogi, Nourash Azmine Chowdhury, Muhammad Rafsan Kabir, Mohammad Ashrafuzzaman Khan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06826">https://arxiv.org/abs/2509.06826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06826">https://arxiv.org/pdf/2509.06826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06826]] Video-Based MPAA Rating Prediction: An Attention-Driven Hybrid Architecture Using Contrastive Learning(https://arxiv.org/abs/2509.06826)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The rapid growth of visual content consumption across platforms necessitates automated video classification for age-suitability standards like the MPAA rating system (G, PG, PG-13, R). Traditional methods struggle with large labeled data requirements, poor generalization, and inefficient feature learning. To address these challenges, we employ contrastive learning for improved discrimination and adaptability, exploring three frameworks: Instance Discrimination, Contextual Contrastive Learning, and Multi-View Contrastive Learning. Our hybrid architecture integrates an LRCN (CNN+LSTM) backbone with a Bahdanau attention mechanism, achieving state-of-the-art performance in the Contextual Contrastive Learning framework, with 88% accuracy and an F1 score of 0.8815. By combining CNNs for spatial features, LSTMs for temporal modeling, and attention mechanisms for dynamic frame prioritization, the model excels in fine-grained borderline distinctions, such as differentiating PG-13 and R-rated content. We evaluate the model's performance across various contrastive loss functions, including NT-Xent, NT-logistic, and Margin Triplet, demonstrating the robustness of our proposed architecture. To ensure practical application, the model is deployed as a web application for real-time MPAA rating classification, offering an efficient solution for automated content compliance across streaming platforms.</li>
</ul>

<h3>Title: Evaluating the Impact of Adversarial Attacks on Traffic Sign Classification using the LISA Dataset</h3>
<ul>
<li><strong>Authors: </strong>Nabeyou Tadessa, Balaji Iyangar, Mashrur Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06835">https://arxiv.org/abs/2509.06835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06835">https://arxiv.org/pdf/2509.06835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06835]] Evaluating the Impact of Adversarial Attacks on Traffic Sign Classification using the LISA Dataset(https://arxiv.org/abs/2509.06835)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Adversarial attacks pose significant threats to machine learning models by introducing carefully crafted perturbations that cause misclassification. While prior work has primarily focused on MNIST and similar datasets, this paper investigates the vulnerability of traffic sign classifiers using the LISA Traffic Sign dataset. We train a convolutional neural network to classify 47 different traffic signs and evaluate its robustness against Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks. Our results show a sharp decline in classification accuracy as the perturbation magnitude increases, highlighting the models susceptibility to adversarial examples. This study lays the groundwork for future exploration into defense mechanisms tailored for real-world traffic sign recognition systems.</li>
</ul>

<h3>Title: COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens</h3>
<ul>
<li><strong>Authors: </strong>Eugene Kwek, Wenpeng Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06836">https://arxiv.org/abs/2509.06836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06836">https://arxiv.org/pdf/2509.06836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06836]] COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens(https://arxiv.org/abs/2509.06836)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Making LLMs more efficient in memory, latency, and serving cost is crucial for edge deployment, interactive applications, and sustainable inference at scale. Pruning is a key technique toward this goal. However, prior pruning methods are limited: width pruning often breaks the standard transformer layout or requires custom inference code, while depth pruning removes entire layers and can cause abrupt accuracy drops. In this work, we propose COMPACT, which jointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii) prunes FFN intermediate channels using common-token-weighted activations, aligning importance with the post-pruning token distribution. COMPACT enjoys merits of both depth and width pruning, such as: deployment-friendliness (keeps a standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN pruning), training-free operation with competitive pruning time, and strong memory savings alongside throughput gains. Experiments across Qwen, LLaMA, and Gemma families (0.5B-70B) show state-of-the-art downstream task performance at similar or higher pruning ratios, with substantial reductions in parameters, GPU memory, and end-to-end latency.</li>
</ul>

<h3>Title: EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Reza Mirbagheri, Mohammad Mahdi Mirkamali, Zahra Motoshaker Arani, Ali Javeri, Amir Mahdi Sadeghzadeh, Rasool Jalili</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06838">https://arxiv.org/abs/2509.06838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06838">https://arxiv.org/pdf/2509.06838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06838]] EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models(https://arxiv.org/abs/2509.06838)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), trained on extensive datasets using advanced deep learning architectures, have demonstrated remarkable performance across a wide range of language tasks, becoming a cornerstone of modern AI technologies. However, ensuring their trustworthiness remains a critical challenge, as reliability is essential not only for accurate performance but also for upholding ethical, cultural, and social values. Careful alignment of training data and culturally grounded evaluation criteria are vital for developing responsible AI systems. In this study, we introduce the EPT (Evaluation of Persian Trustworthiness) metric, a culturally informed benchmark specifically designed to assess the trustworthiness of LLMs across six key aspects: truthfulness, safety, fairness, robustness, privacy, and ethical alignment. We curated a labeled dataset and evaluated the performance of several leading models - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and Qwen - using both automated LLM-based and human assessments. Our results reveal significant deficiencies in the safety dimension, underscoring the urgent need for focused attention on this critical aspect of model behavior. Furthermore, our findings offer valuable insights into the alignment of these models with Persian ethical-cultural values and highlight critical gaps and opportunities for advancing trustworthy and culturally responsible AI. The dataset is publicly available at: this https URL.</li>
</ul>

<h3>Title: Automated Radiographic Total Sharp Score (ARTSS) in Rheumatoid Arthritis: A Solution to Reduce Inter-Intra Reader Variation and Enhancing Clinical Practice</h3>
<ul>
<li><strong>Authors: </strong>Hajar Moradmand, Lei Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06854">https://arxiv.org/abs/2509.06854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06854">https://arxiv.org/pdf/2509.06854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06854]] Automated Radiographic Total Sharp Score (ARTSS) in Rheumatoid Arthritis: A Solution to Reduce Inter-Intra Reader Variation and Enhancing Clinical Practice(https://arxiv.org/abs/2509.06854)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Assessing the severity of rheumatoid arthritis (RA) using the Total Sharp/Van Der Heijde Score (TSS) is crucial, but manual scoring is often time-consuming and subjective. This study introduces an Automated Radiographic Sharp Scoring (ARTSS) framework that leverages deep learning to analyze full-hand X-ray images, aiming to reduce inter- and intra-observer variability. The research uniquely accommodates patients with joint disappearance and variable-length image sequences. We developed ARTSS using data from 970 patients, structured into four stages: I) Image pre-processing and re-orientation using ResNet50, II) Hand segmentation using UNet.3, III) Joint identification using YOLOv7, and IV) TSS prediction using models such as VGG16, VGG19, ResNet50, DenseNet201, EfficientNetB0, and Vision Transformer (ViT). We evaluated model performance with Intersection over Union (IoU), Mean Average Precision (MAP), mean absolute error (MAE), Root Mean Squared Error (RMSE), and Huber loss. The average TSS from two radiologists was used as the ground truth. Model training employed 3-fold cross-validation, with each fold consisting of 452 training and 227 validation samples, and external testing included 291 unseen subjects. Our joint identification model achieved 99% accuracy. The best-performing model, ViT, achieved a notably low Huber loss of 0.87 for TSS prediction. Our results demonstrate the potential of deep learning to automate RA scoring, which can significantly enhance clinical practice. Our approach addresses the challenge of joint disappearance and variable joint numbers, offers timesaving benefits, reduces inter- and intra-reader variability, improves radiologist accuracy, and aids rheumatologists in making more informed decisions.</li>
</ul>

<h3>Title: floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL</h3>
<ul>
<li><strong>Authors: </strong>Bhavya Agrawalla, Michal Nauman, Khush Agarwal, Aviral Kumar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06863">https://arxiv.org/abs/2509.06863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06863">https://arxiv.org/pdf/2509.06863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06863]] floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL(https://arxiv.org/abs/2509.06863)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>A hallmark of modern large-scale machine learning techniques is the use of training objectives that provide dense supervision to intermediate computations, such as teacher forcing the next token in language models or denoising step-by-step in diffusion models. This enables models to learn complex functions in a generalizable manner. Motivated by this observation, we investigate the benefits of iterative computation for temporal difference (TD) methods in reinforcement learning (RL). Typically they represent value functions in a monolithic fashion, without iterative compute. We introduce floq (flow-matching Q-functions), an approach that parameterizes the Q-function using a velocity field and trains it using techniques from flow-matching, typically used in generative modeling. This velocity field underneath the flow is trained using a TD-learning objective, which bootstraps from values produced by a target velocity field, computed by running multiple steps of numerical integration. Crucially, floq allows for more fine-grained control and scaling of the Q-function capacity than monolithic architectures, by appropriately setting the number of integration steps. Across a suite of challenging offline RL benchmarks and online fine-tuning tasks, floq improves performance by nearly 1.8x. floq scales capacity far better than standard TD-learning architectures, highlighting the potential of iterative computation for value learning.</li>
</ul>

<h3>Title: Concolic Testing on Individual Fairness of Neural Network Models</h3>
<ul>
<li><strong>Authors: </strong>Ming-I Huang, Chih-Duo Hong, Fang Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06864">https://arxiv.org/abs/2509.06864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06864">https://arxiv.org/pdf/2509.06864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06864]] Concolic Testing on Individual Fairness of Neural Network Models(https://arxiv.org/abs/2509.06864)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>This paper introduces PyFair, a formal framework for evaluating and verifying individual fairness of Deep Neural Networks (DNNs). By adapting the concolic testing tool PyCT, we generate fairness-specific path constraints to systematically explore DNN behaviors. Our key innovation is a dual network architecture that enables comprehensive fairness assessments and provides completeness guarantees for certain network types. We evaluate PyFair on 25 benchmark models, including those enhanced by existing bias mitigation techniques. Results demonstrate PyFair's efficacy in detecting discriminatory instances and verifying fairness, while also revealing scalability challenges for complex models. This work advances algorithmic fairness in critical domains by offering a rigorous, systematic method for fairness testing and verification of pre-trained DNNs.</li>
</ul>

<h3>Title: A New Hybrid Model of Generative Adversarial Network and You Only Look Once Algorithm for Automatic License-Plate Recognition</h3>
<ul>
<li><strong>Authors: </strong>Behnoud Shafiezadeh, Amir Mashmool, Farshad Eshghi, Manoochehr Kelarestaghi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06868">https://arxiv.org/abs/2509.06868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06868">https://arxiv.org/pdf/2509.06868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06868]] A New Hybrid Model of Generative Adversarial Network and You Only Look Once Algorithm for Automatic License-Plate Recognition(https://arxiv.org/abs/2509.06868)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Automatic License-Plate Recognition (ALPR) plays a pivotal role in Intelligent Transportation Systems (ITS) as a fundamental element of Smart Cities. However, due to its high variability, ALPR faces challenging issues more efficiently addressed by deep learning techniques. In this paper, a selective Generative Adversarial Network (GAN) is proposed for deblurring in the preprocessing step, coupled with the state-of-the-art You-Only-Look-Once (YOLO)v5 object detection architectures for License-Plate Detection (LPD), and the integrated Character Segmentation (CS) and Character Recognition (CR) steps. The selective preprocessing bypasses unnecessary and sometimes counter-productive input manipulations, while YOLOv5 LPD/CS+CR delivers high accuracy and low computing cost. As a result, YOLOv5 achieves a detection time of 0.026 seconds for both LP and CR detection stages, facilitating real-time applications with exceptionally rapid responsiveness. Moreover, the proposed model achieves accuracy rates of 95\% and 97\% in the LPD and CR detection phases, respectively. Furthermore, the inclusion of the Deblur-GAN pre-processor significantly improves detection accuracy by nearly 40\%, especially when encountering blurred License Plates (LPs).To train and test the learning components, we generated and publicly released our blur and ALPR datasets (using Iranian license plates as a use-case), which are more representative of close-to-real-life ad-hoc situations. The findings demonstrate that employing the state-of-the-art YOLO model results in excellent overall precision and detection time, making it well-suited for portable applications. Additionally, integrating the Deblur-GAN model as a preliminary processing step enhances the overall effectiveness of our comprehensive model, particularly when confronted with blurred scenes captured by the camera as input.</li>
</ul>

<h3>Title: The Majority is not always right: RL training for solution aggregation</h3>
<ul>
<li><strong>Authors: </strong>Wenting Zhao, Pranjal Aggarwal, Swarnadeep Saha, Asli Celikyilmaz, Jason Weston, Ilia Kulikov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06870">https://arxiv.org/abs/2509.06870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06870">https://arxiv.org/pdf/2509.06870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06870]] The Majority is not always right: RL training for solution aggregation(https://arxiv.org/abs/2509.06870)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Scaling up test-time compute, by generating multiple independent solutions and selecting or aggregating among them, has become a central paradigm for improving large language models (LLMs) on challenging reasoning tasks. While most prior work relies on simple majority voting or reward model ranking to aggregate solutions, these approaches may only yield limited benefits. In this work, we propose to learn aggregation as an explicit reasoning skill: given a set of candidate solutions, we train an aggregator model to review, reconcile, and synthesize a final, correct answer using reinforcement learning from verifiable rewards. A key ingredient is careful balancing of easy and hard training examples, allowing the model to learn both to recover minority-but-correct answers as well as easy majority-correct answers. Empirically, we find our method, AggLM, outperforms both strong rule-based and reward-model baselines, across multiple benchmarks. Furthermore, it generalizes effectively to solutions from differing models, including stronger ones than contained in the training data, all while requiring substantially fewer tokens than majority voting with larger numbers of solutions.</li>
</ul>

<h3>Title: UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction</h3>
<ul>
<li><strong>Authors: </strong>Joe Wilder, Nikhil Kadapala, Benji Xu, Mohammed Alsaadi, Aiden Parsons, Mitchell Rogers, Palash Agarwal, Adam Hassick, Laura Dietz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06883">https://arxiv.org/abs/2509.06883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06883">https://arxiv.org/pdf/2509.06883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06883]] UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction(https://arxiv.org/abs/2509.06883)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>We participate in CheckThat! Task 2 English and explore various methods of prompting and in-context learning, including few-shot prompting and fine-tuning with different LLM families, with the goal of extracting check-worthy claims from social media passages. Our best METEOR score is achieved by fine-tuning a FLAN-T5 model. However, we observe that higher-quality claims can sometimes be extracted using other methods, even when their METEOR scores are lower.</li>
</ul>

<h3>Title: Barlow-Swin: Toward a novel siamese-based segmentation architecture using Swin-Transformers</h3>
<ul>
<li><strong>Authors: </strong>Morteza Kiani Haftlang, Mohammadhossein Malmir, Foroutan Parand, Umberto Michelucci, Safouane El Ghazouali</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06885">https://arxiv.org/abs/2509.06885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06885">https://arxiv.org/pdf/2509.06885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06885]] Barlow-Swin: Toward a novel siamese-based segmentation architecture using Swin-Transformers(https://arxiv.org/abs/2509.06885)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation is a critical task in clinical workflows, particularly for the detection and delineation of pathological regions. While convolutional architectures like U-Net have become standard for such tasks, their limited receptive field restricts global context modeling. Recent efforts integrating transformers have addressed this, but often result in deep, computationally expensive models unsuitable for real-time use. In this work, we present a novel end-to-end lightweight architecture designed specifically for real-time binary medical image segmentation. Our model combines a Swin Transformer-like encoder with a U-Net-like decoder, connected via skip pathways to preserve spatial detail while capturing contextual information. Unlike existing designs such as Swin Transformer or U-Net, our architecture is significantly shallower and competitively efficient. To improve the encoder's ability to learn meaningful features without relying on large amounts of labeled data, we first train it using Barlow Twins, a self-supervised learning method that helps the model focus on important patterns by reducing unnecessary repetition in the learned features. After this pretraining, we fine-tune the entire model for our specific task. Experiments on benchmark binary segmentation tasks demonstrate that our model achieves competitive accuracy with substantially reduced parameter count and faster inference, positioning it as a practical alternative for deployment in real-time and resource-limited clinical environments. The code for our method is available at Github repository: this https URL.</li>
</ul>

<h3>Title: Intraoperative 2D/3D Registration via Spherical Similarity Learning and Inference-Time Differentiable Levenberg-Marquardt Optimization</h3>
<ul>
<li><strong>Authors: </strong>Minheng Chen, Youyong Kong</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06890">https://arxiv.org/abs/2509.06890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06890">https://arxiv.org/pdf/2509.06890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06890]] Intraoperative 2D/3D Registration via Spherical Similarity Learning and Inference-Time Differentiable Levenberg-Marquardt Optimization(https://arxiv.org/abs/2509.06890)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Intraoperative 2D/3D registration aligns preoperative 3D volumes with real-time 2D radiographs, enabling accurate localization of instruments and implants. A recent fully differentiable similarity learning framework approximates geodesic distances on SE(3), expanding the capture range of registration and mitigating the effects of substantial disturbances, but existing Euclidean approximations distort manifold structure and slow convergence. To address these limitations, we explore similarity learning in non-Euclidean spherical feature spaces to better capture and fit complex manifold structure. We extract feature embeddings using a CNN-Transformer encoder, project them into spherical space, and approximate their geodesic distances with Riemannian distances in the bi-invariant SO(4) space. This enables a more expressive and geometrically consistent deep similarity metric, enhancing the ability to distinguish subtle pose differences. During inference, we replace gradient descent with fully differentiable Levenberg-Marquardt optimization to accelerate convergence. Experiments on real and synthetic datasets show superior accuracy in both patient-specific and patient-agnostic scenarios.</li>
</ul>

<h3>Title: Not All Samples Are Equal: Quantifying Instance-level Difficulty in Targeted Data Poisoning</h3>
<ul>
<li><strong>Authors: </strong>William Xu, Yiwei Lu, Yihan Wang, Matthew Y.R. Yang, Zuoqiu Liu, Gautam Kamath, Yaoliang Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06896">https://arxiv.org/abs/2509.06896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06896">https://arxiv.org/pdf/2509.06896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06896]] Not All Samples Are Equal: Quantifying Instance-level Difficulty in Targeted Data Poisoning(https://arxiv.org/abs/2509.06896)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Targeted data poisoning attacks pose an increasingly serious threat due to their ease of deployment and high success rates. These attacks aim to manipulate the prediction for a single test sample in classification models. Unlike indiscriminate attacks that aim to decrease overall test performance, targeted attacks present a unique threat to individual test instances. This threat model raises a fundamental question: what factors make certain test samples more susceptible to successful poisoning than others? We investigate how attack difficulty varies across different test instances and identify key characteristics that influence vulnerability. This paper introduces three predictive criteria for targeted data poisoning difficulty: ergodic prediction accuracy (analyzed through clean training dynamics), poison distance, and poison budget. Our experimental results demonstrate that these metrics effectively predict the varying difficulty of real-world targeted poisoning attacks across diverse scenarios, offering practitioners valuable insights for vulnerability assessment and understanding data poisoning attacks.</li>
</ul>

<h3>Title: Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification</h3>
<ul>
<li><strong>Authors: </strong>Aivin V. Solatorio</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.DB, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06902">https://arxiv.org/abs/2509.06902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06902">https://arxiv.org/pdf/2509.06902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06902]] Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification(https://arxiv.org/abs/2509.06902)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) as stochastic systems may generate numbers that deviate from available data, a failure known as \emph{numeric hallucination}. Existing safeguards -- retrieval-augmented generation, citations, and uncertainty estimation -- improve transparency but cannot guarantee fidelity: fabricated or misquoted values may still be displayed as if correct. We propose \textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that enforces numeric fidelity through mechanical verification. Under PCN, numeric spans are emitted as \emph{claim-bound tokens} tied to structured claims, and a verifier checks each token under a declared policy (e.g., exact equality, rounding, aliases, or tolerance with qualifiers). Crucially, PCN places verification in the \emph{renderer}, not the model: only claim-checked numbers are marked as verified, and all others default to unverified. This separation prevents spoofing and guarantees fail-closed behavior. We formalize PCN and prove soundness, completeness under honest tokens, fail-closed behavior, and monotonicity under policy refinement. PCN is lightweight and model-agnostic, integrates seamlessly into existing applications, and can be extended with cryptographic commitments. By enforcing verification as a mandatory step before display, PCN establishes a simple contract for numerically sensitive settings: \emph{trust is earned only by proof}, while the absence of a mark communicates uncertainty.</li>
</ul>

<h3>Title: BIR-Adapter: A Low-Complexity Diffusion Model Adapter for Blind Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Cem Eteke, Alexander Griessel, Wolfgang Kellerer, Eckehard Steinbach</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06904">https://arxiv.org/abs/2509.06904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06904">https://arxiv.org/pdf/2509.06904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06904]] BIR-Adapter: A Low-Complexity Diffusion Model Adapter for Blind Image Restoration(https://arxiv.org/abs/2509.06904)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>This paper introduces BIR-Adapter, a low-complexity blind image restoration adapter for diffusion models. The BIR-Adapter enables the utilization of the prior of pre-trained large-scale diffusion models on blind image restoration without training any auxiliary feature extractor. We take advantage of the robustness of pretrained models. We extract features from degraded images via the model itself and extend the self-attention mechanism with these degraded features. We introduce a sampling guidance mechanism to reduce hallucinations. We perform experiments on synthetic and real-world degradations and demonstrate that BIR-Adapter achieves competitive or better performance compared to state-of-the-art methods while having significantly lower complexity. Additionally, its adapter-based design enables integration into other diffusion models, enabling broader applications in image restoration tasks. We showcase this by extending a super-resolution-only model to perform better under additional unknown degradations.</li>
</ul>

<h3>Title: FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data</h3>
<ul>
<li><strong>Authors: </strong>Bing Han, Chen Zhu, Dong Han, Rui Yu, Songliang Cao, Jianhui Wu, Scott Chapman, Zijian Wang, Bangyou Zheng, Wei Guo, Marie Weiss, Benoit de Solan, Andreas Hund, Lukas Roth, Kirchgessner Norbert, Andrea Visioni, Yufeng Ge, Wenjuan Li, Alexis Comar, Dong Jiang, Dejun Han, Fred Baret, Yanfeng Ding, Hao Lu, Shouyang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06907">https://arxiv.org/abs/2509.06907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06907">https://arxiv.org/pdf/2509.06907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06907]] FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data(https://arxiv.org/abs/2509.06907)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-driven field monitoring is central to digital agriculture, yet models built on general-domain pretrained backbones often fail to generalize across tasks, owing to the interaction of fine, variable canopy structures with fluctuating field conditions. We present FoMo4Wheat, one of the first crop-domain vision foundation model pretrained with self-supervision on ImAg4Wheat, the largest and most diverse wheat image dataset to date (2.5 million high-resolution images collected over a decade at 30 global sites, spanning >2,000 genotypes and >500 environmental conditions). This wheat-specific pretraining yields representations that are robust for wheat and transferable to other crops and weeds. Across ten in-field vision tasks at canopy and organ levels, FoMo4Wheat models consistently outperform state-of-the-art models pretrained on general-domain dataset. These results demonstrate the value of crop-specific foundation models for reliable in-field perception and chart a path toward a universal crop foundation model with cross-species and cross-task capabilities. FoMo4Wheat models and the ImAg4Wheat dataset are publicly available online: this https URL and this https URL. The demonstration website is: this https URL.</li>
</ul>

<h3>Title: Tackling the Noisy Elephant in the Room: Label Noise-robust Out-of-Distribution Detection via Loss Correction and Low-rank Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Tarhib Al Azad, Shahana Ibrahim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06918">https://arxiv.org/abs/2509.06918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06918">https://arxiv.org/pdf/2509.06918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06918]] Tackling the Noisy Elephant in the Room: Label Noise-robust Out-of-Distribution Detection via Loss Correction and Low-rank Decomposition(https://arxiv.org/abs/2509.06918)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Robust out-of-distribution (OOD) detection is an indispensable component of modern artificial intelligence (AI) systems, especially in safety-critical applications where models must identify inputs from unfamiliar classes not seen during training. While OOD detection has been extensively studied in the machine learning literature--with both post hoc and training-based approaches--its effectiveness under noisy training labels remains underexplored. Recent studies suggest that label noise can significantly degrade OOD performance, yet principled solutions to this issue are lacking. In this work, we demonstrate that directly combining existing label noise-robust methods with OOD detection strategies is insufficient to address this critical challenge. To overcome this, we propose a robust OOD detection framework that integrates loss correction techniques from the noisy label learning literature with low-rank and sparse decomposition methods from signal processing. Extensive experiments on both synthetic and real-world datasets demonstrate that our method significantly outperforms the state-of-the-art OOD detection techniques, particularly under severe noisy label settings.</li>
</ul>

<h3>Title: An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection</h3>
<ul>
<li><strong>Authors: </strong>Haywood Gelman, John D. Hastings, David Kenley</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06920">https://arxiv.org/abs/2509.06920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06920">https://arxiv.org/pdf/2509.06920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06920]] An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection(https://arxiv.org/abs/2509.06920)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Insider threats are a growing organizational problem due to the complexity of identifying their technical and behavioral elements. A large research body is dedicated to the study of insider threats from technological, psychological, and educational perspectives. However, research in this domain has been generally dependent on datasets that are static and limited access which restricts the development of adaptive detection models. This study introduces a novel, ethically grounded approach that uses the large language model (LLM) Claude Sonnet 3.7 to dynamically synthesize syslog messages, some of which contain indicators of insider threat scenarios. The messages reflect real-world data distributions by being highly imbalanced (1% insider threats). The syslogs were analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with their performance evaluated through statistical metrics including precision, recall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across nearly all metrics, particularly in reducing false alarms and improving detection accuracy. The results show strong promise for the use of LLMs in synthetic dataset generation and insider threat detection.</li>
</ul>

<h3>Title: Neuro-Symbolic AI for Cybersecurity: State of the Art, Challenges, and Opportunities</h3>
<ul>
<li><strong>Authors: </strong>Safayat Bin Hakim, Muhammad Adil, Alvaro Velasquez, Shouhuai Xu, Houbing Herbert Song</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06921">https://arxiv.org/abs/2509.06921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06921">https://arxiv.org/pdf/2509.06921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06921]] Neuro-Symbolic AI for Cybersecurity: State of the Art, Challenges, and Opportunities(https://arxiv.org/abs/2509.06921)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Traditional Artificial Intelligence (AI) approaches in cybersecurity exhibit fundamental limitations: inadequate conceptual grounding leading to non-robustness against novel attacks; limited instructibility impeding analyst-guided adaptation; and misalignment with cybersecurity objectives. Neuro-Symbolic (NeSy) AI has emerged with the potential to revolutionize cybersecurity AI. However, there is no systematic understanding of this emerging approach. These hybrid systems address critical cybersecurity challenges by combining neural pattern recognition with symbolic reasoning, enabling enhanced threat understanding while introducing concerning autonomous offensive capabilities that reshape threat landscapes. In this survey, we systematically characterize this field by analyzing 127 publications spanning 2019-July 2025. We introduce a Grounding-Instructibility-Alignment (G-I-A) framework to evaluate these systems, focusing on both cyber defense and cyber offense across network security, malware analysis, and cyber operations. Our analysis shows advantages of multi-agent NeSy architectures and identifies critical implementation challenges including standardization gaps, computational complexity, and human-AI collaboration requirements that constrain deployment. We show that causal reasoning integration is the most transformative advancement, enabling proactive defense beyond correlation-based approaches. Our findings highlight dual-use implications where autonomous systems demonstrate substantial capabilities in zero-day exploitation while achieving significant cost reductions, altering threat dynamics. We provide insights and future research directions, emphasizing the urgent need for community-driven standardization frameworks and responsible development practices that ensure advancement serves defensive cybersecurity objectives while maintaining societal alignment.</li>
</ul>

<h3>Title: Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding</h3>
<ul>
<li><strong>Authors: </strong>Ziheng Li, Zexu Sun, Jinman Zhao, Erxue Min, Yongcheng Zeng, Hui Wu, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Xu Chen, Zhi-Hong Deng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06923">https://arxiv.org/abs/2509.06923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06923">https://arxiv.org/pdf/2509.06923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06923]] Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding(https://arxiv.org/abs/2509.06923)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable success in enhancing the reasoning capabilities of large language models (LLMs). However, existing RLVR methods often suffer from exploration inefficiency due to mismatches between the training data's difficulty and the model's capability. LLMs fail to discover viable reasoning paths when problems are overly difficult, while learning little new capability when problems are too simple. In this work, we formalize the impact of problem difficulty by quantifying the relationship between loss descent speed and rollout accuracy. Building on this analysis, we propose SEELE, a novel supervision-aided RLVR framework that dynamically adjusts problem difficulty to stay within the high-efficiency region. SEELE augments each training sample by appending a hint (part of a full solution) after the original problem. Unlike previous hint-based approaches, SEELE deliberately and adaptively adjusts the hint length for each problem to achieve an optimal difficulty. To determine the optimal hint length, SEELE employs a multi-round rollout sampling strategy. In each round, it fits an item response theory model to the accuracy-hint pairs collected in preceding rounds to predict the required hint length for the next round. This instance-level, real-time difficulty adjustment aligns problem difficulty with the evolving model capability, thereby improving exploration efficiency. Experimental results show that SEELE outperforms Group Relative Policy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5 points, respectively, and surpasses the best previous supervision-aided approach by +3.6 points on average across six math reasoning benchmarks.</li>
</ul>

<h3>Title: Neutron Reflectometry by Gradient Descent</h3>
<ul>
<li><strong>Authors: </strong>Max D.Champneys, Andrew J.Parnell, Philipp Gutfreund, Maximilian W. A. Skoda, . Patrick A. Fairclough, Timothy J.Rogers, Stephanie L.Burg</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06924">https://arxiv.org/abs/2509.06924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06924">https://arxiv.org/pdf/2509.06924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06924]] Neutron Reflectometry by Gradient Descent(https://arxiv.org/abs/2509.06924)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neutron reflectometry (NR) is a powerful technique to probe surfaces and interfaces. NR is inherently an indirect measurement technique, access to the physical quantities of interest (layer thickness, scattering length density, roughness), necessitate the solution of an inverse modelling problem, that is inefficient for large amounts of data or complex multiplayer structures (e.g. lithium batteries / electrodes). Recently, surrogate machine learning models have been proposed as an alternative to existing optimisation routines. Although such approaches have been successful, physical intuition is lost when replacing governing equations with fast neural networks. Instead, we propose a novel and efficient approach; to optimise reflectivity data analysis by performing gradient descent on the forward reflection model itself. Herein, automatic differentiation techniques are used to evaluate exact gradients of the error function with respect to the parameters of interest. Access to these quantities enables users of neutron reflectometry to harness a host of powerful modern optimisation and inference techniques that remain thus far unexploited in the context of neutron reflectometry. This paper presents two benchmark case studies; demonstrating state-of-the-art performance on a thick oxide quartz film, and robust co-fitting performance in the high complexity regime of organic LED multilayer devices. Additionally, we provide an open-source library of differentiable reflectometry kernels in the python programming language so that gradient based approaches can readily be applied to other NR datasets.</li>
</ul>

<h3>Title: From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Praneet Suresh, Jack Stanley, Sonia Joseph, Luca Scimeca, Danilo Bzdok</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06938">https://arxiv.org/abs/2509.06938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06938">https://arxiv.org/pdf/2509.06938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06938]] From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers(https://arxiv.org/abs/2509.06938)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>As generative AI systems become competent and democratized in science, business, and government, deeper insight into their failure modes now poses an acute need. The occasional volatility in their behavior, such as the propensity of transformer models to hallucinate, impedes trust and adoption of emerging AI solutions in high-stakes areas. In the present work, we establish how and when hallucinations arise in pre-trained transformer models through concept representations captured by sparse autoencoders, under scenarios with experimentally controlled uncertainty in the input space. Our systematic experiments reveal that the number of semantic concepts used by the transformer model grows as the input information becomes increasingly unstructured. In the face of growing uncertainty in the input space, the transformer model becomes prone to activate coherent yet input-insensitive semantic features, leading to hallucinated output. At its extreme, for pure-noise inputs, we identify a wide variety of robustly triggered and meaningful concepts in the intermediate activations of pre-trained transformer models, whose functional integrity we confirm through targeted steering. We also show that hallucinations in the output of a transformer model can be reliably predicted from the concept patterns embedded in transformer layer activations. This collection of insights on transformer internal processing mechanics has immediate consequences for aligning AI models with human values, AI safety, opening the attack surface for potential adversarial attacks, and providing a basis for automatic quantification of a model's hallucination risk.</li>
</ul>

<h3>Title: Outcome-based Exploration for LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yuda Song, Julia Kempe, Remi Munos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06941">https://arxiv.org/abs/2509.06941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06941">https://arxiv.org/pdf/2509.06941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06941]] Outcome-based Exploration for LLM Reasoning(https://arxiv.org/abs/2509.06941)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has emerged as a powerful method for improving the reasoning abilities of large language models (LLMs). Outcome-based RL, which rewards policies solely for the correctness of the final answer, yields substantial accuracy gains but also induces a systematic loss in generation diversity. This collapse undermines real-world performance, where diversity is critical for test-time scaling. We analyze this phenomenon by viewing RL post-training as a sampling process and show that, strikingly, RL can reduce effective diversity even on the training set relative to the base model. Our study highlights two central findings: (i) a transfer of diversity degradation, where reduced diversity on solved problems propagates to unsolved ones, and (ii) the tractability of the outcome space, since reasoning tasks admit only a limited set of distinct answers. Motivated by these insights, we propose outcome-based exploration, which assigns exploration bonuses according to final outcomes. We introduce two complementary algorithms: historical exploration, which encourages rarely observed answers via UCB-style bonuses, and batch exploration, which penalizes within-batch repetition to promote test-time diversity. Experiments on standard competition math with Llama and Qwen models demonstrate that both methods improve accuracy while mitigating diversity collapse. On the theoretical side, we formalize the benefit of outcome-based exploration through a new model of outcome-based bandits. Together, these contributions chart a practical path toward RL methods that enhance reasoning without sacrificing the diversity essential for scalable deployment.</li>
</ul>

<h3>Title: Interleaving Reasoning for Better Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, Junbo Qiao, Yue Guo, Yao Hu, Zhenfei Yin, Philip Torr, Yu Cheng, Wanli Ouyang, Shaohui Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06945">https://arxiv.org/abs/2509.06945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06945">https://arxiv.org/pdf/2509.06945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06945]] Interleaving Reasoning for Better Text-to-Image Generation(https://arxiv.org/abs/2509.06945)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: this https URL .</li>
</ul>

<h3>Title: Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Liang Chen, Xueting Han, Li Shen, Jing Bai, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06948">https://arxiv.org/abs/2509.06948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06948">https://arxiv.org/pdf/2509.06948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06948]] Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning(https://arxiv.org/abs/2509.06948)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has proven effective in incentivizing the reasoning abilities of large language models (LLMs), but suffers from severe efficiency challenges due to its trial-and-error nature. While the common practice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this decoupled two-stage approach limits interaction between SFT and RL, thereby constraining overall effectiveness. This study introduces a novel method for learning reasoning models that employs bilevel optimization to facilitate better cooperation between these training paradigms. By conditioning the SFT objective on the optimal RL policy, our approach enables SFT to meta-learn how to guide RL's optimization process. During training, the lower level performs RL updates while simultaneously receiving SFT supervision, and the upper level explicitly maximizes the cooperative gain-the performance advantage of joint SFT-RL training over RL alone. Empirical evaluations on five reasoning benchmarks demonstrate that our method consistently outperforms baselines and achieves a better balance between effectiveness and efficiency.</li>
</ul>

<h3>Title: Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, Mengdi Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06949">https://arxiv.org/abs/2509.06949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06949">https://arxiv.org/pdf/2509.06949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06949]] Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models(https://arxiv.org/abs/2509.06949)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: this https URL</li>
</ul>

<h3>Title: H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Shijian Lu, Nicu Sebe</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.06956">https://arxiv.org/abs/2509.06956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.06956">https://arxiv.org/pdf/2509.06956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.06956]] H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers(https://arxiv.org/abs/2509.06956)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a hierarchical plug-and-play pruning-and-recovering framework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient transformer-based 3D human pose estimation from videos. H$_{2}$OT begins with progressively pruning pose tokens of redundant frames and ends with recovering full-length sequences, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. It works with two key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module (TRM). TPM dynamically selects a few representative tokens to eliminate the redundancy of video frames, while TRM restores the detailed spatio-temporal information based on the selected tokens, thereby expanding the network output to the original full-length temporal resolution for fast inference. Our method is general-purpose: it can be easily incorporated into common VPT models on both seq2seq and seq2frame pipelines while effectively accommodating different token pruning and recovery strategies. In addition, our H$_{2}$OT reveals that maintaining the full pose sequence is unnecessary, and a few pose tokens of representative frames can achieve both high efficiency and estimation accuracy. Extensive experiments on multiple benchmark datasets demonstrate both the effectiveness and efficiency of the proposed method. Code and models are available at this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
