<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Verifi-Chain: A Credentials Verifier using Blockchain and IPFS. (arXiv:2307.05797v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05797">http://arxiv.org/abs/2307.05797</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05797] Verifi-Chain: A Credentials Verifier using Blockchain and IPFS](http://arxiv.org/abs/2307.05797) #secure</code></li>
<li>Summary: <p>Submitting fake certificates is a common problem in Southeast Asia, which
prevents qualified candidates from getting the jobs they deserve. When applying
for a job, students must provide academic credentials as proof of their
qualifications, acquired both inside and outside the classroom. Verifying
academic documents before hiring is crucial to prevent fraud. Employing
blockchain technology has the potential to address this issue. Blockchain
provides an electronic certificate that is tamper-proof and non-repudiable,
making it difficult for students to manipulate their academic credentials. This
paper presents a prototype for an academic credential verification model that
leverages the security features of blockchain and IPFS (Interplanetary File
System). Certificates are temporarily stored in a database before being
transferred to IPFS, where a unique hash code is generated using a hashing
algorithm. This hash code serves as the certificate's unique identity and is
stored in the blockchain nodes. Companies can verify an applicant's credentials
by searching for the applicant and accessing their already verified
certificates. Utilizing IPFS as a middleman storage platform lowers the
expenses of directly storing massive data on the blockchain. To sum it up, the
proposed solution would make the process of certificate verification more
efficient, secure, and cost-effective. It would save time and resources that
would otherwise be used to manually verify certificates.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: SepHRNet: Generating High-Resolution Crop Maps from Remote Sensing imagery using HRNet with Separable Convolution. (arXiv:2307.05700v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05700">http://arxiv.org/abs/2307.05700</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05700] SepHRNet: Generating High-Resolution Crop Maps from Remote Sensing imagery using HRNet with Separable Convolution](http://arxiv.org/abs/2307.05700) #security</code></li>
<li>Summary: <p>The accurate mapping of crop production is crucial for ensuring food
security, effective resource management, and sustainable agricultural
practices. One way to achieve this is by analyzing high-resolution satellite
imagery. Deep Learning has been successful in analyzing images, including
remote sensing imagery. However, capturing intricate crop patterns is
challenging due to their complexity and variability. In this paper, we propose
a novel Deep learning approach that integrates HRNet with Separable
Convolutional layers to capture spatial patterns and Self-attention to capture
temporal patterns of the data. The HRNet model acts as a backbone and extracts
high-resolution features from crop images. Spatially separable convolution in
the shallow layers of the HRNet model captures intricate crop patterns more
effectively while reducing the computational cost. The multi-head attention
mechanism captures long-term temporal dependencies from the encoded vector
representation of the images. Finally, a CNN decoder generates a crop map from
the aggregated representation. Adaboost is used on top of this to further
improve accuracy. The proposed algorithm achieves a high classification
accuracy of 97.5\% and IoU of 55.2\% in generating crop maps. We evaluate the
performance of our pipeline on the Zuericrop dataset and demonstrate that our
results outperform state-of-the-art models such as U-Net++, ResNet50, VGG19,
InceptionV3, DenseNet, and EfficientNet. This research showcases the potential
of Deep Learning for Earth Observation Systems.
</p></li>
</ul>

<h3>Title: A New Dataset and Comparative Study for Aphid Cluster Detection. (arXiv:2307.05929v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05929">http://arxiv.org/abs/2307.05929</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05929] A New Dataset and Comparative Study for Aphid Cluster Detection](http://arxiv.org/abs/2307.05929) #security</code></li>
<li>Summary: <p>Aphids are one of the main threats to crops, rural families, and global food
security. Chemical pest control is a necessary component of crop production for
maximizing yields, however, it is unnecessary to apply the chemical approaches
to the entire fields in consideration of the environmental pollution and the
cost. Thus, accurately localizing the aphid and estimating the infestation
level is crucial to the precise local application of pesticides. Aphid
detection is very challenging as each individual aphid is really small and all
aphids are crowded together as clusters. In this paper, we propose to estimate
the infection level by detecting aphid clusters. We have taken millions of
images in the sorghum fields, manually selected 5,447 images that contain
aphids, and annotated each aphid cluster in the image. To use these images for
machine learning models, we crop the images into patches and created a labeled
dataset with over 151,000 image patches. Then, we implement and compare the
performance of four state-of-the-art object detection models.
</p></li>
</ul>

<h3>Title: CloudSec: An Extensible Automated Reasoning Framework for Cloud Security Policies. (arXiv:2307.05745v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05745">http://arxiv.org/abs/2307.05745</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05745] CloudSec: An Extensible Automated Reasoning Framework for Cloud Security Policies](http://arxiv.org/abs/2307.05745) #security</code></li>
<li>Summary: <p>Users increasingly create, manage and share digital resources, including
sensitive data, via cloud platforms and APIs. Platforms encode the rules
governing access to these resources, referred to as \textit{security policies},
using different systems and semantics. As the number of resources and rules
grows, the challenge of reasoning about them collectively increases. Formal
methods tools, such as Satisfiability Modulo Theories (SMT) libraries, can be
used to automate the analysis of security policies, but several challenges,
including the highly specialized, technical nature of the libraries as well as
their variable performance, prevent their broad adoption in cloud systems. In
this paper, we present CloudSec, an extensible framework for reasoning about
cloud security policies using SMT. CloudSec provides a high-level API that can
be used to encode different types of cloud security policies without knowledge
of SMT. Further, it is trivial for applications written with CloudSec to
utilize and switch between different SMT libraries such as Z3 and CVC5. We
demonstrate the use of CloudSec to analyze security policies in Tapis, a
cloud-based API for distributed computational research used by tens of
thousands of researchers.
</p></li>
</ul>

<h3>Title: Formal and Fuzzing Amplification: Targeting Vulnerability Detection in 5G and Beyond. (arXiv:2307.05758v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05758">http://arxiv.org/abs/2307.05758</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05758] Formal and Fuzzing Amplification: Targeting Vulnerability Detection in 5G and Beyond](http://arxiv.org/abs/2307.05758) #security</code></li>
<li>Summary: <p>Softwarization and virtualization in 5G and beyond require rigorous testing
against vulnerabilities and unintended emergent behaviors for critical
infrastructure and network security assurance. Formal methods operates
efficiently in protocol-level abstract specification models, and fuzz testing
offers comprehensive experimental evaluation of system implementations. In this
paper, we propose a novel framework that leverages the respective advantages
and coverage of both formal and fuzzing methods to efficiently detect
vulnerabilities from protocol logic to implementation stacks hierarchically.
The detected attack traces from the formal verification results in critical
protocols guide the case generation of fuzz testing, and the feedbacks from
fuzz testing further broaden the scope of the formal verification. We examine
the proposed framework with the 5G Non Standard-Alone (NSA) security processes,
focusing on the Radio Resource Control (RRC) connection process. We first
identify protocol-level vulnerabilities of user credentials via formal methods.
Following this, we implement bit-level fuzzing to evaluate potential impacts
and risks of integrity-vulnerable identifier variation. Concurrently, we
conduct command-level mutation-based fuzzing by fixing the assumption
identifier to assess the potential impacts and risks of
confidentiality-vulnerable identifiers. During this approach, we established 1
attack model and detected 53 vulnerabilities. The vulnerabilities identified
used to fortify protocol-level assumptions could further refine search space
for the following detection cycles. Consequently, it addresses the prevalent
scalability challenges in detecting vulnerabilities and unintended emergent
behaviors in large-scale systems in 5G and beyond.
</p></li>
</ul>

<h3>Title: Introducing Packet-Level Analysis in Programmable Data Planes to Advance Network Intrusion Detection. (arXiv:2307.05936v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05936">http://arxiv.org/abs/2307.05936</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05936] Introducing Packet-Level Analysis in Programmable Data Planes to Advance Network Intrusion Detection](http://arxiv.org/abs/2307.05936) #security</code></li>
<li>Summary: <p>Programmable data planes offer precise control over the low-level processing
steps applied to network packets, serving as a valuable tool for analysing
malicious flows in the field of intrusion detection. Albeit with limitations on
physical resources and capabilities, they allow for the efficient extraction of
detailed traffic information, which can then be utilised by Machine Learning
(ML) algorithms responsible for identifying security threats. In addressing
resource constraints, existing solutions in the literature rely on compressing
network data through the collection of statistical traffic features in the data
plane. While this compression saves memory resources in switches and minimises
the burden on the control channel between the data and the control plane, it
also results in a loss of information available to the Network Intrusion
Detection System (NIDS), limiting access to packet payload, categorical
features, and the semantic understanding of network communications, such as the
behaviour of packets within traffic flows. This paper proposes P4DDLe, a
framework that exploits the flexibility of P4-based programmable data planes
for packet-level feature extraction and pre-processing. P4DDLe leverages the
programmable data plane to extract raw packet features from the network
traffic, categorical features included, and to organise them in a way that the
semantics of traffic flows is preserved. To minimise memory and control channel
overheads, P4DDLe selectively processes and filters packet-level data, so that
all and only the relevant features required by the NIDS are collected. The
experimental evaluation with recent Distributed Denial of Service (DDoS) attack
data demonstrates that the proposed approach is very efficient in collecting
compact and high-quality representations of network flows, ensuring precise
detection of DDoS attacks.
</p></li>
</ul>

<h3>Title: Security in Online Freelance Software Development: A case for Distributed Security Responsibility. (arXiv:2307.06066v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06066">http://arxiv.org/abs/2307.06066</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06066] Security in Online Freelance Software Development: A case for Distributed Security Responsibility](http://arxiv.org/abs/2307.06066) #security</code></li>
<li>Summary: <p>Secure software is a cornerstone to safe and resilient digital ecosystems. It
offers strong foundation to protect users' sensitive data and guard against
cyber-threats. The rapidly increasing landscape of digital economy has
encouraged developers from different socio-technical and socio-economic
backgrounds to join online freelance marketplaces. While, secure software
practices facilitate software developers in developing secure software, there
is paucity of research on how freelance developers adhere to security practices
and how they can be facilitated to improve their security behavior in
under-resourced environments. Moreover, freelance developers are often held
responsible for producing insecure code. In this position paper, we review
existing literature and argue for the case of distributed security
responsibilities in online freelance environment. We propose a research agenda
aimed at offering an organized and systematic effort by researchers to address
security needs and challenges of online freelance marketplaces. These include:
characterising software security and defining separation of responsibilities,
building trust in online freelance development communities, leveraging the
potential of online freelancing platforms in the promotion of secure software
development and building adaptive security interventions for online freelance
software development. The research has the potential to bring forth existing
security solutions to wider developer community and deliver substantial
benefits to the broader security ecosystem.
</p></li>
</ul>

<h3>Title: Transaction Fraud Detection via an Adaptive Graph Neural Network. (arXiv:2307.05633v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05633">http://arxiv.org/abs/2307.05633</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05633] Transaction Fraud Detection via an Adaptive Graph Neural Network](http://arxiv.org/abs/2307.05633) #security</code></li>
<li>Summary: <p>Many machine learning methods have been proposed to achieve accurate
transaction fraud detection, which is essential to the financial security of
individuals and banks. However, most existing methods leverage original
features only or require manual feature engineering. They lack the ability to
learn discriminative representations from transaction data. Moreover, criminals
often commit fraud by imitating cardholders' behaviors, which causes the poor
performance of existing detection models. In this paper, we propose an Adaptive
Sampling and Aggregation-based Graph Neural Network (ASA-GNN) that learns
discriminative representations to improve the performance of transaction fraud
detection. A neighbor sampling strategy is performed to filter noisy nodes and
supplement information for fraudulent nodes. Specifically, we leverage cosine
similarity and edge weights to adaptively select neighbors with similar
behavior patterns for target nodes and then find multi-hop neighbors for
fraudulent nodes. A neighbor diversity metric is designed by calculating the
entropy among neighbors to tackle the camouflage issue of fraudsters and
explicitly alleviate the over-smoothing phenomena. Extensive experiments on
three real financial datasets demonstrate that the proposed method ASA-GNN
outperforms state-of-the-art ones.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: R\'enyiTester: A Variational Approach to Testing Differential Privacy. (arXiv:2307.05608v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05608">http://arxiv.org/abs/2307.05608</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05608] R\'enyiTester: A Variational Approach to Testing Differential Privacy](http://arxiv.org/abs/2307.05608) #privacy</code></li>
<li>Summary: <p>Governments and industries have widely adopted differential privacy as a
measure to protect users' sensitive data, creating the need for new
implementations of differentially private algorithms. In order to properly test
and audit these algorithms, a suite of tools for testing the property of
differential privacy is needed. In this work we expand this testing suite and
introduce R\'enyiTester, an algorithm that can verify if a mechanism is R\'enyi
differentially private. Our algorithm computes computes a lower bound of the
R\'enyi divergence between the distributions of a mechanism on neighboring
datasets, only requiring black-box access to samples from the audited
mechanism. We test this approach on a variety of pure and R\'enyi
differentially private mechanisms with diverse output spaces and show that
R\'enyiTester detects bugs in mechanisms' implementations and design flaws.
While detecting that a general mechanism is differentially private is known to
be NP hard, we empirically show that tools like R\'enyiTester provide a way for
researchers and engineers to decrease the risk of deploying mechanisms that
expose users' privacy.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: ObNoCs: Protecting Network-on-Chip Fabrics Against Reverse-Engineering Attacks. (arXiv:2307.05815v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05815">http://arxiv.org/abs/2307.05815</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05815] ObNoCs: Protecting Network-on-Chip Fabrics Against Reverse-Engineering Attacks](http://arxiv.org/abs/2307.05815) #protect</code></li>
<li>Summary: <p>Modern System-on-Chip designs typically use Network-on-Chip (NoC) fabrics to
implement coordination among integrated hardware blocks. An important class of
security vulnerabilities involves a rogue foundry reverse-engineering the NoC
topology and routing logic. In this paper, we develop an infrastructure,
$\obnocs$, for protecting NoC fabrics against such attacks. $\obnocs$
systematically replaces router connections with switches that can be programmed
after fabrication to induce the desired topology. Our approach provides
provable redaction of NoC functionality: switch configurations induce a large
number of legal topologies, only one of which corresponds to the intended
topology. We implement the $\obnocs$ methodology on Intel
Quartus\texttrademark\ Platform, and experimental results on realistic SoC
designs show that the architecture incurs minimal overhead in power, resource
utilization, and system latency.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: RidgeBase: A Cross-Sensor Multi-Finger Contactless Fingerprint Dataset. (arXiv:2307.05563v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05563">http://arxiv.org/abs/2307.05563</a></li>
<li>Code URL: <a href="https://github.com/bhavinjawade/fingerprintcameraapp">https://github.com/bhavinjawade/fingerprintcameraapp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05563] RidgeBase: A Cross-Sensor Multi-Finger Contactless Fingerprint Dataset](http://arxiv.org/abs/2307.05563) #attack</code></li>
<li>Summary: <p>Contactless fingerprint matching using smartphone cameras can alleviate major
challenges of traditional fingerprint systems including hygienic acquisition,
portability and presentation attacks. However, development of practical and
robust contactless fingerprint matching techniques is constrained by the
limited availability of large scale real-world datasets. To motivate further
advances in contactless fingerprint matching across sensors, we introduce the
RidgeBase benchmark dataset. RidgeBase consists of more than 15,000 contactless
and contact-based fingerprint image pairs acquired from 88 individuals under
different background and lighting conditions using two smartphone cameras and
one flatbed contact sensor. Unlike existing datasets, RidgeBase is designed to
promote research under different matching scenarios that include Single Finger
Matching and Multi-Finger Matching for both contactless- to-contactless (CL2CL)
and contact-to-contactless (C2CL) verification and identification. Furthermore,
due to the high intra-sample variance in contactless fingerprints belonging to
the same finger, we propose a set-based matching protocol inspired by the
advances in facial recognition datasets. This protocol is specifically designed
for pragmatic contactless fingerprint matching that can account for variances
in focus, polarity and finger-angles. We report qualitative and quantitative
baseline results for different protocols using a COTS fingerprint matcher
(Verifinger) and a Deep CNN based approach on the RidgeBase dataset. The
dataset can be downloaded here:
https://www.buffalo.edu/cubs/research/datasets/ridgebase-benchmark-dataset.html
</p></li>
</ul>

<h3>Title: SoK: Comparing Different Membership Inference Attacks with a Comprehensive Benchmark. (arXiv:2307.06123v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06123">http://arxiv.org/abs/2307.06123</a></li>
<li>Code URL: <a href="https://github.com/mibench/mibench.github.io">https://github.com/mibench/mibench.github.io</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06123] SoK: Comparing Different Membership Inference Attacks with a Comprehensive Benchmark](http://arxiv.org/abs/2307.06123) #attack</code></li>
<li>Summary: <p>Membership inference (MI) attacks threaten user privacy through determining
if a given data example has been used to train a target model. However, it has
been increasingly recognized that the "comparing different MI attacks"
methodology used in the existing works has serious limitations. Due to these
limitations, we found (through the experiments in this work) that some
comparison results reported in the literature are quite misleading. In this
paper, we seek to develop a comprehensive benchmark for comparing different MI
attacks, called MIBench, which consists not only the evaluation metrics, but
also the evaluation scenarios. And we design the evaluation scenarios from four
perspectives: the distance distribution of data samples in the target dataset,
the distance between data samples of the target dataset, the differential
distance between two datasets (i.e., the target dataset and a generated dataset
with only nonmembers), and the ratio of the samples that are made no inferences
by an MI attack. The evaluation metrics consist of ten typical evaluation
metrics. We have identified three principles for the proposed "comparing
different MI attacks" methodology, and we have designed and implemented the
MIBench benchmark with 84 evaluation scenarios for each dataset. In total, we
have used our benchmark to fairly and systematically compare 15
state-of-the-art MI attack algorithms across 588 evaluation scenarios, and
these evaluation scenarios cover 7 widely used datasets and 7 representative
types of models. All codes and evaluations of MIBench are publicly available at
https://github.com/MIBench/MIBench.github.io/blob/main/README.md.
</p></li>
</ul>

<h3>Title: Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep Learning. (arXiv:2307.05772v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05772">http://arxiv.org/abs/2307.05772</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05772] Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep Learning](http://arxiv.org/abs/2307.05772) #attack</code></li>
<li>Summary: <p>Machine learning is increasingly deployed in safety-critical domains where
robustness against adversarial attacks is crucial and erroneous predictions
could lead to potentially catastrophic consequences. This highlights the need
for learning systems to be equipped with the means to determine a model's
confidence in its prediction and the epistemic uncertainty associated with it,
'to know when a model does not know'. In this paper, we propose a novel
Random-Set Convolutional Neural Network (RS-CNN) for classification which
predicts belief functions rather than probability vectors over the set of
classes, using the mathematics of random sets, i.e., distributions over the
power set of the sample space. Based on the epistemic deep learning approach,
random-set models are capable of representing the 'epistemic' uncertainty
induced in machine learning by limited training sets. We estimate epistemic
uncertainty by approximating the size of credal sets associated with the
predicted belief functions, and experimentally demonstrate how our approach
outperforms competing uncertainty-aware approaches in a classical evaluation
setting. The performance of RS-CNN is best demonstrated on OOD samples where it
manages to capture the true prediction while standard CNNs fail.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Active Learning for Video Classification with Frame Level Queries. (arXiv:2307.05587v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05587">http://arxiv.org/abs/2307.05587</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05587] Active Learning for Video Classification with Frame Level Queries](http://arxiv.org/abs/2307.05587) #robust</code></li>
<li>Summary: <p>Deep learning algorithms have pushed the boundaries of computer vision
research and have depicted commendable performance in a variety of
applications. However, training a robust deep neural network necessitates a
large amount of labeled training data, acquiring which involves significant
time and human effort. This problem is even more serious for an application
like video classification, where a human annotator has to watch an entire video
end-to-end to furnish a label. Active learning algorithms automatically
identify the most informative samples from large amounts of unlabeled data;
this tremendously reduces the human annotation effort in inducing a machine
learning model, as only the few samples that are identified by the algorithm,
need to be labeled manually. In this paper, we propose a novel active learning
framework for video classification, with the goal of further reducing the
labeling onus on the human annotators. Our framework identifies a batch of
exemplar videos, together with a set of informative frames for each video; the
human annotator needs to merely review the frames and provide a label for each
video. This involves much less manual work than watching the complete video to
come up with a label. We formulate a criterion based on uncertainty and
diversity to identify the informative videos and exploit representative
sampling techniques to extract a set of exemplar frames from each video. To the
best of our knowledge, this is the first research effort to develop an active
learning framework for video classification, where the annotators need to
inspect only a few frames to produce a label, rather than watching the
end-to-end video.
</p></li>
</ul>

<h3>Title: MoP-CLIP: A Mixture of Prompt-Tuned CLIP Models for Domain Incremental Learning. (arXiv:2307.05707v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05707">http://arxiv.org/abs/2307.05707</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05707] MoP-CLIP: A Mixture of Prompt-Tuned CLIP Models for Domain Incremental Learning](http://arxiv.org/abs/2307.05707) #robust</code></li>
<li>Summary: <p>Despite the recent progress in incremental learning, addressing catastrophic
forgetting under distributional drift is still an open and important problem.
Indeed, while state-of-the-art domain incremental learning (DIL) methods
perform satisfactorily within known domains, their performance largely degrades
in the presence of novel domains. This limitation hampers their
generalizability, and restricts their scalability to more realistic settings
where train and test data are drawn from different distributions. To address
these limitations, we present a novel DIL approach based on a mixture of
prompt-tuned CLIP models (MoP-CLIP), which generalizes the paradigm of
S-Prompting to handle both in-distribution and out-of-distribution data at
inference. In particular, at the training stage we model the features
distribution of every class in each domain, learning individual text and visual
prompts to adapt to a given domain. At inference, the learned distributions
allow us to identify whether a given test sample belongs to a known domain,
selecting the correct prompt for the classification task, or from an unseen
domain, leveraging a mixture of the prompt-tuned CLIP models. Our empirical
evaluation reveals the poor performance of existing DIL methods under domain
shift, and suggests that the proposed MoP-CLIP performs competitively in the
standard DIL settings while outperforming state-of-the-art methods in OOD
scenarios. These results demonstrate the superiority of MoP-CLIP, offering a
robust and general solution to the problem of domain incremental learning.
</p></li>
</ul>

<h3>Title: Multi-Object Tracking as Attention Mechanism. (arXiv:2307.05874v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05874">http://arxiv.org/abs/2307.05874</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05874] Multi-Object Tracking as Attention Mechanism](http://arxiv.org/abs/2307.05874) #robust</code></li>
<li>Summary: <p>We propose a conceptually simple and thus fast multi-object tracking (MOT)
model that does not require any attached modules, such as the Kalman filter,
Hungarian algorithm, transformer blocks, or graph networks. Conventional MOT
models are built upon the multi-step modules listed above, and thus the
computational cost is high. Our proposed end-to-end MOT model,
\textit{TicrossNet}, is composed of a base detector and a cross-attention
module only. As a result, the overhead of tracking does not increase
significantly even when the number of instances ($N_t$) increases. We show that
TicrossNet runs \textit{in real-time}; specifically, it achieves 32.6 FPS on
MOT17 and 31.0 FPS on MOT20 (Tesla V100), which includes as many as $>$100
instances per frame. We also demonstrate that TicrossNet is robust to $N_t$;
thus, it does not have to change the size of the base detector, depending on
$N_t$, as is often done by other models for real-time processing.
</p></li>
</ul>

<h3>Title: Rectifying Noisy Labels with Sequential Prior: Multi-Scale Temporal Feature Affinity Learning for Robust Video Segmentation. (arXiv:2307.05898v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05898">http://arxiv.org/abs/2307.05898</a></li>
<li>Code URL: <a href="https://github.com/beileicui/ms-tfal">https://github.com/beileicui/ms-tfal</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05898] Rectifying Noisy Labels with Sequential Prior: Multi-Scale Temporal Feature Affinity Learning for Robust Video Segmentation](http://arxiv.org/abs/2307.05898) #robust</code></li>
<li>Summary: <p>Noisy label problems are inevitably in existence within medical image
segmentation causing severe performance degradation. Previous segmentation
methods for noisy label problems only utilize a single image while the
potential of leveraging the correlation between images has been overlooked.
Especially for video segmentation, adjacent frames contain rich contextual
information beneficial in cognizing noisy labels. Based on two insights, we
propose a Multi-Scale Temporal Feature Affinity Learning (MS-TFAL) framework to
resolve noisy-labeled medical video segmentation issues. First, we argue the
sequential prior of videos is an effective reference, i.e., pixel-level
features from adjacent frames are close in distance for the same class and far
in distance otherwise. Therefore, Temporal Feature Affinity Learning (TFAL) is
devised to indicate possible noisy labels by evaluating the affinity between
pixels in two adjacent frames. We also notice that the noise distribution
exhibits considerable variations across video, image, and pixel levels. In this
way, we introduce Multi-Scale Supervision (MSS) to supervise the network from
three different perspectives by re-weighting and refining the samples. This
design enables the network to concentrate on clean samples in a coarse-to-fine
manner. Experiments with both synthetic and real-world label noise demonstrate
that our method outperforms recent state-of-the-art robust segmentation
approaches. Code is available at https://github.com/BeileiCui/MS-TFAL.
</p></li>
</ul>

<h3>Title: Single Domain Generalization via Normalised Cross-correlation Based Convolutions. (arXiv:2307.05901v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05901">http://arxiv.org/abs/2307.05901</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05901] Single Domain Generalization via Normalised Cross-correlation Based Convolutions](http://arxiv.org/abs/2307.05901) #robust</code></li>
<li>Summary: <p>Deep learning techniques often perform poorly in the presence of domain
shift, where the test data follows a different distribution than the training
data. The most practically desirable approach to address this issue is Single
Domain Generalization (S-DG), which aims to train robust models using data from
a single source. Prior work on S-DG has primarily focused on using data
augmentation techniques to generate diverse training data. In this paper, we
explore an alternative approach by investigating the robustness of linear
operators, such as convolution and dense layers commonly used in deep learning.
We propose a novel operator called XCNorm that computes the normalized
cross-correlation between weights and an input feature patch. This approach is
invariant to both affine shifts and changes in energy within a local feature
patch and eliminates the need for commonly used non-linear activation
functions. We show that deep neural networks composed of this operator are
robust to common semantic distribution shifts. Furthermore, our empirical
results on single-domain generalization benchmarks demonstrate that our
proposed technique performs comparably to the state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Unsupervised Optical Flow Estimation with Dynamic Timing Representation for Spike Camera. (arXiv:2307.06003v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06003">http://arxiv.org/abs/2307.06003</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06003] Unsupervised Optical Flow Estimation with Dynamic Timing Representation for Spike Camera](http://arxiv.org/abs/2307.06003) #robust</code></li>
<li>Summary: <p>Efficiently selecting an appropriate spike stream data length to extract
precise information is the key to the spike vision tasks. To address this
issue, we propose a dynamic timing representation for spike streams. Based on
multi-layers architecture, it applies dilated convolutions on temporal
dimension to extract features on multi-temporal scales with few parameters. And
we design layer attention to dynamically fuse these features. Moreover, we
propose an unsupervised learning method for optical flow estimation in a
spike-based manner to break the dependence on labeled data. In addition, to
verify the robustness, we also build a spike-based synthetic validation dataset
for extreme scenarios in autonomous driving, denoted as SSES dataset. It
consists of various corner cases. Experiments show that our method can predict
optical flow from spike streams in different high-speed scenes, including real
scenes. For instance, our method gets $15\%$ and $19\%$ error reduction from
the best spike-based work, SCFlow, in $\Delta t=10$ and $\Delta t=20$
respectively which are the same settings as the previous works.
</p></li>
</ul>

<h3>Title: MMBench: Is Your Multi-modal Model an All-around Player?. (arXiv:2307.06281v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06281">http://arxiv.org/abs/2307.06281</a></li>
<li>Code URL: <a href="https://github.com/InternLM/opencompass">https://github.com/InternLM/opencompass</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06281] MMBench: Is Your Multi-modal Model an All-around Player?](http://arxiv.org/abs/2307.06281) #robust</code></li>
<li>Summary: <p>Large vision-language models have recently achieved remarkable progress,
exhibiting great perception and reasoning abilities concerning visual
information. However, how to effectively evaluate these large vision-language
models remains a major obstacle, hindering future model development.
Traditional benchmarks like VQAv2 or COCO Caption provide quantitative
performance measurements but suffer from a lack of fine-grained ability
assessment and non-robust evaluation metrics. Recent subjective benchmarks,
such as OwlEval, offer comprehensive evaluations of a model's abilities by
incorporating human labor, but they are not scalable and display significant
bias. In response to these challenges, we propose MMBench, a novel
multi-modality benchmark. MMBench methodically develops a comprehensive
evaluation pipeline, primarily comprised of two elements. The first element is
a meticulously curated dataset that surpasses existing similar benchmarks in
terms of the number and variety of evaluation questions and abilities. The
second element introduces a novel CircularEval strategy and incorporates the
use of ChatGPT. This implementation is designed to convert free-form
predictions into pre-defined choices, thereby facilitating a more robust
evaluation of the model's predictions. MMBench is a systematically-designed
objective benchmark for robustly evaluating the various abilities of
vision-language models. We hope MMBench will assist the research community in
better evaluating their models and encourage future advancements in this
domain. Project page: https://opencompass.org.cn/mmbench.
</p></li>
</ul>

<h3>Title: Towards Robust and Efficient Continual Language Learning. (arXiv:2307.05741v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05741">http://arxiv.org/abs/2307.05741</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05741] Towards Robust and Efficient Continual Language Learning](http://arxiv.org/abs/2307.05741) #robust</code></li>
<li>Summary: <p>As the application space of language models continues to evolve, a natural
question to ask is how we can quickly adapt models to new tasks. We approach
this classic question from a continual learning perspective, in which we aim to
continue fine-tuning models trained on past tasks on new tasks, with the goal
of "transferring" relevant knowledge. However, this strategy also runs the risk
of doing more harm than good, i.e., negative transfer. In this paper, we
construct a new benchmark of task sequences that target different possible
transfer scenarios one might face, such as a sequence of tasks with high
potential of positive transfer, high potential for negative transfer, no
expected effect, or a mixture of each. An ideal learner should be able to
maximally exploit information from all tasks that have any potential for
positive transfer, while also avoiding the negative effects of any distracting
tasks that may confuse it. We then propose a simple, yet effective, learner
that satisfies many of our desiderata simply by leveraging a selective strategy
for initializing new models from past task checkpoints. Still, limitations
remain, and we hope this benchmark can help the community to further build and
analyze such learners.
</p></li>
</ul>

<h3>Title: Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes. (arXiv:2307.05862v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05862">http://arxiv.org/abs/2307.05862</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05862] Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes](http://arxiv.org/abs/2307.05862) #robust</code></li>
<li>Summary: <p>Machine learning is traditionally studied at the model level: researchers
measure and improve the accuracy, robustness, bias, efficiency, and other
dimensions of specific models. In practice, the societal impact of machine
learning is determined by the surrounding context of machine learning
deployments. To capture this, we introduce ecosystem-level analysis: rather
than analyzing a single model, we consider the collection of models that are
deployed in a given context. For example, ecosystem-level analysis in hiring
recognizes that a job candidate's outcomes are not only determined by a single
hiring algorithm or firm but instead by the collective decisions of all the
firms they applied to. Across three modalities (text, images, speech) and 11
datasets, we establish a clear trend: deployed machine learning is prone to
systemic failure, meaning some users are exclusively misclassified by all
models available. Even when individual models improve at the population level
over time, we find these improvements rarely reduce the prevalence of systemic
failure. Instead, the benefits of these improvements predominantly accrue to
individuals who are already correctly classified by other models. In light of
these trends, we consider medical imaging for dermatology where the costs of
systemic failure are especially high. While traditional analyses reveal racial
performance disparities for both models and humans, ecosystem-level analysis
reveals new forms of racial disparity in model predictions that do not present
in human predictions. These examples demonstrate ecosystem-level analysis has
unique strengths for characterizing the societal impact of machine learning.
</p></li>
</ul>

<h3>Title: PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks. (arXiv:2307.05891v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05891">http://arxiv.org/abs/2307.05891</a></li>
<li>Code URL: <a href="https://github.com/ianchar/gpide">https://github.com/ianchar/gpide</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05891] PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks](http://arxiv.org/abs/2307.05891) #robust</code></li>
<li>Summary: <p>Deep reinforcement learning (RL) has shown immense potential for learning to
control systems through data alone. However, one challenge deep RL faces is
that the full state of the system is often not observable. When this is the
case, the policy needs to leverage the history of observations to infer the
current state. At the same time, differences between the training and testing
environments makes it critical for the policy not to overfit to the sequence of
observations it sees at training time. As such, there is an important balancing
act between having the history encoder be flexible enough to extract relevant
information, yet be robust to changes in the environment. To strike this
balance, we look to the PID controller for inspiration. We assert the PID
controller's success shows that only summing and differencing are needed to
accumulate information over time for many control tasks. Following this
principle, we propose two architectures for encoding history: one that directly
uses PID features and another that extends these core ideas and can be used in
arbitrary control tasks. When compared with prior approaches, our encoders
produce policies that are often more robust and achieve better performance on a
variety of tracking tasks. Going beyond tracking tasks, our policies achieve
1.7x better performance on average over previous state-of-the-art methods on a
suite of high dimensional control tasks.
</p></li>
</ul>

<h3>Title: Function-Space Regularization for Deep Bayesian Classification. (arXiv:2307.06055v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06055">http://arxiv.org/abs/2307.06055</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06055] Function-Space Regularization for Deep Bayesian Classification](http://arxiv.org/abs/2307.06055) #robust</code></li>
<li>Summary: <p>Bayesian deep learning approaches assume model parameters to be latent random
variables and infer posterior distributions to quantify uncertainty, increase
safety and trust, and prevent overconfident and unpredictable behavior.
However, weight-space priors are model-specific, can be difficult to interpret
and are hard to specify. Instead, we apply a Dirichlet prior in predictive
space and perform approximate function-space variational inference. To this
end, we interpret conventional categorical predictions from stochastic neural
network classifiers as samples from an implicit Dirichlet distribution. By
adapting the inference, the same function-space prior can be combined with
different models without affecting model architecture or size. We illustrate
the flexibility and efficacy of such a prior with toy experiments and
demonstrate scalability, improved uncertainty quantification and adversarial
robustness with large-scale image classification experiments.
</p></li>
</ul>

<h3>Title: Learning Stochastic Dynamical Systems as an Implicit Regularization with Graph Neural Networks. (arXiv:2307.06097v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06097">http://arxiv.org/abs/2307.06097</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06097] Learning Stochastic Dynamical Systems as an Implicit Regularization with Graph Neural Networks](http://arxiv.org/abs/2307.06097) #robust</code></li>
<li>Summary: <p>Stochastic Gumbel graph networks are proposed to learn high-dimensional time
series, where the observed dimensions are often spatially correlated. To that
end, the observed randomness and spatial-correlations are captured by learning
the drift and diffusion terms of the stochastic differential equation with a
Gumble matrix embedding, respectively. In particular, this novel framework
enables us to investigate the implicit regularization effect of the noise terms
in S-GGNs. We provide a theoretical guarantee for the proposed S-GGNs by
deriving the difference between the two corresponding loss functions in a small
neighborhood of weight. Then, we employ Kuramoto's model to generate data for
comparing the spectral density from the Hessian Matrix of the two loss
functions. Experimental results on real-world data, demonstrate that S-GGNs
exhibit superior convergence, robustness, and generalization, compared with
state-of-the-arts.
</p></li>
</ul>

<h3>Title: Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation. (arXiv:2307.06333v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06333">http://arxiv.org/abs/2307.06333</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06333] Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation](http://arxiv.org/abs/2307.06333) #robust</code></li>
<li>Summary: <p>Policies often fail due to distribution shift -- changes in the state and
reward that occur when a policy is deployed in new environments. Data
augmentation can increase robustness by making the model invariant to
task-irrelevant changes in the agent's observation. However, designers don't
know which concepts are irrelevant a priori, especially when different end
users have different preferences about how the task is performed. We propose an
interactive framework to leverage feedback directly from the user to identify
personalized task-irrelevant concepts. Our key idea is to generate
counterfactual demonstrations that allow users to quickly identify possible
task-relevant and irrelevant concepts. The knowledge of task-irrelevant
concepts is then used to perform data augmentation and thus obtain a policy
adapted to personalized user objectives. We present experiments validating our
framework on discrete and continuous control tasks with real human users. Our
method (1) enables users to better understand agent failure, (2) reduces the
number of demonstrations required for fine-tuning, and (3) aligns the agent to
individual user task preferences.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Face Image Quality Enhancement Study for Face Recognition. (arXiv:2307.05534v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05534">http://arxiv.org/abs/2307.05534</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05534] Face Image Quality Enhancement Study for Face Recognition](http://arxiv.org/abs/2307.05534) #biometric</code></li>
<li>Summary: <p>Unconstrained face recognition is an active research area among computer
vision and biometric researchers for many years now. Still the problem of face
recognition in low quality photos has not been well-studied so far. In this
paper, we explore the face recognition performance on low quality photos, and
we try to improve the accuracy in dealing with low quality face images. We
assemble a large database with low quality photos, and examine the performance
of face recognition algorithms for three different quality sets. Using
state-of-the-art facial image enhancement approaches, we explore the face
recognition performance for the enhanced face images. To perform this without
experimental bias, we have developed a new protocol for recognition with low
quality face photos and validate the performance experimentally. Our designed
protocol for face recognition with low quality face images can be useful to
other researchers. Moreover, experiment results show some of the challenging
aspects of this problem.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Deep Learning of Crystalline Defects from TEM images: A Solution for the Problem of "Never Enough Training Data". (arXiv:2307.06322v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06322">http://arxiv.org/abs/2307.06322</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06322] Deep Learning of Crystalline Defects from TEM images: A Solution for the Problem of "Never Enough Training Data"](http://arxiv.org/abs/2307.06322) #extraction</code></li>
<li>Summary: <p>Crystalline defects, such as line-like dislocations, play an important role
for the performance and reliability of many metallic devices. Their interaction
and evolution still poses a multitude of open questions to materials science
and materials physics. In-situ TEM experiments can provide important insights
into how dislocations behave and move. During such experiments, the dislocation
microstructure is captured in form of videos. The analysis of individual video
frames can provide useful insights but is limited by the capabilities of
automated identification, digitization, and quantitative extraction of the
dislocations as curved objects. The vast amount of data also makes manual
annotation very time consuming, thereby limiting the use of Deep
Learning-based, automated image analysis and segmentation of the dislocation
microstructure. In this work, a parametric model for generating synthetic
training data for segmentation of dislocations is developed. Even though domain
scientists might dismiss synthetic training images sometimes as too artificial,
our findings show that they can result in superior performance, particularly
regarding the generalizing of the Deep Learning models with respect to
different microstructures and imaging conditions. Additionally, we propose an
enhanced deep learning method optimized for segmenting overlapping or
intersecting dislocation lines. Upon testing this framework on four distinct
real datasets, we find that our synthetic training data are able to yield
high-quality results also on real images-even more so if fine-tune on a few
real images was done.
</p></li>
</ul>

<h3>Title: Automatic Coding at Scale: Design and Deployment of a Nationwide System for Normalizing Referrals in the Chilean Public Healthcare System. (arXiv:2307.05560v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05560">http://arxiv.org/abs/2307.05560</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05560] Automatic Coding at Scale: Design and Deployment of a Nationwide System for Normalizing Referrals in the Chilean Public Healthcare System](http://arxiv.org/abs/2307.05560) #extraction</code></li>
<li>Summary: <p>The disease coding task involves assigning a unique identifier from a
controlled vocabulary to each disease mentioned in a clinical document. This
task is relevant since it allows information extraction from unstructured data
to perform, for example, epidemiological studies about the incidence and
prevalence of diseases in a determined context. However, the manual coding
process is subject to errors as it requires medical personnel to be competent
in coding rules and terminology. In addition, this process consumes a lot of
time and energy, which could be allocated to more clinically relevant tasks.
These difficulties can be addressed by developing computational systems that
automatically assign codes to diseases. In this way, we propose a two-step
system for automatically coding diseases in referrals from the Chilean public
healthcare system. Specifically, our model uses a state-of-the-art NER model
for recognizing disease mentions and a search engine system based on
Elasticsearch for assigning the most relevant codes associated with these
disease mentions. The system's performance was evaluated on referrals manually
coded by clinical experts. Our system obtained a MAP score of 0.63 for the
subcategory level and 0.83 for the category level, close to the best-performing
models in the literature. This system could be a support tool for health
professionals, optimizing the coding and management process. Finally, to
guarantee reproducibility, we publicly release the code of our models and
experiments.
</p></li>
</ul>

<h3>Title: Event Extraction as Question Generation and Answering. (arXiv:2307.05567v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05567">http://arxiv.org/abs/2307.05567</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05567] Event Extraction as Question Generation and Answering](http://arxiv.org/abs/2307.05567) #extraction</code></li>
<li>Summary: <p>Recent work on Event Extraction has reframed the task as Question Answering
(QA), with promising results. The advantage of this approach is that it
addresses the error propagation issue found in traditional token-based
classification approaches by directly predicting event arguments without
extracting candidates first. However, the questions are typically based on
fixed templates and they rarely leverage contextual information such as
relevant arguments. In addition, prior QA-based approaches have difficulty
handling cases where there are multiple arguments for the same role. In this
paper, we propose QGA-EE, which enables a Question Generation (QG) model to
generate questions that incorporate rich contextual information instead of
using fixed templates. We also propose dynamic templates to assist the training
of QG model. Experiments show that QGA-EE outperforms all prior
single-task-based models on the ACE05 English dataset.
</p></li>
</ul>

<h3>Title: Relational Extraction on Wikipedia Tables using Convolutional and Memory Networks. (arXiv:2307.05827v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05827">http://arxiv.org/abs/2307.05827</a></li>
<li>Code URL: <a href="https://github.com/simpleparadox/re_656">https://github.com/simpleparadox/re_656</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05827] Relational Extraction on Wikipedia Tables using Convolutional and Memory Networks](http://arxiv.org/abs/2307.05827) #extraction</code></li>
<li>Summary: <p>Relation extraction (RE) is the task of extracting relations between entities
in text. Most RE methods extract relations from free-form running text and
leave out other rich data sources, such as tables. We explore RE from the
perspective of applying neural methods on tabularly organized data. We
introduce a new model consisting of Convolutional Neural Network (CNN) and
Bidirectional-Long Short Term Memory (BiLSTM) network to encode entities and
learn dependencies among them, respectively. We evaluate our model on a large
and recent dataset and compare results with previous neural methods.
Experimental results show that our model consistently outperforms the previous
model for the task of relation extraction on tabular data. We perform
comprehensive error analyses and ablation study to show the contribution of
various components of our model. Finally, we discuss the usefulness and
trade-offs of our approach, and provide suggestions for fostering further
research.
</p></li>
</ul>

<h3>Title: Ashaar: Automatic Analysis and Generation of Arabic Poetry Using Deep Learning Approaches. (arXiv:2307.06218v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06218">http://arxiv.org/abs/2307.06218</a></li>
<li>Code URL: <a href="https://github.com/arbml/ashaar">https://github.com/arbml/ashaar</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06218] Ashaar: Automatic Analysis and Generation of Arabic Poetry Using Deep Learning Approaches](http://arxiv.org/abs/2307.06218) #extraction</code></li>
<li>Summary: <p>Poetry holds immense significance within the cultural and traditional fabric
of any nation. It serves as a vehicle for poets to articulate their emotions,
preserve customs, and convey the essence of their culture. Arabic poetry is no
exception, having played a cherished role in the heritage of the Arabic
community throughout history and maintaining its relevance in the present era.
Typically, comprehending Arabic poetry necessitates the expertise of a linguist
who can analyze its content and assess its quality. This paper presents the
introduction of a framework called \textit{Ashaar}
https://github.com/ARBML/Ashaar, which encompasses a collection of datasets and
pre-trained models designed specifically for the analysis and generation of
Arabic poetry. The pipeline established within our proposed approach
encompasses various aspects of poetry, such as meter, theme, and era
classification. It also incorporates automatic poetry diacritization, enabling
more intricate analyses like automated extraction of the \textit{Arudi} style.
Additionally, we explore the feasibility of generating conditional poetry
through the pre-training of a character-based GPT model. Furthermore, as part
of this endeavor, we provide four datasets: one for poetry generation, another
for diacritization, and two for Arudi-style prediction. These datasets aim to
facilitate research and development in the field of Arabic poetry by enabling
researchers and enthusiasts to delve into the nuances of this rich literary
tradition.
</p></li>
</ul>

<h3>Title: Time Moves Faster When There is Nothing You Anticipate: The Role of Time in MEV Rewards. (arXiv:2307.05814v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05814">http://arxiv.org/abs/2307.05814</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05814] Time Moves Faster When There is Nothing You Anticipate: The Role of Time in MEV Rewards](http://arxiv.org/abs/2307.05814) #extraction</code></li>
<li>Summary: <p>This study explores the intricacies of waiting games, a novel dynamic that
emerged with Ethereum's transition to a Proof-of-Stake (PoS)-based block
proposer selection protocol. Within this PoS framework, validators acquire a
distinct monopoly position during their assigned slots, given that block
proposal rights are set deterministically, contrasting with Proof-of-Work (PoW)
protocols. Consequently, validators have the power to delay block proposals,
stepping outside the honest validator specs, optimizing potential returns
through MEV payments. Nonetheless, this strategic behaviour introduces the risk
of orphaning if attestors fail to observe and vote on the block timely. Our
quantitative analysis of this waiting phenomenon and its associated risks
reveals an opportunity for enhanced MEV extraction, exceeding standard protocol
rewards, and providing sufficient incentives for validators to play the game.
Notably, our findings indicate that delayed proposals do not always result in
orphaning and orphaned blocks are not consistently proposed later than
non-orphaned ones. To further examine consensus stability under varying network
conditions, we adopt an agent-based simulation model tailored for PoS-Ethereum,
illustrating that consensus disruption will not be observed unless significant
delay strategies are adopted. Ultimately, this research offers valuable
insights into the advent of waiting games on Ethereum, providing a
comprehensive understanding of trade-offs and potential profits for validators
within the blockchain ecosystem.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: DBFed: Debiasing Federated Learning Framework based on Domain-Independent. (arXiv:2307.05582v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05582">http://arxiv.org/abs/2307.05582</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05582] DBFed: Debiasing Federated Learning Framework based on Domain-Independent](http://arxiv.org/abs/2307.05582) #federate</code></li>
<li>Summary: <p>As digital transformation continues, enterprises are generating, managing,
and storing vast amounts of data, while artificial intelligence technology is
rapidly advancing. However, it brings challenges in information security and
data security. Data security refers to the protection of digital information
from unauthorized access, damage, theft, etc. throughout its entire life cycle.
With the promulgation and implementation of data security laws and the emphasis
on data security and data privacy by organizations and users,
Privacy-preserving technology represented by federated learning has a wide
range of application scenarios. Federated learning is a distributed machine
learning computing framework that allows multiple subjects to train joint
models without sharing data to protect data privacy and solve the problem of
data islands. However, the data among multiple subjects are independent of each
other, and the data differences in quality may cause fairness issues in
federated learning modeling, such as data bias among multiple subjects,
resulting in biased and discriminatory models. Therefore, we propose DBFed, a
debiasing federated learning framework based on domain-independent, which
mitigates model bias by explicitly encoding sensitive attributes during
client-side training. This paper conducts experiments on three real datasets
and uses five evaluation metrics of accuracy and fairness to quantify the
effect of the model. Most metrics of DBFed exceed those of the other three
comparative methods, fully demonstrating the debiasing effect of DBFed.
</p></li>
</ul>

<h3>Title: Tackling Computational Heterogeneity in FL: A Few Theoretical Insights. (arXiv:2307.06283v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06283">http://arxiv.org/abs/2307.06283</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06283] Tackling Computational Heterogeneity in FL: A Few Theoretical Insights](http://arxiv.org/abs/2307.06283) #federate</code></li>
<li>Summary: <p>The future of machine learning lies in moving data collection along with
training to the edge. Federated Learning, for short FL, has been recently
proposed to achieve this goal. The principle of this approach is to aggregate
models learned over a large number of distributed clients, i.e.,
resource-constrained mobile devices that collect data from their environment,
to obtain a new more general model. The latter is subsequently redistributed to
clients for further training. A key feature that distinguishes federated
learning from data-center-based distributed training is the inherent
heterogeneity. In this work, we introduce and analyse a novel aggregation
framework that allows for formalizing and tackling computational heterogeneity
in federated optimization, in terms of both heterogeneous data and local
updates. Proposed aggregation algorithms are extensively analyzed from a
theoretical, and an experimental prospective.
</p></li>
</ul>

<h3>Title: Locally Adaptive Federated Learning via Stochastic Polyak Stepsizes. (arXiv:2307.06306v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06306">http://arxiv.org/abs/2307.06306</a></li>
<li>Code URL: <a href="https://github.com/IssamLaradji/sps">https://github.com/IssamLaradji/sps</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06306] Locally Adaptive Federated Learning via Stochastic Polyak Stepsizes](http://arxiv.org/abs/2307.06306) #federate</code></li>
<li>Summary: <p>State-of-the-art federated learning algorithms such as FedAvg require
carefully tuned stepsizes to achieve their best performance. The improvements
proposed by existing adaptive federated methods involve tuning of additional
hyperparameters such as momentum parameters, and consider adaptivity only in
the server aggregation round, but not locally. These methods can be inefficient
in many practical scenarios because they require excessive tuning of
hyperparameters and do not capture local geometric information. In this work,
we extend the recently proposed stochastic Polyak stepsize (SPS) to the
federated learning setting, and propose new locally adaptive and nearly
parameter-free distributed SPS variants (FedSPS and FedDecSPS). We prove that
FedSPS converges linearly in strongly convex and sublinearly in convex settings
when the interpolation condition (overparametrization) is satisfied, and
converges to a neighborhood of the solution in the general case. We extend our
proposed method to a decreasing stepsize version FedDecSPS, that converges also
when the interpolation condition does not hold. We validate our theoretical
claims by performing illustrative convex experiments. Our proposed algorithms
match the optimization performance of FedAvg with the best tuned
hyperparameters in the i.i.d. case, and outperform FedAvg in the non-i.i.d.
case.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators. (arXiv:2307.05532v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05532">http://arxiv.org/abs/2307.05532</a></li>
<li>Code URL: <a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io">https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05532] Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators](http://arxiv.org/abs/2307.05532) #fair</code></li>
<li>Summary: <p>Large language models that exhibit instruction-following behaviour represent
one of the biggest recent upheavals in conversational interfaces, a trend in
large part fuelled by the release of OpenAI's ChatGPT, a proprietary large
language model for text generation fine-tuned through reinforcement learning
from human feedback (LLM+RLHF). We review the risks of relying on proprietary
software and survey the first crop of open-source projects of comparable
architecture and functionality. The main contribution of this paper is to show
that openness is differentiated, and to offer scientific documentation of
degrees of openness in this fast-moving field. We evaluate projects in terms of
openness of code, training data, model weights, RLHF data, licensing,
scientific documentation, and access methods. We find that while there is a
fast-growing list of projects billing themselves as 'open source', many inherit
undocumented data of dubious legality, few share the all-important
instruction-tuning (a key site where human annotation labour is involved), and
careful scientific documentation is exceedingly rare. Degrees of openness are
relevant to fairness and accountability at all points, from data collection and
curation to model architecture, and from training and fine-tuning to release
and deployment.
</p></li>
</ul>

<h3>Title: Towards A Scalable Solution for Improving Multi-Group Fairness in Compositional Classification. (arXiv:2307.05728v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05728">http://arxiv.org/abs/2307.05728</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05728] Towards A Scalable Solution for Improving Multi-Group Fairness in Compositional Classification](http://arxiv.org/abs/2307.05728) #fair</code></li>
<li>Summary: <p>Despite the rich literature on machine learning fairness, relatively little
attention has been paid to remediating complex systems, where the final
prediction is the combination of multiple classifiers and where multiple groups
are present. In this paper, we first show that natural baseline approaches for
improving equal opportunity fairness scale linearly with the product of the
number of remediated groups and the number of remediated prediction labels,
rendering them impractical. We then introduce two simple techniques, called
{\em task-overconditioning} and {\em group-interleaving}, to achieve a constant
scaling in this multi-group multi-label setup. Our experimental results in
academic and real-world environments demonstrate the effectiveness of our
proposal at mitigation within this environment.
</p></li>
</ul>

<h3>Title: FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for Human-in-the-Loop Systems. (arXiv:2307.05857v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05857">http://arxiv.org/abs/2307.05857</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05857] FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for Human-in-the-Loop Systems](http://arxiv.org/abs/2307.05857) #fair</code></li>
<li>Summary: <p>Achieving fairness in sequential-decision making systems within
Human-in-the-Loop (HITL) environments is a critical concern, especially when
multiple humans with different behavior and expectations are affected by the
same adaptation decisions in the system. This human variability factor adds
more complexity since policies deemed fair at one point in time may become
discriminatory over time due to variations in human preferences resulting from
inter- and intra-human variability. This paper addresses the fairness problem
from an equity lens, considering human behavior variability, and the changes in
human preferences over time. We propose FAIRO, a novel algorithm for
fairness-aware sequential-decision making in HITL adaptation, which
incorporates these notions into the decision-making process. In particular,
FAIRO decomposes this complex fairness task into adaptive sub-tasks based on
individual human preferences through leveraging the Options reinforcement
learning framework. We design FAIRO to generalize to three types of HITL
application setups that have the shared adaptation decision problem.
Furthermore, we recognize that fairness-aware policies can sometimes conflict
with the application's utility. To address this challenge, we provide a
fairness-utility tradeoff in FAIRO, allowing system designers to balance the
objectives of fairness and utility based on specific application requirements.
Extensive evaluations of FAIRO on the three HITL applications demonstrate its
generalizability and effectiveness in promoting fairness while accounting for
human variability. On average, FAIRO can improve fairness compared with other
methods across all three applications by 35.36%.
</p></li>
</ul>

<h3>Title: Deep learning for dynamic graphs: models and benchmarks. (arXiv:2307.06104v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06104">http://arxiv.org/abs/2307.06104</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06104] Deep learning for dynamic graphs: models and benchmarks](http://arxiv.org/abs/2307.06104) #fair</code></li>
<li>Summary: <p>Recent progress in research on Deep Graph Networks (DGNs) has led to a
maturation of the domain of learning on graphs. Despite the growth of this
research field, there are still important challenges that are yet unsolved.
Specifically, there is an urge of making DGNs suitable for predictive tasks on
realworld systems of interconnected entities, which evolve over time. With the
aim of fostering research in the domain of dynamic graphs, at first, we survey
recent advantages in learning both temporal and spatial information, providing
a comprehensive overview of the current state-of-the-art in the domain of
representation learning for dynamic graphs. Secondly, we conduct a fair
performance comparison among the most popular proposed approaches, leveraging
rigorous model selection and assessment for all the methods, thus establishing
a sound baseline for evaluating new architectures and approaches
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: DiffuseGAE: Controllable and High-fidelity Image Manipulation from Disentangled Representation. (arXiv:2307.05899v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05899">http://arxiv.org/abs/2307.05899</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05899] DiffuseGAE: Controllable and High-fidelity Image Manipulation from Disentangled Representation](http://arxiv.org/abs/2307.05899) #interpretability</code></li>
<li>Summary: <p>Diffusion probabilistic models (DPMs) have shown remarkable results on
various image synthesis tasks such as text-to-image generation and image
inpainting. However, compared to other generative methods like VAEs and GANs,
DPMs lack a low-dimensional, interpretable, and well-decoupled latent code.
Recently, diffusion autoencoders (Diff-AE) were proposed to explore the
potential of DPMs for representation learning via autoencoding. Diff-AE
provides an accessible latent space that exhibits remarkable interpretability,
allowing us to manipulate image attributes based on latent codes from the
space. However, previous works are not generic as they only operated on a few
limited attributes. To further explore the latent space of Diff-AE and achieve
a generic editing pipeline, we proposed a module called Group-supervised
AutoEncoder(dubbed GAE) for Diff-AE to achieve better disentanglement on the
latent code. Our proposed GAE has trained via an attribute-swap strategy to
acquire the latent codes for multi-attribute image manipulation based on
examples. We empirically demonstrate that our method enables
multiple-attributes manipulation and achieves convincing sample quality and
attribute alignments, while significantly reducing computational requirements
compared to pixel-based approaches for representational decoupling. Code will
be released soon.
</p></li>
</ul>

<h3>Title: Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks. (arXiv:2307.05639v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05639">http://arxiv.org/abs/2307.05639</a></li>
<li>Code URL: <a href="https://github.com/dannyzx/grbf-nns">https://github.com/dannyzx/grbf-nns</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05639] Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks](http://arxiv.org/abs/2307.05639) #interpretability</code></li>
<li>Summary: <p>Providing a model that achieves a strong predictive performance and at the
same time is interpretable by humans is one of the most difficult challenges in
machine learning research due to the conflicting nature of these two
objectives. To address this challenge, we propose a modification of the Radial
Basis Function Neural Network model by equipping its Gaussian kernel with a
learnable precision matrix. We show that precious information is contained in
the spectrum of the precision matrix that can be extracted once the training of
the model is completed. In particular, the eigenvectors explain the directions
of maximum sensitivity of the model revealing the active subspace and
suggesting potential applications for supervised dimensionality reduction. At
the same time, the eigenvectors highlight the relationship in terms of absolute
variation between the input and the latent variables, thereby allowing us to
extract a ranking of the input variables based on their importance to the
prediction task enhancing the model interpretability. We conducted numerical
experiments for regression, classification, and feature selection tasks,
comparing our model against popular machine learning models and the
state-of-the-art deep learning-based embedding feature selection techniques.
Our results demonstrate that the proposed model does not only yield an
attractive prediction performance with respect to the competitors but also
provides meaningful and interpretable results that potentially could assist the
decision-making process in real-world applications. A PyTorch implementation of
the model is available on GitHub at the following link.
https://github.com/dannyzx/GRBF-NNs
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Impact of Feature Encoding on Malware Classification Explainability. (arXiv:2307.05614v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05614">http://arxiv.org/abs/2307.05614</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05614] Impact of Feature Encoding on Malware Classification Explainability](http://arxiv.org/abs/2307.05614) #explainability</code></li>
<li>Summary: <p>This paper investigates the impact of feature encoding techniques on the
explainability of XAI (Explainable Artificial Intelligence) algorithms. Using a
malware classification dataset, we trained an XGBoost model and compared the
performance of two feature encoding methods: Label Encoding (LE) and One Hot
Encoding (OHE). Our findings reveal a marginal performance loss when using OHE
instead of LE. However, the more detailed explanations provided by OHE
compensated for this loss. We observed that OHE enables deeper exploration of
details in both global and local contexts, facilitating more comprehensive
answers. Additionally, we observed that using OHE resulted in smaller
explanation files and reduced analysis time for human analysts. These findings
emphasize the significance of considering feature encoding techniques in XAI
research and suggest potential for further exploration by incorporating
additional encoding methods and innovative visualization approaches.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Merging multiple input descriptors and supervisors in a deep neural network for tractogram filtering. (arXiv:2307.05786v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05786">http://arxiv.org/abs/2307.05786</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05786] Merging multiple input descriptors and supervisors in a deep neural network for tractogram filtering](http://arxiv.org/abs/2307.05786) #diffusion</code></li>
<li>Summary: <p>One of the main issues of the current tractography methods is their high
false-positive rate. Tractogram filtering is an option to remove false-positive
streamlines from tractography data in a post-processing step. In this paper, we
train a deep neural network for filtering tractography data in which every
streamline of a tractogram is classified as {\em plausible, implausible}, or
{\em inconclusive}. For this, we use four different tractogram filtering
strategies as supervisors: TractQuerier, RecobundlesX, TractSeg, and an
anatomy-inspired filter. Their outputs are combined to obtain the
classification labels for the streamlines. We assessed the importance of
different types of information along the streamlines for performing this
classification task, including the coordinates of the streamlines, diffusion
data, landmarks, T1-weighted information, and a brain parcellation. We found
that the streamline coordinates are the most relevant followed by the diffusion
data in this particular classification task.
</p></li>
</ul>

<h3>Title: Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models. (arXiv:2307.05977v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05977">http://arxiv.org/abs/2307.05977</a></li>
<li>Code URL: <a href="https://github.com/nannullna/safe-diffusion">https://github.com/nannullna/safe-diffusion</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05977] Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models](http://arxiv.org/abs/2307.05977) #diffusion</code></li>
<li>Summary: <p>Large-scale image generation models, with impressive quality made possible by
the vast amount of data available on the Internet, raise social concerns that
these models may generate harmful or copyrighted content. The biases and
harmfulness arise throughout the entire training process and are hard to
completely remove, which have become significant hurdles to the safe deployment
of these models. In this paper, we propose a method called SDD to prevent
problematic content generation in text-to-image diffusion models. We
self-distill the diffusion model to guide the noise estimate conditioned on the
target removal concept to match the unconditional one. Compared to the previous
methods, our method eliminates a much greater proportion of harmful content
from the generated images without degrading the overall image quality.
Furthermore, our method allows the removal of multiple concepts at once,
whereas previous works are limited to removing a single concept at a time.
</p></li>
</ul>

<h3>Title: Exposing the Fake: Effective Diffusion-Generated Images Detection. (arXiv:2307.06272v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06272">http://arxiv.org/abs/2307.06272</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06272] Exposing the Fake: Effective Diffusion-Generated Images Detection](http://arxiv.org/abs/2307.06272) #diffusion</code></li>
<li>Summary: <p>Image synthesis has seen significant advancements with the advent of
diffusion-based generative models like Denoising Diffusion Probabilistic Models
(DDPM) and text-to-image diffusion models. Despite their efficacy, there is a
dearth of research dedicated to detecting diffusion-generated images, which
could pose potential security and privacy risks. This paper addresses this gap
by proposing a novel detection method called Stepwise Error for
Diffusion-generated Image Detection (SeDID). Comprising statistical-based
$\text{SeDID}<em>{\text{Stat}}$ and neural network-based
$\text{SeDID}</em>{\text{NNs}}$, SeDID exploits the unique attributes of diffusion
models, namely deterministic reverse and deterministic denoising computation
errors. Our evaluations demonstrate SeDID's superior performance over existing
methods when applied to diffusion models. Thus, our work makes a pivotal
contribution to distinguishing diffusion model-generated images, marking a
significant step in the domain of artificial intelligence security.
</p></li>
</ul>

<h3>Title: Augmenters at SemEval-2023 Task 1: Enhancing CLIP in Handling Compositionality and Ambiguity for Zero-Shot Visual WSD through Prompt Augmentation and Text-To-Image Diffusion. (arXiv:2307.05564v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05564">http://arxiv.org/abs/2307.05564</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05564] Augmenters at SemEval-2023 Task 1: Enhancing CLIP in Handling Compositionality and Ambiguity for Zero-Shot Visual WSD through Prompt Augmentation and Text-To-Image Diffusion](http://arxiv.org/abs/2307.05564) #diffusion</code></li>
<li>Summary: <p>This paper describes our zero-shot approaches for the Visual Word Sense
Disambiguation (VWSD) Task in English. Our preliminary study shows that the
simple approach of matching candidate images with the phrase using CLIP suffers
from the many-to-many nature of image-text pairs. We find that the CLIP text
encoder may have limited abilities in capturing the compositionality in natural
language. Conversely, the descriptive focus of the phrase varies from instance
to instance. We address these issues in our two systems, Augment-CLIP and
Stable Diffusion Sampling (SD Sampling). Augment-CLIP augments the text prompt
by generating sentences that contain the context phrase with the help of large
language models (LLMs). We further explore CLIP models in other languages, as
the an ambiguous word may be translated into an unambiguous one in the other
language. SD Sampling uses text-to-image Stable Diffusion to generate multiple
images from the given phrase, increasing the likelihood that a subset of images
match the one that paired with the text.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement. (arXiv:2307.05561v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05561">http://arxiv.org/abs/2307.05561</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05561] TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement](http://arxiv.org/abs/2307.05561) #transformer</code></li>
<li>Summary: <p>As demand for robotics manipulation application increases, accurate
vision-based 6D pose estimation becomes essential for autonomous operations.
Convolutional Neural Networks (CNNs) based approaches for pose estimation have
been previously introduced. However, the quest for better performance still
persists especially for accurate robotics manipulation. This quest extends to
the Agri-robotics domain. In this paper, we propose TransPose, an improved
Transformer-based 6D pose estimation with a depth refinement module. The
architecture takes in only an RGB image as input with no additional
supplementing modalities such as depth or thermal images. The architecture
encompasses an innovative lighter depth estimation network that estimates depth
from an RGB image using feature pyramid with an up-sampling method. A
transformer-based detection network with additional prediction heads is
proposed to directly regress the object's centre and predict the 6D pose of the
target. A novel depth refinement module is then used alongside the predicted
centers, 6D poses and depth patches to refine the accuracy of the estimated 6D
pose. We extensively compared our results with other state-of-the-art methods
and analysed our results for fruit-picking applications. The results we
achieved show that our proposed technique outperforms the other methods
available in the literature.
</p></li>
</ul>

<h3>Title: Image Reconstruction using Enhanced Vision Transformer. (arXiv:2307.05616v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05616">http://arxiv.org/abs/2307.05616</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05616] Image Reconstruction using Enhanced Vision Transformer](http://arxiv.org/abs/2307.05616) #transformer</code></li>
<li>Summary: <p>Removing noise from images is a challenging and fundamental problem in the
field of computer vision. Images captured by modern cameras are inevitably
degraded by noise which limits the accuracy of any quantitative measurements on
those images. In this project, we propose a novel image reconstruction
framework which can be used for tasks such as image denoising, deblurring or
inpainting. The model proposed in this project is based on Vision Transformer
(ViT) that takes 2D images as input and outputs embeddings which can be used
for reconstructing denoised images. We incorporate four additional optimization
techniques in the framework to improve the model reconstruction capability,
namely Locality Sensitive Attention (LSA), Shifted Patch Tokenization (SPT),
Rotary Position Embeddings (RoPE) and adversarial loss function inspired from
Generative Adversarial Networks (GANs). LSA, SPT and RoPE enable the
transformer to learn from the dataset more efficiently, while the adversarial
loss function enhances the resolution of the reconstructed images. Based on our
experiments, the proposed architecture outperforms the benchmark U-Net model by
more than 3.5\% structural similarity (SSIM) for the reconstruction tasks of
image denoising and inpainting. The proposed enhancements further show an
improvement of \textasciitilde5\% SSIM over the benchmark for both tasks.
</p></li>
</ul>

<h3>Title: PIGEON: Predicting Image Geolocations. (arXiv:2307.05845v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05845">http://arxiv.org/abs/2307.05845</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05845] PIGEON: Predicting Image Geolocations](http://arxiv.org/abs/2307.05845) #transformer</code></li>
<li>Summary: <p>We introduce PIGEON, a multi-task end-to-end system for planet-scale image
geolocalization that achieves state-of-the-art performance on both external
benchmarks and in human evaluation. Our work incorporates semantic geocell
creation with label smoothing, conducts pretraining of a vision transformer on
images with geographic information, and refines location predictions with
ProtoNets across a candidate set of geocells. The contributions of PIGEON are
three-fold: first, we design a semantic geocells creation and splitting
algorithm based on open-source data which can be adapted to any geospatial
dataset. Second, we show the effectiveness of intra-geocell refinement and the
applicability of unsupervised clustering and ProtNets to the task. Finally, we
make our pre-trained CLIP transformer model, StreetCLIP, publicly available for
use in adjacent domains with applications to fighting climate change and urban
and rural scene understanding.
</p></li>
</ul>

<h3>Title: SwiFT: Swin 4D fMRI Transformer. (arXiv:2307.05916v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05916">http://arxiv.org/abs/2307.05916</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05916] SwiFT: Swin 4D fMRI Transformer](http://arxiv.org/abs/2307.05916) #transformer</code></li>
<li>Summary: <p>The modeling of spatiotemporal brain dynamics from high-dimensional data,
such as 4D functional MRI, is a formidable task in neuroscience. To address
this challenge, we present SwiFT (Swin 4D fMRI Transformer), a Swin Transformer
architecture that can learn brain dynamics directly from 4D functional brain
MRI data in a memory and computation-efficient manner. SwiFT achieves this by
implementing a 4D window multi-head self-attention mechanism and absolute
positional embeddings. We evaluate SwiFT using multiple largest-scale human
functional brain imaging datasets in tasks such as predicting sex, age, and
cognitive intelligence. Our experimental outcomes reveal that SwiFT
consistently outperforms recent state-of-the-art models. To the best of our
knowledge, SwiFT is the first Swin Transformer architecture that can process
dimensional spatiotemporal brain functional data in an end-to-end fashion.
Furthermore, due to the end-to-end learning capability, we also show that
contrastive loss-based self-supervised pre-training of SwiFT is also feasible
for achieving improved performance on a downstream task. We believe that our
work holds substantial potential in facilitating scalable learning of
functional brain imaging in neuroscience research by reducing the hurdles
associated with applying Transformer models to high-dimensional fMRI.
</p></li>
</ul>

<h3>Title: Transformers in Reinforcement Learning: A Survey. (arXiv:2307.05979v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05979">http://arxiv.org/abs/2307.05979</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05979] Transformers in Reinforcement Learning: A Survey](http://arxiv.org/abs/2307.05979) #transformer</code></li>
<li>Summary: <p>Transformers have significantly impacted domains like natural language
processing, computer vision, and robotics, where they improve performance
compared to other neural networks. This survey explores how transformers are
used in reinforcement learning (RL), where they are seen as a promising
solution for addressing challenges such as unstable training, credit
assignment, lack of interpretability, and partial observability. We begin by
providing a brief domain overview of RL, followed by a discussion on the
challenges of classical RL algorithms. Next, we delve into the properties of
the transformer and its variants and discuss the characteristics that make them
well-suited to address the challenges inherent in RL. We examine the
application of transformers to various aspects of RL, including representation
learning, transition and reward function modeling, and policy optimization. We
also discuss recent research that aims to enhance the interpretability and
efficiency of transformers in RL, using visualization techniques and efficient
training strategies. Often, the transformer architecture must be tailored to
the specific needs of a given application. We present a broad overview of how
transformers have been adapted for several applications, including robotics,
medicine, language modeling, cloud computing, and combinatorial optimization.
We conclude by discussing the limitations of using transformers in RL and
assess their potential for catalyzing future breakthroughs in this field.
</p></li>
</ul>

<h3>Title: What Happens During Finetuning of Vision Transformers: An Invariance Based Investigation. (arXiv:2307.06006v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06006">http://arxiv.org/abs/2307.06006</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06006] What Happens During Finetuning of Vision Transformers: An Invariance Based Investigation](http://arxiv.org/abs/2307.06006) #transformer</code></li>
<li>Summary: <p>The pretrain-finetune paradigm usually improves downstream performance over
training a model from scratch on the same task, becoming commonplace across
many areas of machine learning. While pretraining is empirically observed to be
beneficial for a range of tasks, there is not a clear understanding yet of the
reasons for this effect. In this work, we examine the relationship between
pretrained vision transformers and the corresponding finetuned versions on
several benchmark datasets and tasks. We present new metrics that specifically
investigate the degree to which invariances learned by a pretrained model are
retained or forgotten during finetuning. Using these metrics, we present a
suite of empirical findings, including that pretraining induces transferable
invariances in shallow layers and that invariances from deeper pretrained
layers are compressed towards shallower layers during finetuning. Together,
these findings contribute to understanding some of the reasons for the
successes of pretrained models and the changes that a pretrained model
undergoes when finetuned on a downstream task.
</p></li>
</ul>

<h3>Title: AICT: An Adaptive Image Compression Transformer. (arXiv:2307.06091v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06091">http://arxiv.org/abs/2307.06091</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06091] AICT: An Adaptive Image Compression Transformer](http://arxiv.org/abs/2307.06091) #transformer</code></li>
<li>Summary: <p>Motivated by the efficiency investigation of the Tranformer-based transform
coding framework, namely SwinT-ChARM, we propose to enhance the latter, as
first, with a more straightforward yet effective Tranformer-based channel-wise
auto-regressive prior model, resulting in an absolute image compression
transformer (ICT). Current methods that still rely on ConvNet-based entropy
coding are limited in long-range modeling dependencies due to their local
connectivity and an increasing number of architectural biases and priors. On
the contrary, the proposed ICT can capture both global and local contexts from
the latent representations and better parameterize the distribution of the
quantized latents. Further, we leverage a learnable scaling module with a
sandwich ConvNeXt-based pre/post-processor to accurately extract more compact
latent representation while reconstructing higher-quality images. Extensive
experimental results on benchmark datasets showed that the proposed adaptive
image compression transformer (AICT) framework significantly improves the
trade-off between coding efficiency and decoder complexity over the versatile
video coding (VVC) reference encoder (VTM-18.0) and the neural codec
SwinT-ChARM.
</p></li>
</ul>

<h3>Title: TreeFormer: a Semi-Supervised Transformer-based Framework for Tree Counting from a Single High Resolution Image. (arXiv:2307.06118v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06118">http://arxiv.org/abs/2307.06118</a></li>
<li>Code URL: <a href="https://github.com/haaclassic/treeformer">https://github.com/haaclassic/treeformer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06118] TreeFormer: a Semi-Supervised Transformer-based Framework for Tree Counting from a Single High Resolution Image](http://arxiv.org/abs/2307.06118) #transformer</code></li>
<li>Summary: <p>Automatic tree density estimation and counting using single aerial and
satellite images is a challenging task in photogrammetry and remote sensing,
yet has an important role in forest management. In this paper, we propose the
first semisupervised transformer-based framework for tree counting which
reduces the expensive tree annotations for remote sensing images. Our method,
termed as TreeFormer, first develops a pyramid tree representation module based
on transformer blocks to extract multi-scale features during the encoding
stage. Contextual attention-based feature fusion and tree density regressor
modules are further designed to utilize the robust features from the encoder to
estimate tree density maps in the decoder. Moreover, we propose a pyramid
learning strategy that includes local tree density consistency and local tree
count ranking losses to utilize unlabeled images into the training process.
Finally, the tree counter token is introduced to regulate the network by
computing the global tree counts for both labeled and unlabeled images. Our
model was evaluated on two benchmark tree counting datasets, Jiangsu, and
Yosemite, as well as a new dataset, KCL-London, created by ourselves. Our
TreeFormer outperforms the state of the art semi-supervised methods under the
same setting and exceeds the fully-supervised methods using the same number of
labeled images. The codes and datasets are available at
https://github.com/HAAClassic/TreeFormer.
</p></li>
</ul>

<h3>Title: UGCANet: A Unified Global Context-Aware Transformer-based Network with Feature Alignment for Endoscopic Image Analysis. (arXiv:2307.06260v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06260">http://arxiv.org/abs/2307.06260</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06260] UGCANet: A Unified Global Context-Aware Transformer-based Network with Feature Alignment for Endoscopic Image Analysis](http://arxiv.org/abs/2307.06260) #transformer</code></li>
<li>Summary: <p>Gastrointestinal endoscopy is a medical procedure that utilizes a flexible
tube equipped with a camera and other instruments to examine the digestive
tract. This minimally invasive technique allows for diagnosing and managing
various gastrointestinal conditions, including inflammatory bowel disease,
gastrointestinal bleeding, and colon cancer. The early detection and
identification of lesions in the upper gastrointestinal tract and the
identification of malignant polyps that may pose a risk of cancer development
are critical components of gastrointestinal endoscopy's diagnostic and
therapeutic applications. Therefore, enhancing the detection rates of
gastrointestinal disorders can significantly improve a patient's prognosis by
increasing the likelihood of timely medical intervention, which may prolong the
patient's lifespan and improve overall health outcomes. This paper presents a
novel Transformer-based deep neural network designed to perform multiple tasks
simultaneously, thereby enabling accurate identification of both upper
gastrointestinal tract lesions and colon polyps. Our approach proposes a unique
global context-aware module and leverages the powerful MiT backbone, along with
a feature alignment block, to enhance the network's representation capability.
This novel design leads to a significant improvement in performance across
various endoscopic diagnosis tasks. Extensive experiments demonstrate the
superior performance of our method compared to other state-of-the-art
approaches.
</p></li>
</ul>

<h3>Title: Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution. (arXiv:2307.06304v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06304">http://arxiv.org/abs/2307.06304</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06304] Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution](http://arxiv.org/abs/2307.06304) #transformer</code></li>
<li>Summary: <p>The ubiquitous and demonstrably suboptimal choice of resizing images to a
fixed resolution before processing them with computer vision models has not yet
been successfully challenged. However, models such as the Vision Transformer
(ViT) offer flexible sequence-based modeling, and hence varying input sequence
lengths. We take advantage of this with NaViT (Native Resolution ViT) which
uses sequence packing during training to process inputs of arbitrary
resolutions and aspect ratios. Alongside flexible model usage, we demonstrate
improved training efficiency for large-scale supervised and contrastive
image-text pretraining. NaViT can be efficiently transferred to standard tasks
such as image and video classification, object detection, and semantic
segmentation and leads to improved results on robustness and fairness
benchmarks. At inference time, the input resolution flexibility can be used to
smoothly navigate the test-time cost-performance trade-off. We believe that
NaViT marks a departure from the standard, CNN-designed, input and modelling
pipeline used by most computer vision models, and represents a promising
direction for ViTs.
</p></li>
</ul>

<h3>Title: Separate-and-Aggregate: A Transformer-based Patch Refinement Model for Knowledge Graph Completion. (arXiv:2307.05627v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05627">http://arxiv.org/abs/2307.05627</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05627] Separate-and-Aggregate: A Transformer-based Patch Refinement Model for Knowledge Graph Completion](http://arxiv.org/abs/2307.05627) #transformer</code></li>
<li>Summary: <p>Knowledge graph completion (KGC) is the task of inferencing missing facts
from any given knowledge graphs (KG). Previous KGC methods typically represent
knowledge graph entities and relations as trainable continuous embeddings and
fuse the embeddings of the entity $h$ (or $t$) and relation $r$ into hidden
representations of query $(h, r, ?)$ (or $(?, r, t$)) to approximate the
missing entities. To achieve this, they either use shallow linear
transformations or deep convolutional modules. However, the linear
transformations suffer from the expressiveness issue while the deep
convolutional modules introduce unnecessary inductive bias, which could
potentially degrade the model performance. Thus, we propose a novel
Transformer-based Patch Refinement Model (PatReFormer) for KGC. PatReFormer
first segments the embedding into a sequence of patches and then employs
cross-attention modules to allow bi-directional embedding feature interaction
between the entities and relations, leading to a better understanding of the
underlying KG. We conduct experiments on four popular KGC benchmarks, WN18RR,
FB15k-237, YAGO37 and DB100K. The experimental results show significant
performance improvement from existing KGC methods on standard KGC evaluation
metrics, e.g., MRR and H@n. Our analysis first verifies the effectiveness of
our model design choices in PatReFormer. We then find that PatReFormer can
better capture KG information from a large relation embedding dimension.
Finally, we demonstrate that the strength of PatReFormer is at complex relation
types, compared to other KGC models
</p></li>
</ul>

<h3>Title: Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05695">http://arxiv.org/abs/2307.05695</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05695] Stack More Layers Differently: High-Rank Training Through Low-Rank Updates](http://arxiv.org/abs/2307.05695) #transformer</code></li>
<li>Summary: <p>Despite the dominance and effectiveness of scaling, resulting in large
networks with hundreds of billions of parameters, the necessity to train
overparametrized models remains poorly understood, and alternative approaches
do not necessarily make it cheaper to train high-performance models. In this
paper, we explore low-rank training techniques as an alternative approach to
training large neural networks. We introduce a novel method called ReLoRA,
which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to
pre-training transformer language models with up to 350M parameters and
demonstrate comparable performance to regular neural network training.
Furthermore, we observe that the efficiency of ReLoRA increases with model
size, making it a promising approach for training multi-billion-parameter
networks efficiently. Our findings shed light on the potential of low-rank
training techniques and their implications for scaling laws.
</p></li>
</ul>

<h3>Title: Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models. (arXiv:2307.05972v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05972">http://arxiv.org/abs/2307.05972</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05972] Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models](http://arxiv.org/abs/2307.05972) #transformer</code></li>
<li>Summary: <p>We investigate the effects of post-training quantization and
quantization-aware training on the generalization of Transformer language
models. We present a new method called self-distilled quantization (SDQ) that
minimizes accumulative quantization errors and outperforms baselines. We apply
SDQ to multilingual models XLM-R-Base and InfoXLM-Base and demonstrate that
both models can be reduced from 32-bit floating point weights to 8-bit integer
weights while maintaining a high level of performance on the XGLUE benchmark.
Our results also highlight the challenges of quantizing multilingual models,
which must generalize to languages they were not fine-tuned on.
</p></li>
</ul>

<h3>Title: NLP Meets RNA: Unsupervised Embedding Learning for Ribozymes with Word2Vec. (arXiv:2307.05537v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05537">http://arxiv.org/abs/2307.05537</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05537] NLP Meets RNA: Unsupervised Embedding Learning for Ribozymes with Word2Vec](http://arxiv.org/abs/2307.05537) #transformer</code></li>
<li>Summary: <p>Ribozymes, RNA molecules with distinct 3D structures and catalytic activity,
have widespread applications in synthetic biology and therapeutics. However,
relatively little research has focused on leveraging deep learning to enhance
our understanding of ribozymes. This study implements Word2Vec, an unsupervised
learning technique for natural language processing, to learn ribozyme
embeddings. Ribo2Vec was trained on over 9,000 diverse ribozymes, learning to
map sequences to 128 and 256-dimensional vector spaces. Using Ribo2Vec,
sequence embeddings for five classes of ribozymes (hatchet, pistol, hairpin,
hovlinc, and twister sister) were calculated. Principal component analysis
demonstrated the ability of these embeddings to distinguish between ribozyme
classes. Furthermore, a simple SVM classifier trained on ribozyme embeddings
showed promising results in accurately classifying ribozyme types. Our results
suggest that the embedding vectors contained meaningful information about
ribozymes. Interestingly, 256-dimensional embeddings behaved similarly to
128-dimensional embeddings, suggesting that a lower dimension vector space is
generally sufficient to capture ribozyme features. This approach demonstrates
the potential of Word2Vec for bioinformatics, opening new avenues for ribozyme
research. Future research includes using a Transformer-based method to learn
RNA embeddings, which can capture long-range interactions between nucleotides.
</p></li>
</ul>

<h3>Title: Multiobjective Hydropower Reservoir Operation Optimization with Transformer-Based Deep Reinforcement Learning. (arXiv:2307.05643v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05643">http://arxiv.org/abs/2307.05643</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05643] Multiobjective Hydropower Reservoir Operation Optimization with Transformer-Based Deep Reinforcement Learning](http://arxiv.org/abs/2307.05643) #transformer</code></li>
<li>Summary: <p>Due to shortage of water resources and increasing water demands, the joint
operation of multireservoir systems for balancing power generation, ecological
protection, and the residential water supply has become a critical issue in
hydropower management. However, the numerous constraints and nonlinearity of
multiple reservoirs make solving this problem time-consuming. To address this
challenge, a deep reinforcement learning approach that incorporates a
transformer framework is proposed. The multihead attention mechanism of the
encoder effectively extracts information from reservoirs and residential areas,
and the multireservoir attention network of the decoder generates suitable
operational decisions. The proposed method is applied to Lake Mead and Lake
Powell in the Colorado River Basin. The experimental results demonstrate that
the transformer-based deep reinforcement learning approach can produce
appropriate operational outcomes. Compared to a state-of-the-art method, the
operation strategies produced by the proposed approach generate 10.11% more
electricity, reduce the amended annual proportional flow deviation by 39.69%,
and increase water supply revenue by 4.10%. Consequently, the proposed approach
offers an effective method for the multiobjective operation of multihydropower
reservoir systems.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05591">http://arxiv.org/abs/2307.05591</a></li>
<li>Code URL: <a href="https://github.com/ml-jku/semantic-image-text-alignment">https://github.com/ml-jku/semantic-image-text-alignment</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05591] SITTA: A Semantic Image-Text Alignment for Image Captioning](http://arxiv.org/abs/2307.05591) #generative</code></li>
<li>Summary: <p>Textual and semantic comprehension of images is essential for generating
proper captions. The comprehension requires detection of objects, modeling of
relations between them, an assessment of the semantics of the scene and,
finally, representing the extracted knowledge in a language space. To achieve
rich language capabilities while ensuring good image-language mappings,
pretrained language models (LMs) were conditioned on pretrained multi-modal
(image-text) models that allow for image inputs. This requires an alignment of
the image representation of the multi-modal model with the language
representations of a generative LM. However, it is not clear how to best
transfer semantics detected by the vision encoder of the multi-modal model to
the LM. We introduce two novel ways of constructing a linear mapping that
successfully transfers semantics between the embedding spaces of the two
pretrained models. The first aligns the embedding space of the multi-modal
language encoder with the embedding space of the pretrained LM via token
correspondences. The latter leverages additional data that consists of
image-text pairs to construct the mapping directly from vision to language
space. Using our semantic mappings, we unlock image captioning for LMs without
access to gradient information. By using different sources of data we achieve
strong captioning performance on MS-COCO and Flickr30k datasets. Even in the
face of limited data, our method partly exceeds the performance of other
zero-shot and even finetuned competitors. Our ablation studies show that even
LMs at a scale of merely 250M parameters can generate decent captions employing
our semantic mappings. Our approach makes image captioning more accessible for
institutions with restricted computational resources.
</p></li>
</ul>

<h3>Title: Line Art Colorization of Fakemon using Generative Adversarial Neural Networks. (arXiv:2307.05760v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05760">http://arxiv.org/abs/2307.05760</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05760] Line Art Colorization of Fakemon using Generative Adversarial Neural Networks](http://arxiv.org/abs/2307.05760) #generative</code></li>
<li>Summary: <p>This work proposes a complete methodology to colorize images of Fakemon,
anime-style monster-like creatures. In addition, we propose algorithms to
extract the line art from colorized images as well as to extract color hints.
Our work is the first in the literature to use automatic color hint extraction,
to train the networks specifically with anime-styled creatures and to combine
the Pix2Pix and CycleGAN approaches, two different generative adversarial
networks that create a single final result. Visual results of the colorizations
are feasible but there is still room for improvement.
</p></li>
</ul>

<h3>Title: Operational Support Estimator Networks. (arXiv:2307.06065v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06065">http://arxiv.org/abs/2307.06065</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06065] Operational Support Estimator Networks](http://arxiv.org/abs/2307.06065) #generative</code></li>
<li>Summary: <p>In this work, we propose a novel approach called Operational Support
Estimator Networks (OSENs) for the support estimation task. Support Estimation
(SE) is defined as finding the locations of non-zero elements in a sparse
signal. By its very nature, the mapping between the measurement and sparse
signal is a non-linear operation. Traditional support estimators rely on
computationally expensive iterative signal recovery techniques to achieve such
non-linearity. Contrary to the convolution layers, the proposed OSEN approach
consists of operational layers that can learn such complex non-linearities
without the need for deep networks. In this way, the performance of the
non-iterative support estimation is greatly improved. Moreover, the operational
layers comprise so-called generative \textit{super neurons} with non-local
kernels. The kernel location for each neuron/feature map is optimized jointly
for the SE task during the training. We evaluate the OSENs in three different
applications: i. support estimation from Compressive Sensing (CS) measurements,
ii. representation-based classification, and iii. learning-aided CS
reconstruction where the output of OSENs is used as prior knowledge to the CS
algorithm for an enhanced reconstruction. Experimental results show that the
proposed approach achieves computational efficiency and outperforms competing
methods, especially at low measurement rates by a significant margin. The
software implementation is publicly shared at
https://github.com/meteahishali/OSEN.
</p></li>
</ul>

<h3>Title: Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times and Location Reasoning. (arXiv:2307.06166v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06166">http://arxiv.org/abs/2307.06166</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06166] Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times and Location Reasoning](http://arxiv.org/abs/2307.06166) #generative</code></li>
<li>Summary: <p>Vision-Language Models (VLMs) are expected to be capable of reasoning with
commonsense knowledge as human beings. One example is that humans can reason
where and when an image is taken based on their knowledge. This makes us wonder
if, based on visual cues, Vision-Language Models that are pre-trained with
large-scale image-text resources can achieve and even outperform human's
capability in reasoning times and location. To address this question, we
propose a two-stage \recognition\space and \reasoning\space probing task,
applied to discriminative and generative VLMs to uncover whether VLMs can
recognize times and location-relevant features and further reason about it. To
facilitate the investigation, we introduce WikiTiLo, a well-curated image
dataset compromising images with rich socio-cultural cues. In the extensive
experimental studies, we find that although VLMs can effectively retain
relevant features in visual encoders, they still fail to make perfect
reasoning. We will release our dataset and codes to facilitate future studies.
</p></li>
</ul>

<h3>Title: Facial Reenactment Through a Personalized Generator. (arXiv:2307.06307v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06307">http://arxiv.org/abs/2307.06307</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06307] Facial Reenactment Through a Personalized Generator](http://arxiv.org/abs/2307.06307) #generative</code></li>
<li>Summary: <p>In recent years, the role of image generative models in facial reenactment
has been steadily increasing. Such models are usually subject-agnostic and
trained on domain-wide datasets. The appearance of the reenacted individual is
learned from a single image, and hence, the entire breadth of the individual's
appearance is not entirely captured, leading these methods to resort to
unfaithful hallucination. Thanks to recent advancements, it is now possible to
train a personalized generative model tailored specifically to a given
individual. In this paper, we propose a novel method for facial reenactment
using a personalized generator. We train the generator using frames from a
short, yet varied, self-scan video captured using a simple commodity camera.
Images synthesized by the personalized generator are guaranteed to preserve
identity. The premise of our work is that the task of reenactment is thus
reduced to accurately mimicking head poses and expressions. To this end, we
locate the desired frames in the latent space of the personalized generator
using carefully designed latent optimization. Through extensive evaluation, we
demonstrate state-of-the-art performance for facial reenactment. Furthermore,
we show that since our reenactment takes place in a semantic latent space, it
can be semantically edited and stylized in post-processing.
</p></li>
</ul>

<h3>Title: Neural Machine Translation Data Generation and Augmentation using ChatGPT. (arXiv:2307.05779v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05779">http://arxiv.org/abs/2307.05779</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05779] Neural Machine Translation Data Generation and Augmentation using ChatGPT](http://arxiv.org/abs/2307.05779) #generative</code></li>
<li>Summary: <p>Neural models have revolutionized the field of machine translation, but
creating parallel corpora is expensive and time-consuming. We investigate an
alternative to manual parallel corpora - hallucinated parallel corpora created
by generative language models. Although these models are themselves trained on
parallel data, they can leverage a multilingual vector space to create data,
and may be able to supplement small manually-procured corpora. Our experiments
highlight two key findings - despite a lack of diversity in their output, the
hallucinated data improves the translation signal, even when the domain clashes
with the original dataset.
</p></li>
</ul>

<h3>Title: Improved POS tagging for spontaneous, clinical speech using data augmentation. (arXiv:2307.05796v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05796">http://arxiv.org/abs/2307.05796</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05796] Improved POS tagging for spontaneous, clinical speech using data augmentation](http://arxiv.org/abs/2307.05796) #generative</code></li>
<li>Summary: <p>This paper addresses the problem of improving POS tagging of transcripts of
speech from clinical populations. In contrast to prior work on parsing and POS
tagging of transcribed speech, we do not make use of an in domain treebank for
training. Instead, we train on an out of domain treebank of newswire using data
augmentation techniques to make these structures resemble natural, spontaneous
speech. We trained a parser with and without the augmented data and tested its
performance using manually validated POS tags in clinical speech produced by
patients with various types of neurodegenerative conditions.
</p></li>
</ul>

<h3>Title: GOKU-UI: Ubiquitous Inference through Attention and Multiple Shooting for Continuous-time Generative Models. (arXiv:2307.05735v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05735">http://arxiv.org/abs/2307.05735</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05735] GOKU-UI: Ubiquitous Inference through Attention and Multiple Shooting for Continuous-time Generative Models](http://arxiv.org/abs/2307.05735) #generative</code></li>
<li>Summary: <p>Scientific Machine Learning (SciML) is a burgeoning field that
synergistically combines domain-aware and interpretable models with agnostic
machine learning techniques. In this work, we introduce GOKU-UI, an evolution
of the SciML generative model GOKU-nets. The GOKU-UI broadens the original
model's spectrum to incorporate other classes of differential equations, such
as Stochastic Differential Equations (SDEs), and integrates a distributed, i.e.
ubiquitous, inference through attention mechanisms and a novel multiple
shooting training strategy in the latent space. These enhancements have led to
a significant increase in its performance in both reconstruction and forecast
tasks, as demonstrated by our evaluation of simulated and empirical data.
Specifically, GOKU-UI outperformed all baseline models on synthetic datasets
even with a training set 32-fold smaller, underscoring its remarkable data
efficiency. Furthermore, when applied to empirical human brain data, while
incorporating stochastic Stuart-Landau oscillators into its dynamical core, it
not only surpassed state-of-the-art baseline methods in the reconstruction
task, but also demonstrated better prediction of future brain activity up to 12
seconds ahead. By training GOKU-UI on resting-state fMRI data, we encoded
whole-brain dynamics into a latent representation, learning an effective
low-dimensional dynamical system model that could offer insights into brain
functionality and open avenues for practical applications such as mental state
or psychiatric condition classification. Ultimately, our research provides
further impetus for the field of Scientific Machine Learning, showcasing the
potential for advancements when established scientific insights are interwoven
with modern machine learning.
</p></li>
</ul>

<h3>Title: Prompt Generate Train (PGT): A framework for few-shot domain adaptation, alignment, and uncertainty calibration of a retriever augmented generation (RAG) model for domain specific open book question-answering. (arXiv:2307.05915v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05915">http://arxiv.org/abs/2307.05915</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05915] Prompt Generate Train (PGT): A framework for few-shot domain adaptation, alignment, and uncertainty calibration of a retriever augmented generation (RAG) model for domain specific open book question-answering](http://arxiv.org/abs/2307.05915) #generative</code></li>
<li>Summary: <p>We present a framework - Prompt, Generate, Train (PGT) - to efficiently
develop a generative question-answering model for open-book question-answering
over a proprietary collection of text documents. The framework adapts a
retriever augmented generation model to the target domain using supervised
finetuning and reinforcement learning with synthetic feedback in a few-shot
setting. This yields an aligned, uncertainty calibrated model that is
competitive with GPT-4 based in-context retrieval augmented generation in
generating relevant answers at lower serving costs. The synthetic generation
pipeline generates high quality synthetic training data musing a medium sized
LLM, Flan-T5 XXL, and a novel consistency filtering scheme. The pipeline is
designed to generate both abstractive and extractive questions that span the
entire corpus. Using samples from this dataset, the framework fine-tunes a
smaller RAG model comprising a dense retriever and a smaller sized LLM on
samples from the dataset. In parallel, the framework trains a Reward model to
score domain grounded answers higher than hallucinated answers. In the next
phase, the framework aligns to the RAG model with the target domain using
reinforcement learning. This step improves the RAG model's ability to generate
grounded answers and ignore out of domain questions. In the final phase, the
framework calibrates the model uncertainty for extractive question-answers.
This is a desirable feature since the model can be integrated into a cascading
system where the RAG model's answer is surfaced only when the model is
confident of its answer.
</p></li>
</ul>

<h3>Title: Diversity-enhancing Generative Network for Few-shot Hypothesis Adaptation. (arXiv:2307.05948v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05948">http://arxiv.org/abs/2307.05948</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05948] Diversity-enhancing Generative Network for Few-shot Hypothesis Adaptation](http://arxiv.org/abs/2307.05948) #generative</code></li>
<li>Summary: <p>Generating unlabeled data has been recently shown to help address the
few-shot hypothesis adaptation (FHA) problem, where we aim to train a
classifier for the target domain with a few labeled target-domain data and a
well-trained source-domain classifier (i.e., a source hypothesis), for the
additional information of the highly-compatible unlabeled data. However, the
generated data of the existing methods are extremely similar or even the same.
The strong dependency among the generated data will lead the learning to fail.
In this paper, we propose a diversity-enhancing generative network (DEG-Net)
for the FHA problem, which can generate diverse unlabeled data with the help of
a kernel independence measure: the Hilbert-Schmidt independence criterion
(HSIC). Specifically, DEG-Net will generate data via minimizing the HSIC value
(i.e., maximizing the independence) among the semantic features of the
generated data. By DEG-Net, the generated unlabeled data are more diverse and
more effective for addressing the FHA problem. Experimental results show that
the DEG-Net outperforms existing FHA baselines and further verifies that
generating diverse data plays a vital role in addressing the FHA problem
</p></li>
</ul>

<h3>Title: NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services. (arXiv:2307.06148v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06148">http://arxiv.org/abs/2307.06148</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06148] NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services](http://arxiv.org/abs/2307.06148) #generative</code></li>
<li>Summary: <p>Large language models (LLMs) have triggered tremendous success to empower
daily life by generative information, and the personalization of LLMs could
further contribute to their applications due to better alignment with human
intents. Towards personalized generative services, a collaborative cloud-edge
methodology sounds promising, as it facilitates the effective orchestration of
heterogeneous distributed communication and computing resources. In this
article, after discussing the pros and cons of several candidate cloud-edge
collaboration techniques, we put forward NetGPT to capably deploy appropriate
LLMs at the edge and the cloud in accordance with their computing capacity. In
addition, edge LLMs could efficiently leverage location-based information for
personalized prompt completion, thus benefiting the interaction with cloud
LLMs. After deploying representative open-source LLMs (e.g., GPT-2-base and
LLaMA model) at the edge and the cloud, we present the feasibility of NetGPT on
the basis of low-rank adaptation-based light-weight fine-tuning. Subsequently,
we highlight substantial essential changes required for a native artificial
intelligence (AI) network architecture towards NetGPT, with special emphasis on
deeper integration of communications and computing resources and careful
calibration of logical AI workflow. Furthermore, we demonstrate several
by-product benefits of NetGPT, given edge LLM's astonishing capability to
predict trends and infer intents, which possibly leads to a unified solution
for intelligent network management \&amp; orchestration. In a nutshell, we argue
that NetGPT is a promising native-AI network architecture beyond provisioning
personalized generative services.
</p></li>
</ul>

<h3>Title: Deep Generative Models for Physiological Signals: A Systematic Literature Review. (arXiv:2307.06162v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06162">http://arxiv.org/abs/2307.06162</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06162] Deep Generative Models for Physiological Signals: A Systematic Literature Review](http://arxiv.org/abs/2307.06162) #generative</code></li>
<li>Summary: <p>In this paper, we present a systematic literature review on deep generative
models for physiological signals, particularly electrocardiogram,
electroencephalogram, photoplethysmogram and electromyogram. Compared to the
existing review papers, we present the first review that summarizes the recent
state-of-the-art deep generative models. By analysing the state-of-the-art
research related to deep generative models along with their main applications
and challenges, this review contributes to the overall understanding of these
models applied to physiological signals. Additionally, by highlighting the
employed evaluation protocol and the most used physiological databases, this
review facilitates the assessment and benchmarking of deep generative models.
</p></li>
</ul>

<h3>Title: Unified Molecular Modeling via Modality Blending. (arXiv:2307.06235v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06235">http://arxiv.org/abs/2307.06235</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06235] Unified Molecular Modeling via Modality Blending](http://arxiv.org/abs/2307.06235) #generative</code></li>
<li>Summary: <p>Self-supervised molecular representation learning is critical for
molecule-based tasks such as AI-assisted drug discovery. Recent studies
consider leveraging both 2D and 3D information for representation learning,
with straightforward alignment strategies that treat each modality separately.
In this work, we introduce a novel "blend-then-predict" self-supervised
learning method (MoleBLEND), which blends atom relations from different
modalities into one unified relation matrix for encoding, then recovers
modality-specific information for both 2D and 3D structures. By treating atom
relationships as anchors, seemingly dissimilar 2D and 3D manifolds are aligned
and integrated at fine-grained relation-level organically. Extensive
experiments show that MoleBLEND achieves state-of-the-art performance across
major 2D/3D benchmarks. We further provide theoretical insights from the
perspective of mutual-information maximization, demonstrating that our method
unifies contrastive, generative (inter-modal prediction) and mask-then-predict
(intra-modal prediction) objectives into a single cohesive blend-then-predict
framework.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Better Handling Coreference Resolution in Aspect Level Sentiment Classification by Fine-Tuning Language Models. (arXiv:2307.05646v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05646">http://arxiv.org/abs/2307.05646</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05646] Better Handling Coreference Resolution in Aspect Level Sentiment Classification by Fine-Tuning Language Models](http://arxiv.org/abs/2307.05646) #large language model</code></li>
<li>Summary: <p>Customer feedback is invaluable to companies as they refine their products.
Monitoring customer feedback can be automated with Aspect Level Sentiment
Classification (ALSC) which allows us to analyse specific aspects of the
products in reviews. Large Language Models (LLMs) are the heart of many
state-of-the-art ALSC solutions, but they perform poorly in some scenarios
requiring Coreference Resolution (CR). In this work, we propose a framework to
improve an LLM's performance on CR-containing reviews by fine tuning on highly
inferential tasks. We show that the performance improvement is likely
attributed to the improved model CR ability. We also release a new dataset that
focuses on CR in ALSC.
</p></li>
</ul>

<h3>Title: Large Language Models. (arXiv:2307.05782v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05782">http://arxiv.org/abs/2307.05782</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05782] Large Language Models](http://arxiv.org/abs/2307.05782) #large language model</code></li>
<li>Summary: <p>Artificial intelligence is making spectacular progress, and one of the best
examples is the development of large language models (LLMs) such as OpenAI's
GPT series. In these lectures, written for readers with a background in
mathematics or physics, we give a brief history and survey of the state of the
art, and describe the underlying transformer architecture in detail. We then
explore some current ideas on how LLMs work and how models trained to predict
the next word in a text are able to perform other tasks displaying
intelligence.
</p></li>
</ul>

<h3>Title: Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding. (arXiv:2307.05908v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05908">http://arxiv.org/abs/2307.05908</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05908] Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding](http://arxiv.org/abs/2307.05908) #large language model</code></li>
<li>Summary: <p>This paper presents "Predictive Pipelined Decoding (PPD)," an approach that
speeds up greedy decoding in Large Language Models (LLMs) while maintaining the
exact same output as the original decoding. Unlike conventional strategies, PPD
employs additional compute resources to parallelize the initiation of
subsequent token decoding during the current token decoding. This innovative
method reduces decoding latency and reshapes the understanding of trade-offs in
LLM decoding strategies. We have developed a theoretical framework that allows
us to analyze the trade-off between computation and latency. Using this
framework, we can analytically estimate the potential reduction in latency
associated with our proposed method, achieved through the assessment of the
match rate, represented as p_correct. The results demonstrate that the use of
extra computational resources has the potential to accelerate LLM greedy
decoding.
</p></li>
</ul>

<h3>Title: PolyLM: An Open Source Polyglot Large Language Model. (arXiv:2307.06018v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06018">http://arxiv.org/abs/2307.06018</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06018] PolyLM: An Open Source Polyglot Large Language Model](http://arxiv.org/abs/2307.06018) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) demonstrate remarkable ability to comprehend,
reason, and generate following nature language instructions. However, the
development of LLMs has been primarily focused on high-resource languages, such
as English, thereby limiting their applicability and research in other
languages. Consequently, we present PolyLM, a multilingual LLM trained on 640
billion (B) tokens, avaliable in two model sizes: 1.7B and 13B. To enhance its
multilingual capabilities, we 1) integrate bilingual data into training data;
and 2) adopt a curriculum learning strategy that increases the proportion of
non-English data from 30% in the first stage to 60% in the final stage during
pre-training. Further, we propose a multilingual self-instruct method which
automatically generates 132.7K diverse multilingual instructions for model
fine-tuning. To assess the model's performance, we collect several existing
multilingual tasks, including multilingual understanding, question answering,
generation, and translation. Extensive experiments show that PolyLM surpasses
other open-source models such as LLaMA and BLOOM on multilingual tasks while
maintaining comparable performance in English. Our models, alone with the
instruction data and multilingual benchmark, are available at:
\url{https://modelscope.cn/models/damo/nlp_polylm_13b_text_generation}.
</p></li>
</ul>

<h3>Title: Instruction Mining: High-Quality Instruction Data Selection for Large Language Models. (arXiv:2307.06290v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06290">http://arxiv.org/abs/2307.06290</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06290] Instruction Mining: High-Quality Instruction Data Selection for Large Language Models](http://arxiv.org/abs/2307.06290) #large language model</code></li>
<li>Summary: <p>Large language models typically undergo two training stages, pretraining and
finetuning. Despite that large-scale pretraining endows the model with strong
capabilities to generate natural language responses, these pretrained models
can still fail to understand human instructions at times. To enhance language
models' ability of interpreting and responding to instructions, instruction
finetuning has emerged as a critical method in this area. Recent studies found
that large language models can be finetuned to perform well even with a small
amount of high-quality instruction-following data. However, the selection of
high-quality datasets for finetuning language models still lacks clear
guidelines to follow. In this paper, we propose InstructMining, a linear rule
for evaluating instruction-following data quality. We formulate InstructMining
using specific natural language indicators. To investigate the relationship
between data quality and these indicators, we further conduct extensive
finetuning experiments. The experiment results are then applied to estimating
parameters in InstructMining. To further investigate its performance, we use
InstructMining to select high-quality data from unseen datasets. Results
demonstrate that InstructMining can help select relatively high-quality samples
from various instruction-following datasets. Compared to models finetuned on
unfiltered datasets, models finetuned on InstructMining selected datasets
perform better on 42.5% cases.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: $\mathrm{SAM^{Med}}$: A medical image annotation framework based on large vision model. (arXiv:2307.05617v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05617">http://arxiv.org/abs/2307.05617</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05617] $\mathrm{SAM^{Med}}$: A medical image annotation framework based on large vision model](http://arxiv.org/abs/2307.05617) #segmentation</code></li>
<li>Summary: <p>Recently, large vision model, Segment Anything Model (SAM), has
revolutionized the computer vision field, especially for image segmentation.
SAM presented a new promptable segmentation paradigm that exhibit its
remarkable zero-shot generalization ability. An extensive researches have
explore the potential and limits of SAM in various downstream tasks. In this
study, we presents $\mathrm{SAM^{Med}}$, an enhanced framework for medical
image annotation that leverages the capabilities of SAM. $\mathrm{SAM^{Med}}$
framework consisted of two submodules, namely $\mathrm{SAM^{assist}}$ and
$\mathrm{SAM^{auto}}$. The $\mathrm{SAM^{assist}}$ demonstrates the
generalization ability of SAM to the downstream medical segmentation task using
the prompt-learning approach. Results show a significant improvement in
segmentation accuracy with only approximately 5 input points. The
$\mathrm{SAM^{auto}}$ model aims to accelerate the annotation process by
automatically generating input prompts. The proposed SAP-Net model achieves
superior segmentation performance with only five annotated slices, achieving an
average Dice coefficient of 0.80 and 0.82 for kidney and liver segmentation,
respectively. Overall, $\mathrm{SAM^{Med}}$ demonstrates promising results in
medical image annotation. These findings highlight the potential of leveraging
large-scale vision models in medical image annotation tasks.
</p></li>
</ul>

<h3>Title: HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding. (arXiv:2307.05721v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05721">http://arxiv.org/abs/2307.05721</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05721] HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding](http://arxiv.org/abs/2307.05721) #segmentation</code></li>
<li>Summary: <p>Understanding comprehensive assembly knowledge from videos is critical for
futuristic ultra-intelligent industry. To enable technological breakthrough, we
present HA-ViD - the first human assembly video dataset that features
representative industrial assembly scenarios, natural procedural knowledge
acquisition process, and consistent human-robot shared annotations.
Specifically, HA-ViD captures diverse collaboration patterns of real-world
assembly, natural human behaviors and learning progression during assembly, and
granulate action annotations to subject, action verb, manipulated object,
target object, and tool. We provide 3222 multi-view, multi-modality videos
(each video contains one assembly task), 1.5M frames, 96K temporal labels and
2M spatial labels. We benchmark four foundational video understanding tasks:
action recognition, action segmentation, object detection and multi-object
tracking. Importantly, we analyze their performance for comprehending knowledge
in assembly progress, process efficiency, task collaboration, skill parameters
and human intention. Details of HA-ViD is available at:
https://iai-hrc.github.io/ha-vid.
</p></li>
</ul>

<h3>Title: OG: Equip vision occupancy with instance segmentation and visual grounding. (arXiv:2307.05873v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05873">http://arxiv.org/abs/2307.05873</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05873] OG: Equip vision occupancy with instance segmentation and visual grounding](http://arxiv.org/abs/2307.05873) #segmentation</code></li>
<li>Summary: <p>Occupancy prediction tasks focus on the inference of both geometry and
semantic labels for each voxel, which is an important perception mission.
However, it is still a semantic segmentation task without distinguishing
various instances. Further, although some existing works, such as
Open-Vocabulary Occupancy (OVO), have already solved the problem of open
vocabulary detection, visual grounding in occupancy has not been solved to the
best of our knowledge. To tackle the above two limitations, this paper proposes
Occupancy Grounding (OG), a novel method that equips vanilla occupancy instance
segmentation ability and could operate visual grounding in a voxel manner with
the help of grounded-SAM. Keys to our approach are (1) affinity field
prediction for instance clustering and (2) association strategy for aligning 2D
instance masks and 3D occupancy instances. Extensive experiments have been
conducted whose visualization results and analysis are shown below. Our code
will be publicly released soon.
</p></li>
</ul>

<h3>Title: RFENet: Towards Reciprocal Feature Evolution for Glass Segmentation. (arXiv:2307.06099v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06099">http://arxiv.org/abs/2307.06099</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06099] RFENet: Towards Reciprocal Feature Evolution for Glass Segmentation](http://arxiv.org/abs/2307.06099) #segmentation</code></li>
<li>Summary: <p>Glass-like objects are widespread in daily life but remain intractable to be
segmented for most existing methods. The transparent property makes it
difficult to be distinguished from background, while the tiny separation
boundary further impedes the acquisition of their exact contour. In this paper,
by revealing the key co-evolution demand of semantic and boundary learning, we
propose a Selective Mutual Evolution (SME) module to enable the reciprocal
feature learning between them. Then to exploit the global shape context, we
propose a Structurally Attentive Refinement (SAR) module to conduct a
fine-grained feature refinement for those ambiguous points around the boundary.
Finally, to further utilize the multi-scale representation, we integrate the
above two modules into a cascaded structure and then introduce a Reciprocal
Feature Evolution Network (RFENet) for effective glass-like object
segmentation. Extensive experiments demonstrate that our RFENet achieves
state-of-the-art performance on three popular public datasets.
</p></li>
</ul>

<h3>Title: Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation. (arXiv:2307.06312v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.06312">http://arxiv.org/abs/2307.06312</a></li>
<li>Code URL: <a href="https://github.com/herschel555/caml">https://github.com/herschel555/caml</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.06312] Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation](http://arxiv.org/abs/2307.06312) #segmentation</code></li>
<li>Summary: <p>Semi-supervised learning has become increasingly popular in medical image
segmentation due to its ability to leverage large amounts of unlabeled data to
extract additional information. However, most existing semi-supervised
segmentation methods only focus on extracting information from unlabeled data,
disregarding the potential of labeled data to further improve the performance
of the model. In this paper, we propose a novel Correlation Aware Mutual
Learning (CAML) framework that leverages labeled data to guide the extraction
of information from unlabeled data. Our approach is based on a mutual learning
strategy that incorporates two modules: the Cross-sample Mutual Attention
Module (CMA) and the Omni-Correlation Consistency Module (OCC). The CMA module
establishes dense cross-sample correlations among a group of samples, enabling
the transfer of label prior knowledge to unlabeled data. The OCC module
constructs omni-correlations between the unlabeled and labeled datasets and
regularizes dual models by constraining the omni-correlation matrix of each
sub-model to be consistent. Experiments on the Atrial Segmentation Challenge
dataset demonstrate that our proposed approach outperforms state-of-the-art
methods, highlighting the effectiveness of our framework in medical image
segmentation tasks. The codes, pre-trained weights, and data are publicly
available.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
