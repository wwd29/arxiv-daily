<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Blockchain inspired secure and reliable data exchange architecture for cyber-physical healthcare system 4.0. (arXiv:2307.13603v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13603">http://arxiv.org/abs/2307.13603</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13603] Blockchain inspired secure and reliable data exchange architecture for cyber-physical healthcare system 4](http://arxiv.org/abs/2307.13603) #secure</code></li>
<li>Summary: <p>A cyber-physical system is considered to be a collection of strongly coupled
communication systems and devices that poses numerous security trials in
various industrial applications including healthcare. The security and privacy
of patient data is still a big concern because healthcare data is sensitive and
valuable, and it is most targeted over the internet. Moreover, from the
industrial perspective, the cyber-physical system plays a crucial role in the
exchange of data remotely using sensor nodes in distributed environments. In
the healthcare industry, Blockchain technology offers a promising solution to
resolve most securities-related issues due to its decentralized, immutability,
and transparency properties. In this paper, a blockchain-inspired secure and
reliable data exchange architecture is proposed in the cyber-physical
healthcare industry 4.0. The proposed system uses the BigchainDB, Tendermint,
Inter-Planetary-File-System (IPFS), MongoDB, and AES encryption algorithms to
improve Healthcare 4.0. Furthermore, blockchain-enabled secure healthcare
architecture for accessing and managing the records between Doctors and
Patients is introduced. The development of a blockchain-based Electronic
Healthcare Record (EHR) exchange system is purely patient-centric, which means
the entire control of data is in the owner's hand which is backed by blockchain
for security and privacy. Our experimental results reveal that the proposed
architecture is robust to handle more security attacks and can recover the data
if 2/3 of nodes are failed. The proposed model is patient-centric, and control
of data is in the patient's hand to enhance security and privacy, even system
administrators can't access data without user permission.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Fake It Without Making It: Conditioned Face Generation for Accurate 3D Face Shape Estimation. (arXiv:2307.13639v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13639">http://arxiv.org/abs/2307.13639</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13639] Fake It Without Making It: Conditioned Face Generation for Accurate 3D Face Shape Estimation](http://arxiv.org/abs/2307.13639) #security</code></li>
<li>Summary: <p>Accurate 3D face shape estimation is an enabling technology with applications
in healthcare, security, and creative industries, yet current state-of-the-art
methods either rely on self-supervised training with 2D image data or
supervised training with very limited 3D data. To bridge this gap, we present a
novel approach which uses a conditioned stable diffusion model for face image
generation, leveraging the abundance of 2D facial information to inform 3D
space. By conditioning stable diffusion on depth maps sampled from a 3D
Morphable Model (3DMM) of the human face, we generate diverse and
shape-consistent images, forming the basis of SynthFace. We introduce this
large-scale synthesised dataset of 250K photorealistic images and corresponding
3DMM parameters. We further propose ControlFace, a deep neural network, trained
on SynthFace, which achieves competitive performance on the NoW benchmark,
without requiring 3D supervision or manual 3D asset creation.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering. (arXiv:2307.13231v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13231">http://arxiv.org/abs/2307.13231</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13231] Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering](http://arxiv.org/abs/2307.13231) #privacy</code></li>
<li>Summary: <p>Differential privacy is a widely accepted measure of privacy in the context
of deep learning algorithms, and achieving it relies on a noisy training
approach known as differentially private stochastic gradient descent (DP-SGD).
DP-SGD requires direct noise addition to every gradient in a dense neural
network, the privacy is achieved at a significant utility cost. In this work,
we present Spectral-DP, a new differentially private learning approach which
combines gradient perturbation in the spectral domain with spectral filtering
to achieve a desired privacy guarantee with a lower noise scale and thus better
utility. We develop differentially private deep learning methods based on
Spectral-DP for architectures that contain both convolution and fully connected
layers. In particular, for fully connected layers, we combine a block-circulant
based spatial restructuring with Spectral-DP to achieve better utility. Through
comprehensive experiments, we study and provide guidelines to implement
Spectral-DP deep learning on benchmark datasets. In comparison with
state-of-the-art DP-SGD based approaches, Spectral-DP is shown to have
uniformly better utility performance in both training from scratch and transfer
learning settings.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Personal Protective Equipment Detection in Extreme Construction Conditions. (arXiv:2307.13654v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13654">http://arxiv.org/abs/2307.13654</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13654] Personal Protective Equipment Detection in Extreme Construction Conditions](http://arxiv.org/abs/2307.13654) #protect</code></li>
<li>Summary: <p>Object detection has been widely applied for construction safety management,
especially personal protective equipment (PPE) detection. Though the existing
PPE detection models trained on conventional datasets have achieved excellent
results, their performance dramatically declines in extreme construction
conditions. A robust detection model NST-YOLOv5 is developed by combining the
neural style transfer (NST) and YOLOv5 technologies. Five extreme conditions
are considered and simulated via the NST module to endow the detection model
with excellent robustness, including low light, intense light, sand dust, fog,
and rain. Experiments show that the NST has great potential as a tool for
extreme data synthesis since it is better at simulating extreme conditions than
other traditional image processing algorithms and helps the NST-YOLOv5 achieve
0.141 and 0.083 mAP_(05:95) improvements in synthesized and real-world extreme
data. This study provides a new feasible way to obtain a more robust detection
model for extreme construction conditions.
</p></li>
</ul>

<h3>Title: Malware Resistant Data Protection in Hyper-connected Networks: A survey. (arXiv:2307.13164v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13164">http://arxiv.org/abs/2307.13164</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13164] Malware Resistant Data Protection in Hyper-connected Networks: A survey](http://arxiv.org/abs/2307.13164) #protect</code></li>
<li>Summary: <p>Data protection is the process of securing sensitive information from being
corrupted, compromised, or lost. A hyperconnected network, on the other hand,
is a computer networking trend in which communication occurs over a network.
However, what about malware. Malware is malicious software meant to penetrate
private data, threaten a computer system, or gain unauthorised network access
without the users consent. Due to the increasing applications of computers and
dependency on electronically saved private data, malware attacks on sensitive
information have become a dangerous issue for individuals and organizations
across the world. Hence, malware defense is critical for keeping our computer
systems and data protected. Many recent survey articles have focused on either
malware detection systems or single attacking strategies variously. To the best
of our knowledge, no survey paper demonstrates malware attack patterns and
defense strategies combinedly. Through this survey, this paper aims to address
this issue by merging diverse malicious attack patterns and machine learning
(ML) based detection models for modern and sophisticated malware. In doing so,
we focus on the taxonomy of malware attack patterns based on four fundamental
dimensions the primary goal of the attack, method of attack, targeted exposure
and execution process, and types of malware that perform each attack. Detailed
information on malware analysis approaches is also investigated. In addition,
existing malware detection techniques employing feature extraction and ML
algorithms are discussed extensively. Finally, it discusses research
difficulties and unsolved problems, including future research directions.
</p></li>
</ul>

<h3>Title: The Double-Edged Sword of Big Data and Information Technology for the Disadvantaged: A Cautionary Tale from Open Banking. (arXiv:2307.13408v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13408">http://arxiv.org/abs/2307.13408</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13408] The Double-Edged Sword of Big Data and Information Technology for the Disadvantaged: A Cautionary Tale from Open Banking](http://arxiv.org/abs/2307.13408) #protect</code></li>
<li>Summary: <p>This research article analyses and demonstrates the hidden implications for
fairness of seemingly neutral data coupled with powerful technology, such as
machine learning (ML), using Open Banking as an example. Open Banking has
ignited a revolution in financial services, opening new opportunities for
customer acquisition, management, retention, and risk assessment. However, the
granularity of transaction data holds potential for harm where unnoticed
proxies for sensitive and prohibited characteristics may lead to indirect
discrimination. Against this backdrop, we investigate the dimensions of
financial vulnerability (FV), a global concern resulting from COVID-19 and
rising inflation. Specifically, we look to understand the behavioral elements
leading up to FV and its impact on at-risk, disadvantaged groups through the
lens of fair interpretation. Using a unique dataset from a UK FinTech lender,
we demonstrate the power of fine-grained transaction data while simultaneously
cautioning its safe usage. Three ML classifiers are compared in predicting the
likelihood of FV, and groups exhibiting different magnitudes and forms of FV
are identified via clustering to highlight the effects of feature combination.
Our results indicate that engineered features of financial behavior can be
predictive of omitted personal information, particularly sensitive or protected
characteristics, shedding light on the hidden dangers of Open Banking data. We
discuss the implications and conclude fairness via unawareness is ineffective
in this new technological environment.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Imperceptible Physical Attack against Face Recognition Systems via LED Illumination Modulation. (arXiv:2307.13294v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13294">http://arxiv.org/abs/2307.13294</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13294] Imperceptible Physical Attack against Face Recognition Systems via LED Illumination Modulation](http://arxiv.org/abs/2307.13294) #attack</code></li>
<li>Summary: <p>Although face recognition starts to play an important role in our daily life,
we need to pay attention that data-driven face recognition vision systems are
vulnerable to adversarial attacks. However, the current two categories of
adversarial attacks, namely digital attacks and physical attacks both have
drawbacks, with the former ones impractical and the latter one conspicuous,
high-computational and inexecutable. To address the issues, we propose a
practical, executable, inconspicuous and low computational adversarial attack
based on LED illumination modulation. To fool the systems, the proposed attack
generates imperceptible luminance changes to human eyes through fast intensity
modulation of scene LED illumination and uses the rolling shutter effect of
CMOS image sensors in face recognition systems to implant luminance information
perturbation to the captured face images. In summary,we present a
denial-of-service (DoS) attack for face detection and a dodging attack for face
verification. We also evaluate their effectiveness against well-known face
detection models, Dlib, MTCNN and RetinaFace , and face verification models,
Dlib, FaceNet,and ArcFace.The extensive experiments show that the success rates
of DoS attacks against face detection models reach 97.67%, 100%, and 100%,
respectively, and the success rates of dodging attacks against all face
verification models reach 100%.
</p></li>
</ul>

<h3>Title: Mitigating Cross-client GANs-based Attack in Federated Learning. (arXiv:2307.13314v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13314">http://arxiv.org/abs/2307.13314</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13314] Mitigating Cross-client GANs-based Attack in Federated Learning](http://arxiv.org/abs/2307.13314) #attack</code></li>
<li>Summary: <p>Machine learning makes multimedia data (e.g., images) more attractive,
however, multimedia data is usually distributed and privacy sensitive. Multiple
distributed multimedia clients can resort to federated learning (FL) to jointly
learn a global shared model without requiring to share their private samples
with any third-party entities. In this paper, we show that FL suffers from the
cross-client generative adversarial networks (GANs)-based (C-GANs) attack, in
which a malicious client (i.e., adversary) can reconstruct samples with the
same distribution as the training samples from other clients (i.e., victims).
Since a benign client's data can be leaked to the adversary, this attack brings
the risk of local data leakage for clients in many security-critical FL
applications. Thus, we propose Fed-EDKD (i.e., Federated Ensemble Data-free
Knowledge Distillation) technique to improve the current popular FL schemes to
resist C-GANs attack. In Fed-EDKD, each client submits a local model to the
server for obtaining an ensemble global model. Then, to avoid model expansion,
Fed-EDKD adopts data-free knowledge distillation techniques to transfer
knowledge from the ensemble global model to a compressed model. By this way,
Fed-EDKD reduces the adversary's control capability over the global model, so
Fed-EDKD can effectively mitigate C-GANs attack. Finally, the experimental
results demonstrate that Fed-EDKD significantly mitigates C-GANs attack while
only incurring a slight accuracy degradation of FL.
</p></li>
</ul>

<h3>Title: Why Don't You Clean Your Glasses? Perception Attacks with Dynamic Optical Perturbations. (arXiv:2307.13131v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13131">http://arxiv.org/abs/2307.13131</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13131] Why Don't You Clean Your Glasses? Perception Attacks with Dynamic Optical Perturbations](http://arxiv.org/abs/2307.13131) #attack</code></li>
<li>Summary: <p>Camera-based autonomous systems that emulate human perception are
increasingly being integrated into safety-critical platforms. Consequently, an
established body of literature has emerged that explores adversarial attacks
targeting the underlying machine learning models. Adapting adversarial attacks
to the physical world is desirable for the attacker, as this removes the need
to compromise digital systems. However, the real world poses challenges related
to the "survivability" of adversarial manipulations given environmental noise
in perception pipelines and the dynamicity of autonomous systems. In this
paper, we take a sensor-first approach. We present EvilEye, a man-in-the-middle
perception attack that leverages transparent displays to generate dynamic
physical adversarial examples. EvilEye exploits the camera's optics to induce
misclassifications under a variety of illumination conditions. To generate
dynamic perturbations, we formalize the projection of a digital attack into the
physical domain by modeling the transformation function of the captured image
through the optical pipeline. Our extensive experiments show that EvilEye's
generated adversarial perturbations are much more robust across varying
environmental light conditions relative to existing physical perturbation
frameworks, achieving a high attack success rate (ASR) while bypassing
state-of-the-art physical adversarial detection frameworks. We demonstrate that
the dynamic nature of EvilEye enables attackers to adapt adversarial examples
across a variety of objects with a significantly higher ASR compared to
state-of-the-art physical world attack frameworks. Finally, we discuss
mitigation strategies against the EvilEye attack.
</p></li>
</ul>

<h3>Title: Attacks on Dynamic DeFi Interest Rate Curves. (arXiv:2307.13139v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13139">http://arxiv.org/abs/2307.13139</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13139] Attacks on Dynamic DeFi Interest Rate Curves](http://arxiv.org/abs/2307.13139) #attack</code></li>
<li>Summary: <p>As decentralized money market protocols continue to grow in value locked,
there have been a number of optimizations proposed for improving capital
efficiency. One set of proposals from Euler Finance and Mars Protocol is to
have an interest rate curve that is a proportional-integral-derivative (PID)
controller. In this paper, we demonstrate attacks on proportional and
proportional-integral controlled interest rate curves. The attack allows one to
manipulate the interest rate curve to take a higher proportion of the earned
yield than their pro-rata share of the lending pool. We conclude with an
argument that PID interest rate curves can actually \emph{reduce} capital
efficiency (due to attack mitigations) unless supply and demand elasticity to
rate changes are sufficiently high.
</p></li>
</ul>

<h3>Title: High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers. (arXiv:2307.13352v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13352">http://arxiv.org/abs/2307.13352</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13352] High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers](http://arxiv.org/abs/2307.13352) #attack</code></li>
<li>Summary: <p>Robust distributed learning with Byzantine failures has attracted extensive
research interests in recent years. However, most of existing methods suffer
from curse of dimensionality, which is increasingly serious with the growing
complexity of modern machine learning models. In this paper, we design a new
method that is suitable for high dimensional problems, under arbitrary number
of Byzantine attackers. The core of our design is a direct high dimensional
semi-verified mean estimation method. Our idea is to identify a subspace first.
The components of mean value perpendicular to this subspace can be estimated
via gradient vectors uploaded from worker machines, while the components within
this subspace are estimated using auxiliary dataset. We then use our new method
as the aggregator of distributed learning problems. Our theoretical analysis
shows that the new method has minimax optimal statistical rates. In particular,
the dependence on dimensionality is significantly improved compared with
previous works.
</p></li>
</ul>

<h3>Title: Node Injection Link Stealing Attack. (arXiv:2307.13548v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13548">http://arxiv.org/abs/2307.13548</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13548] Node Injection Link Stealing Attack](http://arxiv.org/abs/2307.13548) #attack</code></li>
<li>Summary: <p>In this paper, we present a stealthy and effective attack that exposes
privacy vulnerabilities in Graph Neural Networks (GNNs) by inferring private
links within graph-structured data. Focusing on the inductive setting where new
nodes join the graph and an API is used to query predictions, we investigate
the potential leakage of private edge information. We also propose methods to
preserve privacy while maintaining model utility. Our attack demonstrates
superior performance in inferring the links compared to the state of the art.
Furthermore, we examine the application of differential privacy (DP) mechanisms
to mitigate the impact of our proposed attack, we analyze the trade-off between
privacy preservation and model utility. Our work highlights the privacy
vulnerabilities inherent in GNNs, underscoring the importance of developing
robust privacy-preserving mechanisms for their application.
</p></li>
</ul>

<h3>Title: Backdoor Attacks against Voice Recognition Systems: A Survey. (arXiv:2307.13643v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13643">http://arxiv.org/abs/2307.13643</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13643] Backdoor Attacks against Voice Recognition Systems: A Survey](http://arxiv.org/abs/2307.13643) #attack</code></li>
<li>Summary: <p>Voice Recognition Systems (VRSs) employ deep learning for speech recognition
and speaker recognition. They have been widely deployed in various real-world
applications, from intelligent voice assistance to telephony surveillance and
biometric authentication. However, prior research has revealed the
vulnerability of VRSs to backdoor attacks, which pose a significant threat to
the security and privacy of VRSs. Unfortunately, existing literature lacks a
thorough review on this topic. This paper fills this research gap by conducting
a comprehensive survey on backdoor attacks against VRSs. We first present an
overview of VRSs and backdoor attacks, elucidating their basic knowledge. Then
we propose a set of evaluation criteria to assess the performance of backdoor
attack methods. Next, we present a comprehensive taxonomy of backdoor attacks
against VRSs from different perspectives and analyze the characteristic of
different categories. After that, we comprehensively review existing attack
methods and analyze their pros and cons based on the proposed criteria.
Furthermore, we review classic backdoor defense methods and generic audio
defense techniques. Then we discuss the feasibility of deploying them on VRSs.
Finally, we figure out several open issues and further suggest future research
directions to motivate the research of VRSs security.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs. (arXiv:2307.13078v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13078">http://arxiv.org/abs/2307.13078</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13078] Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs](http://arxiv.org/abs/2307.13078) #robust</code></li>
<li>Summary: <p>As deep learning models continue to advance and are increasingly utilized in
real-world systems, the issue of robustness remains a major challenge. Existing
certified training methods produce models that achieve high provable robustness
guarantees at certain perturbation levels. However, the main problem of such
models is a dramatically low standard accuracy, i.e. accuracy on clean
unperturbed data, that makes them impractical. In this work, we consider a more
realistic perspective of maximizing the robustness of a model at certain levels
of (high) standard accuracy. To this end, we propose a novel certified training
method based on a key insight that training with adaptive certified radii helps
to improve both the accuracy and robustness of the model, advancing
state-of-the-art accuracy-robustness tradeoffs. We demonstrate the
effectiveness of the proposed method on MNIST, CIFAR-10, and TinyImageNet
datasets. Particularly, on CIFAR-10 and TinyImageNet, our method yields models
with up to two times higher robustness, measured as an average certified radius
of a test set, at the same levels of standard accuracy compared to baseline
approaches.
</p></li>
</ul>

<h3>Title: Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?. (arXiv:2307.13136v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13136">http://arxiv.org/abs/2307.13136</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13136] Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?](http://arxiv.org/abs/2307.13136) #robust</code></li>
<li>Summary: <p>For more than a decade, researchers have measured progress in object
recognition on ImageNet-based generalization benchmarks such as ImageNet-A, -C,
and -R. Recent advances in foundation models, trained on orders of magnitude
more data, have begun to saturate these standard benchmarks, but remain brittle
in practice. This suggests standard benchmarks, which tend to focus on
predefined or synthetic changes, may not be sufficient for measuring real world
generalization. Consequently, we propose studying generalization across
geography as a more realistic measure of progress using two datasets of objects
from households across the globe. We conduct an extensive empirical evaluation
of progress across nearly 100 vision models up to most recent foundation
models. We first identify a progress gap between standard benchmarks and
real-world, geographical shifts: progress on ImageNet results in up to 2.5x
more progress on standard generalization benchmarks than real-world
distribution shifts. Second, we study model generalization across geographies
by measuring the disparities in performance across regions, a more fine-grained
measure of real world generalization. We observe all models have large
geographic disparities, even foundation CLIP models, with differences of 7-20%
in accuracy between regions. Counter to modern intuition, we discover progress
on standard benchmarks fails to improve geographic disparities and often
exacerbates them: geographic disparities between the least performant models
and today's best models have more than tripled. Our results suggest scaling
alone is insufficient for consistent robustness to real-world distribution
shifts. Finally, we highlight in early experiments how simple last layer
retraining on more representative, curated data can complement scaling as a
promising direction of future work, reducing geographic disparity on both
benchmarks by over two-thirds.
</p></li>
</ul>

<h3>Title: A signal processing interpretation of noise-reduction convolutional neural networks. (arXiv:2307.13425v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13425">http://arxiv.org/abs/2307.13425</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13425] A signal processing interpretation of noise-reduction convolutional neural networks](http://arxiv.org/abs/2307.13425) #robust</code></li>
<li>Summary: <p>Encoding-decoding CNNs play a central role in data-driven noise reduction and
can be found within numerous deep-learning algorithms. However, the development
of these CNN architectures is often done in ad-hoc fashion and theoretical
underpinnings for important design choices is generally lacking. Up to this
moment there are different existing relevant works that strive to explain the
internal operation of these CNNs. Still, these ideas are either scattered
and/or may require significant expertise to be accessible for a bigger
audience. In order to open up this exciting field, this article builds
intuition on the theory of deep convolutional framelets and explains diverse ED
CNN architectures in a unified theoretical framework. By connecting basic
principles from signal processing to the field of deep learning, this
self-contained material offers significant guidance for designing robust and
efficient novel CNN architectures.
</p></li>
</ul>

<h3>Title: NormAUG: Normalization-guided Augmentation for Domain Generalization. (arXiv:2307.13492v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13492">http://arxiv.org/abs/2307.13492</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13492] NormAUG: Normalization-guided Augmentation for Domain Generalization](http://arxiv.org/abs/2307.13492) #robust</code></li>
<li>Summary: <p>Deep learning has made significant advancements in supervised learning.
However, models trained in this setting often face challenges due to domain
shift between training and test sets, resulting in a significant drop in
performance during testing. To address this issue, several domain
generalization methods have been developed to learn robust and domain-invariant
features from multiple training domains that can generalize well to unseen test
domains. Data augmentation plays a crucial role in achieving this goal by
enhancing the diversity of the training data. In this paper, inspired by the
observation that normalizing an image with different statistics generated by
different batches with various domains can perturb its feature, we propose a
simple yet effective method called NormAUG (Normalization-guided Augmentation).
Our method includes two paths: the main path and the auxiliary (augmented)
path. During training, the auxiliary path includes multiple sub-paths, each
corresponding to batch normalization for a single domain or a random
combination of multiple domains. This introduces diverse information at the
feature level and improves the generalization of the main path. Moreover, our
NormAUG method effectively reduces the existing upper boundary for
generalization based on theoretical perspectives. During the test stage, we
leverage an ensemble strategy to combine the predictions from the auxiliary
path of our model, further boosting performance. Extensive experiments are
conducted on multiple benchmark datasets to validate the effectiveness of our
proposed method.
</p></li>
</ul>

<h3>Title: MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning. (arXiv:2307.13055v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13055">http://arxiv.org/abs/2307.13055</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13055] MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning](http://arxiv.org/abs/2307.13055) #robust</code></li>
<li>Summary: <p>In this work, we investigate the problem of out-of-distribution (OOD)
generalization for unsupervised learning methods on graph data. This scenario
is particularly challenging because graph neural networks (GNNs) have been
shown to be sensitive to distributional shifts, even when labels are available.
To address this challenge, we propose a \underline{M}odel-\underline{A}gnostic
\underline{R}ecipe for \underline{I}mproving \underline{O}OD generalizability
of unsupervised graph contrastive learning methods, which we refer to as MARIO.
MARIO introduces two principles aimed at developing distributional-shift-robust
graph contrastive methods to overcome the limitations of existing frameworks:
(i) Information Bottleneck (IB) principle for achieving generalizable
representations and (ii) Invariant principle that incorporates adversarial data
augmentation to obtain invariant representations. To the best of our knowledge,
this is the first work that investigates the OOD generalization problem of
graph contrastive learning, with a specific focus on node-level tasks. Through
extensive experiments, we demonstrate that our method achieves state-of-the-art
performance on the OOD test set, while maintaining comparable performance on
the in-distribution test set when compared to existing approaches. The source
code for our method can be found at: https://github.com/ZhuYun97/MARIO
</p></li>
</ul>

<h3>Title: Label Noise: Correcting a Correction. (arXiv:2307.13100v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13100">http://arxiv.org/abs/2307.13100</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13100] Label Noise: Correcting a Correction](http://arxiv.org/abs/2307.13100) #robust</code></li>
<li>Summary: <p>Training neural network classifiers on datasets with label noise poses a risk
of overfitting them to the noisy labels. To address this issue, researchers
have explored alternative loss functions that aim to be more robust. However,
many of these alternatives are heuristic in nature and still vulnerable to
overfitting or underfitting. In this work, we propose a more direct approach to
tackling overfitting caused by label noise. We observe that the presence of
label noise implies a lower bound on the noisy generalised risk. Building upon
this observation, we propose imposing a lower bound on the empirical risk
during training to mitigate overfitting. Our main contribution is providing
theoretical results that yield explicit, easily computable bounds on the
minimum achievable noisy risk for different loss functions. We empirically
demonstrate that using these bounds significantly enhances robustness in
various settings, with virtually no additional computational cost.
</p></li>
</ul>

<h3>Title: Contrastive Example-Based Control. (arXiv:2307.13101v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13101">http://arxiv.org/abs/2307.13101</a></li>
<li>Code URL: <a href="https://github.com/khatch31/laeo">https://github.com/khatch31/laeo</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13101] Contrastive Example-Based Control](http://arxiv.org/abs/2307.13101) #robust</code></li>
<li>Summary: <p>While many real-world problems that might benefit from reinforcement
learning, these problems rarely fit into the MDP mold: interacting with the
environment is often expensive and specifying reward functions is challenging.
Motivated by these challenges, prior work has developed data-driven approaches
that learn entirely from samples from the transition dynamics and examples of
high-return states. These methods typically learn a reward function from
high-return states, use that reward function to label the transitions, and then
apply an offline RL algorithm to these transitions. While these methods can
achieve good results on many tasks, they can be complex, often requiring
regularization and temporal difference updates. In this paper, we propose a
method for offline, example-based control that learns an implicit model of
multi-step transitions, rather than a reward function. We show that this
implicit model can represent the Q-values for the example-based control
problem. Across a range of state-based and image-based offline control tasks,
our method outperforms baselines that use learned reward functions; additional
experiments demonstrate improved robustness and scaling with dataset size.
</p></li>
</ul>

<h3>Title: RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision. (arXiv:2307.13239v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13239">http://arxiv.org/abs/2307.13239</a></li>
<li>Code URL: <a href="https://github.com/xuhongzuo/rosas">https://github.com/xuhongzuo/rosas</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13239] RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision](http://arxiv.org/abs/2307.13239) #robust</code></li>
<li>Summary: <p>Semi-supervised anomaly detection methods leverage a few anomaly examples to
yield drastically improved performance compared to unsupervised models.
However, they still suffer from two limitations: 1) unlabeled anomalies (i.e.,
anomaly contamination) may mislead the learning process when all the unlabeled
data are employed as inliers for model training; 2) only discrete supervision
information (such as binary or ordinal data labels) is exploited, which leads
to suboptimal learning of anomaly scores that essentially take on a continuous
distribution. Therefore, this paper proposes a novel semi-supervised anomaly
detection method, which devises \textit{contamination-resilient continuous
supervisory signals}. Specifically, we propose a mass interpolation method to
diffuse the abnormality of labeled anomalies, thereby creating new data samples
labeled with continuous abnormal degrees. Meanwhile, the contaminated area can
be covered by new data samples generated via combinations of data with correct
labels. A feature learning-based objective is added to serve as an optimization
constraint to regularize the network and further enhance the robustness w.r.t.
anomaly contamination. Extensive experiments on 11 real-world datasets show
that our approach significantly outperforms state-of-the-art competitors by
20%-30% in AUC-PR and obtains more robust and superior performance in settings
with different anomaly contamination levels and varying numbers of labeled
anomalies. The source code is available at https://github.com/xuhongzuo/rosas/.
</p></li>
</ul>

<h3>Title: Scaff-PD: Communication Efficient Fair and Robust Federated Learning. (arXiv:2307.13381v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13381">http://arxiv.org/abs/2307.13381</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13381] Scaff-PD: Communication Efficient Fair and Robust Federated Learning](http://arxiv.org/abs/2307.13381) #robust</code></li>
<li>Summary: <p>We present Scaff-PD, a fast and communication-efficient algorithm for
distributionally robust federated learning. Our approach improves fairness by
optimizing a family of distributionally robust objectives tailored to
heterogeneous clients. We leverage the special structure of these objectives,
and design an accelerated primal dual (APD) algorithm which uses bias corrected
local steps (as in Scaffold) to achieve significant gains in communication
efficiency and convergence speed. We evaluate Scaff-PD on several benchmark
datasets and demonstrate its effectiveness in improving fairness and robustness
while maintaining competitive accuracy. Our results suggest that Scaff-PD is a
promising approach for federated learning in resource-constrained and
heterogeneous settings.
</p></li>
</ul>

<h3>Title: PT$\mathrm{L}^{p}$: Partial Transport $\mathrm{L}^{p}$ Distances. (arXiv:2307.13571v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13571">http://arxiv.org/abs/2307.13571</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13571] PT$\mathrm{L}^{p}$: Partial Transport $\mathrm{L}^{p}$ Distances](http://arxiv.org/abs/2307.13571) #robust</code></li>
<li>Summary: <p>Optimal transport and its related problems, including optimal partial
transport, have proven to be valuable tools in machine learning for computing
meaningful distances between probability or positive measures. This success has
led to a growing interest in defining transport-based distances that allow for
comparing signed measures and, more generally, multi-channeled signals.
Transport $\mathrm{L}^{p}$ distances are notable extensions of the optimal
transport framework to signed and possibly multi-channeled signals. In this
paper, we introduce partial transport $\mathrm{L}^{p}$ distances as a new
family of metrics for comparing generic signals, benefiting from the robustness
of partial transport distances. We provide theoretical background such as the
existence of optimal plans and the behavior of the distance in various limits.
Furthermore, we introduce the sliced variation of these distances, which allows
for rapid comparison of generic signals. Finally, we demonstrate the
application of the proposed distances in signal class separability and nearest
neighbor classification.
</p></li>
</ul>

<h3>Title: Scaling machine learning-based chemical plant simulation: A method for fine-tuning a model to induce stable fixed points. (arXiv:2307.13621v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13621">http://arxiv.org/abs/2307.13621</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13621] Scaling machine learning-based chemical plant simulation: A method for fine-tuning a model to induce stable fixed points](http://arxiv.org/abs/2307.13621) #robust</code></li>
<li>Summary: <p>Idealized first-principles models of chemical plants can be inaccurate. An
alternative is to fit a Machine Learning (ML) model directly to plant sensor
data. We use a structured approach: Each unit within the plant gets represented
by one ML model. After fitting the models to the data, the models are connected
into a flowsheet-like directed graph. We find that for smaller plants, this
approach works well, but for larger plants, the complex dynamics arising from
large and nested cycles in the flowsheet lead to instabilities in the cycle
solver. We analyze this problem in depth and show that it is not merely a
specialized concern but rather a more pervasive challenge that will likely
occur whenever ML is applied to larger plants. To address this problem, we
present a way to fine-tune ML models such that solving cycles with the usual
methods becomes robust again.
</p></li>
</ul>

<h3>Title: Safety Margins for Reinforcement Learning. (arXiv:2307.13642v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13642">http://arxiv.org/abs/2307.13642</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13642] Safety Margins for Reinforcement Learning](http://arxiv.org/abs/2307.13642) #robust</code></li>
<li>Summary: <p>Any autonomous controller will be unsafe in some situations. The ability to
quantitatively identify when these unsafe situations are about to occur is
crucial for drawing timely human oversight in, e.g., freight transportation
applications. In this work, we demonstrate that the true criticality of an
agent's situation can be robustly defined as the mean reduction in reward given
some number of random actions. Proxy criticality metrics that are computable in
real-time (i.e., without actually simulating the effects of random actions) can
be compared to the true criticality, and we show how to leverage these proxy
metrics to generate safety margins, which directly tie the consequences of
potentially incorrect actions to an anticipated loss in overall performance. We
evaluate our approach on learned policies from APE-X and A3C within an Atari
environment, and demonstrate how safety margins decrease as agents approach
failure states. The integration of safety margins into programs for monitoring
deployed agents allows for the real-time identification of potentially
catastrophic situations.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: An Explainable Model-Agnostic Algorithm for CNN-based Biometrics Verification. (arXiv:2307.13428v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13428">http://arxiv.org/abs/2307.13428</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13428] An Explainable Model-Agnostic Algorithm for CNN-based Biometrics Verification](http://arxiv.org/abs/2307.13428) #biometric</code></li>
<li>Summary: <p>This paper describes an adaptation of the Local Interpretable Model-Agnostic
Explanations (LIME) AI method to operate under a biometric verification
setting. LIME was initially proposed for networks with the same output classes
used for training, and it employs the softmax probability to determine which
regions of the image contribute the most to classification. However, in a
verification setting, the classes to be recognized have not been seen during
training. In addition, instead of using the softmax output, face descriptors
are usually obtained from a layer before the classification layer. The model is
adapted to achieve explainability via cosine similarity between feature vectors
of perturbated versions of the input image. The method is showcased for face
biometrics with two CNN models based on MobileNetv2 and ResNet50.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Prior Based Online Lane Graph Extraction from Single Onboard Camera Image. (arXiv:2307.13344v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13344">http://arxiv.org/abs/2307.13344</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13344] Prior Based Online Lane Graph Extraction from Single Onboard Camera Image](http://arxiv.org/abs/2307.13344) #extraction</code></li>
<li>Summary: <p>The local road network information is essential for autonomous navigation.
This information is commonly obtained from offline HD-Maps in terms of lane
graphs. However, the local road network at a given moment can be drastically
different than the one given in the offline maps; due to construction works,
accidents etc. Moreover, the autonomous vehicle might be at a location not
covered in the offline HD-Map. Thus, online estimation of the lane graph is
crucial for widespread and reliable autonomous navigation. In this work, we
tackle online Bird's-Eye-View lane graph extraction from a single onboard
camera image. We propose to use prior information to increase quality of the
estimations. The prior is extracted from the dataset through a transformer
based Wasserstein Autoencoder. The autoencoder is then used to enhance the
initial lane graph estimates. This is done through optimization of the latent
space vector. The optimization encourages the lane graph estimation to be
logical by discouraging it to diverge from the prior distribution. We test the
method on two benchmark datasets, NuScenes and Argoverse. The results show that
the proposed method significantly improves the performance compared to
state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Weakly-supervised 3D Pose Transfer with Keypoints. (arXiv:2307.13459v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13459">http://arxiv.org/abs/2307.13459</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13459] Weakly-supervised 3D Pose Transfer with Keypoints](http://arxiv.org/abs/2307.13459) #extraction</code></li>
<li>Summary: <p>The main challenges of 3D pose transfer are: 1) Lack of paired training data
with different characters performing the same pose; 2) Disentangling pose and
shape information from the target mesh; 3) Difficulty in applying to meshes
with different topologies. We thus propose a novel weakly-supervised
keypoint-based framework to overcome these difficulties. Specifically, we use a
topology-agnostic keypoint detector with inverse kinematics to compute
transformations between the source and target meshes. Our method only requires
supervision on the keypoints, can be applied to meshes with different
topologies and is shape-invariant for the target which allows extraction of
pose-only information from the target meshes without transferring shape
information. We further design a cycle reconstruction to perform
self-supervised pose transfer without the need for ground truth deformed mesh
with the same pose and shape as the target and source, respectively. We
evaluate our approach on benchmark human and animal datasets, where we achieve
superior performance compared to the state-of-the-art unsupervised approaches
and even comparable performance with the fully supervised approaches. We test
on the more challenging Mixamo dataset to verify our approach's ability in
handling meshes with different topologies and complex clothes. Cross-dataset
evaluation further shows the strong generalization ability of our approach.
</p></li>
</ul>

<h3>Title: Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction. (arXiv:2307.13497v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13497">http://arxiv.org/abs/2307.13497</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13497] Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction](http://arxiv.org/abs/2307.13497) #extraction</code></li>
<li>Summary: <p>The Zero-Shot Learning (ZSL) task pertains to the identification of entities
or relations in texts that were not seen during training. ZSL has emerged as a
critical research area due to the scarcity of labeled data in specific domains,
and its applications have grown significantly in recent years. With the advent
of large pretrained language models, several novel methods have been proposed,
resulting in substantial improvements in ZSL performance. There is a growing
demand, both in the research community and industry, for a comprehensive ZSL
framework that facilitates the development and accessibility of the latest
methods and pretrained models.In this study, we propose a novel ZSL framework
called Zshot that aims to address the aforementioned challenges. Our primary
objective is to provide a platform that allows researchers to compare different
state-of-the-art ZSL methods with standard benchmark datasets. Additionally, we
have designed our framework to support the industry with readily available APIs
for production under the standard SpaCy NLP pipeline. Our API is extendible and
evaluable, moreover, we include numerous enhancements such as boosting the
accuracy with pipeline ensembling and visualization utilities available as a
SpaCy extension.
</p></li>
</ul>

<h3>Title: Contributions to the Improvement of Question Answering Systems in the Biomedical Domain. (arXiv:2307.13631v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13631">http://arxiv.org/abs/2307.13631</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13631] Contributions to the Improvement of Question Answering Systems in the Biomedical Domain](http://arxiv.org/abs/2307.13631) #extraction</code></li>
<li>Summary: <p>This thesis work falls within the framework of question answering (QA) in the
biomedical domain where several specific challenges are addressed, such as
specialized lexicons and terminologies, the types of treated questions, and the
characteristics of targeted documents. We are particularly interested in
studying and improving methods that aim at finding accurate and short answers
to biomedical natural language questions from a large scale of biomedical
textual documents in English. QA aims at providing inquirers with direct, short
and precise answers to their natural language questions. In this Ph.D. thesis,
we propose four contributions to improve the performance of QA in the
biomedical domain. In our first contribution, we propose a machine
learning-based method for question type classification to determine the types
of given questions which enable to a biomedical QA system to use the
appropriate answer extraction method. We also propose an another machine
learning-based method to assign one or more topics (e.g., pharmacological,
test, treatment, etc.) to given questions in order to determine the semantic
types of the expected answers which are very useful in generating specific
answer retrieval strategies. In the second contribution, we first propose a
document retrieval method to retrieve a set of relevant documents that are
likely to contain the answers to biomedical questions from the MEDLINE
database. We then present a passage retrieval method to retrieve a set of
relevant passages to questions. In the third contribution, we propose specific
answer extraction methods to generate both exact and ideal answers. Finally, in
the fourth contribution, we develop a fully automated semantic biomedical QA
system called SemBioNLQA which is able to deal with a variety of natural
language questions and to generate appropriate answers by providing both exact
and ideal answers.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning. (arXiv:2307.13214v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13214">http://arxiv.org/abs/2307.13214</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13214] FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning](http://arxiv.org/abs/2307.13214) #federate</code></li>
<li>Summary: <p>Federated learning (FL) enables a decentralized machine learning paradigm for
multiple clients to collaboratively train a generalized global model without
sharing their private data. Most existing works simply propose typical FL
systems for single-modal data, thus limiting its potential on exploiting
valuable multimodal data for future personalized applications. Furthermore, the
majority of FL approaches still rely on the labeled data at the client side,
which is limited in real-world applications due to the inability of
self-annotation from users. In light of these limitations, we propose a novel
multimodal FL framework that employs a semi-supervised learning approach to
leverage the representations from different modalities. Bringing this concept
into a system, we develop a distillation-based multimodal embedding knowledge
transfer mechanism, namely FedMEKT, which allows the server and clients to
exchange the joint knowledge of their learning models extracted from a small
multimodal proxy dataset. Our FedMEKT iteratively updates the generalized
global encoders with the joint embedding knowledge from the participating
clients. Thereby, to address the modality discrepancy and labeled data
constraint in existing FL systems, our proposed FedMEKT comprises local
multimodal autoencoder learning, generalized multimodal autoencoder
construction, and generalized classifier learning. Through extensive
experiments on three multimodal human activity recognition datasets, we
demonstrate that FedMEKT achieves superior global encoder performance on linear
evaluation and guarantees user privacy for personal data and model parameters
while demanding less communication cost than other baselines.
</p></li>
</ul>

<h3>Title: Federated Split Learning with Only Positive Labels for resource-constrained IoT environment. (arXiv:2307.13266v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13266">http://arxiv.org/abs/2307.13266</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13266] Federated Split Learning with Only Positive Labels for resource-constrained IoT environment](http://arxiv.org/abs/2307.13266) #federate</code></li>
<li>Summary: <p>Distributed collaborative machine learning (DCML) is a promising method in
the Internet of Things (IoT) domain for training deep learning models, as data
is distributed across multiple devices. A key advantage of this approach is
that it improves data privacy by removing the necessity for the centralized
aggregation of raw data but also empowers IoT devices with low computational
power. Among various techniques in a DCML framework, federated split learning,
known as splitfed learning (SFL), is the most suitable for efficient training
and testing when devices have limited computational capabilities. Nevertheless,
when resource-constrained IoT devices have only positive labeled data,
multiclass classification deep learning models in SFL fail to converge or
provide suboptimal results. To overcome these challenges, we propose splitfed
learning with positive labels (SFPL). SFPL applies a random shuffling function
to the smashed data received from clients before supplying it to the server for
model training. Additionally, SFPL incorporates the local batch normalization
for the client-side model portion during the inference phase. Our results
demonstrate that SFPL outperforms SFL: (i) by factors of 51.54 and 32.57 for
ResNet-56 and ResNet-32, respectively, with the CIFAR-100 dataset, and (ii) by
factors of 9.23 and 8.52 for ResNet-32 and ResNet-8, respectively, with
CIFAR-10 dataset. Overall, this investigation underscores the efficacy of the
proposed SFPL framework in DCML.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Making Metadata More FAIR Using Large Language Models. (arXiv:2307.13085v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13085">http://arxiv.org/abs/2307.13085</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13085] Making Metadata More FAIR Using Large Language Models](http://arxiv.org/abs/2307.13085) #fair</code></li>
<li>Summary: <p>With the global increase in experimental data artifacts, harnessing them in a
unified fashion leads to a major stumbling block - bad metadata. To bridge this
gap, this work presents a Natural Language Processing (NLP) informed
application, called FAIRMetaText, that compares metadata. Specifically,
FAIRMetaText analyzes the natural language descriptions of metadata and
provides a mathematical similarity measure between two terms. This measure can
then be utilized for analyzing varied metadata, by suggesting terms for
compliance or grouping similar terms for identification of replaceable terms.
The efficacy of the algorithm is presented qualitatively and quantitatively on
publicly available research artifacts and demonstrates large gains across
metadata related tasks through an in-depth study of a wide variety of Large
Language Models (LLMs). This software can drastically reduce the human effort
in sifting through various natural language metadata while employing several
experimental datasets on the same topic.
</p></li>
</ul>

<h3>Title: Fairness Under Demographic Scarce Regime. (arXiv:2307.13081v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13081">http://arxiv.org/abs/2307.13081</a></li>
<li>Code URL: <a href="https://github.com/patrikken/fair-dsr">https://github.com/patrikken/fair-dsr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13081] Fairness Under Demographic Scarce Regime](http://arxiv.org/abs/2307.13081) #fair</code></li>
<li>Summary: <p>Most existing works on fairness assume the model has full access to
demographic information. However, there exist scenarios where demographic
information is partially available because a record was not maintained
throughout data collection or due to privacy reasons. This setting is known as
demographic scarce regime. Prior research have shown that training an attribute
classifier to replace the missing sensitive attributes (proxy) can still
improve fairness. However, the use of proxy-sensitive attributes worsens
fairness-accuracy trade-offs compared to true sensitive attributes. To address
this limitation, we propose a framework to build attribute classifiers that
achieve better fairness-accuracy trade-offs. Our method introduces uncertainty
awareness in the attribute classifier and enforces fairness on samples with
demographic information inferred with the lowest uncertainty. We show
empirically that enforcing fairness constraints on samples with uncertain
sensitive attributes is detrimental to fairness and accuracy. Our experiments
on two datasets showed that the proposed framework yields models with
significantly better fairness-accuracy trade-offs compared to classic attribute
classifiers. Surprisingly, our framework outperforms models trained with
constraints on the true sensitive attributes.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Feature Importance Measurement based on Decision Tree Sampling. (arXiv:2307.13333v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13333">http://arxiv.org/abs/2307.13333</a></li>
<li>Code URL: <a href="https://github.com/tsudalab/dt-sampler">https://github.com/tsudalab/dt-sampler</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13333] Feature Importance Measurement based on Decision Tree Sampling](http://arxiv.org/abs/2307.13333) #interpretability</code></li>
<li>Summary: <p>Random forest is effective for prediction tasks but the randomness of tree
generation hinders interpretability in feature importance analysis. To address
this, we proposed DT-Sampler, a SAT-based method for measuring feature
importance in tree-based model. Our method has fewer parameters than random
forest and provides higher interpretability and stability for the analysis in
real-world problems. An implementation of DT-Sampler is available at
https://github.com/tsudalab/DT-sampler.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment. (arXiv:2307.13108v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13108">http://arxiv.org/abs/2307.13108</a></li>
<li>Code URL: <a href="https://github.com/favour-nerrise/xgw-gat">https://github.com/favour-nerrise/xgw-gat</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13108] An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment](http://arxiv.org/abs/2307.13108) #explainability</code></li>
<li>Summary: <p>One of the hallmark symptoms of Parkinson's Disease (PD) is the progressive
loss of postural reflexes, which eventually leads to gait difficulties and
balance problems. Identifying disruptions in brain function associated with
gait impairment could be crucial in better understanding PD motor progression,
thus advancing the development of more effective and personalized therapeutics.
In this work, we present an explainable, geometric, weighted-graph attention
neural network (xGW-GAT) to identify functional networks predictive of the
progression of gait difficulties in individuals with PD. xGW-GAT predicts the
multi-class gait impairment on the MDS Unified PD Rating Scale (MDS-UPDRS). Our
computational- and data-efficient model represents functional connectomes as
symmetric positive definite (SPD) matrices on a Riemannian manifold to
explicitly encode pairwise interactions of entire connectomes, based on which
we learn an attention mask yielding individual- and group-level explainability.
Applied to our resting-state functional MRI (rs-fMRI) dataset of individuals
with PD, xGW-GAT identifies functional connectivity patterns associated with
gait impairment in PD and offers interpretable explanations of functional
subnetworks associated with motor impairment. Our model successfully
outperforms several existing methods while simultaneously revealing
clinically-relevant connectivity patterns. The source code is available at
https://github.com/favour-nerrise/xGW-GAT .
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Fashion Matrix: Editing Photos by Just Talking. (arXiv:2307.13240v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13240">http://arxiv.org/abs/2307.13240</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13240] Fashion Matrix: Editing Photos by Just Talking](http://arxiv.org/abs/2307.13240) #diffusion</code></li>
<li>Summary: <p>The utilization of Large Language Models (LLMs) for the construction of AI
systems has garnered significant attention across diverse fields. The extension
of LLMs to the domain of fashion holds substantial commercial potential but
also inherent challenges due to the intricate semantic interactions in
fashion-related generation. To address this issue, we developed a hierarchical
AI system called Fashion Matrix dedicated to editing photos by just talking.
This system facilitates diverse prompt-driven tasks, encompassing garment or
accessory replacement, recoloring, addition, and removal. Specifically, Fashion
Matrix employs LLM as its foundational support and engages in iterative
interactions with users. It employs a range of Semantic Segmentation Models
(e.g., Grounded-SAM, MattingAnything, etc.) to delineate the specific editing
masks based on user instructions. Subsequently, Visual Foundation Models (e.g.,
Stable Diffusion, ControlNet, etc.) are leveraged to generate edited images
from text prompts and masks, thereby facilitating the automation of fashion
editing processes. Experiments demonstrate the outstanding ability of Fashion
Matrix to explores the collaborative potential of functionally diverse
pre-trained models in the domain of fashion editing.
</p></li>
</ul>

<h3>Title: Not with my name! Inferring artists' names of input strings employed by Diffusion Models. (arXiv:2307.13527v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13527">http://arxiv.org/abs/2307.13527</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13527] Not with my name! Inferring artists' names of input strings employed by Diffusion Models](http://arxiv.org/abs/2307.13527) #diffusion</code></li>
<li>Summary: <p>Diffusion Models (DM) are highly effective at generating realistic,
high-quality images. However, these models lack creativity and merely compose
outputs based on their training data, guided by a textual input provided at
creation time. Is it acceptable to generate images reminiscent of an artist,
employing his name as input? This imply that if the DM is able to replicate an
artist's work then it was trained on some or all of his artworks thus violating
copyright. In this paper, a preliminary study to infer the probability of use
of an artist's name in the input string of a generated image is presented. To
this aim we focused only on images generated by the famous DALL-E 2 and
collected images (both original and generated) of five renowned artists.
Finally, a dedicated Siamese Neural Network was employed to have a first kind
of probability. Experimental results demonstrate that our approach is an
optimal starting point and can be employed as a prior for predicting a complete
input string of an investigated image. Dataset and code are available at:
https://github.com/ictlab-unict/not-with-my-name .
</p></li>
</ul>

<h3>Title: XDLM: Cross-lingual Diffusion Language Model for Machine Translation. (arXiv:2307.13560v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13560">http://arxiv.org/abs/2307.13560</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13560] XDLM: Cross-lingual Diffusion Language Model for Machine Translation](http://arxiv.org/abs/2307.13560) #diffusion</code></li>
<li>Summary: <p>Recently, diffusion models have excelled in image generation tasks and have
also been applied to neural language processing (NLP) for controllable text
generation. However, the application of diffusion models in a cross-lingual
setting is less unexplored. Additionally, while pretraining with diffusion
models has been studied within a single language, the potential of
cross-lingual pretraining remains understudied. To address these gaps, we
propose XDLM, a novel Cross-lingual diffusion model for machine translation,
consisting of pretraining and fine-tuning stages. In the pretraining stage, we
propose TLDM, a new training objective for mastering the mapping between
different languages; in the fine-tuning stage, we build up the translation
system based on the pretrained model. We evaluate the result on several machine
translation benchmarks and outperformed both diffusion and Transformer
baselines.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Multi-Granularity Prediction with Learnable Fusion for Scene Text Recognition. (arXiv:2307.13244v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13244">http://arxiv.org/abs/2307.13244</a></li>
<li>Code URL: <a href="https://github.com/alibabaresearch/advancedliteratemachinery">https://github.com/alibabaresearch/advancedliteratemachinery</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13244] Multi-Granularity Prediction with Learnable Fusion for Scene Text Recognition](http://arxiv.org/abs/2307.13244) #transformer</code></li>
<li>Summary: <p>Due to the enormous technical challenges and wide range of applications,
scene text recognition (STR) has been an active research topic in computer
vision for years. To tackle this tough problem, numerous innovative methods
have been successively proposed, and incorporating linguistic knowledge into
STR models has recently become a prominent trend. In this work, we first draw
inspiration from the recent progress in Vision Transformer (ViT) to construct a
conceptually simple yet functionally powerful vision STR model, which is built
upon ViT and a tailored Adaptive Addressing and Aggregation (A$^3$) module. It
already outperforms most previous state-of-the-art models for scene text
recognition, including both pure vision models and language-augmented methods.
To integrate linguistic knowledge, we further propose a Multi-Granularity
Prediction strategy to inject information from the language modality into the
model in an implicit way, \ie, subword representations (BPE and WordPiece)
widely used in NLP are introduced into the output space, in addition to the
conventional character level representation, while no independent language
model (LM) is adopted. To produce the final recognition results, two strategies
for effectively fusing the multi-granularity predictions are devised. The
resultant algorithm (termed MGP-STR) is able to push the performance envelope
of STR to an even higher level. Specifically, MGP-STR achieves an average
recognition accuracy of $94\%$ on standard benchmarks for scene text
recognition. Moreover, it also achieves state-of-the-art results on widely-used
handwritten benchmarks as well as more challenging scene text datasets,
demonstrating the generality of the proposed MGP-STR algorithm. The source code
and models will be available at:
\url{https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR}.
</p></li>
</ul>

<h3>Title: Conditional Cross Attention Network for Multi-Space Embedding without Entanglement in Only a SINGLE Network. (arXiv:2307.13254v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13254">http://arxiv.org/abs/2307.13254</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13254] Conditional Cross Attention Network for Multi-Space Embedding without Entanglement in Only a SINGLE Network](http://arxiv.org/abs/2307.13254) #transformer</code></li>
<li>Summary: <p>Many studies in vision tasks have aimed to create effective embedding spaces
for single-label object prediction within an image. However, in reality, most
objects possess multiple specific attributes, such as shape, color, and length,
with each attribute composed of various classes. To apply models in real-world
scenarios, it is essential to be able to distinguish between the granular
components of an object. Conventional approaches to embedding multiple specific
attributes into a single network often result in entanglement, where
fine-grained features of each attribute cannot be identified separately. To
address this problem, we propose a Conditional Cross-Attention Network that
induces disentangled multi-space embeddings for various specific attributes
with only a single backbone. Firstly, we employ a cross-attention mechanism to
fuse and switch the information of conditions (specific attributes), and we
demonstrate its effectiveness through a diverse visualization example.
Secondly, we leverage the vision transformer for the first time to a
fine-grained image retrieval task and present a simple yet effective framework
compared to existing methods. Unlike previous studies where performance varied
depending on the benchmark dataset, our proposed method achieved consistent
state-of-the-art performance on the FashionAI, DARN, DeepFashion, and Zappos50K
benchmark datasets.
</p></li>
</ul>

<h3>Title: CT-Net: Arbitrary-Shaped Text Detection via Contour Transformer. (arXiv:2307.13310v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13310">http://arxiv.org/abs/2307.13310</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13310] CT-Net: Arbitrary-Shaped Text Detection via Contour Transformer](http://arxiv.org/abs/2307.13310) #transformer</code></li>
<li>Summary: <p>Contour based scene text detection methods have rapidly developed recently,
but still suffer from inaccurate frontend contour initialization, multi-stage
error accumulation, or deficient local information aggregation. To tackle these
limitations, we propose a novel arbitrary-shaped scene text detection framework
named CT-Net by progressive contour regression with contour transformers.
Specifically, we first employ a contour initialization module that generates
coarse text contours without any post-processing. Then, we adopt contour
refinement modules to adaptively refine text contours in an iterative manner,
which are beneficial for context information capturing and progressive global
contour deformation. Besides, we propose an adaptive training strategy to
enable the contour transformers to learn more potential deformation paths, and
introduce a re-score mechanism that can effectively suppress false positives.
Extensive experiments are conducted on four challenging datasets, which
demonstrate the accuracy and efficiency of our CT-Net over state-of-the-art
methods. Particularly, CT-Net achieves F-measure of 86.1 at 11.2 frames per
second (FPS) and F-measure of 87.8 at 10.1 FPS for CTW1500 and Total-Text
datasets, respectively.
</p></li>
</ul>

<h3>Title: Towards Resolving Word Ambiguity with Word Embeddings. (arXiv:2307.13417v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13417">http://arxiv.org/abs/2307.13417</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13417] Towards Resolving Word Ambiguity with Word Embeddings](http://arxiv.org/abs/2307.13417) #transformer</code></li>
<li>Summary: <p>Ambiguity is ubiquitous in natural language. Resolving ambiguous meanings is
especially important in information retrieval tasks. While word embeddings
carry semantic information, they fail to handle ambiguity well. Transformer
models have been shown to handle word ambiguity for complex queries, but they
cannot be used to identify ambiguous words, e.g. for a 1-word query.
Furthermore, training these models is costly in terms of time, hardware
resources, and training data, prohibiting their use in specialized environments
with sensitive data. Word embeddings can be trained using moderate hardware
resources. This paper shows that applying DBSCAN clustering to the latent space
can identify ambiguous words and evaluate their level of ambiguity. An
automatic DBSCAN parameter selection leads to high-quality clusters, which are
semantically coherent and correspond well to the perceived meanings of a given
word.
</p></li>
</ul>

<h3>Title: Curvature-based Transformer for Molecular Property Prediction. (arXiv:2307.13275v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13275">http://arxiv.org/abs/2307.13275</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13275] Curvature-based Transformer for Molecular Property Prediction](http://arxiv.org/abs/2307.13275) #transformer</code></li>
<li>Summary: <p>The prediction of molecular properties is one of the most important and
challenging tasks in the field of artificial intelligence-based drug design.
Among the current mainstream methods, the most commonly used feature
representation for training DNN models is based on SMILES and molecular graphs,
although these methods are concise and effective, they also limit the ability
to capture spatial information. In this work, we propose Curvature-based
Transformer to improve the ability of Graph Transformer neural network models
to extract structural information on molecular graph data by introducing
Discretization of Ricci Curvature. To embed the curvature in the model, we add
the curvature information of the graph as positional Encoding to the node
features during the attention-score calculation. This method can introduce
curvature information from graph data without changing the original network
architecture, and it has the potential to be extended to other models. We
performed experiments on chemical molecular datasets including PCQM4M-LST,
MoleculeNet and compared with models such as Uni-Mol, Graphormer, and the
results show that this method can achieve the state-of-the-art results. It is
proved that the discretized Ricci curvature also reflects the structural and
functional relationship while describing the local geometry of the graph
molecular data.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Benchmarking and Analyzing Generative Data for Visual Recognition. (arXiv:2307.13697v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13697">http://arxiv.org/abs/2307.13697</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13697] Benchmarking and Analyzing Generative Data for Visual Recognition](http://arxiv.org/abs/2307.13697) #generative</code></li>
<li>Summary: <p>Advancements in large pre-trained generative models have expanded their
potential as effective data generators in visual recognition. This work delves
into the impact of generative images, primarily comparing paradigms that
harness external data (\ie generative \vs retrieval \vs original).
</p></li>
</ul>

<p>Our key contributions are: \textbf{1) GenBench Construction:} We devise
\textbf{GenBench}, a broad benchmark comprising 22 datasets with 2548
categories, to appraise generative data across various visual recognition
tasks. \textbf{2) CLER Score:} To address the insufficient correlation of
existing metrics (\eg, FID, CLIP score) with downstream recognition
performance, we propose \textbf{CLER}, a training-free metric indicating
generative data's efficiency for recognition tasks prior to training.
\textbf{3) New Baselines:} Comparisons of generative data with retrieved data
from the same external pool help to elucidate the unique traits of generative
data. \textbf{4) External Knowledge Injection:} By fine-tuning special token
embeddings for each category via Textual Inversion, performance improves across
17 datasets, except when dealing with low-resolution reference images.
</p>
<p>Our exhaustive benchmark and analysis spotlight generative data's promise in
visual recognition, while identifying key challenges for future investigation.
</p>

<h3>Title: Opinion Mining Using Population-tuned Generative Language Models. (arXiv:2307.13173v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13173">http://arxiv.org/abs/2307.13173</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13173] Opinion Mining Using Population-tuned Generative Language Models](http://arxiv.org/abs/2307.13173) #generative</code></li>
<li>Summary: <p>We present a novel method for mining opinions from text collections using
generative language models trained on data collected from different
populations. We describe the basic definitions, methodology and a generic
algorithm for opinion insight mining. We demonstrate the performance of our
method in an experiment where a pre-trained generative model is fine-tuned
using specifically tailored content with unnatural and fully annotated
opinions. We show that our approach can learn and transfer the opinions to the
semantic classes while maintaining the proportion of polarisation. Finally, we
demonstrate the usage of an insight mining system to scale up the discovery of
opinion insights from a real text corpus.
</p></li>
</ul>

<h3>Title: FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios. (arXiv:2307.13528v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13528">http://arxiv.org/abs/2307.13528</a></li>
<li>Code URL: <a href="https://github.com/gair-nlp/factool">https://github.com/gair-nlp/factool</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13528] FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios](http://arxiv.org/abs/2307.13528) #generative</code></li>
<li>Summary: <p>The emergence of generative pre-trained models has facilitated the synthesis
of high-quality text, but it has also posed challenges in identifying factual
errors in the generated text. In particular: (1) A wider range of tasks now
face an increasing risk of containing factual errors when handled by generative
models. (2) Generated texts tend to be lengthy and lack a clearly defined
granularity for individual facts. (3) There is a scarcity of explicit evidence
available during the process of fact checking. With the above challenges in
mind, in this paper, we propose FacTool, a task and domain agnostic framework
for detecting factual errors of texts generated by large language models (e.g.,
ChatGPT). Experiments on four different tasks (knowledge-based QA, code
generation, mathematical reasoning, and scientific literature review) show the
efficacy of the proposed method.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Multilevel Large Language Models for Everyone. (arXiv:2307.13221v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13221">http://arxiv.org/abs/2307.13221</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13221] Multilevel Large Language Models for Everyone](http://arxiv.org/abs/2307.13221) #large language model</code></li>
<li>Summary: <p>Large language models have made significant progress in the past few years.
However, they are either generic {\it or} field specific, splitting the
community into different groups. In this paper, we unify these large language
models into a larger map, where the generic {\it and} specific models are
linked together and can improve each other, based on the user personal input
and information from the internet. The idea of linking several large language
models together is inspired by the functionality of human brain. The specific
regions on the brain cortex are specific for certain low level functionality.
And these regions can jointly work together to achieve more complex high level
functionality. Such behavior on human brain cortex sheds the light to design
the multilevel large language models that contain global level, field level and
user level models. The user level models run on local machines to achieve
efficient response and protect the user's privacy. Such multilevel models
reduce some redundancy and perform better than the single level models. The
proposed multilevel idea can be applied in various applications, such as
natural language processing, computer vision tasks, professional assistant,
business and healthcare.
</p></li>
</ul>

<h3>Title: The potential of LLMs for coding with low-resource and domain-specific programming languages. (arXiv:2307.13018v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13018">http://arxiv.org/abs/2307.13018</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13018] The potential of LLMs for coding with low-resource and domain-specific programming languages](http://arxiv.org/abs/2307.13018) #large language model</code></li>
<li>Summary: <p>This paper presents a study on the feasibility of using large language models
(LLM) for coding with low-resource and domain-specific programming languages
that typically lack the amount of data required for effective LLM processing
techniques. This study focuses on the econometric scripting language named
hansl of the open-source software gretl and employs a proprietary LLM based on
GPT-3.5. Our findings suggest that LLMs can be a useful tool for writing,
understanding, improving, and documenting gretl code, which includes generating
descriptive docstrings for functions and providing precise explanations for
abstract and poorly documented econometric code. While the LLM showcased
promoting docstring-to-code translation capability, we also identify some
limitations, such as its inability to improve certain sections of code and to
write accurate unit tests. This study is a step towards leveraging the power of
LLMs to facilitate software development in low-resource programming languages
and ultimately to lower barriers to entry for their adoption.
</p></li>
</ul>

<h3>Title: How to use LLMs for Text Analysis. (arXiv:2307.13106v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13106">http://arxiv.org/abs/2307.13106</a></li>
<li>Code URL: <a href="https://github.com/cssmodels/howtousellms">https://github.com/cssmodels/howtousellms</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13106] How to use LLMs for Text Analysis](http://arxiv.org/abs/2307.13106) #large language model</code></li>
<li>Summary: <p>This guide introduces Large Language Models (LLM) as a highly versatile text
analysis method within the social sciences. As LLMs are easy-to-use, cheap,
fast, and applicable on a broad range of text analysis tasks, ranging from text
annotation and classification to sentiment analysis and critical discourse
analysis, many scholars believe that LLMs will transform how we do text
analysis. This how-to guide is aimed at students and researchers with limited
programming experience, and offers a simple introduction to how LLMs can be
used for text analysis in your own research project, as well as advice on best
practices. We will go through each of the steps of analyzing textual data with
LLMs using Python: installing the software, setting up the API, loading the
data, developing an analysis prompt, analyzing the text, and validating the
results. As an illustrative example, we will use the challenging task of
identifying populism in political texts, and show how LLMs move beyond the
existing state-of-the-art.
</p></li>
</ul>

<h3>Title: QuIP: 2-Bit Quantization of Large Language Models With Guarantees. (arXiv:2307.13304v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13304">http://arxiv.org/abs/2307.13304</a></li>
<li>Code URL: <a href="https://github.com/jerry-chee/quip">https://github.com/jerry-chee/quip</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13304] QuIP: 2-Bit Quantization of Large Language Models With Guarantees](http://arxiv.org/abs/2307.13304) #large language model</code></li>
<li>Summary: <p>This work studies post-training parameter quantization in large language
models (LLMs). We introduce quantization with incoherence processing (QuIP), a
new method based on the insight that quantization benefits from incoherent
weight and Hessian matrices, i.e., from the weights and the directions in which
it is important to round them accurately being unaligned with the coordinate
axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing
a quadratic proxy objective; (2) efficient pre- and post-processing that
ensures weight and Hessian incoherence via multiplication by random orthogonal
matrices. We complement QuIP with the first theoretical analysis for an
LLM-scale quantization algorithm, and show that our theory also applies to an
existing method, OPTQ. Empirically, we find that our incoherence preprocessing
improves several existing quantization algorithms and yields the first LLM
quantization methods that produce viable results using only two bits per
weight. Our code can be found at https://github.com/jerry-chee/QuIP .
</p></li>
</ul>

<h3>Title: Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions. (arXiv:2307.13339v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13339">http://arxiv.org/abs/2307.13339</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13339] Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions](http://arxiv.org/abs/2307.13339) #large language model</code></li>
<li>Summary: <p>Chain-of-thought (CoT) prompting has been shown to empirically improve the
accuracy of large language models (LLMs) on various question answering tasks.
While understanding why CoT prompting is effective is crucial to ensuring that
this phenomenon is a consequence of desired model behavior, little work has
addressed this; nonetheless, such an understanding is a critical prerequisite
for responsible model deployment. We address this question by leveraging
gradient-based feature attribution methods which produce saliency scores that
capture the influence of input tokens on model output. Specifically, we probe
several open-source LLMs to investigate whether CoT prompting affects the
relative importances they assign to particular input tokens. Our results
indicate that while CoT prompting does not increase the magnitude of saliency
scores attributed to semantically relevant tokens in the prompt compared to
standard few-shot prompting, it increases the robustness of saliency scores to
question perturbations and variations in model output.
</p></li>
</ul>

<h3>Title: Empower Your Model with Longer and Better Context Comprehension. (arXiv:2307.13365v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13365">http://arxiv.org/abs/2307.13365</a></li>
<li>Code URL: <a href="https://github.com/yileijin/attention-transition">https://github.com/yileijin/attention-transition</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13365] Empower Your Model with Longer and Better Context Comprehension](http://arxiv.org/abs/2307.13365) #large language model</code></li>
<li>Summary: <p>Recently, with the emergence of numerous Large Language Models (LLMs), the
implementation of AI has entered a new era. Irrespective of these models' own
capacity and structure, there is a growing demand for LLMs to possess enhanced
comprehension of longer and more complex contexts with relatively smaller
sizes. Models often encounter an upper limit when processing sequences of
sentences that extend beyond their comprehension capacity and result in
off-topic or even chaotic responses. While several recent works attempt to
address this issue in various ways, they rarely focus on "why models are unable
to compensate or strengthen their capabilities on their own". In this paper, we
thoroughly investigate the nature of information transfer within LLMs and
propose a novel technique called Attention Transition. This technique empowers
models to achieve longer and better context comprehension with minimal
additional training or impact on generation fluency. Our experiments are
conducted in XSum and achieve substantial improvement compared with the
original generation results.
</p></li>
</ul>

<h3>Title: GPT-3 Models are Few-Shot Financial Reasoners. (arXiv:2307.13617v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13617">http://arxiv.org/abs/2307.13617</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13617] GPT-3 Models are Few-Shot Financial Reasoners](http://arxiv.org/abs/2307.13617) #large language model</code></li>
<li>Summary: <p>Financial analysis is an important tool for evaluating company performance.
Practitioners work to answer financial questions to make profitable investment
decisions, and use advanced quantitative analyses to do so. As a result,
Financial Question Answering (QA) is a question answering task that requires
deep reasoning about numbers. Furthermore, it is unknown how well pre-trained
language models can reason in the financial domain. The current
state-of-the-art requires a retriever to collect relevant facts about the
financial question from the text and a generator to produce a valid financial
program and a final answer. However, recently large language models like GPT-3
have achieved state-of-the-art performance on wide variety of tasks with just a
few shot examples. We run several experiments with GPT-3 and find that a
separate retrieval model and logic engine continue to be essential components
to achieving SOTA performance in this task, particularly due to the precise
nature of financial questions and the complex information stored in financial
documents. With this understanding, our refined prompt-engineering approach on
GPT-3 achieves near SOTA accuracy without any fine-tuning.
</p></li>
</ul>

<h3>Title: ARB: Advanced Reasoning Benchmark for Large Language Models. (arXiv:2307.13692v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13692">http://arxiv.org/abs/2307.13692</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13692] ARB: Advanced Reasoning Benchmark for Large Language Models](http://arxiv.org/abs/2307.13692) #large language model</code></li>
<li>Summary: <p>Large Language Models (LLMs) have demonstrated remarkable performance on
various quantitative reasoning and knowledge benchmarks. However, many of these
benchmarks are losing utility as LLMs get increasingly high scores, despite not
yet reaching expert performance in these domains. We introduce ARB, a novel
benchmark composed of advanced reasoning problems in multiple fields. ARB
presents a more challenging test than prior benchmarks, featuring problems in
mathematics, physics, biology, chemistry, and law. As a subset of ARB, we
introduce a challenging set of math and physics problems which require advanced
symbolic reasoning and domain knowledge. We evaluate recent models such as
GPT-4 and Claude on ARB and demonstrate that current models score well below
50% on more demanding tasks. In order to improve both automatic and assisted
evaluation capabilities, we introduce a rubric-based evaluation approach,
allowing GPT-4 to score its own intermediate reasoning steps. Further, we
conduct a human evaluation of the symbolic subset of ARB, finding promising
agreement between annotators and GPT-4 rubric evaluation scores.
</p></li>
</ul>

<h3>Title: Evaluating Large Language Models for Radiology Natural Language Processing. (arXiv:2307.13693v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13693">http://arxiv.org/abs/2307.13693</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13693] Evaluating Large Language Models for Radiology Natural Language Processing](http://arxiv.org/abs/2307.13693) #large language model</code></li>
<li>Summary: <p>The rise of large language models (LLMs) has marked a pivotal shift in the
field of natural language processing (NLP). LLMs have revolutionized a
multitude of domains, and they have made a significant impact in the medical
field. Large language models are now more abundant than ever, and many of these
models exhibit bilingual capabilities, proficient in both English and Chinese.
However, a comprehensive evaluation of these models remains to be conducted.
This lack of assessment is especially apparent within the context of radiology
NLP. This study seeks to bridge this gap by critically evaluating thirty two
LLMs in interpreting radiology reports, a crucial component of radiology NLP.
Specifically, the ability to derive impressions from radiologic findings is
assessed. The outcomes of this evaluation provide key insights into the
performance, strengths, and weaknesses of these LLMs, informing their practical
applications within the medical domain.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Image Segmentation Keras : Implementation of Segnet, FCN, UNet, PSPNet and other models in Keras. (arXiv:2307.13215v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13215">http://arxiv.org/abs/2307.13215</a></li>
<li>Code URL: <a href="https://github.com/divamgupta/image-segmentation-keras">https://github.com/divamgupta/image-segmentation-keras</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13215] Image Segmentation Keras : Implementation of Segnet, FCN, UNet, PSPNet and other models in Keras](http://arxiv.org/abs/2307.13215) #segmentation</code></li>
<li>Summary: <p>Semantic segmentation plays a vital role in computer vision tasks, enabling
precise pixel-level understanding of images. In this paper, we present a
comprehensive library for semantic segmentation, which contains implementations
of popular segmentation models like SegNet, FCN, UNet, and PSPNet. We also
evaluate and compare these models on several datasets, offering researchers and
practitioners a powerful toolset for tackling diverse segmentation challenges.
</p></li>
</ul>

<h3>Title: GaPro: Box-Supervised 3D Point Cloud Instance Segmentation Using Gaussian Processes as Pseudo Labelers. (arXiv:2307.13251v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13251">http://arxiv.org/abs/2307.13251</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13251] GaPro: Box-Supervised 3D Point Cloud Instance Segmentation Using Gaussian Processes as Pseudo Labelers](http://arxiv.org/abs/2307.13251) #segmentation</code></li>
<li>Summary: <p>Instance segmentation on 3D point clouds (3DIS) is a longstanding challenge
in computer vision, where state-of-the-art methods are mainly based on full
supervision. As annotating ground truth dense instance masks is tedious and
expensive, solving 3DIS with weak supervision has become more practical. In
this paper, we propose GaPro, a new instance segmentation for 3D point clouds
using axis-aligned 3D bounding box supervision. Our two-step approach involves
generating pseudo labels from box annotations and training a 3DIS network with
the resulting labels. Additionally, we employ the self-training strategy to
improve the performance of our method further. We devise an effective Gaussian
Process to generate pseudo instance masks from the bounding boxes and resolve
ambiguities when they overlap, resulting in pseudo instance masks with their
uncertainty values. Our experiments show that GaPro outperforms previous weakly
supervised 3D instance segmentation methods and has competitive performance
compared to state-of-the-art fully supervised ones. Furthermore, we demonstrate
the robustness of our approach, where we can adapt various state-of-the-art
fully supervised methods to the weak supervision task by using our pseudo
labels for training. The source code and trained models are available at
https://github.com/VinAIResearch/GaPro.
</p></li>
</ul>

<h3>Title: Unmasking Anomalies in Road-Scene Segmentation. (arXiv:2307.13316v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13316">http://arxiv.org/abs/2307.13316</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13316] Unmasking Anomalies in Road-Scene Segmentation](http://arxiv.org/abs/2307.13316) #segmentation</code></li>
<li>Summary: <p>Anomaly segmentation is a critical task for driving applications, and it is
approached traditionally as a per-pixel classification problem. However,
reasoning individually about each pixel without considering their contextual
semantics results in high uncertainty around the objects' boundaries and
numerous false positives. We propose a paradigm change by shifting from a
per-pixel classification to a mask classification. Our mask-based method,
Mask2Anomaly, demonstrates the feasibility of integrating an anomaly detection
method in a mask-classification architecture. Mask2Anomaly includes several
technical novelties that are designed to improve the detection of anomalies in
masks: i) a global masked attention module to focus individually on the
foreground and background regions; ii) a mask contrastive learning that
maximizes the margin between an anomaly and known classes; and iii) a mask
refinement solution to reduce false positives. Mask2Anomaly achieves new
state-of-the-art results across a range of benchmarks, both in the per-pixel
and component-level evaluations. In particular, Mask2Anomaly reduces the
average false positives rate by 60% wrt the previous state-of-the-art. Github
page:
https://github.com/shyam671/Mask2Anomaly-Unmasking-Anomalies-in-Road-Scene-Segmentation.
</p></li>
</ul>

<h3>Title: Spectrum-guided Multi-granularity Referring Video Object Segmentation. (arXiv:2307.13537v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13537">http://arxiv.org/abs/2307.13537</a></li>
<li>Code URL: <a href="https://github.com/bo-miao/sgmg">https://github.com/bo-miao/sgmg</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13537] Spectrum-guided Multi-granularity Referring Video Object Segmentation](http://arxiv.org/abs/2307.13537) #segmentation</code></li>
<li>Summary: <p>Current referring video object segmentation (R-VOS) techniques extract
conditional kernels from encoded (low-resolution) vision-language features to
segment the decoded high-resolution features. We discovered that this causes
significant feature drift, which the segmentation kernels struggle to perceive
during the forward computation. This negatively affects the ability of
segmentation kernels. To address the drift problem, we propose a
Spectrum-guided Multi-granularity (SgMg) approach, which performs direct
segmentation on the encoded features and employs visual details to further
optimize the masks. In addition, we propose Spectrum-guided Cross-modal Fusion
(SCF) to perform intra-frame global interactions in the spectral domain for
effective multimodal representation. Finally, we extend SgMg to perform
multi-object R-VOS, a new paradigm that enables simultaneous segmentation of
multiple referred objects in a video. This not only makes R-VOS faster, but
also more practical. Extensive experiments show that SgMg achieves
state-of-the-art performance on four video benchmark datasets, outperforming
the nearest competitor by 2.8% points on Ref-YouTube-VOS. Our extended SgMg
enables multi-object R-VOS, runs about 3 times faster while maintaining
satisfactory performance. Code is available at https://github.com/bo-miao/SgMg.
</p></li>
</ul>

<h3>Title: Optical Flow boosts Unsupervised Localization and Segmentation. (arXiv:2307.13640v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13640">http://arxiv.org/abs/2307.13640</a></li>
<li>Code URL: <a href="https://github.com/mlzxy/flowdino">https://github.com/mlzxy/flowdino</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13640] Optical Flow boosts Unsupervised Localization and Segmentation](http://arxiv.org/abs/2307.13640) #segmentation</code></li>
<li>Summary: <p>Unsupervised localization and segmentation are long-standing robot vision
challenges that describe the critical ability for an autonomous robot to learn
to decompose images into individual objects without labeled data. These tasks
are important because of the limited availability of dense image manual
annotation and the promising vision of adapting to an evolving set of object
categories in lifelong learning. Most recent methods focus on using visual
appearance continuity as object cues by spatially clustering features obtained
from self-supervised vision transformers (ViT). In this work, we leverage
motion cues, inspired by the common fate principle that pixels that share
similar movements tend to belong to the same object. We propose a new loss term
formulation that uses optical flow in unlabeled videos to encourage
self-supervised ViT features to become closer to each other if their
corresponding spatial locations share similar movements, and vice versa. We use
the proposed loss function to finetune vision transformers that were originally
trained on static images. Our fine-tuning procedure outperforms
state-of-the-art techniques for unsupervised semantic segmentation through
linear probing, without the use of any labeled data. This procedure also
demonstrates increased performance over original ViT networks across
unsupervised object localization and semantic segmentation benchmarks.
</p></li>
</ul>

<h3>Title: Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation. (arXiv:2307.13645v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.13645">http://arxiv.org/abs/2307.13645</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.13645] Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation](http://arxiv.org/abs/2307.13645) #segmentation</code></li>
<li>Summary: <p>Obtaining labelled data in medical image segmentation is challenging due to
the need for pixel-level annotations by experts. Recent works have shown that
augmenting the object of interest with deformable transformations can help
mitigate this challenge. However, these transformations have been learned
globally for the image, limiting their transferability across datasets or
applicability in problems where image alignment is difficult. While
object-centric augmentations provide a great opportunity to overcome these
issues, existing works are only focused on position and random transformations
without considering shape variations of the objects. To this end, we propose a
novel object-centric data augmentation model that is able to learn the shape
variations for the objects of interest and augment the object in place without
modifying the rest of the image. We demonstrated its effectiveness in improving
kidney tumour segmentation when leveraging shape variations learned both from
within the same dataset and transferred from external datasets.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
