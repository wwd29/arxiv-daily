<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Controlling Large Language Models to Generate Secure and Vulnerable Code. (arXiv:2302.05319v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05319">http://arxiv.org/abs/2302.05319</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05319] Controlling Large Language Models to Generate Secure and Vulnerable Code](http://arxiv.org/abs/2302.05319) #secure</code></li>
<li>Summary: <p>Large language models (LMs) are increasingly pretrained on massive corpora of
open-source programs and applied to solve program synthesis tasks. However, a
fundamental limitation of LMs is their unawareness of security and
vulnerability during pretraining and inference. As a result, LMs produce secure
or vulnerable programs with high uncertainty (e.g., around 60%/40% chances for
GitHub Copilot according to a recent study). This greatly impairs LMs'
usability, especially in security-sensitive scenarios.
</p></li>
</ul>

<p>To address this limitation, this work formulates a new problem called
controlled code generation, which allows users to input a boolean property into
an LM to control if the LM generates secure or vulnerable code. We propose
svGen, an effective and lightweight learning approach for solving controlled
code generation. svGen leverages property-specific continuous vectors to steer
program generation toward the given property, without altering the weights of
the LM. svGen's training optimizes those continuous vectors by carefully
applying specialized loss terms on different regions of code.
</p>
<p>Our extensive evaluation shows that svGen achieves strong control capability
across various software vulnerabilities and LMs of different parameter sizes.
For example, on 9 dangerous vulnerabilities, a state-of-the-art CodeGen LM with
2.7B parameters generates secure programs with a 57% chance. When we use svGen
to control the LM to generate secure (resp., vulnerable) programs, the chance
is significantly increased to 82% (resp., decreased to 35%).
</p>

<h2>security</h2>
<h3>Title: PETIoT: PEnetration Testing the Internet of Things. (arXiv:2302.04900v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04900">http://arxiv.org/abs/2302.04900</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04900] PETIoT: PEnetration Testing the Internet of Things](http://arxiv.org/abs/2302.04900) #security</code></li>
<li>Summary: <p>Attackers may attempt exploiting Internet of Things (IoT) devices to operate
them unduly as well as to gather personal data of the legitimate device
owners'. Vulnerability Assessment and Penetration Testing (VAPT) sessions help
to verify the effectiveness of the adopted security measures. However, VAPT
over IoT devices, namely VAPT targeted at IoT devices, is an open research
challenge due to the variety of target technologies and to the creativity it
may require. Therefore, this article aims at guiding penetration testers to
conduct VAPT sessions over IoT devices by means of a new cyber Kill Chain (KC)
termed PETIoT. Several practical applications of PETIoT confirm that it is
general, while its main novelty lies in the combination of attack and defence
steps. PETIoT is demonstrated on a relevant example, the best-selling IP camera
on Amazon Italy, the TAPO C200 by TP-Link, assuming an attacker who sits on the
same network as the device's in order to assess all the network interfaces of
the device. Additional knowledge is generated in terms of three zero-day
vulnerabilities found and practically exploited on the camera, one of these
with High severity and the other two with Medium severity by the CVSS standard.
These are camera Denial of Service (DoS), motion detection breach and video
stream breach. The application of PETIoT culminates with the proof-of-concept
of a home-made fix, based on an inexpensive Raspberry Pi 4 Model B device, for
the last vulnerability. Ultimately, our responsible disclosure with the camera
vendor led to the release of a firmware update that fixes all found
vulnerabilities, confirming that PetIoT has valid impact in real-world
scenarios.
</p></li>
</ul>

<h3>Title: Fee-Redistribution Smart Contracts for Transaction-Fee-Based Regime of Blockchains with the Longest Chain Rule. (arXiv:2302.04910v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04910">http://arxiv.org/abs/2302.04910</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04910] Fee-Redistribution Smart Contracts for Transaction-Fee-Based Regime of Blockchains with the Longest Chain Rule](http://arxiv.org/abs/2302.04910) #security</code></li>
<li>Summary: <p>In this paper, we review the undercutting attacks in the
transaction-fee-based regime of proof-of-work (PoW) blockchains with the
longest chain fork-choice rule. Next, we focus on the problem of fluctuations
in mining revenue and the mining gap - i.e., a situation, in which the
immediate reward from transaction fees does not cover miners' expenditures.
</p></li>
</ul>

<p>To mitigate these issues, we propose a solution that splits transaction fees
from a mined block into two parts - (1) an instant reward for the miner of a
block and (2) a deposit sent to one or more fee-redistribution smart contracts
($\mathcal{FRSC}$s) that are part of the consensus protocol. At the same time,
these redistribution smart contracts reward the miner of a block with a certain
fraction of the accumulated funds of the incoming fees over a predefined time.
This setting enables us to achieve several interesting properties that are
beneficial for the incentive stability and security of the protocol.
</p>
<p>With our solution, the fraction of Default-Compliant miners who strictly do
not execute undercutting attacks is lowered from the state-of-the-art result of
66% to 30%.
</p>

<h3>Title: An Assessment Methodology and Instrument for Cybersecurity: The Ireland Use Case. (arXiv:2302.05166v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05166">http://arxiv.org/abs/2302.05166</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05166] An Assessment Methodology and Instrument for Cybersecurity: The Ireland Use Case](http://arxiv.org/abs/2302.05166) #security</code></li>
<li>Summary: <p>Governments around the world are required to strengthen their national
cybersecurity capabilities to respond effectively to the growing, changing, and
sophisticated cyber threats and attacks, thus protecting society and the way of
life as a whole. Responsible government institutions need to revise, evaluate,
and bolster their national cybersecurity capabilities to fulfill the new
requirements, for example regarding new trends affecting cybersecurity, key
supporting laws and regulations, and implementations risk and challenges. This
report presents a comprehensive assessment instrument for cybersecurity at the
national level in order to help countries to ensure optimum response capability
and more effective use of critical resources of each state. More precisely, the
report - builds a common understanding of the critical cybersecurity
capabilities and competence to be assessed at the national level, - adds value
to national strategic planning and implementation which impact the development
and adaptation of national cybersecurity strategies, - provides an overview of
the assessment approaches at the national level, including capabilities,
frameworks, and controls, - introduces a comprehensive cybersecurity instrument
for countries to determine areas of improvement and develop enduring national
capabilities, - describes how to apply the proposed national cybersecurity
assessment framework in a real-world case, and - presents the results and
lessons learned of the application of the assessment framework at the national
level to assist governments in further building cybersecurity capabilities.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Key Design Choices for Double-Transfer in Source-Free Unsupervised Domain Adaptation. (arXiv:2302.05379v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05379">http://arxiv.org/abs/2302.05379</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05379] Key Design Choices for Double-Transfer in Source-Free Unsupervised Domain Adaptation](http://arxiv.org/abs/2302.05379) #privacy</code></li>
<li>Summary: <p>Fine-tuning and Domain Adaptation emerged as effective strategies for
efficiently transferring deep learning models to new target tasks. However,
target domain labels are not accessible in many real-world scenarios. This led
to the development of Unsupervised Domain Adaptation (UDA) methods, which only
employ unlabeled target samples. Furthermore, efficiency and privacy
requirements may also prevent the use of source domain data during the
adaptation stage. This challenging setting, known as Source-Free Unsupervised
Domain Adaptation (SF-UDA), is gaining interest among researchers and
practitioners due to its potential for real-world applications. In this paper,
we provide the first in-depth analysis of the main design choices in SF-UDA
through a large-scale empirical study across 500 models and 74 domain pairs. We
pinpoint the normalization approach, pre-training strategy, and backbone
architecture as the most critical factors. Based on our quantitative findings,
we propose recipes to best tackle SF-UDA scenarios. Moreover, we show that
SF-UDA is competitive also beyond standard benchmarks and backbone
architectures, performing on par with UDA at a fraction of the data and
computational cost. In the interest of reproducibility, we include the full
experimental results and code as supplementary material.
</p></li>
</ul>

<h3>Title: On Achieving Privacy-Preserving State-of-the-Art Edge Intelligence. (arXiv:2302.05323v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05323">http://arxiv.org/abs/2302.05323</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05323] On Achieving Privacy-Preserving State-of-the-Art Edge Intelligence](http://arxiv.org/abs/2302.05323) #privacy</code></li>
<li>Summary: <p>Deep Neural Network (DNN) Inference in Edge Computing, often called Edge
Intelligence, requires solutions to insure that sensitive data confidentiality
and intellectual property are not revealed in the process. Privacy-preserving
Edge Intelligence is only emerging, despite the growing prevalence of Edge
Computing as a context of Machine-Learning-as-a-Service. Solutions are yet to
be applied, and possibly adapted, to state-of-the-art DNNs. This position paper
provides an original assessment of the compatibility of existing techniques for
privacy-preserving DNN Inference with the characteristics of an Edge Computing
setup, highlighting the appropriateness of secret sharing in this context. We
then address the future role of model compression methods in the research
towards secret sharing on DNNs with state-of-the-art performance.
</p></li>
</ul>

<h3>Title: Building cross-language corpora for human understanding of privacy policies. (arXiv:2302.05355v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05355">http://arxiv.org/abs/2302.05355</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05355] Building cross-language corpora for human understanding of privacy policies](http://arxiv.org/abs/2302.05355) #privacy</code></li>
<li>Summary: <p>Making sure that users understand privacy policies that impact them is a key
challenge for a real GDPR deployment. Research studies are mostly carried in
English, but in Europe and elsewhere, users speak a language that is not
English. Replicating studies in different languages requires the availability
of comparable cross-language privacy policies corpora. This work provides a
methodology for building comparable cross-language in a national language and a
reference study language. We provide an application example of our methodology
comparing English and Italian extending the corpus of one of the first studies
about users understanding of technical terms in privacy policies. We also
investigate other open issues that can make replication harder.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Piecewise Linear and Stochastic Models for the Analysis of Cyber Resilience. (arXiv:2302.04982v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04982">http://arxiv.org/abs/2302.04982</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04982] Piecewise Linear and Stochastic Models for the Analysis of Cyber Resilience](http://arxiv.org/abs/2302.04982) #defense</code></li>
<li>Summary: <p>We model a vehicle equipped with an autonomous cyber-defense system in
addition to its inherent physical resilience features. When attacked, this
ensemble of cyber-physical features (i.e., ``bonware'') strives to resist and
recover from the performance degradation caused by the malware's attack. We
model the underlying differential equations governing such attacks for
piecewise linear characterizations of malware and bonware, develop a discrete
time stochastic model, and show that averages of instantiations of the
stochastic model approximate solutions to the continuous differential equation.
We develop a theory and methodology for approximating the parameters associated
with these equations.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples. (arXiv:2302.05086v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05086">http://arxiv.org/abs/2302.05086</a></li>
<li>Code URL: <a href="https://github.com/qizhangli/morebayesian-attack">https://github.com/qizhangli/morebayesian-attack</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05086] Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples](http://arxiv.org/abs/2302.05086) #attack</code></li>
<li>Summary: <p>The transferability of adversarial examples across deep neural networks
(DNNs) is the crux of many black-box attacks. Many prior efforts have been
devoted to improving the transferability via increasing the diversity in inputs
of some substitute models. In this paper, by contrast, we opt for the diversity
in substitute models and advocate to attack a Bayesian model for achieving
desirable transferability. Deriving from the Bayesian formulation, we develop a
principled strategy for possible finetuning, which can be combined with many
off-the-shelf Gaussian posterior approximations over DNN parameters. Extensive
experiments have been conducted to verify the effectiveness of our method, on
common benchmark datasets, and the results demonstrate that our method
outperforms recent state-of-the-arts by large margins (roughly 19% absolute
increase in average attack success rate on ImageNet), and, by combining with
these recent methods, further performance gain can be obtained. Our code:
https://github.com/qizhangli/MoreBayesian-attack.
</p></li>
</ul>

<h3>Title: Step by Step Loss Goes Very Far: Multi-Step Quantization for Adversarial Text Attacks. (arXiv:2302.05120v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05120">http://arxiv.org/abs/2302.05120</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05120] Step by Step Loss Goes Very Far: Multi-Step Quantization for Adversarial Text Attacks](http://arxiv.org/abs/2302.05120) #attack</code></li>
<li>Summary: <p>We propose a novel gradient-based attack against transformer-based language
models that searches for an adversarial example in a continuous space of token
probabilities. Our algorithm mitigates the gap between adversarial loss for
continuous and discrete text representations by performing multi-step
quantization in a quantization-compensation loop. Experiments show that our
method significantly outperforms other approaches on various natural language
processing (NLP) tasks.
</p></li>
</ul>

<h3>Title: Cyber-Resilience Approaches for Cyber-Physical Systems. (arXiv:2302.05402v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05402">http://arxiv.org/abs/2302.05402</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05402] Cyber-Resilience Approaches for Cyber-Physical Systems](http://arxiv.org/abs/2302.05402) #attack</code></li>
<li>Summary: <p>Concerns for the resilience of Cyber-Physical Systems (CPS) in critical
infrastructure are growing. CPS integrate sensing, computation, control and
networking into physical objects and mission-critical services, connecting
traditional infrastructure to internet technologies. While this integration
increases service efficiency, it has to face the possibility of new threats
posed by the new functionalities. This leads to cyber-threats, such as
denial-of-service, modification of data, information leakage, spreading of
malware, and many others. Cyber-resilience refers to the ability of a CPS to
prepare, absorb, recover, and adapt to the adverse effects associated with
cyber-threats, e.g., physical degradation of the CPS performance resulting from
a cyber-attack. Cyber-resilience aims at ensuring CPS survival, by keeping the
core functionalities of the CPS in case of extreme events. The literature on
cyber-resilience is rapidly increasing, leading to a broad variety of research
works addressing this new topic. In this article, we create a systematization
of knowledge about existing scientific efforts of making CPS cyber-resilient.
We systematically survey recent literature addressing cyber-resilience with a
focus on techniques that may be used on CPS. We first provide preliminaries and
background on CPS and threats, and subsequently survey state-of-the-art
approaches that have been proposed by recent research work applicable to CPS.
In particular, we aim at differentiating research work from traditional risk
management approaches, based on the general acceptance that it is unfeasible to
prevent and mitigate all possible risks threatening a CPS. We also discuss
questions and research challenges, with a focus on the practical aspects of
cyber-resilience, such as the use of metrics and evaluation methods, as well as
testing and validation environments.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Hyperparameter Search Is All You Need For Training-Agnostic Backdoor Robustness. (arXiv:2302.04977v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04977">http://arxiv.org/abs/2302.04977</a></li>
<li>Code URL: <a href="https://github.com/ebagdasa/mithridates">https://github.com/ebagdasa/mithridates</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04977] Hyperparameter Search Is All You Need For Training-Agnostic Backdoor Robustness](http://arxiv.org/abs/2302.04977) #robust</code></li>
<li>Summary: <p>Commoditization and broad adoption of machine learning (ML) technologies
expose users of these technologies to new security risks. Many models today are
based on neural networks. Training and deploying these models for real-world
applications involves complex hardware and software pipelines applied to
training data from many sources. Models trained on untrusted data are
vulnerable to poisoning attacks that introduce "backdoor" functionality.
Compromising a fraction of the training data requires few resources from the
attacker, but defending against these attacks is a challenge. Although there
have been dozens of defenses proposed in the research literature, most of them
are expensive to integrate or incompatible with the existing training
pipelines.
</p></li>
</ul>

<p>In this paper, we take a pragmatic, developer-centric view and show how
practitioners can answer two actionable questions: (1) how robust is my model
to backdoor poisoning attacks?, and (2) how can I make it more robust without
changing the training pipeline? We focus on the size of the compromised subset
of the training data as a universal metric. We propose an easy-to-learn
primitive sub-task to estimate this metric, thus providing a baseline on
backdoor poisoning. Next, we show how to leverage hyperparameter search - a
tool that ML developers already extensively use - to balance the model's
accuracy and robustness to poisoning, without changes to the training pipeline.
</p>
<p>We demonstrate how to use our metric to estimate the robustness of models to
backdoor attacks. We then design, implement, and evaluate a multi-stage
hyperparameter search method we call Mithridates that strengthens robustness by
3-5x with only a slight impact on the model's accuracy. We show that the
hyperparameters found by our method increase robustness against multiple types
of backdoor attacks and extend our method to AutoML and federated learning.
</p>

<h3>Title: Predicting Out-of-Distribution Error with Confidence Optimal Transport. (arXiv:2302.05018v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05018">http://arxiv.org/abs/2302.05018</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05018] Predicting Out-of-Distribution Error with Confidence Optimal Transport](http://arxiv.org/abs/2302.05018) #robust</code></li>
<li>Summary: <p>Out-of-distribution (OOD) data poses serious challenges in deployed machine
learning models as even subtle changes could incur significant performance
drops. Being able to estimate a model's performance on test data is important
in practice as it indicates when to trust to model's decisions. We present a
simple yet effective method to predict a model's performance on an unknown
distribution without any addition annotation. Our approach is rooted in the
Optimal Transport theory, viewing test samples' output softmax scores from deep
neural networks as empirical samples from an unknown distribution. We show that
our method, Confidence Optimal Transport (COT), provides robust estimates of a
model's performance on a target domain. Despite its simplicity, our method
achieves state-of-the-art results on three benchmark datasets and outperforms
existing methods by a large margin.
</p></li>
</ul>

<h3>Title: Long-Tailed Partial Label Learning via Dynamic Rebalancing. (arXiv:2302.05080v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05080">http://arxiv.org/abs/2302.05080</a></li>
<li>Code URL: <a href="https://github.com/mediabrain-sjtu/records-ltpll">https://github.com/mediabrain-sjtu/records-ltpll</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05080] Long-Tailed Partial Label Learning via Dynamic Rebalancing](http://arxiv.org/abs/2302.05080) #robust</code></li>
<li>Summary: <p>Real-world data usually couples the label ambiguity and heavy imbalance,
challenging the algorithmic robustness of partial label learning (PLL) and
long-tailed learning (LT). The straightforward combination of LT and PLL, i.e.,
LT-PLL, suffers from a fundamental dilemma: LT methods build upon a given class
distribution that is unavailable in PLL, and the performance of PLL is severely
influenced in long-tailed context. We show that even with the auxiliary of an
oracle class prior, the state-of-the-art methods underperform due to an adverse
fact that the constant rebalancing in LT is harsh to the label disambiguation
in PLL. To overcome this challenge, we thus propose a dynamic rebalancing
method, termed as RECORDS, without assuming any prior knowledge about the class
distribution. Based on a parametric decomposition of the biased output, our
method constructs a dynamic adjustment that is benign to the label
disambiguation process and theoretically converges to the oracle class prior.
Extensive experiments on three benchmark datasets demonstrate the significant
gain of RECORDS compared with a range of baselines. The code is publicly
available.
</p></li>
</ul>

<h3>Title: CCDN: Checkerboard Corner Detection Network for Robust Camera Calibration. (arXiv:2302.05097v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05097">http://arxiv.org/abs/2302.05097</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05097] CCDN: Checkerboard Corner Detection Network for Robust Camera Calibration](http://arxiv.org/abs/2302.05097) #robust</code></li>
<li>Summary: <p>Aiming to improve the checkerboard corner detection robustness against the
images with poor quality, such as lens distortion, extreme poses, and noise, we
propose a novel detection algorithm which can maintain high accuracy on inputs
under multiply scenarios without any prior knowledge of the checkerboard
pattern. This whole algorithm includes a checkerboard corner detection network
and some post-processing techniques. The network model is a fully convolutional
network with improvements of loss function and learning rate, which can deal
with the images of arbitrary size and produce correspondingly-sized output with
a corner score on each pixel by efficient inference and learning. Besides, in
order to remove the false positives, we employ three post-processing techniques
including threshold related to maximum response, non-maximum suppression, and
clustering. Evaluations on two different datasets show its superior robustness,
accuracy and wide applicability in quantitative comparisons with the
state-of-the-art methods, like MATE, ChESS, ROCHADE and OCamCalib.
</p></li>
</ul>

<h3>Title: Exploiting Neighborhood Structural Features for Change Detection. (arXiv:2302.05114v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05114">http://arxiv.org/abs/2302.05114</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05114] Exploiting Neighborhood Structural Features for Change Detection](http://arxiv.org/abs/2302.05114) #robust</code></li>
<li>Summary: <p>In this letter, a novel method for change detection is proposed using
neighborhood structure correlation. Because structure features are insensitive
to the intensity differences between bi-temporal images, we perform the
correlation analysis on structure features rather than intensity information.
First, we extract the structure feature maps by using multi-orientated gradient
information. Then, the structure feature maps are used to obtain the
Neighborhood Structural Correlation Image (NSCI), which can represent the
context structure information. In addition, we introduce a measure named
matching error which can be used to improve neighborhood information.
Subsequently, a change detection model based on the random forest is
constructed. The NSCI feature and matching error are used as the model inputs
for training and prediction. Finally, the decision tree voting is used to
produce the change detection result. To evaluate the performance of the
proposed method, it was compared with three state-of-the-art change detection
methods. The experimental results on two datasets demonstrated the
effectiveness and robustness of the proposed method.
</p></li>
</ul>

<h3>Title: TTN: A Domain-Shift Aware Batch Normalization in Test-Time Adaptation. (arXiv:2302.05155v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05155">http://arxiv.org/abs/2302.05155</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05155] TTN: A Domain-Shift Aware Batch Normalization in Test-Time Adaptation](http://arxiv.org/abs/2302.05155) #robust</code></li>
<li>Summary: <p>This paper proposes a novel batch normalization strategy for test-time
adaptation. Recent test-time adaptation methods heavily rely on the modified
batch normalization, i.e., transductive batch normalization (TBN), which
calculates the mean and the variance from the current test batch rather than
using the running mean and variance obtained from the source data, i.e.,
conventional batch normalization (CBN). Adopting TBN that employs test batch
statistics mitigates the performance degradation caused by the domain shift.
However, re-estimating normalization statistics using test data depends on
impractical assumptions that a test batch should be large enough and be drawn
from i.i.d. stream, and we observed that the previous methods with TBN show
critical performance drop without the assumptions. In this paper, we identify
that CBN and TBN are in a trade-off relationship and present a new test-time
normalization (TTN) method that interpolates the statistics by adjusting the
importance between CBN and TBN according to the domain-shift sensitivity of
each BN layer. Our proposed TTN improves model robustness to shifted domains
across a wide range of batch sizes and in various realistic evaluation
scenarios. TTN is widely applicable to other test-time adaptation methods that
rely on updating model parameters via backpropagation. We demonstrate that
adopting TTN further improves their performance and achieves state-of-the-art
performance in various standard benchmarks.
</p></li>
</ul>

<h3>Title: Dual Memory Units with Uncertainty Regulation for Weakly Supervised Video Anomaly Detection. (arXiv:2302.05160v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05160">http://arxiv.org/abs/2302.05160</a></li>
<li>Code URL: <a href="https://github.com/henrryzh1/UR-DMU">https://github.com/henrryzh1/UR-DMU</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05160] Dual Memory Units with Uncertainty Regulation for Weakly Supervised Video Anomaly Detection](http://arxiv.org/abs/2302.05160) #robust</code></li>
<li>Summary: <p>Learning discriminative features for effectively separating abnormal events
from normality is crucial for weakly supervised video anomaly detection
(WS-VAD) tasks. Existing approaches, both video and segment-level label
oriented, mainly focus on extracting representations for anomaly data while
neglecting the implication of normal data. We observe that such a scheme is
sub-optimal, i.e., for better distinguishing anomaly one needs to understand
what is a normal state, and may yield a higher false alarm rate. To address
this issue, we propose an Uncertainty Regulated Dual Memory Units (UR-DMU)
model to learn both the representations of normal data and discriminative
features of abnormal data. To be specific, inspired by the traditional global
and local structure on graph convolutional networks, we introduce a Global and
Local Multi-Head Self Attention (GL-MHSA) module for the Transformer network to
obtain more expressive embeddings for capturing associations in videos. Then,
we use two memory banks, one additional abnormal memory for tackling hard
samples, to store and separate abnormal and normal prototypes and maximize the
margins between the two representations. Finally, we propose an uncertainty
learning scheme to learn the normal data latent space, that is robust to noise
from camera switching, object changing, scene transforming, etc. Extensive
experiments on XD-Violence and UCF-Crime datasets demonstrate that our method
outperforms the state-of-the-art methods by a sizable margin.
</p></li>
</ul>

<h3>Title: MoreauGrad: Sparse and Robust Interpretation of Neural Networks via Moreau Envelope. (arXiv:2302.05294v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05294">http://arxiv.org/abs/2302.05294</a></li>
<li>Code URL: <a href="https://github.com/buyeah1109/moreaugrad">https://github.com/buyeah1109/moreaugrad</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05294] MoreauGrad: Sparse and Robust Interpretation of Neural Networks via Moreau Envelope](http://arxiv.org/abs/2302.05294) #robust</code></li>
<li>Summary: <p>Explaining the predictions of deep neural nets has been a topic of great
interest in the computer vision literature. While several gradient-based
interpretation schemes have been proposed to reveal the influential variables
in a neural net's prediction, standard gradient-based interpretation frameworks
have been commonly observed to lack robustness to input perturbations and
flexibility for incorporating prior knowledge of sparsity and group-sparsity
structures. In this work, we propose MoreauGrad as an interpretation scheme
based on the classifier neural net's Moreau envelope. We demonstrate that
MoreauGrad results in a smooth and robust interpretation of a multi-layer
neural network and can be efficiently computed through first-order optimization
methods. Furthermore, we show that MoreauGrad can be naturally combined with
$L_1$-norm regularization techniques to output a sparse or group-sparse
explanation which are prior conditions applicable to a wide range of deep
learning applications. We empirically evaluate the proposed MoreauGrad scheme
on standard computer vision datasets, showing the qualitative and quantitative
success of the MoreauGrad approach in comparison to standard gradient-based
interpretation methods.
</p></li>
</ul>

<h3>Title: Scaling Vision Transformers to 22 Billion Parameters. (arXiv:2302.05442v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05442">http://arxiv.org/abs/2302.05442</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05442] Scaling Vision Transformers to 22 Billion Parameters](http://arxiv.org/abs/2302.05442) #robust</code></li>
<li>Summary: <p>The scaling of Transformers has driven breakthrough capabilities for language
models. At present, the largest large language models (LLMs) contain upwards of
100B parameters. Vision Transformers (ViT) have introduced the same
architecture to image and video modelling, but these have not yet been
successfully scaled to nearly the same degree; the largest dense ViT contains
4B parameters (Chen et al., 2022). We present a recipe for highly efficient and
stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of
experiments on the resulting model. When evaluated on downstream tasks (often
with a lightweight linear model on frozen features), ViT-22B demonstrates
increasing performance with scale. We further observe other interesting
benefits of scale, including an improved tradeoff between fairness and
performance, state-of-the-art alignment to human visual perception in terms of
shape/texture bias, and improved robustness. ViT-22B demonstrates the potential
for "LLM-like" scaling in vision, and provides key steps towards getting there.
</p></li>
</ul>

<h3>Title: PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR Error Correction. (arXiv:2302.05040v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05040">http://arxiv.org/abs/2302.05040</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05040] PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR Error Correction](http://arxiv.org/abs/2302.05040) #robust</code></li>
<li>Summary: <p>Speech-to-text errors made by automatic speech recognition (ASR) system
negatively impact downstream models relying on ASR transcriptions. Language
error correction models as a post-processing text editing approach have been
recently developed for refining the source sentences. However, efficient models
for correcting errors in ASR transcriptions that meet the low latency
requirements of industrial grade production systems have not been well studied.
In this work, we propose a novel non-autoregressive (NAR) error correction
approach to improve the transcription quality by reducing word error rate (WER)
and achieve robust performance across different upstream ASR systems. Our
approach augments the text encoding of the Transformer model with a phoneme
encoder that embeds pronunciation information. The representations from phoneme
encoder and text encoder are combined via multi-modal fusion before feeding
into the length tagging predictor for predicting target sequence lengths. The
joint encoders also provide inputs to the attention mechanism in the NAR
decoder. We experiment on 3 open-source ASR systems with varying speech-to-text
transcription quality and their erroneous transcriptions on 2 public English
corpus datasets. Results show that our PATCorrect (Phoneme Augmented
Transformer for ASR error Correction) consistently outperforms state-of-the-art
NAR error correction method on English corpus across different upstream ASR
systems. For example, PATCorrect achieves 11.62% WER reduction (WERR) averaged
on 3 ASR systems compared to 9.46% WERR achieved by other method using text
only modality and also achieves an inference latency comparable to other NAR
models at tens of millisecond scale, especially on GPU hardware, while still
being 4.2 - 6.7x times faster than autoregressive models on Common Voice and
LibriSpeech datasets.
</p></li>
</ul>

<h3>Title: Debiasing Recommendation by Learning Identifiable Latent Confounders. (arXiv:2302.05052v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05052">http://arxiv.org/abs/2302.05052</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05052] Debiasing Recommendation by Learning Identifiable Latent Confounders](http://arxiv.org/abs/2302.05052) #robust</code></li>
<li>Summary: <p>Recommendation systems aim to predict users' feedback on items not exposed to
them.
</p></li>
</ul>

<p>Confounding bias arises due to the presence of unmeasured variables (e.g.,
the socio-economic status of a user) that can affect both a user's exposure and
feedback. Existing methods either (1) make untenable assumptions about these
unmeasured variables or (2) directly infer latent confounders from users'
exposure. However, they cannot guarantee the identification of counterfactual
feedback, which can lead to biased predictions. In this work, we propose a
novel method, i.e., identifiable deconfounder (iDCF), which leverages a set of
proxy variables (e.g., observed user features) to resolve the aforementioned
non-identification issue. The proposed iDCF is a general deconfounded
recommendation framework that applies proximal causal inference to infer the
unmeasured confounders and identify the counterfactual feedback with
theoretical guarantees. Extensive experiments on various real-world and
synthetic datasets verify the proposed method's effectiveness and robustness.
</p>

<h3>Title: Monte Carlo Neural Operator for Learning PDEs via Probabilistic Representation. (arXiv:2302.05104v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05104">http://arxiv.org/abs/2302.05104</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05104] Monte Carlo Neural Operator for Learning PDEs via Probabilistic Representation](http://arxiv.org/abs/2302.05104) #robust</code></li>
<li>Summary: <p>Neural operators, which use deep neural networks to approximate the solution
mappings of partial differential equation (PDE) systems, are emerging as a new
paradigm for PDE simulation. The neural operators could be trained in
supervised or unsupervised ways, i.e., by using the generated data or the PDE
information. The unsupervised training approach is essential when data
generation is costly or the data is less qualified (e.g., insufficient and
noisy). However, its performance and efficiency have plenty of room for
improvement. To this end, we design a new loss function based on the
Feynman-Kac formula and call the developed neural operator Monte-Carlo Neural
Operator (MCNO), which can allow larger temporal steps and efficiently handle
fractional diffusion operators. Our analyses show that MCNO has advantages in
handling complex spatial conditions and larger temporal steps compared with
other unsupervised methods. Furthermore, MCNO is more robust with the
perturbation raised by the numerical scheme and operator approximation.
Numerical experiments on the diffusion equation and Navier-Stokes equation show
significant accuracy improvement compared with other unsupervised baselines,
especially for the vibrated initial condition and long-time simulation
settings.
</p></li>
</ul>

<h3>Title: Beyond In-Domain Scenarios: Robust Density-Aware Calibration. (arXiv:2302.05118v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05118">http://arxiv.org/abs/2302.05118</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05118] Beyond In-Domain Scenarios: Robust Density-Aware Calibration](http://arxiv.org/abs/2302.05118) #robust</code></li>
<li>Summary: <p>Calibrating deep learning models to yield uncertainty-aware predictions is
crucial as deep neural networks get increasingly deployed in safety-critical
applications. While existing post-hoc calibration methods achieve impressive
results on in-domain test datasets, they are limited by their inability to
yield reliable uncertainty estimates in domain-shift and out-of-domain (OOD)
scenarios. We aim to bridge this gap by proposing DAC, an accuracy-preserving
as well as Density-Aware Calibration method based on k-nearest-neighbors (KNN).
In contrast to existing post-hoc methods, we utilize hidden layers of
classifiers as a source for uncertainty-related information and study their
importance. We show that DAC is a generic method that can readily be combined
with state-of-the-art post-hoc methods. DAC boosts the robustness of
calibration performance in domain-shift and OOD, while maintaining excellent
in-domain predictive uncertainty estimates. We demonstrate that DAC leads to
consistently better calibration across a large number of model architectures,
datasets, and metrics. Additionally, we show that DAC improves calibration
substantially on recent large-scale neural networks pre-trained on vast amounts
of data.
</p></li>
</ul>

<h3>Title: Discovering Sparse Hysteresis Models for Piezoelectric Materials: A Data-Driven Study and Perspectives into Modelling Magnetic Hysteresis. (arXiv:2302.05313v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05313">http://arxiv.org/abs/2302.05313</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05313] Discovering Sparse Hysteresis Models for Piezoelectric Materials: A Data-Driven Study and Perspectives into Modelling Magnetic Hysteresis](http://arxiv.org/abs/2302.05313) #robust</code></li>
<li>Summary: <p>This article presents an approach for modelling hysteresis in piezoelectric
materials that leverages recent advancements in machine learning, particularly
in sparse-regression techniques. While sparse regression has previously been
used to model various scientific and engineering phenomena, its application to
nonlinear hysteresis modelling in piezoelectric materials has yet to be
explored. The study employs the sequential threshold least-squares algorithm to
model the dynamic system responsible for hysteresis, resulting in a concise
model that accurately predicts hysteresis for both simulated and experimental
piezoelectric material data. Additionally, insights are provided on sparse
white-box modelling of hysteresis for magnetic materials taking non-oriented
electrical steel as an example. The presented approach is compared to
traditional regression-based and neural network methods, demonstrating its
efficiency and robustness.
</p></li>
</ul>

<h3>Title: Towards Minimax Optimality of Model-based Robust Reinforcement Learning. (arXiv:2302.05372v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05372">http://arxiv.org/abs/2302.05372</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05372] Towards Minimax Optimality of Model-based Robust Reinforcement Learning](http://arxiv.org/abs/2302.05372) #robust</code></li>
<li>Summary: <p>We study the sample complexity of obtaining an $\epsilon$-optimal policy in
\emph{Robust} discounted Markov Decision Processes (RMDPs), given only access
to a generative model of the nominal kernel. This problem is widely studied in
the non-robust case, and it is known that any planning approach applied to an
empirical MDP estimated with $\tilde{\mathcal{O}}(\frac{H^3 \mid S \mid\mid A
\mid}{\epsilon^2})$ samples provides an $\epsilon$-optimal policy, which is
minimax optimal. Results in the robust case are much more scarce. For $sa$-
(resp $s$-)rectangular uncertainty sets, the best known sample complexity is
$\tilde{\mathcal{O}}(\frac{H^4 \mid S \mid^2\mid A \mid}{\epsilon^2})$ (resp.
$\tilde{\mathcal{O}}(\frac{H^4 \mid S \mid^2\mid A \mid^2}{\epsilon^2})$), for
specific algorithms and when the uncertainty set is based on the total
variation (TV), the KL or the Chi-square divergences. In this paper, we
consider uncertainty sets defined with an $L_p$-ball (recovering the TV case),
and study the sample complexity of \emph{any} planning algorithm (with high
accuracy guarantee on the solution) applied to an empirical RMDP estimated
using the generative model. In the general case, we prove a sample complexity
of $\tilde{\mathcal{O}}(\frac{H^4 \mid S \mid\mid A \mid}{\epsilon^2})$ for
both the $sa$- and $s$-rectangular cases (improvements of $\mid S \mid$ and
$\mid S \mid\mid A \mid$ respectively). When the size of the uncertainty is
small enough, we improve the sample complexity to
$\tilde{\mathcal{O}}(\frac{H^3 \mid S \mid\mid A \mid }{\epsilon^2})$,
recovering the lower-bound for the non-robust case for the first time and a
robust lower-bound when the size of the uncertainty is small enough.
</p></li>
</ul>

<h3>Title: A Practical Mixed Precision Algorithm for Post-Training Quantization. (arXiv:2302.05397v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05397">http://arxiv.org/abs/2302.05397</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05397] A Practical Mixed Precision Algorithm for Post-Training Quantization](http://arxiv.org/abs/2302.05397) #robust</code></li>
<li>Summary: <p>Neural network quantization is frequently used to optimize model size,
latency and power consumption for on-device deployment of neural networks. In
many cases, a target bit-width is set for an entire network, meaning every
layer get quantized to the same number of bits. However, for many networks some
layers are significantly more robust to quantization noise than others, leaving
an important axis of improvement unused. As many hardware solutions provide
multiple different bit-width settings, mixed-precision quantization has emerged
as a promising solution to find a better performance-efficiency trade-off than
homogeneous quantization. However, most existing mixed precision algorithms are
rather difficult to use for practitioners as they require access to the
training data, have many hyper-parameters to tune or even depend on end-to-end
retraining of the entire model. In this work, we present a simple post-training
mixed precision algorithm that only requires a small unlabeled calibration
dataset to automatically select suitable bit-widths for each layer for
desirable on-device performance. Our algorithm requires no hyper-parameter
tuning, is robust to data variation and takes into account practical hardware
deployment constraints making it a great candidate for practical use. We
experimentally validate our proposed method on several computer vision tasks,
natural language processing tasks and many different networks, and show that we
can find mixed precision networks that provide a better trade-off between
accuracy and efficiency than their homogeneous bit-width equivalents.
</p></li>
</ul>

<h3>Title: Project and Probe: Sample-Efficient Domain Adaptation by Interpolating Orthogonal Features. (arXiv:2302.05441v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05441">http://arxiv.org/abs/2302.05441</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05441] Project and Probe: Sample-Efficient Domain Adaptation by Interpolating Orthogonal Features](http://arxiv.org/abs/2302.05441) #robust</code></li>
<li>Summary: <p>Conventional approaches to robustness try to learn a model based on causal
features. However, identifying maximally robust or causal features may be
difficult in some scenarios, and in others, non-causal "shortcut" features may
actually be more predictive. We propose a lightweight, sample-efficient
approach that learns a diverse set of features and adapts to a target
distribution by interpolating these features with a small target dataset. Our
approach, Project and Probe (Pro$^2$), first learns a linear projection that
maps a pre-trained embedding onto orthogonal directions while being predictive
of labels in the source dataset. The goal of this step is to learn a variety of
predictive features, so that at least some of them remain useful after
distribution shift. Pro$^2$ then learns a linear classifier on top of these
projected features using a small target dataset. We theoretically show that
Pro$^2$ learns a projection matrix that is optimal for classification in an
information-theoretic sense, resulting in better generalization due to a
favorable bias-variance tradeoff. Our experiments on four datasets, with
multiple distribution shift settings for each, show that Pro$^2$ improves
performance by 5-15% when given limited target data compared to prior methods
such as standard linear probing.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Fast Learnings of Coupled Nonnegative Tensor Decomposition Using Optimal Gradient and Low-rank Approximation. (arXiv:2302.05119v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05119">http://arxiv.org/abs/2302.05119</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05119] Fast Learnings of Coupled Nonnegative Tensor Decomposition Using Optimal Gradient and Low-rank Approximation](http://arxiv.org/abs/2302.05119) #extraction</code></li>
<li>Summary: <p>Nonnegative tensor decomposition has been widely applied in signal processing
and neuroscience, etc. When it comes to group analysis of multi-block tensors,
traditional tensor decomposition is insufficient to utilize the shared/similar
information among tensors. In this study, we propose a coupled nonnegative
CANDECOMP/PARAFAC decomposition algorithm optimized by the alternating proximal
gradient method (CoNCPDAPG), which is capable of a simultaneous decomposition
of tensors from different samples that are partially linked and a simultaneous
extraction of common components, individual components and core tensors. Due to
the low optimization efficiency brought by the nonnegative constraint and the
high-dimensional nature of the data, we further propose the lraCoNCPD-APG
algorithm by combining low-rank approximation and the proposed CoNCPD-APG
method. When processing multi-block large-scale tensors, the proposed
lraCoNCPD-APG algorithm can greatly reduce the computational load without
compromising the decomposition quality. Experiment results of coupled
nonnegative tensor decomposition problems designed for synthetic data,
real-world face images and event-related potential data demonstrate the
practicability and superiority of the proposed algorithms.
</p></li>
</ul>

<h3>Title: Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments. (arXiv:2302.05425v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05425">http://arxiv.org/abs/2302.05425</a></li>
<li>Code URL: <a href="https://github.com/erkara/trackingdroplets">https://github.com/erkara/trackingdroplets</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05425] Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments](http://arxiv.org/abs/2302.05425) #extraction</code></li>
<li>Summary: <p>We present a deep-learning based tracking objects of interest in walking
droplet and granular intruder experiments. In a typical walking droplet
experiment, a liquid droplet, known as \textit{walker}, propels itself
laterally on the free surface of a vibrating bath of the same liquid. This
motion is the result of the interaction between the droplets and the surface
waves generated by the droplet itself after each successive bounce. A walker
can exhibit a highly irregular trajectory over the course of its motion,
including rapid acceleration and complex interactions with the other walkers
present in the same bath. In analogy with the hydrodynamic experiments, the
granular matter experiments consist of a vibrating bath of very small solid
particles and a larger solid \textit{intruder}. Like the fluid droplets, the
intruder interacts with and travels the domain due to the waves of the bath but
tends to move much slower and much less smoothly than the droplets. When
multiple intruders are introduced, they also exhibit complex interactions with
each other. We leverage the state-of-art object detection model YOLO and the
Hungarian Algorithm to accurately extract the trajectory of a walker or
intruder in real-time. Our proposed methodology is capable of tracking
individual walker(s) or intruder(s) in digital images acquired from a broad
spectrum of experimental settings and does not suffer from any identity-switch
issues. Thus, the deep learning approach developed in this work could be used
to automatize the efficient, fast and accurate extraction of observables of
interests in walking droplet and granular flow experiments. Such extraction
capabilities are critically enabling for downstream tasks such as building
data-driven dynamical models for the coarse-grained dynamics and interactions
of the objects of interest.
</p></li>
</ul>

<h3>Title: Event Temporal Relation Extraction with Bayesian Translational Model. (arXiv:2302.04985v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04985">http://arxiv.org/abs/2302.04985</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04985] Event Temporal Relation Extraction with Bayesian Translational Model](http://arxiv.org/abs/2302.04985) #extraction</code></li>
<li>Summary: <p>Existing models to extract temporal relations between events lack a
principled method to incorporate external knowledge. In this study, we
introduce Bayesian-Trans, a Bayesian learning-based method that models the
temporal relation representations as latent variables and infers their values
via Bayesian inference and translational functions. Compared to conventional
neural approaches, instead of performing point estimation to find the best set
parameters, the proposed model infers the parameters' posterior distribution
directly, enhancing the model's capability to encode and express uncertainty
about the predictions. Experimental results on the three widely used datasets
show that Bayesian-Trans outperforms existing approaches for event temporal
relation extraction. We additionally present detailed analyses on uncertainty
quantification, comparison of priors, and ablation studies, illustrating the
benefits of the proposed approach.
</p></li>
</ul>

<h3>Title: GCI: A (G)raph (C)oncept (I)nterpretation Framework. (arXiv:2302.04899v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04899">http://arxiv.org/abs/2302.04899</a></li>
<li>Code URL: <a href="https://github.com/dmitrykazhdan/gci">https://github.com/dmitrykazhdan/gci</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04899] GCI: A (G)raph (C)oncept (I)nterpretation Framework](http://arxiv.org/abs/2302.04899) #extraction</code></li>
<li>Summary: <p>Explainable AI (XAI) underwent a recent surge in research on concept
extraction, focusing on extracting human-interpretable concepts from Deep
Neural Networks. An important challenge facing concept extraction approaches is
the difficulty of interpreting and evaluating discovered concepts, especially
for complex tasks such as molecular property prediction. We address this
challenge by presenting GCI: a (G)raph (C)oncept (I)nterpretation framework,
used for quantitatively measuring alignment between concepts discovered from
Graph Neural Networks (GNNs) and their corresponding human interpretations. GCI
encodes concept interpretations as functions, which can be used to
quantitatively measure the alignment between a given interpretation and concept
definition. We demonstrate four applications of GCI: (i) quantitatively
evaluating concept extractors, (ii) measuring alignment between concept
extractors and human interpretations, (iii) measuring the completeness of
interpretations with respect to an end task and (iv) a practical application of
GCI to molecular property prediction, in which we demonstrate how to use
chemical functional groups to explain GNNs trained on molecular property
prediction tasks, and implement interpretations with a 0.76 AUCROC completeness
score.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Communication-Efficient Federated Hypergradient Computation via Aggregated Iterative Differentiation. (arXiv:2302.04969v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04969">http://arxiv.org/abs/2302.04969</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04969] Communication-Efficient Federated Hypergradient Computation via Aggregated Iterative Differentiation](http://arxiv.org/abs/2302.04969) #federate</code></li>
<li>Summary: <p>Federated bilevel optimization has attracted increasing attention due to
emerging machine learning and communication applications. The biggest challenge
lies in computing the gradient of the upper-level objective function (i.e.,
hypergradient) in the federated setting due to the nonlinear and distributed
construction of a series of global Hessian matrices. In this paper, we propose
a novel communication-efficient federated hypergradient estimator via
aggregated iterative differentiation (AggITD). AggITD is simple to implement
and significantly reduces the communication cost by conducting the federated
hypergradient estimation and the lower-level optimization simultaneously. We
show that the proposed AggITD-based algorithm achieves the same sample
complexity as existing approximate implicit differentiation (AID)-based
approaches with much fewer communication rounds in the presence of data
heterogeneity. Our results also shed light on the great advantage of ITD over
AID in the federated/distributed hypergradient estimation. This differs from
the comparison in the non-distributed bilevel optimization, where ITD is less
efficient than AID. Our extensive experiments demonstrate the great
effectiveness and communication efficiency of the proposed method.
</p></li>
</ul>

<h3>Title: Federated Domain Adaptation via Gradient Projection. (arXiv:2302.05049v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05049">http://arxiv.org/abs/2302.05049</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05049] Federated Domain Adaptation via Gradient Projection](http://arxiv.org/abs/2302.05049) #federate</code></li>
<li>Summary: <p>Federated Domain Adaptation (FDA) describes the federated learning setting
where a set of source clients work collaboratively to improve the performance
of a target client and where the target client has limited labeled data. The
domain shift between the source and target domains, combined with limited
samples in the target domain, makes FDA a challenging problem, e.g., common
techniques such as FedAvg and fine-tuning fail with a large domain shift. To
fill this gap, we propose Federated Gradient Projection ($\texttt{FedGP}$), a
novel aggregation rule for FDA, used to aggregate the source gradients and
target gradient during training. Further, we introduce metrics that
characterize the FDA setting and propose a theoretical framework for analyzing
the performance of aggregation rules, which may be of independent interest.
Using this framework, we theoretically characterize how, when, and why
$\texttt{FedGP}$ works compared to baselines. Our theory suggests certain
practical rules that are predictive of practice. Experiments on synthetic and
real-world datasets verify the theoretical insights and illustrate the
effectiveness of the proposed method in practice.
</p></li>
</ul>

<h3>Title: XFL: A High Performace, Lightweighted Federated Learning Framework. (arXiv:2302.05076v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05076">http://arxiv.org/abs/2302.05076</a></li>
<li>Code URL: <a href="https://github.com/paritybit-ai/xfl">https://github.com/paritybit-ai/xfl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05076] XFL: A High Performace, Lightweighted Federated Learning Framework](http://arxiv.org/abs/2302.05076) #federate</code></li>
<li>Summary: <p>This paper introduces XFL, an industrial-grade federated learning project.
XFL supports training AI models collaboratively on multiple devices, while
utilizes homomorphic encryption, differential privacy, secure multi-party
computation and other security technologies ensuring no leakage of data. XFL
provides an abundant algorithms library, integrating a large number of
pre-built, secure and outstanding federated learning algorithms, covering both
the horizontally and vertically federated learning scenarios. Numerical
experiments have shown the prominent performace of these algorithms. XFL builds
a concise configuration interfaces with presettings for all federation
algorithms, and supports the rapid deployment via docker containers.Therefore,
we believe XFL is the most user-friendly and easy-to-develop federated learning
framework. XFL is open-sourced, and both the code and documents are available
at https://github.com/paritybit-ai/XFL.
</p></li>
</ul>

<h3>Title: Achieving Linear Speedup in Non-IID Federated Bilevel Learning. (arXiv:2302.05412v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05412">http://arxiv.org/abs/2302.05412</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05412] Achieving Linear Speedup in Non-IID Federated Bilevel Learning](http://arxiv.org/abs/2302.05412) #federate</code></li>
<li>Summary: <p>Federated bilevel optimization has received increasing attention in various
emerging machine learning and communication applications. Recently, several
Hessian-vector-based algorithms have been proposed to solve the federated
bilevel optimization problem. However, several important properties in
federated learning such as the partial client participation and the linear
speedup for convergence (i.e., the convergence rate and complexity are improved
linearly with respect to the number of sampled clients) in the presence of
non-i.i.d.~datasets, still remain open. In this paper, we fill these gaps by
proposing a new federated bilevel algorithm named FedMBO with a novel client
sampling scheme in the federated hypergradient estimation. We show that FedMBO
achieves a convergence rate of
$\mathcal{O}\big(\frac{1}{\sqrt{nK}}+\frac{1}{K}+\frac{\sqrt{n}}{K^{3/2}}\big)$
on non-i.i.d.~datasets, where $n$ is the number of participating clients in
each round, and $K$ is the total number of iteration. This is the first
theoretical linear speedup result for non-i.i.d.~federated bilevel
optimization. Extensive experiments validate our theoretical results and
demonstrate the effectiveness of our proposed method.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: AutoNMT: A Framework to Streamline the Research of Seq2Seq Models. (arXiv:2302.04981v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04981">http://arxiv.org/abs/2302.04981</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04981] AutoNMT: A Framework to Streamline the Research of Seq2Seq Models](http://arxiv.org/abs/2302.04981) #fair</code></li>
<li>Summary: <p>We present AutoNMT, a framework to streamline the research of seq-to-seq
models by automating the data pipeline (i.e., file management, data
preprocessing, and exploratory analysis), automating experimentation in a
toolkit-agnostic manner, which allows users to use either their own models or
existing seq-to-seq toolkits such as Fairseq or OpenNMT, and finally,
automating the report generation (plots and summaries). Furthermore, this
library comes with its own seq-to-seq toolkit so that users can easily
customize it for non-standard tasks.
</p></li>
</ul>

<h3>Title: DRGCN: Dynamic Evolving Initial Residual for Deep Graph Convolutional Networks. (arXiv:2302.05083v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05083">http://arxiv.org/abs/2302.05083</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05083] DRGCN: Dynamic Evolving Initial Residual for Deep Graph Convolutional Networks](http://arxiv.org/abs/2302.05083) #fair</code></li>
<li>Summary: <p>Graph convolutional networks (GCNs) have been proved to be very practical to
handle various graph-related tasks. It has attracted considerable research
interest to study deep GCNs, due to their potential superior performance
compared with shallow ones. However, simply increasing network depth will, on
the contrary, hurt the performance due to the over-smoothing problem. Adding
residual connection is proved to be effective for learning deep convolutional
neural networks (deep CNNs), it is not trivial when applied to deep GCNs.
Recent works proposed an initial residual mechanism that did alleviate the
over-smoothing problem in deep GCNs. However, according to our study, their
algorithms are quite sensitive to different datasets. In their setting, the
personalization (dynamic) and correlation (evolving) of how residual applies
are ignored. To this end, we propose a novel model called Dynamic evolving
initial Residual Graph Convolutional Network (DRGCN). Firstly, we use a dynamic
block for each node to adaptively fetch information from the initial
representation. Secondly, we use an evolving block to model the residual
evolving pattern between layers. Our experimental results show that our model
effectively relieves the problem of over-smoothing in deep GCNs and outperforms
the state-of-the-art (SOTA) methods on various benchmark datasets. Moreover, we
develop a mini-batch version of DRGCN which can be applied to large-scale data.
Coupling with several fair training techniques, our model reaches new SOTA
results on the large-scale ogbn-arxiv dataset of Open Graph Benchmark (OGB).
Our reproducible code is available on GitHub.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Gaussian Process-Gated Hierarchical Mixtures of Experts. (arXiv:2302.04947v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.04947">http://arxiv.org/abs/2302.04947</a></li>
<li>Code URL: <a href="https://github.com/yuhaoliu94/gphme">https://github.com/yuhaoliu94/gphme</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.04947] Gaussian Process-Gated Hierarchical Mixtures of Experts](http://arxiv.org/abs/2302.04947) #interpretability</code></li>
<li>Summary: <p>In this paper, we propose novel Gaussian process-gated hierarchical mixtures
of experts (GPHMEs) that are used for building gates and experts. Unlike in
other mixtures of experts where the gating models are linear to the input, the
gating functions of our model are inner nodes built with Gaussian processes
based on random features that are non-linear and non-parametric. Further, the
experts are also built with Gaussian processes and provide predictions that
depend on test data. The optimization of the GPHMEs is carried out by
variational inference. There are several advantages of the proposed GPHMEs. One
is that they outperform tree-based HME benchmarks that partition the data in
the input space. Another advantage is that they achieve good performance with
reduced complexity. A third advantage of the GPHMEs is that they provide
interpretability of deep Gaussian processes and more generally of deep Bayesian
neural networks. Our GPHMEs demonstrate excellent performance for large-scale
data sets even with quite modest sizes.
</p></li>
</ul>

<h3>Title: ShapeWordNet: An Interpretable Shapelet Neural Network for Physiological Signal Classification. (arXiv:2302.05021v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05021">http://arxiv.org/abs/2302.05021</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05021] ShapeWordNet: An Interpretable Shapelet Neural Network for Physiological Signal Classification](http://arxiv.org/abs/2302.05021) #interpretability</code></li>
<li>Summary: <p>Physiological signals are high-dimensional time series of great practical
values in medical and healthcare applications. However, previous works on its
classification fail to obtain promising results due to the intractable data
characteristics and the severe label sparsity issues. In this paper, we try to
address these challenges by proposing a more effective and interpretable scheme
tailored for the physiological signal classification task. Specifically, we
exploit the time series shapelets to extract prominent local patterns and
perform interpretable sequence discretization to distill the whole-series
information. By doing so, the long and continuous raw signals are compressed
into short and discrete token sequences, where both local patterns and global
contexts are well preserved. Moreover, to alleviate the label sparsity issue, a
multi-scale transformation strategy is adaptively designed to augment data and
a cross-scale contrastive learning mechanism is accordingly devised to guide
the model training. We name our method as ShapeWordNet and conduct extensive
experiments on three real-world datasets to investigate its effectiveness.
Comparative results show that our proposed scheme remarkably outperforms four
categories of cutting-edge approaches. Visualization analysis further witnesses
the good interpretability of the sequence discretization idea based on
shapelets.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Conceptual Views on Tree Ensemble Classifiers. (arXiv:2302.05270v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05270">http://arxiv.org/abs/2302.05270</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05270] Conceptual Views on Tree Ensemble Classifiers](http://arxiv.org/abs/2302.05270) #explainability</code></li>
<li>Summary: <p>Random Forests and related tree-based methods are popular for supervised
learning from table based data. Apart from their ease of parallelization, their
classification performance is also superior. However, this performance,
especially parallelizability, is offset by the loss of explainability.
Statistical methods are often used to compensate for this disadvantage. Yet,
their ability for local explanations, and in particular for global
explanations, is limited. In the present work we propose an algebraic method,
rooted in lattice theory, for the (global) explanation of tree ensembles. In
detail, we introduce two novel conceptual views on tree ensemble classifiers
and demonstrate their explanatory capabilities on Random Forests that were
trained with standard parameters.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Removing Structured Noise with Diffusion Models. (arXiv:2302.05290v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.05290">http://arxiv.org/abs/2302.05290</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.05290] Removing Structured Noise with Diffusion Models](http://arxiv.org/abs/2302.05290) #diffusion</code></li>
<li>Summary: <p>Solving ill-posed inverse problems requires careful formulation of prior
beliefs over the signals of interest and an accurate description of their
manifestation into noisy measurements. Handcrafted signal priors based on e.g.
sparsity are increasingly replaced by data-driven deep generative models, and
several groups have recently shown that state-of-the-art score-based diffusion
models yield particularly strong performance and flexibility. In this paper, we
show that the powerful paradigm of posterior sampling with diffusion models can
be extended to include rich, structured, noise models. To that end, we propose
a joint conditional reverse diffusion process with learned scores for the noise
and signal-generating distribution. We demonstrate strong performance gains
across various inverse problems with structured noise, outperforming
competitive baselines that use normalizing flows and adversarial networks. This
opens up new opportunities and relevant practical applications of diffusion
modeling for inverse problems in the context of non-Gaussian measurements.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
