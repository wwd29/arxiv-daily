<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h2>privacy</h2>
<h3>Title: Trustworthy Human Computation: A Survey. (arXiv:2210.12324v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12324">http://arxiv.org/abs/2210.12324</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12324] Trustworthy Human Computation: A Survey](http://arxiv.org/abs/2210.12324)</code></li>
<li>Summary: <p>Human computation is an approach to solving problems that prove difficult
using AI only, and involves the cooperation of many humans. Because human
computation requires close engagement with both "human populations as users"
and "human populations as driving forces," establishing mutual trust between AI
and humans is an important issue to further the development of human
computation. This survey lays the groundwork for the realization of trustworthy
human computation. First, the trustworthiness of human computation as computing
systems, that is, trust offered by humans to AI, is examined using the RAS
(Reliability, Availability, and Serviceability) analogy, which define measures
of trustworthiness in conventional computer systems. Next, the social
trustworthiness provided by human computation systems to users or participants
is discussed from the perspective of AI ethics, including fairness, privacy,
and transparency. Then, we consider human--AI collaboration based on two-way
trust, in which humans and AI build mutual trust and accomplish difficult tasks
through reciprocal collaboration. Finally, future challenges and research
directions for realizing trustworthy human computation are discussed.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: ADDMU: Detection of Far-Boundary Adversarial Examples with Data and Model Uncertainty Estimation. (arXiv:2210.12396v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12396">http://arxiv.org/abs/2210.12396</a></li>
<li>Code URL: <a href="https://github.com/uclanlp/advexdetection-addmu">https://github.com/uclanlp/advexdetection-addmu</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12396] ADDMU: Detection of Far-Boundary Adversarial Examples with Data and Model Uncertainty Estimation](http://arxiv.org/abs/2210.12396)</code></li>
<li>Summary: <p>Adversarial Examples Detection (AED) is a crucial defense technique against
adversarial attacks and has drawn increasing attention from the Natural
Language Processing (NLP) community. Despite the surge of new AED methods, our
studies show that existing methods heavily rely on a shortcut to achieve good
performance. In other words, current search-based adversarial attacks in NLP
stop once model predictions change, and thus most adversarial examples
generated by those attacks are located near model decision boundaries. To
surpass this shortcut and fairly evaluate AED methods, we propose to test AED
methods with \textbf{F}ar \textbf{B}oundary (\textbf{FB}) adversarial examples.
Existing methods show worse than random guess performance under this scenario.
To overcome this limitation, we propose a new technique, \textbf{ADDMU},
\textbf{a}dversary \textbf{d}etection with \textbf{d}ata and \textbf{m}odel
\textbf{u}ncertainty, which combines two types of uncertainty estimation for
both regular and FB adversarial example detection. Our new method outperforms
previous methods by 3.6 and 6.0 \emph{AUC} points under each scenario. Finally,
our analysis shows that the two types of uncertainty provided by \textbf{ADDMU}
can be leveraged to characterize adversarial examples and identify the ones
that contribute most to model's robustness in adversarial training.
</p></li>
</ul>

<h3>Title: The Dark Side of AutoML: Towards Architectural Backdoor Search. (arXiv:2210.12179v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12179">http://arxiv.org/abs/2210.12179</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12179] The Dark Side of AutoML: Towards Architectural Backdoor Search](http://arxiv.org/abs/2210.12179)</code></li>
<li>Summary: <p>This paper asks the intriguing question: is it possible to exploit neural
architecture search (NAS) as a new attack vector to launch previously
improbable attacks? Specifically, we present EVAS, a new attack that leverages
NAS to find neural architectures with inherent backdoors and exploits such
vulnerability using input-aware triggers. Compared with existing attacks, EVAS
demonstrates many interesting properties: (i) it does not require polluting
training data or perturbing model parameters; (ii) it is agnostic to downstream
fine-tuning or even re-training from scratch; (iii) it naturally evades
defenses that rely on inspecting model parameters or training data. With
extensive evaluation on benchmark datasets, we show that EVAS features high
evasiveness, transferability, and robustness, thereby expanding the adversary's
design spectrum. We further characterize the mechanisms underlying EVAS, which
are possibly explainable by architecture-level ``shortcuts'' that recognize
trigger patterns. This work raises concerns about the current practice of NAS
and points to potential directions to develop effective countermeasures.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: TCAB: A Large-Scale Text Classification Attack Benchmark. (arXiv:2210.12233v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12233">http://arxiv.org/abs/2210.12233</a></li>
<li>Code URL: <a href="https://github.com/react-nlp/tcab_generation">https://github.com/react-nlp/tcab_generation</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12233] TCAB: A Large-Scale Text Classification Attack Benchmark](http://arxiv.org/abs/2210.12233)</code></li>
<li>Summary: <p>We introduce the Text Classification Attack Benchmark (TCAB), a dataset for
analyzing, understanding, detecting, and labeling adversarial attacks against
text classifiers. TCAB includes 1.5 million attack instances, generated by
twelve adversarial attacks targeting three classifiers trained on six source
datasets for sentiment analysis and abuse detection in English. Unlike standard
text classification, text attacks must be understood in the context of the
target classifier that is being attacked, and thus features of the target
classifier are important as well. TCAB includes all attack instances that are
successful in flipping the predicted label; a subset of the attacks are also
labeled by human annotators to determine how frequently the primary semantics
are preserved. The process of generating attacks is automated, so that TCAB can
easily be extended to incorporate new text attacks and better classifiers as
they are developed. In addition to the primary tasks of detecting and labeling
attacks, TCAB can also be used for attack localization, attack target labeling,
and attack characterization. TCAB code and dataset are available at
https://react-nlp.github.io/tcab/.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Learning a Grammar Inducer from Massive Uncurated Instructional Videos. (arXiv:2210.12309v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12309">http://arxiv.org/abs/2210.12309</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12309] Learning a Grammar Inducer from Massive Uncurated Instructional Videos](http://arxiv.org/abs/2210.12309)</code></li>
<li>Summary: <p>Video-aided grammar induction aims to leverage video information for finding
more accurate syntactic grammars for accompanying text. While previous work
focuses on building systems for inducing grammars on text that are well-aligned
with video content, we investigate the scenario, in which text and video are
only in loose correspondence. Such data can be found in abundance online, and
the weak correspondence is similar to the indeterminacy problem studied in
language acquisition. Furthermore, we build a new model that can better learn
video-span correlation without manually designed features adopted by previous
work. Experiments show that our model trained only on large-scale YouTube data
with no text-video alignment reports strong and robust performances across
three unseen datasets, despite domain shift and noisy label issues. Furthermore
our model yields higher F1 scores than the previous state-of-the-art systems
trained on in-domain data.
</p></li>
</ul>

<h3>Title: A Flexible-Frame-Rate Vision-Aided Inertial Object Tracking System for Mobile Devices. (arXiv:2210.12476v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12476">http://arxiv.org/abs/2210.12476</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12476] A Flexible-Frame-Rate Vision-Aided Inertial Object Tracking System for Mobile Devices](http://arxiv.org/abs/2210.12476)</code></li>
<li>Summary: <p>Real-time object pose estimation and tracking is challenging but essential
for emerging augmented reality (AR) applications. In general, state-of-the-art
methods address this problem using deep neural networks which indeed yield
satisfactory results. Nevertheless, the high computational cost of these
methods makes them unsuitable for mobile devices where real-world applications
usually take place. In addition, head-mounted displays such as AR glasses
require at least 90~FPS to avoid motion sickness, which further complicates the
problem. We propose a flexible-frame-rate object pose estimation and tracking
system for mobile devices. It is a monocular visual-inertial-based system with
a client-server architecture. Inertial measurement unit (IMU) pose propagation
is performed on the client side for high speed tracking, and RGB image-based 3D
pose estimation is performed on the server side to obtain accurate poses, after
which the pose is sent to the client side for visual-inertial fusion, where we
propose a bias self-correction mechanism to reduce drift. We also propose a
pose inspection algorithm to detect tracking failures and incorrect pose
estimation. Connected by high-speed networking, our system supports flexible
frame rates up to 120 FPS and guarantees high precision and real-time tracking
on low-end devices. Both simulations and real world experiments show that our
method achieves accurate and robust object tracking.
</p></li>
</ul>

<h3>Title: DOROTHIE: Spoken Dialogue for Handling Unexpected Situations in Interactive Autonomous Driving Agents. (arXiv:2210.12511v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12511">http://arxiv.org/abs/2210.12511</a></li>
<li>Code URL: <a href="https://github.com/sled-group/dorothie">https://github.com/sled-group/dorothie</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12511] DOROTHIE: Spoken Dialogue for Handling Unexpected Situations in Interactive Autonomous Driving Agents](http://arxiv.org/abs/2210.12511)</code></li>
<li>Summary: <p>In the real world, autonomous driving agents navigate in highly dynamic
environments full of unexpected situations where pre-trained models are
unreliable. In these situations, what is immediately available to vehicles is
often only human operators. Empowering autonomous driving agents with the
ability to navigate in a continuous and dynamic environment and to communicate
with humans through sensorimotor-grounded dialogue becomes critical. To this
end, we introduce Dialogue On the ROad To Handle Irregular Events (DOROTHIE), a
novel interactive simulation platform that enables the creation of unexpected
situations on the fly to support empirical studies on situated communication
with autonomous driving agents. Based on this platform, we created the Situated
Dialogue Navigation (SDN), a navigation benchmark of 183 trials with a total of
8415 utterances, around 18.7 hours of control streams, and 2.9 hours of trimmed
audio. SDN is developed to evaluate the agent's ability to predict dialogue
moves from humans as well as generate its own dialogue moves and physical
navigation actions. We further developed a transformer-based baseline model for
these SDN tasks. Our empirical results indicate that language guided-navigation
in a highly dynamic environment is an extremely difficult task for end-to-end
models. These results will provide insight towards future work on robust
autonomous driving agents. The DOROTHIE platform, SDN benchmark, and code for
the baseline model are available at https://github.com/sled-group/DOROTHIE.
</p></li>
</ul>

<h3>Title: How Real is Real: Evaluating the Robustness of Real-World Super Resolution. (arXiv:2210.12523v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12523">http://arxiv.org/abs/2210.12523</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12523] How Real is Real: Evaluating the Robustness of Real-World Super Resolution](http://arxiv.org/abs/2210.12523)</code></li>
<li>Summary: <p>Image super-resolution (SR) is a field in computer vision that focuses on
reconstructing high-resolution images from the respective low-resolution image.
However, super-resolution is a well-known ill-posed problem as most methods
rely on the downsampling method performed on the high-resolution image to form
the low-resolution image to be known. Unfortunately, this is not something that
is available in real-life super-resolution applications such as increasing the
quality of a photo taken on a mobile phone. In this paper we will evaluate
multiple state-of-the-art super-resolution methods and gauge their performance
when presented with various types of real-life images and discuss the benefits
and drawbacks of each method. We also introduce a novel dataset, WideRealSR,
containing real images from a wide variety of sources. Finally, through careful
experimentation and evaluation, we will present a potential solution to
alleviate the generalization problem which is imminent in most state-of-the-art
super-resolution models.
</p></li>
</ul>

<h3>Title: Life is a Circus and We are the Clowns: Automatically Finding Analogies between Situations and Processes. (arXiv:2210.12197v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12197">http://arxiv.org/abs/2210.12197</a></li>
<li>Code URL: <a href="https://github.com/orensul/analogies_mining">https://github.com/orensul/analogies_mining</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12197] Life is a Circus and We are the Clowns: Automatically Finding Analogies between Situations and Processes](http://arxiv.org/abs/2210.12197)</code></li>
<li>Summary: <p>Analogy-making gives rise to reasoning, abstraction, flexible categorization
and counterfactual inference -- abilities lacking in even the best AI systems
today. Much research has suggested that analogies are key to non-brittle
systems that can adapt to new domains. Despite their importance, analogies
received little attention in the NLP community, with most research focusing on
simple word analogies. Work that tackled more complex analogies relied heavily
on manually constructed, hard-to-scale input representations. In this work, we
explore a more realistic, challenging setup: our input is a pair of natural
language procedural texts, describing a situation or a process (e.g., how the
heart works/how a pump works). Our goal is to automatically extract entities
and their relations from the text and find a mapping between the different
domains based on relational similarity (e.g., blood is mapped to water). We
develop an interpretable, scalable algorithm and demonstrate that it identifies
the correct mappings 87% of the time for procedural texts and 94% for stories
from cognitive-psychology literature. We show it can extract analogies from a
large dataset of procedural texts, achieving 79% precision (analogy prevalence
in data: 3%). Lastly, we demonstrate that our algorithm is robust to
paraphrasing the input texts.
</p></li>
</ul>

<h3>Title: Enhancing Tabular Reasoning with Pattern Exploiting Training. (arXiv:2210.12259v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12259">http://arxiv.org/abs/2210.12259</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12259] Enhancing Tabular Reasoning with Pattern Exploiting Training](http://arxiv.org/abs/2210.12259)</code></li>
<li>Summary: <p>Recent methods based on pre-trained language models have exhibited superior
performance over tabular tasks (e.g., tabular NLI), despite showing inherent
problems such as not using the right evidence and inconsistent predictions
across inputs while reasoning over the tabular data. In this work, we utilize
Pattern-Exploiting Training (PET) (i.e., strategic MLM) on pre-trained language
models to strengthen these tabular reasoning models' pre-existing knowledge and
reasoning abilities. Our upgraded model exhibits a superior understanding of
knowledge facts and tabular reasoning compared to current baselines.
Additionally, we demonstrate that such models are more effective for underlying
downstream tasks of tabular inference on InfoTabs. Furthermore, we show our
model's robustness against adversarial sets generated through various character
and word level perturbations.
</p></li>
</ul>

<h3>Title: Text Editing as Imitation Game. (arXiv:2210.12276v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12276">http://arxiv.org/abs/2210.12276</a></li>
<li>Code URL: <a href="https://github.com/shininglab/text-editing-as-imitation-game">https://github.com/shininglab/text-editing-as-imitation-game</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12276] Text Editing as Imitation Game](http://arxiv.org/abs/2210.12276)</code></li>
<li>Summary: <p>Text editing, such as grammatical error correction, arises naturally from
imperfect textual data. Recent works frame text editing as a multi-round
sequence tagging task, where operations -- such as insertion and substitution
-- are represented as a sequence of tags. While achieving good results, this
encoding is limited in flexibility as all actions are bound to token-level
tags. In this work, we reformulate text editing as an imitation game using
behavioral cloning. Specifically, we convert conventional sequence-to-sequence
data into state-to-action demonstrations, where the action space can be as
flexible as needed. Instead of generating the actions one at a time, we
introduce a dual decoders structure to parallel the decoding while retaining
the dependencies between action tokens, coupled with trajectory augmentation to
alleviate the distribution shift that imitation learning often suffers. In
experiments on a suite of Arithmetic Equation benchmarks, our model
consistently outperforms the autoregressive baselines in terms of performance,
efficiency, and robustness. We hope our findings will shed light on future
studies in reinforcement learning applying sequence-level action generation to
natural language processing.
</p></li>
</ul>

<h3>Title: R$^2$F: A General Retrieval, Reading and Fusion Framework for Document-level Natural Language Inference. (arXiv:2210.12328v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12328">http://arxiv.org/abs/2210.12328</a></li>
<li>Code URL: <a href="https://github.com/phoenixsecularbird/r2f">https://github.com/phoenixsecularbird/r2f</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12328] R$^2$F: A General Retrieval, Reading and Fusion Framework for Document-level Natural Language Inference](http://arxiv.org/abs/2210.12328)</code></li>
<li>Summary: <p>Document-level natural language inference (DOCNLI) is a new challenging task
in natural language processing, aiming at judging the entailment relationship
between a pair of hypothesis and premise documents. Current datasets and
baselines largely follow sentence-level settings, but fail to address the
issues raised by longer documents. In this paper, we establish a general
solution, named Retrieval, Reading and Fusion (R2F) framework, and a new
setting, by analyzing the main challenges of DOCNLI: interpretability,
long-range dependency, and cross-sentence inference. The basic idea of the
framework is to simplify document-level task into a set of sentence-level
tasks, and improve both performance and interpretability with the power of
evidence. For each hypothesis sentence, the framework retrieves evidence
sentences from the premise, and reads to estimate its credibility. Then the
sentence-level results are fused to judge the relationship between the
documents. For the setting, we contribute complementary evidence and entailment
label annotation on hypothesis sentences, for interpretability study. Our
experimental results show that R2F framework can obtain state-of-the-art
performance and is robust for diverse evidence retrieval methods. Moreover, it
can give more interpretable prediction results. Our model and code are released
at https://github.com/phoenixsecularbird/R2F.
</p></li>
</ul>

<h3>Title: NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer Data Augmentation. (arXiv:2210.12365v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12365">http://arxiv.org/abs/2210.12365</a></li>
<li>Code URL: <a href="https://github.com/intellabs/neurocounterfactuals">https://github.com/intellabs/neurocounterfactuals</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12365] NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer Data Augmentation](http://arxiv.org/abs/2210.12365)</code></li>
<li>Summary: <p>While counterfactual data augmentation offers a promising step towards robust
generalization in natural language processing, producing a set of
counterfactuals that offer valuable inductive bias for models remains a
challenge. Most existing approaches for producing counterfactuals, manual or
automated, rely on small perturbations via minimal edits, resulting in
simplistic changes. We introduce NeuroCounterfactuals, designed as loose
counterfactuals, allowing for larger edits which result in naturalistic
generations containing linguistic diversity, while still bearing similarity to
the original document. Our novel generative approach bridges the benefits of
constrained decoding, with those of language model adaptation for sentiment
steering. Training data augmentation with our generations results in both
in-domain and out-of-domain improvements for sentiment classification,
outperforming even manually curated counterfactuals, under select settings. We
further present detailed analyses to show the advantages of
NeuroCounterfactuals over approaches involving simple, minimal edits.
</p></li>
</ul>

<h3>Title: Precisely the Point: Adversarial Augmentations for Faithful and Informative Text Generation. (arXiv:2210.12367v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12367">http://arxiv.org/abs/2210.12367</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12367] Precisely the Point: Adversarial Augmentations for Faithful and Informative Text Generation](http://arxiv.org/abs/2210.12367)</code></li>
<li>Summary: <p>Though model robustness has been extensively studied in language
understanding, the robustness of Seq2Seq generation remains understudied. In
this paper, we conduct the first quantitative analysis on the robustness of
pre-trained Seq2Seq models. We find that even current SOTA pre-trained Seq2Seq
model (BART) is still vulnerable, which leads to significant degeneration in
faithfulness and informativeness for text generation tasks. This motivated us
to further propose a novel adversarial augmentation framework, namely AdvSeq,
for generally improving faithfulness and informativeness of Seq2Seq models via
enhancing their robustness. AdvSeq automatically constructs two types of
adversarial augmentations during training, including implicit adversarial
samples by perturbing word representations and explicit adversarial samples by
word swapping, both of which effectively improve Seq2Seq robustness. Extensive
experiments on three popular text generation tasks demonstrate that AdvSeq
significantly improves both the faithfulness and informativeness of Seq2Seq
generation under both automatic and human evaluation settings.
</p></li>
</ul>

<h3>Title: Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling. (arXiv:2210.12378v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12378">http://arxiv.org/abs/2210.12378</a></li>
<li>Code URL: <a href="https://github.com/vidhishanair/factedit">https://github.com/vidhishanair/factedit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12378] Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling](http://arxiv.org/abs/2210.12378)</code></li>
<li>Summary: <p>Abstractive summarization models often generate inconsistent summaries
containing factual errors or hallucinated content. Recent works focus on
correcting factual errors in generated summaries via post-editing. Such
correction models are trained using adversarial non-factual summaries
constructed using heuristic rules for injecting errors. However, generating
non-factual summaries using heuristics often does not generalize well to actual
model errors. In this work, we propose to generate hard, representative
synthetic examples of non-factual summaries through infilling language models.
With this data, we train a more robust fact-correction model to post-edit the
summaries to improve factual consistency. Through quantitative and qualitative
experiments on two popular summarization datasets -- CNN/DM and XSum -- we show
that our approach vastly outperforms prior methods in correcting erroneous
summaries. Our model -- FactEdit -- improves factuality scores by over ~11
points on CNN/DM and over ~31 points on XSum on average across multiple
summarization models, producing more factual summaries while maintaining
competitive summarization quality.
</p></li>
</ul>

<h3>Title: MetaASSIST: Robust Dialogue State Tracking with Meta Learning. (arXiv:2210.12397v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12397">http://arxiv.org/abs/2210.12397</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12397] MetaASSIST: Robust Dialogue State Tracking with Meta Learning](http://arxiv.org/abs/2210.12397)</code></li>
<li>Summary: <p>Existing dialogue datasets contain lots of noise in their state annotations.
Such noise can hurt model training and ultimately lead to poor generalization
performance. A general framework named ASSIST has recently been proposed to
train robust dialogue state tracking (DST) models. It introduces an auxiliary
model to generate pseudo labels for the noisy training set. These pseudo labels
are combined with vanilla labels by a common fixed weighting parameter to train
the primary DST model. Notwithstanding the improvements of ASSIST on DST,
tuning the weighting parameter is challenging. Moreover, a single parameter
shared by all slots and all instances may be suboptimal. To overcome these
limitations, we propose a meta learning-based framework MetaASSIST to
adaptively learn the weighting parameter. Specifically, we propose three
schemes with varying degrees of flexibility, ranging from slot-wise to both
slot-wise and instance-wise, to convert the weighting parameter into learnable
functions. These functions are trained in a meta-learning manner by taking the
validation set as meta data. Experimental results demonstrate that all three
schemes can achieve competitive performance. Most impressively, we achieve a
state-of-the-art joint goal accuracy of 80.10% on MultiWOZ 2.4.
</p></li>
</ul>

<h3>Title: Hard Gate Knowledge Distillation -- Leverage Calibration for Robust and Reliable Language Model. (arXiv:2210.12427v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12427">http://arxiv.org/abs/2210.12427</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12427] Hard Gate Knowledge Distillation -- Leverage Calibration for Robust and Reliable Language Model](http://arxiv.org/abs/2210.12427)</code></li>
<li>Summary: <p>In knowledge distillation, a student model is trained with supervisions from
both knowledge from a teacher and observations drawn from a training data
distribution. Knowledge of a teacher is considered a subject that holds
inter-class relations which send a meaningful supervision to a student; hence,
much effort has been put to find such knowledge to be distilled. In this paper,
we explore a question that has been given little attention: "when to distill
such knowledge." The question is answered in our work with the concept of model
calibration; we view a teacher model not only as a source of knowledge but also
as a gauge to detect miscalibration of a student. This simple and yet novel
view leads to a hard gate knowledge distillation scheme that switches between
learning from a teacher model and training data. We verify the gating mechanism
in the context of natural language generation at both the token-level and the
sentence-level. Empirical comparisons with strong baselines show that hard gate
knowledge distillation not only improves model generalization, but also
significantly lowers model calibration error.
</p></li>
</ul>

<h3>Title: Exploring The Landscape of Distributional Robustness for Question Answering Models. (arXiv:2210.12517v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12517">http://arxiv.org/abs/2210.12517</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12517] Exploring The Landscape of Distributional Robustness for Question Answering Models](http://arxiv.org/abs/2210.12517)</code></li>
<li>Summary: <p>We conduct a large empirical evaluation to investigate the landscape of
distributional robustness in question answering. Our investigation spans over
350 models and 16 question answering datasets, including a diverse set of
architectures, model sizes, and adaptation methods (e.g., fine-tuning, adapter
tuning, in-context learning, etc.). We find that, in many cases, model
variations do not affect robustness and in-distribution performance alone
determines out-of-distribution performance. Moreover, our findings indicate
that i) zero-shot and in-context learning methods are more robust to
distribution shifts than fully fine-tuned models; ii) few-shot prompt
fine-tuned models exhibit better robustness than few-shot fine-tuned span
prediction models; iii) parameter-efficient and robustness enhancing training
methods provide no significant robustness improvements. In addition, we
publicly release all evaluations to encourage researchers to further analyze
robustness trends for question answering models.
</p></li>
</ul>

<h3>Title: Just Mix Once: Worst-group Generalization by Group Interpolation. (arXiv:2210.12195v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12195">http://arxiv.org/abs/2210.12195</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12195] Just Mix Once: Worst-group Generalization by Group Interpolation](http://arxiv.org/abs/2210.12195)</code></li>
<li>Summary: <p>Advances in deep learning theory have revealed how average generalization
relies on superficial patterns in data. The consequences are brittle models
with poor performance with shift in group distribution at test time. When group
annotation is available, we can use robust optimization tools to tackle the
problem. However, identification and annotation are time-consuming, especially
on large datasets. A recent line of work leverages self-supervision and
oversampling to improve generalization on minority groups without group
annotation. We propose to unify and generalize these approaches using a
class-conditional variant of mixup tailored for worst-group generalization. Our
approach, Just Mix Once (JM1), interpolates samples during learning, augmenting
the training distribution with a continuous mixture of groups. JM1 is domain
agnostic and computationally efficient, can be used with any level of group
annotation, and performs on par or better than the state-of-the-art on
worst-group generalization. Additionally, we provide a simple explanation of
why JM1 works.
</p></li>
</ul>

<h3>Title: Group Distributionally Robust Reinforcement Learning with Hierarchical Latent Variables. (arXiv:2210.12262v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12262">http://arxiv.org/abs/2210.12262</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12262] Group Distributionally Robust Reinforcement Learning with Hierarchical Latent Variables](http://arxiv.org/abs/2210.12262)</code></li>
<li>Summary: <p>One key challenge for multi-task Reinforcement learning (RL) in practice is
the absence of task indicators. Robust RL has been applied to deal with task
ambiguity, but may result in over-conservative policies. To balance the
worst-case (robustness) and average performance, we propose Group
Distributionally Robust Markov Decision Process (GDR-MDP), a flexible
hierarchical MDP formulation that encodes task groups via a latent mixture
model. GDR-MDP identifies the optimal policy that maximizes the expected return
under the worst-possible qualified belief over task groups within an ambiguity
set. We rigorously show that GDR-MDP's hierarchical structure improves
distributional robustness by adding regularization to the worst possible
outcomes. We then develop deep RL algorithms for GDR-MDP for both value-based
and policy-based RL methods. Extensive experiments on Box2D control tasks,
MuJoCo benchmarks, and Google football platforms show that our algorithms
outperform classic robust training algorithms across diverse environments in
terms of robustness under belief uncertainties. Demos are available on our
project page (\url{https://sites.google.com/view/gdr-rl/home}).
</p></li>
</ul>

<h3>Title: Quantifying Complexity: An Object-Relations Approach to Complex Systems. (arXiv:2210.12347v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12347">http://arxiv.org/abs/2210.12347</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12347] Quantifying Complexity: An Object-Relations Approach to Complex Systems](http://arxiv.org/abs/2210.12347)</code></li>
<li>Summary: <p>The best way to model, understand, and quantify the information contained in
complex systems is an open question in physics, mathematics, and computer
science. The uncertain relationship between entropy and complexity further
complicates this question. With ideas drawn from the object-relations theory of
psychology, this paper develops an object-relations model of complex systems
which generalizes to systems of all types, including mathematical operations,
machines, biological organisms, and social structures. The resulting Complex
Information Entropy (CIE) equation is a robust method to quantify complexity
across various contexts. The paper also describes algorithms to iteratively
update and improve approximate solutions to the CIE equation, to recursively
infer the composition of complex systems, and to discover the connections among
objects across different lengthscales and timescales. Applications are
discussed in the fields of engineering design, atomic and molecular physics,
chemistry, materials science, neuroscience, psychology, sociology, ecology,
economics, and medicine.
</p></li>
</ul>

<h3>Title: torchode: A Parallel ODE Solver for PyTorch. (arXiv:2210.12375v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12375">http://arxiv.org/abs/2210.12375</a></li>
<li>Code URL: <a href="https://github.com/martenlienen/torchode">https://github.com/martenlienen/torchode</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12375] torchode: A Parallel ODE Solver for PyTorch](http://arxiv.org/abs/2210.12375)</code></li>
<li>Summary: <p>We introduce an ODE solver for the PyTorch ecosystem that can solve multiple
ODEs in parallel independently from each other while achieving significant
performance gains. Our implementation tracks each ODE's progress separately and
is carefully optimized for GPUs and compatibility with PyTorch's JIT compiler.
Its design lets researchers easily augment any aspect of the solver and collect
and analyze internal solver statistics. In our experiments, our implementation
is up to 4.3 times faster per step than other ODE solvers and it is robust
against within-batch interactions that lead other solvers to take up to 4 times
as many steps.
</p></li>
</ul>

<h3>Title: On-Demand Sampling: Learning Optimally from Multiple Distributions. (arXiv:2210.12529v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12529">http://arxiv.org/abs/2210.12529</a></li>
<li>Code URL: <a href="https://github.com/ericzhao28/multidistributionlearning">https://github.com/ericzhao28/multidistributionlearning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12529] On-Demand Sampling: Learning Optimally from Multiple Distributions](http://arxiv.org/abs/2210.12529)</code></li>
<li>Summary: <p>Social and real-world considerations such as robustness, fairness, social
welfare and multi-agent tradeoffs have given rise to multi-distribution
learning paradigms, such as collaborative, group distributionally robust, and
fair federated learning. In each of these settings, a learner seeks to minimize
its worst-case loss over a set of $n$ predefined distributions, while using as
few samples as possible. In this paper, we establish the optimal sample
complexity of these learning paradigms and give algorithms that meet this
sample complexity. Importantly, our sample complexity bounds exceed that of the
sample complexity of learning a single distribution only by an additive factor
of $n \log(n) / \epsilon^2$. These improve upon the best known sample
complexity of agnostic federated learning by Mohri et al. by a multiplicative
factor of $n$, the sample complexity of collaborative learning by Nguyen and
Zakynthinou by a multiplicative factor $\log n / \epsilon^3$, and give the
first sample complexity bounds for the group DRO objective of Sagawa et al. To
achieve optimal sample complexity, our algorithms learn to sample and learn
from distributions on demand. Our algorithm design and analysis is enabled by
our extensions of stochastic optimization techniques for solving stochastic
zero-sum games. In particular, we contribute variants of Stochastic Mirror
Descent that can trade off between players' access to cheap one-off samples or
more expensive reusable ones.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: AI-based Arabic Language and Speech Tutor. (arXiv:2210.12346v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12346">http://arxiv.org/abs/2210.12346</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12346] AI-based Arabic Language and Speech Tutor](http://arxiv.org/abs/2210.12346)</code></li>
<li>Summary: <p>In the past decade, we have observed a growing interest in using technologies
such as artificial intelligence (AI), machine learning, and chatbots to provide
assistance to language learners, especially in second language learning. By
using AI and natural language processing (NLP) and chatbots, we can create an
intelligent self-learning environment that goes beyond multiple-choice
questions and/or fill in the blank exercises. In addition, NLP allows for
learning to be adaptive in that it offers more than an indication that an error
has occurred. It also provides a description of the error, uses linguistic
analysis to isolate the source of the error, and then suggests additional
drills to achieve optimal individualized learning outcomes. In this paper, we
present our approach for developing an Artificial Intelligence-based Arabic
Language and Speech Tutor (AI-ALST) for teaching the Moroccan Arabic dialect.
The AI-ALST system is an intelligent tutor that provides analysis and
assessment of students learning the Moroccan dialect at University of Arizona
(UA). The AI-ALST provides a self-learned environment to practice each lesson
for pronunciation training. In this paper, we present our initial experimental
evaluation of the AI-ALST that is based on MFCC (Mel frequency cepstrum
coefficient) feature extraction, bidirectional LSTM (Long Short-Term Memory),
attention mechanism, and a cost-based strategy for dealing with class-imbalance
learning. We evaluated our tutor on the word pronunciation of lesson 1 of the
Moroccan Arabic dialect class. The experimental results show that the AI-ALST
can effectively and successfully detect pronunciation errors and evaluate its
performance by using F_1-score, accuracy, precision, and recall.
</p></li>
</ul>

<h3>Title: PcMSP: A Dataset for Scientific Action Graphs Extraction from Polycrystalline Materials Synthesis Procedure Text. (arXiv:2210.12401v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12401">http://arxiv.org/abs/2210.12401</a></li>
<li>Code URL: <a href="https://github.com/xianjun-yang/pcmsp">https://github.com/xianjun-yang/pcmsp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12401] PcMSP: A Dataset for Scientific Action Graphs Extraction from Polycrystalline Materials Synthesis Procedure Text](http://arxiv.org/abs/2210.12401)</code></li>
<li>Summary: <p>Scientific action graphs extraction from materials synthesis procedures is
important for reproducible research, machine automation, and material
prediction. But the lack of annotated data has hindered progress in this field.
We demonstrate an effort to annotate Polycrystalline Materials Synthesis
Procedures (PcMSP) from 305 open access scientific articles for the
construction of synthesis action graphs. This is a new dataset for material
science information extraction that simultaneously contains the synthesis
sentences extracted from the experimental paragraphs, as well as the entity
mentions and intra-sentence relations. A two-step human annotation and
inter-annotator agreement study guarantee the high quality of the PcMSP corpus.
We introduce four natural language processing tasks: sentence classification,
named entity recognition, relation classification, and joint extraction of
entities and relations. Comprehensive experiments validate the effectiveness of
several state-of-the-art models for these challenges while leaving large space
for improvement. We also perform the error analysis and point out some unique
challenges that require further investigation. We will release our annotation
scheme, the corpus, and codes to the research community to alleviate the
scarcity of labeled data in this domain.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Calibration and Evaluation of Binary Classifiers. (arXiv:2210.12526v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12526">http://arxiv.org/abs/2210.12526</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12526] Federated Calibration and Evaluation of Binary Classifiers](http://arxiv.org/abs/2210.12526)</code></li>
<li>Summary: <p>We address two major obstacles to practical use of supervised classifiers on
distributed private data. Whether a classifier was trained by a federation of
cooperating clients or trained centrally out of distribution, (1) the output
scores must be calibrated, and (2) performance metrics must be evaluated -- all
without assembling labels in one place. In particular, we show how to perform
calibration and compute precision, recall, accuracy and ROC-AUC in the
federated setting under three privacy models (i) secure aggregation, (ii)
distributed differential privacy, (iii) local differential privacy. Our
theorems and experiments clarify tradeoffs between privacy, accuracy, and data
efficiency. They also help decide whether a given application has sufficient
data to support federated calibration and evaluation.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Policy Optimization with Advantage Regularization for Long-Term Fairness in Decision Systems. (arXiv:2210.12546v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12546">http://arxiv.org/abs/2210.12546</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12546] Policy Optimization with Advantage Regularization for Long-Term Fairness in Decision Systems](http://arxiv.org/abs/2210.12546)</code></li>
<li>Summary: <p>Long-term fairness is an important factor of consideration in designing and
deploying learning-based decision systems in high-stake decision-making
contexts. Recent work has proposed the use of Markov Decision Processes (MDPs)
to formulate decision-making with long-term fairness requirements in
dynamically changing environments, and demonstrated major challenges in
directly deploying heuristic and rule-based policies that worked well in static
environments. We show that policy optimization methods from deep reinforcement
learning can be used to find strictly better decision policies that can often
achieve both higher overall utility and less violation of the fairness
requirements, compared to previously-known strategies. In particular, we
propose new methods for imposing fairness requirements in policy optimization
by regularizing the advantage evaluation of different actions. Our proposed
methods make it easy to impose fairness constraints without reward engineering
or sacrificing training efficiency. We perform detailed analyses in three
established case studies, including attention allocation in incident
monitoring, bank loan approval, and vaccine distribution in population
networks.
</p></li>
</ul>

<h3>Title: Abstract Interpretation-Based Feature Importance for SVMs. (arXiv:2210.12456v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12456">http://arxiv.org/abs/2210.12456</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12456] Abstract Interpretation-Based Feature Importance for SVMs](http://arxiv.org/abs/2210.12456)</code></li>
<li>Summary: <p>We propose a symbolic representation for support vector machines (SVMs) by
means of abstract interpretation, a well-known and successful technique for
designing and implementing static program analyses. We leverage this
abstraction in two ways: (1) to enhance the interpretability of SVMs by
deriving a novel feature importance measure, called abstract feature importance
(AFI), that does not depend in any way on a given dataset of the accuracy of
the SVM and is very fast to compute, and (2) for verifying stability, notably
individual fairness, of SVMs and producing concrete counterexamples when the
verification fails. We implemented our approach and we empirically demonstrated
its effectiveness on SVMs based on linear and non-linear (polynomial and radial
basis function) kernels. Our experimental results show that, independently of
the accuracy of the SVM, our AFI measure correlates much more strongly with the
stability of the SVM to feature perturbations than feature importance measures
widely available in machine learning software such as permutation feature
importance. It thus gives better insight into the trustworthiness of SVMs.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Probing with Noise: Unpicking the Warp and Weft of Embeddings. (arXiv:2210.12206v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12206">http://arxiv.org/abs/2210.12206</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12206] Probing with Noise: Unpicking the Warp and Weft of Embeddings](http://arxiv.org/abs/2210.12206)</code></li>
<li>Summary: <p>Improving our understanding of how information is encoded in vector space can
yield valuable interpretability insights. Alongside vector dimensions, we argue
that it is possible for the vector norm to also carry linguistic information.
We develop a method to test this: an extension of the probing framework which
allows for relative intrinsic interpretations of probing results. It relies on
introducing noise that ablates information encoded in embeddings, grounded in
random baselines and confidence intervals. We apply the method to
well-established probing tasks and find evidence that confirms the existence of
separate information containers in English GloVe and BERT embeddings. Our
correlation analysis aligns with the experimental findings that different
encoders use the norm to encode different kinds of information: GloVe stores
syntactic and sentence length information in the vector norm, while BERT uses
it to encode contextual incongruity.
</p></li>
</ul>

<h3>Title: Towards Efficient Dialogue Pre-training with Transferable and Interpretable Latent Structure. (arXiv:2210.12461v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.12461">http://arxiv.org/abs/2210.12461</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.12461] Towards Efficient Dialogue Pre-training with Transferable and Interpretable Latent Structure](http://arxiv.org/abs/2210.12461)</code></li>
<li>Summary: <p>With the availability of massive general-domain dialogue data, pre-trained
dialogue generation appears to be super appealing to transfer knowledge from
the general domain to downstream applications. In most existing work, such
transferable ability is mainly obtained by fitting a large model with hundreds
of millions of parameters on massive data in an exhaustive way, leading to
inefficient running and poor interpretability. This paper proposes a novel
dialogue generation model with a latent structure that is easily transferable
from the general domain to downstream tasks in a lightweight and transparent
way. Experiments on two benchmarks validate the effectiveness of the proposed
model. Thanks to the transferable latent structure, our model is able to yield
better dialogue responses than four strong baselines in terms of both automatic
and human evaluations, and our model with about 22% parameters particularly
delivers a 5x speedup in running time compared with the strongest baseline.
Moreover, the proposed model is explainable by interpreting the discrete latent
variables.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
