<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-02-19</h1>
<h3>Title: Mining Social Determinants of Health for Heart Failure Patient 30-Day Readmission via Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Mingchen Shao, Youjeong Kang, Xiao Hu, Hyunjung Gloria Kwak, Carl Yang, Jiaying Lu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12158">https://arxiv.org/abs/2502.12158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12158">https://arxiv.org/pdf/2502.12158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12158]] Mining Social Determinants of Health for Heart Failure Patient 30-Day Readmission via Large Language Model(https://arxiv.org/abs/2502.12158)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Heart Failure (HF) affects millions of Americans and leads to high readmission rates, posing significant healthcare challenges. While Social Determinants of Health (SDOH) such as socioeconomic status and housing stability play critical roles in health outcomes, they are often underrepresented in structured EHRs and hidden in unstructured clinical notes. This study leverages advanced large language models (LLMs) to extract SDOHs from clinical text and uses logistic regression to analyze their association with HF readmissions. By identifying key SDOHs (e.g. tobacco usage, limited transportation) linked to readmission risk, this work also offers actionable insights for reducing readmissions and improving patient care.</li>
</ul>

<h3>Title: MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections</h3>
<ul>
<li><strong>Authors: </strong>Da Xiao, Qingye Meng, Shengping Li, Xingyuan Yuan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12170">https://arxiv.org/abs/2502.12170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12170">https://arxiv.org/pdf/2502.12170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12170]] MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections(https://arxiv.org/abs/2502.12170)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at this https URL .</li>
</ul>

<h3>Title: GoRA: Gradient-driven Adaptive Low Rank Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Haonan He, Peng Ye, Yuchen Ren, Yuan Yuan, Lei Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12171">https://arxiv.org/abs/2502.12171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12171">https://arxiv.org/pdf/2502.12171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12171]] GoRA: Gradient-driven Adaptive Low Rank Adaptation(https://arxiv.org/abs/2502.12171)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) is a crucial method for efficiently fine-tuning pretrained large language models (LLMs), with its performance largely influenced by two key factors: rank and initialization strategy. Numerous LoRA variants have been proposed to enhance its performance by addressing these factors. However, these variants often compromise LoRA's usability or efficiency. In this paper, we analyze the fundamental limitations of existing methods and introduce a novel approach, GoRA (Gradient-driven Adaptive Low Rank Adaptation), which adaptively assigns ranks and initializes weights for low-rank adapters simultaneously based on gradient information. Extensive experimental results demonstrate that GoRA significantly improves performance while preserving the high usability and efficiency of LoRA. On the T5 model fine-tuned for the GLUE benchmark, GoRA achieves a 5.88-point improvement over LoRA and slightly surpasses full fine-tuning. Similarly, on the Llama3.1-8B-Base model fine-tuned for GSM8k tasks, GoRA outperforms LoRA with a 5.13-point improvement and exceeds full fine-tuning in high-rank settings by a margin of 2.05 points.</li>
</ul>

<h3>Title: Ten Challenging Problems in Federated Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Tao Fan, Hanlin Gu, Xuemei Cao, Chee Seng Chan, Qian Chen, Yiqiang Chen, Yihui Feng, Yang Gu, Jiaxiang Geng, Bing Luo, Shuoling Liu, Win Kent Ong, Chao Ren, Jiaqi Shao, Chuan Sun, Xiaoli Tang, Hong Xi Tae, Yongxin Tong, Shuyue Wei, Fan Wu, Wei Xi, Mingcong Xu, He Yang, Xin Yang, Jiangpeng Yan, Hao Yu, Han Yu, Teng Zhang, Yifei Zhang, Xiaojin Zhang, Zhenzhe Zheng, Lixin Fan, Qiang Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12176">https://arxiv.org/abs/2502.12176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12176">https://arxiv.org/pdf/2502.12176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12176]] Ten Challenging Problems in Federated Foundation Models(https://arxiv.org/abs/2502.12176)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, defense, attack, robust, federate, watermark</a></li>
<li><strong>Abstract: </strong>Federated Foundation Models (FedFMs) represent a distributed learning paradigm that fuses general competences of foundation models as well as privacy-preserving capabilities of federated learning. This combination allows the large foundation models and the small local domain models at the remote clients to learn from each other in a teacher-student learning setting. This paper provides a comprehensive summary of the ten challenging problems inherent in FedFMs, encompassing foundational theory, utilization of private data, continual learning, unlearning, Non-IID and graph data, bidirectional knowledge transfer, incentive mechanism design, game mechanism design, model watermarking, and efficiency. The ten challenging problems manifest in five pivotal aspects: ``Foundational Theory," which aims to establish a coherent and unifying theoretical framework for FedFMs. ``Data," addressing the difficulties in leveraging domain-specific knowledge from private data while maintaining privacy; ``Heterogeneity," examining variations in data, model, and computational resources across clients; ``Security and Privacy," focusing on defenses against malicious attacks and model theft; and ``Efficiency," highlighting the need for improvements in training, communication, and parameter efficiency. For each problem, we offer a clear mathematical definition on the objective function, analyze existing methods, and discuss the key challenges and potential solutions. This in-depth exploration aims to advance the theoretical foundations of FedFMs, guide practical implementations, and inspire future research to overcome these obstacles, thereby enabling the robust, efficient, and privacy-preserving FedFMs in various real-world applications.</li>
</ul>

<h3>Title: Direct Preference Optimization-Enhanced Multi-Guided Diffusion Model for Traffic Scenario Generation</h3>
<ul>
<li><strong>Authors: </strong>Seungjun Yu, Kisung Kim, Daejung Kim, Haewook Han, Jinhan Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12178">https://arxiv.org/abs/2502.12178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12178">https://arxiv.org/pdf/2502.12178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12178]] Direct Preference Optimization-Enhanced Multi-Guided Diffusion Model for Traffic Scenario Generation(https://arxiv.org/abs/2502.12178)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based models are recognized for their effectiveness in using real-world driving data to generate realistic and diverse traffic scenarios. These models employ guided sampling to incorporate specific traffic preferences and enhance scenario realism. However, guiding the sampling process to conform to traffic rules and preferences can result in deviations from real-world traffic priors and potentially leading to unrealistic behaviors. To address this challenge, we introduce a multi-guided diffusion model that utilizes a novel training strategy to closely adhere to traffic priors, even when employing various combinations of guides. This model adopts a multi-task learning framework, enabling a single diffusion model to process various guide inputs. For increased guided sampling precision, our model is fine-tuned using the Direct Preference Optimization (DPO) algorithm. This algorithm optimizes preferences based on guide scores, effectively navigating the complexities and challenges associated with the expensive and often non-differentiable gradient calculations during the guided sampling fine-tuning process. Evaluated using the nuScenes dataset our model provides a strong baseline for balancing realism, diversity and controllability in the traffic scenario generation.</li>
</ul>

<h3>Title: Identifiable Steering via Sparse Autoencoding of Multi-Concept Shifts</h3>
<ul>
<li><strong>Authors: </strong>Shruti Joshi, Andrea Dittadi, Sébastien Lachapelle, Dhanya Sridhar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12179">https://arxiv.org/abs/2502.12179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12179">https://arxiv.org/pdf/2502.12179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12179]] Identifiable Steering via Sparse Autoencoding of Multi-Concept Shifts(https://arxiv.org/abs/2502.12179)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Steering methods manipulate the representations of large language models (LLMs) to induce responses that have desired properties, e.g., truthfulness, offering a promising approach for LLM alignment without the need for fine-tuning. Traditionally, steering has relied on supervision, such as from contrastive pairs of prompts that vary in a single target concept, which is costly to obtain and limits the speed of steering research. An appealing alternative is to use unsupervised approaches such as sparse autoencoders (SAEs) to map LLM embeddings to sparse representations that capture human-interpretable concepts. However, without further assumptions, SAEs may not be identifiable: they could learn latent dimensions that entangle multiple concepts, leading to unintentional steering of unrelated properties. We introduce Sparse Shift Autoencoders (SSAEs) that instead map the differences between embeddings to sparse representations. Crucially, we show that SSAEs are identifiable from paired observations that vary in \textit{multiple unknown concepts}, leading to accurate steering of single concepts without the need for supervision. We empirically demonstrate accurate steering across semi-synthetic and real-world language datasets using Llama-3.1 embeddings.</li>
</ul>

<h3>Title: Leveraging large language models for structured information extraction from pathology reports</h3>
<ul>
<li><strong>Authors: </strong>Jeya Balaji Balasubramanian, Daniel Adams, Ioannis Roxanis, Amy Berrington de Gonzalez, Penny Coulson, Jonas S. Almeida, Montserrat García-Closas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12183">https://arxiv.org/abs/2502.12183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12183">https://arxiv.org/pdf/2502.12183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12183]] Leveraging large language models for structured information extraction from pathology reports(https://arxiv.org/abs/2502.12183)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Background: Structured information extraction from unstructured histopathology reports facilitates data accessibility for clinical research. Manual extraction by experts is time-consuming and expensive, limiting scalability. Large language models (LLMs) offer efficient automated extraction through zero-shot prompting, requiring only natural language instructions without labeled data or training. We evaluate LLMs' accuracy in extracting structured information from breast cancer histopathology reports, compared to manual extraction by a trained human annotator. Methods: We developed the Medical Report Information Extractor, a web application leveraging LLMs for automated extraction. We developed a gold standard extraction dataset to evaluate the human annotator alongside five LLMs including GPT-4o, a leading proprietary model, and the Llama 3 model family, which allows self-hosting for data privacy. Our assessment involved 111 histopathology reports from the Breast Cancer Now (BCN) Generations Study, extracting 51 pathology features specified in the study's data dictionary. Results: Evaluation against the gold standard dataset showed that both Llama 3.1 405B (94.7% accuracy) and GPT-4o (96.1%) achieved extraction accuracy comparable to the human annotator (95.4%; p = 0.146 and p = 0.106, respectively). While Llama 3.1 70B (91.6%) performed below human accuracy (p <0.001), its reduced computational requirements make it a viable option for self-hosting. Conclusion: We developed an open-source tool for structured information extraction that can be customized by non-programmers using natural language. Its modular design enables reuse for various extraction tasks, producing standardized, structured data from unstructured text reports to facilitate analytics through improved accessibility and interoperability.</li>
</ul>

<h3>Title: Large Language Models for Extrapolative Modeling of Manufacturing Processes</h3>
<ul>
<li><strong>Authors: </strong>Kiarash Naghavi Khanghah, Anandkumar Patel, Rajiv Malhotra, Hongyi Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12185">https://arxiv.org/abs/2502.12185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12185">https://arxiv.org/pdf/2502.12185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12185]] Large Language Models for Extrapolative Modeling of Manufacturing Processes(https://arxiv.org/abs/2502.12185)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Conventional predictive modeling of parametric relationships in manufacturing processes is limited by the subjectivity of human expertise and intuition on the one hand and by the cost and time of experimental data generation on the other hand. This work addresses this issue by establishing a new Large Language Model (LLM) framework. The novelty lies in combining automatic extraction of process-relevant knowledge embedded in the literature with iterative model refinement based on a small amount of experimental data. This approach is evaluated on three distinct manufacturing processes that are based on machining, deformation, and additive principles. The results show that for the same small experimental data budget the models derived by our framework have unexpectedly high extrapolative performance, often surpassing the capabilities of conventional Machine Learning. Further, our approach eliminates manual generation of initial models or expertise-dependent interpretation of the literature. The results also reveal the importance of the nature of the knowledge extracted from the literature and the significance of both the knowledge extraction and model refinement components.</li>
</ul>

<h3>Title: E2CB2former: Effecitve and Explainable Transformer for CB2 Receptor Ligand Activity Prediction</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Xie, Yingrui Ji, Linghuan Zeng, Xi Xiao, Gaofei Chen, Lijing Zhu, Joyanta Jyoti Mondal, Jiansheng Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12186">https://arxiv.org/abs/2502.12186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12186">https://arxiv.org/pdf/2502.12186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12186]] E2CB2former: Effecitve and Explainable Transformer for CB2 Receptor Ligand Activity Prediction(https://arxiv.org/abs/2502.12186)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, generative</a></li>
<li><strong>Abstract: </strong>Accurate prediction of CB2 receptor ligand activity is pivotal for advancing drug discovery targeting this receptor, which is implicated in inflammation, pain management, and neurodegenerative conditions. Although conventional machine learning and deep learning techniques have shown promise, their limited interpretability remains a significant barrier to rational drug design. In this work, we introduce CB2former, a framework that combines a Graph Convolutional Network with a Transformer architecture to predict CB2 receptor ligand activity. By leveraging the Transformer's self attention mechanism alongside the GCN's structural learning capability, CB2former not only enhances predictive performance but also offers insights into the molecular features underlying receptor activity. We benchmark CB2former against diverse baseline models including Random Forest, Support Vector Machine, K Nearest Neighbors, Gradient Boosting, Extreme Gradient Boosting, Multilayer Perceptron, Convolutional Neural Network, and Recurrent Neural Network and demonstrate its superior performance with an R squared of 0.685, an RMSE of 0.675, and an AUC of 0.940. Moreover, attention weight analysis reveals key molecular substructures influencing CB2 receptor activity, underscoring the model's potential as an interpretable AI tool for drug discovery. This ability to pinpoint critical molecular motifs can streamline virtual screening, guide lead optimization, and expedite therapeutic development. Overall, our results showcase the transformative potential of advanced AI approaches exemplified by CB2former in delivering both accurate predictions and actionable molecular insights, thus fostering interdisciplinary collaboration and innovation in drug discovery.</li>
</ul>

<h3>Title: Boosting Generalization in Diffusion-Based Neural Combinatorial Solver via Energy-guided Sampling</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Lei, Kaiwen Zhou, Yinchuan Li, Zhitang Chen, Farzan Farnia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12188">https://arxiv.org/abs/2502.12188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12188">https://arxiv.org/pdf/2502.12188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12188]] Boosting Generalization in Diffusion-Based Neural Combinatorial Solver via Energy-guided Sampling(https://arxiv.org/abs/2502.12188)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based Neural Combinatorial Optimization (NCO) has demonstrated effectiveness in solving NP-complete (NPC) problems by learning discrete diffusion models for solution generation, eliminating hand-crafted domain knowledge. Despite their success, existing NCO methods face significant challenges in both cross-scale and cross-problem generalization, and high training costs compared to traditional solvers. While recent studies have introduced training-free guidance approaches that leverage pre-defined guidance functions for zero-shot conditional generation, such methodologies have not been extensively explored in combinatorial optimization. To bridge this gap, we propose a general energy-guided sampling framework during inference time that enhances both the cross-scale and cross-problem generalization capabilities of diffusion-based NCO solvers without requiring additional training. We provide theoretical analysis that helps understanding the cross-problem transfer capability. Our experimental results demonstrate that a diffusion solver, trained exclusively on the Traveling Salesman Problem (TSP), can achieve competitive zero-shot solution generation on TSP variants, such as Prize Collecting TSP (PCTSP) and the Orienteering Problem (OP), through energy-guided sampling across different problem scales.</li>
</ul>

<h3>Title: GeneralizeFormer: Layer-Adaptive Model Generation across Test-Time Distribution Shifts</h3>
<ul>
<li><strong>Authors: </strong>Sameer Ambekar, Zehao Xiao, Xiantong Zhen, Cees G. M. Snoek</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12195">https://arxiv.org/abs/2502.12195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12195">https://arxiv.org/pdf/2502.12195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12195]] GeneralizeFormer: Layer-Adaptive Model Generation across Test-Time Distribution Shifts(https://arxiv.org/abs/2502.12195)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We consider the problem of test-time domain generalization, where a model is trained on several source domains and adjusted on target domains never seen during training. Different from the common methods that fine-tune the model or adjust the classifier parameters online, we propose to generate multiple layer parameters on the fly during inference by a lightweight meta-learned transformer, which we call \textit{GeneralizeFormer}. The layer-wise parameters are generated per target batch without fine-tuning or online adjustment. By doing so, our method is more effective in dynamic scenarios with multiple target distributions and also avoids forgetting valuable source distribution characteristics. Moreover, by considering layer-wise gradients, the proposed method adapts itself to various distribution shifts. To reduce the computational and time cost, we fix the convolutional parameters while only generating parameters of the Batch Normalization layers and the linear classifier. Experiments on six widely used domain generalization datasets demonstrate the benefits and abilities of the proposed method to efficiently handle various distribution shifts, generalize in dynamic scenarios, and avoid forgetting.</li>
</ul>

<h3>Title: A Closer Look at System Prompt Robustness</h3>
<ul>
<li><strong>Authors: </strong>Norman Mu, Jonathan Lu, Michael Lavery, David Wagner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12197">https://arxiv.org/abs/2502.12197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12197">https://arxiv.org/pdf/2502.12197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12197]] A Closer Look at System Prompt Robustness(https://arxiv.org/abs/2502.12197)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>System prompts have emerged as a critical control surface for specifying the behavior of LLMs in chat and agent settings. Developers depend on system prompts to specify important context, output format, personalities, guardrails, content policies, and safety countermeasures, all of which require models to robustly adhere to the system prompt, especially when facing conflicting or adversarial user inputs. In practice, models often forget to consider relevant guardrails or fail to resolve conflicting demands between the system and the user. In this work, we study various methods for improving system prompt robustness by creating realistic new evaluation and fine-tuning datasets based on prompts collected from from OpenAI's GPT Store and HuggingFace's HuggingChat. Our experiments assessing models with a panel of new and existing benchmarks show that performance can be considerably improved with realistic fine-tuning data, as well as inference-time interventions such as classifier-free guidance. Finally, we analyze the results of recently released reasoning models from OpenAI and DeepSeek, which show exciting but uneven improvements on the benchmarks we study. Overall, current techniques fall short of ensuring system prompt robustness and further study is warranted.</li>
</ul>

<h3>Title: Maximize Your Diffusion: A Study into Reward Maximization and Alignment for Diffusion-based Control</h3>
<ul>
<li><strong>Authors: </strong>Dom Huh, Prasant Mohapatra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12198">https://arxiv.org/abs/2502.12198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12198">https://arxiv.org/pdf/2502.12198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12198]] Maximize Your Diffusion: A Study into Reward Maximization and Alignment for Diffusion-based Control(https://arxiv.org/abs/2502.12198)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based planning, learning, and control methods present a promising branch of powerful and expressive decision-making solutions. Given the growing interest, such methods have undergone numerous refinements over the past years. However, despite these advancements, existing methods are limited in their investigations regarding general methods for reward maximization within the decision-making process. In this work, we study extensions of fine-tuning approaches for control applications. Specifically, we explore extensions and various design choices for four fine-tuning approaches: reward alignment through reinforcement learning, direct preference optimization, supervised fine-tuning, and cascading diffusion. We optimize their usage to merge these independent efforts into one unified paradigm. We show the utility of such propositions in offline RL settings and demonstrate empirical improvements over a rich array of control tasks.</li>
</ul>

<h3>Title: BoT: Breaking Long Thought Processes of o1-like Large Language Models through Backdoor Attack</h3>
<ul>
<li><strong>Authors: </strong>Zihao Zhu, Hongbao Zhang, Mingda Zhang, Ruotong Wang, Guanzong Wu, Ke Xu, Baoyuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12202">https://arxiv.org/abs/2502.12202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12202">https://arxiv.org/pdf/2502.12202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12202]] BoT: Breaking Long Thought Processes of o1-like Large Language Models through Backdoor Attack(https://arxiv.org/abs/2502.12202)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Longer thought, better performance: large language models with deep reasoning capabilities, particularly o1-like models, have demonstrated remarkable performance by generating extensive thought processes during inference. This trade-off reveals a potential vulnerability: adversaries could compromise model performance by forcing immediate responses without thought processes. To this end, in this paper, we introduce a novel attack scenario targeting the long thought processes of o1-like models and propose BoT (Break CoT), which can selectively break intrinsic reasoning mechanisms through backdoor attacks. BoT constructs poisoned datasets with designed triggers and injects backdoor by either supervised fine-tuning or direct preference optimization. When triggered, the model directly generates answers without thought processes, while maintaining normal reasoning capabilities for clean inputs. Extensive experiments on open-source o1-like models, including recent DeepSeek-R1, demonstrate that BoT nearly achieves high attack success rates while maintaining clean accuracy, highlighting the critical safety risk in current models. Furthermore, the relationship between task difficulty and helpfulness reveals a potential application for good, enabling users to customize model behavior based on task complexity. Code is available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: An Interpretable Automated Mechanism Design Framework with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiayuan Liu, Mingyu Guo, Vincent Conitzer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.GT, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12203">https://arxiv.org/abs/2502.12203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12203">https://arxiv.org/pdf/2502.12203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12203]] An Interpretable Automated Mechanism Design Framework with Large Language Models(https://arxiv.org/abs/2502.12203)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative, large language model</a></li>
<li><strong>Abstract: </strong>Mechanism design has long been a cornerstone of economic theory, with traditional approaches relying on mathematical derivations. Recently, automated approaches, including differentiable economics with neural networks, have emerged for designing payments and allocations. While both analytical and automated methods have advanced the field, they each face significant weaknesses: mathematical derivations are not automated and often struggle to scale to complex problems, while automated and especially neural-network-based approaches suffer from limited interpretability. To address these challenges, we introduce a novel framework that reformulates mechanism design as a code generation task. Using large language models (LLMs), we generate heuristic mechanisms described in code and evolve them to optimize over some evaluation metrics while ensuring key design criteria (e.g., strategy-proofness) through a problem-specific fixing process. This fixing process ensures any mechanism violating the design criteria is adjusted to satisfy them, albeit with some trade-offs in performance metrics. These trade-offs are factored in during the LLM-based evolution process. The code generation capabilities of LLMs enable the discovery of novel and interpretable solutions, bridging the symbolic logic of mechanism design and the generative power of modern AI. Through rigorous experimentation, we demonstrate that LLM-generated mechanisms achieve competitive performance while offering greater interpretability compared to previous approaches. Notably, our framework can rediscover existing manually designed mechanisms and provide insights into neural-network based solutions through Programming-by-Example. These results highlight the potential of LLMs to not only automate but also enhance the transparency and scalability of mechanism design, ensuring safe deployment of the mechanisms in society.</li>
</ul>

<h3>Title: PAR-AdvGAN: Improving Adversarial Attack Capability with Progressive Auto-Regression AdvGAN</h3>
<ul>
<li><strong>Authors: </strong>Jiayu Zhang, Zhiyu Zhu, Xinyi Wang, Silin Liao, Zhibo Jin, Flora D. Salim, Huaming Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12207">https://arxiv.org/abs/2502.12207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12207">https://arxiv.org/pdf/2502.12207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12207]] PAR-AdvGAN: Improving Adversarial Attack Capability with Progressive Auto-Regression AdvGAN(https://arxiv.org/abs/2502.12207)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, generative</a></li>
<li><strong>Abstract: </strong>Deep neural networks have demonstrated remarkable performance across various domains. However, they are vulnerable to adversarial examples, which can lead to erroneous predictions. Generative Adversarial Networks (GANs) can leverage the generators and discriminators model to quickly produce high-quality adversarial examples. Since both modules train in a competitive and simultaneous manner, GAN-based algorithms like AdvGAN can generate adversarial examples with better transferability compared to traditional methods. However, the generation of perturbations is usually limited to a single iteration, preventing these examples from fully exploiting the potential of the methods. To tackle this issue, we introduce a novel approach named Progressive Auto-Regression AdvGAN (PAR-AdvGAN). It incorporates an auto-regressive iteration mechanism within a progressive generation network to craft adversarial examples with enhanced attack capability. We thoroughly evaluate our PAR-AdvGAN method with a large-scale experiment, demonstrating its superior performance over various state-of-the-art black-box adversarial attacks, as well as the original this http URL, PAR-AdvGAN significantly accelerates the adversarial example generation, i.e., achieving the speeds of up to 335.5 frames per second on Inception-v3 model, outperforming the gradient-based transferable attack algorithms. Our code is available at: this https URL</li>
</ul>

<h3>Title: Enhancing Frame Detection with Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Papa Abdou Karim Karou Diallo, Amal Zouaq</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12210">https://arxiv.org/abs/2502.12210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12210">https://arxiv.org/pdf/2502.12210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12210]] Enhancing Frame Detection with Retrieval Augmented Generation(https://arxiv.org/abs/2502.12210)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Recent advancements in Natural Language Processing have significantly improved the extraction of structured semantic representations from unstructured text, especially through Frame Semantic Role Labeling (FSRL). Despite this progress, the potential of Retrieval-Augmented Generation (RAG) models for frame detection remains under-explored. In this paper, we present the first RAG-based approach for frame detection called RCIF (Retrieve Candidates and Identify Frames). RCIF is also the first approach to operate without the need for explicit target span and comprises three main stages: (1) generation of frame embeddings from various representations ; (2) retrieval of candidate frames given an input text; and (3) identification of the most suitable frames. We conducted extensive experiments across multiple configurations, including zero-shot, few-shot, and fine-tuning settings. Our results show that our retrieval component significantly reduces the complexity of the task by narrowing the search space thus allowing the frame identifier to refine and complete the set of candidates. Our approach achieves state-of-the-art performance on FrameNet 1.5 and 1.7, demonstrating its robustness in scenarios where only raw text is provided. Furthermore, we leverage the structured representation obtained through this method as a proxy to enhance generalization across lexical variations in the task of translating natural language questions into SPARQL queries.</li>
</ul>

<h3>Title: Zero Token-Driven Deep Thinking in LLMs: Unlocking the Full Potential of Existing Parameters via Cyclic Refinement</h3>
<ul>
<li><strong>Authors: </strong>Guanghao Li, Wenhao Jiang, Li Shen, Ming Tang, Chun Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12214">https://arxiv.org/abs/2502.12214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12214">https://arxiv.org/pdf/2502.12214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12214]] Zero Token-Driven Deep Thinking in LLMs: Unlocking the Full Potential of Existing Parameters via Cyclic Refinement(https://arxiv.org/abs/2502.12214)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Resource limitations often constrain the parameter counts of Large Language Models (LLMs), hindering their performance. While existing methods employ parameter sharing to reuse the same parameter set under fixed budgets, such approaches typically force each layer to assume multiple roles with a predetermined number of iterations, restricting efficiency and adaptability. In this work, we propose the Zero Token Transformer (ZTT), which features a head-tail decoupled parameter cycling method. We disentangle the first (head) and last (tail) layers from parameter cycling and iteratively refine only the intermediate layers. Furthermore, we introduce a Zero-Token Mechanism, an internal architectural component rather than an input token, to guide layer-specific computation. At each cycle, the model retrieves a zero token (with trainable key values) from a Zero-Token Pool, integrating it alongside regular tokens in the attention mechanism. The corresponding attention scores not only reflect each layer's computational importance but also enable dynamic early exits without sacrificing overall model accuracy. Our approach achieves superior performance under tight parameter budgets, effectively reduces computational overhead via early exits, and can be readily applied to fine-tune existing pre-trained models for enhanced efficiency and adaptability.</li>
</ul>

<h3>Title: Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12215">https://arxiv.org/abs/2502.12215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12215">https://arxiv.org/pdf/2502.12215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12215]] Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?(https://arxiv.org/abs/2502.12215)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The advent of test-time scaling in large language models (LLMs), exemplified by OpenAI's o1 series, has advanced reasoning capabilities by scaling computational resource allocation during inference. While successors like QwQ, Deepseek-R1 (R1) and LIMO replicate these advancements, whether these models truly possess test-time scaling capabilities remains underexplored. This study found that longer CoTs of these o1-like models do not consistently enhance accuracy; in fact, correct solutions are often shorter than incorrect ones for the same questions. Further investigation shows this phenomenon is closely related to models' self-revision capabilities - longer CoTs contain more self-revisions, which often lead to performance degradation. We then compare sequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that parallel scaling achieves better coverage and scalability. Based on these insights, we propose Shortest Majority Vote, a method that combines parallel scaling strategies with CoT length characteristics, significantly improving models' test-time scalability compared to conventional majority voting approaches.</li>
</ul>

<h3>Title: Optimal Brain Iterative Merging: Mitigating Interference in LLM Merging</h3>
<ul>
<li><strong>Authors: </strong>Zhixiang Wang, Zhenyu Mao, Yixuan Qiao, Yunfang Wu, Biye Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12217">https://arxiv.org/abs/2502.12217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12217">https://arxiv.org/pdf/2502.12217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12217]] Optimal Brain Iterative Merging: Mitigating Interference in LLM Merging(https://arxiv.org/abs/2502.12217)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive capabilities, but their high computational costs pose challenges for customization. Model merging offers a cost-effective alternative, yet existing methods suffer from interference among parameters, leading to performance degradation. In this work, we propose Optimal Brain Iterative Merging (OBIM), a novel method designed to mitigate both intra-model and inter-model interference. OBIM consists of two key components: (1) A saliency measurement mechanism that evaluates parameter importance based on loss changes induced by individual weight alterations, reducing intra-model interference by preserving only high-saliency parameters. (2) A mutually exclusive iterative merging framework, which incrementally integrates models using a binary mask to avoid direct parameter averaging, thereby mitigating inter-model interference. We validate OBIM through experiments on both Supervised Fine-Tuned (SFT) models and post-pretrained checkpoints. The results show that OBIM significantly outperforms existing merging techniques. Overall, OBIM provides an effective and practical solution for enhancing LLM merging.</li>
</ul>

<h3>Title: GLoT: A Novel Gated-Logarithmic Transformer for Efficient Sign Language Translation</h3>
<ul>
<li><strong>Authors: </strong>Nada Shahin, Leila Ismail</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12223">https://arxiv.org/abs/2502.12223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12223">https://arxiv.org/pdf/2502.12223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12223]] GLoT: A Novel Gated-Logarithmic Transformer for Efficient Sign Language Translation(https://arxiv.org/abs/2502.12223)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Machine Translation has played a critical role in reducing language barriers, but its adaptation for Sign Language Machine Translation (SLMT) has been less explored. Existing works on SLMT mostly use the Transformer neural network which exhibits low performance due to the dynamic nature of the sign language. In this paper, we propose a novel Gated-Logarithmic Transformer (GLoT) that captures the long-term temporal dependencies of the sign language as a time-series data. We perform a comprehensive evaluation of GloT with the transformer and transformer-fusion models as a baseline, for Sign-to-Gloss-to-Text translation. Our results demonstrate that GLoT consistently outperforms the other models across all metrics. These findings underscore its potential to address the communication challenges faced by the Deaf and Hard of Hearing community.</li>
</ul>

<h3>Title: On Creating a Causally Grounded Usable Rating Method for Assessing the Robustness of Foundation Models Supporting Time Series</h3>
<ul>
<li><strong>Authors: </strong>Kausik Lakkaraju, Rachneet Kaur, Parisa Zehtabi, Sunandita Patra, Siva Likitha Valluru, Zhen Zeng, Biplav Srivastava, Marco Valtorta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12226">https://arxiv.org/abs/2502.12226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12226">https://arxiv.org/pdf/2502.12226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12226]] On Creating a Causally Grounded Usable Rating Method for Assessing the Robustness of Foundation Models Supporting Time Series(https://arxiv.org/abs/2502.12226)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Foundation Models (FMs) have improved time series forecasting in various sectors, such as finance, but their vulnerability to input disturbances can hinder their adoption by stakeholders, such as investors and analysts. To address this, we propose a causally grounded rating framework to study the robustness of Foundational Models for Time Series (FMTS) with respect to input perturbations. We evaluate our approach to the stock price prediction problem, a well-studied problem with easily accessible public data, evaluating six state-of-the-art (some multi-modal) FMTS across six prominent stocks spanning three industries. The ratings proposed by our framework effectively assess the robustness of FMTS and also offer actionable insights for model selection and deployment. Within the scope of our study, we find that (1) multi-modal FMTS exhibit better robustness and accuracy compared to their uni-modal versions and, (2) FMTS pre-trained on time series forecasting task exhibit better robustness and forecasting accuracy compared to general-purpose FMTS pre-trained across diverse settings. Further, to validate our framework's usability, we conduct a user study showcasing FMTS prediction errors along with our computed ratings. The study confirmed that our ratings reduced the difficulty for users in comparing the robustness of different systems.</li>
</ul>

<h3>Title: InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context</h3>
<ul>
<li><strong>Authors: </strong>Bryan L. M. de Oliveira, Luana G. B. Martins, Bruno Brandão, Luckeciano C. Melo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12257">https://arxiv.org/abs/2502.12257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12257">https://arxiv.org/pdf/2502.12257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12257]] InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context(https://arxiv.org/abs/2502.12257)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models excel at following explicit instructions, they often struggle with ambiguous or incomplete user requests, defaulting to verbose, generic responses rather than seeking clarification. We introduce InfoQuest, a multi-turn chat benchmark designed to evaluate how dialogue agents handle hidden context in open-ended user requests. The benchmark presents intentionally ambiguous scenarios that require models to engage in information-seeking dialogue through clarifying questions before providing appropriate responses. Our evaluation of both open and closed-source models reveals that while proprietary models generally perform better, all current assistants struggle with effectively gathering critical information, often requiring multiple turns to infer user intent and frequently defaulting to generic responses without proper clarification. We provide a systematic methodology for generating diverse scenarios and evaluating models' information-seeking capabilities, offering insights into the current limitations of language models in handling ambiguous requests through multi-turn interactions.</li>
</ul>

<h3>Title: SmokeNet: Efficient Smoke Segmentation Leveraging Multiscale Convolutions and Multiview Attention Mechanisms</h3>
<ul>
<li><strong>Authors: </strong>Xuesong Liu, Emmett J. Ientilucci</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12258">https://arxiv.org/abs/2502.12258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12258">https://arxiv.org/pdf/2502.12258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12258]] SmokeNet: Efficient Smoke Segmentation Leveraging Multiscale Convolutions and Multiview Attention Mechanisms(https://arxiv.org/abs/2502.12258)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Efficient segmentation of smoke plumes is crucial for environmental monitoring and industrial safety, enabling the detection and mitigation of harmful emissions from activities like quarry blasts and wildfires. Accurate segmentation facilitates environmental impact assessments, timely interventions, and compliance with safety standards. However, existing models often face high computational demands and limited adaptability to diverse smoke appearances, restricting their deployment in resource-constrained environments. To address these issues, we introduce SmokeNet, a novel deep learning architecture that leverages multiscale convolutions and multiview linear attention mechanisms combined with layer-specific loss functions to handle the complex dynamics of diverse smoke plumes, ensuring efficient and accurate segmentation across varied environments. Additionally, we evaluate SmokeNet's performance and versatility using four datasets, including our quarry blast smoke dataset made available to the community. The results demonstrate that SmokeNet maintains a favorable balance between computational efficiency and segmentation accuracy, making it suitable for deployment in environmental monitoring and safety management systems. By contributing a new dataset and offering an efficient segmentation model, SmokeNet advances smoke segmentation capabilities in diverse and challenging environments.</li>
</ul>

<h3>Title: Learning to Reason at the Frontier of Learnability</h3>
<ul>
<li><strong>Authors: </strong>Thomas Foster, Jakob Foerster</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12272">https://arxiv.org/abs/2502.12272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12272">https://arxiv.org/pdf/2502.12272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12272]] Learning to Reason at the Frontier of Learnability(https://arxiv.org/abs/2502.12272)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning is now widely adopted as the final stage of large language model training, especially for reasoning-style tasks such as maths problems. Typically, models attempt each question many times during a single training step and attempt to learn from their successes and failures. However, we demonstrate that throughout training with two popular algorithms (PPO and VinePPO) on two widely used datasets, many questions are either solved by all attempts - meaning they are already learned - or by none - providing no meaningful training signal. To address this, we adapt a method from the reinforcement learning literature - sampling for learnability - and apply it to the reinforcement learning stage of LLM training. Our curriculum prioritises questions with high variance of success, i.e. those where the agent sometimes succeeds, but not always. Our findings demonstrate that this curriculum consistently boosts training performance across multiple algorithms and datasets, paving the way for more efficient and effective reinforcement learning in LLMs.</li>
</ul>

<h3>Title: Healthcare cost prediction for heterogeneous patient profiles using deep learning models with administrative claims data</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Amin Morid, Olivia R. Liu Sheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12277">https://arxiv.org/abs/2502.12277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12277">https://arxiv.org/pdf/2502.12277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12277]] Healthcare cost prediction for heterogeneous patient profiles using deep learning models with administrative claims data(https://arxiv.org/abs/2502.12277)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Problem: How can we design patient cost prediction models that effectively address the challenges of heterogeneity in administrative claims (AC) data to ensure accurate, fair, and generalizable predictions, especially for high-need (HN) patients with complex chronic conditions? Relevance: Accurate and equitable patient cost predictions are vital for developing health management policies and optimizing resource allocation, which can lead to significant cost savings for healthcare payers, including government agencies and private insurers. Addressing disparities in prediction outcomes for HN patients ensures better economic and clinical decision-making, benefiting both patients and payers. Methodology: This study is grounded in socio-technical considerations that emphasize the interplay between technical systems (e.g., deep learning models) and humanistic outcomes (e.g., fairness in healthcare decisions). It incorporates representation learning and entropy measurement to address heterogeneity and complexity in data and patient profiles, particularly for HN patients. We propose a channel-wise deep learning framework that mitigates data heterogeneity by segmenting AC data into separate channels based on types of codes (e.g., diagnosis, procedures) and costs. This approach is paired with a flexible evaluation design that uses multi-channel entropy measurement to assess patient heterogeneity. Results: The proposed channel-wise models reduce prediction errors by 23% compared to single-channel models, leading to 16.4% and 19.3% reductions in overpayments and underpayments, respectively. Notably, the reduction in prediction bias is significantly higher for HN patients, demonstrating effectiveness in handling heterogeneity and complexity in data and patient profiles. This demonstrates the potential for applying channel-wise modeling to domains with similar heterogeneity challenges.</li>
</ul>

<h3>Title: Evaluating Step-by-step Reasoning Traces: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Jinu Lee, Julia Hockenmaier</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12289">https://arxiv.org/abs/2502.12289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12289">https://arxiv.org/pdf/2502.12289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12289]] Evaluating Step-by-step Reasoning Traces: A Survey(https://arxiv.org/abs/2502.12289)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Step-by-step reasoning is widely used to enhance the reasoning ability of large language models (LLMs) in complex problems. Evaluating the quality of reasoning traces is crucial for understanding and improving LLM reasoning. However, the evaluation criteria remain highly unstandardized, leading to fragmented efforts in developing metrics and meta-evaluation benchmarks. To address this gap, this survey provides a comprehensive overview of step-by-step reasoning evaluation, proposing a taxonomy of evaluation criteria with four top-level categories (groundedness, validity, coherence, and utility). We then categorize metrics based on their implementations, survey which metrics are used for assessing each criterion, and explore whether evaluator models can transfer across different criteria. Finally, we identify key directions for future research.</li>
</ul>

<h3>Title: Independence Tests for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sally Zhu, Ahmed Ahmed, Rohith Kuditipudi, Percy Liang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12292">https://arxiv.org/abs/2502.12292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12292">https://arxiv.org/pdf/2502.12292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12292]] Independence Tests for Language Models(https://arxiv.org/abs/2502.12292)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>We consider the following problem: given the weights of two models, can we test whether they were trained independently -- i.e., from independent random initializations? We consider two settings: constrained and unconstrained. In the constrained setting, we make assumptions about model architecture and training and propose a family of statistical tests that yield exact p-values with respect to the null hypothesis that the models are trained from independent random initializations. These p-values are valid regardless of the composition of either model's training data; we compute them by simulating exchangeable copies of each model under our assumptions and comparing various similarity measures of weights and activations between the original two models versus these copies. We report the p-values from these tests on pairs of 21 open-weight models (210 total pairs) and correctly identify all pairs of non-independent models. Our tests remain effective even if one model was fine-tuned for many tokens. In the unconstrained setting, where we make no assumptions about training procedures, can change model architecture, and allow for adversarial evasion attacks, the previous tests no longer work. Instead, we propose a new test which matches hidden activations between two models, and which is robust to adversarial transformations and to changes in model architecture. The test can also do localized testing: identifying specific non-independent components of models. Though we no longer obtain exact p-values from this, empirically we find it behaves as one and reliably identifies non-independent models. Notably, we can use the test to identify specific parts of one model that are derived from another (e.g., how Llama 3.1-8B was pruned to initialize Llama 3.2-3B, or shared layers between Mistral-7B and StripedHyena-7B), and it is even robust to retraining individual layers of either model from scratch.</li>
</ul>

<h3>Title: SMOL: Professionally translated parallel data for 115 under-represented languages</h3>
<ul>
<li><strong>Authors: </strong>Isaac Caswell, Elizabeth Nielsen, Jiaming Luo, Colin Cherry, Geza Kovacs, Hadar Shemtov, Partha Talukdar, Dinesh Tewari, Baba Mamadi Diane, Koulako Moussa Doumbouya, Djibrila Diane, Solo Farabado Cissé</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12301">https://arxiv.org/abs/2502.12301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12301">https://arxiv.org/pdf/2502.12301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12301]] SMOL: Professionally translated parallel data for 115 under-represented languages(https://arxiv.org/abs/2502.12301)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We open-source SMOL (Set of Maximal Overall Leverage), a suite of training data to unlock translation for low-resource languages (LRLs). SMOL has been translated into 115 under-resourced languages, including many for which there exist no previous public resources, for a total of 6.1M translated tokens. SMOL comprises two sub-datasets, each carefully chosen for maximum impact given its size: SMOL-Sent, a set of sentences chosen for broad unique token coverage, and SMOL-Doc, a document-level source focusing on a broad topic coverage. They join the already released GATITOS for a trifecta of paragraph, sentence, and token-level content. We demonstrate that using SMOL to prompt or fine-tune Large Language Models yields robust ChrF improvements. In addition to translation, we provide factuality ratings and rationales for all documents in SMOL-Doc, yielding the first factuality datasets for most of these languages.</li>
</ul>

<h3>Title: Chaotic Map based Compression Approach to Classification</h3>
<ul>
<li><strong>Authors: </strong>Harikrishnan N B, Anuja Vats, Nithin Nagaraj, Marius Pedersen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12302">https://arxiv.org/abs/2502.12302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12302">https://arxiv.org/pdf/2502.12302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12302]] Chaotic Map based Compression Approach to Classification(https://arxiv.org/abs/2502.12302)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Modern machine learning approaches often prioritize performance at the cost of increased complexity, computational demands, and reduced interpretability. This paper introduces a novel framework that challenges this trend by reinterpreting learning from an information-theoretic perspective, viewing it as a search for encoding schemes that capture intrinsic data structures through compact representations. Rather than following the conventional approach of fitting data to complex models, we propose a fundamentally different method that maps data to intervals of initial conditions in a dynamical system. Our GLS (Generalized Lüroth Series) coding compression classifier employs skew tent maps - a class of chaotic maps - both for encoding data into initial conditions and for subsequent recovery. The effectiveness of this simple framework is noteworthy, with performance closely approaching that of well-established machine learning methods. On the breast cancer dataset, our approach achieves 92.98\% accuracy, comparable to Naive Bayes at 94.74\%. While these results do not exceed state-of-the-art performance, the significance of our contribution lies not in outperforming existing methods but in demonstrating that a fundamentally simpler, more interpretable approach can achieve competitive results.</li>
</ul>

<h3>Title: From Gaming to Research: GTA V for Synthetic Data Generation for Robotics and Navigations</h3>
<ul>
<li><strong>Authors: </strong>Matteo Scucchia, Matteo Ferrara, Davide Maltoni</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12303">https://arxiv.org/abs/2502.12303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12303">https://arxiv.org/pdf/2502.12303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12303]] From Gaming to Research: GTA V for Synthetic Data Generation for Robotics and Navigations(https://arxiv.org/abs/2502.12303)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In computer vision, the development of robust algorithms capable of generalizing effectively in real-world scenarios more and more often requires large-scale datasets collected under diverse environmental conditions. However, acquiring such datasets is time-consuming, costly, and sometimes unfeasible. To address these limitations, the use of synthetic data has gained attention as a viable alternative, allowing researchers to generate vast amounts of data while simulating various environmental contexts in a controlled setting. In this study, we investigate the use of synthetic data in robotics and navigation, specifically focusing on Simultaneous Localization and Mapping (SLAM) and Visual Place Recognition (VPR). In particular, we introduce a synthetic dataset created using the virtual environment of the video game Grand Theft Auto V (GTA V), along with an algorithm designed to generate a VPR dataset, without human supervision. Through a series of experiments centered on SLAM and VPR, we demonstrate that synthetic data derived from GTA V are qualitatively comparable to real-world data. Furthermore, these synthetic data can complement or even substitute real-world data in these applications. This study sets the stage for the creation of large-scale synthetic datasets, offering a cost-effective and scalable solution for future research and development.</li>
</ul>

<h3>Title: Warmup Generations: A Task-Agnostic Approach for Guiding Sequence-to-Sequence Learning with Unsupervised Initial State Generation</h3>
<ul>
<li><strong>Authors: </strong>Senyu Li, Zipeng Sun, Jiayi Wang, Xue Liu, Pontus Stenetorp, Siva Reddy, David Ifeoluwa Adelani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12304">https://arxiv.org/abs/2502.12304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12304">https://arxiv.org/pdf/2502.12304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12304]] Warmup Generations: A Task-Agnostic Approach for Guiding Sequence-to-Sequence Learning with Unsupervised Initial State Generation(https://arxiv.org/abs/2502.12304)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Traditional supervised fine-tuning (SFT) strategies for sequence-to-sequence tasks often train models to directly generate the target output. Recent work has shown that guiding models with intermediate steps, such as keywords, outlines, or reasoning chains, can significantly improve performance, coherence, and interpretability. However, these methods often depend on predefined intermediate formats and annotated data, limiting their scalability and generalizability. In this work, we introduce a task-agnostic framework that enables models to generate intermediate "warmup" sequences. These warmup sequences, serving as an initial state for subsequent generation, are optimized to enhance the probability of generating the target sequence without relying on external supervision or human-designed structures. Drawing inspiration from reinforcement learning principles, our method iteratively refines these intermediate steps to maximize their contribution to the final output, similar to reward-driven optimization in reinforcement learning with human feedback. Experimental results across tasks such as translation, summarization, and multi-choice question answering for logical reasoning show that our approach outperforms traditional SFT methods, and offers a scalable and flexible solution for sequence-to-sequence tasks.</li>
</ul>

<h3>Title: VIC: Evasive Video Game Cheating via Virtual Machine Introspection</h3>
<ul>
<li><strong>Authors: </strong>Panicos Karkallis, Jorge Blasco Alis</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12322">https://arxiv.org/abs/2502.12322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12322">https://arxiv.org/pdf/2502.12322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12322]] VIC: Evasive Video Game Cheating via Virtual Machine Introspection(https://arxiv.org/abs/2502.12322)</code><input type="text"></li>
<li><strong>Keywords: </strong>steal, fair</a></li>
<li><strong>Abstract: </strong>Video game cheats modify a video game behaviour to give unfair advantages to some players while bypassing the methods game developers use to detect them. This destroys the experience of online gaming and can result in financial losses for game developers. In this work, we present a new type of game cheat, Virtual machine Introspection Cheat (VIC), that takes advantage of virtual machines to stealthy execute game cheats. VIC employees a hypervisor with introspection enabled to lower the bar of cheating against legacy and modern anti-cheat systems. We demonstrate the feasibility and stealthiness of VIC against three popular games (Fortnite, BlackSquad and Team Fortress 2) that include five different anti-cheats. In particular, we use VIC to implement a cheat radar, a wall-hack cheat and a trigger-bot. To support our claim that this type of cheats can be effectively used, we present the performance impact VICs have on gameplay by monitoring the frames per second (fps) while the cheats are activated. Our experimentation also shows how these cheats are currently undetected by the most popular anti-cheat systems, enabling a new paradigm that can take advantage of cloud infrastructure to offer cheating-as-a-service.</li>
</ul>

<h3>Title: From Dense to Dynamic: Token-Difficulty Driven MoEfication of Pre-Trained LLMs</h3>
<ul>
<li><strong>Authors: </strong>Kumari Nishu, Sachin Mehta, Samira Abnar, Mehrdad Farajtabar, Maxwell Horton, Mahyar Najibi, Moin Nabi, Minsik Cho, Devang Naik</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12325">https://arxiv.org/abs/2502.12325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12325">https://arxiv.org/pdf/2502.12325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12325]] From Dense to Dynamic: Token-Difficulty Driven MoEfication of Pre-Trained LLMs(https://arxiv.org/abs/2502.12325)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Training large language models (LLMs) for different inference constraints is computationally expensive, limiting control over efficiency-accuracy trade-offs. Moreover, once trained, these models typically process tokens uniformly, regardless of their complexity, leading to static and inflexible behavior. In this paper, we introduce a post-training optimization framework, DynaMoE, that adapts a pre-trained dense LLM to a token-difficulty-driven Mixture-of-Experts model with minimal fine-tuning cost. This adaptation makes the model dynamic, with sensitivity control to customize the balance between efficiency and accuracy. DynaMoE features a token-difficulty-aware router that predicts the difficulty of tokens and directs them to the appropriate sub-networks or experts, enabling larger experts to handle more complex tokens and smaller experts to process simpler ones. Our experiments demonstrate that DynaMoE can generate a range of adaptive model variants of the existing trained LLM with a single fine-tuning step, utilizing only $10B$ tokens, a minimal cost compared to the base model's training. Each variant offers distinct trade-offs between accuracy and performance. Compared to the baseline post-training optimization framework, Flextron, our method achieves similar aggregated accuracy across downstream tasks, despite using only $\frac{1}{9}\text{th}$ of their fine-tuning cost.</li>
</ul>

<h3>Title: Understanding Silent Data Corruption in LLM Training</h3>
<ul>
<li><strong>Authors: </strong>Jeffrey Ma, Hengzhi Pei, Leonard Lausen, George Karypis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12340">https://arxiv.org/abs/2502.12340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12340">https://arxiv.org/pdf/2502.12340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12340]] Understanding Silent Data Corruption in LLM Training(https://arxiv.org/abs/2502.12340)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As the scale of training large language models (LLMs) increases, one emergent failure is silent data corruption (SDC), where hardware produces incorrect computations without explicit failure signals. In this work, we are the first to investigate the impact of real-world SDCs on LLM training by comparing model training between healthy production nodes and unhealthy nodes exhibiting SDCs. With the help from a cloud computing platform, we access the unhealthy nodes that were swept out from production by automated fleet management. Using deterministic execution via XLA compiler and our proposed synchronization mechanisms, we isolate and analyze the impact of SDC errors on these nodes at three levels: at each submodule computation, at a single optimizer step, and at a training period. Our results reveal that the impact of SDCs on computation varies on different unhealthy nodes. Although in most cases the perturbations from SDCs on submodule computation and gradients are relatively small, SDCs can lead models to converge to different optima with different weights and even cause spikes in the training loss. Our analysis sheds light on further understanding and mitigating the impact of SDCs.</li>
</ul>

<h3>Title: QuZO: Quantized Zeroth-Order Fine-Tuning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Zhou, Yifan Yang, Kai Zhen, Ziyue Liu, Yequan Zhao, Ershad Banijamali, Athanasios Mouchtaris, Ngai Wong, Zheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12346">https://arxiv.org/abs/2502.12346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12346">https://arxiv.org/pdf/2502.12346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12346]] QuZO: Quantized Zeroth-Order Fine-Tuning for Large Language Models(https://arxiv.org/abs/2502.12346)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Language Models (LLMs) are often quantized to lower precision to reduce the memory cost and latency in inference. However, quantization often degrades model performance, thus fine-tuning is required for various down-stream tasks. Traditional fine-tuning methods such as stochastic gradient descent and Adam optimization require backpropagation, which are error-prone in the low-precision settings. To overcome these limitations, we propose the Quantized Zeroth-Order (QuZO) framework, specifically designed for fine-tuning LLMs through low-precision (e.g., 4- or 8-bit) forward passes. Our method can avoid the error-prone low-precision straight-through estimator, and utilizes optimized stochastic rounding to mitigate the increased bias. QuZO simplifies the training process, while achieving results comparable to first-order methods in ${\rm FP}8$ and superior accuracy in ${\rm INT}8$ and ${\rm INT}4$ training. Experiments demonstrate that low-bit training QuZO achieves performance comparable to MeZO optimization on GLUE, Multi-Choice, and Generation tasks, while reducing memory cost by $2.94 \times$ in LLaMA2-7B fine-tuning compared to quantized first-order methods.</li>
</ul>

<h3>Title: Towards Mechanistic Interpretability of Graph Transformers via Attention Graphs</h3>
<ul>
<li><strong>Authors: </strong>Batu El, Deepro Choudhury, Pietro Liò, Chaitanya K. Joshi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12352">https://arxiv.org/abs/2502.12352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12352">https://arxiv.org/pdf/2502.12352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12352]] Towards Mechanistic Interpretability of Graph Transformers via Attention Graphs(https://arxiv.org/abs/2502.12352)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>We introduce Attention Graphs, a new tool for mechanistic interpretability of Graph Neural Networks (GNNs) and Graph Transformers based on the mathematical equivalence between message passing in GNNs and the self-attention mechanism in Transformers. Attention Graphs aggregate attention matrices across Transformer layers and heads to describe how information flows among input nodes. Through experiments on homophilous and heterophilous node classification tasks, we analyze Attention Graphs from a network science perspective and find that: (1) When Graph Transformers are allowed to learn the optimal graph structure using all-to-all attention among input nodes, the Attention Graphs learned by the model do not tend to correlate with the input/original graph structure; and (2) For heterophilous graphs, different Graph Transformer variants can achieve similar performance while utilising distinct information flow patterns. Open source code: this https URL</li>
</ul>

<h3>Title: ConFit v2: Improving Resume-Job Matching using Hypothetical Resume Embedding and Runner-Up Hard-Negative Mining</h3>
<ul>
<li><strong>Authors: </strong>Xiao Yu, Ruize Xu, Chengyuan Xue, Jinzhong Zhang, Zhou Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12361">https://arxiv.org/abs/2502.12361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12361">https://arxiv.org/pdf/2502.12361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12361]] ConFit v2: Improving Resume-Job Matching using Hypothetical Resume Embedding and Runner-Up Hard-Negative Mining(https://arxiv.org/abs/2502.12361)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A reliable resume-job matching system helps a company recommend suitable candidates from a pool of resumes and helps a job seeker find relevant jobs from a list of job posts. However, since job seekers apply only to a few jobs, interaction labels in resume-job datasets are sparse. We introduce ConFit v2, an improvement over ConFit to tackle this sparsity problem. We propose two techniques to enhance the encoder's contrastive training process: augmenting job data with hypothetical reference resume generated by a large language model; and creating high-quality hard negatives from unlabeled resume/job pairs using a novel hard-negative mining strategy. We evaluate ConFit v2 on two real-world datasets and demonstrate that it outperforms ConFit and prior methods (including BM25 and OpenAI text-embedding-003), achieving an average absolute improvement of 13.8% in recall and 17.5% in nDCG across job-ranking and resume-ranking tasks.</li>
</ul>

<h3>Title: Positional Encoding in Transformer-Based Time Series Models: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Habib Irani, Vangelis Metsis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12370">https://arxiv.org/abs/2502.12370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12370">https://arxiv.org/pdf/2502.12370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12370]] Positional Encoding in Transformer-Based Time Series Models: A Survey(https://arxiv.org/abs/2502.12370)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Recent advancements in transformer-based models have greatly improved time series analysis, providing robust solutions for tasks such as forecasting, anomaly detection, and classification. A crucial element of these models is positional encoding, which allows transformers to capture the intrinsic sequential nature of time series data. This survey systematically examines existing techniques for positional encoding in transformer-based time series models. We investigate a variety of methods, including fixed, learnable, relative, and hybrid approaches, and evaluate their effectiveness in different time series classification tasks. Furthermore, we outline key challenges and suggest potential research directions to enhance positional encoding strategies. By delivering a comprehensive overview and quantitative benchmarking, this survey intends to assist researchers and practitioners in selecting and designing effective positional encoding methods for transformer-based time series models.</li>
</ul>

<h3>Title: Factual Inconsistency in Data-to-Text Generation Scales Exponentially with LLM Size: A Statistical Validation</h3>
<ul>
<li><strong>Authors: </strong>Joy Mahapatra, Soumyajit Roy, Utpal Garain</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12372">https://arxiv.org/abs/2502.12372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12372">https://arxiv.org/pdf/2502.12372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12372]] Factual Inconsistency in Data-to-Text Generation Scales Exponentially with LLM Size: A Statistical Validation(https://arxiv.org/abs/2502.12372)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Monitoring factual inconsistency is essential for ensuring trustworthiness in data-to-text generation (D2T). While large language models (LLMs) have demonstrated exceptional performance across various D2T tasks, previous studies on scaling laws have primarily focused on generalization error through power law scaling to LLM size (i.e., the number of model parameters). However, no research has examined the impact of LLM size on factual inconsistency in D2T. In this paper, we investigate how factual inconsistency in D2T scales with LLM size by exploring two scaling laws: power law and exponential scaling. To rigorously evaluate and compare these scaling laws, we employ a statistical validation framework consisting of three key stages: predictive performance estimation, goodness-of-fit assessment, and comparative analysis. For a comprehensive empirical study, we analyze three popular LLM families across five D2T datasets, measuring factual inconsistency inversely using four state-of-the-art consistency metrics. Our findings, based on exhaustive empirical results and validated through our framework, reveal that, contrary to the widely assumed power law scaling, factual inconsistency in D2T follows an exponential scaling with LLM size.</li>
</ul>

<h3>Title: Alignment and Adversarial Robustness: Are More Human-Like Models More Secure?</h3>
<ul>
<li><strong>Authors: </strong>Blaine Hoak, Kunyang Li, Patrick McDaniel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12377">https://arxiv.org/abs/2502.12377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12377">https://arxiv.org/pdf/2502.12377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12377]] Alignment and Adversarial Robustness: Are More Human-Like Models More Secure?(https://arxiv.org/abs/2502.12377)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack, robust</a></li>
<li><strong>Abstract: </strong>Representational alignment refers to the extent to which a model's internal representations mirror biological vision, offering insights into both neural similarity and functional correspondence. Recently, some more aligned models have demonstrated higher resiliency to adversarial examples, raising the question of whether more human-aligned models are inherently more secure. In this work, we conduct a large-scale empirical analysis to systematically investigate the relationship between representational alignment and adversarial robustness. We evaluate 118 models spanning diverse architectures and training paradigms, measuring their neural and behavioral alignment and engineering task performance across 106 benchmarks as well as their adversarial robustness via AutoAttack. Our findings reveal that while average alignment and robustness exhibit a weak overall correlation, specific alignment benchmarks serve as strong predictors of adversarial robustness, particularly those that measure selectivity towards texture or shape. These results suggest that different forms of alignment play distinct roles in model robustness, motivating further investigation into how alignment-driven approaches can be leveraged to build more secure and perceptually-grounded vision models.</li>
</ul>

<h3>Title: Pragmatics in the Era of Large Language Models: A Survey on Datasets, Evaluation, Opportunities and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Bolei Ma, Yuting Li, Wei Zhou, Ziwei Gong, Yang Janet Liu, Katja Jasinskaja, Annemarie Friedrich, Julia Hirschberg, Frauke Kreuter, Barbara Plank</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12378">https://arxiv.org/abs/2502.12378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12378">https://arxiv.org/pdf/2502.12378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12378]] Pragmatics in the Era of Large Language Models: A Survey on Datasets, Evaluation, Opportunities and Challenges(https://arxiv.org/abs/2502.12378)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding pragmatics-the use of language in context-is crucial for developing NLP systems capable of interpreting nuanced language use. Despite recent advances in language technologies, including large language models, evaluating their ability to handle pragmatic phenomena such as implicatures and references remains challenging. To advance pragmatic abilities in models, it is essential to understand current evaluation trends and identify existing limitations. In this survey, we provide a comprehensive review of resources designed for evaluating pragmatic capabilities in NLP, categorizing datasets by the pragmatics phenomena they address. We analyze task designs, data collection methods, evaluation approaches, and their relevance to real-world applications. By examining these resources in the context of modern language models, we highlight emerging trends, challenges, and gaps in existing benchmarks. Our survey aims to clarify the landscape of pragmatic evaluation and guide the development of more comprehensive and targeted benchmarks, ultimately contributing to more nuanced and context-aware NLP models.</li>
</ul>

<h3>Title: OCT Data is All You Need: How Vision Transformers with and without Pre-training Benefit Imaging</h3>
<ul>
<li><strong>Authors: </strong>Zihao Han, Philippe De Wilde</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12379">https://arxiv.org/abs/2502.12379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12379">https://arxiv.org/pdf/2502.12379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12379]] OCT Data is All You Need: How Vision Transformers with and without Pre-training Benefit Imaging(https://arxiv.org/abs/2502.12379)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Optical Coherence Tomography (OCT) provides high-resolution cross-sectional images useful for diagnosing various diseases, but their distinct characteristics from natural images raise questions about whether large-scale pre-training on datasets like ImageNet is always beneficial. In this paper, we investigate the impact of ImageNet-based pre-training on Vision Transformer (ViT) performance for OCT image classification across different dataset sizes. Our experiments cover four-category retinal pathologies (CNV, DME, Drusen, Normal). Results suggest that while pre-training can accelerate convergence and potentially offer better performance in smaller datasets, training from scratch may achieve comparable or even superior accuracy when sufficient OCT data is available. Our findings highlight the importance of matching domain characteristics in pre-training and call for further study on large-scale OCT-specific pre-training.</li>
</ul>

<h3>Title: DiffuRNN: Harnessing Diffusion Processes for Global Interactions</h3>
<ul>
<li><strong>Authors: </strong>Jacob Fein-Ashley</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12381">https://arxiv.org/abs/2502.12381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12381">https://arxiv.org/pdf/2502.12381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12381]] DiffuRNN: Harnessing Diffusion Processes for Global Interactions(https://arxiv.org/abs/2502.12381)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion kernels capture global dependencies. We present DiffuRNN, a novel architecture that reinterprets sequential data processing as a unified diffusion process. Our model integrates adaptive diffusion modules with localized nonlinear updates and a diffusion-inspired attention mechanism. This design enables efficient global information propagation while preserving fine-grained temporal details. DiffuRNN overcomes the limitations of conventional recurrent and transformer models by allowing full parallelization across time steps and supporting robust multi-scale temporal representations. Experiments on benchmark sequence modeling tasks demonstrate that DiffuRNN delivers superior performance and scalability, setting a new standard for global interaction in sequential data.</li>
</ul>

<h3>Title: Hybrid Machine Learning Models for Intrusion Detection in IoT: Leveraging a Real-World IoT Dataset</h3>
<ul>
<li><strong>Authors: </strong>Md Ahnaf Akif, Ismail Butun, Andre Williams, Imadeldin Mahgoub</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12382">https://arxiv.org/abs/2502.12382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12382">https://arxiv.org/pdf/2502.12382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12382]] Hybrid Machine Learning Models for Intrusion Detection in IoT: Leveraging a Real-World IoT Dataset(https://arxiv.org/abs/2502.12382)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, fair</a></li>
<li><strong>Abstract: </strong>The rapid growth of the Internet of Things (IoT) has revolutionized industries, enabling unprecedented connectivity and functionality. However, this expansion also increases vulnerabilities, exposing IoT networks to increasingly sophisticated cyberattacks. Intrusion Detection Systems (IDS) are crucial for mitigating these threats, and recent advancements in Machine Learning (ML) offer promising avenues for improvement. This research explores a hybrid approach, combining several standalone ML models such as Random Forest (RF), XGBoost, K-Nearest Neighbors (KNN), and AdaBoost, in a voting-based hybrid classifier for effective IoT intrusion detection. This ensemble method leverages the strengths of individual algorithms to enhance accuracy and address challenges related to data complexity and scalability. Using the widely-cited IoT-23 dataset, a prominent benchmark in IoT cybersecurity research, we evaluate our hybrid classifiers for both binary and multi-class intrusion detection problems, ensuring a fair comparison with existing literature. Results demonstrate that our proposed hybrid models, designed for robustness and scalability, outperform standalone approaches in IoT environments. This work contributes to the development of advanced, intelligent IDS frameworks capable of addressing evolving cyber threats.</li>
</ul>

<h3>Title: Locally-Deployed Chain-of-Thought (CoT) Reasoning Model in Chemical Engineering: Starting from 30 Experimental Data</h3>
<ul>
<li><strong>Authors: </strong>Tianhang Zhou, Yingchun Niu, Xingying Lan, Chunming Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12383">https://arxiv.org/abs/2502.12383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12383">https://arxiv.org/pdf/2502.12383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12383]] Locally-Deployed Chain-of-Thought (CoT) Reasoning Model in Chemical Engineering: Starting from 30 Experimental Data(https://arxiv.org/abs/2502.12383)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the field of chemical engineering, traditional data-processing and prediction methods face significant challenges. Machine-learning and large-language models (LLMs) also have their respective limitations. This paper explores the application of the Chain-of-Thought (CoT) reasoning model in chemical engineering, starting from 30 experimental data points. By integrating traditional surrogate models like Gaussian processes and random forests with powerful LLMs such as DeepSeek-R1, a hierarchical architecture is proposed. Two CoT-building methods, Large Language Model-Chain of Thought (LLM-CoT) and Machine Learning-Large Language Model-Chain of Thought (ML-LLM-CoT), are studied. The LLM-CoT combines local models DeepSeek-r1:14b and Qwen2:7b with Ollama. The ML-LLM-CoT integrates a pre-trained Gaussian ML model with the LLM-based CoT framework. Our results show that during construction, ML-LLM-CoT is more efficient. It only has 2 points that require rethink and a total of 4 rethink times, while LLM-CoT has 5 points that need to be re-thought and 34 total rethink times. In predicting the solubility of 20 molecules with dissimilar structures, the number of molecules with a prediction deviation higher than 100\% for the Gaussian model, LLM-CoT, and ML-LLM-CoT is 7, 6, and 4 respectively. These results indicate that ML-LLM-CoT performs better in controlling the number of high-deviation molecules, optimizing the average deviation, and achieving a higher success rate in solubility judgment, providing a more reliable method for chemical engineering and molecular property prediction. This study breaks through the limitations of traditional methods and offers new solutions for rapid property prediction and process optimization in chemical engineering.</li>
</ul>

<h3>Title: Reward-Safety Balance in Offline Safe RL via Diffusion Regularization</h3>
<ul>
<li><strong>Authors: </strong>Junyu Guo, Zhi Zheng, Donghao Ying, Ming Jin, Shangding Gu, Costas Spanos, Javad Lavaei</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12391">https://arxiv.org/abs/2502.12391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12391">https://arxiv.org/pdf/2502.12391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12391]] Reward-Safety Balance in Offline Safe RL via Diffusion Regularization(https://arxiv.org/abs/2502.12391)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Constrained reinforcement learning (RL) seeks high-performance policies under safety constraints. We focus on an offline setting where the agent has only a fixed dataset -- common in realistic tasks to prevent unsafe exploration. To address this, we propose Diffusion-Regularized Constrained Offline Reinforcement Learning (DRCORL), which first uses a diffusion model to capture the behavioral policy from offline data and then extracts a simplified policy to enable efficient inference. We further apply gradient manipulation for safety adaptation, balancing the reward objective and constraint satisfaction. This approach leverages high-quality offline data while incorporating safety requirements. Empirical results show that DRCORL achieves reliable safety performance, fast inference, and strong reward outcomes across robot learning tasks. Compared to existing safe offline RL methods, it consistently meets cost limits and performs well with the same hyperparameters, indicating practical applicability in real-world scenarios.</li>
</ul>

<h3>Title: Efficient Neural SDE Training using Wiener-Space Cubature</h3>
<ul>
<li><strong>Authors: </strong>Luke Snow, Vikram Krishnamurthy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12395">https://arxiv.org/abs/2502.12395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12395">https://arxiv.org/pdf/2502.12395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12395]] Efficient Neural SDE Training using Wiener-Space Cubature(https://arxiv.org/abs/2502.12395)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>A neural stochastic differential equation (SDE) is an SDE with drift and diffusion terms parametrized by neural networks. The training procedure for neural SDEs consists of optimizing the SDE vector field (neural network) parameters to minimize the expected value of an objective functional on infinite-dimensional path-space. Existing training techniques focus on methods to efficiently compute path-wise gradients of the objective functional with respect to these parameters, then pair this with Monte-Carlo simulation to estimate the expectation, and stochastic gradient descent to optimize. In this work we introduce a novel training technique which bypasses and improves upon Monte-Carlo simulation; we extend results in the theory of Wiener-space cubature to approximate the expected objective functional by a weighted sum of deterministic ODE solutions. This allows us to compute gradients by efficient ODE adjoint methods. Furthermore, we exploit a high-order recombination scheme to drastically reduce the number of ODE solutions necessary to achieve a reasonable approximation. We show that this Wiener-space cubature approach can surpass the O(1/sqrt(n)) rate of Monte-Carlo simulation, or the O(log(n)/n) rate of quasi-Monte-Carlo, to achieve a O(1/n) rate under reasonable assumptions.</li>
</ul>

<h3>Title: WMT24++: Expanding the Language Coverage of WMT24 to 55 Languages & Dialects</h3>
<ul>
<li><strong>Authors: </strong>Daniel Deutsch, Eleftheria Briakou, Isaac Caswell, Mara Finkelstein, Rebecca Galor, Juraj Juraska, Geza Kovacs, Alison Lui, Ricardo Rei, Jason Riesa, Shruti Rijhwani, Parker Riley, Elizabeth Salesky, Firas Trabelsi, Stephanie Winkler, Biao Zhang, Markus Freitag</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12404">https://arxiv.org/abs/2502.12404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12404">https://arxiv.org/pdf/2502.12404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12404]] WMT24++: Expanding the Language Coverage of WMT24 to 55 Languages & Dialects(https://arxiv.org/abs/2502.12404)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLM) become more and more capable in languages other than English, it is important to collect benchmark datasets in order to evaluate their multilingual performance, including on tasks like machine translation (MT). In this work, we extend the WMT24 dataset to cover 55 languages by collecting new human-written references and post-edits for 46 new languages and dialects in addition to post-edits of the references in 8 out of 9 languages in the original WMT24 dataset. The dataset covers four domains: literary, news, social, and speech. We benchmark a variety of MT providers and LLMs on the collected dataset using automatic metrics and find that LLMs are the best-performing MT systems in all 55 languages. These results should be confirmed using a human-based evaluation, which we leave for future work.</li>
</ul>

<h3>Title: On the Robust Approximation of ASR Metrics</h3>
<ul>
<li><strong>Authors: </strong>Abdul Waheed, Hanin Atwany, Rita Singh, Bhiksha Raj</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12408">https://arxiv.org/abs/2502.12408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12408">https://arxiv.org/pdf/2502.12408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12408]] On the Robust Approximation of ASR Metrics(https://arxiv.org/abs/2502.12408)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advances in speech foundation models are largely driven by scaling both model size and data, enabling them to perform a wide range of tasks, including speech recognition. Traditionally, ASR models are evaluated using metrics like Word Error Rate (WER) and Character Error Rate (CER), which depend on ground truth labels. As a result of limited labeled data from diverse domains and testing conditions, the true generalization capabilities of these models beyond standard benchmarks remain unclear. Moreover, labeling data is both costly and time-consuming. To address this, we propose a novel label-free approach for approximating ASR performance metrics, eliminating the need for ground truth labels. Our method utilizes multimodal embeddings in a unified space for speech and transcription representations, combined with a high-quality proxy model to compute proxy metrics. These features are used to train a regression model to predict key ASR metrics like Word Error Rate (WER) and Character Error Rate (CER). We experiment with over 40 models across 14 datasets representing both standard and in-the-wild testing conditions. Our results show that we approximate the metrics within a single-digit absolute difference across all experimental configurations, outperforming the most recent baseline by more than 50\%.</li>
</ul>

<h3>Title: Gradient Co-occurrence Analysis for Detecting Unsafe Prompts in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jingyuan Yang, Bowen Yan, Rongjun Li, Ziyu Zhou, Xin Chen, Zhiyong Feng, Wei Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12411">https://arxiv.org/abs/2502.12411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12411">https://arxiv.org/pdf/2502.12411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12411]] Gradient Co-occurrence Analysis for Detecting Unsafe Prompts in Large Language Models(https://arxiv.org/abs/2502.12411)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Unsafe prompts pose significant safety risks to large language models (LLMs). Existing methods for detecting unsafe prompts rely on data-driven fine-tuning to train guardrail models, necessitating significant data and computational resources. In contrast, recent few-shot gradient-based methods emerge, requiring only few safe and unsafe reference prompts. A gradient-based approach identifies unsafe prompts by analyzing consistent patterns of the gradients of safety-critical parameters in LLMs. Although effective, its restriction to directional similarity (cosine similarity) introduces ``directional bias'', limiting its capability to identify unsafe prompts. To overcome this limitation, we introduce GradCoo, a novel gradient co-occurrence analysis method that expands the scope of safety-critical parameter identification to include unsigned gradient similarity, thereby reducing the impact of ``directional bias'' and enhancing the accuracy of unsafe prompt detection. Comprehensive experiments on the widely-used benchmark datasets ToxicChat and XStest demonstrate that our proposed method can achieve state-of-the-art (SOTA) performance compared to existing methods. Moreover, we confirm the generalizability of GradCoo in detecting unsafe prompts across a range of LLM base models with various sizes and origins.</li>
</ul>

<h3>Title: Incomplete Graph Learning: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Riting Xia, Huibo Liu, Anchen Li, Xueyan Liu, Yan Zhang, Chunxu Zhang, Bo Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12412">https://arxiv.org/abs/2502.12412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12412">https://arxiv.org/pdf/2502.12412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12412]] Incomplete Graph Learning: A Comprehensive Survey(https://arxiv.org/abs/2502.12412)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Graph learning is a prevalent field that operates on ubiquitous graph data. Effective graph learning methods can extract valuable information from graphs. However, these methods are non-robust and affected by missing attributes in graphs, resulting in sub-optimal outcomes. This has led to the emergence of incomplete graph learning, which aims to process and learn from incomplete graphs to achieve more accurate and representative results. In this paper, we conducted a comprehensive review of the literature on incomplete graph learning. Initially, we categorize incomplete graphs and provide precise definitions of relevant concepts, terminologies, and techniques, thereby establishing a solid understanding for readers. Subsequently, we classify incomplete graph learning methods according to the types of incompleteness: (1) attribute-incomplete graph learning methods, (2) attribute-missing graph learning methods, and (3) hybrid-absent graph learning methods. By systematically classifying and summarizing incomplete graph learning methods, we highlight the commonalities and differences among existing approaches, aiding readers in selecting methods and laying the groundwork for further advancements. In addition, we summarize the datasets, incomplete processing modes, evaluation metrics, and application domains used by the current methods. Lastly, we discuss the current challenges and propose future directions for incomplete graph learning, with the aim of stimulating further innovations in this crucial field. To our knowledge, this is the first review dedicated to incomplete graph learning, aiming to offer valuable insights for researchers in related this http URL developed an online resource to follow relevant research based on this review, available at this https URL</li>
</ul>

<h3>Title: Lost in Transcription, Found in Distribution Shift: Demystifying Hallucination in Speech Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Hanin Atwany, Abdul Waheed, Rita Singh, Monojit Choudhury, Bhiksha Raj</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12414">https://arxiv.org/abs/2502.12414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12414">https://arxiv.org/pdf/2502.12414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12414]] Lost in Transcription, Found in Distribution Shift: Demystifying Hallucination in Speech Foundation Models(https://arxiv.org/abs/2502.12414)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Speech foundation models trained at a massive scale, both in terms of model and data size, result in robust systems capable of performing multiple speech tasks, including automatic speech recognition (ASR). These models transcend language and domain barriers, yet effectively measuring their performance remains a challenge. Traditional metrics like word error rate (WER) and character error rate (CER) are commonly used to evaluate ASR performance but often fail to reflect transcription quality in critical contexts, particularly when detecting fabricated outputs. This phenomenon, known as hallucination, is especially concerning in high-stakes domains such as healthcare, legal, and aviation, where errors can have severe consequences. In our work, we address this gap by investigating hallucination in ASR models. We examine how factors such as distribution shifts, model size, and model architecture influence the hallucination error rate (HER), a metric we introduce to quantify hallucinations. Our analysis of 20 ASR models reveals \numinsights~key insights: (1) High WERs can mask low hallucination rates, while low WERs may conceal dangerous hallucinations. (2) Synthetic noise, both adversarial and common perturbations like white noise, pitch shift, and time stretching, increase HER. (3) Distribution shift correlates strongly with HER ($\alpha = 0.91$). Our findings highlight the importance of incorporating HER alongside traditional metrics like WER to better assess ASR model performance, particularly in high-stakes domains.</li>
</ul>

<h3>Title: Boosting Illuminant Estimation in Deep Color Constancy through Enhancing Brightness Robustness</h3>
<ul>
<li><strong>Authors: </strong>Mengda Xie, Chengzhi Zhong, Yiling He, Zhan Qin, Meie Fang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12418">https://arxiv.org/abs/2502.12418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12418">https://arxiv.org/pdf/2502.12418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12418]] Boosting Illuminant Estimation in Deep Color Constancy through Enhancing Brightness Robustness(https://arxiv.org/abs/2502.12418)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Color constancy estimates illuminant chromaticity to correct color-biased images. Recently, Deep Neural Network-driven Color Constancy (DNNCC) models have made substantial advancements. Nevertheless, the potential risks in DNNCC due to the vulnerability of deep neural networks have not yet been explored. In this paper, we conduct the first investigation into the impact of a key factor in color constancy-brightness-on DNNCC from a robustness perspective. Our evaluation reveals that several mainstream DNNCC models exhibit high sensitivity to brightness despite their focus on chromaticity estimation. This sheds light on a potential limitation of existing DNNCC models: their sensitivity to brightness may hinder performance given the widespread brightness variations in real-world datasets. From the insights of our analysis, we propose a simple yet effective brightness robustness enhancement strategy for DNNCC models, termed BRE. The core of BRE is built upon the adaptive step-size adversarial brightness augmentation technique, which identifies high-risk brightness variation and generates augmented images via explicit brightness adjustment. Subsequently, BRE develops a brightness-robustness-aware model optimization strategy that integrates adversarial brightness training and brightness contrastive loss, significantly bolstering the brightness robustness of DNNCC models. BRE is hyperparameter-free and can be integrated into existing DNNCC models, without incurring additional overhead during the testing phase. Experiments on two public color constancy datasets-ColorChecker and Cube+-demonstrate that the proposed BRE consistently enhances the illuminant estimation performance of existing DNNCC models, reducing the estimation error by an average of 5.04% across six mainstream DNNCC models, underscoring the critical role of enhancing brightness robustness in these models.</li>
</ul>

<h3>Title: Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shuqi Liu, Han Wu, Bowei He, Xiongwei Han, Mingxuan Yuan, Linqin Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12420">https://arxiv.org/abs/2502.12420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12420">https://arxiv.org/pdf/2502.12420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12420]] Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models(https://arxiv.org/abs/2502.12420)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task vector-based merging methods show promise, they typically apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks. We present Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances existing model merging techniques by operating at both task-specific and cross-task levels. Our method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients. Extensive experiments on Mistral 7B and LLaMA2-7B/13B models demonstrate that Sens-Merging significantly improves performance across general knowledge, mathematical reasoning, and code generation tasks. Notably, when combined with existing merging techniques, our method enables merged models to outperform specialized fine-tuned models, particularly in code generation tasks. Our findings reveal important trade-offs between task-specific and cross-task scalings, providing insights for future model merging strategies.</li>
</ul>

<h3>Title: Wi-Chat: Large Language Model Powered Wi-Fi Sensing</h3>
<ul>
<li><strong>Authors: </strong>Haopeng Zhang, Yili Ren, Haohan Yuan, Jingzhe Zhang, Yitong Shen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12421">https://arxiv.org/abs/2502.12421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12421">https://arxiv.org/pdf/2502.12421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12421]] Wi-Chat: Large Language Model Powered Wi-Fi Sensing(https://arxiv.org/abs/2502.12421)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse tasks. However, their potential to integrate physical model knowledge for real-world signal interpretation remains largely unexplored. In this work, we introduce Wi-Chat, the first LLM-powered Wi-Fi-based human activity recognition system. We demonstrate that LLMs can process raw Wi-Fi signals and infer human activities by incorporating Wi-Fi sensing principles into prompts. Our approach leverages physical model insights to guide LLMs in interpreting Channel State Information (CSI) data without traditional signal processing techniques. Through experiments on real-world Wi-Fi datasets, we show that LLMs exhibit strong reasoning capabilities, achieving zero-shot activity recognition. These findings highlight a new paradigm for Wi-Fi sensing, expanding LLM applications beyond conventional language tasks and enhancing the accessibility of wireless sensing for real-world deployments.</li>
</ul>

<h3>Title: Robust Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Mengshi Qi, Changsheng Lv, Huadong Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12425">https://arxiv.org/abs/2502.12425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12425">https://arxiv.org/pdf/2502.12425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12425]] Robust Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning(https://arxiv.org/abs/2502.12425)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a new Robust Disentangled Counterfactual Learning (RDCL) approach for physical audiovisual commonsense reasoning. The task aims to infer objects' physics commonsense based on both video and audio input, with the main challenge being how to imitate the reasoning ability of humans, even under the scenario of missing modalities. Most of the current methods fail to take full advantage of different characteristics in multi-modal data, and lacking causal reasoning ability in models impedes the progress of implicit physical knowledge inferring. To address these issues, our proposed RDCL method decouples videos into static (time-invariant) and dynamic (time-varying) factors in the latent space by the disentangled sequential encoder, which adopts a variational autoencoder (VAE) to maximize the mutual information with a contrastive loss function. Furthermore, we introduce a counterfactual learning module to augment the model's reasoning ability by modeling physical knowledge relationships among different objects under counterfactual intervention. To alleviate the incomplete modality data issue, we introduce a robust multimodal learning method to recover the missing data by decomposing the shared features and model-specific features. Our proposed method is a plug-and-play module that can be incorporated into any baseline including VLMs. In experiments, we show that our proposed method improves the reasoning accuracy and robustness of baseline methods and achieves the state-of-the-art performance.</li>
</ul>

<h3>Title: Multi Image Super Resolution Modeling for Earth System Models</h3>
<ul>
<li><strong>Authors: </strong>Ehsan Zeraatkar, Salah A Faroughi, Jelena Tešić</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12427">https://arxiv.org/abs/2502.12427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12427">https://arxiv.org/pdf/2502.12427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12427]] Multi Image Super Resolution Modeling for Earth System Models(https://arxiv.org/abs/2502.12427)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Super-resolution (SR) techniques are essential for improving Earth System Model (ESM) data's spatial resolution, which helps better understand complex environmental processes. This paper presents a new algorithm, ViFOR, which combines Vision Transformers (ViT) and Implicit Neural Representation Networks (INRs) to generate High-Resolution (HR) images from Low-Resolution (LR) inputs. ViFOR introduces a novel integration of Fourier-based activation functions within the Vision Transformer architecture, enabling it to effectively capture global context and high-frequency details critical for accurate SR reconstruction. The results show that ViFOR outperforms state-of-the-art methods such as ViT, Sinusoidal Representation Networks (SIREN), and SR Generative Adversarial Networks (SRGANs) based on metrics like Peak Signal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE) both for global as well as the local imagery. ViFOR improves PSNR of up to 4.18 dB, 1.56 dB, and 1.73 dB over ViT for full images in the Source Temperature, Shortwave, and Longwave Flux.</li>
</ul>

<h3>Title: Bridge the Gaps between Machine Unlearning and AI Regulation</h3>
<ul>
<li><strong>Authors: </strong>Bill Marino, Meghdad Kurmanji, Nicholas D. Lane</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12430">https://arxiv.org/abs/2502.12430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12430">https://arxiv.org/pdf/2502.12430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12430]] Bridge the Gaps between Machine Unlearning and AI Regulation(https://arxiv.org/abs/2502.12430)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The "right to be forgotten" and the data privacy laws that encode it have motivated machine unlearning since its earliest days. Now, an inbound wave of artificial intelligence regulations - like the European Union's Artificial Intelligence Act (AIA) - potentially offer important new use cases for machine unlearning. However, this position paper argues, this opportunity will only be realized if researchers, aided by policymakers, proactively bridge the (sometimes sizable) gaps between machine unlearning's state of the art and its potential applications to AI regulation. To demonstrate this point, we use the AIA as an example. Specifically, we deliver a "state of the union" as regards machine unlearning's current potential for aiding compliance with the AIA. This starts with a precise cataloging of the potential applications of machine unlearning to AIA compliance. For each, we flag any legal ambiguities clouding the potential application and, moreover, flag the technical gaps that exist between the potential application and the state of the art of machine unlearning. Finally, we end with a call to action: for both machine learning researchers and policymakers, to, respectively, solve the open technical and legal questions that will unlock machine unlearning's potential to assist compliance with the AIA - and other AI regulation like it.</li>
</ul>

<h3>Title: Should I Trust You? Detecting Deception in Negotiations using Counterfactual RL</h3>
<ul>
<li><strong>Authors: </strong>Wichayaporn Wongkamjan, Yanze Wang, Feng Gu, Denis Peskoff, Jonathan K. Kummerfeld, Jonathan May, Jordan Lee Boyd-Graber</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12436">https://arxiv.org/abs/2502.12436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12436">https://arxiv.org/pdf/2502.12436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12436]] Should I Trust You? Detecting Deception in Negotiations using Counterfactual RL(https://arxiv.org/abs/2502.12436)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>An increasingly prevalent socio-technical problem is people being taken in by offers that sound ``too good to be true'', where persuasion and trust shape decision-making. This paper investigates how \abr{ai} can help detect these deceptive scenarios. We analyze how humans strategically deceive each other in \textit{Diplomacy}, a board game that requires both natural language communication and strategic reasoning. This requires extracting logical forms of proposed agreements in player communications and computing the relative rewards of the proposal using agents' value functions. Combined with text-based features, this can improve our deception detection. Our method detects human deception with a high precision when compared to a Large Language Model approach that flags many true messages as deceptive. Future human-\abr{ai} interaction tools can build on our methods for deception detection by triggering \textit{friction} to give users a chance of interrogating suspicious proposals.</li>
</ul>

<h3>Title: SparAMX: Accelerating Compressed LLMs Token Generation on AMX-powered CPUs</h3>
<ul>
<li><strong>Authors: </strong>Ahmed F. AbouElhamayed, Jordan Dotzel, Yash Akhauri, Chi-Chih Chang, Sameh Gobriel, J. Pablo Muñoz, Vui Seng Chua, Nilesh Jain, Mohamed S. Abdelfattah</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.AR, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12444">https://arxiv.org/abs/2502.12444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12444">https://arxiv.org/pdf/2502.12444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12444]] SparAMX: Accelerating Compressed LLMs Token Generation on AMX-powered CPUs(https://arxiv.org/abs/2502.12444)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models have high compute, latency, and memory requirements. While specialized accelerators such as GPUs and TPUs typically run these workloads, CPUs are more widely available and consume less energy. Accelerating LLMs with CPUs enables broader AI access at a lower cost and power consumption. This acceleration potential for CPUs is especially relevant during the memory-bound decoding stage of LLM inference, which processes one token at a time and is becoming increasingly utilized with reasoning models. We utilize Advanced Matrix Extensions (AMX) support on the latest Intel CPUs together with unstructured sparsity to achieve a $1.42 \times$ reduction in end-to-end latency compared to the current PyTorch implementation by applying our technique in linear layers. We provide a set of open-source customized sparse kernels that can speed up any PyTorch model by automatically replacing all linear layers with our custom sparse implementation. Furthermore, we demonstrate for the first time the use of unstructured sparsity in the attention computation achieving a $1.14 \times$ speedup over the current systems without compromising accuracy. Code: this https URL</li>
</ul>

<h3>Title: Multi-Attribute Steering of Language Models via Targeted Intervention</h3>
<ul>
<li><strong>Authors: </strong>Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12446">https://arxiv.org/abs/2502.12446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12446">https://arxiv.org/pdf/2502.12446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12446]] Multi-Attribute Steering of Language Models via Targeted Intervention(https://arxiv.org/abs/2502.12446)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Inference-time intervention (ITI) has emerged as a promising method for steering large language model (LLM) behavior in a particular direction (e.g., improving helpfulness) by intervening on token representations without costly updates to the LLM's parameters. However, existing ITI approaches fail to scale to multi-attribute settings with conflicts, such as enhancing helpfulness while also reducing toxicity. To address this, we introduce Multi-Attribute Targeted Steering (MAT-Steer), a novel steering framework designed for selective token-level intervention across multiple attributes. MAT-Steer learns steering vectors using an alignment objective that shifts the model's internal representations of undesirable outputs closer to those of desirable ones while enforcing sparsity and orthogonality among vectors for different attributes, thereby reducing inter-attribute conflicts. We evaluate MAT-Steer in two distinct settings: (i) on question answering (QA) tasks where we balance attributes like truthfulness, bias, and toxicity; (ii) on generative tasks where we simultaneously improve attributes like helpfulness, correctness, and coherence. MAT-Steer outperforms existing ITI and parameter-efficient finetuning approaches across both task types (e.g., 3% average accuracy gain across QA tasks and 55.82% win rate against the best ITI baseline).</li>
</ul>

<h3>Title: YUNet: Improved YOLOv11 Network for Skyline Detection</h3>
<ul>
<li><strong>Authors: </strong>Gang Yang, Miao Wang, Quan Zhou, Jiangchuan Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12449">https://arxiv.org/abs/2502.12449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12449">https://arxiv.org/pdf/2502.12449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12449]] YUNet: Improved YOLOv11 Network for Skyline Detection(https://arxiv.org/abs/2502.12449)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, segmentation</a></li>
<li><strong>Abstract: </strong>Skyline detection plays an important role in geolocalizaion, flight control, visual navigation, port security, etc. The appearance of the sky and non-sky areas are variable, because of different weather or illumination environment, which brings challenges to skyline detection. In this research, we proposed the YUNet algorithm, which improved the YOLOv11 architecture to segment the sky region and extract the skyline in complicated and variable circumstances. To improve the ability of multi-scale and large range contextual feature fusion, the YOLOv11 architecture is extended as an UNet-like architecture, consisting of an encoder, neck and decoder submodule. The encoder extracts the multi-scale features from the given images. The neck makes fusion of these multi-scale features. The decoder applies the fused features to complete the prediction rebuilding. To validate the proposed approach, the YUNet was tested on Skyfinder and CH1 datasets for segmentation and skyline detection respectively. Our test shows that the IoU of YUnet segmentation can reach 0.9858, and the average error of YUnet skyline detection is just 1.36 pixels. The implementation is published at this https URL.</li>
</ul>

<h3>Title: Benchmarking Zero-Shot Facial Emotion Annotation with Large Language Models: A Multi-Class and Multi-Frame Approach in DailyLife</h3>
<ul>
<li><strong>Authors: </strong>He Zhang, Xinyi Fu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12454">https://arxiv.org/abs/2502.12454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12454">https://arxiv.org/pdf/2502.12454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12454]] Benchmarking Zero-Shot Facial Emotion Annotation with Large Language Models: A Multi-Class and Multi-Frame Approach in DailyLife(https://arxiv.org/abs/2502.12454)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study investigates the feasibility and performance of using large language models (LLMs) to automatically annotate human emotions in everyday scenarios. We conducted experiments on the DailyLife subset of the publicly available FERV39k dataset, employing the GPT-4o-mini model for rapid, zero-shot labeling of key frames extracted from video segments. Under a seven-class emotion taxonomy ("Angry," "Disgust," "Fear," "Happy," "Neutral," "Sad," "Surprise"), the LLM achieved an average precision of approximately 50%. In contrast, when limited to ternary emotion classification (negative/neutral/positive), the average precision increased to approximately 64%. Additionally, we explored a strategy that integrates multiple frames within 1-2 second video clips to enhance labeling performance and reduce costs. The results indicate that this approach can slightly improve annotation accuracy. Overall, our preliminary findings highlight the potential application of zero-shot LLMs in human facial emotion annotation tasks, offering new avenues for reducing labeling costs and broadening the applicability of LLMs in complex multimodal environments.</li>
</ul>

<h3>Title: DSMoE: Matrix-Partitioned Experts with Dynamic Routing for Computation-Efficient Dense LLMs</h3>
<ul>
<li><strong>Authors: </strong>Minxuan Lv, Zhenpeng Su, Leiyu Pan, Yizhe Xiong, Zijia Lin, Hui Chen, Wei Zhou, Jungong Han, Guiguang Ding, Cheng Luo, Di Zhang, Kun Gai, Songlin Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12455">https://arxiv.org/abs/2502.12455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12455">https://arxiv.org/pdf/2502.12455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12455]] DSMoE: Matrix-Partitioned Experts with Dynamic Routing for Computation-Efficient Dense LLMs(https://arxiv.org/abs/2502.12455)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models continue to scale, computational costs and resource consumption have emerged as significant challenges. While existing sparsification methods like pruning reduce computational overhead, they risk losing model knowledge through parameter removal. This paper proposes DSMoE (Dynamic Sparse Mixture-of-Experts), a novel approach that achieves sparsification by partitioning pre-trained FFN layers into computational blocks. We implement adaptive expert routing using sigmoid activation and straight-through estimators, enabling tokens to flexibly access different aspects of model knowledge based on input complexity. Additionally, we introduce a sparsity loss term to balance performance and computational efficiency. Extensive experiments on LLaMA models demonstrate that under equivalent computational constraints, DSMoE achieves superior performance compared to existing pruning and MoE approaches across language modeling and downstream tasks, particularly excelling in generation tasks. Analysis reveals that DSMoE learns distinctive layerwise activation patterns, providing new insights for future MoE architecture design.</li>
</ul>

<h3>Title: Not-So-Optimal Transport Flows for 3D Point Cloud Generation</h3>
<ul>
<li><strong>Authors: </strong>Ka-Hei Hui, Chao Liu, Xiaohui Zeng, Chi-Wing Fu, Arash Vahdat</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12456">https://arxiv.org/abs/2502.12456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12456">https://arxiv.org/pdf/2502.12456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12456]] Not-So-Optimal Transport Flows for 3D Point Cloud Generation(https://arxiv.org/abs/2502.12456)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Learning generative models of 3D point clouds is one of the fundamental problems in 3D generative learning. One of the key properties of point clouds is their permutation invariance, i.e., changing the order of points in a point cloud does not change the shape they represent. In this paper, we analyze the recently proposed equivariant OT flows that learn permutation invariant generative models for point-based molecular data and we show that these models scale poorly on large point clouds. Also, we observe learning (equivariant) OT flows is generally challenging since straightening flow trajectories makes the learned flow model complex at the beginning of the trajectory. To remedy these, we propose not-so-optimal transport flow models that obtain an approximate OT by an offline OT precomputation, enabling an efficient construction of OT pairs for training. During training, we can additionally construct a hybrid coupling by combining our approximate OT and independent coupling to make the target flow models easier to learn. In an extensive empirical study, we show that our proposed model outperforms prior diffusion- and flow-based approaches on a wide range of unconditional generation and shape completion on the ShapeNet benchmark.</li>
</ul>

<h3>Title: An Empirical Evaluation of Encoder Architectures for Fast Real-Time Long Conversational Understanding</h3>
<ul>
<li><strong>Authors: </strong>Annamalai Senthilnathan, Kristjan Arumae, Mohammed Khalilia, Zhengzheng Xing, Aaron R. Colak</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12458">https://arxiv.org/abs/2502.12458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12458">https://arxiv.org/pdf/2502.12458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12458]] An Empirical Evaluation of Encoder Architectures for Fast Real-Time Long Conversational Understanding(https://arxiv.org/abs/2502.12458)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Analyzing long text data such as customer call transcripts is a cost-intensive and tedious task. Machine learning methods, namely Transformers, are leveraged to model agent-customer interactions. Unfortunately, Transformers adhere to fixed-length architectures and their self-attention mechanism scales quadratically with input length. Such limitations make it challenging to leverage traditional Transformers for long sequence tasks, such as conversational understanding, especially in real-time use cases. In this paper we explore and evaluate recently proposed efficient Transformer variants (e.g. Performer, Reformer) and a CNN-based architecture for real-time and near real-time long conversational understanding tasks. We show that CNN-based models are dynamic, ~2.6x faster to train, ~80% faster inference and ~72% more memory efficient compared to Transformers on average. Additionally, we evaluate the CNN model using the Long Range Arena benchmark to demonstrate competitiveness in general long document analysis.</li>
</ul>

<h3>Title: Stress Testing Generalization: How Minor Modifications Undermine Large Language Model Performance</h3>
<ul>
<li><strong>Authors: </strong>Guangxiang Zhao, Saier Hu, Xiaoqi Jian, Jinzhu Wu, Yuhan Wu, Change Jia, Lin Sun, Xiangzheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12459">https://arxiv.org/abs/2502.12459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12459">https://arxiv.org/pdf/2502.12459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12459]] Stress Testing Generalization: How Minor Modifications Undermine Large Language Model Performance(https://arxiv.org/abs/2502.12459)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper investigates the fragility of Large Language Models (LLMs) in generalizing to novel inputs, specifically focusing on minor perturbations in well-established benchmarks (e.g., slight changes in question format or distractor length). Despite high benchmark scores, LLMs exhibit significant accuracy drops and unexpected biases (e.g., preference for longer distractors) when faced with these minor but content-preserving modifications. For example, Qwen 2.5 1.5B's MMLU score rises from 60 to 89 and drops from 89 to 36 when option lengths are changed without altering the question. Even GPT-4 experiences a 25-point accuracy loss when question types are changed, with a 6-point drop across all three modification categories. These analyses suggest that LLMs rely heavily on superficial cues rather than forming robust, abstract representations that generalize across formats, lexical variations, and irrelevant content shifts. This work aligns with the ACL 2025 theme track on the Generalization of NLP models, proposing a "Generalization Stress Test" to assess performance shifts under controlled perturbations. The study calls for reevaluating benchmarks and developing more reliable evaluation methodologies to capture LLM generalization abilities better.</li>
</ul>

<h3>Title: LMN: A Tool for Generating Machine Enforceable Policies from Natural Language Access Control Rules using LLMs</h3>
<ul>
<li><strong>Authors: </strong>Pratik Sonune (Indian Institute of Technology Kharagpur, India), Ritwik Rai (Indian Institute of Technology Kharagpur, India), Shamik Sural (Indian Institute of Technology Kharagpur, India), Vijayalakshmi Atluri (Rutgers University, Newark, USA), Ashish Kundu (CISCO Research, USA)</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12460">https://arxiv.org/abs/2502.12460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12460">https://arxiv.org/pdf/2502.12460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12460]] LMN: A Tool for Generating Machine Enforceable Policies from Natural Language Access Control Rules using LLMs(https://arxiv.org/abs/2502.12460)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Organizations often lay down rules or guidelines called Natural Language Access Control Policies (NLACPs) for specifying who gets access to which information and when. However, these cannot be directly used in a target access control model like Attribute-based Access Control (ABAC). Manually translating the NLACP rules into Machine Enforceable Security Policies (MESPs) is both time consuming and resource intensive, rendering it infeasible especially for large organizations. Automated machine translation workflows, on the other hand, require information security officers to be adept at using such processes. To effectively address this problem, we have developed a free web-based publicly accessible tool called LMN (LLMs for generating MESPs from NLACPs) that takes an NLACP as input and converts it into a corresponding MESP. Internally, LMN uses the GPT 3.5 API calls and an appropriately chosen prompt. Extensive experiments with different prompts and performance metrics firmly establish the usefulness of LMN.</li>
</ul>

<h3>Title: Emulating Retrieval Augmented Generation via Prompt Engineering for Enhanced Long Context Comprehension in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Joon Park, Kyohei Atarashi, Koh Takeuchi, Hisashi Kashima</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12462">https://arxiv.org/abs/2502.12462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12462">https://arxiv.org/pdf/2502.12462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12462]] Emulating Retrieval Augmented Generation via Prompt Engineering for Enhanced Long Context Comprehension in LLMs(https://arxiv.org/abs/2502.12462)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper addresses the challenge of comprehending very long contexts in Large Language Models (LLMs) by proposing a method that emulates Retrieval Augmented Generation (RAG) through specialized prompt engineering and chain-of-thought (CoT) reasoning. While recent LLMs support over 100,000 tokens in a single prompt, simply enlarging context windows has not guaranteed robust multi-hop reasoning when key details are scattered across massive input. Our approach treats the model as both the retriever and the reasoner: it first tags relevant segments within a long passage, then employs a stepwise CoT workflow to integrate these pieces of evidence. This single-pass method thereby reduces reliance on an external retriever, yet maintains focus on crucial segments. We evaluate our approach on selected tasks from BABILong, which interleaves standard bAbI QA problems with large amounts of distractor text. Compared to baseline (no retrieval) and naive RAG pipelines, our approach more accurately handles multi-fact questions such as object location tracking, counting, and indefinite knowledge. Furthermore, we analyze how prompt structure, including the order of question, relevant-text tags, and overall instructions, significantly affects performance. These findings underscore that optimized prompt engineering, combined with guided reasoning, can enhance LLMs' long-context comprehension and serve as a lightweight alternative to traditional retrieval pipelines.</li>
</ul>

<h3>Title: SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Seanie Lee, Dong Bok Lee, Dominik Wagner, Minki Kang, Haebin Seong, Tobias Bocklet, Juho Lee, Sung Ju Hwang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12464">https://arxiv.org/abs/2502.12464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12464">https://arxiv.org/pdf/2502.12464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12464]] SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models(https://arxiv.org/abs/2502.12464)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Deploying large language models (LLMs) in real-world applications requires robust safety guard models to detect and block harmful user prompts. While large safety guard models achieve strong performance, their computational cost is substantial. To mitigate this, smaller distilled models are used, but they often underperform on "hard" examples where the larger model provides accurate predictions. We observe that many inputs can be reliably handled by the smaller model, while only a small fraction require the larger model's capacity. Motivated by this, we propose SafeRoute, a binary router that distinguishes hard examples from easy ones. Our method selectively applies the larger safety guard model to the data that the router considers hard, improving efficiency while maintaining accuracy compared to solely using the larger safety guard model. Experimental results on multiple benchmark datasets demonstrate that our adaptive model selection significantly enhances the trade-off between computational cost and safety performance, outperforming relevant baselines.</li>
</ul>

<h3>Title: Computational-Statistical Tradeoffs at the Next-Token Prediction Barrier: Autoregressive and Imitation Learning under Misspecification</h3>
<ul>
<li><strong>Authors: </strong>Dhruv Rohatgi, Adam Block, Audrey Huang, Akshay Krishnamurthy, Dylan J. Foster</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12465">https://arxiv.org/abs/2502.12465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12465">https://arxiv.org/pdf/2502.12465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12465]] Computational-Statistical Tradeoffs at the Next-Token Prediction Barrier: Autoregressive and Imitation Learning under Misspecification(https://arxiv.org/abs/2502.12465)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Next-token prediction with the logarithmic loss is a cornerstone of autoregressive sequence modeling, but, in practice, suffers from error amplification, where errors in the model compound and generation quality degrades as sequence length $H$ increases. From a theoretical perspective, this phenomenon should not appear in well-specified settings, and, indeed, a growing body of empirical work hypothesizes that misspecification, where the learner is not sufficiently expressive to represent the target distribution, may be the root cause. Under misspecification -- where the goal is to learn as well as the best-in-class model up to a multiplicative approximation factor $C\geq 1$ -- we confirm that $C$ indeed grows with $H$ for next-token prediction, lending theoretical support to this empirical hypothesis. We then ask whether this mode of error amplification is avoidable algorithmically, computationally, or information-theoretically, and uncover inherent computational-statistical tradeoffs. We show: (1) Information-theoretically, one can avoid error amplification and achieve $C=O(1)$. (2) Next-token prediction can be made robust so as to achieve $C=\tilde O(H)$, representing moderate error amplification, but this is an inherent barrier: any next-token prediction-style objective must suffer $C=\Omega(H)$. (3) For the natural testbed of autoregressive linear models, no computationally efficient algorithm can achieve sub-polynomial approximation factor $C=e^{(\log H)^{1-\Omega(1)}}$; however, at least for binary token spaces, one can smoothly trade compute for statistical power and improve on $C=\Omega(H)$ in sub-exponential time. Our results have consequences in the more general setting of imitation learning, where the widely-used behavior cloning algorithm generalizes next-token prediction.</li>
</ul>

<h3>Title: EquiBench: Benchmarking Code Reasoning Capabilities of Large Language Models via Equivalence Checking</h3>
<ul>
<li><strong>Authors: </strong>Anjiang Wei, Jiannan Cao, Ran Li, Hongyu Chen, Yuhui Zhang, Ziheng Wang, Yaofeng Sun, Yuan Liu, Thiago S. F. X. Teixeira, Diyi Yang, Ke Wang, Alex Aiken</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.PL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12466">https://arxiv.org/abs/2502.12466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12466">https://arxiv.org/pdf/2502.12466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12466]] EquiBench: Benchmarking Code Reasoning Capabilities of Large Language Models via Equivalence Checking(https://arxiv.org/abs/2502.12466)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Equivalence checking, i.e., determining whether two programs produce identical outputs for all possible inputs, underpins a broad range of applications, including software refactoring, testing, and optimization. We present the task of equivalence checking as a new way to evaluate the code reasoning abilities of large language models (LLMs). We introduce EquiBench, a dataset of 2400 program pairs spanning four programming languages and six equivalence categories. These pairs are systematically generated through program analysis, compiler scheduling, and superoptimization, covering nontrivial structural transformations that demand deep semantic reasoning beyond simple syntactic variations. Our evaluation of 17 state-of-the-art LLMs shows that OpenAI o3-mini achieves the highest overall accuracy of 78.0%. In the most challenging categories, the best accuracies are 62.3% and 68.8%, only modestly above the 50% random baseline for binary classification, indicating significant room for improvement in current models' code reasoning capabilities.</li>
</ul>

<h3>Title: MCTS-Judge: Test-Time Scaling in LLM-as-a-Judge for Code Correctness Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Yutong Wang, Pengliang Ji, Chaoqun Yang, Kaixin Li, Ming Hu, Jiaoyang Li, Guillaume Sartoretti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12468">https://arxiv.org/abs/2502.12468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12468">https://arxiv.org/pdf/2502.12468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12468]] MCTS-Judge: Test-Time Scaling in LLM-as-a-Judge for Code Correctness Evaluation(https://arxiv.org/abs/2502.12468)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The LLM-as-a-Judge paradigm shows promise for evaluating generative content but lacks reliability in reasoning-intensive scenarios, such as programming. Inspired by recent advances in reasoning models and shifts in scaling laws, we pioneer bringing test-time computation into LLM-as-a-Judge, proposing MCTS-Judge, a resource-efficient, System-2 thinking framework for code correctness evaluation. MCTS-Judge leverages Monte Carlo Tree Search (MCTS) to decompose problems into simpler, multi-perspective evaluations. Through a node-selection strategy that combines self-assessment based on historical actions in the current trajectory and the Upper Confidence Bound for Trees based on prior rollouts, MCTS-Judge balances global optimization and refinement of the current trajectory. We further designed a high-precision, unit-test-level reward mechanism to encourage the Large Language Model (LLM) to perform line-by-line analysis. Extensive experiments on three benchmarks and five LLMs demonstrate the effectiveness of MCTS-Judge, which improves the base model's accuracy from 41% to 80%, surpassing the o1-series models with 3x fewer tokens. Further evaluations validate the superiority of its reasoning trajectory in logic, analytics, thoroughness, and overall quality, while revealing the test-time scaling law of the LLM-as-a-Judge paradigm.</li>
</ul>

<h3>Title: Reasoning on a Spectrum: Aligning LLMs to System 1 and System 2 Thinking</h3>
<ul>
<li><strong>Authors: </strong>Alireza S. Ziabari, Nona Ghazizadeh, Zhivar Sourati, Farzan Karimi-Malekabadi, Payam Piray, Morteza Dehghani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12470">https://arxiv.org/abs/2502.12470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12470">https://arxiv.org/pdf/2502.12470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12470]] Reasoning on a Spectrum: Aligning LLMs to System 1 and System 2 Thinking(https://arxiv.org/abs/2502.12470)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit impressive reasoning abilities, yet their reliance on structured step-by-step processing reveals a critical limitation. While human cognition fluidly adapts between intuitive, heuristic (System 1) and analytical, deliberative (System 2) reasoning depending on the context, LLMs lack this dynamic flexibility. This rigidity can lead to brittle and unreliable performance when faced with tasks that deviate from their trained patterns. To address this, we create a dataset of 2,000 samples with valid System 1 and System 2 answers, explicitly align LLMs with these reasoning styles, and evaluate their performance across reasoning benchmarks. Our results reveal an accuracy-efficiency trade-off: System 2-aligned models excel in arithmetic and symbolic reasoning, while System 1-aligned models perform better in commonsense tasks. A mechanistic analysis of model responses shows that System 1 models employ more definitive answers, whereas System 2 models demonstrate greater uncertainty. Interpolating between these extremes produces a monotonic transition in reasoning accuracy, preserving coherence. This work challenges the assumption that step-by-step reasoning is always optimal and highlights the need for adapting reasoning strategies based on task demands.</li>
</ul>

<h3>Title: CoCo-CoLa: Evaluating Language Adherence in Multilingual LLMs</h3>
<ul>
<li><strong>Authors: </strong>Elnaz Rahmati, Alireza S. Ziabari, Morteza Dehghani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12476">https://arxiv.org/abs/2502.12476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12476">https://arxiv.org/pdf/2502.12476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12476]] CoCo-CoLa: Evaluating Language Adherence in Multilingual LLMs(https://arxiv.org/abs/2502.12476)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multilingual Large Language Models (LLMs) develop cross-lingual abilities despite being trained on limited parallel data. However, they often struggle to generate responses in the intended language, favoring high-resource languages such as English. In this work, we introduce CoCo-CoLa (Correct Concept - Correct Language), a novel metric to evaluate language adherence in multilingual LLMs. Using fine-tuning experiments on a closed-book QA task across seven languages, we analyze how training in one language affects others' performance. Our findings reveal that multilingual models share task knowledge across languages but exhibit biases in the selection of output language. We identify language-specific layers, showing that final layers play a crucial role in determining output language. Accordingly, we propose a partial training strategy that selectively fine-tunes key layers, improving language adherence while significantly reducing computational cost. Our method achieves comparable or superior performance to full fine-tuning, particularly for low-resource languages, offering a more efficient multilingual adaptation.</li>
</ul>

<h3>Title: Savaal: Scalable Concept-Driven Question Generation to Enhance Human Learning</h3>
<ul>
<li><strong>Authors: </strong>Kimia Noorbakhsh, Joseph Chandler, Pantea Karimi, Mohammad Alizadeh, Hari Balakrishnan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12477">https://arxiv.org/abs/2502.12477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12477">https://arxiv.org/pdf/2502.12477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12477]] Savaal: Scalable Concept-Driven Question Generation to Enhance Human Learning(https://arxiv.org/abs/2502.12477)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Assessing and enhancing human learning through question-answering is vital, yet automating this process remains challenging. While large language models (LLMs) excel at summarization and query responses, their ability to generate meaningful questions for learners is underexplored. We propose Savaal, a scalable question-generation system with three objectives: (i) scalability, enabling question generation from hundreds of pages of text (ii) depth of understanding, producing questions beyond factual recall to test conceptual reasoning, and (iii) domain-independence, automatically generating questions across diverse knowledge areas. Instead of providing an LLM with large documents as context, Savaal improves results with a three-stage processing pipeline. Our evaluation with 76 human experts on 71 papers and PhD dissertations shows that Savaal generates questions that better test depth of understanding by 6.5X for dissertations and 1.5X for papers compared to a direct-prompting LLM baseline. Notably, as document length increases, Savaal's advantages in higher question quality and lower cost become more pronounced.</li>
</ul>

<h3>Title: MSE-Adapter: A Lightweight Plugin Endowing LLMs with the Capability to Perform Multimodal Sentiment Analysis and Emotion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yang Yang, Xunde Dong, Yupeng Qiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12478">https://arxiv.org/abs/2502.12478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12478">https://arxiv.org/pdf/2502.12478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12478]] MSE-Adapter: A Lightweight Plugin Endowing LLMs with the Capability to Perform Multimodal Sentiment Analysis and Emotion Recognition(https://arxiv.org/abs/2502.12478)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Current Multimodal Sentiment Analysis (MSA) and Emotion Recognition in Conversations (ERC) methods based on pre-trained language models exhibit two primary limitations: 1) Once trained for MSA and ERC tasks, these pre-trained language models lose their original generalized capabilities. 2) They demand considerable computational resources. As the size of pre-trained language models continues to grow, training larger multimodal sentiment analysis models using previous approaches could result in unnecessary computational cost. In response to this challenge, we propose \textbf{M}ultimodal \textbf{S}entiment Analysis and \textbf{E}motion Recognition \textbf{Adapter} (MSE-Adapter), a lightweight and adaptable plugin. This plugin enables a large language model (LLM) to carry out MSA or ERC tasks with minimal computational overhead (only introduces approximately 2.6M to 2.8M trainable parameters upon the 6/7B models), while preserving the intrinsic capabilities of the LLM. In the MSE-Adapter, the Text-Guide-Mixer (TGM) module is introduced to establish explicit connections between non-textual and textual modalities through the Hadamard product. This allows non-textual modalities to better align with textual modalities at the feature level, promoting the generation of higher-quality pseudo tokens. Extensive experiments were conducted on four public English and Chinese datasets using consumer-grade GPUs and open-source LLMs (Qwen-1.8B, ChatGLM3-6B-base, and LLaMA2-7B) as the backbone. The results demonstrate the effectiveness of the proposed plugin. The code will be released on GitHub after a blind review.</li>
</ul>

<h3>Title: MotifBench: A standardized protein design benchmark for motif-scaffolding problems</h3>
<ul>
<li><strong>Authors: </strong>Zhuoqi Zheng, Bo Zhang, Kieran Didi, Kevin K. Yang, Jason Yim, Joseph L. Watson, Hai-Feng Chen, Brian L. Trippe</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12479">https://arxiv.org/abs/2502.12479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12479">https://arxiv.org/pdf/2502.12479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12479]] MotifBench: A standardized protein design benchmark for motif-scaffolding problems(https://arxiv.org/abs/2502.12479)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The motif-scaffolding problem is a central task in computational protein design: Given the coordinates of atoms in a geometry chosen to confer a desired biochemical function (a motif), the task is to identify diverse protein structures (scaffolds) that include the motif and maintain its geometry. Significant recent progress on motif-scaffolding has been made due to computational evaluation with reliable protein structure prediction and fixed-backbone sequence design methods. However, significant variability in evaluation strategies across publications has hindered comparability of results, challenged reproducibility, and impeded robust progress. In response we introduce MotifBench, comprising (1) a precisely specified pipeline and evaluation metrics, (2) a collection of 30 benchmark problems, and (3) an implementation of this benchmark and leaderboard at this http URL. The MotifBench test cases are more difficult compared to earlier benchmarks, and include protein design problems for which solutions are known but on which, to the best of our knowledge, state-of-the-art methods fail to identify any solution.</li>
</ul>

<h3>Title: The Knowledge Microscope: Features as Better Analytical Lenses than Neurons</h3>
<ul>
<li><strong>Authors: </strong>Yuheng Chen, Pengfei Cao, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12483">https://arxiv.org/abs/2502.12483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12483">https://arxiv.org/pdf/2502.12483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12483]] The Knowledge Microscope: Features as Better Analytical Lenses than Neurons(https://arxiv.org/abs/2502.12483)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, interpretability</a></li>
<li><strong>Abstract: </strong>Previous studies primarily utilize MLP neurons as units of analysis for understanding the mechanisms of factual knowledge in Language Models (LMs); however, neurons suffer from polysemanticity, leading to limited knowledge expression and poor interpretability. In this paper, we first conduct preliminary experiments to validate that Sparse Autoencoders (SAE) can effectively decompose neurons into features, which serve as alternative analytical units. With this established, our core findings reveal three key advantages of features over neurons: (1) Features exhibit stronger influence on knowledge expression and superior interpretability. (2) Features demonstrate enhanced monosemanticity, showing distinct activation patterns between related and unrelated facts. (3) Features achieve better privacy protection than neurons, demonstrated through our proposed FeatureEdit method, which significantly outperforms existing neuron-based approaches in erasing privacy-sensitive information from this http URL and dataset will be available.</li>
</ul>

<h3>Title: Safe at the Margins: A General Approach to Safety Alignment in Low-Resource English Languages -- A Singlish Case Study</h3>
<ul>
<li><strong>Authors: </strong>Isaac Lim, Shaun Khoo, Watson Chua, Goh Jiayi, Jessica Foo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12485">https://arxiv.org/abs/2502.12485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12485">https://arxiv.org/pdf/2502.12485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12485]] Safe at the Margins: A General Approach to Safety Alignment in Low-Resource English Languages -- A Singlish Case Study(https://arxiv.org/abs/2502.12485)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To ensure safe usage, Large Language Models (LLMs) typically undergo alignment with human-defined values. However, this alignment often relies on primarily English data and is biased towards Western-centric values, limiting its effectiveness in low-resource language settings. In this paper, we describe our approach for aligning SEA-Lion-v2.1-Instruct (a Llama3-8B variant) to minimize toxicity in Singlish, an English creole specific to Singapore. We find that supervised fine-tuning and Kahneman-Tversky Optimization (KTO) on paired and unpaired preferences is more sample efficient and yields significantly better results than Direct Preference Optimization (DPO). Our analysis reveals that DPO implicitly enforces a weaker safety objective than KTO, and that SFT complements KTO by improving training stability. Finally, we introduce a simple but novel modification to KTO, KTO-S, which improves training stability through better gradient exploitation. Overall, we present a general approach for safety alignment conducive to low-resource English languages, successfully reducing toxicity by 99\% on our Singlish benchmark, with gains generalizing to the broader TOXIGEN dataset while maintaining strong performance across standard LLM benchmarks.</li>
</ul>

<h3>Title: EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Xiaoqian Liu, Ke Wang, Yongbin Li, Yuchuan Wu, Wentao Ma, Aobo Kong, Fei Huang, Jianbin Jiao, Junge Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12486">https://arxiv.org/abs/2502.12486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12486">https://arxiv.org/pdf/2502.12486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12486]] EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning(https://arxiv.org/abs/2502.12486)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning-an ability to navigate dynamic environments and align long-term goals amidst uncertainty. Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts. To address these issues, we propose explicit policy optimization (EPO) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior. To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL) using process rewards and iterative self-play, without supervised fine-tuning (SFT) as a preliminary step. Experiments across social and physical domains demonstrate EPO's ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in EPO and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications.</li>
</ul>

<h3>Title: Enhancing Audio-Visual Spiking Neural Networks through Semantic-Alignment and Cross-Modal Residual Learning</h3>
<ul>
<li><strong>Authors: </strong>Xiang He, Dongcheng Zhao, Yiting Dong, Guobin Shen, Xin Yang, Yi Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12488">https://arxiv.org/abs/2502.12488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12488">https://arxiv.org/pdf/2502.12488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12488]] Enhancing Audio-Visual Spiking Neural Networks through Semantic-Alignment and Cross-Modal Residual Learning(https://arxiv.org/abs/2502.12488)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Humans interpret and perceive the world by integrating sensory information from multiple modalities, such as vision and hearing. Spiking Neural Networks (SNNs), as brain-inspired computational models, exhibit unique advantages in emulating the brain's information processing mechanisms. However, existing SNN models primarily focus on unimodal processing and lack efficient cross-modal information fusion, thereby limiting their effectiveness in real-world multimodal scenarios. To address this challenge, we propose a semantic-alignment cross-modal residual learning (S-CMRL) framework, a Transformer-based multimodal SNN architecture designed for effective audio-visual integration. S-CMRL leverages a spatiotemporal spiking attention mechanism to extract complementary features across modalities, and incorporates a cross-modal residual learning strategy to enhance feature integration. Additionally, a semantic alignment optimization mechanism is introduced to align cross-modal features within a shared semantic space, improving their consistency and complementarity. Extensive experiments on three benchmark datasets CREMA-D, UrbanSound8K-AV, and MNISTDVS-NTIDIGITS demonstrate that S-CMRL significantly outperforms existing multimodal SNN methods, achieving the state-of-the-art performance. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: PKE and ABE with Collusion-Resistant Secure Key Leasing</h3>
<ul>
<li><strong>Authors: </strong>Fuyuki Kitagawa, Ryo Nishimaki, Nikhil Pappu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12491">https://arxiv.org/abs/2502.12491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12491">https://arxiv.org/pdf/2502.12491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12491]] PKE and ABE with Collusion-Resistant Secure Key Leasing(https://arxiv.org/abs/2502.12491)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Secure key leasing (SKL) is an advanced encryption functionality that allows a secret key holder to generate a quantum decryption key and securely lease it to a user. Once the user returns the quantum decryption key (or provides a classical certificate confirming its deletion), they lose their decryption capability. Previous works on public key encryption with SKL (PKE-SKL) have only considered the single-key security model, where the adversary receives at most one quantum decryption key. However, this model does not accurately reflect real-world applications of PKE-SKL. To address this limitation, we introduce collusion-resistant security for PKE-SKL (denoted as PKE-CR-SKL). In this model, the adversary can adaptively obtain multiple quantum decryption keys and access a verification oracle which validates the correctness of queried quantum decryption keys. Importantly, the size of the public key and ciphertexts must remain independent of the total number of generated quantum decryption keys. We present the following constructions: - A PKE-CR-SKL scheme based on the learning with errors (LWE) assumption. - An attribute-based encryption scheme with collusion-resistant SKL (ABE-CR-SKL), also based on the LWE assumption. - An ABE-CR-SKL scheme with classical certificates, relying on multi-input ABE with polynomial arity.</li>
</ul>

<h3>Title: EDGE: Efficient Data Selection for LLM Agents via Guideline Effectiveness</h3>
<ul>
<li><strong>Authors: </strong>Yunxiao Zhang, Guanming Xiong, Haochen Li, Wen Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12494">https://arxiv.org/abs/2502.12494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12494">https://arxiv.org/pdf/2502.12494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12494]] EDGE: Efficient Data Selection for LLM Agents via Guideline Effectiveness(https://arxiv.org/abs/2502.12494)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable capabilities as AI agents. However, existing methods for enhancing LLM-agent abilities often lack a focus on data quality, leading to inefficiencies and suboptimal results in both fine-tuning and prompt engineering. To address this issue, we introduce EDGE, a novel approach for identifying informative samples without needing golden answers. We propose the Guideline Effectiveness (GE) metric, which selects challenging samples by measuring the impact of human-provided guidelines in multi-turn interaction tasks. A low GE score indicates that the human expertise required for a sample is missing from the guideline, making the sample more informative. By selecting samples with low GE scores, we can improve the efficiency and outcomes of both prompt engineering and fine-tuning processes for LLMs. Extensive experiments validate the performance of our method. Our method achieves competitive results on the HotpotQA and WebShop and datasets, requiring 75\% and 50\% less data, respectively, while outperforming existing methods. We also provide a fresh perspective on the data quality of LLM-agent fine-tuning.</li>
</ul>

<h3>Title: SoK: Understanding Vulnerabilities in the Large Language Model Supply Chain</h3>
<ul>
<li><strong>Authors: </strong>Shenao Wang, Yanjie Zhao, Zhao Liu, Quanchen Zou, Haoyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12497">https://arxiv.org/abs/2502.12497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12497">https://arxiv.org/pdf/2502.12497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12497]] SoK: Understanding Vulnerabilities in the Large Language Model Supply Chain(https://arxiv.org/abs/2502.12497)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) transform artificial intelligence, driving advancements in natural language understanding, text generation, and autonomous systems. The increasing complexity of their development and deployment introduces significant security challenges, particularly within the LLM supply chain. However, existing research primarily focuses on content safety, such as adversarial attacks, jailbreaking, and backdoor attacks, while overlooking security vulnerabilities in the underlying software systems. To address this gap, this study systematically analyzes 529 vulnerabilities reported across 75 prominent projects spanning 13 lifecycle stages. The findings show that vulnerabilities are concentrated in the application (50.3%) and model (42.7%) layers, with improper resource control (45.7%) and improper neutralization (25.1%) identified as the leading root causes. Additionally, while 56.7% of the vulnerabilities have available fixes, 8% of these patches are ineffective, resulting in recurring vulnerabilities. This study underscores the challenges of securing the LLM ecosystem and provides actionable insights to guide future research and mitigation strategies.</li>
</ul>

<h3>Title: Efficient OpAmp Adaptation for Zoom Attention to Golden Contexts</h3>
<ul>
<li><strong>Authors: </strong>Haoyuan Wu, Rui Ming, Haisheng Zheng, Zhuolun He, Bei Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12502">https://arxiv.org/abs/2502.12502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12502">https://arxiv.org/pdf/2502.12502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12502]] Efficient OpAmp Adaptation for Zoom Attention to Golden Contexts(https://arxiv.org/abs/2502.12502)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown significant promise in question-answering (QA) tasks, particularly in retrieval-augmented generation (RAG) scenarios and long-context applications. However, their performance is hindered by noisy reference documents, which often distract from essential information. Despite fine-tuning efforts, Transformer-based architectures struggle to prioritize relevant content. This is evidenced by their tendency to allocate disproportionate attention to irrelevant or later-positioned documents. Recent work proposes the differential attention mechanism to address this issue, but this mechanism is limited by an unsuitable common-mode rejection ratio (CMRR) and high computational costs. Inspired by the operational amplifier (OpAmp), we propose the OpAmp adaptation to address these challenges, which is implemented with adapters efficiently. By integrating the adapter into pre-trained Transformer blocks, our approach enhances focus on the golden context without costly training from scratch. Empirical evaluations on noisy-context benchmarks reveal that our Qwen2.5-OpAmp-72B model, trained with our OpAmp adaptation, surpasses the performance of state-of-the-art LLMs, including DeepSeek-V3 and GPT-4o.</li>
</ul>

<h3>Title: Mixture of Attention Yields Accurate Results for Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Xuechen Li, Yupeng Li, Jian Liu, Xiaolin Jin, Tian Yang, Xin Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12507">https://arxiv.org/abs/2502.12507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12507">https://arxiv.org/pdf/2502.12507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12507]] Mixture of Attention Yields Accurate Results for Tabular Data(https://arxiv.org/abs/2502.12507)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Tabular data inherently exhibits significant feature heterogeneity, but existing transformer-based methods lack specialized mechanisms to handle this property. To bridge the gap, we propose MAYA, an encoder-decoder transformer-based framework. In the encoder, we design a Mixture of Attention (MOA) that constructs multiple parallel attention branches and averages the features at each branch, effectively fusing heterogeneous features while limiting parameter growth. Additionally, we employ collaborative learning with a dynamic consistency weight constraint to produce more robust representations. In the decoder stage, cross-attention is utilized to seamlessly integrate tabular data with corresponding label features. This dual-attention mechanism effectively captures both intra-instance and inter-instance interactions. We evaluate the proposed method on a wide range of datasets and compare it with other state-of-the-art transformer-based methods. Extensive experiments demonstrate that our model achieves superior performance among transformer-based methods in both tabular classification and regression tasks.</li>
</ul>

<h3>Title: Understanding Generalization in Transformers: Error Bounds and Training Dynamics Under Benign and Harmful Overfitting</h3>
<ul>
<li><strong>Authors: </strong>Yingying Zhang, Zhenyu Wu, Jian Li, Yong Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12508">https://arxiv.org/abs/2502.12508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12508">https://arxiv.org/pdf/2502.12508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12508]] Understanding Generalization in Transformers: Error Bounds and Training Dynamics Under Benign and Harmful Overfitting(https://arxiv.org/abs/2502.12508)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers serve as the foundational architecture for many successful large-scale models, demonstrating the ability to overfit the training data while maintaining strong generalization on unseen data, a phenomenon known as benign overfitting. However, research on how the training dynamics influence error bounds within the context of benign overfitting has been limited. This paper addresses this gap by developing a generalization theory for a two-layer transformer with labeled flip noise. Specifically, we present generalization error bounds for both benign and harmful overfitting under varying signal-to-noise ratios (SNR), where the training dynamics are categorized into three distinct stages, each with its corresponding error bounds. Additionally, we conduct extensive experiments to identify key factors that influence test errors in transformers. Our experimental results align closely with the theoretical predictions, validating our findings.</li>
</ul>

<h3>Title: LegalCore: A Dataset for Legal Documents Event Coreference Resolution</h3>
<ul>
<li><strong>Authors: </strong>Kangda Wei, Xi Shi, Jonathan Tong, Sai Ramana Reddy, Anandhavelu Natarajan, Rajiv Jain, Aparna Garimella, Ruihong Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12509">https://arxiv.org/abs/2502.12509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12509">https://arxiv.org/pdf/2502.12509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12509]] LegalCore: A Dataset for Legal Documents Event Coreference Resolution(https://arxiv.org/abs/2502.12509)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recognizing events and their coreferential mentions in a document is essential for understanding semantic meanings of text. The existing research on event coreference resolution is mostly limited to news articles. In this paper, we present the first dataset for the legal domain, LegalCore, which has been annotated with comprehensive event and event coreference information. The legal contract documents we annotated in this dataset are several times longer than news articles, with an average length of around 25k tokens per document. The annotations show that legal documents have dense event mentions and feature both short-distance and super long-distance coreference links between event mentions. We further benchmark mainstream Large Language Models (LLMs) on this dataset for both event detection and event coreference resolution tasks, and find that this dataset poses significant challenges for state-of-the-art open-source and proprietary LLMs, which perform significantly worse than a supervised baseline. We will publish the dataset as well as the code.</li>
</ul>

<h3>Title: Aspect-Guided Multi-Level Perturbation Analysis of Large Language Models in Automated Peer Review</h3>
<ul>
<li><strong>Authors: </strong>Jiatao Li, Yanheng Li, Xinyu Hu, Mingqi Gao, Xiaojun Wan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12510">https://arxiv.org/abs/2502.12510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12510">https://arxiv.org/pdf/2502.12510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12510]] Aspect-Guided Multi-Level Perturbation Analysis of Large Language Models in Automated Peer Review(https://arxiv.org/abs/2502.12510)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We propose an aspect-guided, multi-level perturbation framework to evaluate the robustness of Large Language Models (LLMs) in automated peer review. Our framework explores perturbations in three key components of the peer review process-papers, reviews, and rebuttals-across several quality aspects, including contribution, soundness, presentation, tone, and completeness. By applying targeted perturbations and examining their effects on both LLM-as-Reviewer and LLM-as-Meta-Reviewer, we investigate how aspect-based manipulations, such as omitting methodological details from papers or altering reviewer conclusions, can introduce significant biases in the review process. We identify several potential vulnerabilities: review conclusions that recommend a strong reject may significantly influence meta-reviews, negative or misleading reviews may be wrongly interpreted as thorough, and incomplete or hostile rebuttals can unexpectedly lead to higher acceptance rates. Statistical tests show that these biases persist under various Chain-of-Thought prompting strategies, highlighting the lack of robust critical evaluation in current LLMs. Our framework offers a practical methodology for diagnosing these vulnerabilities, thereby contributing to the development of more reliable and robust automated reviewing systems.</li>
</ul>

<h3>Title: RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm</h3>
<ul>
<li><strong>Authors: </strong>Tiancheng Gu, Kaicheng Yang, Chaoyi Zhang, Yin Xie, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai, Jiankang Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12513">https://arxiv.org/abs/2502.12513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12513">https://arxiv.org/pdf/2502.12513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12513]] RealSyn: An Effective and Scalable Multimodal Interleaved Document Transformation Paradigm(https://arxiv.org/abs/2502.12513)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of non-paired data, such as multimodal interleaved documents, remains underutilized for vision-language representation learning. To fully leverage these unpaired documents, we initially establish a Real-World Data Extraction pipeline to extract high-quality images and texts. Then we design a hierarchical retrieval method to efficiently associate each image with multiple semantically relevant realistic texts. To further enhance fine-grained visual information, we propose an image semantic augmented generation module for synthetic text production. Furthermore, we employ a semantic balance sampling strategy to improve dataset diversity, enabling better learning of long-tail concepts. Based on these innovations, we construct RealSyn, a dataset combining realistic and synthetic texts, available in three scales: 15M, 30M, and 100M. Extensive experiments demonstrate that RealSyn effectively advances vision-language representation learning and exhibits strong scalability. Models pre-trained on RealSyn achieve state-of-the-art performance on multiple downstream tasks. To facilitate future research, the RealSyn dataset and pre-trained model weights are released at this https URL.</li>
</ul>

<h3>Title: Can LLMs Extract Frame-Semantic Arguments?</h3>
<ul>
<li><strong>Authors: </strong>Jacob Devasier, Rishabh Mediratta, Chengkai Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12516">https://arxiv.org/abs/2502.12516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12516">https://arxiv.org/pdf/2502.12516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12516]] Can LLMs Extract Frame-Semantic Arguments?(https://arxiv.org/abs/2502.12516)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Frame-semantic parsing is a critical task in natural language understanding, yet the ability of large language models (LLMs) to extract frame-semantic arguments remains underexplored. This paper presents a comprehensive evaluation of LLMs on frame-semantic argument identification, analyzing the impact of input representation formats, model architectures, and generalization to unseen and out-of-domain samples. Our experiments, spanning models from 0.5B to 78B parameters, reveal that JSON-based representations significantly enhance performance, and while larger models generally perform better, smaller models can achieve competitive results through fine-tuning. We also introduce a novel approach to frame identification leveraging predicted frame elements, achieving state-of-the-art performance on ambiguous targets. Despite strong generalization capabilities, our analysis finds that LLMs still struggle with out-of-domain data.</li>
</ul>

<h3>Title: SAFEERASER: Enhancing Safety in Multimodal Large Language Models through Multimodal Machine Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Junkai Chen, Zhijie Deng, Kening Zheng, Yibo Yan, Shuliang Liu, PeiJun Wu, Peijie Jiang, Jia Liu, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12520">https://arxiv.org/abs/2502.12520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12520">https://arxiv.org/pdf/2502.12520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12520]] SAFEERASER: Enhancing Safety in Multimodal Large Language Models through Multimodal Machine Unlearning(https://arxiv.org/abs/2502.12520)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>As Multimodal Large Language Models (MLLMs) develop, their potential security issues have become increasingly prominent. Machine Unlearning (MU), as an effective strategy for forgetting specific knowledge in training data, has been widely used in privacy protection. However, MU for safety in MLLM has yet to be fully explored. To address this issue, we propose SAFEERASER, a safety unlearning benchmark for MLLMs, consisting of 3,000 images and 28.8K VQA pairs. We comprehensively evaluate unlearning methods from two perspectives: forget quality and model utility. Our findings show that existing MU methods struggle to maintain model performance while implementing the forget operation and often suffer from over-forgetting. Hence, we introduce Prompt Decouple (PD) Loss to alleviate over-forgetting through decouple prompt during unlearning process. To quantitatively measure over-forgetting mitigated by PD Loss, we propose a new metric called Safe Answer Refusal Rate (SARR). Experimental results demonstrate that combining PD Loss with existing unlearning methods can effectively prevent over-forgetting and achieve a decrease of 79.5% in the SARR metric of LLaVA-7B and LLaVA-13B, while maintaining forget quality and model utility. Our code and dataset will be released upon acceptance. Warning: This paper contains examples of harmful language and images, and reader discretion is recommended.</li>
</ul>

<h3>Title: From Abstract to Actionable: Pairwise Shapley Values for Explainable AI</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Xu, Hung Chau, Angela Burden</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12525">https://arxiv.org/abs/2502.12525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12525">https://arxiv.org/pdf/2502.12525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12525]] From Abstract to Actionable: Pairwise Shapley Values for Explainable AI(https://arxiv.org/abs/2502.12525)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability</a></li>
<li><strong>Abstract: </strong>Explainable AI (XAI) is critical for ensuring transparency, accountability, and trust in machine learning systems as black-box models are increasingly deployed within high-stakes domains. Among XAI methods, Shapley values are widely used for their fairness and consistency axioms. However, prevalent Shapley value approximation methods commonly rely on abstract baselines or computationally intensive calculations, which can limit their interpretability and scalability. To address such challenges, we propose Pairwise Shapley Values, a novel framework that grounds feature attributions in explicit, human-relatable comparisons between pairs of data instances proximal in feature space. Our method introduces pairwise reference selection combined with single-value imputation to deliver intuitive, model-agnostic explanations while significantly reducing computational overhead. Here, we demonstrate that Pairwise Shapley Values enhance interpretability across diverse regression and classification scenarios--including real estate pricing, polymer property prediction, and drug discovery datasets. We conclude that the proposed methods enable more transparent AI systems and advance the real-world applicability of XAI.</li>
</ul>

<h3>Title: Comprehensive Assessment and Analysis for NSFW Content Erasure in Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Die Chen, Zhiwen Li, Cen Chen, Xiaodan Li, Jinyan Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12527">https://arxiv.org/abs/2502.12527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12527">https://arxiv.org/pdf/2502.12527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12527]] Comprehensive Assessment and Analysis for NSFW Content Erasure in Text-to-Image Diffusion Models(https://arxiv.org/abs/2502.12527)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) diffusion models have gained widespread application across various domains, demonstrating remarkable creative potential. However, the strong generalization capabilities of these models can inadvertently led they to generate NSFW content even with efforts on filtering NSFW content from the training dataset, posing risks to their safe deployment. While several concept erasure methods have been proposed to mitigate this issue, a comprehensive evaluation of their effectiveness remains absent. To bridge this gap, we present the first systematic investigation of concept erasure methods for NSFW content and its sub-themes in text-to-image diffusion models. At the task level, we provide a holistic evaluation of 11 state-of-the-art baseline methods with 14 variants. Specifically, we analyze these methods from six distinct assessment perspectives, including three conventional perspectives, i.e., erasure proportion, image quality, and semantic alignment, and three new perspectives, i.e., excessive erasure, the impact of explicit and implicit unsafe prompts, and robustness. At the tool level, we perform a detailed toxicity analysis of NSFW datasets and compare the performance of different NSFW classifiers, offering deeper insights into their performance alongside a compilation of comprehensive evaluation metrics. Our benchmark not only systematically evaluates concept erasure methods, but also delves into the underlying factors influencing their performance at the insight level. By synthesizing insights from various evaluation perspectives, we provide a deeper understanding of the challenges and opportunities in the field, offering actionable guidance and inspiration for advancing research and practical applications in concept erasure.</li>
</ul>

<h3>Title: Policy-to-Language: Train LLMs to Explain Decisions with Flow-Matching Generated Rewards</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Yang, Liang Zeng, Heng Dong, Chao Yu, Xiaoran Wu, Huazhong Yang, Yu Wang, Milind Tambe, Tonghan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12530">https://arxiv.org/abs/2502.12530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12530">https://arxiv.org/pdf/2502.12530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12530]] Policy-to-Language: Train LLMs to Explain Decisions with Flow-Matching Generated Rewards(https://arxiv.org/abs/2502.12530)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As humans increasingly share environments with diverse agents powered by RL, LLMs, and beyond, the ability to explain their policies in natural language will be vital for reliable coexistence. In this paper, we build a model-agnostic explanation generator based on an LLM. The technical novelty is that the rewards for training this LLM are generated by a generative flow matching model. This model has a specially designed structure with a hidden layer merged with an LLM to harness the linguistic cues of explanations into generating appropriate rewards. Experiments on both RL and LLM tasks demonstrate that our method can generate dense and effective rewards while saving on expensive human feedback; it thus enables effective explanations and even improves the accuracy of the decisions in original tasks.</li>
</ul>

<h3>Title: NoKSR: Kernel-Free Neural Surface Reconstruction via Point Cloud Serialization</h3>
<ul>
<li><strong>Authors: </strong>Zhen Li, Weiwei Sun, Shrisudhan Govindarajan, Shaobo Xia, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12534">https://arxiv.org/abs/2502.12534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12534">https://arxiv.org/pdf/2502.12534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12534]] NoKSR: Kernel-Free Neural Surface Reconstruction via Point Cloud Serialization(https://arxiv.org/abs/2502.12534)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present a novel approach to large-scale point cloud surface reconstruction by developing an efficient framework that converts an irregular point cloud into a signed distance field (SDF). Our backbone builds upon recent transformer-based architectures (i.e., PointTransformerV3), that serializes the point cloud into a locality-preserving sequence of tokens. We efficiently predict the SDF value at a point by aggregating nearby tokens, where fast approximate neighbors can be retrieved thanks to the serialization. We serialize the point cloud at different levels/scales, and non-linearly aggregate a feature to predict the SDF value. We show that aggregating across multiple scales is critical to overcome the approximations introduced by the serialization (i.e. false negatives in the neighborhood). Our frameworks sets the new state-of-the-art in terms of accuracy and efficiency (better or similar performance with half the latency of the best prior method, coupled with a simpler implementation), particularly on outdoor datasets where sparse-grid methods have shown limited performance.</li>
</ul>

<h3>Title: When Segmentation Meets Hyperspectral Image: New Paradigm for Hyperspectral Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Weilian Zhou, Weixuan Xie, Sei-ichiro Kamata, Man Sing Wong, Huiying (Cynthia)Hou, Haipeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12541">https://arxiv.org/abs/2502.12541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12541">https://arxiv.org/pdf/2502.12541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12541]] When Segmentation Meets Hyperspectral Image: New Paradigm for Hyperspectral Image Classification(https://arxiv.org/abs/2502.12541)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Hyperspectral image (HSI) classification is a cornerstone of remote sensing, enabling precise material and land-cover identification through rich spectral information. While deep learning has driven significant progress in this task, small patch-based classifiers, which account for over 90% of the progress, face limitations: (1) the small patch (e.g., 7x7, 9x9)-based sampling approach considers a limited receptive field, resulting in insufficient spatial structural information critical for object-level identification and noise-like misclassifications even within uniform regions; (2) undefined optimal patch sizes lead to coarse label predictions, which degrade performance; and (3) a lack of multi-shape awareness around objects. To address these challenges, we draw inspiration from large-scale image segmentation techniques, which excel at handling object boundaries-a capability essential for semantic labeling in HSI classification. However, their application remains under-explored in this task due to (1) the prevailing notion that larger patch sizes degrade performance, (2) the extensive unlabeled regions in HSI groundtruth, and (3) the misalignment of input shapes between HSI data and segmentation models. Thus, in this study, we propose a novel paradigm and baseline, HSIseg, for HSI classification that leverages segmentation techniques combined with a novel Dynamic Shifted Regional Transformer (DSRT) to overcome these challenges. We also introduce an intuitive progressive learning framework with adaptive pseudo-labeling to iteratively incorporate unlabeled regions into the training process, thereby advancing the application of segmentation techniques. Additionally, we incorporate auxiliary data through multi-source data collaboration, promoting better feature interaction. Validated on five public HSI datasets, our proposal outperforms state-of-the-art methods.</li>
</ul>

<h3>Title: Mixing Algorithm for Extending the Tiers of the Unapparent Information Send through the Audio Streams</h3>
<ul>
<li><strong>Authors: </strong>Sachith Dassanayaka</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12544">https://arxiv.org/abs/2502.12544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12544">https://arxiv.org/pdf/2502.12544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12544]] Mixing Algorithm for Extending the Tiers of the Unapparent Information Send through the Audio Streams(https://arxiv.org/abs/2502.12544)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, robust</a></li>
<li><strong>Abstract: </strong>Usage of the fast development of real-life digital applications in modern technology should guarantee novel and efficient way-outs of their protection. Encryption facilitates the data hiding. With the express development of technology, people tend to figure out a method that is capable of hiding a message and the survival of the message. Secrecy and efficiency can be obtained through steganographic involvement, a novel approach, along with multipurpose audio streams. Generally, steganography advantages are not used among industry and learners even though it is extensively discussed in the present information world. Information hiding in audio files is exclusively inspiring due to the compassion of the Human Auditory System (HAS). The proposed resolution supports Advance Encryption Standard (AES)256 key encryption and tolerates all existing audio file types as the container. This paper analyzes and proposes a way out according to the performance based on robustness, security, and hiding capacity. Furthermore, a survey of audio steganography applications, as well as a proposed resolution, is discussed in this paper.</li>
</ul>

<h3>Title: MomentSeeker: A Comprehensive Benchmark and A Strong Baseline For Moment Retrieval Within Long Videos</h3>
<ul>
<li><strong>Authors: </strong>Huaying Yuan, Jian Ni, Yueze Wang, Junjie Zhou, Zhengyang Liang, Zheng Liu, Zhao Cao, Zhicheng Dou, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12558">https://arxiv.org/abs/2502.12558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12558">https://arxiv.org/pdf/2502.12558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12558]] MomentSeeker: A Comprehensive Benchmark and A Strong Baseline For Moment Retrieval Within Long Videos(https://arxiv.org/abs/2502.12558)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval augmented generation (RAG) holds great promise in addressing challenges associated with long video understanding. These methods retrieve useful moments from long videos for their presented tasks, thereby enabling multimodal large language models (MLLMs) to generate high-quality answers in a cost-effective way. In this work, we present MomentSeeker, a comprehensive benchmark to evaluate retrieval models' performance in handling general long-video moment retrieval (LVMR) tasks. MomentSeeker offers three key advantages. First, it incorporates long videos of over 500 seconds on average, making it the first benchmark specialized for long-video moment retrieval. Second, it covers a wide range of task categories (including Moment Search, Caption Alignment, Image-conditioned Moment Search, and Video-conditioned Moment Search) and diverse application scenarios (e.g., sports, movies, cartoons, and ego), making it a comprehensive tool for assessing retrieval models' general LVMR performance. Additionally, the evaluation tasks are carefully curated through human annotation, ensuring the reliability of assessment. We further fine-tune an MLLM-based LVMR retriever on synthetic data, which demonstrates strong performance on our benchmark. We perform extensive experiments with various popular multimodal retrievers based on our benchmark, whose results highlight the challenges of LVMR and limitations for existing methods. Our created resources will be shared with community to advance future research in this field.</li>
</ul>

<h3>Title: How does a Language-Specific Tokenizer affect LLMs?</h3>
<ul>
<li><strong>Authors: </strong>Jean Seo, Jaeyoon Kim, SungJoo Byun, Hyopil Shin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12560">https://arxiv.org/abs/2502.12560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12560">https://arxiv.org/pdf/2502.12560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12560]] How does a Language-Specific Tokenizer affect LLMs?(https://arxiv.org/abs/2502.12560)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The necessity of language-specific tokenizers intuitively appears crucial for effective natural language processing, yet empirical analyses on their significance and underlying reasons are lacking. This study explores how language-specific tokenizers influence the behavior of Large Language Models predominantly trained with English text data, through the case study of Korean. The research unfolds in two main stages: (1) the development of a Korean-specific extended tokenizer and (2) experiments to compare models with the basic tokenizer and the extended tokenizer through various Next Token Prediction tasks. Our in-depth analysis reveals that the extended tokenizer decreases confidence in incorrect predictions during generation and reduces cross-entropy in complex tasks, indicating a tendency to produce less nonsensical outputs. Consequently, the extended tokenizer provides stability during generation, potentially leading to higher performance in downstream tasks.</li>
</ul>

<h3>Title: SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Weikai Lu, Hao Peng, Huiping Zhuang, Cen Chen, Ziqian Zeng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12562">https://arxiv.org/abs/2502.12562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12562">https://arxiv.org/pdf/2502.12562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12562]] SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings(https://arxiv.org/abs/2502.12562)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have serious security this http URL safety alignment using multimodal datasets consisting of text and data of additional modalities can effectively enhance MLLM's security, it is costly to construct these datasets. Existing low-resource security alignment methods, including textual alignment, have been found to struggle with the security risks posed by additional modalities. To address this, we propose Synthetic Embedding augmented safety Alignment (SEA), which optimizes embeddings of additional modality through gradient updates to expand textual datasets. This enables multimodal safety alignment training even when only textual data is available. Extensive experiments on image, video, and audio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding on a single RTX3090 GPU within 24 seconds. SEA significantly improves the security of MLLMs when faced with threats from additional modalities. To assess the security risks introduced by video and audio, we also introduced a new benchmark called VA-SafetyBench. High attack success rates across multiple MLLMs validate its challenge. Our code and data will be available at this https URL.</li>
</ul>

<h3>Title: Evaluating Language Models on Grooming Risk Estimation Using Fuzzy Theory</h3>
<ul>
<li><strong>Authors: </strong>Geetanjali Bihani, Tatiana Ringenberg, Julia Rayz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12563">https://arxiv.org/abs/2502.12563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12563">https://arxiv.org/pdf/2502.12563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12563]] Evaluating Language Models on Grooming Risk Estimation Using Fuzzy Theory(https://arxiv.org/abs/2502.12563)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Encoding implicit language presents a challenge for language models, especially in high-risk domains where maintaining high precision is important. Automated detection of online child grooming is one such critical domain, where predators manipulate victims using a combination of explicit and implicit language to convey harmful intentions. While recent studies have shown the potential of Transformer language models like SBERT for preemptive grooming detection, they primarily depend on surface-level features and approximate real victim grooming processes using vigilante and law enforcement conversations. The question of whether these features and approximations are reasonable has not been addressed thus far. In this paper, we address this gap and study whether SBERT can effectively discern varying degrees of grooming risk inherent in conversations, and evaluate its results across different participant groups. Our analysis reveals that while fine-tuning aids language models in learning to assign grooming scores, they show high variance in predictions, especially for contexts containing higher degrees of grooming risk. These errors appear in cases that 1) utilize indirect speech pathways to manipulate victims and 2) lack sexually explicit content. This finding underscores the necessity for robust modeling of indirect speech acts by language models, particularly those employed by predators.</li>
</ul>

<h3>Title: Self Iterative Label Refinement via Robust Unlabeled Learning</h3>
<ul>
<li><strong>Authors: </strong>Hikaru Asano, Tadashi Kozuno, Yukino Baba</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12565">https://arxiv.org/abs/2502.12565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12565">https://arxiv.org/pdf/2502.12565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12565]] Self Iterative Label Refinement via Robust Unlabeled Learning(https://arxiv.org/abs/2502.12565)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have yielded impressive performance on various tasks, yet they often depend on high-quality feedback that can be costly. Self-refinement methods attempt to leverage LLMs' internal evaluation mechanisms with minimal human supervision; however, these approaches frequently suffer from inherent biases and overconfidence, especially in domains where the models lack sufficient internal knowledge, resulting in performance degradation. As an initial step toward enhancing self-refinement for broader applications, we introduce an iterative refinement pipeline that employs the Unlabeled-Unlabeled learning framework to improve LLM-generated pseudo-labels for classification tasks. By exploiting two unlabeled datasets with differing positive class ratios, our approach iteratively denoises and refines the initial pseudo-labels, thereby mitigating the adverse effects of internal biases with minimal human supervision. Evaluations on diverse datasets, including low-resource language corpora, patent classifications, and protein structure categorizations, demonstrate that our method consistently outperforms both initial LLM's classification performance and the self-refinement approaches by cutting-edge models (e.g., GPT-4o and DeepSeek-R1).</li>
</ul>

<h3>Title: DeltaDiff: A Residual-Guided Diffusion Model for Enhanced Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Chao Yang, Yong Fan, Cheng Lu, Zhijing Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12567">https://arxiv.org/abs/2502.12567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12567">https://arxiv.org/pdf/2502.12567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12567]] DeltaDiff: A Residual-Guided Diffusion Model for Enhanced Image Super-Resolution(https://arxiv.org/abs/2502.12567)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, the application of diffusion models in super-resolution tasks has become a popular research direction. Existing work is focused on fully migrating diffusion models to SR tasks. The diffusion model is proposed in the field of image generation, so in order to make the generated results diverse, the diffusion model combines random Gaussian noise and distributed sampling to increase the randomness of the model. However, the essence of super-resolution tasks requires the model to generate high-resolution images with fidelity. Excessive addition of random factors can result in the model generating detailed information that does not belong to the HR image. To address this issue, we propose a new diffusion model called Deltadiff, which uses only residuals between images for diffusion, making the entire diffusion process more stable. The experimental results show that our method surpasses state-of-the-art models and generates results with better fidelity. Our code and model are publicly available at this https URL</li>
</ul>

<h3>Title: A Cognitive Writing Perspective for Constrained Long-Form Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Kaiyang Wan, Honglin Mu, Rui Hao, Haoran Luo, Tianle Gu, Xiuying Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12568">https://arxiv.org/abs/2502.12568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12568">https://arxiv.org/pdf/2502.12568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12568]] A Cognitive Writing Perspective for Constrained Long-Form Text Generation(https://arxiv.org/abs/2502.12568)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Like humans, Large Language Models (LLMs) struggle to generate high-quality long-form text that adheres to strict requirements in a single pass. This challenge is unsurprising, as successful human writing, according to the Cognitive Writing Theory, is a complex cognitive process involving iterative planning, translating, reviewing, and monitoring. Motivated by these cognitive principles, we aim to equip LLMs with human-like cognitive writing capabilities through CogWriter, a novel training-free framework that transforms LLM constrained long-form text generation into a systematic cognitive writing paradigm. Our framework consists of two key modules: (1) a Planning Agent that performs hierarchical planning to decompose the task, and (2) multiple Generation Agents that execute these plans in parallel. The system maintains quality via continuous monitoring and reviewing mechanisms, which evaluate outputs against specified requirements and trigger necessary revisions. CogWriter demonstrates exceptional performance on LongGenBench, a benchmark for complex constrained long-form text generation. Even when using Qwen-2.5-14B as its backbone, CogWriter surpasses GPT-4o by 22% in complex instruction completion accuracy while reliably generating texts exceeding 10,000 words. We hope this cognitive science-inspired approach provides a paradigm for LLM writing advancements: \href{this https URL}{CogWriter}.</li>
</ul>

<h3>Title: GVTNet: Graph Vision Transformer For Face Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Chao Yang, Yong Fan, Cheng Lu, Minghao Yuan, Zhijing Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12570">https://arxiv.org/abs/2502.12570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12570">https://arxiv.org/pdf/2502.12570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12570]] GVTNet: Graph Vision Transformer For Face Super-Resolution(https://arxiv.org/abs/2502.12570)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent advances in face super-resolution research have utilized the Transformer architecture. This method processes the input image into a series of small patches. However, because of the strong correlation between different facial components in facial images. When it comes to super-resolution of low-resolution images, existing algorithms cannot handle the relationships between patches well, resulting in distorted facial components in the super-resolution results. To solve the problem, we propose a transformer architecture based on graph neural networks called graph vision transformer network. We treat each patch as a graph node and establish an adjacency matrix based on the information between patches. In this way, the patch only interacts between neighboring patches, further processing the relationship of facial components. Quantitative and visualization experiments have underscored the superiority of our algorithm over state-of-the-art techniques. Through detailed comparisons, we have demonstrated that our algorithm possesses more advanced super-resolution capabilities, particularly in enhancing facial components. The PyTorch code is available at this https URL</li>
</ul>

<h3>Title: HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading</h3>
<ul>
<li><strong>Authors: </strong>Cheng Luo, Zefan Cai, Hanshi Sun, Jinqi Xiao, Bo Yuan, Wen Xiao, Junjie Hu, Jiawei Zhao, Beidi Chen, Anima Anandkumar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12574">https://arxiv.org/abs/2502.12574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12574">https://arxiv.org/pdf/2502.12574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12574]] HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading(https://arxiv.org/abs/2502.12574)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloads the KV cache to CPU RAM while avoiding the need to fully store the KV cache for any transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise offloading strategy, maintaining only selective attention heads KV cache on the GPU while computing attention output dynamically. Through roofline analysis, we demonstrate that HEADINFER maintains computational efficiency while significantly reducing memory footprint. We evaluate HEADINFER on the Llama-3-8B model with a 1-million-token sequence, reducing the GPU memory footprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage from 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline inference. Notably, HEADINFER enables 4-million-token inference with an 8B model on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without approximation methods.</li>
</ul>

<h3>Title: DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent</h3>
<ul>
<li><strong>Authors: </strong>Pengyu Zhu, Zhenhong Zhou, Yuanhe Zhang, Shilinlu Yan, Kun Wang, Sen Su</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12575">https://arxiv.org/abs/2502.12575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12575">https://arxiv.org/pdf/2502.12575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12575]] DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent(https://arxiv.org/abs/2502.12575)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, steal</a></li>
<li><strong>Abstract: </strong>As LLM-based agents become increasingly prevalent, backdoors can be implanted into agents through user queries or environment feedback, raising critical concerns regarding safety vulnerabilities. However, backdoor attacks are typically detectable by safety audits that analyze the reasoning process of agents. To this end, we propose a novel backdoor implantation strategy called \textbf{Dynamically Encrypted Multi-Backdoor Implantation Attack}. Specifically, we introduce dynamic encryption, which maps the backdoor into benign content, effectively circumventing safety audits. To enhance stealthiness, we further decompose the backdoor into multiple sub-backdoor fragments. Based on these advancements, backdoors are allowed to bypass safety audits significantly. Additionally, we present AgentBackdoorEval, a dataset designed for the comprehensive evaluation of agent backdoor attacks. Experimental results across multiple datasets demonstrate that our method achieves an attack success rate nearing 100\% while maintaining a detection rate of 0\%, illustrating its effectiveness in evading safety audits. Our findings highlight the limitations of existing safety mechanisms in detecting advanced attacks, underscoring the urgent need for more robust defenses against backdoor threats. Code and data are available at this https URL.</li>
</ul>

<h3>Title: A Fuzzy Evaluation of Sentence Encoders on Grooming Risk Classification</h3>
<ul>
<li><strong>Authors: </strong>Geetanjali Bihani, Julia Rayz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12576">https://arxiv.org/abs/2502.12576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12576">https://arxiv.org/pdf/2502.12576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12576]] A Fuzzy Evaluation of Sentence Encoders on Grooming Risk Classification(https://arxiv.org/abs/2502.12576)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>With the advent of social media, children are becoming increasingly vulnerable to the risk of grooming in online settings. Detecting grooming instances in an online conversation poses a significant challenge as the interactions are not necessarily sexually explicit, since the predators take time to build trust and a relationship with their victim. Moreover, predators evade detection using indirect and coded language. While previous studies have fine-tuned Transformers to automatically identify grooming in chat conversations, they overlook the impact of coded and indirect language on model predictions, and how these align with human perceptions of grooming. In this paper, we address this gap and evaluate bi-encoders on the task of classifying different degrees of grooming risk in chat contexts, for three different participant groups, i.e. law enforcement officers, real victims, and decoys. Using a fuzzy-theoretic framework, we map human assessments of grooming behaviors to estimate the actual degree of grooming risk. Our analysis reveals that fine-tuned models fail to tag instances where the predator uses indirect speech pathways and coded language to evade detection. Further, we find that such instances are characterized by a higher presence of out-of-vocabulary (OOV) words in samples, causing the model to misclassify. Our findings highlight the need for more robust models to identify coded language from noisy chat inputs in grooming contexts.</li>
</ul>

<h3>Title: CHATS: Combining Human-Aligned Optimization and Test-Time Sampling for Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Minghao Fu, Guo-Hua Wang, Liangfu Cao, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12579">https://arxiv.org/abs/2502.12579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12579">https://arxiv.org/pdf/2502.12579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12579]] CHATS: Combining Human-Aligned Optimization and Test-Time Sampling for Text-to-Image Generation(https://arxiv.org/abs/2502.12579)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as a dominant approach for text-to-image generation. Key components such as the human preference alignment and classifier-free guidance play a crucial role in ensuring generation quality. However, their independent application in current text-to-image models continues to face significant challenges in achieving strong text-image alignment, high generation quality, and consistency with human aesthetic standards. In this work, we for the first time, explore facilitating the collaboration of human performance alignment and test-time sampling to unlock the potential of text-to-image models. Consequently, we introduce CHATS (Combining Human-Aligned optimization and Test-time Sampling), a novel generative framework that separately models the preferred and dispreferred distributions and employs a proxy-prompt-based sampling strategy to utilize the useful information contained in both distributions. We observe that CHATS exhibits exceptional data efficiency, achieving strong performance with only a small, high-quality funetuning dataset. Extensive experiments demonstrate that CHATS surpasses traditional preference alignment methods, setting new state-of-the-art across various standard benchmarks.</li>
</ul>

<h3>Title: Adaptive Prototype Model for Attribute-based Multi-label Few-shot Action Recognition</h3>
<ul>
<li><strong>Authors: </strong>Juefeng Xiao, Tianqi Xiang, Zhigang Tu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12582">https://arxiv.org/abs/2502.12582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12582">https://arxiv.org/pdf/2502.12582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12582]] Adaptive Prototype Model for Attribute-based Multi-label Few-shot Action Recognition(https://arxiv.org/abs/2502.12582)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In real-world action recognition systems, incorporating more attributes helps achieve a more comprehensive understanding of human behavior. However, using a single model to simultaneously recognize multiple attributes can lead to a decrease in accuracy. In this work, we propose a novel method i.e. Adaptive Attribute Prototype Model (AAPM) for human action recognition, which captures rich action-relevant attribute information and strikes a balance between accuracy and robustness. Firstly, we introduce the Text-Constrain Module (TCM) to incorporate textual information from potential labels, and constrain the construction of different attributes prototype representations. In addition, we explore the Attribute Assignment Method (AAM) to address the issue of training bias and increase robustness during the training this http URL, we construct a new video dataset with attribute-based multi-label called Multi-Kinetics for evaluation, which contains various attribute labels (e.g. action, scene, object, etc.) related to human behavior. Extensive experiments demonstrate that our AAPM achieves the state-of-the-art performance in both attribute-based multi-label few-shot action recognition and single-label few-shot action recognition. The project and dataset are available at an anonymous account this https URL</li>
</ul>

<h3>Title: LongFaith: Enhancing Long-Context Reasoning in LLMs with Faithful Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Cehao Yang, Xueyuan Lin, Chengjin Xu, Xuhui Jiang, Shengjie Ma, Aofan Liu, Hui Xiong, Jian Guo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12583">https://arxiv.org/abs/2502.12583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12583">https://arxiv.org/pdf/2502.12583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12583]] LongFaith: Enhancing Long-Context Reasoning in LLMs with Faithful Synthetic Data(https://arxiv.org/abs/2502.12583)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the growing development of long-context large language models (LLMs), data-centric approaches relying on synthetic data have been hindered by issues related to faithfulness, which limit their effectiveness in enhancing model performance on tasks such as long-context reasoning and question answering (QA). These challenges are often exacerbated by misinformation caused by lack of verification, reasoning without attribution, and potential knowledge conflicts. We propose LongFaith, a novel pipeline for synthesizing faithful long-context reasoning instruction datasets. By integrating ground truth and citation-based reasoning prompts, we eliminate distractions and improve the accuracy of reasoning chains, thus mitigating the need for costly verification processes. We open-source two synthesized datasets, LongFaith-SFT and LongFaith-PO, which systematically address multiple dimensions of faithfulness, including verified reasoning, attribution, and contextual grounding. Extensive experiments on multi-hop reasoning datasets and LongBench demonstrate that models fine-tuned on these datasets significantly improve performance. Our ablation studies highlight the scalability and adaptability of the LongFaith pipeline, showcasing its broad applicability in developing long-context LLMs.</li>
</ul>

<h3>Title: Enhancing Semi-supervised Learning with Noisy Zero-shot Pseudolabels</h3>
<ul>
<li><strong>Authors: </strong>Jichan Chung, Irene Y. Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12584">https://arxiv.org/abs/2502.12584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12584">https://arxiv.org/pdf/2502.12584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12584]] Enhancing Semi-supervised Learning with Noisy Zero-shot Pseudolabels(https://arxiv.org/abs/2502.12584)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Semi-supervised learning (SSL) leverages limited labeled data alongside abundant unlabeled data to address labeling costs in machine learning. While recent foundation models enable zero-shot inference, attempts to integrate these capabilities into SSL through pseudo-labeling have shown mixed results due to unreliable zero-shot predictions. We present ZMT (Zero-Shot Multi-Task Learning), a framework that jointly optimizes zero-shot pseudo-labels and unsupervised representation learning objectives from contemporary SSL approaches. Our method introduces a multi-task learning-based mechanism that incorporates pseudo-labels while ensuring robustness to varying pseudo-label quality. Experiments across 8 datasets in vision, language, and audio domains demonstrate that ZMT reduces error by up to 56% compared to traditional SSL methods, with particularly compelling results when pseudo-labels are noisy and unreliable. ZMT represents a significant step toward making semi-supervised learning more effective and accessible in resource-constrained environments.</li>
</ul>

<h3>Title: PASER: Post-Training Data Selection for Efficient Pruned Large Language Model Recovery</h3>
<ul>
<li><strong>Authors: </strong>Bowei He, Lihao Yin, Hui-Ling Zhen, Xiaokun Zhang, Mingxuan Yuan, Chen Ma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12594">https://arxiv.org/abs/2502.12594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12594">https://arxiv.org/pdf/2502.12594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12594]] PASER: Post-Training Data Selection for Efficient Pruned Large Language Model Recovery(https://arxiv.org/abs/2502.12594)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Model pruning is an effective approach for compressing large language models. However, this process often leads to significant degradation of model capabilities. While post-training techniques such as instruction tuning are commonly employed to recover model performance, existing methods often overlook the uneven deterioration of model capabilities and incur high computational costs. Moreover, some instruction data irrelevant to model capability recovery may introduce negative effects. To address these challenges, we propose the \textbf{P}ost-training d\textbf{A}ta \textbf{S}election method for \textbf{E}fficient pruned large language model \textbf{R}ecovery (\textbf{PASER}). PASER aims to identify instructions where model capabilities are most severely compromised within a certain recovery data budget. Our approach first applies manifold learning and spectral clustering to group recovery data in the semantic space, revealing capability-specific instruction sets. We then adaptively allocate the data budget to different clusters based on the degrees of model capability degradation. In each cluster, we prioritize data samples where model performance has declined dramatically. To mitigate potential negative transfer, we also detect and filter out conflicting or irrelevant recovery data. Extensive experiments demonstrate that PASER significantly outperforms conventional baselines, effectively recovering the general capabilities of pruned LLMs while utilizing merely 4\%-20\% of the original post-training data.</li>
</ul>

<h3>Title: Bring Your Own Knowledge: A Survey of Methods for LLM Knowledge Expansion</h3>
<ul>
<li><strong>Authors: </strong>Mingyang Wang, Alisa Stoll, Lukas Lange, Heike Adel, Hinrich Schütze, Jannik Strötgen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12598">https://arxiv.org/abs/2502.12598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12598">https://arxiv.org/pdf/2502.12598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12598]] Bring Your Own Knowledge: A Survey of Methods for LLM Knowledge Expansion(https://arxiv.org/abs/2502.12598)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Adapting large language models (LLMs) to new and diverse knowledge is essential for their lasting effectiveness in real-world applications. This survey provides an overview of state-of-the-art methods for expanding the knowledge of LLMs, focusing on integrating various knowledge types, including factual information, domain expertise, language proficiency, and user preferences. We explore techniques, such as continual learning, model editing, and retrieval-based explicit adaptation, while discussing challenges like knowledge consistency and scalability. Designed as a guide for researchers and practitioners, this survey sheds light on opportunities for advancing LLMs as adaptable and robust knowledge systems.</li>
</ul>

<h3>Title: Revisiting the Generalization Problem of Low-level Vision Models Through the Lens of Image Deraining</h3>
<ul>
<li><strong>Authors: </strong>Jinfan Hu, Zhiyuan You, Jinjin Gu, Kaiwen Zhu, Tianfan Xue, Chao Dong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12600">https://arxiv.org/abs/2502.12600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12600">https://arxiv.org/pdf/2502.12600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12600]] Revisiting the Generalization Problem of Low-level Vision Models Through the Lens of Image Deraining(https://arxiv.org/abs/2502.12600)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generalization remains a significant challenge for low-level vision models, which often struggle with unseen degradations in real-world scenarios despite their success in controlled benchmarks. In this paper, we revisit the generalization problem in low-level vision models. Image deraining is selected as a case study due to its well-defined and easily decoupled structure, allowing for more effective observation and analysis. Through comprehensive experiments, we reveal that the generalization issue is not primarily due to limited network capacity but rather the failure of existing training strategies, which leads networks to overfit specific degradation patterns. Our findings show that guiding networks to focus on learning the underlying image content, rather than the degradation patterns, is key to improving generalization. We demonstrate that balancing the complexity of background images and degradations in the training data helps networks better fit the image distribution. Furthermore, incorporating content priors from pre-trained generative models significantly enhances generalization. Experiments on both image deraining and image denoising validate the proposed strategies. We believe the insights and solutions will inspire further research and improve the generalization of low-level vision models.</li>
</ul>

<h3>Title: COPU: Conformal Prediction for Uncertainty Quantification in Natural Language Generation</h3>
<ul>
<li><strong>Authors: </strong>Sean Wang, Yicheng Jiang, Yuxin Tang, Lu Cheng, Hanjie Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12601">https://arxiv.org/abs/2502.12601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12601">https://arxiv.org/pdf/2502.12601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12601]] COPU: Conformal Prediction for Uncertainty Quantification in Natural Language Generation(https://arxiv.org/abs/2502.12601)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Uncertainty Quantification (UQ) for Natural Language Generation (NLG) is crucial for assessing the performance of Large Language Models (LLMs), as it reveals confidence in predictions, identifies failure modes, and gauges output reliability. Conformal Prediction (CP), a model-agnostic method that generates prediction sets with a specified error rate, has been adopted for UQ in classification tasks, where the size of the prediction set indicates the model's uncertainty. However, when adapting CP to NLG, the sampling-based method for generating candidate outputs cannot guarantee the inclusion of the ground truth, limiting its applicability across a wide range of error rates. To address this, we propose \ourmethod, a method that explicitly adds the ground truth to the candidate outputs and uses logit scores to measure nonconformity. Our experiments with six LLMs on four NLG tasks show that \ourmethod outperforms baseline methods in calibrating error rates and empirical cover rates, offering accurate UQ across a wide range of user-specified error rates.</li>
</ul>

<h3>Title: S2C: Learning Noise-Resistant Differences for Unsupervised Change Detection in Multimodal Remote Sensing Images</h3>
<ul>
<li><strong>Authors: </strong>Lei Ding, Xibing Zuo, Danfeng Hong, Haitao Guo, Jun Lu, Zhihui Gong, Lorenzo Bruzzone</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12604">https://arxiv.org/abs/2502.12604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12604">https://arxiv.org/pdf/2502.12604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12604]] S2C: Learning Noise-Resistant Differences for Unsupervised Change Detection in Multimodal Remote Sensing Images(https://arxiv.org/abs/2502.12604)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Unsupervised Change Detection (UCD) in multimodal Remote Sensing (RS) images remains a difficult challenge due to the inherent spatio-temporal complexity within data, and the heterogeneity arising from different imaging sensors. Inspired by recent advancements in Visual Foundation Models (VFMs) and Contrastive Learning (CL) methodologies, this research aims to develop CL methodologies to translate implicit knowledge in VFM into change representations, thus eliminating the need for explicit supervision. To this end, we introduce a Semantic-to-Change (S2C) learning framework for UCD in both homogeneous and multimodal RS images. Differently from existing CL methodologies that typically focus on learning multi-temporal similarities, we introduce a novel triplet learning strategy that explicitly models temporal differences, which are crucial to the CD task. Furthermore, random spatial and spectral perturbations are introduced during the training to enhance robustness to temporal noise. In addition, a grid sparsity regularization is defined to suppress insignificant changes, and an IoU-matching algorithm is developed to refine the CD results. Experiments on four benchmark CD datasets demonstrate that the proposed S2C learning framework achieves significant improvements in accuracy, surpassing current state-of-the-art by over 31\%, 9\%, 23\%, and 15\%, respectively. It also demonstrates robustness and sample efficiency, suitable for training and adaptation of various Visual Foundation Models (VFMs) or backbone neural networks. The relevant code will be available at: this http URL.</li>
</ul>

<h3>Title: Unveiling Mode Connectivity in Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Bingheng Li, Zhikai Chen, Haoyu Han, Shenglai Zeng, Jingzhe Liu, Jiliang Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12608">https://arxiv.org/abs/2502.12608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12608">https://arxiv.org/pdf/2502.12608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12608]] Unveiling Mode Connectivity in Graph Neural Networks(https://arxiv.org/abs/2502.12608)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>A fundamental challenge in understanding graph neural networks (GNNs) lies in characterizing their optimization dynamics and loss landscape geometry, critical for improving interpretability and robustness. While mode connectivity, a lens for analyzing geometric properties of loss landscapes has proven insightful for other deep learning architectures, its implications for GNNs remain unexplored. This work presents the first investigation of mode connectivity in GNNs. We uncover that GNNs exhibit distinct non-linear mode connectivity, diverging from patterns observed in fully-connected networks or CNNs. Crucially, we demonstrate that graph structure, rather than model architecture, dominates this behavior, with graph properties like homophily correlating with mode connectivity patterns. We further establish a link between mode connectivity and generalization, proposing a generalization bound based on loss barriers and revealing its utility as a diagnostic tool. Our findings further bridge theoretical insights with practical implications: they rationalize domain alignment strategies in graph learning and provide a foundation for refining GNN training paradigms.</li>
</ul>

<h3>Title: Who Writes What: Unveiling the Impact of Author Roles on AI-generated Text Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiatao Li, Xiaojun Wan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12611">https://arxiv.org/abs/2502.12611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12611">https://arxiv.org/pdf/2502.12611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12611]] Who Writes What: Unveiling the Impact of Author Roles on AI-generated Text Detection(https://arxiv.org/abs/2502.12611)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>The rise of Large Language Models (LLMs) necessitates accurate AI-generated text detection. However, current approaches largely overlook the influence of author characteristics. We investigate how sociolinguistic attributes-gender, CEFR proficiency, academic field, and language environment-impact state-of-the-art AI text detectors. Using the ICNALE corpus of human-authored texts and parallel AI-generated texts from diverse LLMs, we conduct a rigorous evaluation employing multi-factor ANOVA and weighted least squares (WLS). Our results reveal significant biases: CEFR proficiency and language environment consistently affected detector accuracy, while gender and academic field showed detector-dependent effects. These findings highlight the crucial need for socially aware AI text detection to avoid unfairly penalizing specific demographic groups. We offer novel empirical evidence, a robust statistical framework, and actionable insights for developing more equitable and reliable detection systems in real-world, out-of-domain contexts. This work paves the way for future research on bias mitigation, inclusive evaluation benchmarks, and socially responsible LLM detectors.</li>
</ul>

<h3>Title: Label Drop for Multi-Aspect Relation Modeling in Universal Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Lu Yang, Jiajia Li, En Ci, Lefei Zhang, Zuchao Li, Ping Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12614">https://arxiv.org/abs/2502.12614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12614">https://arxiv.org/pdf/2502.12614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12614]] Label Drop for Multi-Aspect Relation Modeling in Universal Information Extraction(https://arxiv.org/abs/2502.12614)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Universal Information Extraction (UIE) has garnered significant attention due to its ability to address model explosion problems effectively. Extractive UIE can achieve strong performance using a relatively small model, making it widely adopted. Extractive UIEs generally rely on task instructions for different tasks, including single-target instructions and multiple-target instructions. Single-target instruction UIE enables the extraction of only one type of relation at a time, limiting its ability to model correlations between relations and thus restricting its capability to extract complex relations. While multiple-target instruction UIE allows for the extraction of multiple relations simultaneously, the inclusion of irrelevant relations introduces decision complexity and impacts extraction accuracy. Therefore, for multi-relation extraction, we propose LDNet, which incorporates multi-aspect relation modeling and a label drop mechanism. By assigning different relations to different levels for understanding and decision-making, we reduce decision confusion. Additionally, the label drop mechanism effectively mitigates the impact of irrelevant relations. Experiments show that LDNet outperforms or achieves competitive performance with state-of-the-art systems on 9 tasks, 33 datasets, in both single-modal and multi-modal, few-shot and zero-shot settings.\footnote{this https URL}</li>
</ul>

<h3>Title: Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions</h3>
<ul>
<li><strong>Authors: </strong>Leonardo Ranaldi, Marco Valentino, Alexander Polonsky, Andrè Freitas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12616">https://arxiv.org/abs/2502.12616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12616">https://arxiv.org/pdf/2502.12616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12616]] Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions(https://arxiv.org/abs/2502.12616)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-Though (CoT) represents a common strategy for reasoning in Large Language Models (LLMs) by decomposing complex tasks into intermediate inference steps. However, explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness. To mitigate existing limitations, recent work has proposed using logical formalisms coupled with external symbolic solvers. However, fully symbolic approaches possess the bottleneck of requiring a complete translation from natural language to formal languages, a process that affects efficiency and flexibility. To achieve a trade-off, this paper investigates methods to disentangle content from logical reasoning without a complete formalisation. In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations. Our framework leverages the capability of LLMs to formalise only relevant variables and predicates, enabling the coexistence of symbolic elements with natural language. We show the impact of QuaSAR for in-context learning and for constructing demonstrations to improve the reasoning capabilities of smaller models. Our experiments show that quasi-symbolic abstractions can improve CoT-based methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e. MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic).</li>
</ul>

<h3>Title: Implicit Repair with Reinforcement Learning in Emergent Communication</h3>
<ul>
<li><strong>Authors: </strong>Fábio Vital, Alberto Sardinha, Francisco S. Melo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12624">https://arxiv.org/abs/2502.12624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12624">https://arxiv.org/pdf/2502.12624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12624]] Implicit Repair with Reinforcement Learning in Emergent Communication(https://arxiv.org/abs/2502.12624)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Conversational repair is a mechanism used to detect and resolve miscommunication and misinformation problems when two or more agents interact. One particular and underexplored form of repair in emergent communication is the implicit repair mechanism, where the interlocutor purposely conveys the desired information in such a way as to prevent misinformation from any other interlocutor. This work explores how redundancy can modify the emergent communication protocol to continue conveying the necessary information to complete the underlying task, even with additional external environmental pressures such as noise. We focus on extending the signaling game, called the Lewis Game, by adding noise in the communication channel and inputs received by the agents. Our analysis shows that agents add redundancy to the transmitted messages as an outcome to prevent the negative impact of noise on the task success. Additionally, we observe that the emerging communication protocol's generalization capabilities remain equivalent to architectures employed in simpler games that are entirely deterministic. Additionally, our method is the only one suitable for producing robust communication protocols that can handle cases with and without noise while maintaining increased generalization performance levels.</li>
</ul>

<h3>Title: DAMamba: Vision State Space Model with Dynamic Adaptive Scan</h3>
<ul>
<li><strong>Authors: </strong>Tanzhe Li, Caoshuo Li, Jiayi Lyu, Hongjuan Pei, Baochang Zhang, Taisong Jin, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12627">https://arxiv.org/abs/2502.12627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12627">https://arxiv.org/pdf/2502.12627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12627]] DAMamba: Vision State Space Model with Dynamic Adaptive Scan(https://arxiv.org/abs/2502.12627)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>State space models (SSMs) have recently garnered significant attention in computer vision. However, due to the unique characteristics of image data, adapting SSMs from natural language processing to computer vision has not outperformed the state-of-the-art convolutional neural networks (CNNs) and Vision Transformers (ViTs). Existing vision SSMs primarily leverage manually designed scans to flatten image patches into sequences locally or globally. This approach disrupts the original semantic spatial adjacency of the image and lacks flexibility, making it difficult to capture complex image structures. To address this limitation, we propose Dynamic Adaptive Scan (DAS), a data-driven method that adaptively allocates scanning orders and regions. This enables more flexible modeling capabilities while maintaining linear computational complexity and global modeling capacity. Based on DAS, we further propose the vision backbone DAMamba, which significantly outperforms current state-of-the-art vision Mamba models in vision tasks such as image classification, object detection, instance segmentation, and semantic segmentation. Notably, it surpasses some of the latest state-of-the-art CNNs and ViTs. Code will be available at this https URL.</li>
</ul>

<h3>Title: Cryptanalysis on Lightweight Verifiable Homomorphic Encryption</h3>
<ul>
<li><strong>Authors: </strong>Jung Hee Cheon, Daehyun Jang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12628">https://arxiv.org/abs/2502.12628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12628">https://arxiv.org/pdf/2502.12628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12628]] Cryptanalysis on Lightweight Verifiable Homomorphic Encryption(https://arxiv.org/abs/2502.12628)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack</a></li>
<li><strong>Abstract: </strong>Verifiable Homomorphic Encryption (VHE) is a cryptographic technique that integrates Homomorphic Encryption (HE) with Verifiable Computation (VC). It serves as a crucial technology for ensuring both privacy and integrity in outsourced computation, where a client sends input ciphertexts $\mathsf{ct}$ and a function $f$ to a server and verifies the correctness of the evaluation upon receiving the evaluation result $f(\mathsf{ct})$ from the server. In CCS 2024, Chatel et al. [CKP+24] introduced two lightweight VHE schemes: Replication Encoding (REP) and Polynomial Encoding (PE). A similar approach to REP was used by Albrecht et al. [ADDG24] in Eurocrypt 2024 to develop a Verifiable Oblivious PRF scheme (vADDG). A key approach in these schemes is to embed specific secret information within HE ciphertexts to verify homomorphic evaluations. This paper presents efficient attacks that exploit the homomorphic properties of encryption schemes. The one strategy is to retrieve the secret information in encrypted state from the input ciphertexts and then leverage it to modify the resulting ciphertext without being detected by the verification algorithm. The other is to exploit the secret embedding structure for modification of the evaluation function $f$ into $f'$ which works well on input values for verification purpose. Our forgery attack on vADDG achieves a success probability of $70.2\%$ under the suggested 80-bit security parameter. Our attack on REP and PE achieves a probability 1 attack with linear time complexity when using fully homomorphic encryption.</li>
</ul>

<h3>Title: Automating Prompt Leakage Attacks on Large Language Models Using Agentic Approach</h3>
<ul>
<li><strong>Authors: </strong>Tvrtko Sternak, Davor Runje, Dorian Granoša, Chi Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12630">https://arxiv.org/abs/2502.12630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12630">https://arxiv.org/pdf/2502.12630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12630]] Automating Prompt Leakage Attacks on Large Language Models Using Agentic Approach(https://arxiv.org/abs/2502.12630)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a novel approach to evaluating the security of large language models (LLMs) against prompt leakage-the exposure of system-level prompts or proprietary configurations. We define prompt leakage as a critical threat to secure LLM deployment and introduce a framework for testing the robustness of LLMs using agentic teams. Leveraging AG2 (formerly AutoGen), we implement a multi-agent system where cooperative agents are tasked with probing and exploiting the target LLM to elicit its prompt. Guided by traditional definitions of security in cryptography, we further define a prompt leakage-safe system as one in which an attacker cannot distinguish between two agents: one initialized with an original prompt and the other with a prompt stripped of all sensitive information. In a safe system, the agents' outputs will be indistinguishable to the attacker, ensuring that sensitive information remains secure. This cryptographically inspired framework provides a rigorous standard for evaluating and designing secure LLMs. This work establishes a systematic methodology for adversarial testing of prompt leakage, bridging the gap between automated threat modeling and practical LLM security. You can find the implementation of our prompt leakage probing on GitHub.</li>
</ul>

<h3>Title: Score-Based Diffusion Policy Compatible with Reinforcement Learning via Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Mingyang Sun, Pengxiang Ding, Weinan Zhang, Donglin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12631">https://arxiv.org/abs/2502.12631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12631">https://arxiv.org/pdf/2502.12631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12631]] Score-Based Diffusion Policy Compatible with Reinforcement Learning via Optimal Transport(https://arxiv.org/abs/2502.12631)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion policies have shown promise in learning complex behaviors from demonstrations, particularly for tasks requiring precise control and long-term planning. However, they face challenges in robustness when encountering distribution shifts. This paper explores improving diffusion-based imitation learning models through online interactions with the environment. We propose OTPR (Optimal Transport-guided score-based diffusion Policy for Reinforcement learning fine-tuning), a novel method that integrates diffusion policies with RL using optimal transport theory. OTPR leverages the Q-function as a transport cost and views the policy as an optimal transport map, enabling efficient and stable fine-tuning. Moreover, we introduce masked optimal transport to guide state-action matching using expert keypoints and a compatibility-based resampling strategy to enhance training stability. Experiments on three simulation tasks demonstrate OTPR's superior performance and robustness compared to existing methods, especially in complex and sparse-reward environments. In sum, OTPR provides an effective framework for combining IL and RL, achieving versatile and reliable policy learning. The code will be released at this https URL.</li>
</ul>

<h3>Title: MALT Diffusion: Memory-Augmented Latent Transformers for Any-Length Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Sihyun Yu, Meera Hahn, Dan Kondratyuk, Jinwoo Shin, Agrim Gupta, José Lezama, Irfan Essa, David Ross, Jonathan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12632">https://arxiv.org/abs/2502.12632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12632">https://arxiv.org/pdf/2502.12632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12632]] MALT Diffusion: Memory-Augmented Latent Transformers for Any-Length Video Generation(https://arxiv.org/abs/2502.12632)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion models are successful for synthesizing high-quality videos but are limited to generating short clips (e.g., 2-10 seconds). Synthesizing sustained footage (e.g. over minutes) still remains an open research question. In this paper, we propose MALT Diffusion (using Memory-Augmented Latent Transformers), a new diffusion model specialized for long video generation. MALT Diffusion (or just MALT) handles long videos by subdividing them into short segments and doing segment-level autoregressive generation. To achieve this, we first propose recurrent attention layers that encode multiple segments into a compact memory latent vector; by maintaining this memory vector over time, MALT is able to condition on it and continuously generate new footage based on a long temporal context. We also present several training techniques that enable the model to generate frames over a long horizon with consistent quality and minimal degradation. We validate the effectiveness of MALT through experiments on long video benchmarks. We first perform extensive analysis of MALT in long-contextual understanding capability and stability using popular long video benchmarks. For example, MALT achieves an FVD score of 220.4 on 128-frame video generation on UCF-101, outperforming the previous state-of-the-art of 648.4. Finally, we explore MALT's capabilities in a text-to-video generation setting and show that it can produce long videos compared with recent techniques for long text-to-video generation.</li>
</ul>

<h3>Title: \textit{One Size doesn't Fit All}: A Personalized Conversational Tutoring Agent for Mathematics Instruction</h3>
<ul>
<li><strong>Authors: </strong>Ben Liu, Jihan Zhang, Fangquan Lin, Xu Jia, Min Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12633">https://arxiv.org/abs/2502.12633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12633">https://arxiv.org/pdf/2502.12633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12633]] \textit{One Size doesn't Fit All}: A Personalized Conversational Tutoring Agent for Mathematics Instruction(https://arxiv.org/abs/2502.12633)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been increasingly employed in various intelligent educational systems, simulating human tutors to facilitate effective human-machine interaction. However, previous studies often overlook the significance of recognizing and adapting to individual learner characteristics. Such adaptation is crucial for enhancing student engagement and learning efficiency, particularly in mathematics instruction, where diverse learning styles require personalized strategies to promote comprehension and enthusiasm. In this paper, we propose a \textbf{P}erson\textbf{A}lized \textbf{C}onversational tutoring ag\textbf{E}nt (PACE) for mathematics instruction. PACE simulates students' learning styles based on the Felder and Silverman learning style model, aligning with each student's persona. In this way, our PACE can effectively assess the personality of students, allowing to develop individualized teaching strategies that resonate with their unique learning styles. To further enhance students' comprehension, PACE employs the Socratic teaching method to provide instant feedback and encourage deep thinking. By constructing personalized teaching data and training models, PACE demonstrates the ability to identify and adapt to the unique needs of each student, significantly improving the overall learning experience and outcomes. Moreover, we establish multi-aspect evaluation criteria and conduct extensive analysis to assess the performance of personalized teaching. Experimental results demonstrate the superiority of our model in personalizing the educational experience and motivating students compared to existing methods.</li>
</ul>

<h3>Title: Corrupted but Not Broken: Rethinking the Impact of Corrupted Data in Visual Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yunhao Gou, Hansi Yang, Zhili Liu, Kai Chen, Yihan Zeng, Lanqing Hong, Zhenguo Li, Qun Liu, James T. Kwok, Yu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12635">https://arxiv.org/abs/2502.12635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12635">https://arxiv.org/pdf/2502.12635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12635]] Corrupted but Not Broken: Rethinking the Impact of Corrupted Data in Visual Instruction Tuning(https://arxiv.org/abs/2502.12635)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Visual Instruction Tuning (VIT) enhances Multimodal Large Language Models (MLLMs) but it is hindered by corrupted datasets containing hallucinated content, incorrect responses, and poor OCR quality. While prior works focus on dataset refinement through high-quality data collection or rule-based filtering, they are costly or limited to specific types of corruption. To deeply understand how corrupted data affects MLLMs, in this paper, we systematically investigate this issue and find that while corrupted data degrades the performance of MLLMs, its effects are largely superficial in that the performance of MLLMs can be largely restored by either disabling a small subset of parameters or post-training with a small amount of clean data. Additionally, corrupted MLLMs exhibit improved ability to distinguish clean samples from corrupted ones, enabling the dataset cleaning without external help. Based on those insights, we propose a corruption-robust training paradigm combining self-validation and post-training, which significantly outperforms existing corruption mitigation strategies.</li>
</ul>

<h3>Title: Chronus: Understanding and Securing the Cutting-Edge Industry Solutions to DRAM Read Disturbance</h3>
<ul>
<li><strong>Authors: </strong>Oğuzhan Canpolat, A. Giray Yağlıkçı, Geraldo F. Oliveira, Ataberk Olgun, Nisa Bostancı, İsmail Emir Yüksel, Haocong Luo, Oğuz Ergin, Onur Mutlu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12650">https://arxiv.org/abs/2502.12650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12650">https://arxiv.org/pdf/2502.12650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12650]] Chronus: Understanding and Securing the Cutting-Edge Industry Solutions to DRAM Read Disturbance(https://arxiv.org/abs/2502.12650)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>We 1) present the first rigorous security, performance, energy, and cost analyses of the state-of-the-art on-DRAM-die read disturbance mitigation method, Per Row Activation Counting (PRAC) and 2) propose Chronus, a new mechanism that addresses PRAC's two major weaknesses. Our analysis shows that PRAC's system performance overhead on benign applications is non-negligible for modern DRAM chips and prohibitively large for future DRAM chips that are more vulnerable to read disturbance. We identify two weaknesses of PRAC that cause these overheads. First, PRAC increases critical DRAM access latency parameters due to the additional time required to increment activation counters. Second, PRAC performs a constant number of preventive refreshes at a time, making it vulnerable to an adversarial access pattern, known as the wave attack, and consequently requiring it to be configured for significantly smaller activation thresholds. To address PRAC's two weaknesses, we propose a new on-DRAM-die RowHammer mitigation mechanism, Chronus. Chronus 1) updates row activation counters concurrently while serving accesses by separating counters from the data and 2) prevents the wave attack by dynamically controlling the number of preventive refreshes performed. Our performance analysis shows that Chronus's system performance overhead is near-zero for modern DRAM chips and very low for future DRAM chips. Chronus outperforms three variants of PRAC and three other state-of-the-art read disturbance solutions. We discuss Chronus's and PRAC's implications for future systems and foreshadow future research directions. To aid future research, we open-source our Chronus implementation at this https URL.</li>
</ul>

<h3>Title: R.R.: Unveiling LLM Training Privacy through Recollection and Ranking</h3>
<ul>
<li><strong>Authors: </strong>Wenlong Meng, Zhenyuan Guo, Lenan Wu, Chen Gong, Wenyan Liu, Weixian Li, Chengkun Wei, Wenzhi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12658">https://arxiv.org/abs/2502.12658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12658">https://arxiv.org/pdf/2502.12658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12658]] R.R.: Unveiling LLM Training Privacy through Recollection and Ranking(https://arxiv.org/abs/2502.12658)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, steal, extraction, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) pose significant privacy risks, potentially leaking training data due to implicit memorization. Existing privacy attacks primarily focus on membership inference attacks (MIAs) or data extraction attacks, but reconstructing specific personally identifiable information (PII) in LLM's training data remains challenging. In this paper, we propose R.R. (Recollect and Rank), a novel two-step privacy stealing attack that enables attackers to reconstruct PII entities from scrubbed training data where the PII entities have been masked. In the first stage, we introduce a prompt paradigm named recollection, which instructs the LLM to repeat a masked text but fill in masks. Then we can use PII identifiers to extract recollected PII candidates. In the second stage, we design a new criterion to score each PII candidate and rank them. Motivated by membership inference, we leverage the reference model as a calibration to our criterion. Experiments across three popular PII datasets demonstrate that the R.R. achieves better PII identical performance compared to baselines. These results highlight the vulnerability of LLMs to PII leakage even when training data has been scrubbed. We release the replicate package of R.R. at a link.</li>
</ul>

<h3>Title: Demystifying Multilingual Chain-of-Thought in Process Reward Modeling</h3>
<ul>
<li><strong>Authors: </strong>Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12663">https://arxiv.org/abs/2502.12663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12663">https://arxiv.org/pdf/2502.12663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12663]] Demystifying Multilingual Chain-of-Thought in Process Reward Modeling(https://arxiv.org/abs/2502.12663)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are designed to perform a wide range of tasks. To improve their ability to solve complex problems requiring multi-step reasoning, recent research leverages process reward modeling to provide fine-grained feedback at each step of the reasoning process for reinforcement learning (RL), but it predominantly focuses on English. In this paper, we tackle the critical challenge of extending process reward models (PRMs) to multilingual settings. To achieve this, we train multilingual PRMs on a dataset spanning seven languages, which is translated from English. Through comprehensive evaluations on two widely used reasoning benchmarks across 11 languages, we demonstrate that multilingual PRMs not only improve average accuracy but also reduce early-stage reasoning errors. Furthermore, our results highlight the sensitivity of multilingual PRMs to both the number of training languages and the volume of English data, while also uncovering the benefits arising from more candidate responses and trainable parameters. This work opens promising avenues for robust multilingual applications in complex, multi-step reasoning tasks. In addition, we release the code to foster research along this line.</li>
</ul>

<h3>Title: A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary Position Embedding and Query-Aware Vector Quantization</h3>
<ul>
<li><strong>Authors: </strong>Junhui He, Junna Xing, Nan Wang, Rui Xu, Shangyu Wu, Peng Zhou, Qiang Liu, Chun Jason Xue, Qingan Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12665">https://arxiv.org/abs/2502.12665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12665">https://arxiv.org/pdf/2502.12665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12665]] A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary Position Embedding and Query-Aware Vector Quantization(https://arxiv.org/abs/2502.12665)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long context large language models (LLMs) pose significant challenges for efficient serving due to the large memory footprint and high access overhead of KV cache. Retrieval-based KV cache reduction methods can mitigate these challenges, typically by offloading the complete KV cache to CPU and retrieving necessary tokens on demand during inference. However, these methods still suffer from unsatisfactory accuracy degradation and extra retrieval overhead. To address these limitations, this paper proposes A$^2$ATS, a novel retrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate approximation of attention scores by applying the vector quantization technique to key states, thereby enabling efficient and precise retrieval of the top-K tokens. First, we propose Windowed Rotary Position Embedding, which decouples the positional dependency from query and key states after position embedding. Then, we propose query-aware vector quantization that optimizes the objective of attention score approximation directly. Finally, we design the heterogeneous inference architecture for KV cache offloading, enabling long context serving with larger batch sizes. Experimental results demonstrate that A$^2$ATS can achieve a lower performance degradation with similar or lower overhead compared to existing methods, thereby increasing long context serving throughput by up to $2.7 \times$.</li>
</ul>

<h3>Title: Evaluation of Best-of-N Sampling Strategies for Language Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yuki Ichihara, Yuu Jinnai, Tetsuro Morimura, Kaito Ariu, Kenshi Abe, Mitsuki Sakamoto, Eiji Uchibe</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12668">https://arxiv.org/abs/2502.12668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12668">https://arxiv.org/pdf/2502.12668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12668]] Evaluation of Best-of-N Sampling Strategies for Language Model Alignment(https://arxiv.org/abs/2502.12668)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) with human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Since the reward model is an imperfect proxy for the true objective, an excessive focus on optimizing its value can lead to a compromise of its performance on the true objective. Previous work proposes Regularized BoN sampling (RBoN), a BoN sampling with regularization to the objective, and shows that it outperforms BoN sampling so that it mitigates reward hacking and empirically (Jinnai et al., 2024). However, Jinnai et al. (2024) introduce RBoN based on a heuristic and they lack the analysis of why such regularization strategy improves the performance of BoN sampling. The aim of this study is to analyze the effect of BoN sampling on regularization strategies. Using the regularization strategies corresponds to robust optimization, which maximizes the worst case over a set of possible perturbations in the proxy reward. Although the theoretical guarantees are not directly applicable to RBoN, RBoN corresponds to a practical implementation. This paper proposes an extension of the RBoN framework, called Stochastic RBoN sampling (SRBoN), which is a theoretically guaranteed approach to worst-case RBoN in proxy reward. We then perform an empirical evaluation using the AlpacaFarm and Anthropic's hh-rlhf datasets to evaluate which factors of the regularization strategies contribute to the improvement of the true proxy reward. In addition, we also propose another simple RBoN method, the Sentence Length Regularized BoN, which has a better performance in the experiment as compared to the previous methods.</li>
</ul>

<h3>Title: Baichuan-M1: Pushing the Medical Capability of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bingning Wang, Haizhou Zhao, Huozhi Zhou, Liang Song, Mingyu Xu, Wei Cheng, Xiangrong Zeng, Yupeng Zhang, Yuqi Huo, Zecheng Wang, Zhengyun Zhao, Da Pan, Fan Yang, Fei Kou, Fei Li, Fuzhong Chen, Guosheng Dong, Han Liu, Hongda Zhang, Jin He, Jinjie Yang, Kangxi Wu, Kegeng Wu, Lei Su, Linlin Niu, Linzhuang Sun, Mang Wang, Pengcheng Fan, Qianli Shen, Rihui Xin, Shunya Dang, Songchi Zhou, Weipeng Chen, Wenjing Luo, Xin Chen, Xin Men, Xionghai Lin, Xuezhen Dong, Yan Zhang, Yifei Duan, Yuyan Zhou, Zhi Ma, Zhiying Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12671">https://arxiv.org/abs/2502.12671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12671">https://arxiv.org/pdf/2502.12671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12671]] Baichuan-M1: Pushing the Medical Capability of Large Language Models(https://arxiv.org/abs/2502.12671)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The current generation of large language models (LLMs) is typically designed for broad, general-purpose applications, while domain-specific LLMs, especially in vertical fields like medicine, remain relatively scarce. In particular, the development of highly efficient and practical LLMs for the medical domain is challenging due to the complexity of medical knowledge and the limited availability of high-quality data. To bridge this gap, we introduce Baichuan-M1, a series of large language models specifically optimized for medical applications. Unlike traditional approaches that simply continue pretraining on existing models or apply post-training to a general base model, Baichuan-M1 is trained from scratch with a dedicated focus on enhancing medical capabilities. Our model is trained on 20 trillion tokens and incorporates a range of effective training methods that strike a balance between general capabilities and medical expertise. As a result, Baichuan-M1 not only performs strongly across general domains such as mathematics and coding but also excels in specialized medical fields. We have open-sourced Baichuan-M1-14B, a mini version of our model, which can be accessed through the following links.</li>
</ul>

<h3>Title: Spiking Vision Transformer with Saccadic Attention</h3>
<ul>
<li><strong>Authors: </strong>Shuai Wang, Malu Zhang, Dehao Zhang, Ammar Belatreche, Yichen Xiao, Yu Liang, Yimeng Shan, Qian Sun, Enqi Zhang, Yang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12677">https://arxiv.org/abs/2502.12677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12677">https://arxiv.org/pdf/2502.12677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12677]] Spiking Vision Transformer with Saccadic Attention(https://arxiv.org/abs/2502.12677)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The combination of Spiking Neural Networks (SNNs) and Vision Transformers (ViTs) holds potential for achieving both energy efficiency and high performance, particularly suitable for edge vision applications. However, a significant performance gap still exists between SNN-based ViTs and their ANN counterparts. Here, we first analyze why SNN-based ViTs suffer from limited performance and identify a mismatch between the vanilla self-attention mechanism and spatio-temporal spike trains. This mismatch results in degraded spatial relevance and limited temporal interactions. To address these issues, we draw inspiration from biological saccadic attention mechanisms and introduce an innovative Saccadic Spike Self-Attention (SSSA) method. Specifically, in the spatial domain, SSSA employs a novel spike distribution-based method to effectively assess the relevance between Query and Key pairs in SNN-based ViTs. Temporally, SSSA employs a saccadic interaction module that dynamically focuses on selected visual areas at each timestep and significantly enhances whole scene understanding through temporal interactions. Building on the SSSA mechanism, we develop a SNN-based Vision Transformer (SNN-ViT). Extensive experiments across various visual tasks demonstrate that SNN-ViT achieves state-of-the-art performance with linear computational complexity. The effectiveness and efficiency of the SNN-ViT highlight its potential for power-critical edge vision applications.</li>
</ul>

<h3>Title: Multi-Step Alignment as Markov Games: An Optimistic Online Gradient Descent Approach with Convergence Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Yongtao Wu, Luca Viano, Yihang Chen, Zhenyu Zhu, Kimon Antonakopoulos, Quanquan Gu, Volkan Cevher</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12678">https://arxiv.org/abs/2502.12678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12678">https://arxiv.org/pdf/2502.12678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12678]] Multi-Step Alignment as Markov Games: An Optimistic Online Gradient Descent Approach with Convergence Guarantees(https://arxiv.org/abs/2502.12678)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) has been highly successful in aligning large language models with human preferences. While prevalent methods like DPO have demonstrated strong performance, they frame interactions with the language model as a bandit problem, which limits their applicability in real-world scenarios where multi-turn conversations are common. Additionally, DPO relies on the Bradley-Terry model assumption, which does not adequately capture the non-transitive nature of human preferences. In this paper, we address these challenges by modeling the alignment problem as a two-player constant-sum Markov game, where each player seeks to maximize their winning rate against the other across all steps of the conversation. Our approach Multi-step Preference Optimization (MPO) is built upon the natural actor-critic framework~\citep{peters2008natural}. We further develop OMPO based on the optimistic online gradient descent algorithm~\citep{rakhlin2013online,joulani17a}. Theoretically, we provide a rigorous analysis for both algorithms on convergence and show that OMPO requires $\mathcal{O}(\epsilon^{-1})$ policy updates to converge to an $\epsilon$-approximate Nash equilibrium. We also validate the effectiveness of our method on multi-turn conversations dataset and math reasoning dataset.</li>
</ul>

<h3>Title: Spherical Dense Text-to-Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Timon Winter, Stanislav Frolov, Brian Bernhard Moser, Andreas Dengel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12691">https://arxiv.org/abs/2502.12691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12691">https://arxiv.org/pdf/2502.12691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12691]] Spherical Dense Text-to-Image Synthesis(https://arxiv.org/abs/2502.12691)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-image (T2I) have improved synthesis results, but challenges remain in layout control and generating omnidirectional panoramic images. Dense T2I (DT2I) and spherical T2I (ST2I) models address these issues, but so far no unified approach exists. Trivial approaches, like prompting a DT2I model to generate panoramas can not generate proper spherical distortions and seamless transitions at the borders. Our work shows that spherical dense text-to-image (SDT2I) can be achieved by integrating training-free DT2I approaches into finetuned panorama models. Specifically, we propose MultiStitchDiffusion (MSTD) and MultiPanFusion (MPF) by integrating MultiDiffusion into StitchDiffusion and PanFusion, respectively. Since no benchmark for SDT2I exists, we further construct Dense-Synthetic-View (DSynView), a new synthetic dataset containing spherical layouts to evaluate our models. Our results show that MSTD outperforms MPF across image quality as well as prompt- and layout adherence. MultiPanFusion generates more diverse images but struggles to synthesize flawless foreground objects. We propose bootstrap-coupling and turning off equirectangular perspective-projection attention in the foreground as an improvement of MPF.</li>
</ul>

<h3>Title: Multi-Novelty: Improve the Diversity and Novelty of Contents Generated by Large Language Models via inference-time Multi-Views Brainstorming</h3>
<ul>
<li><strong>Authors: </strong>Arash Lagzian, Srinivas Anumasa, Dianbo Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12700">https://arxiv.org/abs/2502.12700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12700">https://arxiv.org/pdf/2502.12700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12700]] Multi-Novelty: Improve the Diversity and Novelty of Contents Generated by Large Language Models via inference-time Multi-Views Brainstorming(https://arxiv.org/abs/2502.12700)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate remarkable proficiency in generating accurate and fluent text. However, they often struggle with diversity and novelty, leading to repetitive or overly deterministic responses. These limitations stem from constraints in training data, including gaps in specific knowledge domains, outdated information, and an over-reliance on textual sources. Such shortcomings reduce their effectiveness in tasks requiring creativity, multi-perspective reasoning, and exploratory thinking, such as LLM based AI scientist agents and creative artist agents . To address this challenge, we introduce inference-time multi-view brainstorming method, a novel approach that enriches input prompts with diverse perspectives derived from both textual and visual sources, which we refere to as "Multi-Novelty". By incorporating additional contextual information as diverse starting point for chain of thoughts, this method enhances the variety and creativity of generated outputs. Importantly, our approach is model-agnostic, requiring no architectural modifications and being compatible with both open-source and proprietary LLMs.</li>
</ul>

<h3>Title: CausalMan: A physics-based simulator for large-scale causality</h3>
<ul>
<li><strong>Authors: </strong>Nicholas Tagliapietra, Juergen Luettin, Lavdim Halilaj, Moritz Willig, Tim Pychynski, Kristian Kersting</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12707">https://arxiv.org/abs/2502.12707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12707">https://arxiv.org/pdf/2502.12707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12707]] CausalMan: A physics-based simulator for large-scale causality(https://arxiv.org/abs/2502.12707)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>A comprehensive understanding of causality is critical for navigating and operating within today's complex real-world systems. The absence of realistic causal models with known data generating processes complicates fair benchmarking. In this paper, we present the CausalMan simulator, modeled after a real-world production line. The simulator features a diverse range of linear and non-linear mechanisms and challenging-to-predict behaviors, such as discrete mode changes. We demonstrate the inadequacy of many state-of-the-art approaches and analyze the significant differences in their performance and tractability, both in terms of runtime and memory complexity. As a contribution, we will release the CausalMan large-scale simulator. We present two derived datasets, and perform an extensive evaluation of both.</li>
</ul>

<h3>Title: TREND: A Whitespace Replacement Information Hiding Method</h3>
<ul>
<li><strong>Authors: </strong>Malte Hellmeier, Hendrik Norkowski, Ernst-Christoph Schrewe, Haydar Qarawlus, Falk Howar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12710">https://arxiv.org/abs/2502.12710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12710">https://arxiv.org/pdf/2502.12710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12710]] TREND: A Whitespace Replacement Information Hiding Method(https://arxiv.org/abs/2502.12710)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have gained significant popularity in recent years. Differentiating between a text written by a human and a text generated by an LLM has become almost impossible. Information hiding techniques such as digital watermarking or steganography can help by embedding information inside text without being noticed. However, existing techniques, such as linguistic-based or format-based methods, change the semantics or do not work on pure, unformatted text. In this paper, we introduce a novel method for information hiding termed TREND, which is able to conceal any byte-encoded sequence within a cover text. The proposed method is implemented as a multi-platform library using the Kotlin programming language, accompanied by a command-line tool and a web interface provided as examples of usage. By substituting conventional whitespace characters with visually similar Unicode whitespace characters, our proposed scheme preserves the semantics of the cover text without increasing the number of characters. Furthermore, we propose a specified structure for secret messages that enables configurable compression, encryption, hashing, and error correction. Our experimental benchmark comparison on a dataset of one million Wikipedia articles compares ten algorithms from literature and practice. It proves the robustness of our proposed method in various applications while remaining imperceptible to humans. We discuss the limitations of limited embedding capacity and further robustness, which guide implications for future work.</li>
</ul>

<h3>Title: Uncertainty Propagation for Echocardiography Clinical Metric Estimation via Contour Sampling</h3>
<ul>
<li><strong>Authors: </strong>Thierry Judge, Olivier Bernard, Woo-Jin Cho Kim, Alberto Gomez, Arian Beqiri, Agisilaos Chartsias, Pierre-Marc Jodoin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12713">https://arxiv.org/abs/2502.12713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12713">https://arxiv.org/pdf/2502.12713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12713]] Uncertainty Propagation for Echocardiography Clinical Metric Estimation via Contour Sampling(https://arxiv.org/abs/2502.12713)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Echocardiography plays a fundamental role in the extraction of important clinical parameters (e.g. left ventricular volume and ejection fraction) required to determine the presence and severity of heart-related conditions. When deploying automated techniques for computing these parameters, uncertainty estimation is crucial for assessing their utility. Since clinical parameters are usually derived from segmentation maps, there is no clear path for converting pixel-wise uncertainty values into uncertainty estimates in the downstream clinical metric calculation. In this work, we propose a novel uncertainty estimation method based on contouring rather than segmentation. Our method explicitly predicts contour location uncertainty from which contour samples can be drawn. Finally, the sampled contours can be used to propagate uncertainty to clinical metrics. Our proposed method not only provides accurate uncertainty estimations for the task of contouring but also for the downstream clinical metrics on two cardiac ultrasound datasets. Code is available at: this https URL.</li>
</ul>

<h3>Title: Learning the symmetric group: large from small</h3>
<ul>
<li><strong>Authors: </strong>Max Petschack, Alexandr Garbali, Jan de Gier</a></li>
<li><strong>Subjects: </strong>cs.LG, math.CO, math.RT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12717">https://arxiv.org/abs/2502.12717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12717">https://arxiv.org/pdf/2502.12717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12717]] Learning the symmetric group: large from small(https://arxiv.org/abs/2502.12717)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Machine learning explorations can make significant inroads into solving difficult problems in pure mathematics. One advantage of this approach is that mathematical datasets do not suffer from noise, but a challenge is the amount of data required to train these models and that this data can be computationally expensive to generate. Key challenges further comprise difficulty in a posteriori interpretation of statistical models and the implementation of deep and abstract mathematical problems. We propose a method for scalable tasks, by which models trained on simpler versions of a task can then generalize to the full task. Specifically, we demonstrate that a transformer neural-network trained on predicting permutations from words formed by general transpositions in the symmetric group $S_{10}$ can generalize to the symmetric group $S_{25}$ with near 100\% accuracy. We also show that $S_{10}$ generalizes to $S_{16}$ with similar performance if we only use adjacent transpositions. We employ identity augmentation as a key tool to manage variable word lengths, and partitioned windows for training on adjacent transpositions. Finally we compare variations of the method used and discuss potential challenges with extending the method to other tasks.</li>
</ul>

<h3>Title: Circuit Representation Learning with Masked Gate Modeling and Verilog-AIG Alignment</h3>
<ul>
<li><strong>Authors: </strong>Haoyuan Wu, Haisheng Zheng, Yuan Pu, Bei Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12732">https://arxiv.org/abs/2502.12732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12732">https://arxiv.org/pdf/2502.12732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12732]] Circuit Representation Learning with Masked Gate Modeling and Verilog-AIG Alignment(https://arxiv.org/abs/2502.12732)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding the structure and function of circuits is crucial for electronic design automation (EDA). Circuits can be formulated as And-Inverter graphs (AIGs), enabling efficient implementation of representation learning through graph neural networks (GNNs). Masked modeling paradigms have been proven effective in graph representation learning. However, masking augmentation to original circuits will destroy their logical equivalence, which is unsuitable for circuit representation learning. Moreover, existing masked modeling paradigms often prioritize structural information at the expense of abstract information such as circuit function. To address these limitations, we introduce MGVGA, a novel constrained masked modeling paradigm incorporating masked gate modeling (MGM) and Verilog-AIG alignment (VGA). Specifically, MGM preserves logical equivalence by masking gates in the latent space rather than in the original circuits, subsequently reconstructing the attributes of these masked gates. Meanwhile, large language models (LLMs) have demonstrated an excellent understanding of the Verilog code functionality. Building upon this capability, VGA performs masking operations on original circuits and reconstructs masked gates under the constraints of equivalent Verilog codes, enabling GNNs to learn circuit functions from LLMs. We evaluate MGVGA on various logic synthesis tasks for EDA and show the superior performance of MGVGA compared to previous state-of-the-art methods. Our code is available at this https URL.</li>
</ul>

<h3>Title: Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text Detection with Adversarial Training</h3>
<ul>
<li><strong>Authors: </strong>Yuanfan Li, Zhaohan Zhang, Chengzhengxu Li, Chao Shen, Xiaoming Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12734">https://arxiv.org/abs/2502.12734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12734">https://arxiv.org/pdf/2502.12734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12734]] Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text Detection with Adversarial Training(https://arxiv.org/abs/2502.12734)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, steal</a></li>
<li><strong>Abstract: </strong>Machine-generated Text (MGT) detection is crucial for regulating and attributing online texts. While the existing MGT detectors achieve strong performance, they remain vulnerable to simple perturbations and adversarial attacks. To build an effective defense against malicious perturbations, we view MGT detection from a threat modeling perspective, that is, analyzing the model's vulnerability from an adversary's point of view and exploring effective mitigations. To this end, we introduce an adversarial framework for training a robust MGT detector, named GREedy Adversary PromoTed DefendER (GREATER). The GREATER consists of two key components: an adversary GREATER-A and a detector GREATER-D. The GREATER-D learns to defend against the adversarial attack from GREATER-A and generalizes the defense to other attacks. GREATER-A identifies and perturbs the critical tokens in embedding space, along with greedy search and pruning to generate stealthy and disruptive adversarial examples. Besides, we update the GREATER-A and GREATER-D synchronously, encouraging the GREATER-D to generalize its defense to different attacks and varying attack intensities. Our experimental results across 9 text perturbation strategies and 5 adversarial attacks show that our GREATER-D reduces the Attack Success Rate (ASR) by 10.61% compared with SOTA defense methods while our GREATER-A is demonstrated to be more effective and efficient than SOTA attack approaches.</li>
</ul>

<h3>Title: 3D Shape-to-Image Brownian Bridge Diffusion for Brain MRI Synthesis from Cortical Surfaces</h3>
<ul>
<li><strong>Authors: </strong>Fabian Bongratz, Yitong Li, Sama Elbaroudy, Christian Wachinger</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12742">https://arxiv.org/abs/2502.12742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12742">https://arxiv.org/pdf/2502.12742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12742]] 3D Shape-to-Image Brownian Bridge Diffusion for Brain MRI Synthesis from Cortical Surfaces(https://arxiv.org/abs/2502.12742)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite recent advances in medical image generation, existing methods struggle to produce anatomically plausible 3D structures. In synthetic brain magnetic resonance images (MRIs), characteristic fissures are often missing, and reconstructed cortical surfaces appear scattered rather than densely convoluted. To address this issue, we introduce Cor2Vox, the first diffusion model-based method that translates continuous cortical shape priors to synthetic brain MRIs. To achieve this, we leverage a Brownian bridge process which allows for direct structured mapping between shape contours and medical images. Specifically, we adapt the concept of the Brownian bridge diffusion model to 3D and extend it to embrace various complementary shape representations. Our experiments demonstrate significant improvements in the geometric accuracy of reconstructed structures compared to previous voxel-based approaches. Moreover, Cor2Vox excels in image quality and diversity, yielding high variation in non-target structures like the skull. Finally, we highlight the capability of our approach to simulate cortical atrophy at the sub-voxel level. Our code is available at this https URL.</li>
</ul>

<h3>Title: "I know myself better, but not really greatly": Using LLMs to Detect and Explain LLM-Generated Texts</h3>
<ul>
<li><strong>Authors: </strong>Jiazhou Ji, Jie Guo, Weidong Qiu, Zheng Huang, Yang Xu, Xinru Lu, Xiaoyu Jiang, Ruizhe Li, Shujun Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12743">https://arxiv.org/abs/2502.12743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12743">https://arxiv.org/pdf/2502.12743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12743]] "I know myself better, but not really greatly": Using LLMs to Detect and Explain LLM-Generated Texts(https://arxiv.org/abs/2502.12743)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated impressive capabilities in generating human-like texts, but the potential misuse of such LLM-generated texts raises the need to distinguish between human-generated and LLM-generated content. This paper explores the detection and explanation capabilities of LLM-based detectors of LLM-generated texts, in the context of a binary classification task (human-generated texts vs LLM-generated texts) and a ternary classification task (human-generated texts, LLM-generated texts, and undecided). By evaluating on six close/open-source LLMs with different sizes, our findings reveal that while self-detection consistently outperforms cross-detection, i.e., LLMs can detect texts generated by themselves more accurately than those generated by other LLMs, the performance of self-detection is still far from ideal, indicating that further improvements are needed. We also show that extending the binary to the ternary classification task with a new class "Undecided" can enhance both detection accuracy and explanation quality, with improvements being statistically significant and consistent across all LLMs. We finally conducted comprehensive qualitative and quantitative analyses on the explanation errors, which are categorized into three types: reliance on inaccurate features (the most frequent error), hallucinations, and incorrect reasoning. These findings with our human-annotated dataset emphasize the need for further research into improving both self-detection and self-explanation, particularly to address overfitting issues that may hinder generalization.</li>
</ul>

<h3>Title: Self-Enhanced Reasoning Training: Activating Latent Reasoning in Small Models for Enhanced Reasoning Distillation</h3>
<ul>
<li><strong>Authors: </strong>Yong Zhang, Bingyuan Zhang, Zhitao Li, Ming Li, Ning Cheng, Minchuan Chen, Tao Wei, Jun Ma, Shaojun Wang, Jing Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12744">https://arxiv.org/abs/2502.12744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12744">https://arxiv.org/pdf/2502.12744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12744]] Self-Enhanced Reasoning Training: Activating Latent Reasoning in Small Models for Enhanced Reasoning Distillation(https://arxiv.org/abs/2502.12744)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has significantly enhanced their reasoning abilities, enabling increasingly complex tasks. However, these capabilities often diminish in smaller, more computationally efficient models like GPT-2. Recent research shows that reasoning distillation can help small models acquire reasoning capabilities, but most existing methods focus primarily on improving teacher-generated reasoning paths. Our observations reveal that small models can generate high-quality reasoning paths during sampling, even without chain-of-thought prompting, though these paths are often latent due to their low probability under standard decoding strategies. To address this, we propose Self-Enhanced Reasoning Training (SERT), which activates and leverages latent reasoning capabilities in small models through self-training on filtered, self-generated reasoning paths under zero-shot conditions. Experiments using OpenAI's GPT-3.5 as the teacher model and GPT-2 models as the student models demonstrate that SERT enhances the reasoning abilities of small models, improving their performance in reasoning distillation.</li>
</ul>

<h3>Title: Architect of the Bits World: Masked Autoregressive Modeling for Circuit Generation Guided by Truth Table</h3>
<ul>
<li><strong>Authors: </strong>Haoyuan Wu, Haisheng Zheng, Shoubo Hu, Zhuolun He, Bei Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12751">https://arxiv.org/abs/2502.12751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12751">https://arxiv.org/pdf/2502.12751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12751]] Architect of the Bits World: Masked Autoregressive Modeling for Circuit Generation Guided by Truth Table(https://arxiv.org/abs/2502.12751)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Logic synthesis, a critical stage in electronic design automation (EDA), optimizes gate-level circuits to minimize power consumption and area occupancy in integrated circuits (ICs). Traditional logic synthesis tools rely on human-designed heuristics, often yielding suboptimal results. Although differentiable architecture search (DAS) has shown promise in generating circuits from truth tables, it faces challenges such as high computational complexity, convergence to local optima, and extensive hyperparameter tuning. Consequently, we propose a novel approach integrating conditional generative models with DAS for circuit generation. Our approach first introduces CircuitVQ, a circuit tokenizer trained based on our Circuit AutoEncoder We then develop CircuitAR, a masked autoregressive model leveraging CircuitVQ as the tokenizer. CircuitAR can generate preliminary circuit structures from truth tables, which guide DAS in producing functionally equivalent circuits. Notably, we observe the scalability and emergent capability in generating complex circuit structures of our CircuitAR models. Extensive experiments also show the superior performance of our method. This research bridges the gap between probabilistic generative models and precise circuit generation, offering a robust solution for logic synthesis.</li>
</ul>

<h3>Title: High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Xiang Zhang, Yang Zhang, Lukas Mehl, Markus Gross, Christopher Schroers</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12752">https://arxiv.org/abs/2502.12752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12752">https://arxiv.org/pdf/2502.12752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12752]] High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion(https://arxiv.org/abs/2502.12752)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite recent advances in Novel View Synthesis (NVS), generating high-fidelity views from single or sparse observations remains a significant challenge. Existing splatting-based approaches often produce distorted geometry due to splatting errors. While diffusion-based methods leverage rich 3D priors to achieve improved geometry, they often suffer from texture hallucination. In this paper, we introduce SplatDiff, a pixel-splatting-guided video diffusion model designed to synthesize high-fidelity novel views from a single image. Specifically, we propose an aligned synthesis strategy for precise control of target viewpoints and geometry-consistent view synthesis. To mitigate texture hallucination, we design a texture bridge module that enables high-fidelity texture generation through adaptive feature fusion. In this manner, SplatDiff leverages the strengths of splatting and diffusion to generate novel views with consistent geometry and high-fidelity details. Extensive experiments verify the state-of-the-art performance of SplatDiff in single-view NVS. Additionally, without extra training, SplatDiff shows remarkable zero-shot performance across diverse tasks, including sparse-view NVS and stereo video conversion.</li>
</ul>

<h3>Title: Efficient Machine Translation Corpus Generation: Integrating Human-in-the-Loop Post-Editing with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kamer Ali Yuksel, Ahmet Gunduz, Abdul Baseet Anees, Hassan Sawaf</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12755">https://arxiv.org/abs/2502.12755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12755">https://arxiv.org/pdf/2502.12755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12755]] Efficient Machine Translation Corpus Generation: Integrating Human-in-the-Loop Post-Editing with Large Language Models(https://arxiv.org/abs/2502.12755)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces an advanced methodology for machine translation (MT) corpus generation, integrating semi-automated, human-in-the-loop post-editing with large language models (LLMs) to enhance efficiency and translation quality. Building upon previous work that utilized real-time training of a custom MT quality estimation metric, this system incorporates novel LLM features such as Enhanced Translation Synthesis and Assisted Annotation Analysis, which improve initial translation hypotheses and quality assessments, respectively. Additionally, the system employs LLM-Driven Pseudo Labeling and a Translation Recommendation System to reduce human annotator workload in specific contexts. These improvements not only retain the original benefits of cost reduction and enhanced post-edit quality but also open new avenues for leveraging cutting-edge LLM advancements. The project's source code is available for community use, promoting collaborative developments in the field. The demo video can be accessed here.</li>
</ul>

<h3>Title: One-bit Compressed Sensing using Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Swatantra Kafle, Geethu Joseph, Pramod K. Varshney</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12762">https://arxiv.org/abs/2502.12762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12762">https://arxiv.org/pdf/2502.12762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12762]] One-bit Compressed Sensing using Generative Models(https://arxiv.org/abs/2502.12762)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper addresses the classical problem of one-bit compressed sensing using a deep learning-based reconstruction algorithm that leverages a trained generative model to enhance the signal reconstruction performance. The generator, a pre-trained neural network, learns to map from a low-dimensional latent space to a higher-dimensional set of sparse vectors. This generator is then used to reconstruct sparse vectors from their one-bit measurements by searching over its range. The presented algorithm provides an excellent reconstruction performance because the generative model can learn additional structural information about the signal beyond sparsity. Furthermore, we provide theoretical guarantees on the reconstruction accuracy and sample complexity of the algorithm. Through numerical experiments using three publicly available image datasets, MNIST, Fashion-MNIST, and Omniglot, we demonstrate the superior performance of the algorithm compared to other existing algorithms and show that our algorithm can recover both the amplitude and the direction of the signal from one-bit measurements.</li>
</ul>

<h3>Title: R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Sumin Jo, Junseong Choi, Jiho Kim, Edward Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12767">https://arxiv.org/abs/2502.12767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12767">https://arxiv.org/pdf/2502.12767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12767]] R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs(https://arxiv.org/abs/2502.12767)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent studies have combined Large Language Models (LLMs) with Knowledge Graphs (KGs) to enhance reasoning, improving inference accuracy without additional training while mitigating hallucination. However, existing frameworks are often rigid, struggling to adapt to KG or task changes. They also rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning. To address this, We introduce R2-KG, a plug-and-play, dual-agent framework that separates reasoning into two roles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor (a high-capacity LLM) that makes final judgments. This design is cost-efficient for LLM inference while still maintaining strong reasoning accuracy. Additionally, R2-KG employs an Abstention mechanism, generating answers only when sufficient evidence is collected from KG, which significantly enhances reliability. Experiments across multiple KG-based reasoning tasks show that R2-KG consistently outperforms baselines in both accuracy and reliability, regardless of the inherent capability of LLMs used as the Operator. Further experiments reveal that the single-agent version of R2-KG, equipped with a strict self-consistency strategy, achieves significantly higher-than-baseline reliability while reducing inference cost. However, it also leads to a higher abstention rate in complex KGs. Our findings establish R2-KG as a flexible and cost-effective solution for KG-based reasoning. It reduces reliance on high-capacity LLMs while ensuring trustworthy inference.</li>
</ul>

<h3>Title: How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Saad Obaid ul Islam, Anne Lauscher, Goran Glavaš</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12769">https://arxiv.org/abs/2502.12769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12769">https://arxiv.org/pdf/2502.12769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12769]] How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild(https://arxiv.org/abs/2502.12769)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the age of misinformation, hallucination -- the tendency of Large Language Models (LLMs) to generate non-factual or unfaithful responses -- represents the main risk for their global utility. Despite LLMs becoming increasingly multilingual, the vast majority of research on detecting and quantifying LLM hallucination are (a) English-centric and (b) focus on machine translation (MT) and summarization, tasks that are less common ``in the wild'' than open information seeking. In contrast, we aim to quantify the extent of LLM hallucination across languages in knowledge-intensive long-form question answering. To this end, we train a multilingual hallucination detection model and conduct a large-scale study across 30 languages and 6 open-source LLM families. We start from an English hallucination detection dataset and rely on MT to generate (noisy) training data in other languages. We also manually annotate gold data for five high-resource languages; we then demonstrate, for these languages, that the estimates of hallucination rates are similar between silver (LLM-generated) and gold test sets, validating the use of silver data for estimating hallucination rates for other languages. For the final rates estimation, we build a knowledge-intensive QA dataset for 30 languages with LLM-generated prompts and Wikipedia articles as references. We find that, while LLMs generate longer responses with more hallucinated tokens for higher-resource languages, there is no correlation between length-normalized hallucination rates of languages and their digital representation. Further, we find that smaller LLMs exhibit larger hallucination rates than larger models.</li>
</ul>

<h3>Title: Mind the Gap: Aligning the Brain with Language Models Requires a Nonlinear and Multimodal Approach</h3>
<ul>
<li><strong>Authors: </strong>Danny Dongyeop Han, Yunju Cho, Jiook Cha, Jay-Yoon Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12771">https://arxiv.org/abs/2502.12771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12771">https://arxiv.org/pdf/2502.12771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12771]] Mind the Gap: Aligning the Brain with Language Models Requires a Nonlinear and Multimodal Approach(https://arxiv.org/abs/2502.12771)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Self-supervised language and audio models effectively predict brain responses to speech. However, traditional prediction models rely on linear mappings from unimodal features, despite the complex integration of auditory signals with linguistic and semantic information across widespread brain networks during speech comprehension. Here, we introduce a nonlinear, multimodal prediction model that combines audio and linguistic features from pre-trained models (e.g., LLAMA, Whisper). Our approach achieves a 17.2% and 17.9% improvement in prediction performance (unnormalized and normalized correlation) over traditional unimodal linear models, as well as a 7.7% and 14.4% improvement, respectively, over prior state-of-the-art models. These improvements represent a major step towards future robust in-silico testing and improved decoding performance. They also reveal how auditory and semantic information are fused in motor, somatosensory, and higher-level semantic regions, aligning with existing neurolinguistic theories. Overall, our work highlights the often neglected potential of nonlinear and multimodal approaches to brain modeling, paving the way for future studies to embrace these strategies in naturalistic neurolinguistics research.</li>
</ul>

<h3>Title: Commonsense Reasoning in Arab Culture</h3>
<ul>
<li><strong>Authors: </strong>Abdelrahman Sadallah, Junior Cedric Tonga, Khalid Almubarak, Saeed Almheiri, Farah Atif, Chatrine Qwaider, Karima Kadaoui, Sara Shatnawi, Yaser Alesh, Fajri Koto</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12788">https://arxiv.org/abs/2502.12788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12788">https://arxiv.org/pdf/2502.12788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12788]] Commonsense Reasoning in Arab Culture(https://arxiv.org/abs/2502.12788)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite progress in Arabic large language models, such as Jais and AceGPT, their evaluation on commonsense reasoning has largely relied on machine-translated datasets, which lack cultural depth and may introduce Anglocentric biases. Commonsense reasoning is shaped by geographical and cultural contexts, and existing English datasets fail to capture the diversity of the Arab world. To address this, we introduce \datasetname, a commonsense reasoning dataset in Modern Standard Arabic (MSA), covering cultures of 13 countries across the Gulf, Levant, North Africa, and the Nile Valley. The dataset was built from scratch by engaging native speakers to write and validate culturally relevant questions for their respective countries. \datasetname spans 12 daily life domains with 54 fine-grained subtopics, reflecting various aspects of social norms, traditions, and everyday experiences. Zero-shot evaluations show that open-weight language models with up to 32B parameters struggle to comprehend diverse Arab cultures, with performance varying across regions. These findings highlight the need for more culturally aware models and datasets tailored to the Arabic-speaking world.</li>
</ul>

<h3>Title: Beyond Timesteps: A Novel Activation-wise Membrane Potential Propagation Mechanism for Spiking Neural Networks in 3D cloud</h3>
<ul>
<li><strong>Authors: </strong>Jian Song, Boxuan Zheng, Xiangfei Yang, Donglin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12791">https://arxiv.org/abs/2502.12791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12791">https://arxiv.org/pdf/2502.12791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12791]] Beyond Timesteps: A Novel Activation-wise Membrane Potential Propagation Mechanism for Spiking Neural Networks in 3D cloud(https://arxiv.org/abs/2502.12791)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Due to the similar characteristics between event-based visual data and point clouds, recent studies have emerged that treat event data as event clouds to learn based on point cloud analysis. Additionally, some works approach point clouds from the perspective of event vision, employing Spiking Neural Network (SNN) due to their asynchronous nature. However, these contributions are often domain-specific, making it difficult to extend their applicability to other intersecting fields. Moreover, while SNN-based visual tasks have seen significant growth, the conventional timestep-wise iterative activation strategy largely limits their real-world applications by large timesteps, resulting in significant delays and increased computational costs. Although some innovative methods achieve good performance with short timesteps (<10), few have fundamentally restructured the update strategy of spiking neurons to completely overcome the limitations of timesteps. In response to these concerns, we propose a novel and general activation strategy for spiking neurons called Activation-wise Membrane Potential Propagation (AMP2). This approach extends the concept of timesteps from a manually crafted parameter within the activation function to any existing network structure. In experiments on common point cloud tasks (classification, object, and scene segmentation) and event cloud tasks (action recognition), we found that AMP2 stabilizes SNN training, maintains competitive performance, and reduces latency compared to the traditional timestep-wise activation paradigm.</li>
</ul>

<h3>Title: RAPID: Retrieval Augmented Training of Differentially Private Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Tanqiu Jiang, Changjiang Li, Fenglong Ma, Ting Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12794">https://arxiv.org/abs/2502.12794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12794">https://arxiv.org/pdf/2502.12794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12794]] RAPID: Retrieval Augmented Training of Differentially Private Diffusion Models(https://arxiv.org/abs/2502.12794)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Differentially private diffusion models (DPDMs) harness the remarkable generative capabilities of diffusion models while enforcing differential privacy (DP) for sensitive data. However, existing DPDM training approaches often suffer from significant utility loss, large memory footprint, and expensive inference cost, impeding their practical uses. To overcome such limitations, we present RAPID: Retrieval Augmented PrIvate Diffusion model, a novel approach that integrates retrieval augmented generation (RAG) into DPDM training. Specifically, RAPID leverages available public data to build a knowledge base of sample trajectories; when training the diffusion model on private data, RAPID computes the early sampling steps as queries, retrieves similar trajectories from the knowledge base as surrogates, and focuses on training the later sampling steps in a differentially private manner. Extensive evaluation using benchmark datasets and models demonstrates that, with the same privacy guarantee, RAPID significantly outperforms state-of-the-art approaches by large margins in generative quality, memory footprint, and inference cost, suggesting that retrieval-augmented DP training represents a promising direction for developing future privacy-preserving generative models. The code is available at: this https URL</li>
</ul>

<h3>Title: Learning Counterfactually Fair Models via Improved Generation with Neural Causal Models</h3>
<ul>
<li><strong>Authors: </strong>Krishn Vishwas Kher, Aditya Varun V, Shantanu Das, SakethaNath Jagarlapudi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12796">https://arxiv.org/abs/2502.12796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12796">https://arxiv.org/pdf/2502.12796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12796]] Learning Counterfactually Fair Models via Improved Generation with Neural Causal Models(https://arxiv.org/abs/2502.12796)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>One of the main concerns while deploying machine learning models in real-world applications is fairness. Counterfactual fairness has emerged as an intuitive and natural definition of fairness. However, existing methodologies for enforcing counterfactual fairness seem to have two limitations: (i) generating counterfactual samples faithful to the underlying causal graph, and (ii) as we argue in this paper, existing regularizers are mere proxies and do not directly enforce the exact definition of counterfactual fairness. In this work, our aim is to mitigate both issues. Firstly, we propose employing Neural Causal Models (NCMs) for generating the counterfactual samples. For implementing the abduction step in NCMs, the posteriors of the exogenous variables need to be estimated given a counterfactual query, as they are not readily available. As a consequence, $\mathcal{L}_3$ consistency with respect to the underlying causal graph cannot be guaranteed in practice due to the estimation errors involved. To mitigate this issue, we propose a novel kernel least squares loss term that enforces the $\mathcal{L}_3$ constraints explicitly. Thus, we obtain an improved counterfactual generation suitable for the counterfactual fairness task. Secondly, we propose a new MMD-based regularizer term that explicitly enforces the counterfactual fairness conditions into the base model while training. We show an improved trade-off between counterfactual fairness and generalization over existing baselines on synthetic and benchmark datasets.</li>
</ul>

<h3>Title: Towards Text-Image Interleaved Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Xin Zhang, Ziqi Dai, Yongqi Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Jun Yu, Wenjie Li, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12799">https://arxiv.org/abs/2502.12799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12799">https://arxiv.org/pdf/2502.12799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12799]] Towards Text-Image Interleaved Retrieval(https://arxiv.org/abs/2502.12799)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Current multimodal information retrieval studies mainly focus on single-image inputs, which limits real-world applications involving multiple images and text-image interleaved content. In this work, we introduce the text-image interleaved retrieval (TIIR) task, where the query and document are interleaved text-image sequences, and the model is required to understand the semantics from the interleaved context for effective retrieval. We construct a TIIR benchmark based on naturally interleaved wikiHow tutorials, where a specific pipeline is designed to generate interleaved queries. To explore the task, we adapt several off-the-shelf retrievers and build a dense baseline by interleaved multimodal large language model (MLLM). We then propose a novel Matryoshka Multimodal Embedder (MME), which compresses the number of visual tokens at different granularity, to address the challenge of excessive visual tokens in MLLM-based TIIR models. Experiments demonstrate that simple adaption of existing models does not consistently yield effective results. Our MME achieves significant improvements over the baseline by substantially fewer visual tokens. We provide extensive analysis and will release the dataset and code to facilitate future research.</li>
</ul>

<h3>Title: Learning Wall Segmentation in 3D Vessel Trees using Sparse Annotations</h3>
<ul>
<li><strong>Authors: </strong>Hinrich Rahlfs, Markus Hüllebrand, Sebastian Schmitter, Christoph Strecker, Andreas Harloff, Anja Hennemuth</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12801">https://arxiv.org/abs/2502.12801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12801">https://arxiv.org/pdf/2502.12801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12801]] Learning Wall Segmentation in 3D Vessel Trees using Sparse Annotations(https://arxiv.org/abs/2502.12801)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>We propose a novel approach that uses sparse annotations from clinical studies to train a 3D segmentation of the carotid artery wall. We use a centerline annotation to sample perpendicular cross-sections of the carotid artery and use an adversarial 2D network to segment them. These annotations are then transformed into 3D pseudo-labels for training of a 3D convolutional neural network, circumventing the creation of manual 3D masks. For pseudo-label creation in the bifurcation area we propose the use of cross-sections perpendicular to the bifurcation axis and show that this enhances segmentation performance. Different sampling distances had a lesser impact. The proposed method allows for efficient training of 3D segmentation, offering potential improvements in the assessment of carotid artery stenosis and allowing the extraction of 3D biomarkers such as plaque volume.</li>
</ul>

<h3>Title: Simulating User Diversity in Task-Oriented Dialogue Systems using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Adnan Ahmad, Stefan Hillmann, Sebastian Möller</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12813">https://arxiv.org/abs/2502.12813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12813">https://arxiv.org/pdf/2502.12813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12813]] Simulating User Diversity in Task-Oriented Dialogue Systems using Large Language Models(https://arxiv.org/abs/2502.12813)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this study, we explore the application of Large Language Models (LLMs) for generating synthetic users and simulating user conversations with a task-oriented dialogue system and present detailed results and their analysis. We propose a comprehensive novel approach to user simulation technique that uses LLMs to create diverse user profiles, set goals, engage in multi-turn dialogues, and evaluate the conversation success. We employ two proprietary LLMs, namely GPT-4o and GPT-o1 (Achiam et al., 2023), to generate a heterogeneous base of user profiles, characterized by varied demographics, multiple user goals, different conversational styles, initial knowledge levels, interests, and conversational objectives. We perform a detailed analysis of the user profiles generated by LLMs to assess the diversity, consistency, and potential biases inherent in these LLM-generated user simulations. We find that GPT-o1 generates more heterogeneous user distribution across most user attributes, while GPT-4o generates more skewed user attributes. The generated set of user profiles are then utilized to simulate dialogue sessions by interacting with a task-oriented dialogue system.</li>
</ul>

<h3>Title: Carotid Artery Plaque Analysis in 3D Based on Distance Encoding in Mesh Representations</h3>
<ul>
<li><strong>Authors: </strong>Hinrich Rahlfs, Markus Hüllebrand, Sebastian Schmitter, Christoph Strecker, Andreas Harloff, Anja Hennemuth</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12819">https://arxiv.org/abs/2502.12819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12819">https://arxiv.org/pdf/2502.12819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12819]] Carotid Artery Plaque Analysis in 3D Based on Distance Encoding in Mesh Representations(https://arxiv.org/abs/2502.12819)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Purpose: Enabling a comprehensive and robust assessment of carotid artery plaques in 3D through extraction and visualization of quantitative plaque parameters. These parameters have potential applications in stroke risk analysis, evaluation of therapy effectiveness, and plaque progression prediction. Methods: We propose a novel method for extracting a plaque mesh from 3D vessel wall segmentation using distance encoding on the inner and outer wall mesh for precise plaque structure analysis. A case-specific threshold, derived from the normal vessel wall thickness, was applied to extract plaques from a dataset of 202 T1-weighted black-blood MRI scans of subjects with up to 50% stenosis. Applied to baseline and one-year follow-up data, the method supports detailed plaque morphology analysis over time, including plaque volume quantification, aided by improved visualization via mesh unfolding. Results: We successfully extracted plaque meshes from 341 carotid arteries, capturing a wide range of plaque shapes with volumes ranging from 2.69{\mu}l to 847.7{\mu}l. The use of a case-specific threshold effectively eliminated false positives in young, healthy subjects. Conclusion: The proposed method enables precise extraction of plaque meshes from 3D vessel wall segmentation masks enabling a correspondence between baseline and one-year follow-up examinations. Unfolding the plaque meshes enhances visualization, while the mesh-based analysis allows quantification of plaque parameters independent of voxel resolution.</li>
</ul>

<h3>Title: Pitfalls of Scale: Investigating the Inverse Task of Redefinition in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Elena Stringli, Maria Lymperaiou, Giorgos Filandrianos, Giorgos Stamou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12821">https://arxiv.org/abs/2502.12821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12821">https://arxiv.org/pdf/2502.12821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12821]] Pitfalls of Scale: Investigating the Inverse Task of Redefinition in Large Language Models(https://arxiv.org/abs/2502.12821)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Inverse tasks can uncover potential reasoning gaps as Large Language Models (LLMs) scale up. In this work, we explore the redefinition task, in which we assign alternative values to well-known physical constants and units of measure, prompting LLMs to respond accordingly. Our findings show that not only does model performance degrade with scale, but its false confidence also rises. Moreover, while factors such as prompting strategies or response formatting are influential, they do not preclude LLMs from anchoring to memorized values.</li>
</ul>

<h3>Title: Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Rubing Lu, João Sedoc, Arun Sundararajan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12825">https://arxiv.org/abs/2502.12825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12825">https://arxiv.org/pdf/2502.12825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12825]] Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models(https://arxiv.org/abs/2502.12825)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>When encountering increasingly frequent performance improvements or cost reductions from a new large language model (LLM), developers of applications leveraging LLMs must decide whether to take advantage of these improvements or stay with older tried-and-tested models. Low perceived switching frictions can lead to choices that do not consider more subtle behavior changes that the transition may induce. Our experiments use a popular game-theoretic behavioral economics model of trust to show stark differences in the trusting behavior of OpenAI's and DeepSeek's models. We highlight a collapse in the economic trust behavior of the o1-mini and o3-mini models as they reconcile profit-maximizing and risk-seeking with future returns from trust, and contrast it with DeepSeek's more sophisticated and profitable trusting behavior that stems from an ability to incorporate deeper concepts like forward planning and theory-of-mind. As LLMs form the basis for high-stakes commercial systems, our results highlight the perils of relying on LLM performance benchmarks that are too narrowly defined and suggest that careful analysis of their hidden fault lines should be part of any organization's AI strategy.</li>
</ul>

<h3>Title: KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional Knowledge of Kazakhstan</h3>
<ul>
<li><strong>Authors: </strong>Mukhammed Togmanov, Nurdaulet Mukhituly, Diana Turmakhan, Jonibek Mansurov, Maiya Goloburda, Akhmed Sakip, Zhuohan Xie, Yuxia Wang, Bekassyl Syzdykov, Nurkhan Laiyk, Alham Fikri Aji, Ekaterina Kochmar, Preslav Nakov, Fajri Koto</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12829">https://arxiv.org/abs/2502.12829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12829">https://arxiv.org/pdf/2502.12829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12829]] KazMMLU: Evaluating Language Models on Kazakh, Russian, and Regional Knowledge of Kazakhstan(https://arxiv.org/abs/2502.12829)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite having a population of twenty million, Kazakhstan's culture and language remain underrepresented in the field of natural language processing. Although large language models (LLMs) continue to advance worldwide, progress in Kazakh language has been limited, as seen in the scarcity of dedicated models and benchmark evaluations. To address this gap, we introduce KazMMLU, the first MMLU-style dataset specifically designed for Kazakh language. KazMMLU comprises 23,000 questions that cover various educational levels, including STEM, humanities, and social sciences, sourced from authentic educational materials and manually validated by native speakers and educators. The dataset includes 10,969 Kazakh questions and 12,031 Russian questions, reflecting Kazakhstan's bilingual education system and rich local context. Our evaluation of several state-of-the-art multilingual models (Llama-3.1, Qwen-2.5, GPT-4, and DeepSeek V3) demonstrates substantial room for improvement, as even the best-performing models struggle to achieve competitive performance in Kazakh and Russian. These findings underscore significant performance gaps compared to high-resource languages. We hope that our dataset will enable further research and development of Kazakh-centric LLMs. Data and code will be made available upon acceptance.</li>
</ul>

<h3>Title: An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Feli, Iman Azimi, Pasi Liljeberg, Amir M.Rahmani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12836">https://arxiv.org/abs/2502.12836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12836">https://arxiv.org/pdf/2502.12836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12836]] An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation(https://arxiv.org/abs/2502.12836)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are revolutionizing healthcare by improving diagnosis, patient care, and decision support through interactive communication. More recently, they have been applied to analyzing physiological time-series like wearable data for health insight extraction. Existing methods embed raw numerical sequences directly into prompts, which exceeds token limits and increases computational costs. Additionally, some studies integrated features extracted from time-series in textual prompts or applied multimodal approaches. However, these methods often produce generic and unreliable outputs due to LLMs' limited analytical rigor and inefficiency in interpreting continuous waveforms. In this paper, we develop an LLM-powered agent for physiological time-series analysis aimed to bridge the gap in integrating LLMs with well-established analytical tools. Built on the OpenCHA, an open-source LLM-powered framework, our agent features an orchestrator that integrates user interaction, data sources, and analytical tools to generate accurate health insights. To evaluate its effectiveness, we implement a case study on heart rate (HR) estimation from Photoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram (ECG) recordings in a remote health monitoring study. The agent's performance is benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the gold standard for HR estimation. Results demonstrate that our agent significantly outperforms benchmark models by achieving lower error rates and more reliable HR estimations. The agent implementation is publicly available on GitHub.</li>
</ul>

<h3>Title: MOLLM: Multi-Objective Large Language Model for Molecular Design -- Optimizing with Experts</h3>
<ul>
<li><strong>Authors: </strong>Nian Ran, Yue Wang, Richard Allmendinger</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12845">https://arxiv.org/abs/2502.12845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12845">https://arxiv.org/pdf/2502.12845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12845]] MOLLM: Multi-Objective Large Language Model for Molecular Design -- Optimizing with Experts(https://arxiv.org/abs/2502.12845)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Molecular design plays a critical role in advancing fields such as drug discovery, materials science, and chemical engineering. This work introduces the Multi-Objective Large Language Model for Molecular Design (MOLLM), a novel framework that combines domain-specific knowledge with the adaptability of Large Language Models to optimize molecular properties across multiple objectives. Leveraging in-context learning and multi-objective optimization, MOLLM achieves superior efficiency, innovation, and performance, significantly surpassing state-of-the-art (SOTA) methods. Recognizing the substantial impact of initial populations on evolutionary algorithms, we categorize them into three types: best initial, worst initial, and random initial, to ensure the initial molecules are the same for each method across experiments. Our results demonstrate that MOLLM consistently outperforms SOTA models in all of our experiments. We also provide extensive ablation studies to evaluate the superiority of our components.</li>
</ul>

<h3>Title: Strands Rocq: Why is a Security Protocol Correct, Mechanically?</h3>
<ul>
<li><strong>Authors: </strong>Matteo Busi, Riccardo Focardi, Flaminia L. Luccio</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12848">https://arxiv.org/abs/2502.12848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12848">https://arxiv.org/pdf/2502.12848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12848]] Strands Rocq: Why is a Security Protocol Correct, Mechanically?(https://arxiv.org/abs/2502.12848)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Strand spaces are a formal framework for symbolic protocol verification that allows for pen-and-paper proofs of security. While extremely insightful, pen-and-paper proofs are error-prone, and it is hard to gain confidence on their correctness. To overcome this problem, we developed StrandsRocq, a full mechanization of the strand spaces in Coq (soon to be renamed Rocq). The mechanization was designed to be faithful to the original pen-and-paper development, and it was engineered to be modular and extensible. StrandsRocq incorporates new original proof techniques, a novel notion of maximal penetrator that enables protocol compositionality, and a set of Coq tactics tailored to the domain, facilitating proof automation and reuse, and simplifying the work of protocol analysts. To demonstrate the versatility of our approach, we modelled and analyzed a family of authentication protocols, drawing inspiration from ISO/IEC 9798-2 two-pass authentication, the classical Needham-Schroeder-Lowe protocol, as well as a recently-proposed static analysis for a key management API. The analyses in StrandsRocq confirmed the high degree of proof reuse, and enabled us to distill the minimal requirements for protocol security. Through mechanization, we identified and addressed several issues in the original proofs and we were able to significantly improve the precision of the static analysis for the key management API. Moreover, we were able to leverage the novel notion of maximal penetrator to provide a compositional proof of security for two simple authentication protocols.</li>
</ul>

<h3>Title: MeMo: Towards Language Models with Associative Memory Mechanisms</h3>
<ul>
<li><strong>Authors: </strong>Fabio Massimo Zanzotto, Elena Sofia Ruzzetti, Giancarlo A. Xompero, Leonardo Ranaldi, Davide Venditti, Federico Ranaldi, Cristina Giannone, Andrea Favalli, Raniero Romagnoli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12851">https://arxiv.org/abs/2502.12851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12851">https://arxiv.org/pdf/2502.12851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12851]] MeMo: Towards Language Models with Associative Memory Mechanisms(https://arxiv.org/abs/2502.12851)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Memorization is a fundamental ability of Transformer-based Large Language Models, achieved through learning. In this paper, we propose a paradigm shift by designing an architecture to memorize text directly, bearing in mind the principle that memorization precedes learning. We introduce MeMo, a novel architecture for language modeling that explicitly memorizes sequences of tokens in layered associative memories. By design, MeMo offers transparency and the possibility of model editing, including forgetting texts. We experimented with the MeMo architecture, showing the memorization power of the one-layer and the multi-layer configurations.</li>
</ul>

<h3>Title: Rejected Dialects: Biases Against African American Language in Reward Models</h3>
<ul>
<li><strong>Authors: </strong>Joel Mire, Zubin Trivadi Aysola, Daniel Chechelnitsky, Nicholas Deas, Chrysoula Zerva, Maarten Sap</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12858">https://arxiv.org/abs/2502.12858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12858">https://arxiv.org/pdf/2502.12858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12858]] Rejected Dialects: Biases Against African American Language in Reward Models(https://arxiv.org/abs/2502.12858)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Preference alignment via reward models helps build safe, helpful, and reliable large language models (LLMs). However, subjectivity in preference judgments and the lack of representative sampling in preference data collection can introduce new biases, hindering reward models' fairness and equity. In this work, we introduce a framework for evaluating dialect biases in reward models and conduct a case study on biases against African American Language (AAL) through several experiments comparing reward model preferences and behavior on paired White Mainstream English (WME) and both machine-translated and human-written AAL corpora. We show that reward models are less aligned with human preferences when processing AAL texts vs. WME ones (-4\% accuracy on average), frequently disprefer AAL-aligned texts vs. WME-aligned ones, and steer conversations toward WME, even when prompted with AAL texts. Our findings provide a targeted analysis of anti-AAL biases at a relatively understudied stage in LLM development, highlighting representational harms and ethical questions about the desired behavior of LLMs concerning AAL.</li>
</ul>

<h3>Title: PAFT: Prompt-Agnostic Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Chenxing Wei, Yao Shu, Mingwen Ou, Ying Tiffany He, Fei Richard Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12859">https://arxiv.org/abs/2502.12859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12859">https://arxiv.org/pdf/2502.12859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12859]] PAFT: Prompt-Agnostic Fine-Tuning(https://arxiv.org/abs/2502.12859)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) adapt well to downstream tasks after fine-tuning, this adaptability often compromises prompt robustness, as even minor prompt variations can significantly degrade performance. To address this, we propose Prompt-Agnostic Fine-Tuning(PAFT), a simple yet effective approach that dynamically adjusts prompts during fine-tuning. This encourages the model to learn underlying task principles rather than overfitting to specific prompt formulations. PAFT operates in two stages: First, a diverse set of meaningful, synthetic candidate prompts is constructed. Second, during fine-tuning, prompts are randomly sampled from this set to create dynamic training inputs. Extensive experiments across diverse datasets and LLMs demonstrate that models trained with PAFT exhibit strong robustness and generalization across a wide range of prompts, including unseen ones. This enhanced robustness improves both model performance and inference speed while maintaining training efficiency. Ablation studies further confirm the effectiveness of PAFT.</li>
</ul>

<h3>Title: An Experimental Study of SOTA LiDAR Segmentation Models</h3>
<ul>
<li><strong>Authors: </strong>Bike Chen, Antti Tikanmäki, Juha Röning</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12860">https://arxiv.org/abs/2502.12860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12860">https://arxiv.org/pdf/2502.12860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12860]] An Experimental Study of SOTA LiDAR Segmentation Models(https://arxiv.org/abs/2502.12860)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Point cloud segmentation (PCS) is to classify each point in point clouds. The task enables robots to parse their 3D surroundings and run autonomously. According to different point cloud representations, existing PCS models can be roughly divided into point-, voxel-, and range image-based models. However, no work has been found to report comprehensive comparisons among the state-of-the-art point-, voxel-, and range image-based models from an application perspective, bringing difficulty in utilizing these models for real-world scenarios. In this paper, we provide thorough comparisons among the models by considering the LiDAR data motion compensation and the metrics of model parameters, max GPU memory allocated during testing, inference latency, frames per second, intersection-over-union (IoU) and mean IoU (mIoU) scores. The experimental results benefit engineers when choosing a reasonable PCS model for an application and inspire researchers in the PCS field to design more practical models for a real-world scenario.</li>
</ul>

<h3>Title: Malware Detection based on API calls</h3>
<ul>
<li><strong>Authors: </strong>Christofer Fellicious, Manuel Bischof, Kevin Mayer, Dorian Eikenberg, Stefan Hausotte, Hans P. Reiser, Michael Granitzer</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12863">https://arxiv.org/abs/2502.12863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12863">https://arxiv.org/pdf/2502.12863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12863]] Malware Detection based on API calls(https://arxiv.org/abs/2502.12863)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack</a></li>
<li><strong>Abstract: </strong>Malware attacks pose a significant threat in today's interconnected digital landscape, causing billions of dollars in damages. Detecting and identifying families as early as possible provides an edge in protecting against such malware. We explore a lightweight, order-invariant approach to detecting and mitigating malware threats: analyzing API calls without regard to their sequence. We publish a public dataset of over three hundred thousand samples and their function call parameters for this task, annotated with labels indicating benign or malicious activity. The complete dataset is above 550GB uncompressed in size. We leverage machine learning algorithms, such as random forests, and conduct behavioral analysis by examining patterns and anomalies in API call sequences. By investigating how the function calls occur regardless of their order, we can identify discriminating features that can help us identify malware early on. The models we've developed are not only effective but also efficient. They are lightweight and can run on any machine with minimal performance overhead, while still achieving an impressive F1-Score of over 85\%. We also empirically show that we only need a subset of the function call sequence, specifically calls to the this http URL library, to identify malware. Our research demonstrates the efficacy of this approach through empirical evaluations, underscoring its accuracy and scalability. The code is open source and available at Github along with the dataset on Zenodo.</li>
</ul>

<h3>Title: Testing for Causal Fairness</h3>
<ul>
<li><strong>Authors: </strong>Jiarun Fu, LiZhong Ding, Pengqi Li, Qiuning Wei, Yurong Cheng, Xu Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12874">https://arxiv.org/abs/2502.12874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12874">https://arxiv.org/pdf/2502.12874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12874]] Testing for Causal Fairness(https://arxiv.org/abs/2502.12874)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Causality is widely used in fairness analysis to prevent discrimination on sensitive attributes, such as genders in career recruitment and races in crime prediction. However, the current data-based Potential Outcomes Framework (POF) often leads to untrustworthy fairness analysis results when handling high-dimensional data. To address this, we introduce a distribution-based POF that transform fairness analysis into Distributional Closeness Testing (DCT) by intervening on sensitive attributes. We define counterfactual closeness fairness as the null hypothesis of DCT, where a sensitive attribute is considered fair if its factual and counterfactual potential outcome distributions are sufficiently close. We introduce the Norm-Adaptive Maximum Mean Discrepancy Treatment Effect (N-TE) as a statistic for measuring distributional closeness and apply DCT using the empirical estimator of NTE, referred to Counterfactual Fairness-CLOseness Testing ($\textrm{CF-CLOT}$). To ensure the trustworthiness of testing results, we establish the testing consistency of N-TE through rigorous theoretical analysis. $\textrm{CF-CLOT}$ demonstrates sensitivity in fairness analysis through the flexibility of the closeness parameter $\epsilon$. Unfair sensitive attributes have been successfully tested by $\textrm{CF-CLOT}$ in extensive experiments across various real-world scenarios, which validate the consistency of the testing.</li>
</ul>

<h3>Title: How desirable is alignment between LLMs and linguistically diverse human users?</h3>
<ul>
<li><strong>Authors: </strong>Pia Knoeferle, Sebastian Möller, Dorothea Kolossa, Veronika Solopova, Georg Rehm</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12884">https://arxiv.org/abs/2502.12884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12884">https://arxiv.org/pdf/2502.12884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12884]] How desirable is alignment between LLMs and linguistically diverse human users?(https://arxiv.org/abs/2502.12884)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We discuss how desirable it is that Large Language Models (LLMs) be able to adapt or align their language behavior with users who may be diverse in their language use. User diversity may come about among others due to i) age differences; ii) gender characteristics, and/or iii) multilingual experience, and associated differences in language processing and use. We consider potential consequences for usability, communication, and LLM development.</li>
</ul>

<h3>Title: Are Multilingual Language Models an Off-ramp for Under-resourced Languages? Will we arrive at Digital Language Equality in Europe in 2030?</h3>
<ul>
<li><strong>Authors: </strong>Georg Rehm, Annika Grützner-Zahn, Fabio Barth</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12886">https://arxiv.org/abs/2502.12886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12886">https://arxiv.org/pdf/2502.12886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12886]] Are Multilingual Language Models an Off-ramp for Under-resourced Languages? Will we arrive at Digital Language Equality in Europe in 2030?(https://arxiv.org/abs/2502.12886)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate unprecedented capabilities and define the state of the art for almost all natural language processing (NLP) tasks and also for essentially all Language Technology (LT) applications. LLMs can only be trained for languages for which a sufficient amount of pre-training data is available, effectively excluding many languages that are typically characterised as under-resourced. However, there is both circumstantial and empirical evidence that multilingual LLMs, which have been trained using data sets that cover multiple languages (including under-resourced ones), do exhibit strong capabilities for some of these under-resourced languages. Eventually, this approach may have the potential to be a technological off-ramp for those under-resourced languages for which "native" LLMs, and LLM-based technologies, cannot be developed due to a lack of training data. This paper, which concentrates on European languages, examines this idea, analyses the current situation in terms of technology support and summarises related work. The article concludes by focusing on the key open questions that need to be answered for the approach to be put into practice in a systematic way.</li>
</ul>

<h3>Title: Archetypal SAE: Adaptive and Stable Dictionary Learning for Concept Extraction in Large Vision Models</h3>
<ul>
<li><strong>Authors: </strong>Thomas Fel, Ekdeep Singh Lubana, Jacob S. Prince, Matthew Kowal, Victor Boutin, Isabel Papadimitriou, Binxu Wang, Martin Wattenberg, Demba Ba, Talia Konkle</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12892">https://arxiv.org/abs/2502.12892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12892">https://arxiv.org/pdf/2502.12892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12892]] Archetypal SAE: Adaptive and Stable Dictionary Learning for Concept Extraction in Large Vision Models(https://arxiv.org/abs/2502.12892)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability</a></li>
<li><strong>Abstract: </strong>Sparse Autoencoders (SAEs) have emerged as a powerful framework for machine learning interpretability, enabling the unsupervised decomposition of model representations into a dictionary of abstract, human-interpretable concepts. However, we reveal a fundamental limitation: existing SAEs exhibit severe instability, as identical models trained on similar datasets can produce sharply different dictionaries, undermining their reliability as an interpretability tool. To address this issue, we draw inspiration from the Archetypal Analysis framework introduced by Cutler & Breiman (1994) and present Archetypal SAEs (A-SAE), wherein dictionary atoms are constrained to the convex hull of data. This geometric anchoring significantly enhances the stability of inferred dictionaries, and their mildly relaxed variants RA-SAEs further match state-of-the-art reconstruction abilities. To rigorously assess dictionary quality learned by SAEs, we introduce two new benchmarks that test (i) plausibility, if dictionaries recover "true" classification directions and (ii) identifiability, if dictionaries disentangle synthetic concept mixtures. Across all evaluations, RA-SAEs consistently yield more structured representations while uncovering novel, semantically meaningful concepts in large-scale vision models.</li>
</ul>

<h3>Title: H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking</h3>
<ul>
<li><strong>Authors: </strong>Martin Kuo, Jianyi Zhang, Aolin Ding, Qinsi Wang, Louis DiValentin, Yujia Bao, Wei Wei, Da-Cheng Juan, Hai Li, Yiran Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12893">https://arxiv.org/abs/2502.12893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12893">https://arxiv.org/pdf/2502.12893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12893]] H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models, Including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking(https://arxiv.org/abs/2502.12893)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Large Reasoning Models (LRMs) have recently extended their powerful reasoning capabilities to safety checks-using chain-of-thought reasoning to decide whether a request should be answered. While this new approach offers a promising route for balancing model utility and safety, its robustness remains underexplored. To address this gap, we introduce Malicious-Educator, a benchmark that disguises extremely dangerous or malicious requests beneath seemingly legitimate educational prompts. Our experiments reveal severe security flaws in popular commercial-grade LRMs, including OpenAI o1/o3, DeepSeek-R1, and Gemini 2.0 Flash Thinking. For instance, although OpenAI's o1 model initially maintains a high refusal rate of about 98%, subsequent model updates significantly compromise its safety; and attackers can easily extract criminal strategies from DeepSeek-R1 and Gemini 2.0 Flash Thinking without any additional tricks. To further highlight these vulnerabilities, we propose Hijacking Chain-of-Thought (H-CoT), a universal and transferable attack method that leverages the model's own displayed intermediate reasoning to jailbreak its safety reasoning mechanism. Under H-CoT, refusal rates sharply decline-dropping from 98% to below 2%-and, in some instances, even transform initially cautious tones into ones that are willing to provide harmful content. We hope these findings underscore the urgent need for more robust safety mechanisms to preserve the benefits of advanced reasoning capabilities without compromising ethical standards.</li>
</ul>

<h3>Title: CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image</h3>
<ul>
<li><strong>Authors: </strong>Kaixin Yao, Longwen Zhang, Xinhao Yan, Yan Zeng, Qixuan Zhang, Lan Xu, Wei Yang, Jiayuan Gu, Jingyi Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12894">https://arxiv.org/abs/2502.12894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12894">https://arxiv.org/pdf/2502.12894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12894]] CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image(https://arxiv.org/abs/2502.12894)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recovering high-quality 3D scenes from a single RGB image is a challenging task in computer graphics. Current methods often struggle with domain-specific limitations or low-quality object generation. To address these, we propose CAST (Component-Aligned 3D Scene Reconstruction from a Single RGB Image), a novel method for 3D scene reconstruction and recovery. CAST starts by extracting object-level 2D segmentation and relative depth information from the input image, followed by using a GPT-based model to analyze inter-object spatial relationships. This enables the understanding of how objects relate to each other within the scene, ensuring more coherent reconstruction. CAST then employs an occlusion-aware large-scale 3D generation model to independently generate each object's full geometry, using MAE and point cloud conditioning to mitigate the effects of occlusions and partial object information, ensuring accurate alignment with the source image's geometry and texture. To align each object with the scene, the alignment generation model computes the necessary transformations, allowing the generated meshes to be accurately placed and integrated into the scene's point cloud. Finally, CAST incorporates a physics-aware correction step that leverages a fine-grained relation graph to generate a constraint graph. This graph guides the optimization of object poses, ensuring physical consistency and spatial coherence. By utilizing Signed Distance Fields (SDF), the model effectively addresses issues such as occlusions, object penetration, and floating objects, ensuring that the generated scene accurately reflects real-world physical interactions. CAST can be leveraged in robotics, enabling efficient real-to-simulation workflows and providing realistic, scalable simulation environments for robotic systems.</li>
</ul>

<h3>Title: Multilingual European Language Models: Benchmarking Approaches and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Fabio Barth, Georg Rehm</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12895">https://arxiv.org/abs/2502.12895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12895">https://arxiv.org/pdf/2502.12895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12895]] Multilingual European Language Models: Benchmarking Approaches and Challenges(https://arxiv.org/abs/2502.12895)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The breakthrough of generative large language models (LLMs) that can solve different tasks through chat interaction has led to a significant increase in the use of general benchmarks to assess the quality or performance of these models beyond individual applications. There is also a need for better methods to evaluate and also to compare models due to the ever increasing number of new models published. However, most of the established benchmarks revolve around the English language. This paper analyses the benefits and limitations of current evaluation datasets, focusing on multilingual European benchmarks. We analyse seven multilingual benchmarks and identify four major challenges. Furthermore, we discuss potential solutions to enhance translation quality and mitigate cultural biases, including human-in-the-loop verification and iterative translation ranking. Our analysis highlights the need for culturally aware and rigorously validated benchmarks to assess the reasoning and question-answering capabilities of multilingual LLMs accurately.</li>
</ul>

<h3>Title: None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Eva Sánchez Salido, Julio Gonzalo, Guillermo Marco</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12896">https://arxiv.org/abs/2502.12896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12896">https://arxiv.org/pdf/2502.12896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12896]] None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks(https://arxiv.org/abs/2502.12896)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In LLM evaluations, reasoning is often distinguished from recall/memorization by performing numerical variations to math-oriented questions. Here we introduce a general variation method for multiple-choice questions that completely dissociates the correct answer from previously seen tokens or concepts, requiring LLMs to understand and reason (rather than memorizing) in order to answer correctly. Using this method, we evaluate state-of-the-art proprietary and open-source LLMs on two datasets available in English and Spanish: the public MMLU benchmark and the private UNED-Access 2024 dataset. Results show that all models experience remarkable accuracy drops under our proposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access 2024, ranging from 10% to 93% across models. Notably, the most accurate model in our experimentation (OpenAI-o3-mini) is not the most robust (DeepSeek-R1-70B), suggesting that the best models in standard evaluations may not be the ones with better reasoning capabilities. Also, we see larger accuracy drops in public (vs private) datasets and questions posed in their original language (vs a manual translation), which are signs of contamination and also point to a relevant role of recall/memorization in current LLMs' answers.</li>
</ul>

<h3>Title: The Relationship Between Head Injury and Alzheimer's Disease: A Causal Analysis with Bayesian Networks</h3>
<ul>
<li><strong>Authors: </strong>Andrei Lixandru</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12898">https://arxiv.org/abs/2502.12898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12898">https://arxiv.org/pdf/2502.12898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12898]] The Relationship Between Head Injury and Alzheimer's Disease: A Causal Analysis with Bayesian Networks(https://arxiv.org/abs/2502.12898)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>This study examines the potential causal relationship between head injury and the risk of developing Alzheimer's disease (AD) using Bayesian networks and regression models. Using a dataset of 2,149 patients, we analyze key medical history variables, including head injury history, memory complaints, cardiovascular disease, and diabetes. Logistic regression results suggest an odds ratio of 0.88 for head injury, indicating a potential but statistically insignificant protective effect against AD. In contrast, memory complaints exhibit a strong association with AD, with an odds ratio of 4.59. Linear regression analysis further confirms the lack of statistical significance for head injury (coefficient: -0.0245, p = 0.469) while reinforcing the predictive importance of memory complaints. These findings highlight the complex interplay of medical history factors in AD risk assessment and underscore the need for further research utilizing larger datasets and advanced causal modeling techniques.</li>
</ul>

<h3>Title: Soundwave: Less is More for Speech-Text Alignment in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Zhang, Zhiheng Liu, Fan Bu, Ruiyu Zhang, Benyou Wang, Haizhou Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12900">https://arxiv.org/abs/2502.12900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12900">https://arxiv.org/pdf/2502.12900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12900]] Soundwave: Less is More for Speech-Text Alignment in LLMs(https://arxiv.org/abs/2502.12900)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing end-to-end speech large language models (LLMs) usually rely on large-scale annotated data for training, while data-efficient training has not been discussed in depth. We focus on two fundamental problems between speech and text: the representation space gap and sequence length inconsistency. We propose Soundwave, which utilizes an efficient training strategy and a novel architecture to address these issues. Results show that Soundwave outperforms the advanced Qwen2-Audio in speech translation and AIR-Bench speech tasks, using only one-fiftieth of the training data. Further analysis shows that Soundwave still retains its intelligence during conversation. The project is available at this https URL.</li>
</ul>

<h3>Title: Probabilistic neural operators for functional uncertainty quantification</h3>
<ul>
<li><strong>Authors: </strong>Christopher Bülte, Philipp Scholl, Gitta Kutyniok</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12902">https://arxiv.org/abs/2502.12902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12902">https://arxiv.org/pdf/2502.12902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12902]] Probabilistic neural operators for functional uncertainty quantification(https://arxiv.org/abs/2502.12902)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Neural operators aim to approximate the solution operator of a system of differential equations purely from data. They have shown immense success in modeling complex dynamical systems across various domains. However, the occurrence of uncertainties inherent in both model and data has so far rarely been taken into account\textemdash{}a critical limitation in complex, chaotic systems such as weather forecasting. In this paper, we introduce the probabilistic neural operator (PNO), a framework for learning probability distributions over the output function space of neural operators. PNO extends neural operators with generative modeling based on strictly proper scoring rules, integrating uncertainty information directly into the training process. We provide a theoretical justification for the approach and demonstrate improved performance in quantifying uncertainty across different domains and with respect to different baselines. Furthermore, PNO requires minimal adjustment to existing architectures, shows improved performance for most probabilistic prediction tasks, and leads to well-calibrated predictive distributions and adequate uncertainty representations even for long dynamical trajectories. Implementing our approach into large-scale models for physical applications can lead to improvements in corresponding uncertainty quantification and extreme event identification, ultimately leading to a deeper understanding of the prediction of such surrogate models.</li>
</ul>

<h3>Title: Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of LLM Against Augmented Fraud and Phishing Inducements</h3>
<ul>
<li><strong>Authors: </strong>Shu Yang, Shenzhe Zhu, Zeyu Wu, Keyu Wang, Junchi Yao, Junchao Wu, Lijie Hu, Mengdi Li, Derek F. Wong, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12904">https://arxiv.org/abs/2502.12904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12904">https://arxiv.org/pdf/2502.12904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12904]] Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of LLM Against Augmented Fraud and Phishing Inducements(https://arxiv.org/abs/2502.12904)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce Fraud-R1, a benchmark designed to evaluate LLMs' ability to defend against internet fraud and phishing in dynamic, real-world scenarios. Fraud-R1 comprises 8,564 fraud cases sourced from phishing scams, fake job postings, social media, and news, categorized into 5 major fraud types. Unlike previous benchmarks, Fraud-R1 introduces a multi-round evaluation pipeline to assess LLMs' resistance to fraud at different stages, including credibility building, urgency creation, and emotional manipulation. Furthermore, we evaluate 15 LLMs under two settings: 1. Helpful-Assistant, where the LLM provides general decision-making assistance, and 2. Role-play, where the model assumes a specific persona, widely used in real-world agent-based interactions. Our evaluation reveals the significant challenges in defending against fraud and phishing inducement, especially in role-play settings and fake job postings. Additionally, we observe a substantial performance gap between Chinese and English, underscoring the need for improved multilingual fraud detection capabilities.</li>
</ul>

<h3>Title: GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training for LLMs On-Device Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Sifan Zhou, Shuo Wang, Zhihang Yuan, Mingjia Shi, Yuzhang Shang, Dawei Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12913">https://arxiv.org/abs/2502.12913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12913">https://arxiv.org/pdf/2502.12913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12913]] GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training for LLMs On-Device Fine-tuning(https://arxiv.org/abs/2502.12913)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) fine-tuning technologies have achieved remarkable results. However, traditional LLM fine-tuning approaches face significant challenges: they require large Floating Point (FP) computation, raising privacy concerns when handling sensitive data, and are impractical for resource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT) techniques reduce trainable parameters, their reliance on floating-point arithmetic creates fundamental incompatibilities with edge hardware. In this work, we introduce a novel framework for on-device LLM fine-tuning that eliminates the need for floating-point operations in both inference and training, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer format, which efficiently represents model parameters in integer format using shared exponents among parameter groups. When combined with LoRA-like adapters, this enables fully integer-based fine-tuning that is both memory and compute efficient. We demonstrate that our approach achieves accuracy comparable to FP16-based fine-tuning while significantly reducing memory usage (50%). Moreover, compared to FP8, our method can reduce 5x power consumption and 11x chip area with same performance, making large-scale model adaptation feasible on edge devices.</li>
</ul>

<h3>Title: Q-STRUM Debate: Query-Driven Contrastive Summarization for Recommendation Comparison</h3>
<ul>
<li><strong>Authors: </strong>George-Kirollos Saad, Scott Sanner</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12921">https://arxiv.org/abs/2502.12921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12921">https://arxiv.org/pdf/2502.12921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12921]] Q-STRUM Debate: Query-Driven Contrastive Summarization for Recommendation Comparison(https://arxiv.org/abs/2502.12921)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Query-driven recommendation with unknown items poses a challenge for users to understand why certain items are appropriate for their needs. Query-driven Contrastive Summarization (QCS) is a methodology designed to address this issue by leveraging language-based item descriptions to clarify contrasts between them. However, existing state-of-the-art contrastive summarization methods such as STRUM-LLM fall short of this goal. To overcome these limitations, we introduce Q-STRUM Debate, a novel extension of STRUM-LLM that employs debate-style prompting to generate focused and contrastive summarizations of item aspects relevant to a query. Leveraging modern large language models (LLMs) as powerful tools for generating debates, Q-STRUM Debate provides enhanced contrastive summaries. Experiments across three datasets demonstrate that Q-STRUM Debate yields significant performance improvements over existing methods on key contrastive summarization criteria, thus introducing a novel and performant debate prompting methodology for QCS.</li>
</ul>

<h3>Title: On-Device LLMs for Home Assistant: Dual Role in Intent Detection and Response Generation</h3>
<ul>
<li><strong>Authors: </strong>Rune Birkmose, Nathan Mørkeberg Reece, Esben Hofstedt Norvin, Johannes Bjerva, Mike Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12923">https://arxiv.org/abs/2502.12923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12923">https://arxiv.org/pdf/2502.12923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12923]] On-Device LLMs for Home Assistant: Dual Role in Intent Detection and Response Generation(https://arxiv.org/abs/2502.12923)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>This paper investigates whether Large Language Models (LLMs), fine-tuned on synthetic but domain-representative data, can perform the twofold task of (i) slot and intent detection and (ii) natural language response generation for a smart home assistant, while running solely on resource-limited, CPU-only edge hardware. We fine-tune LLMs to produce both JSON action calls and text responses. Our experiments show that 16-bit and 8-bit quantized variants preserve high accuracy on slot and intent detection and maintain strong semantic coherence in generated text, while the 4-bit model, while retaining generative fluency, suffers a noticeable drop in device-service classification accuracy. Further evaluations on noisy human (non-synthetic) prompts and out-of-domain intents confirm the models' generalization ability, obtaining around 80--86\% accuracy. While the average inference time is 5--6 seconds per query -- acceptable for one-shot commands but suboptimal for multi-turn dialogue -- our results affirm that an on-device LLM can effectively unify command interpretation and flexible response generation for home automation without relying on specialized hardware.</li>
</ul>

<h3>Title: Conditioning LLMs to Generate Code-Switched Text: A Methodology Grounded in Naturally Occurring Data</h3>
<ul>
<li><strong>Authors: </strong>Maite Heredia, Gorka Labaka, Jeremy Barnes, Aitor Soroa</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12924">https://arxiv.org/abs/2502.12924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12924">https://arxiv.org/pdf/2502.12924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12924]] Conditioning LLMs to Generate Code-Switched Text: A Methodology Grounded in Naturally Occurring Data(https://arxiv.org/abs/2502.12924)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Code-switching (CS) is still a critical challenge in Natural Language Processing (NLP). Current Large Language Models (LLMs) struggle to interpret and generate code-switched text, primarily due to the scarcity of large-scale CS datasets for training. This paper presents a novel methodology to generate CS data using LLMs, and test it on the English-Spanish language pair. We propose back-translating natural CS sentences into monolingual English, and using the resulting parallel corpus to fine-tune LLMs to turn monolingual sentences into CS. Unlike previous approaches to CS generation, our methodology uses natural CS data as a starting point, allowing models to learn its natural distribution beyond grammatical patterns. We thoroughly analyse the models' performance through a study on human preferences, a qualitative error analysis and an evaluation with popular automatic metrics. Results show that our methodology generates fluent code-switched text, expanding research opportunities in CS communication, and that traditional metrics do not correlate with human judgement when assessing the quality of the generated CS data. We release our code and generated dataset under a CC-BY-NC-SA license.</li>
</ul>

<h3>Title: SEFL: Harnessing Large Language Model Agents to Improve Educational Feedback Systems</h3>
<ul>
<li><strong>Authors: </strong>Mike Zhang, Amalie Pernille Dilling, Léon Gondelman, Niels Erik Ruan Lyngdorf, Euan D. Lindsay, Johannes Bjerva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12927">https://arxiv.org/abs/2502.12927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12927">https://arxiv.org/pdf/2502.12927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12927]] SEFL: Harnessing Large Language Model Agents to Improve Educational Feedback Systems(https://arxiv.org/abs/2502.12927)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Providing high-quality feedback is crucial for student success but is constrained by time, cost, and limited data availability. We introduce Synthetic Educational Feedback Loops (SEFL), a novel framework designed to deliver immediate, on-demand feedback at scale without relying on extensive, real-world student data. In SEFL, two large language models (LLMs) operate in teacher--student roles to simulate assignment completion and formative feedback, generating abundant synthetic pairs of student work and corresponding critiques. We then fine-tune smaller, more computationally efficient LLMs on these synthetic pairs, enabling them to replicate key features of high-quality, goal-oriented feedback. Unlike personalized tutoring approaches that offer multi-turn, individualized instruction, SEFL specifically focuses on replicating the teacher-->student feedback loop for diverse assignments. Through both LLM-as-a-judge and human evaluations, we demonstrate that SEFL-tuned models outperform their non-tuned counterparts in feedback quality, clarity, and timeliness. These findings reveal SEFL's potential to transform feedback processes for higher education and beyond, offering an ethical and scalable alternative to conventional manual feedback cycles.</li>
</ul>

<h3>Title: Finedeep: Mitigating Sparse Activation in Dense LLMs via Multi-Layer Fine-Grained Experts</h3>
<ul>
<li><strong>Authors: </strong>Leiyu Pan, Zhenpeng Su, Minxuan Lv, Yizhe Xiong, Xiangwen Zhang, Zijia Lin, Hui Chen, Jungong Han, Guiguang Ding, Cheng Luo, Di Zhang, Kun Gai, Deyi Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12928">https://arxiv.org/abs/2502.12928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12928">https://arxiv.org/pdf/2502.12928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12928]] Finedeep: Mitigating Sparse Activation in Dense LLMs via Multi-Layer Fine-Grained Experts(https://arxiv.org/abs/2502.12928)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models have demonstrated exceptional performance across a wide range of tasks. However, dense models usually suffer from sparse activation, where many activation values tend towards zero (i.e., being inactivated). We argue that this could restrict the efficient exploration of model representation space. To mitigate this issue, we propose Finedeep, a deep-layered fine-grained expert architecture for dense models. Our framework partitions the feed-forward neural network layers of traditional dense models into small experts, arranges them across multiple sub-layers. A novel routing mechanism is proposed to determine each expert's contribution. We conduct extensive experiments across various model sizes, demonstrating that our approach significantly outperforms traditional dense architectures in terms of perplexity and benchmark performance while maintaining a comparable number of parameters and floating-point operations. Moreover, we find that Finedeep achieves optimal results when balancing depth and width, specifically by adjusting the number of expert sub-layers and the number of experts per sub-layer. Empirical results confirm that Finedeep effectively alleviates sparse activation and efficiently utilizes representation capacity in dense models.</li>
</ul>

<h3>Title: Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options</h3>
<ul>
<li><strong>Authors: </strong>Lakshmi Nair, Ian Trase, Mark Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12929">https://arxiv.org/abs/2502.12929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12929">https://arxiv.org/pdf/2502.12929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12929]] Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options(https://arxiv.org/abs/2502.12929)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present a novel reasoning approach called Flow-of-Options (FoO), designed to address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs to systematically explore a diverse range of possibilities in their reasoning, as demonstrated by an FoO-based agentic system for autonomously solving Machine Learning tasks (AutoML). Our framework outperforms state-of-the-art baselines, achieving improvements of 38.2% - 69.2% on standard data science tasks, and 37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost under $1 per task, our framework is well-suited for cost-sensitive applications. Beyond classification and regression, we illustrate the broader applicability of our FoO-based agentic system to tasks such as reinforcement learning and image generation. Our framework presents significant advancements compared to current state-of-the-art agentic systems for AutoML, due to the benefits of FoO in enforcing diversity in LLM solutions through compressed, explainable representations that also support long-term memory when combined with case-based reasoning.</li>
</ul>

<h3>Title: LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular Micro-video Generation</h3>
<ul>
<li><strong>Authors: </strong>Junchen Fu, Xuri Ge, Kaiwen Zheng, Ioannis Arapakis, Xin Xin, Joemon M. Jose</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12945">https://arxiv.org/abs/2502.12945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12945">https://arxiv.org/pdf/2502.12945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12945]] LLMPopcorn: An Empirical Study of LLMs as Assistants for Popular Micro-video Generation(https://arxiv.org/abs/2502.12945)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Popular Micro-videos, dominant on platforms like TikTok and YouTube, hold significant commercial value. The rise of high-quality AI-generated content has spurred interest in AI-driven micro-video creation. However, despite the advanced capabilities of large language models (LLMs) like ChatGPT and DeepSeek in text generation and reasoning, their potential to assist the creation of popular micro-videos remains largely unexplored. In this paper, we conduct an empirical study on LLM-assisted popular micro-video generation (LLMPopcorn). Specifically, we investigate the following research questions: (i) How can LLMs be effectively utilized to assist popular micro-video generation? (ii) To what extent can prompt-based enhancements optimize the LLM-generated content for higher popularity? (iii) How well do various LLMs and video generators perform in the popular micro-video generation task? By exploring these questions, we show that advanced LLMs like DeepSeek-V3 enable micro-video generation to achieve popularity comparable to human-created content. Prompt enhancements further boost popularity, and benchmarking highlights DeepSeek-V3 and DeepSeek-R1 among LLMs, while LTX-Video and HunyuanVideo lead in video generation. This pioneering work advances AI-assisted micro-video creation, uncovering new research opportunities. We will release the code and datasets to support future studies.</li>
</ul>

<h3>Title: Every Expert Matters: Towards Effective Knowledge Distillation for Mixture-of-Experts Language Models</h3>
<ul>
<li><strong>Authors: </strong>Gyeongman Kim, Gyouk Chu, Eunho Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12947">https://arxiv.org/abs/2502.12947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12947">https://arxiv.org/pdf/2502.12947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12947]] Every Expert Matters: Towards Effective Knowledge Distillation for Mixture-of-Experts Language Models(https://arxiv.org/abs/2502.12947)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the emergence of Mixture-of-Experts (MoE), the efficient scaling of model size has accelerated the development of large language models in recent years. However, their high memory requirements prevent their use in resource-constrained environments. While knowledge distillation (KD) has been a proven method for model compression, its application to MoE teacher models remains underexplored. Through our investigation, we discover that non-activated experts in MoE models possess valuable knowledge that benefits student models. We further demonstrate that existing KD methods are not optimal for compressing MoE models, as they fail to leverage this knowledge effectively. To address this, we propose two intuitive MoE-specific KD methods for the first time: Knowledge Augmentation (KA) and Student-Aware Router (SAR), both designed to effectively extract knowledge from all experts. Specifically, KA augments knowledge by sampling experts multiple times, while SAR uses all experts and adjusts the expert weights through router training to provide optimal knowledge. Extensive experiments show that our methods outperform conventional KD methods, demonstrating their effectiveness for MoE teacher models.</li>
</ul>

<h3>Title: Efficient Learning Under Density Shift in Incremental Settings Using Cramér-Rao-Based Regularization</h3>
<ul>
<li><strong>Authors: </strong>Behraj Khan, Behroz Mirza, Nouman Durrani, Tahir Syed</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12949">https://arxiv.org/abs/2502.12949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12949">https://arxiv.org/pdf/2502.12949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12949]] Efficient Learning Under Density Shift in Incremental Settings Using Cramér-Rao-Based Regularization(https://arxiv.org/abs/2502.12949)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The continuous surge in data volume and velocity is often dealt with using data orchestration and distributed processing approaches, abstracting away the machine learning challenges that exist at the algorithmic level. With growing interest in automating the learning loop, training with data that arrive in a sequence rather than in the classical in-memory training data form will face a machine learning challenge because of evolving feature distributions across batches of training data biasing the cross-validation step (\cite{sugiyama2012machine}). This work takes a distributed density estimation angle to the problem where data are temporally distributed. It processes data in batches and allows a neural network to treat a batch as training data. The method accumulates knowledge about the data density via posterior probability absorption using the Fisher Information Matrix, which contains information about the local optimization gradients for the batch. This is then used as a regularizer for the loss in the following batch, and therefore the density estimate for the entire dataset constructively gets more robust to the non-iid distribution shift. This needs the presence of a pair of batches in memory at a time, so the space cost is not a function of the size of the complete, distributed dataset. We proposed a novel regularization-based approach Covariate Shift Correction $C^{2}A$ that leverages Fisher information and Kullback-Leibler divergence to adapt to both natural and sequential covariate shift caused by dataset fragmentation. $C^{2}A$ achieves $19\%$ accuracy at maximum against state-of-the-art methods.</li>
</ul>

<h3>Title: Guaranteed Conditional Diffusion: 3D Block-based Models for Scientific Data Compression</h3>
<ul>
<li><strong>Authors: </strong>Jaemoon Lee, Xiao Li, Liangji Zhu, Sanjay Ranka, Anand Rangarajan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12951">https://arxiv.org/abs/2502.12951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12951">https://arxiv.org/pdf/2502.12951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12951]] Guaranteed Conditional Diffusion: 3D Block-based Models for Scientific Data Compression(https://arxiv.org/abs/2502.12951)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper proposes a new compression paradigm -- Guaranteed Conditional Diffusion with Tensor Correction (GCDTC) -- for lossy scientific data compression. The framework is based on recent conditional diffusion (CD) generative models, and it consists of a conditional diffusion model, tensor correction, and error guarantee. Our diffusion model is a mixture of 3D conditioning and 2D denoising U-Net. The approach leverages a 3D block-based compressing module to address spatiotemporal correlations in structured scientific data. Then, the reverse diffusion process for 2D spatial data is conditioned on the ``slices'' of content latent variables produced by the compressing module. After training, the denoising decoder reconstructs the data with zero noise and content latent variables, and thus it is entirely deterministic. The reconstructed outputs of the CD model are further post-processed by our tensor correction and error guarantee steps to control and ensure a maximum error distortion, which is an inevitable requirement in lossy scientific data compression. Our experiments involving two datasets generated by climate and chemical combustion simulations show that our framework outperforms standard convolutional autoencoders and yields competitive compression quality with an existing scientific data compression algorithm.</li>
</ul>

<h3>Title: Preventing the Popular Item Embedding Based Attack in Federated Recommendations</h3>
<ul>
<li><strong>Authors: </strong>Jun Zhang, Huan Li, Dazhong Rong, Yan Zhao, Ke Chen, Lidan Shou</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DB, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12958">https://arxiv.org/abs/2502.12958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12958">https://arxiv.org/pdf/2502.12958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12958]] Preventing the Popular Item Embedding Based Attack in Federated Recommendations(https://arxiv.org/abs/2502.12958)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Privacy concerns have led to the rise of federated recommender systems (FRS), which can create personalized models across distributed clients. However, FRS is vulnerable to poisoning attacks, where malicious users manipulate gradients to promote their target items intentionally. Existing attacks against FRS have limitations, as they depend on specific models and prior knowledge, restricting their real-world applicability. In our exploration of practical FRS vulnerabilities, we devise a model-agnostic and prior-knowledge-free attack, named PIECK (Popular Item Embedding based Attack). The core module of PIECK is popular item mining, which leverages embedding changes during FRS training to effectively identify the popular items. Built upon the core module, PIECK branches into two diverse solutions: The PIECKIPE solution employs an item popularity enhancement module, which aligns the embeddings of targeted items with the mined popular items to increase item exposure. The PIECKUEA further enhances the robustness of the attack by using a user embedding approximation module, which approximates private user embeddings using mined popular items. Upon identifying PIECK, we evaluate existing federated defense methods and find them ineffective against PIECK, as poisonous gradients inevitably overwhelm the cold target items. We then propose a novel defense method by introducing two regularization terms during user training, which constrain item popularity enhancement and user embedding approximation while preserving FRS performance. We evaluate PIECK and its defense across two base models, three real datasets, four top-tier attacks, and six general defense methods, affirming the efficacy of both PIECK and its defense.</li>
</ul>

<h3>Title: Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing</h3>
<ul>
<li><strong>Authors: </strong>Xiaoju Ye, Zhichun Wang, Jingyuan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12962">https://arxiv.org/abs/2502.12962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12962">https://arxiv.org/pdf/2502.12962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12962]] Infinite Retrieval: Attention Enhanced LLMs in Long-Context Processing(https://arxiv.org/abs/2502.12962)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Limited by the context window size of Large Language Models(LLMs), handling various tasks with input tokens exceeding the upper limit has been challenging, whether it is a simple direct retrieval task or a complex multi-hop reasoning task. Although various methods have been proposed to enhance the long-context processing capabilities of LLMs, they either incur substantial post-training costs, or require additional tool modules(e.g.,RAG), or have not shown significant improvement in realistic tasks. Our work observes the correlation between the attention distribution and generated answers across each layer, and establishes the attention allocation aligns with retrieval-augmented capabilities through experiments. Drawing on the above insights, we propose a novel method InfiniRetri that leverages the LLMs's own attention information to enable accurate retrieval across inputs of infinitely length. Our evaluations indicate that InfiniRetri achieves 100% accuracy in the Needle-In-a-Haystack(NIH) test over 1M tokens using a 0.5B parameter model, surpassing other method or larger models and setting a new state-of-the-art(SOTA). Moreover, our method achieves significant performance improvements on real-world benchmarks, with a maximum 288% improvement. In addition, InfiniRetri can be applied to any Transformer-based LLMs without additional training and substantially reduces inference latency and compute overhead in long texts. In summary, our comprehensive studies show InfiniRetri's potential for practical applications and creates a paradigm for retrievaling information using LLMs own capabilities under infinite-length tokens. Code will be released in link.</li>
</ul>

<h3>Title: Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Adi Simhi, Itay Itzhak, Fazl Barez, Gabriel Stanovsky, Yonatan Belinkov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12964">https://arxiv.org/abs/2502.12964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12964">https://arxiv.org/pdf/2502.12964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12964]] Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs(https://arxiv.org/abs/2502.12964)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often generate outputs that lack grounding in real-world facts, a phenomenon known as hallucinations. Prior research has associated hallucinations with model uncertainty, leveraging this relationship for hallucination detection and mitigation. In this paper, we challenge the underlying assumption that all hallucinations are associated with uncertainty. Using knowledge detection and uncertainty measurement methods, we demonstrate that models can hallucinate with high certainty even when they have the correct knowledge. We further show that high-certainty hallucinations are consistent across models and datasets, distinctive enough to be singled out, and challenge existing mitigation methods. Our findings reveal an overlooked aspect of hallucinations, emphasizing the need to understand their origins and improve mitigation strategies to enhance LLM safety. The code is available at this https URL .</li>
</ul>

<h3>Title: Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking</h3>
<ul>
<li><strong>Authors: </strong>Junda Zhu, Lingyong Yan, Shuaiqiang Wang, Dawei Yin, Lei Sha</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12970">https://arxiv.org/abs/2502.12970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12970">https://arxiv.org/pdf/2502.12970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12970]] Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking(https://arxiv.org/abs/2502.12970)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The reasoning abilities of Large Language Models (LLMs) have demonstrated remarkable advancement and exceptional performance across diverse domains. However, leveraging these reasoning capabilities to enhance LLM safety against adversarial attacks and jailbreak queries remains largely unexplored. To bridge this gap, we propose Reasoning-to-Defend (R2D), a novel training paradigm that integrates safety reflections of queries and responses into LLMs' generation process, unlocking a safety-aware reasoning mechanism. This approach enables self-evaluation at each reasoning step to create safety pivot tokens as indicators of the response's safety status. Furthermore, in order to improve the learning efficiency of pivot token prediction, we propose Contrastive Pivot Optimization(CPO), which enhances the model's ability to perceive the safety status of dialogues. Through this mechanism, LLMs dynamically adjust their response strategies during reasoning, significantly enhancing their defense capabilities against jailbreak attacks. Extensive experimental results demonstrate that R2D effectively mitigates various attacks and improves overall safety, highlighting the substantial potential of safety-aware reasoning in strengthening LLMs' robustness against jailbreaks.</li>
</ul>

<h3>Title: Instance-Level Moving Object Segmentation from a Single Image with Events</h3>
<ul>
<li><strong>Authors: </strong>Zhexiong Wan, Bin Fan, Le Hui, Yuchao Dai, Gim Hee Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12975">https://arxiv.org/abs/2502.12975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12975">https://arxiv.org/pdf/2502.12975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12975]] Instance-Level Moving Object Segmentation from a Single Image with Events(https://arxiv.org/abs/2502.12975)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Moving object segmentation plays a crucial role in understanding dynamic scenes involving multiple moving objects, while the difficulties lie in taking into account both spatial texture structures and temporal motion cues. Existing methods based on video frames encounter difficulties in distinguishing whether pixel displacements of an object are caused by camera motion or object motion due to the complexities of accurate image-based motion modeling. Recent advances exploit the motion sensitivity of novel event cameras to counter conventional images' inadequate motion modeling capabilities, but instead lead to challenges in segmenting pixel-level object masks due to the lack of dense texture structures in events. To address these two limitations imposed by unimodal settings, we propose the first instance-level moving object segmentation framework that integrates complementary texture and motion cues. Our model incorporates implicit cross-modal masked attention augmentation, explicit contrastive feature learning, and flow-guided motion enhancement to exploit dense texture information from a single image and rich motion information from events, respectively. By leveraging the augmented texture and motion features, we separate mask segmentation from motion classification to handle varying numbers of independently moving objects. Through extensive evaluations on multiple datasets, as well as ablation experiments with different input settings and real-time efficiency analysis of the proposed framework, we believe that our first attempt to incorporate image and event data for practical deployment can provide new insights for future work in event-based motion related works. The source code with model training and pre-trained weights is released at this https URL</li>
</ul>

<h3>Title: Does Training with Synthetic Data Truly Protect Privacy?</h3>
<ul>
<li><strong>Authors: </strong>Yunpeng Zhao, Jie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12976">https://arxiv.org/abs/2502.12976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12976">https://arxiv.org/pdf/2502.12976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12976]] Does Training with Synthetic Data Truly Protect Privacy?(https://arxiv.org/abs/2502.12976)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, diffusion, data-free</a></li>
<li><strong>Abstract: </strong>As synthetic data becomes increasingly popular in machine learning tasks, numerous methods--without formal differential privacy guarantees--use synthetic data for training. These methods often claim, either explicitly or implicitly, to protect the privacy of the original training data. In this work, we explore four different training paradigms: coreset selection, dataset distillation, data-free knowledge distillation, and synthetic data generated from diffusion models. While all these methods utilize synthetic data for training, they lead to vastly different conclusions regarding privacy preservation. We caution that empirical approaches to preserving data privacy require careful and rigorous evaluation; otherwise, they risk providing a false sense of privacy.</li>
</ul>

<h3>Title: Electron flow matching for generative reaction mechanism prediction obeying conservation laws</h3>
<ul>
<li><strong>Authors: </strong>Joonyoung F. Joung, Mun Hong Fong, Nicholas Casetti, Jordan P. Liles, Ne S. Dassanayake, Connor W. Coley</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12979">https://arxiv.org/abs/2502.12979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12979">https://arxiv.org/pdf/2502.12979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12979]] Electron flow matching for generative reaction mechanism prediction obeying conservation laws(https://arxiv.org/abs/2502.12979)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Central to our understanding of chemical reactivity is the principle of mass conservation, which is fundamental for ensuring physical consistency, balancing equations, and guiding reaction design. However, data-driven computational models for tasks such as reaction product prediction rarely abide by this most basic constraint. In this work, we recast the problem of reaction prediction as a problem of electron redistribution using the modern deep generative framework of flow matching. Our model, FlowER, overcomes limitations inherent in previous approaches by enforcing exact mass conservation, thereby resolving hallucinatory failure modes, recovering mechanistic reaction sequences for unseen substrate scaffolds, and generalizing effectively to out-of-domain reaction classes with extremely data-efficient fine-tuning. FlowER additionally enables estimation of thermodynamic or kinetic feasibility and manifests a degree of chemical intuition in reaction prediction tasks. This inherently interpretable framework represents a significant step in bridging the gap between predictive accuracy and mechanistic understanding in data-driven reaction outcome prediction.</li>
</ul>

<h3>Title: Towards Variational Flow Matching on General Geometries</h3>
<ul>
<li><strong>Authors: </strong>Olga Zaghen, Floor Eijkelboom, Alison Pouplin, Erik J. Bekkers</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12981">https://arxiv.org/abs/2502.12981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12981">https://arxiv.org/pdf/2502.12981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12981]] Towards Variational Flow Matching on General Geometries(https://arxiv.org/abs/2502.12981)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>We introduce Riemannian Gaussian Variational Flow Matching (RG-VFM), an extension of Variational Flow Matching (VFM) that leverages Riemannian Gaussian distributions for generative modeling on structured manifolds. We derive a variational objective for probability flows on manifolds with closed-form geodesics, making RG-VFM comparable - though fundamentally different to Riemannian Flow Matching (RFM) in this geometric setting. Experiments on a checkerboard dataset wrapped on the sphere demonstrate that RG-VFM captures geometric structure more effectively than Euclidean VFM and baseline methods, establishing it as a robust framework for manifold-aware generative modeling.</li>
</ul>

<h3>Title: Ensemble Kalman filter in latent space using a variational autoencoder pair</h3>
<ul>
<li><strong>Authors: </strong>Ivo Pasmans, Yumeng Chen, Tobias Sebastian Finn, Marc Bocquet, Alberto Carrassi</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12987">https://arxiv.org/abs/2502.12987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12987">https://arxiv.org/pdf/2502.12987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12987]] Ensemble Kalman filter in latent space using a variational autoencoder pair(https://arxiv.org/abs/2502.12987)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Popular (ensemble) Kalman filter data assimilation (DA) approaches assume that the errors in both the a priori estimate of the state and those in the observations are Gaussian. For constrained variables, e.g. sea ice concentration or stress, such an assumption does not hold. The variational autoencoder (VAE) is a machine learning (ML) technique that allows to map an arbitrary distribution to/from a latent space in which the distribution is supposedly closer to a Gaussian. We propose a novel hybrid DA-ML approach in which VAEs are incorporated in the DA procedure. Specifically, we introduce a variant of the popular ensemble transform Kalman filter (ETKF) in which the analysis is applied in the latent space of a single VAE or a pair of VAEs. In twin experiments with a simple circular model, whereby the circle represents an underlying submanifold to be respected, we find that the use of a VAE ensures that a posteri ensemble members lie close to the manifold containing the truth. Furthermore, online updating of the VAE is necessary and achievable when this manifold varies in time, i.e. when it is non-stationary. We demonstrate that introducing an additional second latent space for the observational innovations improves robustness against detrimental effects of non-Gaussianity and bias in the observational errors but it slightly lessens the performance if observational errors are strictly Gaussian.</li>
</ul>

<h3>Title: Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zixiao Wang, Duzhen Zhang, Ishita Agrawal, Shen Gao, Le Song, Xiuying Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12988">https://arxiv.org/abs/2502.12988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12988">https://arxiv.org/pdf/2502.12988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12988]] Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in LLMs(https://arxiv.org/abs/2502.12988)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Previous approaches to persona simulation large language models (LLMs) have typically relied on learning basic biographical information, or using limited role-play dialogue datasets to capture a character's responses. However, a holistic representation of an individual goes beyond surface-level facts or conversations to deeper thoughts and thinking. In this work, we introduce CharacterBot, a model designed to replicate both the linguistic patterns and distinctive thought processes of a character. Using Lu Xun, a renowned Chinese writer, as a case study, we propose four training tasks derived from his 17 essay collections. These include a pre-training task focused on mastering external linguistic structures and knowledge, as well as three fine-tuning tasks: multiple-choice question answering, generative question answering, and style transfer, each aligning the LLM with Lu Xun's internal ideation and writing style. To optimize learning across these tasks, we introduce a CharLoRA parameter updating mechanism, where a general linguistic style expert collaborates with other task-specific experts to better study both the language style and the understanding of deeper thoughts. We evaluate CharacterBot on three tasks for linguistic accuracy and opinion comprehension, demonstrating that it significantly outperforms the baselines on our adapted metrics. We hope that this work inspires future research on deep character persona simulation LLM.</li>
</ul>

<h3>Title: B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability</h3>
<ul>
<li><strong>Authors: </strong>Yifan Wang, Sukrut Rao, Ji-Ung Lee, Mayank Jobanputra, Vera Demberg</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12992">https://arxiv.org/abs/2502.12992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12992">https://arxiv.org/pdf/2502.12992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12992]] B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability(https://arxiv.org/abs/2502.12992)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Post-hoc explanation methods for black-box models often struggle with faithfulness and human interpretability due to the lack of explainability in current neural models. Meanwhile, B-cos networks have been introduced to improve model explainability through architectural and computational adaptations, but their application has so far been limited to computer vision models and their associated training pipelines. In this work, we introduce B-cos LMs, i.e., B-cos networks empowered for NLP tasks. Our approach directly transforms pre-trained language models into B-cos LMs by combining B-cos conversion and task fine-tuning, improving efficiency compared to previous B-cos methods. Our automatic and human evaluation results demonstrate that B-cos LMs produce more faithful and human interpretable explanations than post hoc methods, while maintaining task performance comparable to conventional fine-tuning. Our in-depth analysis explores how B-cos LMs differ from conventionally fine-tuned models in their learning processes and explanation patterns. Finally, we provide practical guidelines for effectively building B-cos LMs based on our findings. Our code is available at this https URL.</li>
</ul>

<h3>Title: SHADeS: Self-supervised Monocular Depth Estimation Through Non-Lambertian Image Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Rema Daher, Francisco Vasconcelos, Danail Stoyanov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.12994">https://arxiv.org/abs/2502.12994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.12994">https://arxiv.org/pdf/2502.12994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.12994]] SHADeS: Self-supervised Monocular Depth Estimation Through Non-Lambertian Image Decomposition(https://arxiv.org/abs/2502.12994)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Purpose: Visual 3D scene reconstruction can support colonoscopy navigation. It can help in recognising which portions of the colon have been visualised and characterising the size and shape of polyps. This is still a very challenging problem due to complex illumination variations, including abundant specular reflections. We investigate how to effectively decouple light and depth in this problem. Methods: We introduce a self-supervised model that simultaneously characterises the shape and lighting of the visualised colonoscopy scene. Our model estimates shading, albedo, depth, and specularities (SHADeS) from single images. Unlike previous approaches (IID), we use a non-Lambertian model that treats specular reflections as a separate light component. The implementation of our method is available at this https URL. Results: We demonstrate on real colonoscopy images (Hyper Kvasir) that previous models for light decomposition (IID) and depth estimation (MonoVIT, ModoDepth2) are negatively affected by specularities. In contrast, SHADeS can simultaneously produce light decomposition and depth maps that are robust to specular regions. We also perform a quantitative comparison on phantom data (C3VD) where we further demonstrate the robustness of our model. Conclusion: Modelling specular reflections improves depth estimation in colonoscopy. We propose an effective self-supervised approach that uses this insight to jointly estimate light decomposition and depth. Light decomposition has the potential to help with other problems, such as place recognition within the colon.</li>
</ul>

<h3>Title: Language Barriers: Evaluating Cross-Lingual Performance of CNN and Transformer Architectures for Speech Quality Estimation</h3>
<ul>
<li><strong>Authors: </strong>Wafaa Wardah, Tuğçe Melike Koçak Büyüktaş, Kirill Shchegelskiy, Sebastian Möller, Robert P. Spang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13004">https://arxiv.org/abs/2502.13004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13004">https://arxiv.org/pdf/2502.13004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13004]] Language Barriers: Evaluating Cross-Lingual Performance of CNN and Transformer Architectures for Speech Quality Estimation(https://arxiv.org/abs/2502.13004)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Objective speech quality models aim to predict human-perceived speech quality using automated methods. However, cross-lingual generalization remains a major challenge, as Mean Opinion Scores (MOS) vary across languages due to linguistic, perceptual, and dataset-specific differences. A model trained primarily on English data may struggle to generalize to languages with different phonetic, tonal, and prosodic characteristics, leading to inconsistencies in objective assessments. This study investigates the cross-lingual performance of two speech quality models: NISQA, a CNN-based model, and a Transformer-based Audio Spectrogram Transformer (AST) model. Both models were trained exclusively on English datasets containing over 49,000 speech samples and subsequently evaluated on speech in German, French, Mandarin, Swedish, and Dutch. We analyze model performance using Pearson Correlation Coefficient (PCC) and Root Mean Square Error (RMSE) across five speech quality dimensions: coloration, discontinuity, loudness, noise, and MOS. Our findings show that while AST achieves a more stable cross-lingual performance, both models exhibit noticeable biases. Notably, Mandarin speech quality predictions correlate highly with human MOS scores, whereas Swedish and Dutch present greater prediction challenges. Discontinuities remain difficult to model across all languages. These results highlight the need for more balanced multilingual datasets and architecture-specific adaptations to improve cross-lingual generalization.</li>
</ul>

<h3>Title: Adaptive Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Reza Rezaei, Reza Saadati Fard, Jayson Parker, Rahul G. Krishnan, Milad Lankarany</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13010">https://arxiv.org/abs/2502.13010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13010">https://arxiv.org/pdf/2502.13010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13010]] Adaptive Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge(https://arxiv.org/abs/2502.13010)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have significantly advanced medical question-answering by leveraging extensive clinical data and medical literature. However, the rapid evolution of medical knowledge and the labor-intensive process of manually updating domain-specific resources pose challenges to the reliability of these systems. To address this, we introduce Adaptive Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates the construction and continuous updating of medical knowledge graphs, integrates reasoning, and retrieves current external evidence, such as PubMed and WikiSearch. By dynamically linking new findings and complex medical concepts, AMG-RAG not only improves accuracy but also enhances interpretability in medical queries. Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness of AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of 66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to 100 times larger. Notably, these improvements are achieved without increasing computational overhead, highlighting the critical role of automated knowledge graph generation and external evidence retrieval in delivering up-to-date, trustworthy medical insights.</li>
</ul>

<h3>Title: Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Sha Li, Naren Ramarkrishnan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13019">https://arxiv.org/abs/2502.13019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13019">https://arxiv.org/pdf/2502.13019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13019]] Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation(https://arxiv.org/abs/2502.13019)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the remarkable capabilities of Large Language Models (LLMs) in various NLP tasks, they remain vulnerable to hallucinations due to their limited parametric knowledge and lack of domain-specific expertise. Retrieval-Augmented Generation (RAG) addresses this challenge by incorporating external document retrieval to augment the knowledge base of LLMs. In this approach, RAG retrieves document chunks from an external corpus in response to a query, which are then used as context for the downstream language model to generate an answer. However, these retrieved knowledge sources often include irrelevant or erroneous information, undermining the effectiveness of RAG in downstream tasks. To overcome this limitation, we introduce a compact, efficient, and pluggable module designed to refine external knowledge sources before feeding them to the generator. The module reconstructs retrieved content by extracting the most relevant and supportive information and reorganising it into a concise, query-specific format. Through a three-stage training paradigm - comprising supervised fine-tuning, contrastive multi-task learning, and reinforcement learning-based alignment - it prioritises critical knowledge and aligns it with the generator's preferences. This method enables LLMs to produce outputs that are more accurate, reliable, and contextually appropriate.</li>
</ul>

<h3>Title: Efficient and Sharp Off-Policy Learning under Unobserved Confounding</h3>
<ul>
<li><strong>Authors: </strong>Konstantin Hess, Dennis Frauen, Valentyn Melnychuk, Stefan Feuerriegel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13022">https://arxiv.org/abs/2502.13022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13022">https://arxiv.org/pdf/2502.13022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13022]] Efficient and Sharp Off-Policy Learning under Unobserved Confounding(https://arxiv.org/abs/2502.13022)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We develop a novel method for personalized off-policy learning in scenarios with unobserved confounding. Thereby, we address a key limitation of standard policy learning: standard policy learning assumes unconfoundedness, meaning that no unobserved factors influence both treatment assignment and outcomes. However, this assumption is often violated, because of which standard policy learning produces biased estimates and thus leads to policies that can be harmful. To address this limitation, we employ causal sensitivity analysis and derive a statistically efficient estimator for a sharp bound on the value function under unobserved confounding. Our estimator has three advantages: (1) Unlike existing works, our estimator avoids unstable minimax optimization based on inverse propensity weighted outcomes. (2) Our estimator is statistically efficient. (3) We prove that our estimator leads to the optimal confounding-robust policy. Finally, we extend our theory to the related task of policy improvement under unobserved confounding, i.e., when a baseline policy such as the standard of care is available. We show in experiments with synthetic and real-world data that our method outperforms simple plug-in approaches and existing baselines. Our method is highly relevant for decision-making where unobserved confounding can be problematic, such as in healthcare and public policy.</li>
</ul>

<h3>Title: Detection and Geographic Localization of Natural Objects in the Wild: A Case Study on Palms</h3>
<ul>
<li><strong>Authors: </strong>Kangning Cui, Rongkun Zhu, Manqi Wang, Wei Tang, Gregory D. Larsen, Victor P. Pauca, Sarra Alqahtani, Fan Yang, David Segurado, David Lutz, Jean-Michel Morel, Miles R. Silman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13023">https://arxiv.org/abs/2502.13023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13023">https://arxiv.org/pdf/2502.13023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13023]] Detection and Geographic Localization of Natural Objects in the Wild: A Case Study on Palms(https://arxiv.org/abs/2502.13023)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, segmentation</a></li>
<li><strong>Abstract: </strong>Palms are ecologically and economically indicators of tropical forest health, biodiversity, and human impact that support local economies and global forest product supply chains. While palm detection in plantations is well-studied, efforts to map naturally occurring palms in dense forests remain limited by overlapping crowns, uneven shading, and heterogeneous landscapes. We develop PRISM (Processing, Inference, Segmentation, and Mapping), a flexible pipeline for detecting and localizing palms in dense tropical forests using large orthomosaic images. Orthomosaics are created from thousands of aerial images and spanning several to hundreds of gigabytes. Our contributions are threefold. First, we construct a large UAV-derived orthomosaic dataset collected across 21 ecologically diverse sites in western Ecuador, annotated with 8,830 bounding boxes and 5,026 palm center points. Second, we evaluate multiple state-of-the-art object detectors based on efficiency and performance, integrating zero-shot SAM 2 as the segmentation backbone, and refining the results for precise geographic mapping. Third, we apply calibration methods to align confidence scores with IoU and explore saliency maps for feature explainability. Though optimized for palms, PRISM is adaptable for identifying other natural objects, such as eastern white pines. Future work will explore transfer learning for lower-resolution datasets (0.5 to 1m).</li>
</ul>

<h3>Title: Fragility-aware Classification for Understanding Risk and Improving Generalization</h3>
<ul>
<li><strong>Authors: </strong>Chen Yang, Zheng Cui, Daniel Zhuoyu Long, Jin Qi, Ruohan Zhan</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13024">https://arxiv.org/abs/2502.13024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13024">https://arxiv.org/pdf/2502.13024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13024]] Fragility-aware Classification for Understanding Risk and Improving Generalization(https://arxiv.org/abs/2502.13024)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Classification models play a critical role in data-driven decision-making applications such as medical diagnosis, user profiling, recommendation systems, and default detection. Traditional performance metrics, such as accuracy, focus on overall error rates but fail to account for the confidence of incorrect predictions, thereby overlooking the risk of confident misjudgments. This risk is particularly significant in cost-sensitive and safety-critical domains like medical diagnosis and autonomous driving, where overconfident false predictions may cause severe consequences. To address this issue, we introduce the Fragility Index (FI), a novel metric that evaluates classification performance from a risk-averse perspective by explicitly capturing the tail risk of confident misjudgments. To enhance generalizability, we define FI within the robust satisficing (RS) framework, incorporating data uncertainty. We further develop a model training approach that optimizes FI while maintaining tractability for common loss functions. Specifically, we derive exact reformulations for cross-entropy loss, hinge-type loss, and Lipschitz loss, and extend the approach to deep learning models. Through synthetic experiments and real-world medical diagnosis tasks, we demonstrate that FI effectively identifies misjudgment risk and FI-based training improves model robustness and generalizability. Finally, we extend our framework to deep neural network training, further validating its effectiveness in enhancing deep learning models.</li>
</ul>

<h3>Title: A deep learning framework for efficient pathology image analysis</h3>
<ul>
<li><strong>Authors: </strong>Peter Neidlinger, Tim Lenz, Sebastian Foersch, Chiara M. L. Loeffler, Jan Clusmann, Marco Gustav, Lawrence A. Shaktah, Rupert Langer, Bastian Dislich, Lisa A. Boardman, Amy J. French, Ellen L. Goode, Andrea Gsur, Stefanie Brezina, Marc J. Gunter, Robert Steinfelder, Hans-Michael Behrens, Christoph Röcken, Tabitha Harrison, Ulrike Peters, Amanda I. Phipps, Giuseppe Curigliano, Nicola Fusco, Antonio Marra, Michael Hoffmeister, Hermann Brenner, Jakob Nikolas Kather</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13027">https://arxiv.org/abs/2502.13027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13027">https://arxiv.org/pdf/2502.13027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13027]] A deep learning framework for efficient pathology image analysis(https://arxiv.org/abs/2502.13027)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) has transformed digital pathology by enabling biomarker prediction from high-resolution whole slide images (WSIs). However, current methods are computationally inefficient, processing thousands of redundant tiles per WSI and requiring complex aggregator models. We introduce EAGLE (Efficient Approach for Guided Local Examination), a deep learning framework that emulates pathologists by selectively analyzing informative regions. EAGLE incorporates two foundation models: CHIEF for efficient tile selection and Virchow2 for extracting high-quality features. Benchmarking was conducted against leading slide- and tile-level foundation models across 31 tasks from four cancer types, spanning morphology, biomarker prediction and prognosis. EAGLE outperformed state-of-the-art foundation models by up to 23% and achieved the highest AUROC overall. It processed a slide in 2.27 seconds, reducing computational time by more than 99% compared to existing models. This efficiency enables real-time workflows, allows pathologists to validate all tiles which are used by the model during analysis, and eliminates dependence on high-performance computing, making AI-powered pathology more accessible. By reliably identifying meaningful regions and minimizing artifacts, EAGLE provides robust and interpretable outputs, supporting rapid slide searches, integration into multi-omics pipelines and emerging clinical foundation models.</li>
</ul>

<h3>Title: HPSS: Heuristic Prompting Strategy Search for LLM Evaluators</h3>
<ul>
<li><strong>Authors: </strong>Bosi Wen, Pei Ke, Yufei Sun, Cunxiang Wang, Xiaotao Gu, Jinfeng Zhou, Jie Tang, Hongning Wang, Minlie Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13031">https://arxiv.org/abs/2502.13031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13031">https://arxiv.org/pdf/2502.13031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13031]] HPSS: Heuristic Prompting Strategy Search for LLM Evaluators(https://arxiv.org/abs/2502.13031)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Since the adoption of large language models (LLMs) for text evaluation has become increasingly prevalent in the field of natural language processing (NLP), a series of existing works attempt to optimize the prompts for LLM evaluators to improve their alignment with human judgment. However, their efforts are limited to optimizing individual factors of evaluation prompts, such as evaluation criteria or output formats, neglecting the combinatorial impact of multiple factors, which leads to insufficient optimization of the evaluation pipeline. Nevertheless, identifying well-behaved prompting strategies for adjusting multiple factors requires extensive enumeration. To this end, we comprehensively integrate 8 key factors for evaluation prompts and propose a novel automatic prompting strategy optimization method called Heuristic Prompting Strategy Search (HPSS). Inspired by the genetic algorithm, HPSS conducts an iterative search to find well-behaved prompting strategies for LLM evaluators. A heuristic function is employed to guide the search process, enhancing the performance of our algorithm. Extensive experiments across four evaluation tasks demonstrate the effectiveness of HPSS, consistently outperforming both human-designed evaluation prompts and existing automatic prompt optimization methods.</li>
</ul>

<h3>Title: Enhancing Power Grid Inspections with Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Diogo Lavado, Ricardo Santos, Andre Coelho, Joao Santos, Alessandra Micheletti, Claudia Soares</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13037">https://arxiv.org/abs/2502.13037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13037">https://arxiv.org/pdf/2502.13037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13037]] Enhancing Power Grid Inspections with Machine Learning(https://arxiv.org/abs/2502.13037)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Ensuring the safety and reliability of power grids is critical as global energy demands continue to rise. Traditional inspection methods, such as manual observations or helicopter surveys, are resource-intensive and lack scalability. This paper explores the use of 3D computer vision to automate power grid inspections, utilizing the TS40K dataset -- a high-density, annotated collection of 3D LiDAR point clouds. By concentrating on 3D semantic segmentation, our approach addresses challenges like class imbalance and noisy data to enhance the detection of critical grid components such as power lines and towers. The benchmark results indicate significant performance improvements, with IoU scores reaching 95.53% for the detection of power lines using transformer-based models. Our findings illustrate the potential for integrating ML into grid maintenance workflows, increasing efficiency and enabling proactive risk management strategies.</li>
</ul>

<h3>Title: Do we still need Human Annotators? Prompting Large Language Models for Aspect Sentiment Quad Prediction</h3>
<ul>
<li><strong>Authors: </strong>Nils Constantin Hellwig, Jakob Fehle, Udo Kruschwitz, Christian Wolff</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13044">https://arxiv.org/abs/2502.13044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13044">https://arxiv.org/pdf/2502.13044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13044]] Do we still need Human Annotators? Prompting Large Language Models for Aspect Sentiment Quad Prediction(https://arxiv.org/abs/2502.13044)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Aspect sentiment quadruple prediction (ASQP) facilitates a detailed understanding of opinions expressed in a text by identifying the opinion term, aspect term, aspect category and sentiment polarity for each opinion. However, annotating a full set of training examples to fine-tune models for ASQP is a resource-intensive process. In this study, we explore the capabilities of large language models (LLMs) for zero- and few-shot learning on the ASQP task across five diverse datasets. We report F1 scores slightly below those obtained with state-of-the-art fine-tuned models but exceeding previously reported zero- and few-shot performance. In the 40-shot setting on the Rest16 restaurant domain dataset, LLMs achieved an F1 score of 52.46, compared to 60.39 by the best-performing fine-tuned method MVP. Additionally, we report the performance of LLMs in target aspect sentiment detection (TASD), where the F1 scores were also close to fine-tuned models, achieving 66.03 on Rest16 in the 40-shot setting, compared to 72.76 with MVP. While human annotators remain essential for achieving optimal performance, LLMs can reduce the need for extensive manual annotation in ASQP tasks.</li>
</ul>

<h3>Title: $k$-Graph: A Graph Embedding for Interpretable Time Series Clustering</h3>
<ul>
<li><strong>Authors: </strong>Paul Boniol, Donato Tiano, Angela Bonifati, Themis Palpanas</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13049">https://arxiv.org/abs/2502.13049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13049">https://arxiv.org/pdf/2502.13049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13049]] $k$-Graph: A Graph Embedding for Interpretable Time Series Clustering(https://arxiv.org/abs/2502.13049)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Time series clustering poses a significant challenge with diverse applications across domains. A prominent drawback of existing solutions lies in their limited interpretability, often confined to presenting users with centroids. In addressing this gap, our work presents $k$-Graph, an unsupervised method explicitly crafted to augment interpretability in time series clustering. Leveraging a graph representation of time series subsequences, $k$-Graph constructs multiple graph representations based on different subsequence lengths. This feature accommodates variable-length time series without requiring users to predetermine subsequence lengths. Our experimental results reveal that $k$-Graph outperforms current state-of-the-art time series clustering algorithms in accuracy, while providing users with meaningful explanations and interpretations of the clustering outcomes.</li>
</ul>

<h3>Title: AEIA-MN: Evaluating the Robustness of Multimodal LLM-Powered Mobile Agents Against Active Environmental Injection Attacks</h3>
<ul>
<li><strong>Authors: </strong>Yurun Chen, Xueyu Hu, Keting Yin, Juncheng Li, Shengyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13053">https://arxiv.org/abs/2502.13053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13053">https://arxiv.org/pdf/2502.13053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13053]] AEIA-MN: Evaluating the Robustness of Multimodal LLM-Powered Mobile Agents Against Active Environmental Injection Attacks(https://arxiv.org/abs/2502.13053)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>As researchers continuously optimize AI agents to perform tasks more effectively within operating systems, they often neglect to address the critical need for enabling these agents to identify "impostors" within the system. Through an analysis of the agents' operating environment, we identified a potential threat: attackers can disguise their attack methods as environmental elements, injecting active disturbances into the agents' execution process, thereby disrupting their decision-making. We define this type of attack as Active Environment Injection Attack (AEIA). Based on this, we propose AEIA-MN, an active environment injection attack scheme that exploits interaction vulnerabilities in the mobile operating system to evaluate the robustness of MLLM-based agents against such threats. Experimental results show that even advanced MLLMs are highly vulnerable to this attack, achieving a maximum attack success rate of 93% in the AndroidWorld benchmark.</li>
</ul>

<h3>Title: LAMD: Context-driven Android Malware Detection and Classification with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xingzhi Qian, Xinran Zheng, Yiling He, Shuo Yang, Lorenzo Cavallaro</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13055">https://arxiv.org/abs/2502.13055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13055">https://arxiv.org/pdf/2502.13055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13055]] LAMD: Context-driven Android Malware Detection and Classification with LLMs(https://arxiv.org/abs/2502.13055)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, extraction, explainability, large language model</a></li>
<li><strong>Abstract: </strong>The rapid growth of mobile applications has escalated Android malware threats. Although there are numerous detection methods, they often struggle with evolving attacks, dataset biases, and limited explainability. Large Language Models (LLMs) offer a promising alternative with their zero-shot inference and reasoning capabilities. However, applying LLMs to Android malware detection presents two key challenges: (1)the extensive support code in Android applications, often spanning thousands of classes, exceeds LLMs' context limits and obscures malicious behavior within benign functionality; (2)the structural complexity and interdependencies of Android applications surpass LLMs' sequence-based reasoning, fragmenting code analysis and hindering malicious intent inference. To address these challenges, we propose LAMD, a practical context-driven framework to enable LLM-based Android malware detection. LAMD integrates key context extraction to isolate security-critical code regions and construct program structures, then applies tier-wise code reasoning to analyze application behavior progressively, from low-level instructions to high-level semantics, providing final prediction and explanation. A well-designed factual consistency verification mechanism is equipped to mitigate LLM hallucinations from the first tier. Evaluation in real-world settings demonstrates LAMD's effectiveness over conventional detectors, establishing a feasible basis for LLM-driven malware analysis in dynamic threat landscapes.</li>
</ul>

<h3>Title: SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xianfu Cheng, Wei Zhang, Shiwei Zhang, Jian Yang, Xiangyuan Guan, Xianjie Wu, Xiang Li, Ge Zhang, Jiaheng Liu, Yuying Mai, Yutao Zeng, Zhoufutu Wen, Ke Jin, Baorui Wang, Weixiao Zhou, Yunhong Lu, Tongliang Li, Wenhao Huang, Zhoujun Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13059">https://arxiv.org/abs/2502.13059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13059">https://arxiv.org/pdf/2502.13059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13059]] SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large Language Models(https://arxiv.org/abs/2502.13059)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The increasing application of multi-modal large language models (MLLMs) across various sectors have spotlighted the essence of their output reliability and accuracy, particularly their ability to produce content grounded in factual information (e.g. common and domain-specific knowledge). In this work, we introduce SimpleVQA, the first comprehensive multi-modal benchmark to evaluate the factuality ability of MLLMs to answer natural language short questions. SimpleVQA is characterized by six key features: it covers multiple tasks and multiple scenarios, ensures high quality and challenging queries, maintains static and timeless reference answers, and is straightforward to evaluate. Our approach involves categorizing visual question-answering items into 9 different tasks around objective events or common knowledge and situating these within 9 topics. Rigorous quality control processes are implemented to guarantee high-quality, concise, and clear answers, facilitating evaluation with minimal variance via an LLM-as-a-judge scoring system. Using SimpleVQA, we perform a comprehensive assessment of leading 18 MLLMs and 8 text-only LLMs, delving into their image comprehension and text generation abilities by identifying and analyzing error cases.</li>
</ul>

<h3>Title: Sublinear-Overhead Secure Linear Algebra on a Dishonest Server</h3>
<ul>
<li><strong>Authors: </strong>Mark Braverman, Stephen Newman</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13060">https://arxiv.org/abs/2502.13060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13060">https://arxiv.org/pdf/2502.13060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13060]] Sublinear-Overhead Secure Linear Algebra on a Dishonest Server(https://arxiv.org/abs/2502.13060)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy</a></li>
<li><strong>Abstract: </strong>Most heavy computation occurs on servers owned by a second party. This reduces data privacy, resulting in interest in data-oblivious computation, which typically severely degrades performance. Secure and fast remote computation is particularly important for linear algebra, which comprises a large fraction of total computation and is best run on highly specialized hardware often only accessible through the cloud. We state the natural efficiency and security desiderata for fast, remote, and data-oblivious linear algebra, conjecture the existence of matrix and vector families implying satisfactory algorithms, and provide such an algorithm contingent on common cryptographic assumptions. We achieve sublinear overhead for the server, dramatically reduced computation cost for the client, and various other practical advantages over previous algorithms. Keywords: Data Privacy, Data-Oblivious Computation, Delegation, Homomorphic Encryption, Cloud Computing, Algorithm Efficiency, Sublinear Overhead, LPN, Matrix Multiplication.</li>
</ul>

<h3>Title: Improved Fine-Tuning of Large Multimodal Models for Hateful Meme Detection</h3>
<ul>
<li><strong>Authors: </strong>Jingbiao Mei, Jinghong Chen, Guangyu Yang, Weizhe Lin, Bill Byrne</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13061">https://arxiv.org/abs/2502.13061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13061">https://arxiv.org/pdf/2502.13061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13061]] Improved Fine-Tuning of Large Multimodal Models for Hateful Meme Detection(https://arxiv.org/abs/2502.13061)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Hateful memes have become a significant concern on the Internet, necessitating robust automated detection systems. While large multimodal models have shown strong generalization across various tasks, they exhibit poor generalization to hateful meme detection due to the dynamic nature of memes tied to emerging social trends and breaking news. Recent work further highlights the limitations of conventional supervised fine-tuning for large multimodal models in this context. To address these challenges, we propose Large Multimodal Model Retrieval-Guided Contrastive Learning (LMM-RGCL), a novel two-stage fine-tuning framework designed to improve both in-domain accuracy and cross-domain generalization. Experimental results on six widely used meme classification datasets demonstrate that LMM-RGCL achieves state-of-the-art performance, outperforming agent-based systems such as VPD-PALI-X-55B. Furthermore, our method effectively generalizes to out-of-domain memes under low-resource settings, surpassing models like GPT-4o.</li>
</ul>

<h3>Title: RobuRCDet: Enhancing Robustness of Radar-Camera Fusion in Bird's Eye View for 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Jingtong Yue, Zhiwei Lin, Xin Lin, Xiaoyu Zhou, Xiangtai Li, Lu Qi, Yongtao Wang, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13071">https://arxiv.org/abs/2502.13071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13071">https://arxiv.org/pdf/2502.13071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13071]] RobuRCDet: Enhancing Robustness of Radar-Camera Fusion in Bird's Eye View for 3D Object Detection(https://arxiv.org/abs/2502.13071)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While recent low-cost radar-camera approaches have shown promising results in multi-modal 3D object detection, both sensors face challenges from environmental and intrinsic disturbances. Poor lighting or adverse weather conditions degrade camera performance, while radar suffers from noise and positional ambiguity. Achieving robust radar-camera 3D object detection requires consistent performance across varying conditions, a topic that has not yet been fully explored. In this work, we first conduct a systematic analysis of robustness in radar-camera detection on five kinds of noises and propose RobuRCDet, a robust object detection model in BEV. Specifically, we design a 3D Gaussian Expansion (3DGE) module to mitigate inaccuracies in radar points, including position, Radar Cross-Section (RCS), and velocity. The 3DGE uses RCS and velocity priors to generate a deformable kernel map and variance for kernel size adjustment and value distribution. Additionally, we introduce a weather-adaptive fusion module, which adaptively fuses radar and camera features based on camera signal confidence. Extensive experiments on the popular benchmark, nuScenes, show that our model achieves competitive results in regular and noisy conditions.</li>
</ul>

<h3>Title: BOLIMES: Boruta and LIME optiMized fEature Selection for Gene Expression Classification</h3>
<ul>
<li><strong>Authors: </strong>Bich-Chung Phan, Thanh Ma, Huu-Hoa Nguyen, and Thanh-Nghi Do</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13080">https://arxiv.org/abs/2502.13080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13080">https://arxiv.org/pdf/2502.13080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13080]] BOLIMES: Boruta and LIME optiMized fEature Selection for Gene Expression Classification(https://arxiv.org/abs/2502.13080)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Gene expression classification is a pivotal yet challenging task in bioinformatics, primarily due to the high dimensionality of genomic data and the risk of overfitting. To bridge this gap, we propose BOLIMES, a novel feature selection algorithm designed to enhance gene expression classification by systematically refining the feature subset. Unlike conventional methods that rely solely on statistical ranking or classifier-specific selection, we integrate the robustness of Boruta with the interpretability of LIME, ensuring that only the most relevant and influential genes are retained. BOLIMES first employs Boruta to filter out non-informative genes by comparing each feature against its randomized counterpart, thus preserving valuable information. It then uses LIME to rank the remaining genes based on their local importance to the classifier. Finally, an iterative classification evaluation determines the optimal feature subset by selecting the number of genes that maximizes predictive accuracy. By combining exhaustive feature selection with interpretability-driven refinement, our solution effectively balances dimensionality reduction with high classification performance, offering a powerful solution for high-dimensional gene expression analysis.</li>
</ul>

<h3>Title: Personalized Image Generation with Deep Generative Models: A Decade Survey</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Wei, Yiheng Zheng, Yabo Zhang, Ming Liu, Zhilong Ji, Lei Zhang, Wangmeng Zuo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13081">https://arxiv.org/abs/2502.13081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13081">https://arxiv.org/pdf/2502.13081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13081]] Personalized Image Generation with Deep Generative Models: A Decade Survey(https://arxiv.org/abs/2502.13081)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in generative models have significantly facilitated the development of personalized content creation. Given a small set of images with user-specific concept, personalized image generation allows to create images that incorporate the specified concept and adhere to provided text descriptions. Due to its wide applications in content creation, significant effort has been devoted to this field in recent years. Nonetheless, the technologies used for personalization have evolved alongside the development of generative models, with their distinct and interrelated components. In this survey, we present a comprehensive review of generalized personalized image generation across various generative models, including traditional GANs, contemporary text-to-image diffusion models, and emerging multi-model autoregressive models. We first define a unified framework that standardizes the personalization process across different generative models, encompassing three key components, i.e., inversion spaces, inversion methods, and personalization schemes. This unified framework offers a structured approach to dissecting and comparing personalization techniques across different generative architectures. Building upon this unified framework, we further provide an in-depth analysis of personalization techniques within each generative model, highlighting their unique contributions and innovations. Through comparative analysis, this survey elucidates the current landscape of personalized image generation, identifying commonalities and distinguishing features among existing methods. Finally, we discuss the open challenges in the field and propose potential directions for future research. We keep tracing related works at this https URL.</li>
</ul>

<h3>Title: Text2World: Benchmarking Large Language Models for Symbolic World Model Generation</h3>
<ul>
<li><strong>Authors: </strong>Mengkang Hu, Tianxing Chen, Yude Zou, Yuheng Lei, Qiguang Chen, Ming Li, Hongyuan Zhang, Wenqi Shao, Ping Luo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13092">https://arxiv.org/abs/2502.13092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13092">https://arxiv.org/pdf/2502.13092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13092]] Text2World: Benchmarking Large Language Models for Symbolic World Model Generation(https://arxiv.org/abs/2502.13092)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recently, there has been growing interest in leveraging large language models (LLMs) to generate symbolic world models from textual descriptions. Although LLMs have been extensively explored in the context of world modeling, prior studies encountered several challenges, including evaluation randomness, dependence on indirect metrics, and a limited domain scope. To address these limitations, we introduce a novel benchmark, Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. We benchmark current LLMs using Text2World and find that reasoning models trained with large-scale reinforcement learning outperform others. However, even the best-performing model still demonstrates limited capabilities in world modeling. Building on these insights, we examine several promising strategies to enhance the world modeling capabilities of LLMs, including test-time scaling, agent training, and more. We hope that Text2World can serve as a crucial resource, laying the groundwork for future research in leveraging LLMs as world models. The project page is available at this https URL.</li>
</ul>

<h3>Title: Understanding and Rectifying Safety Perception Distortion in VLMs</h3>
<ul>
<li><strong>Authors: </strong>Xiaohan Zou, Jian Kang, George Kesidis, Lu Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13095">https://arxiv.org/abs/2502.13095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13095">https://arxiv.org/pdf/2502.13095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13095]] Understanding and Rectifying Safety Perception Distortion in VLMs(https://arxiv.org/abs/2502.13095)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Recent studies reveal that vision-language models (VLMs) become more susceptible to harmful requests and jailbreak attacks after integrating the vision modality, exhibiting greater vulnerability than their text-only LLM backbones. To uncover the root cause of this phenomenon, we conduct an in-depth analysis and identify a key issue: multimodal inputs introduce an modality-induced activation shift toward a "safer" direction compared to their text-only counterparts, leading VLMs to systematically overestimate the safety of harmful inputs. We refer to this issue as safety perception distortion. To mitigate such distortion, we propose Activation Shift Disentanglement and Calibration (ShiftDC), a training-free method that decomposes and calibrates the modality-induced activation shift to reduce the impact of modality on safety. By isolating and removing the safety-relevant component, ShiftDC restores the inherent safety alignment of the LLM backbone while preserving the vision-language capabilities of VLMs. Empirical results demonstrate that ShiftDC significantly enhances alignment performance on safety benchmarks without impairing model utility.</li>
</ul>

<h3>Title: WeedsGalore: A Multispectral and Multitemporal UAV-based Dataset for Crop and Weed Segmentation in Agricultural Maize Fields</h3>
<ul>
<li><strong>Authors: </strong>Ekin Celikkan, Timo Kunzmann, Yertay Yeskaliyev, Sibylle Itzerott, Nadja Klein, Martin Herold</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13103">https://arxiv.org/abs/2502.13103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13103">https://arxiv.org/pdf/2502.13103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13103]] WeedsGalore: A Multispectral and Multitemporal UAV-based Dataset for Crop and Weed Segmentation in Agricultural Maize Fields(https://arxiv.org/abs/2502.13103)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Weeds are one of the major reasons for crop yield loss but current weeding practices fail to manage weeds in an efficient and targeted manner. Effective weed management is especially important for crops with high worldwide production such as maize, to maximize crop yield for meeting increasing global demands. Advances in near-sensing and computer vision enable the development of new tools for weed management. Specifically, state-of-the-art segmentation models, coupled with novel sensing technologies, can facilitate timely and accurate weeding and monitoring systems. However, learning-based approaches require annotated data and show a lack of generalization to aerial imaging for different crops. We present a novel dataset for semantic and instance segmentation of crops and weeds in agricultural maize fields. The multispectral UAV-based dataset contains images with RGB, red-edge, and near-infrared bands, a large number of plant instances, dense annotations for maize and four weed classes, and is multitemporal. We provide extensive baseline results for both tasks, including probabilistic methods to quantify prediction uncertainty, improve model calibration, and demonstrate the approach's applicability to out-of-distribution data. The results show the effectiveness of the two additional bands compared to RGB only, and better performance in our target domain than models trained on existing datasets. We hope our dataset advances research on methods and operational systems for fine-grained weed identification, enhancing the robustness and applicability of UAV-based weed management. The dataset and code are available at this https URL</li>
</ul>

<h3>Title: Improving Clinical Question Answering with Multi-Task Learning: A Joint Approach for Answer Extraction and Medical Categorization</h3>
<ul>
<li><strong>Authors: </strong>Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Amit Agarwal, Bhargava Kumar, Srikant Panda, Tejaswini Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13108">https://arxiv.org/abs/2502.13108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13108">https://arxiv.org/pdf/2502.13108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13108]] Improving Clinical Question Answering with Multi-Task Learning: A Joint Approach for Answer Extraction and Medical Categorization(https://arxiv.org/abs/2502.13108)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Clinical Question Answering (CQA) plays a crucial role in medical decision-making, enabling physicians to extract relevant information from Electronic Medical Records (EMRs). While transformer-based models such as BERT, BioBERT, and ClinicalBERT have demonstrated state-of-the-art performance in CQA, existing models lack the ability to categorize extracted answers, which is critical for structured retrieval, content filtering, and medical decision support. To address this limitation, we introduce a Multi-Task Learning (MTL) framework that jointly trains CQA models for both answer extraction and medical categorization. In addition to predicting answer spans, our model classifies responses into five standardized medical categories: Diagnosis, Medication, Symptoms, Procedure, and Lab Reports. This categorization enables more structured and interpretable outputs, making clinical QA models more useful in real-world healthcare settings. We evaluate our approach on emrQA, a large-scale dataset for medical question answering. Results show that MTL improves F1-score by 2.2% compared to standard fine-tuning, while achieving 90.7% accuracy in answer categorization. These findings suggest that MTL not only enhances CQA performance but also introduces an effective mechanism for categorization and structured medical information retrieval.</li>
</ul>

<h3>Title: Near-Optimal Private Learning in Linear Contextual Bandits</h3>
<ul>
<li><strong>Authors: </strong>Fan Chen, Jiachun Li, Alexander Rakhlin, David Simchi-Levi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13115">https://arxiv.org/abs/2502.13115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13115">https://arxiv.org/pdf/2502.13115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13115]] Near-Optimal Private Learning in Linear Contextual Bandits(https://arxiv.org/abs/2502.13115)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>We analyze the problem of private learning in generalized linear contextual bandits. Our approach is based on a novel method of re-weighted regression, yielding an efficient algorithm with regret of order $\sqrt{T}+\frac{1}{\alpha}$ and $\sqrt{T}/\alpha$ in the joint and local model of $\alpha$-privacy, respectively. Further, we provide near-optimal private procedures that achieve dimension-independent rates in private linear models and linear contextual bandits. In particular, our results imply that joint privacy is almost "for free" in all the settings we consider, partially addressing the open problem posed by Azize and Basu (2024).</li>
</ul>

<h3>Title: STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Narun Raman, Taylor Lundy, Thiago Amin, Jesse Perla, Kevin-Leyton Brown</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13119">https://arxiv.org/abs/2502.13119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13119">https://arxiv.org/pdf/2502.13119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13119]] STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models(https://arxiv.org/abs/2502.13119)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>How should one judge whether a given large language model (LLM) can reliably perform economic reasoning? Most existing LLM benchmarks focus on specific applications and fail to present the model with a rich variety of economic tasks. A notable exception is Raman et al. [2024], who offer an approach for comprehensively benchmarking strategic decision-making; however, this approach fails to address the non-strategic settings prevalent in microeconomics, such as supply-and-demand analysis. We address this gap by taxonomizing microeconomic reasoning into $58$ distinct elements, focusing on the logic of supply and demand, each grounded in up to $10$ distinct domains, $5$ perspectives, and $3$ types. The generation of benchmark data across this combinatorial space is powered by a novel LLM-assisted data generation protocol that we dub auto-STEER, which generates a set of questions by adapting handwritten templates to target new domains and perspectives. Because it offers an automated way of generating fresh questions, auto-STEER mitigates the risk that LLMs will be trained to over-fit evaluation benchmarks; we thus hope that it will serve as a useful tool both for evaluating and fine-tuning models for years to come. We demonstrate the usefulness of our benchmark via a case study on $27$ LLMs, ranging from small open-source models to the current state of the art. We examined each model's ability to solve microeconomic problems across our whole taxonomy and present the results across a range of prompting strategies and scoring metrics.</li>
</ul>

<h3>Title: Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language in a Coreference Context</h3>
<ul>
<li><strong>Authors: </strong>Marion Bartl, Thomas Brendan Murphy, Susan Leavy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13120">https://arxiv.org/abs/2502.13120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13120">https://arxiv.org/pdf/2502.13120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13120]] Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language in a Coreference Context(https://arxiv.org/abs/2502.13120)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Gender-inclusive language is often used with the aim of ensuring that all individuals, regardless of gender, can be associated with certain concepts. While psycholinguistic studies have examined its effects in relation to human cognition, it remains unclear how Large Language Models (LLMs) process gender-inclusive language. Given that commercial LLMs are gaining an increasingly strong foothold in everyday applications, it is crucial to examine whether LLMs in fact interpret gender-inclusive language neutrally, because the language they generate has the potential to influence the language of their users. This study examines whether LLM-generated coreferent terms align with a given gender expression or reflect model biases. Adapting psycholinguistic methods from French to English and German, we find that in English, LLMs generally maintain the antecedent's gender but exhibit underlying masculine bias. In German, this bias is much stronger, overriding all tested gender-neutralization strategies.</li>
</ul>

<h3>Title: RuozhiBench: Evaluating LLMs with Logical Fallacies and Misleading Premises</h3>
<ul>
<li><strong>Authors: </strong>Zenan Zhai, Hao Li, Xudong Han, Zhenxuan Zhang, Yixuan Zhang, Timothy Baldwin, Haonan Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13125">https://arxiv.org/abs/2502.13125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13125">https://arxiv.org/pdf/2502.13125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13125]] RuozhiBench: Evaluating LLMs with Logical Fallacies and Misleading Premises(https://arxiv.org/abs/2502.13125)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have shown that they can answer questions requiring complex reasoning. However, their ability to identify and respond to text containing logical fallacies or deliberately misleading premises remains less studied. To address this gap, we introduce RuozhiBench, a bilingual dataset comprising 677 carefully curated questions that contain various forms of deceptive reasoning, meticulously crafted through extensive human effort and expert review. In a comprehensive evaluation of 17 LLMs from 5 Series over RuozhiBench using both open-ended and two-choice formats, we conduct extensive analyses on evaluation protocols and result patterns. Despite their high scores on conventional benchmarks, these models showed limited ability to detect and reason correctly about logical fallacies, with even the best-performing model, Claude-3-haiku, achieving only 62% accuracy compared to the human of more than 90%.</li>
</ul>

<h3>Title: Facilitating Long Context Understanding via Supervised Chain-of-Thought Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jingyang Lin, Andy Wong, Tian Xia, Shenghua He, Hui Wei, Mei Han, Jiebo Luo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13127">https://arxiv.org/abs/2502.13127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13127">https://arxiv.org/pdf/2502.13127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13127]] Facilitating Long Context Understanding via Supervised Chain-of-Thought Reasoning(https://arxiv.org/abs/2502.13127)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) have enabled them to process increasingly longer sequences, ranging from 2K to 2M tokens and even beyond. However, simply extending the input sequence length does not necessarily lead to effective long-context understanding. In this study, we integrate Chain-of-Thought (CoT) reasoning into LLMs in a supervised manner to facilitate effective long-context understanding. To achieve this, we introduce LongFinanceQA, a synthetic dataset in the financial domain designed to improve long-context reasoning. Unlike existing long-context synthetic data, LongFinanceQA includes intermediate CoT reasoning before the final conclusion, which encourages LLMs to perform explicit reasoning, improving accuracy and interpretability in long-context understanding. To generate synthetic CoT reasoning, we propose Property-driven Agentic Inference (PAI), an agentic framework that simulates human-like reasoning steps, including property extraction, retrieval, and summarization. We evaluate PAI's reasoning capabilities by assessing GPT-4o-mini w/ PAI on the Loong benchmark, outperforming standard GPT-4o-mini by 20.0%. Furthermore, we fine-tune LLaMA-3.1-8B-Instruct on LongFinanceQA, achieving a 24.6% gain on Loong's financial subset.</li>
</ul>

<h3>Title: Is Noise Conditioning Necessary for Denoising Generative Models?</h3>
<ul>
<li><strong>Authors: </strong>Qiao Sun, Zhicheng Jiang, Hanhong Zhao, Kaiming He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13129">https://arxiv.org/abs/2502.13129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13129">https://arxiv.org/pdf/2502.13129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13129]] Is Noise Conditioning Necessary for Denoising Generative Models?(https://arxiv.org/abs/2502.13129)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>It is widely believed that noise conditioning is indispensable for denoising diffusion models to work successfully. This work challenges this belief. Motivated by research on blind image denoising, we investigate a variety of denoising-based generative models in the absence of noise conditioning. To our surprise, most models exhibit graceful degradation, and in some cases, they even perform better without noise conditioning. We provide a theoretical analysis of the error caused by removing noise conditioning and demonstrate that our analysis aligns with empirical observations. We further introduce a noise-unconditional model that achieves a competitive FID of 2.23 on CIFAR-10, significantly narrowing the gap to leading noise-conditional models. We hope our findings will inspire the community to revisit the foundations and formulations of denoising generative models.</li>
</ul>

<h3>Title: Learning to Defer for Causal Discovery with Imperfect Experts</h3>
<ul>
<li><strong>Authors: </strong>Oscar Clivio, Divyat Mahajan, Perouz Taslakian, Sara Magliacane, Ioannis Mitliagkas, Valentina Zantedeschi, Alexandre Drouin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13132">https://arxiv.org/abs/2502.13132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13132">https://arxiv.org/pdf/2502.13132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13132]] Learning to Defer for Causal Discovery with Imperfect Experts(https://arxiv.org/abs/2502.13132)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Integrating expert knowledge, e.g. from large language models, into causal discovery algorithms can be challenging when the knowledge is not guaranteed to be correct. Expert recommendations may contradict data-driven results, and their reliability can vary significantly depending on the domain or specific query. Existing methods based on soft constraints or inconsistencies in predicted causal relationships fail to account for these variations in expertise. To remedy this, we propose L2D-CD, a method for gauging the correctness of expert recommendations and optimally combining them with data-driven causal discovery results. By adapting learning-to-defer (L2D) algorithms for pairwise causal discovery (CD), we learn a deferral function that selects whether to rely on classical causal discovery methods using numerical data or expert recommendations based on textual meta-data. We evaluate L2D-CD on the canonical Tübingen pairs dataset and demonstrate its superior performance compared to both the causal discovery method and the expert used in isolation. Moreover, our approach identifies domains where the expert's performance is strong or weak. Finally, we outline a strategy for generalizing this approach to causal discovery on graphs with more than two variables, paving the way for further research in this area.</li>
</ul>

<h3>Title: AV-Flow: Transforming Text to Audio-Visual Human-like Interactions</h3>
<ul>
<li><strong>Authors: </strong>Aggelina Chatziagapi, Louis-Philippe Morency, Hongyu Gong, Michael Zollhoefer, Dimitris Samaras, Alexander Richard</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13133">https://arxiv.org/abs/2502.13133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13133">https://arxiv.org/pdf/2502.13133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13133]] AV-Flow: Transforming Text to Audio-Visual Human-like Interactions(https://arxiv.org/abs/2502.13133)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>We introduce AV-Flow, an audio-visual generative model that animates photo-realistic 4D talking avatars given only text input. In contrast to prior work that assumes an existing speech signal, we synthesize speech and vision jointly. We demonstrate human-like speech synthesis, synchronized lip motion, lively facial expressions and head pose; all generated from just text characters. The core premise of our approach lies in the architecture of our two parallel diffusion transformers. Intermediate highway connections ensure communication between the audio and visual modalities, and thus, synchronized speech intonation and facial dynamics (e.g., eyebrow motion). Our model is trained with flow matching, leading to expressive results and fast inference. In case of dyadic conversations, AV-Flow produces an always-on avatar, that actively listens and reacts to the audio-visual input of a user. Through extensive experiments, we show that our method outperforms prior work, synthesizing natural-looking 4D talking avatars. Project page: this https URL</li>
</ul>

<h3>Title: Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions</h3>
<ul>
<li><strong>Authors: </strong>Taedong Yun, Eric Yang, Mustafa Safdari, Jong Ha Lee, Vaishnavi Vinod Kumar, S. Sara Mahdavi, Jonathan Amar, Derek Peyton, Reut Aharony, Andreas Michaelides, Logan Schneider, Isaac Galatzer-Levy, Yugang Jia, John Canny, Arthur Gretton, Maja Matarić</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13135">https://arxiv.org/abs/2502.13135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13135">https://arxiv.org/pdf/2502.13135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13135]] Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions(https://arxiv.org/abs/2502.13135)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present an end-to-end framework for generating synthetic users for evaluating interactive agents designed to encourage positive behavior changes, such as in health and lifestyle coaching. The synthetic users are grounded in health and lifestyle conditions, specifically sleep and diabetes management in this study, to ensure realistic interactions with the health coaching agent. Synthetic users are created in two stages: first, structured data are generated grounded in real-world health and lifestyle factors in addition to basic demographics and behavioral attributes; second, full profiles of the synthetic users are developed conditioned on the structured data. Interactions between synthetic users and the coaching agent are simulated using generative agent-based models such as Concordia, or directly by prompting a language model. Using two independently-developed agents for sleep and diabetes coaching as case studies, the validity of this framework is demonstrated by analyzing the coaching agent's understanding of the synthetic users' needs and challenges. Finally, through multiple blinded evaluations of user-coach interactions by human experts, we demonstrate that our synthetic users with health and behavioral attributes more accurately portray real human users with the same attributes, compared to generic synthetic users not grounded in such attributes. The proposed framework lays the foundation for efficient development of conversational agents through extensive, realistic, and grounded simulated interactions.</li>
</ul>

<h3>Title: UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Huawei Lin, Yingjie Lao, Tong Geng, Tan Yu, Weijie Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13141">https://arxiv.org/abs/2502.13141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13141">https://arxiv.org/pdf/2502.13141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13141]] UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models(https://arxiv.org/abs/2502.13141)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are vulnerable to attacks like prompt injection, backdoor attacks, and adversarial attacks, which manipulate prompts or models to generate harmful outputs. In this paper, departing from traditional deep learning attack paradigms, we explore their intrinsic relationship and collectively term them Prompt Trigger Attacks (PTA). This raises a key question: Can we determine if a prompt is benign or poisoned? To address this, we propose UniGuardian, the first unified defense mechanism designed to detect prompt injection, backdoor attacks, and adversarial attacks in LLMs. Additionally, we introduce a single-forward strategy to optimize the detection pipeline, enabling simultaneous attack detection and text generation within a single forward pass. Our experiments confirm that UniGuardian accurately and efficiently identifies malicious prompts in LLMs.</li>
</ul>

<h3>Title: Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation</h3>
<ul>
<li><strong>Authors: </strong>Bencheng Liao, Hongyuan Tao, Qian Zhang, Tianheng Cheng, Yingyue Li, Haoran Yin, Wenyu Liu, Xinggang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13145">https://arxiv.org/abs/2502.13145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13145">https://arxiv.org/pdf/2502.13145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13145]] Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation(https://arxiv.org/abs/2502.13145)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-complexity native multimodal state space models through progressive distillation from existing MLLMs using moderate academic computational resources. Our approach enables the direct conversion of trained decoder-only MLLMs to linear-complexity architectures without requiring pre-trained RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba from trained Transformer and a three-stage distillation recipe, which can effectively transfer the knowledge from Transformer to Mamba while preserving multimodal capabilities. Our method also supports flexible hybrid architectures that combine Transformer and Mamba layers for customizable efficiency-performance trade-offs. Distilled from the Transformer-based decoder-only HoVLE, mmMamba-linear achieves competitive performance against existing linear and quadratic-complexity VLMs, while mmMamba-hybrid further improves performance significantly, approaching HoVLE's capabilities. At 103K tokens, mmMamba-linear demonstrates 20.6$\times$ speedup and 75.8% GPU memory reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\times$ speedup and 60.2% memory savings. Code and models are released at this https URL</li>
</ul>

<h3>Title: Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Shuo Xing, Yuping Wang, Peiran Li, Ruizheng Bai, Yueqi Wang, Chengxuan Qian, Huaxiu Yao, Zhengzhong Tu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2502.13146">https://arxiv.org/abs/2502.13146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2502.13146">https://arxiv.org/pdf/2502.13146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2502.13146]] Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization(https://arxiv.org/abs/2502.13146)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The emergence of large Vision Language Models (VLMs) has broadened the scope and capabilities of single-modal Large Language Models (LLMs) by integrating visual modalities, thereby unlocking transformative cross-modal applications in a variety of real-world scenarios. Despite their impressive performance, VLMs are prone to significant hallucinations, particularly in the form of cross-modal inconsistencies. Building on the success of Reinforcement Learning from Human Feedback (RLHF) in aligning LLMs, recent advancements have focused on applying direct preference optimization (DPO) on carefully curated datasets to mitigate these issues. Yet, such approaches typically introduce preference signals in a brute-force manner, neglecting the crucial role of visual information in the alignment process. In this paper, we introduce Re-Align, a novel alignment framework that leverages image retrieval to construct a dual-preference dataset, effectively incorporating both textual and visual preference signals. We further introduce rDPO, an extension of the standard direct preference optimization that incorporates an additional visual preference objective during fine-tuning. Our experimental results demonstrate that Re-Align not only mitigates hallucinations more effectively than previous methods but also yields significant performance gains in general visual question-answering (VQA) tasks. Moreover, we show that Re-Align maintains robustness and scalability across a wide range of VLM sizes and architectures. This work represents a significant step forward in aligning multimodal LLMs, paving the way for more reliable and effective cross-modal applications. We release all the code in this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
