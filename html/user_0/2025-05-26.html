<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-05-26</h1>
<h3>Title: Preparing for the Post Quantum Era: Quantum Ready Architecture for Security and Risk Management (QUASAR) -- A Strategic Framework for Cybersecurity</h3>
<ul>
<li><strong>Authors: </strong>Abraham Itzhak Weinberg</a></li>
<li><strong>Subjects: </strong>cs.CR, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17034">https://arxiv.org/abs/2505.17034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17034">https://arxiv.org/pdf/2505.17034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17034]] Preparing for the Post Quantum Era: Quantum Ready Architecture for Security and Risk Management (QUASAR) -- A Strategic Framework for Cybersecurity(https://arxiv.org/abs/2505.17034)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>As quantum computing progresses, traditional cryptographic systems face the threat of obsolescence due to the capabilities of quantum algorithms. This paper introduces the Quantum-Ready Architecture for Security and Risk Management (QUASAR), a novel framework designed to help organizations prepare for the post-quantum era. QUASAR provides a structured approach to transition from current cryptographic systems to quantum-resistant alternatives, emphasizing technical, security, and operational readiness. The framework integrates a set of actionable components, a timeline for phased implementation, and continuous optimization strategies to ensure ongoing preparedness. Through performance indicators, readiness scores, and optimization functions, QUASAR enables organizations to assess their current state, identify gaps, and execute targeted actions to mitigate risks posed by quantum computing. By offering a comprehensive, adaptable, and quantifiable strategy, QUASAR equips organizations with the tools necessary to future-proof their operations and secure sensitive data against the impending rise of quantum technologies.</li>
</ul>

<h3>Title: Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Dimitri Schreiter</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17037">https://arxiv.org/abs/2505.17037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17037">https://arxiv.org/pdf/2505.17037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17037]] Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge(https://arxiv.org/abs/2505.17037)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prompt engineering has emerged as a critical component in optimizing large language models (LLMs) for domain-specific tasks. However, the role of prompt specificity, especially in domains like STEM (physics, chemistry, biology, computer science and mathematics), medicine, and law, remains underexplored. This thesis addresses the problem of whether increasing the specificity of vocabulary in prompts improves LLM performance in domain-specific question-answering and reasoning tasks. We developed a synonymization framework to systematically substitute nouns, verbs, and adjectives with varying specificity levels, measuring the impact on four LLMs: Llama-3.1-70B-Instruct, Granite-13B-Instruct-V2, Flan-T5-XL, and Mistral-Large 2, across datasets in STEM, law, and medicine. Our results reveal that while generally increasing the specificity of prompts does not have a significant impact, there appears to be a specificity range, across all considered models, where the LLM performs the best. Identifying this optimal specificity range offers a key insight for prompt design, suggesting that manipulating prompts within this range could maximize LLM performance and lead to more efficient applications in specialized domains.</li>
</ul>

<h3>Title: Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion</h3>
<ul>
<li><strong>Authors: </strong>Xian Gong, Paul X. McCarthy, Lin Tian, Marian-Andrei Rizoiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17038">https://arxiv.org/abs/2505.17038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17038">https://arxiv.org/pdf/2505.17038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17038]] Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion(https://arxiv.org/abs/2505.17038)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Massive and diverse web data are increasingly vital for government disaster response, as demonstrated by the 2022 floods in New South Wales (NSW), Australia. This study examines how X (formerly Twitter) and public inquiry submissions provide insights into public behaviour during crises. We analyse more than 55,000 flood-related tweets and 1,450 submissions to identify behavioural patterns during extreme weather events. While social media posts are short and fragmented, inquiry submissions are detailed, multi-page documents offering structured insights. Our methodology integrates Latent Dirichlet Allocation (LDA) for topic modelling with Large Language Models (LLMs) to enhance semantic understanding. LDA reveals distinct opinions and geographic patterns, while LLMs improve filtering by identifying flood-relevant tweets using public submissions as a reference. This Relevance Index method reduces noise and prioritizes actionable content, improving situational awareness for emergency responders. By combining these complementary data streams, our approach introduces a novel AI-driven method to refine crisis-related social media content, improve real-time disaster response, and inform long-term resilience planning.</li>
</ul>

<h3>Title: Generalizing Large Language Model Usability Across Resource-Constrained</h3>
<ul>
<li><strong>Authors: </strong>Yun-Da Tsai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17040">https://arxiv.org/abs/2505.17040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17040">https://arxiv.org/pdf/2505.17040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17040]] Generalizing Large Language Model Usability Across Resource-Constrained(https://arxiv.org/abs/2505.17040)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language tasks, and recent efforts have sought to extend their capabilities to multimodal domains and resource-constrained environments. However, existing approaches often rely on costly supervised fine-tuning or assume fixed training conditions, limiting their generalization when facing unseen modalities, limited data, or restricted compute resources. This dissertation presents a systematic study toward generalizing LLM usability under real-world constraints. First, it introduces a robust text-centric alignment framework that enables LLMs to seamlessly integrate diverse modalities-including text, images, tables, and any modalities - via natural language interfaces. This approach supports in-context adaptation to unseen or dynamically changing modalities without requiring retraining. To enhance robustness against noisy and missing modalities, an adversarial prompting technique is proposed, generating semantically challenging perturbations at the prompt level to stress-test model reliability. Beyond multimodal setting, the dissertation investigates inference-time optimization strategies for LLMs, leveraging prompt search and uncertainty quantification to improve performance without additional model training. This perspective offers an efficient alternative to scaling model parameters or retraining from scratch. Additionally, the work addresses low-resource domains such as Verilog code generation by designing correct-by-construction synthetic data pipelines and logic-enhanced reasoning models, achieving state-of-the-art performance with minimal data. Together, these contributions form a unified effort to enhance the adaptability, scalability, and efficiency of large language models under practical constraints.</li>
</ul>

<h3>Title: Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia</h3>
<ul>
<li><strong>Authors: </strong>Afifah Kashif, Heer Patel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17045">https://arxiv.org/abs/2505.17045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17045">https://arxiv.org/pdf/2505.17045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17045]] Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia(https://arxiv.org/abs/2505.17045)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent studies have separately highlighted significant biases within foundational large language models (LLMs) against certain nationalities and stigmatized social groups. This research investigates the ethical implications of these biases intersecting with outputs of widely-used GPT-3.5/4/4o LLMS. Through structured prompt series, we evaluate model responses to several scenarios involving American and North Korean nationalities with various mental disabilities. Findings reveal significant discrepancies in empathy levels with North Koreans facing greater negative bias, particularly when mental disability is also a factor. This underscores the need for improvements in LLMs designed with a nuanced understanding of intersectional identity.</li>
</ul>

<h3>Title: Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe</h3>
<ul>
<li><strong>Authors: </strong>Erin Palm, Astrit Manikantan, Mark E. Pepin, Herprit Mahal, Srikanth Subramanya Belwadi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17047">https://arxiv.org/abs/2505.17047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17047">https://arxiv.org/pdf/2505.17047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17047]] Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe(https://arxiv.org/abs/2505.17047)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>In medical practices across the United States, physicians have begun implementing generative artificial intelligence (AI) tools to perform the function of scribes in order to reduce the burden of documenting clinical encounters. Despite their widespread use, no established methods exist to gauge the quality of AI scribes. To address this gap, we developed a blinded study comparing the relative performance of large language model (LLM) generated clinical notes with those from field experts based on audio-recorded clinical encounters. Quantitative metrics from the Physician Documentation Quality Instrument (PDQI9) provided a framework to measure note quality, which we adapted to assess relative performance of AI generated notes. Clinical experts spanning 5 medical specialties used the PDQI9 tool to evaluate specialist-drafted Gold notes and LLM authored Ambient notes. Two evaluators from each specialty scored notes drafted from a total of 97 patient visits. We found uniformly high inter rater agreement (RWG greater than 0.7) between evaluators in general medicine, orthopedics, and obstetrics and gynecology, and moderate (RWG 0.5 to 0.7) to high inter rater agreement in pediatrics and cardiology. We found a modest yet significant difference in the overall note quality, wherein Gold notes achieved a score of 4.25 out of 5 and Ambient notes scored 4.20 out of 5 (p = 0.04). Our findings support the use of the PDQI9 instrument as a practical method to gauge the quality of LLM authored notes, as compared to human-authored notes.</li>
</ul>

<h3>Title: Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally</h3>
<ul>
<li><strong>Authors: </strong>Agam Shah, Siddhant Sukhani, Huzaifa Pardawala, Saketh Budideti, Riya Bhadani, Rudra Gopal, Siddhartha Somani, Michael Galarnyk, Soungmin Lee, Arnav Hiray, Akshar Ravichandran, Eric Kim, Pranav Aluru, Joshua Zhang, Sebastian Jaskowski, Veer Guda, Meghaj Tarte, Liqin Ye, Spencer Gosden, Rutwik Routu, Rachel Yuh, Sloka Chava, Sahasra Chava, Dylan Patrick Kelly, Aiden Chiang, Harsit Mittal, Sudheer Chava</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, q-fin.CP, q-fin.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17048">https://arxiv.org/abs/2505.17048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17048">https://arxiv.org/pdf/2505.17048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17048]] Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally(https://arxiv.org/abs/2505.17048)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Central banks around the world play a crucial role in maintaining economic stability. Deciphering policy implications in their communications is essential, especially as misinterpretations can disproportionately impact vulnerable populations. To address this, we introduce the World Central Banks (WCB) dataset, the most comprehensive monetary policy corpus to date, comprising over 380k sentences from 25 central banks across diverse geographic regions, spanning 28 years of historical data. After uniformly sampling 1k sentences per bank (25k total) across all available years, we annotate and review each sentence using dual annotators, disagreement resolutions, and secondary expert reviews. We define three tasks: Stance Detection, Temporal Classification, and Uncertainty Estimation, with each sentence annotated for all three. We benchmark seven Pretrained Language Models (PLMs) and nine Large Language Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on these tasks, running 15,075 benchmarking experiments. We find that a model trained on aggregated data across banks significantly surpasses a model trained on an individual bank's data, confirming the principle "the whole is greater than the sum of its parts." Additionally, rigorous human evaluations, error analyses, and predictive tasks validate our framework's economic utility. Our artifacts are accessible through the HuggingFace and GitHub under the CC-BY-NC-SA 4.0 license.</li>
</ul>

<h3>Title: Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/Résumé Evaluations</h3>
<ul>
<li><strong>Authors: </strong>David Rozado</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17049">https://arxiv.org/abs/2505.17049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17049">https://arxiv.org/pdf/2505.17049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17049]] Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/Résumé Evaluations(https://arxiv.org/abs/2505.17049)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study examines the behavior of Large Language Models (LLMs) when evaluating professional candidates based on their resumes or curricula vitae (CVs). In an experiment involving 22 leading LLMs, each model was systematically given one job description along with a pair of profession-matched CVs, one bearing a male first name, the other a female first name, and asked to select the more suitable candidate for the job. Each CV pair was presented twice, with names swapped to ensure that any observed preferences in candidate selection stemmed from gendered names cues. Despite identical professional qualifications across genders, all LLMs consistently favored female-named candidates across 70 different professions. Adding an explicit gender field (male/female) to the CVs further increased the preference for female applicants. When gendered names were replaced with gender-neutral identifiers "Candidate A" and "Candidate B", several models displayed a preference to select "Candidate A". Counterbalancing gender assignment between these gender-neutral identifiers resulted in gender parity in candidate selection. When asked to rate CVs in isolation rather than compare pairs, LLMs assigned slightly higher average scores to female CVs overall, but the effect size was negligible. Including preferred pronouns (he/him or she/her) next to a candidate's name slightly increased the odds of the candidate being selected regardless of gender. Finally, most models exhibited a substantial positional bias to select the candidate listed first in the prompt. These findings underscore the need for caution when deploying LLMs in high-stakes autonomous decision-making contexts and raise doubts about whether LLMs consistently apply principled reasoning.</li>
</ul>

<h3>Title: Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning</h3>
<ul>
<li><strong>Authors: </strong>Yanhao Jia, Xinyi Wu, Qinglin Zhang, Yiran Qin, Luwei Xiao, Shuai Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CE, cs.CY, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17050">https://arxiv.org/abs/2505.17050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17050">https://arxiv.org/pdf/2505.17050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17050]] Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning(https://arxiv.org/abs/2505.17050)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Project-Based Learning (PBL) involves a variety of highly correlated multimodal data, making it a vital educational approach within STEM disciplines. With the rapid development of multimodal large language models (MLLMs), researchers have begun exploring their potential to enhance tasks such as information retrieval, knowledge comprehension, and data generation in educational settings. However, existing benchmarks fall short in providing both a free-form output structure and a rigorous human expert validation process, limiting their effectiveness in evaluating real-world educational tasks. Additionally, few methods have developed automated pipelines to assist with the complex responsibilities of teachers leveraging MLLMs, largely due to model hallucination and instability, which lead to unreliable implementation. To address this gap, we introduce PBLBench, a novel benchmark designed to evaluate complex reasoning grounded in domain-specific knowledge and long-context understanding, thereby challenging models with tasks that closely resemble those handled by human experts. To establish reliable ground truth, we adopt the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise comparisons to derive structured and weighted evaluation criteria. We assess the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that even the most advanced models achieve only 59% rank accuracy, underscoring the significant challenges presented by this benchmark. We believe PBLBench will serve as a catalyst for the development of more capable AI agents, ultimately aiming to alleviate teacher workload and enhance educational productivity.</li>
</ul>

<h3>Title: Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bernd Huber, Ghazal Fazelnia, Andreas Damianou, Sebastian Peleato, Max Lefarov, Praveen Ravichandran, Marco De Nadai, Mounia Lalmas-Roellke, Paul N. Bennett</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17051">https://arxiv.org/abs/2505.17051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17051">https://arxiv.org/pdf/2505.17051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17051]] Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models(https://arxiv.org/abs/2505.17051)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel at generating contextually relevant content. However, tailoring these outputs to individual users for effective personalization is a significant challenge. While rich user-specific information often exists as pre-existing user representations, such as embeddings learned from preferences or behaviors, current methods to leverage these for LLM personalization typically require costly fine-tuning or token-heavy prompting. We propose Embedding-to-Prefix (E2P), a parameter-efficient method that injects pre-computed context embeddings into an LLM's hidden representation space through a learned projection to a single soft token prefix. This enables effective personalization while keeping the backbone model frozen and avoiding expensive adaptation techniques. We evaluate E2P across two public datasets and in a production setting: dialogue personalization on Persona-Chat, contextual headline generation on PENS, and large-scale personalization for music and podcast consumption. Results show that E2P preserves contextual signals and achieves strong performance with minimal computational overhead, offering a scalable, efficient solution for contextualizing generative AI systems.</li>
</ul>

<h3>Title: SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jinwoo Park, Seunggeun Cho, Dongsu Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17052">https://arxiv.org/abs/2505.17052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17052">https://arxiv.org/pdf/2505.17052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17052]] SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs(https://arxiv.org/abs/2505.17052)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) power many modern applications, but serving them at scale remains costly and resource-intensive. Current server-centric systems overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an edge-assisted inference framework that splits LLM workloads between edge and server GPUs using a speculative decoding scheme, exchanging only token outputs over the network. SpecEdge employs proactive edge drafting to overlap edge token creation with server verification and pipeline-aware scheduling that interleaves multiple user requests to increase server-side throughput. Experiments show SpecEdge enhances overall cost efficiency by 1.91x through achieving 2.22x server throughput, and reduces inter token latency by 11.24% compared to a server-only baseline, introducing a scalable, cost-effective paradigm for LLM serving.</li>
</ul>

<h3>Title: Social preferences with unstable interactive reasoning: Large language models in economic trust games</h3>
<ul>
<li><strong>Authors: </strong>Ou Jiamin, Eikmans Emile, Buskens Vincent, Pankowska Paulina, Shan Yuli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17053">https://arxiv.org/abs/2505.17053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17053">https://arxiv.org/pdf/2505.17053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17053]] Social preferences with unstable interactive reasoning: Large language models in economic trust games(https://arxiv.org/abs/2505.17053)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) have demonstrated remarkable capabilities in understanding human languages, this study explores how they translate this understanding into social exchange contexts that capture certain essences of real world human interactions. Three LLMs - ChatGPT-4, Claude, and Bard - were placed in economic trust games where players balance self-interest with trust and reciprocity, making decisions that reveal their social preferences and interactive reasoning abilities. Our study shows that LLMs deviate from pure self-interest and exhibit trust and reciprocity even without being prompted to adopt a specific persona. In the simplest one-shot interaction, LLMs emulated how human players place trust at the beginning of such a game. Larger human-machine divergences emerged in scenarios involving trust repayment or multi-round interactions, where decisions were influenced by both social preferences and interactive reasoning. LLMs responses varied significantly when prompted to adopt personas like selfish or unselfish players, with the impact outweighing differences between models or game types. Response of ChatGPT-4, in an unselfish or neutral persona, resembled the highest trust and reciprocity, surpassing humans, Claude, and Bard. Claude and Bard displayed trust and reciprocity levels that sometimes exceeded and sometimes fell below human choices. When given selfish personas, all LLMs showed lower trust and reciprocity than humans. Interactive reasoning to the actions of counterparts or changing game mechanics appeared to be random rather than stable, reproducible characteristics in the response of LLMs, though some improvements were observed when ChatGPT-4 responded in selfish or unselfish personas.</li>
</ul>

<h3>Title: METHOD: Modular Efficient Transformer for Health Outcome Discovery</h3>
<ul>
<li><strong>Authors: </strong>Linglong Qian, Zina Ibrahim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17054">https://arxiv.org/abs/2505.17054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17054">https://arxiv.org/pdf/2505.17054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17054]] METHOD: Modular Efficient Transformer for Health Outcome Discovery(https://arxiv.org/abs/2505.17054)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent advances in transformer architectures have revolutionised natural language processing, but their application to healthcare domains presents unique challenges. Patient timelines are characterised by irregular sampling, variable temporal dependencies, and complex contextual relationships that differ substantially from traditional language tasks. This paper introduces \METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel transformer architecture specifically designed to address the challenges of clinical sequence modelling in electronic health records. \METHOD~integrates three key innovations: (1) a patient-aware attention mechanism that prevents information leakage whilst enabling efficient batch processing; (2) an adaptive sliding window attention scheme that captures multi-scale temporal dependencies; and (3) a U-Net inspired architecture with dynamic skip connections for effective long sequence processing. Evaluations on the MIMIC-IV database demonstrate that \METHOD~consistently outperforms the state-of-the-art \ETHOS~model, particularly in predicting high-severity cases that require urgent clinical intervention. \METHOD~exhibits stable performance across varying inference lengths, a crucial feature for clinical deployment where patient histories vary significantly in length. Analysis of learned embeddings reveals that \METHOD~better preserves clinical hierarchies and relationships between medical concepts. These results suggest that \METHOD~represents a significant advancement in transformer architectures optimised for healthcare applications, providing more accurate and clinically relevant predictions whilst maintaining computational efficiency.</li>
</ul>

<h3>Title: Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset</h3>
<ul>
<li><strong>Authors: </strong>Fidaa khandaqji, Huthaifa I. Ashqar, Abdelrahem Atawnih</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17055">https://arxiv.org/abs/2505.17055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17055">https://arxiv.org/pdf/2505.17055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17055]] Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset(https://arxiv.org/abs/2505.17055)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The study aims to enhance mathematics education accessibility for hard-of-hearing students by developing an accurate Palestinian sign language PSL recognition system using advanced artificial intelligence techniques. Due to the scarcity of digital resources for PSL, a custom dataset comprising 41 mathematical gesture classes was created, and recorded by PSL experts to ensure linguistic accuracy and domain specificity. To leverage state-of-the-art-computer vision techniques, a Vision Transformer ViTModel was fine-tuned for gesture classification. The model achieved an accuracy of 97.59%, demonstrating its effectiveness in recognizing mathematical signs with high precision and reliability. This study highlights the role of deep learning in developing intelligent educational tools that bridge the learning gap for hard-of-hearing students by providing AI-driven interactive solutions to enhance mathematical comprehension. This work represents a significant step toward innovative and inclusive frosting digital integration in specialized learning environments. The dataset is hosted on Hugging Face at this https URL.</li>
</ul>

<h3>Title: Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective</h3>
<ul>
<li><strong>Authors: </strong>Luoxi Tang, Tharunya Sundar, Shuai Yang, Ankita Patra, Manohar Chippada, Giqi Zhao, Yi Li, Riteng Zhang, Tunan Zhao, Ting Yang, Yuqiao Meng, Weicheng Ma, Zhaohan Xi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17056">https://arxiv.org/abs/2505.17056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17056">https://arxiv.org/pdf/2505.17056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17056]] Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective(https://arxiv.org/abs/2505.17056)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>AI is transforming education by enabling powerful tools that enhance learning experiences. Among recent advancements, large language models (LLMs) hold particular promise for revolutionizing how learners interact with educational content. In this work, we investigate the potential of LLMs to support standardized test preparation by focusing on English Standardized Tests (ESTs). Specifically, we assess their ability to generate accurate and contextually appropriate solutions across a diverse set of EST question types. We introduce ESTBOOK, a comprehensive benchmark designed to evaluate the capabilities of LLMs in solving EST questions. ESTBOOK aggregates five widely recognized tests, encompassing 29 question types and over 10,576 questions across multiple modalities, including text, images, audio, tables, and mathematical symbols. Using ESTBOOK, we systematically evaluate both the accuracy and inference efficiency of LLMs. Additionally, we propose a breakdown analysis framework that decomposes complex EST questions into task-specific solution steps. This framework allows us to isolate and assess LLM performance at each stage of the reasoning process. Evaluation findings offer insights into the capability of LLMs in educational contexts and point toward targeted strategies for improving their reliability as intelligent tutoring systems.</li>
</ul>

<h3>Title: DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>David Osei Opoku, Ming Sheng, Yong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17058">https://arxiv.org/abs/2505.17058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17058">https://arxiv.org/pdf/2505.17058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17058]] DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation(https://arxiv.org/abs/2505.17058)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Domain-specific QA systems require not just generative fluency but high factual accuracy grounded in structured expert knowledge. While recent Retrieval-Augmented Generation (RAG) frameworks improve context recall, they struggle with integrating heterogeneous data and maintaining reasoning consistency. To address these challenges, we propose DO-RAG, a scalable and customizable hybrid QA framework that integrates multi-level knowledge graph construction with semantic vector retrieval. Our system employs a novel agentic chain-of-thought architecture to extract structured relationships from unstructured, multimodal documents, constructing dynamic knowledge graphs that enhance retrieval precision. At query time, DO-RAG fuses graph and vector retrieval results to generate context-aware responses, followed by hallucination mitigation via grounded refinement. Experimental evaluations in the database and electrical domains show near-perfect recall and over 94% answer relevancy, with DO-RAG outperforming baseline frameworks by up to 33.38%. By combining traceability, adaptability, and performance efficiency, DO-RAG offers a reliable foundation for multi-domain, high-precision QA at scale.</li>
</ul>

<h3>Title: Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large</h3>
<ul>
<li><strong>Authors: </strong>Van-Tinh Nguyen, Hoang-Duong Pham, Thanh-Hai To, Cong-Tuan Hung Do, Thi-Thu-Trang Dong, Vu-Trung Duong Le, Van-Phuc Hoang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17059">https://arxiv.org/abs/2505.17059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17059">https://arxiv.org/pdf/2505.17059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17059]] Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large(https://arxiv.org/abs/2505.17059)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Understanding medical texts presents significant challenges due to complex terminology and context-specific language. This paper introduces Medalyze, an AI-powered application designed to enhance the comprehension of medical texts using three specialized FLAN-T5-Large models. These models are fine-tuned for (1) summarizing medical reports, (2) extracting health issues from patient-doctor conversations, and (3) identifying the key question in a passage. Medalyze is deployed across a web and mobile platform with real-time inference, leveraging scalable API and YugabyteDB. Experimental evaluations demonstrate the system's superior summarization performance over GPT-4 in domain-specific tasks, based on metrics like BLEU, ROUGE-L, BERTScore, and SpaCy Similarity. Medalyze provides a practical, privacy-preserving, and lightweight solution for improving information accessibility in healthcare.</li>
</ul>

<h3>Title: Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Maria-Teresa De Rosa Palmini, Eva Cetinic</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17064">https://arxiv.org/abs/2505.17064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17064">https://arxiv.org/pdf/2505.17064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17064]] Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models(https://arxiv.org/abs/2505.17064)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>As Text-to-Image (TTI) diffusion models become increasingly influential in content creation, growing attention is being directed toward their societal and cultural implications. While prior research has primarily examined demographic and cultural biases, the ability of these models to accurately represent historical contexts remains largely underexplored. In this work, we present a systematic and reproducible methodology for evaluating how TTI systems depict different historical periods. For this purpose, we introduce the HistVis dataset, a curated collection of 30,000 synthetic images generated by three state-of-the-art diffusion models using carefully designed prompts depicting universal human activities across different historical periods. We evaluate generated imagery across three key aspects: (1) Implicit Stylistic Associations: examining default visual styles associated with specific eras; (2) Historical Consistency: identifying anachronisms such as modern artifacts in pre-modern contexts; and (3) Demographic Representation: comparing generated racial and gender distributions against historically plausible baselines. Our findings reveal systematic inaccuracies in historically themed generated imagery, as TTI models frequently stereotype past eras by incorporating unstated stylistic cues, introduce anachronisms, and fail to reflect plausible demographic patterns. By offering a scalable methodology and benchmark for assessing historical representation in generated imagery, this work provides an initial step toward building more historically accurate and culturally aligned TTI models.</li>
</ul>

<h3>Title: Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases</h3>
<ul>
<li><strong>Authors: </strong>Valentina Carbonari, Pierangelo Veltri, Pietro Hiram Guzzi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17065">https://arxiv.org/abs/2505.17065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17065">https://arxiv.org/pdf/2505.17065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17065]] Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases(https://arxiv.org/abs/2505.17065)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in artificial intelligence, particularly large language models LLMs, have shown promising capabilities in transforming rare disease research. This survey paper explores the integration of LLMs in the analysis of rare diseases, highlighting significant strides and pivotal studies that leverage textual data to uncover insights and patterns critical for diagnosis, treatment, and patient care. While current research predominantly employs textual data, the potential for multimodal data integration combining genetic, imaging, and electronic health records stands as a promising frontier. We review foundational papers that demonstrate the application of LLMs in identifying and extracting relevant medical information, simulating intelligent conversational agents for patient interaction, and enabling the formulation of accurate and timely diagnoses. Furthermore, this paper discusses the challenges and ethical considerations inherent in deploying LLMs, including data privacy, model transparency, and the need for robust, inclusive data sets. As part of this exploration, we present a section on experimentation that utilizes multiple LLMs alongside structured questionnaires, specifically designed for diagnostic purposes in the context of different diseases. We conclude with future perspectives on the evolution of LLMs towards truly multimodal platforms, which would integrate diverse data types to provide a more comprehensive understanding of rare diseases, ultimately fostering better outcomes in clinical settings.</li>
</ul>

<h3>Title: Improving LLM Outputs Against Jailbreak Attacks with Expert Model Integration</h3>
<ul>
<li><strong>Authors: </strong>Tatia Tsmindashvili, Ana Kolkhidashvili, Dachi Kurtskhalia, Nino Maghlakelidze, Elene Mekvabishvili, Guram Dentoshvili, Orkhan Shamilov, Zaal Gachechiladze, Steven Saporta, David Dachi Choladze</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17066">https://arxiv.org/abs/2505.17066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17066">https://arxiv.org/pdf/2505.17066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17066]] Improving LLM Outputs Against Jailbreak Attacks with Expert Model Integration(https://arxiv.org/abs/2505.17066)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Using LLMs in a production environment presents security challenges that include vulnerabilities to jailbreaks and prompt injections, which can result in harmful outputs for humans or the enterprise. The challenge is amplified when working within a specific domain, as topics generally accepted for LLMs to address may be irrelevant to that field. These problems can be mitigated, for example, by fine-tuning large language models with domain-specific and security-focused data. However, these alone are insufficient, as jailbreak techniques evolve. Additionally, API-accessed models do not offer the flexibility needed to tailor behavior to industry-specific objectives, and in-context learning is not always sufficient or reliable. In response to these challenges, we introduce Archias, an expert model adept at distinguishing between in-domain and out-of-domain communications. Archias classifies user inquiries into several categories: in-domain (specifically for the automotive industry), malicious questions, price injections, prompt injections, and out-of-domain examples. Our methodology integrates outputs from the expert model (Archias) into prompts, which are then processed by the LLM to generate responses. This method increases the model's ability to understand the user's intention and give appropriate answers. Archias can be adjusted, fine-tuned, and used for many different purposes due to its small size. Therefore, it can be easily customized to the needs of any industry. To validate our approach, we created a benchmark dataset for the automotive industry. Furthermore, in the interest of advancing research and development, we release our benchmark dataset to the community.</li>
</ul>

<h3>Title: What's in a prompt? Language models encode literary style in prompt embeddings</h3>
<ul>
<li><strong>Authors: </strong>Raphaël Sarfati, Haley Moller, Toni J. B. Liu, Nicolas Boullé, Christopher Earls</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17071">https://arxiv.org/abs/2505.17071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17071">https://arxiv.org/pdf/2505.17071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17071]] What's in a prompt? Language models encode literary style in prompt embeddings(https://arxiv.org/abs/2505.17071)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models use high-dimensional latent spaces to encode and process textual information. Much work has investigated how the conceptual content of words translates into geometrical relationships between their vector representations. Fewer studies analyze how the cumulative information of an entire prompt becomes condensed into individual embeddings under the action of transformer layers. We use literary pieces to show that information about intangible, rather than factual, aspects of the prompt are contained in deep representations. We observe that short excerpts (10 - 100 tokens) from different novels separate in the latent space independently from what next-token prediction they converge towards. Ensembles from books from the same authors are much more entangled than across authors, suggesting that embeddings encode stylistic features. This geometry of style may have applications for authorship attribution and literary analysis, but most importantly reveals the sophistication of information processing and compression accomplished by language models.</li>
</ul>

<h3>Title: Safety Alignment Can Be Not Superficial With Explicit Safety Signals</h3>
<ul>
<li><strong>Authors: </strong>Jianwei Li, Jung-Eng Kim</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17072">https://arxiv.org/abs/2505.17072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17072">https://arxiv.org/pdf/2505.17072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17072]] Safety Alignment Can Be Not Superficial With Explicit Safety Signals(https://arxiv.org/abs/2505.17072)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent studies on the safety alignment of large language models (LLMs) have revealed that existing approaches often operate superficially, leaving models vulnerable to various adversarial attacks. Despite their significance, these studies generally fail to offer actionable solutions beyond data augmentation for achieving more robust safety mechanisms. This paper identifies a fundamental cause of this superficiality: existing alignment approaches often presume that models can implicitly learn a safety-related reasoning task during the alignment process, enabling them to refuse harmful requests. However, the learned safety signals are often diluted by other competing objectives, leading models to struggle with drawing a firm safety-conscious decision boundary when confronted with adversarial attacks. Based on this observation, by explicitly introducing a safety-related binary classification task and integrating its signals with our attention and decoding strategies, we eliminate this ambiguity and allow models to respond more responsibly to malicious queries. We emphasize that, with less than 0.2x overhead cost, our approach enables LLMs to assess the safety of both the query and the previously generated tokens at each necessary generating step. Extensive experiments demonstrate that our method significantly improves the resilience of LLMs against various adversarial attacks, offering a promising pathway toward more robust generative AI systems.</li>
</ul>

<h3>Title: Mechanistic Interpretability of GPT-like Models on Summarization Tasks</h3>
<ul>
<li><strong>Authors: </strong>Anurag Mishra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17073">https://arxiv.org/abs/2505.17073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17073">https://arxiv.org/pdf/2505.17073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17073]] Mechanistic Interpretability of GPT-like Models on Summarization Tasks(https://arxiv.org/abs/2505.17073)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative, large language model</a></li>
<li><strong>Abstract: </strong>Mechanistic interpretability research seeks to reveal the inner workings of large language models, yet most work focuses on classification or generative tasks rather than summarization. This paper presents an interpretability framework for analyzing how GPT-like models adapt to summarization tasks. We conduct differential analysis between pre-trained and fine-tuned models, quantifying changes in attention patterns and internal activations. By identifying specific layers and attention heads that undergo significant transformation, we locate the "summarization circuit" within the model architecture. Our findings reveal that middle layers (particularly 2, 3, and 5) exhibit the most dramatic changes, with 62% of attention heads showing decreased entropy, indicating a shift toward focused information selection. We demonstrate that targeted LoRA adaptation of these identified circuits achieves significant performance improvement over standard LoRA fine-tuning while requiring fewer training epochs. This work bridges the gap between black-box evaluation and mechanistic understanding, providing insights into how neural networks perform information selection and compression during summarization.</li>
</ul>

<h3>Title: Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency</h3>
<ul>
<li><strong>Authors: </strong>Ruixiao Li, Fahao Chen, Peng Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17074">https://arxiv.org/abs/2505.17074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17074">https://arxiv.org/pdf/2505.17074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17074]] Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency(https://arxiv.org/abs/2505.17074)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Speculative decoding accelerates Large Language Model (LLM) inference by employing a small speculative model (SSM) to generate multiple candidate tokens and verify them using the LLM in parallel. This technique has been widely integrated into LLM inference serving systems. However, inference requests typically exhibit uncertain execution time, which poses a significant challenge of efficiently scheduling requests in these systems. Existing work estimates execution time based solely on predicted output length, which could be inaccurate because execution time depends on both output length and token acceptance rate of verification by the LLM. In this paper, we propose a semi-clairvoyant request scheduling algorithm called Least-Attained/Perceived-Service for Speculative Decoding (LAPS-SD). Given a number of inference requests, LAPS-SD can effectively minimize average inference latency by adaptively scheduling requests according to their features during decoding. When the token acceptance rate is dynamic and execution time is difficult to estimate, LAPS-SD maintains multiple priority queues and allows request execution preemption across different queues. Once the token acceptance rate becomes stable, LAPS-SD can accurately estimate the execution time and schedule requests accordingly. Extensive experiments show that LAPS-SD reduces inference latency by approximately 39\% compared to state-of-the-art scheduling methods.</li>
</ul>

<h3>Title: Streamlining HTTP Flooding Attack Detection through Incremental Feature Selection</h3>
<ul>
<li><strong>Authors: </strong>Upasana Sarmah, Parthajit Borah, D. K. Bhattacharyya</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17077">https://arxiv.org/abs/2505.17077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17077">https://arxiv.org/pdf/2505.17077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17077]] Streamlining HTTP Flooding Attack Detection through Incremental Feature Selection(https://arxiv.org/abs/2505.17077)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Applications over the Web primarily rely on the HTTP protocol to transmit web pages to and from systems. There are a variety of application layer protocols, but among all, HTTP is the most targeted because of its versatility and ease of integration with online services. The attackers leverage the fact that by default no detection system blocks any HTTP traffic. Thus, by exploiting such characteristics of the protocol, attacks are launched against web applications. HTTP flooding attacks are one such attack in the application layer of the OSI model. In this paper, a method for the detection of such an attack is proposed. The heart of the detection method is an incremental feature subset selection method based on mutual information and correlation. INFS-MICC helps in identifying a subset of highly relevant and independent feature subset so as to detect HTTP Flooding attacks with best possible classification performance in near-real time.</li>
</ul>

<h3>Title: GloSS over Toxicity: Understanding and Mitigating Toxicity in LLMs via Global Toxic Subspace</h3>
<ul>
<li><strong>Authors: </strong>Zenghao Duan, Zhiyi Yin, Zhichao Shi, Liang Pang, Shaoling Jing, Jiayi Wu, Yu Yan, Huawei Shen, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17078">https://arxiv.org/abs/2505.17078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17078">https://arxiv.org/pdf/2505.17078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17078]] GloSS over Toxicity: Understanding and Mitigating Toxicity in LLMs via Global Toxic Subspace(https://arxiv.org/abs/2505.17078)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper investigates the underlying mechanisms of toxicity generation in Large Language Models (LLMs) and proposes an effective detoxification approach. Prior work typically considers the Feed-Forward Network (FFN) as the main source of toxicity, representing toxic regions as a set of toxic vectors or layer-wise subspaces. However, our in-depth analysis reveals that the global toxic subspace offers a more effective and comprehensive representation of toxic region within the model. Building on this insight, we propose GloSS (Global Toxic Subspace Suppression), a lightweight, four-stage method that mitigates toxicity by identifying and removing the global toxic subspace from the parameters of FFN. Experiments across a range of LLMs show that GloSS achieves state-of-the-art detoxification performance while preserving the models general capabilities, without requiring large-scale data or model retraining.</li>
</ul>

<h3>Title: Not Minds, but Signs: Reframing LLMs through Semiotics</h3>
<ul>
<li><strong>Authors: </strong>Davide Picca</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17080">https://arxiv.org/abs/2505.17080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17080">https://arxiv.org/pdf/2505.17080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17080]] Not Minds, but Signs: Reframing LLMs through Semiotics(https://arxiv.org/abs/2505.17080)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper challenges the prevailing tendency to frame Large Language Models (LLMs) as cognitive systems, arguing instead for a semiotic perspective that situates these models within the broader dynamics of sign manipulation and meaning-making. Rather than assuming that LLMs understand language or simulate human thought, we propose that their primary function is to recombine, recontextualize, and circulate linguistic forms based on probabilistic associations. By shifting from a cognitivist to a semiotic framework, we avoid anthropomorphism and gain a more precise understanding of how LLMs participate in cultural processes, not by thinking, but by generating texts that invite interpretation. Through theoretical analysis and practical examples, the paper demonstrates how LLMs function as semiotic agents whose outputs can be treated as interpretive acts, open to contextual negotiation and critical reflection. We explore applications in literature, philosophy, education, and cultural production, emphasizing how LLMs can serve as tools for creativity, dialogue, and critical inquiry. The semiotic paradigm foregrounds the situated, contingent, and socially embedded nature of meaning, offering a more rigorous and ethically aware framework for studying and using LLMs. Ultimately, this approach reframes LLMs as technological participants in an ongoing ecology of signs. They do not possess minds, but they alter how we read, write, and make meaning, compelling us to reconsider the foundations of language, interpretation, and the role of artificial systems in the production of knowledge.</li>
</ul>

<h3>Title: GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data</h3>
<ul>
<li><strong>Authors: </strong>Abderrahman Skiredj, Ferdaous Azhari, Houdaifa Atou, Nouamane Tazi, Ismail Berrada</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17082">https://arxiv.org/abs/2505.17082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17082">https://arxiv.org/pdf/2505.17082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17082]] GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data(https://arxiv.org/abs/2505.17082)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Open-source large language models (LLMs) still marginalise Moroccan Arabic (Darija), forcing practitioners either to bolt on heavyweight Arabic adapters or to sacrifice the very reasoning skills that make LLMs useful. We show that a rigorously quality-over-quantity alignment strategy can surface fluent Darija while safeguarding the backbone s cross-lingual reasoning at a sliver of the usual compute. We translate three compact instruction suites LIMA 1 K, DEITA 6 K and TULU 50 K into Darija, preserve 20 of the English originals, and add mathematics, coding and scientific prompts. A LoRA-tuned Gemma 3-4B trained on 5 K mixed instructions lifts DarijaMMLU from 32.8 to 42.7 ; adding the reasoning-dense TULU portion pushes it to 47.5 with no English regression. Scaling the identical recipe to Gemma 3-27B produces GemMaroc-27B, which matches Atlas-Chat on DarijaMMLU (61.6 ) and leaps ahead on Darija commonsense, scoring 60.5 on HellaSwag versus Atlas-Chat s 48.4 . Crucially, GemMaroc retains Gemma-27B s strong maths and general-reasoning ability, showing only minimal movement on GSM8K and English benchmarks. The entire model is trained in just 48 GPU.h, underscoring a Green AI pathway to inclusive, sustainable language technology. We release code, data and checkpoints to spur Darija-centric applications in education, public services and everyday digital interaction.</li>
</ul>

<h3>Title: From nuclear safety to LLM security: Applying non-probabilistic risk management strategies to build safe and secure LLM-powered systems</h3>
<ul>
<li><strong>Authors: </strong>Alexander Gutfraind, Vicki Bier</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17084">https://arxiv.org/abs/2505.17084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17084">https://arxiv.org/pdf/2505.17084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17084]] From nuclear safety to LLM security: Applying non-probabilistic risk management strategies to build safe and secure LLM-powered systems(https://arxiv.org/abs/2505.17084)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) offer unprecedented and growing capabilities, but also introduce complex safety and security challenges that resist conventional risk management. While conventional probabilistic risk analysis (PRA) requires exhaustive risk enumeration and quantification, the novelty and complexity of these systems make PRA impractical, particularly against adaptive adversaries. Previous research found that risk management in various fields of engineering such as nuclear or civil engineering is often solved by generic (i.e. field-agnostic) strategies such as event tree analysis or robust designs. Here we show how emerging risks in LLM-powered systems could be met with 100+ of these non-probabilistic strategies to risk management, including risks from adaptive adversaries. The strategies are divided into five categories and are mapped to LLM security (and AI safety more broadly). We also present an LLM-powered workflow for applying these strategies and other workflows suitable for solution architects. Overall, these strategies could contribute (despite some limitations) to security, safety and other dimensions of responsible AI.</li>
</ul>

<h3>Title: GSDFuse: Capturing Cognitive Inconsistencies from Multi-Dimensional Weak Signals in Social Media Steganalysis</h3>
<ul>
<li><strong>Authors: </strong>Kaibo Huang, Zipei Zhang, Yukun Wei, TianXin Zhang, Zhongliang Yang, Linna Zhou</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17085">https://arxiv.org/abs/2505.17085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17085">https://arxiv.org/pdf/2505.17085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17085]] GSDFuse: Capturing Cognitive Inconsistencies from Multi-Dimensional Weak Signals in Social Media Steganalysis(https://arxiv.org/abs/2505.17085)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>The ubiquity of social media platforms facilitates malicious linguistic steganography, posing significant security risks. Steganalysis is profoundly hindered by the challenge of identifying subtle cognitive inconsistencies arising from textual fragmentation and complex dialogue structures, and the difficulty in achieving robust aggregation of multi-dimensional weak signals, especially given extreme steganographic sparsity and sophisticated steganography. These core detection difficulties are compounded by significant data imbalance. This paper introduces GSDFuse, a novel method designed to systematically overcome these obstacles. GSDFuse employs a holistic approach, synergistically integrating hierarchical multi-modal feature engineering to capture diverse signals, strategic data augmentation to address sparsity, adaptive evidence fusion to intelligently aggregate weak signals, and discriminative embedding learning to enhance sensitivity to subtle inconsistencies. Experiments on social media datasets demonstrate GSDFuse's state-of-the-art (SOTA) performance in identifying sophisticated steganography within complex dialogue environments. The source code for GSDFuse is available at this https URL.</li>
</ul>

<h3>Title: Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yihong Wu, Liheng Ma, Muzhi Li, Jiaming Zhou, Jianye Hao, Ho-fung Leung, Irwin King, Yingxue Zhang, Jian-Yun Nie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17086">https://arxiv.org/abs/2505.17086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17086">https://arxiv.org/pdf/2505.17086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17086]] Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization(https://arxiv.org/abs/2505.17086)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable versatility, due to the lack of factual knowledge, their application to Question Answering (QA) tasks remains hindered by hallucination. While Retrieval-Augmented Generation mitigates these issues by integrating external knowledge, existing approaches rely heavily on in-context learning, whose performance is constrained by the fundamental reasoning capabilities of LLMs. In this paper, we propose Mujica, a Multi-hop Joint Intelligence for Complex Question Answering, comprising a planner that decomposes questions into a directed acyclic graph of subquestions and a worker that resolves questions via retrieval and reasoning. Additionally, we introduce MyGO (Minimalist policy Gradient Optimization), a novel reinforcement learning method that replaces traditional policy gradient updates with Maximum Likelihood Estimation (MLE) by sampling trajectories from an asymptotically optimal policy. MyGO eliminates the need for gradient rescaling and reference models, ensuring stable and efficient training. Empirical results across multiple datasets demonstrate the effectiveness of Mujica-MyGO in enhancing multi-hop QA performance for various LLMs, offering a scalable and resource-efficient solution for complex QA tasks.</li>
</ul>

<h3>Title: Informatics for Food Processing</h3>
<ul>
<li><strong>Authors: </strong>Gordana Ispirova, Michael Sebek, Giulia Menichetti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.DB, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17087">https://arxiv.org/abs/2505.17087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17087">https://arxiv.org/pdf/2505.17087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17087]] Informatics for Food Processing(https://arxiv.org/abs/2505.17087)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This chapter explores the evolution, classification, and health implications of food processing, while emphasizing the transformative role of machine learning, artificial intelligence (AI), and data science in advancing food informatics. It begins with a historical overview and a critical review of traditional classification frameworks such as NOVA, Nutri-Score, and SIGA, highlighting their strengths and limitations, particularly the subjectivity and reproducibility challenges that hinder epidemiological research and public policy. To address these issues, the chapter presents novel computational approaches, including FoodProX, a random forest model trained on nutrient composition data to infer processing levels and generate a continuous FPro score. It also explores how large language models like BERT and BioBERT can semantically embed food descriptions and ingredient lists for predictive tasks, even in the presence of missing data. A key contribution of the chapter is a novel case study using the Open Food Facts database, showcasing how multimodal AI models can integrate structured and unstructured data to classify foods at scale, offering a new paradigm for food processing assessment in public health and research.</li>
</ul>

<h3>Title: Trust Me, I Can Handle It: Self-Generated Adversarial Scenario Extrapolation for Robust Language Models</h3>
<ul>
<li><strong>Authors: </strong>Md Rafi Ur Rashid, Vishnu Asutosh Dasu, Ye Wang, Gang Tan, Shagufta Mehnaz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17089">https://arxiv.org/abs/2505.17089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17089">https://arxiv.org/pdf/2505.17089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17089]] Trust Me, I Can Handle It: Self-Generated Adversarial Scenario Extrapolation for Robust Language Models(https://arxiv.org/abs/2505.17089)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, defense, attack, robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit impressive capabilities, but remain susceptible to a growing spectrum of safety risks, including jailbreaks, toxic content, hallucinations, and bias. Existing defenses often address only a single threat type or resort to rigid outright rejection, sacrificing user experience and failing to generalize across diverse and novel attacks. This paper introduces Adversarial Scenario Extrapolation (ASE), a novel inference-time computation framework that leverages Chain-of-Thought (CoT) reasoning to simultaneously enhance LLM robustness and seamlessness. ASE guides the LLM through a self-generative process of contemplating potential adversarial scenarios and formulating defensive strategies before generating a response to the user query. Comprehensive evaluation on four adversarial benchmarks with four latest LLMs shows that ASE achieves near-zero jailbreak attack success rates and minimal toxicity, while slashing outright rejections to <4%. ASE outperforms six state-of-the-art defenses in robustness-seamlessness trade-offs, with 92-99% accuracy on adversarial Q&A and 4-10x lower bias scores. By transforming adversarial perception into an intrinsic cognitive process, ASE sets a new paradigm for secure and natural human-AI interaction.</li>
</ul>

<h3>Title: Large Language Models Implicitly Learn to See and Hear Just By Reading</h3>
<ul>
<li><strong>Authors: </strong>Prateek Verma, Mert Pilanci</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17091">https://arxiv.org/abs/2505.17091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17091">https://arxiv.org/pdf/2505.17091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17091]] Large Language Models Implicitly Learn to See and Hear Just By Reading(https://arxiv.org/abs/2505.17091)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a fascinating find: By training an auto-regressive LLM model on text tokens, the text model inherently develops internally an ability to understand images and audio, thereby developing the ability to see and hear just by reading. Popular audio and visual LLM models fine-tune text LLM models to give text output conditioned on images and audio embeddings. On the other hand, our architecture takes in patches of images, audio waveforms or tokens as input. It gives us the embeddings or category labels typical of a classification pipeline. We show the generality of text weights in aiding audio classification for datasets FSD-50K and GTZAN. Further, we show this working for image classification on CIFAR-10 and Fashion-MNIST, as well on image patches. This pushes the notion of text-LLMs learning powerful internal circuits that can be utilized by activating necessary connections for various applications rather than training models from scratch every single time.</li>
</ul>

<h3>Title: Covert Attacks on Machine Learning Training in Passively Secure MPC</h3>
<ul>
<li><strong>Authors: </strong>Matthew Jagielski, Daniel Escudero, Rahul Rachuri, Peter Scholl</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17092">https://arxiv.org/abs/2505.17092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17092">https://arxiv.org/pdf/2505.17092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17092]] Covert Attacks on Machine Learning Training in Passively Secure MPC(https://arxiv.org/abs/2505.17092)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack</a></li>
<li><strong>Abstract: </strong>Secure multiparty computation (MPC) allows data owners to train machine learning models on combined data while keeping the underlying training data private. The MPC threat model either considers an adversary who passively corrupts some parties without affecting their overall behavior, or an adversary who actively modifies the behavior of corrupt parties. It has been argued that in some settings, active security is not a major concern, partly because of the potential risk of reputation loss if a party is detected cheating. In this work we show explicit, simple, and effective attacks that an active adversary can run on existing passively secure MPC training protocols, while keeping essentially zero risk of the attack being detected. The attacks we show can compromise both the integrity and privacy of the model, including attacks reconstructing exact training data. Our results challenge the belief that a threat model that does not include malicious behavior by the involved parties may be reasonable in the context of PPML, motivating the use of actively secure protocols for training.</li>
</ul>

<h3>Title: Neuromorphic Mimicry Attacks Exploiting Brain-Inspired Computing for Covert Cyber Intrusions</h3>
<ul>
<li><strong>Authors: </strong>Hemanth Ravipati</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17094">https://arxiv.org/abs/2505.17094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17094">https://arxiv.org/pdf/2505.17094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17094]] Neuromorphic Mimicry Attacks Exploiting Brain-Inspired Computing for Covert Cyber Intrusions(https://arxiv.org/abs/2505.17094)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack</a></li>
<li><strong>Abstract: </strong>Neuromorphic computing, inspired by the human brain's neural architecture, is revolutionizing artificial intelligence and edge computing with its low-power, adaptive, and event-driven designs. However, these unique characteristics introduce novel cybersecurity risks. This paper proposes Neuromorphic Mimicry Attacks (NMAs), a groundbreaking class of threats that exploit the probabilistic and non-deterministic nature of neuromorphic chips to execute covert intrusions. By mimicking legitimate neural activity through techniques such as synaptic weight tampering and sensory input poisoning, NMAs evade traditional intrusion detection systems, posing risks to applications such as autonomous vehicles, smart medical implants, and IoT networks. This research develops a theoretical framework for NMAs, evaluates their impact using a simulated neuromorphic chip dataset, and proposes countermeasures, including neural-specific anomaly detection and secure synaptic learning protocols. The findings underscore the critical need for tailored cybersecurity measures to protect brain-inspired computing, offering a pioneering exploration of this emerging threat landscape.</li>
</ul>

<h3>Title: Are LLMs reliable? An exploration of the reliability of large language models in clinical note generation</h3>
<ul>
<li><strong>Authors: </strong>Kristine Ann M. Carandang, Jasper Meynard P. Araña, Ethan Robert A. Casin, Christopher P. Monterola, Daniel Stanley Y. Tan, Jesus Felix B. Valenzuela, Christian M. Alis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17095">https://arxiv.org/abs/2505.17095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17095">https://arxiv.org/pdf/2505.17095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17095]] Are LLMs reliable? An exploration of the reliability of large language models in clinical note generation(https://arxiv.org/abs/2505.17095)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>Due to the legal and ethical responsibilities of healthcare providers (HCPs) for accurate documentation and protection of patient data privacy, the natural variability in the responses of large language models (LLMs) presents challenges for incorporating clinical note generation (CNG) systems, driven by LLMs, into real-world clinical processes. The complexity is further amplified by the detailed nature of texts in CNG. To enhance the confidence of HCPs in tools powered by LLMs, this study evaluates the reliability of 12 open-weight and proprietary LLMs from Anthropic, Meta, Mistral, and OpenAI in CNG in terms of their ability to generate notes that are string equivalent (consistency rate), have the same meaning (semantic consistency) and are correct (semantic similarity), across several iterations using the same prompt. The results show that (1) LLMs from all model families are stable, such that their responses are semantically consistent despite being written in various ways, and (2) most of the LLMs generated notes close to the corresponding notes made by experts. Overall, Meta's Llama 70B was the most reliable, followed by Mistral's Small model. With these findings, we recommend the local deployment of these relatively smaller open-weight models for CNG to ensure compliance with data privacy regulations, as well as to improve the efficiency of HCPs in clinical documentation.</li>
</ul>

<h3>Title: TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration</h3>
<ul>
<li><strong>Authors: </strong>Yanshu Li, Tian Yun, Jianjiang Yang, Pinyuan Feng, Jinfa Huang, Ruixiang Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17098">https://arxiv.org/abs/2505.17098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17098">https://arxiv.org/pdf/2505.17098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17098]] TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration(https://arxiv.org/abs/2505.17098)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multimodal in-context learning (ICL) has emerged as a key mechanism for harnessing the capabilities of large vision-language models (LVLMs). However, its effectiveness remains highly sensitive to the quality of input in-context sequences, particularly for tasks involving complex reasoning or open-ended generation. A major limitation is our limited understanding of how LVLMs actually exploit these sequences during inference. To bridge this gap, we systematically interpret multimodal ICL through the lens of task mapping, which reveals how local and global relationships within and among demonstrations guide model reasoning. Building on this insight, we present TACO, a lightweight transformer-based model equipped with task-aware attention that dynamically configures in-context sequences. By injecting task-mapping signals into the autoregressive decoding process, TACO creates a bidirectional synergy between sequence construction and task reasoning. Experiments on five LVLMs and nine datasets demonstrate that TACO consistently surpasses baselines across diverse ICL tasks. These results position task mapping as a valuable perspective for interpreting and improving multimodal ICL.</li>
</ul>

<h3>Title: Learning Interpretable Representations Leads to Semantically Faithful EEG-to-Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaozhao Liu, Dinggang Shen, Xihui Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17099">https://arxiv.org/abs/2505.17099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17099">https://arxiv.org/pdf/2505.17099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17099]] Learning Interpretable Representations Leads to Semantically Faithful EEG-to-Text Generation(https://arxiv.org/abs/2505.17099)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Pretrained generative models have opened new frontiers in brain decoding by enabling the synthesis of realistic texts and images from non-invasive brain recordings. However, the reliability of such outputs remains questionable--whether they truly reflect semantic activation in the brain, or are merely hallucinated by the powerful generative models. In this paper, we focus on EEG-to-text decoding and address its hallucination issue through the lens of posterior collapse. Acknowledging the underlying mismatch in information capacity between EEG and text, we reframe the decoding task as semantic summarization of core meanings rather than previously verbatim reconstruction of stimulus texts. To this end, we propose the Generative Language Inspection Model (GLIM), which emphasizes learning informative and interpretable EEG representations to improve semantic grounding under heterogeneous and small-scale data conditions. Experiments on the public ZuCo dataset demonstrate that GLIM consistently generates fluent, EEG-grounded sentences without teacher forcing. Moreover, it supports more robust evaluation beyond text similarity, through EEG-text retrieval and zero-shot semantic classification across sentiment categories, relation types, and corpus topics. Together, our architecture and evaluation protocols lay the foundation for reliable and scalable benchmarking in generative brain decoding.</li>
</ul>

<h3>Title: Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector</h3>
<ul>
<li><strong>Authors: </strong>Haoyan Yang, Runxue Bao, Cao Xiao, Jun Ma, Parminder Bhatia, Shangqian Gao, Taha Kass-Hout</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17100">https://arxiv.org/abs/2505.17100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17100">https://arxiv.org/pdf/2505.17100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17100]] Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector(https://arxiv.org/abs/2505.17100)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>LLM-as-a-Judge has emerged as a promising tool for automatically evaluating generated outputs, but its reliability is often undermined by potential biases in judgment. Existing efforts to mitigate these biases face key limitations: in-context learning-based methods fail to address rooted biases due to the evaluator's limited capacity for self-reflection, whereas fine-tuning is not applicable to all evaluator types, especially closed-source models. To address this challenge, we introduce the Reasoning-based Bias Detector (RBD), which is a plug-in module that identifies biased evaluations and generates structured reasoning to guide evaluator self-correction. Rather than modifying the evaluator itself, RBD operates externally and engages in an iterative process of bias detection and feedback-driven revision. To support its development, we design a complete pipeline consisting of biased dataset construction, supervision collection, distilled reasoning-based fine-tuning of RBD, and integration with LLM evaluators. We fine-tune four sizes of RBD models, ranging from 1.5B to 14B, and observe consistent performance improvements across all scales. Experimental results on 4 bias types--verbosity, position, bandwagon, and sentiment--evaluated using 8 LLM evaluators demonstrate RBD's strong effectiveness. For example, the RBD-8B model improves evaluation accuracy by an average of 18.5% and consistency by 10.9%, and surpasses prompting-based baselines and fine-tuned judges by 12.8% and 17.2%, respectively. These results highlight RBD's effectiveness and scalability. Additional experiments further demonstrate its strong generalization across biases and domains, as well as its efficiency.</li>
</ul>

<h3>Title: An approach to identify the most semantically informative deep representations of text and images</h3>
<ul>
<li><strong>Authors: </strong>Santiago Acevedo, Andrea Mascaretti, Riccardo Rende, Matéo Mahaut, Marco Baroni, Alessandro Laio</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17101">https://arxiv.org/abs/2505.17101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17101">https://arxiv.org/pdf/2505.17101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17101]] An approach to identify the most semantically informative deep representations of text and images(https://arxiv.org/abs/2505.17101)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Deep neural networks are known to develop similar representations for semantically related data, even when they belong to different domains, such as an image and its description, or the same text in different languages. We present a method for quantitatively investigating this phenomenon by measuring the relative information content of the representations of semantically related data and probing how it is encoded into multiple tokens of large language models (LLMs) and vision transformers. Looking first at how LLMs process pairs of translated sentences, we identify inner ``semantic'' layers containing the most language-transferable information. We find moreover that, on these layers, a larger LLM (DeepSeek-V3) extracts significantly more general information than a smaller one (Llama3.1-8B). Semantic information is spread across many tokens and it is characterized by long-distance correlations between tokens and by a causal left-to-right (i.e., past-future) asymmetry. We also identify layers encoding semantic information within visual transformers. We show that caption representations in the semantic layers of LLMs predict visual representations of the corresponding images. We observe significant and model-dependent information asymmetries between image and text representations.</li>
</ul>

<h3>Title: BanglaByT5: Byte-Level Modelling for Bangla</h3>
<ul>
<li><strong>Authors: </strong>Pramit Bhattacharyya, Arnab Bhattacharya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17102">https://arxiv.org/abs/2505.17102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17102">https://arxiv.org/pdf/2505.17102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17102]] BanglaByT5: Byte-Level Modelling for Bangla(https://arxiv.org/abs/2505.17102)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable success across various natural language processing tasks. However, most LLM models use traditional tokenizers like BPE and SentencePiece, which fail to capture the finer nuances of a morphologically rich language like Bangla (Bengali). In this work, we introduce BanglaByT5, the first byte-level encoder-decoder model explicitly tailored for Bangla. Built upon a small variant of Googles ByT5 architecture, BanglaByT5 is pre-trained on a 14GB curated corpus combining high-quality literary and newspaper articles. Through zeroshot and supervised evaluations across generative and classification tasks, BanglaByT5 demonstrates competitive performance, surpassing several multilingual and larger models. Our findings highlight the efficacy of byte-level modelling for morphologically rich languages and highlight BanglaByT5 potential as a lightweight yet powerful tool for Bangla NLP, particularly in both resource-constrained and scalable environments.</li>
</ul>

<h3>Title: Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Cécile Rousseau, Tobia Boschi, Giandomenico Cornacchia, Dhaval Salwala, Alessandra Pascale, Juan Bernabe Moreno</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17103">https://arxiv.org/abs/2505.17103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17103">https://arxiv.org/pdf/2505.17103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17103]] Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation(https://arxiv.org/abs/2505.17103)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>SDForger is a flexible and efficient framework for generating high-quality multivariate time series using LLMs. Leveraging a compact data representation, SDForger provides synthetic time series generation from a few samples and low-computation fine-tuning of any autoregressive LLM. Specifically, the framework transforms univariate and multivariate signals into tabular embeddings, which are then encoded into text and used to fine-tune the LLM. At inference, new textual embeddings are sampled and decoded into synthetic time series that retain the original data's statistical properties and temporal dynamics. Across a diverse range of datasets, SDForger outperforms existing generative models in many scenarios, both in similarity-based evaluations and downstream forecasting tasks. By enabling textual conditioning in the generation process, SDForger paves the way for multimodal modeling and the streamlined integration of time series with textual information. SDForger source code will be open-sourced soon.</li>
</ul>

<h3>Title: P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Tao Sun, Enhao Pan, Zhengkai Yang, Kaixin Sui, Jiajun Shi, Xianfu Cheng, Tongliang Li, Wenhao Huang, Ge Zhang, Jian Yang, Zhoujun Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17104">https://arxiv.org/abs/2505.17104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17104">https://arxiv.org/pdf/2505.17104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17104]] P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark(https://arxiv.org/abs/2505.17104)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Academic posters are vital for scholarly communication, yet their manual creation is time-consuming. However, automated academic poster generation faces significant challenges in preserving intricate scientific details and achieving effective visual-textual integration. Existing approaches often struggle with semantic richness and structural nuances, and lack standardized benchmarks for evaluating generated academic posters comprehensively. To address these limitations, we introduce P2P, the first flexible, LLM-based multi-agent framework that generates high-quality, HTML-rendered academic posters directly from research papers, demonstrating strong potential for practical applications. P2P employs three specialized agents-for visual element processing, content generation, and final poster assembly-each integrated with dedicated checker modules to enable iterative refinement and ensure output quality. To foster advancements and rigorous evaluation in this domain, we construct and release P2PInstruct, the first large-scale instruction dataset comprising over 30,000 high-quality examples tailored for the academic paper-to-poster generation task. Furthermore, we establish P2PEval, a comprehensive benchmark featuring 121 paper-poster pairs and a dual evaluation methodology (Universal and Fine-Grained) that leverages LLM-as-a-Judge and detailed, human-annotated checklists. Our contributions aim to streamline research dissemination and provide the community with robust tools for developing and evaluating next-generation poster generation systems.</li>
</ul>

<h3>Title: RRTL: Red Teaming Reasoning Large Language Models in Tool Learning</h3>
<ul>
<li><strong>Authors: </strong>Yifei Liu, Yu Cui, Haibin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17106">https://arxiv.org/abs/2505.17106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17106">https://arxiv.org/pdf/2505.17106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17106]] RRTL: Red Teaming Reasoning Large Language Models in Tool Learning(https://arxiv.org/abs/2505.17106)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>While tool learning significantly enhances the capabilities of large language models (LLMs), it also introduces substantial security risks. Prior research has revealed various vulnerabilities in traditional LLMs during tool learning. However, the safety of newly emerging reasoning LLMs (RLLMs), such as DeepSeek-R1, in the context of tool learning remains underexplored. To bridge this gap, we propose RRTL, a red teaming approach specifically designed to evaluate RLLMs in tool learning. It integrates two novel strategies: (1) the identification of deceptive threats, which evaluates the model's behavior in concealing the usage of unsafe tools and their potential risks; and (2) the use of Chain-of-Thought (CoT) prompting to force tool invocation. Our approach also includes a benchmark for traditional LLMs. We conduct a comprehensive evaluation on seven mainstream RLLMs and uncover three key findings: (1) RLLMs generally achieve stronger safety performance than traditional LLMs, yet substantial safety disparities persist across models; (2) RLLMs can pose serious deceptive risks by frequently failing to disclose tool usage and to warn users of potential tool output risks; (3) CoT prompting reveals multi-lingual safety vulnerabilities in RLLMs. Our work provides important insights into enhancing the security of RLLMs in tool learning.</li>
</ul>

<h3>Title: CRAKEN: Cybersecurity LLM Agent with Knowledge-Based Execution</h3>
<ul>
<li><strong>Authors: </strong>Minghao Shao, Haoran Xi, Nanda Rani, Meet Udeshi, Venkata Sai Charan Putrevu, Kimberly Milner, Brendan Dolan-Gavitt, Sandeep Kumar Shukla, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Muhammad Shafique</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17107">https://arxiv.org/abs/2505.17107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17107">https://arxiv.org/pdf/2505.17107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17107]] CRAKEN: Cybersecurity LLM Agent with Knowledge-Based Execution(https://arxiv.org/abs/2505.17107)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) agents can automate cybersecurity tasks and can adapt to the evolving cybersecurity landscape without re-engineering. While LLM agents have demonstrated cybersecurity capabilities on Capture-The-Flag (CTF) competitions, they have two key limitations: accessing latest cybersecurity expertise beyond training data, and integrating new knowledge into complex task planning. Knowledge-based approaches that incorporate technical understanding into the task-solving automation can tackle these limitations. We present CRAKEN, a knowledge-based LLM agent framework that improves cybersecurity capability through three core mechanisms: contextual decomposition of task-critical information, iterative self-reflected knowledge retrieval, and knowledge-hint injection that transforms insights into adaptive attack strategies. Comprehensive evaluations with different configurations show CRAKEN's effectiveness in multi-stage vulnerability detection and exploitation compared to previous approaches. Our extensible architecture establishes new methodologies for embedding new security knowledge into LLM-driven cybersecurity agentic systems. With a knowledge database of CTF writeups, CRAKEN obtained an accuracy of 22% on NYU CTF Bench, outperforming prior works by 3% and achieving state-of-the-art results. On evaluation of MITRE ATT&CK techniques, CRAKEN solves 25-30% more techniques than prior work, demonstrating improved cybersecurity capabilities via knowledge-based execution. We make our framework open source to public this https URL.</li>
</ul>

<h3>Title: Mitigating Cyber Risk in the Age of Open-Weight LLMs: Policy Gaps and Technical Realities</h3>
<ul>
<li><strong>Authors: </strong>Alfonso de Gregorio</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17109">https://arxiv.org/abs/2505.17109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17109">https://arxiv.org/pdf/2505.17109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17109]] Mitigating Cyber Risk in the Age of Open-Weight LLMs: Policy Gaps and Technical Realities(https://arxiv.org/abs/2505.17109)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Open-weight general-purpose AI (GPAI) models offer significant benefits but also introduce substantial cybersecurity risks, as demonstrated by the offensive capabilities of models like DeepSeek-R1 in evaluations such as MITRE's OCCULT. These publicly available models empower a wider range of actors to automate and scale cyberattacks, challenging traditional defence paradigms and regulatory approaches. This paper analyzes the specific threats -- including accelerated malware development and enhanced social engineering -- magnified by open-weight AI release. We critically assess current regulations, notably the EU AI Act and the GPAI Code of Practice, identifying significant gaps stemming from the loss of control inherent in open distribution, which renders many standard security mitigations ineffective. We propose a path forward focusing on evaluating and controlling specific high-risk capabilities rather than entire models, advocating for pragmatic policy interpretations for open-weight systems, promoting defensive AI innovation, and fostering international collaboration on standards and cyber threat intelligence (CTI) sharing to ensure security without unduly stifling open technological progress.</li>
</ul>

<h3>Title: Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling</h3>
<ul>
<li><strong>Authors: </strong>Junlin Li, Guodong DU, Jing Li, Sim Kuan Goh, Wenya Wang, Yequan Wang, Fangming Liu, Ho-Kin Tang, Saleh Alharbi, Daojing He, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17110">https://arxiv.org/abs/2505.17110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17110">https://arxiv.org/pdf/2505.17110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17110]] Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling(https://arxiv.org/abs/2505.17110)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning Large Language Models (LLMs) with multimodal encoders on modality-specific data expands the modalities that LLMs can handle, leading to the formation of Multimodal LLMs (MLLMs). However, this paradigm heavily relies on resource-intensive and inflexible fine-tuning from scratch with new multimodal data. In this paper, we propose MMER (Multi-modality Expansion and Retention), a training-free approach that integrates existing MLLMs for effective multimodal expansion while retaining their original performance. Specifically, MMER reuses MLLMs' multimodal encoders while merging their LLM parameters. By comparing original and merged LLM parameters, MMER generates binary masks to approximately separate LLM parameters for each modality. These decoupled parameters can independently process modality-specific inputs, reducing parameter conflicts and preserving original MLLMs' fidelity. MMER can also mitigate catastrophic forgetting by applying a similar process to MLLMs fine-tuned on new tasks. Extensive experiments show significant improvements over baselines, proving that MMER effectively expands LLMs' multimodal capabilities while retaining 99% of the original performance, and also markedly mitigates catastrophic forgetting.</li>
</ul>

<h3>Title: Cultural Value Alignment in Large Language Models: A Prompt-based Analysis of Schwartz Values in Gemini, ChatGPT, and DeepSeek</h3>
<ul>
<li><strong>Authors: </strong>Robin Segerer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17112">https://arxiv.org/abs/2505.17112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17112">https://arxiv.org/pdf/2505.17112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17112]] Cultural Value Alignment in Large Language Models: A Prompt-based Analysis of Schwartz Values in Gemini, ChatGPT, and DeepSeek(https://arxiv.org/abs/2505.17112)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>This study examines cultural value alignment in large language models (LLMs) by analyzing how Gemini, ChatGPT, and DeepSeek prioritize values from Schwartz's value framework. Using the 40-item Portrait Values Questionnaire, we assessed whether DeepSeek, trained on Chinese-language data, exhibits distinct value preferences compared to Western models. Results of a Bayesian ordinal regression model show that self-transcendence values (e.g., benevolence, universalism) were highly prioritized across all models, reflecting a general LLM tendency to emphasize prosocial values. However, DeepSeek uniquely downplayed self-enhancement values (e.g., power, achievement) compared to ChatGPT and Gemini, aligning with collectivist cultural tendencies. These findings suggest that LLMs reflect culturally situated biases rather than a universal ethical framework. To address value asymmetries in LLMs, we propose multi-perspective reasoning, self-reflective feedback, and dynamic contextualization. This study contributes to discussions on AI fairness, cultural neutrality, and the need for pluralistic AI alignment frameworks that integrate diverse moral perspectives.</li>
</ul>

<h3>Title: RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language</h3>
<ul>
<li><strong>Authors: </strong>Subrata Biswas, Mohammad Nur Hossain Khan, Bashima Islam</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17114">https://arxiv.org/abs/2505.17114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17114">https://arxiv.org/pdf/2505.17114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17114]] RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language(https://arxiv.org/abs/2505.17114)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal question answering (QA) often requires identifying which video, audio, or sensor tokens are relevant to the question. Yet modality disagreements are common: off-camera speech, background noise, or motion outside the field of view often mislead fusion models that weight all streams equally. We present RAVEN, a unified QA architecture whose core is QuART, a query-conditioned cross-modal gating module that assigns scalar relevance scores to each token across modalities, enabling the model to amplify informative signals and suppress distractors before fusion. RAVEN is trained through a three-stage pipeline comprising unimodal pretraining, query-aligned fusion, and disagreement-oriented fine-tuning -- each stage targeting a distinct challenge in multi-modal reasoning: representation quality, cross-modal relevance, and robustness to modality mismatch. To support training and evaluation, we release AVS-QA, a dataset of 300K synchronized Audio--Video-Sensor streams paired with automatically generated question-answer pairs. Experimental results on seven multi-modal QA benchmarks -- including egocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\% and 8.0\% gains in accuracy compared to state-of-the-art multi-modal large language models, respectively. Incorporating sensor data provides an additional 16.4\% boost, and the model remains robust under modality corruption, outperforming SOTA baselines by 50.23\%. Our code and dataset are available at this https URL.</li>
</ul>

<h3>Title: Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language Models to Grid-Structured Geospatial Data</h3>
<ul>
<li><strong>Authors: </strong>Akash Dhruv, Yangxinyu Xie, Jordan Branham, Tanwi Mallick</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17116">https://arxiv.org/abs/2505.17116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17116">https://arxiv.org/pdf/2505.17116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17116]] Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language Models to Grid-Structured Geospatial Data(https://arxiv.org/abs/2505.17116)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a comparative study of large language models (LLMs) in interpreting grid-structured geospatial data. We evaluate the performance of a base model through structured prompting and contrast it with a fine-tuned variant trained on a dataset of user-assistant interactions. Our results highlight the strengths and limitations of zero-shot prompting and demonstrate the benefits of fine-tuning for structured geospatial and temporal reasoning.</li>
</ul>

<h3>Title: From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning</h3>
<ul>
<li><strong>Authors: </strong>Chen Shani, Dan Jurafsky, Yann LeCun, Ravid Shwartz-Ziv</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17117">https://arxiv.org/abs/2505.17117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17117">https://arxiv.org/pdf/2505.17117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17117]] From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning(https://arxiv.org/abs/2505.17117)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Humans organize knowledge into compact categories through semantic compression by mapping diverse instances to abstract representations while preserving meaning (e.g., robin and blue jay are both birds; most birds can fly). These concepts reflect a trade-off between expressive fidelity and representational simplicity. Large Language Models (LLMs) demonstrate remarkable linguistic abilities, yet whether their internal representations strike a human-like trade-off between compression and semantic fidelity is unclear. We introduce a novel information-theoretic framework, drawing from Rate-Distortion Theory and the Information Bottleneck principle, to quantitatively compare these strategies. Analyzing token embeddings from a diverse suite of LLMs against seminal human categorization benchmarks, we uncover key divergences. While LLMs form broad conceptual categories that align with human judgment, they struggle to capture the fine-grained semantic distinctions crucial for human understanding. More fundamentally, LLMs demonstrate a strong bias towards aggressive statistical compression, whereas human conceptual systems appear to prioritize adaptive nuance and contextual richness, even if this results in lower compressional efficiency by our measures. These findings illuminate critical differences between current AI and human cognitive architectures, guiding pathways toward LLMs with more human-aligned conceptual representations.</li>
</ul>

<h3>Title: After Retrieval, Before Generation: Enhancing the Trustworthiness of Large Language Models in RAG</h3>
<ul>
<li><strong>Authors: </strong>Xinbang Dai, Huikang Hu, Yuncheng Hua, Jiaqi Li, Yongrui Chen, Rihui Jin, Nan Hu, Guilin Qi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17118">https://arxiv.org/abs/2505.17118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17118">https://arxiv.org/pdf/2505.17118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17118]] After Retrieval, Before Generation: Enhancing the Trustworthiness of Large Language Models in RAG(https://arxiv.org/abs/2505.17118)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) systems face critical challenges in balancing internal (parametric) and external (retrieved) knowledge, especially when these sources conflict or are unreliable. To analyze these scenarios comprehensively, we construct the Trustworthiness Response Dataset (TRD) with 36,266 questions spanning four RAG settings. We reveal that existing approaches address isolated scenarios-prioritizing one knowledge source, naively merging both, or refusing answers-but lack a unified framework to handle different real-world conditions simultaneously. Therefore, we propose the BRIDGE framework, which dynamically determines a comprehensive response strategy of large language models (LLMs). BRIDGE leverages an adaptive weighting mechanism named soft bias to guide knowledge collection, followed by a Maximum Soft-bias Decision Tree to evaluate knowledge and select optimal response strategies (trust internal/external knowledge, or refuse). Experiments show BRIDGE outperforms baselines by 5-15% in accuracy while maintaining balanced performance across all scenarios. Our work provides an effective solution for LLMs' trustworthy responses in real-world RAG applications.</li>
</ul>

<h3>Title: Systematic Evaluation of Machine-Generated Reasoning and PHQ-9 Labeling for Depression Detection Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zongru Shao, Xin Wang, Zhanyang Liu, Chenhan Wang, K.P. Subbalakshmi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17119">https://arxiv.org/abs/2505.17119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17119">https://arxiv.org/pdf/2505.17119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17119]] Systematic Evaluation of Machine-Generated Reasoning and PHQ-9 Labeling for Depression Detection Using Large Language Models(https://arxiv.org/abs/2505.17119)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent research leverages large language models (LLMs) for early mental health detection, such as depression, often optimized with machine-generated data. However, their detection may be subject to unknown weaknesses. Meanwhile, quality control has not been applied to these generated corpora besides limited human verifications. Our goal is to systematically evaluate LLM reasoning and reveal potential weaknesses. To this end, we first provide a systematic evaluation of the reasoning over machine-generated detection and interpretation. Then we use the models' reasoning abilities to explore mitigation strategies for enhanced performance. Specifically, we do the following: A. Design an LLM instruction strategy that allows for systematic analysis of the detection by breaking down the task into several subtasks. B. Design contrastive few-shot and chain-of-thought prompts by selecting typical positive and negative examples of detection reasoning. C. Perform human annotation for the subtasks identified in the first step and evaluate the performance. D. Identify human-preferred detection with desired logical reasoning from the few-shot generation and use them to explore different optimization strategies. We conducted extensive comparisons on the DepTweet dataset across the following subtasks: 1. identifying whether the speaker is describing their own depression; 2. accurately detecting the presence of PHQ-9 symptoms, and 3. finally, detecting depression. Human verification of statistical outliers shows that LLMs demonstrate greater accuracy in analyzing and detecting explicit language of depression as opposed to implicit expressions of depression. Two optimization methods are used for performance enhancement and reduction of the statistic bias: supervised fine-tuning (SFT) and direct preference optimization (DPO). Notably, the DPO approach achieves significant performance improvement.</li>
</ul>

<h3>Title: Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions, and Improve with Training</h3>
<ul>
<li><strong>Authors: </strong>Dillon Plunkett, Adam Morris, Keerthi Reddy, Jorge Morales</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17120">https://arxiv.org/abs/2505.17120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17120">https://arxiv.org/pdf/2505.17120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17120]] Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions, and Improve with Training(https://arxiv.org/abs/2505.17120)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>We have only limited understanding of how and why large language models (LLMs) respond in the ways that they do. Their neural networks have proven challenging to interpret, and we are only beginning to tease out the function of individual neurons and circuits within them. However, another path to understanding these systems is to investigate and develop their capacity to introspect and explain their own functioning. Here, we show that i) contemporary LLMs are capable of providing accurate, quantitative descriptions of their own internal processes during certain kinds of decision-making, ii) that it is possible to improve these capabilities through training, and iii) that this training generalizes to at least some degree. To do so, we fine-tuned GPT-4o and GPT-4o-mini to make decisions in a wide variety of complex contexts (e.g., choosing between condos, loans, vacations, etc.) according to randomly-generated, quantitative preferences about how to weigh different attributes during decision-making (e.g., the relative importance of natural light versus quiet surroundings for condos). We demonstrate that the LLMs can accurately report these preferences (i.e., the weights that they learned to give to different attributes during decision-making). Next, we demonstrate that these LLMs can be fine-tuned to explain their decision-making even more accurately. Finally, we demonstrate that this training generalizes: It improves the ability of the models to accurately explain what they are doing as they make other complex decisions, not just decisions they have learned to make via fine-tuning. This work is a step towards training LLMs to accurately and broadly report on their own internal processes -- a possibility that would yield substantial benefits for interpretability, control, and safety.</li>
</ul>

<h3>Title: NeSyGeo: A Neuro-Symbolic Framework for Multimodal Geometric Reasoning Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Weiming Wu, Zi-kang Wang, Jin Ye, Zhi Zhou, Yu-Feng Li, Lan-Zhe Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17121">https://arxiv.org/abs/2505.17121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17121">https://arxiv.org/pdf/2505.17121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17121]] NeSyGeo: A Neuro-Symbolic Framework for Multimodal Geometric Reasoning Data Generation(https://arxiv.org/abs/2505.17121)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Obtaining large-scale, high-quality data with reasoning paths is crucial for improving the geometric reasoning capabilities of multi-modal large language models (MLLMs). However, existing data generation methods, whether based on predefined templates or constrained symbolic provers, inevitably face diversity and numerical generalization limitations. To address these limitations, we propose NeSyGeo, a novel neuro-symbolic framework for generating geometric reasoning data. First, we propose a domain-specific language grounded in the entity-relation-constraint paradigm to comprehensively represent all components of plane geometry, along with generative actions defined within this symbolic space. We then design a symbolic-visual-text pipeline that synthesizes symbolic sequences, maps them to corresponding visual and textual representations, and generates diverse question-answer (Q&A) pairs using large language models (LLMs). To the best of our knowledge, we are the first to propose a neuro-symbolic approach in generating multimodal reasoning data. Based on this framework, we construct NeSyGeo-CoT and NeSyGeo-Caption datasets, containing 100k samples, and release a new benchmark NeSyGeo-Test for evaluating geometric reasoning abilities in MLLMs. Experiments demonstrate that the proposal significantly and consistently improves the performance of multiple MLLMs under both reinforcement and supervised fine-tuning. With only 4k samples and two epochs of reinforcement fine-tuning, base models achieve improvements of up to +15.8% on MathVision, +8.4% on MathVerse, and +7.3% on GeoQA. Notably, a 4B model can be improved to outperform an 8B model from the same series on geometric reasoning tasks.</li>
</ul>

<h3>Title: Shallow Preference Signals: Large Language Model Aligns Even Better with Truncated Data?</h3>
<ul>
<li><strong>Authors: </strong>Xuan Qi, Jiahao Qiu, Xinzhe Juan, Yue Wu, Mengdi Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17122">https://arxiv.org/abs/2505.17122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17122">https://arxiv.org/pdf/2505.17122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17122]] Shallow Preference Signals: Large Language Model Aligns Even Better with Truncated Data?(https://arxiv.org/abs/2505.17122)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Aligning large language models (LLMs) with human preferences remains a key challenge in AI. Preference-based optimization methods, such as Reinforcement Learning with Human Feedback (RLHF) and Direct Preference Optimization (DPO), rely on human-annotated datasets to improve alignment. In this work, we identify a crucial property of the existing learning method: the distinguishing signal obtained in preferred responses is often concentrated in the early tokens. We refer to this as shallow preference signals. To explore this property, we systematically truncate preference datasets at various points and train both reward models and DPO models on the truncated data. Surprisingly, models trained on truncated datasets, retaining only the first half or fewer tokens, achieve comparable or even superior performance to those trained on full datasets. For example, a reward model trained on the Skywork-Reward-Preference-80K-v0.2 dataset outperforms the full dataset when trained on a 40\% truncated dataset. This pattern is consistent across multiple datasets, suggesting the widespread presence of shallow preference signals. We further investigate the distribution of the reward signal through decoding strategies. We consider two simple decoding strategies motivated by the shallow reward signal observation, namely Length Control Decoding and KL Threshold Control Decoding, which leverage shallow preference signals to optimize the trade-off between alignment and computational efficiency. The performance is even better, which again validates our hypothesis. The phenomenon of shallow preference signals highlights potential issues in LLM alignment: existing alignment methods often focus on aligning only the initial tokens of responses, rather than considering the full response. This could lead to discrepancies with real-world human preferences, resulting in suboptimal alignment performance.</li>
</ul>

<h3>Title: MTR-Bench: A Comprehensive Benchmark for Multi-Turn Reasoning Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyuan Li, Keqin Bao, Yubo Ma, Moxin Li, Wenjie Wang, Rui Men, Yichang Zhang, Fuli Feng, Dayiheng Liu, Junyang Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17123">https://arxiv.org/abs/2505.17123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17123">https://arxiv.org/pdf/2505.17123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17123]] MTR-Bench: A Comprehensive Benchmark for Multi-Turn Reasoning Evaluation(https://arxiv.org/abs/2505.17123)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) have shown promising results in complex reasoning tasks. However, current evaluations predominantly focus on single-turn reasoning scenarios, leaving interactive tasks largely unexplored. We attribute it to the absence of comprehensive datasets and scalable automatic evaluation protocols. To fill these gaps, we present MTR-Bench for LLMs' Multi-Turn Reasoning evaluation. Comprising 4 classes, 40 tasks, and 3600 instances, MTR-Bench covers diverse reasoning capabilities, fine-grained difficulty granularity, and necessitates multi-turn interactions with the environments. Moreover, MTR-Bench features fully-automated framework spanning both dataset constructions and model evaluations, which enables scalable assessment without human interventions. Extensive experiments reveal that even the cutting-edge reasoning models fall short of multi-turn, interactive reasoning tasks. And the further analysis upon these results brings valuable insights for future research in interactive AI systems.</li>
</ul>

<h3>Title: Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language Models through Visual Counterfacts</h3>
<ul>
<li><strong>Authors: </strong>Michal Golovanevsky, William Rudman, Michael Lepori, Amir Bar, Ritambhara Singh, Carsten Eickhoff</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17127">https://arxiv.org/abs/2505.17127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17127">https://arxiv.org/pdf/2505.17127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17127]] Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language Models through Visual Counterfacts(https://arxiv.org/abs/2505.17127)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) perform well on tasks such as visual question answering, but it remains unclear whether their reasoning relies more on memorized world knowledge or on the visual information present in the input image. To investigate this, we introduce Visual CounterFact, a new dataset of visually-realistic counterfactuals that put world knowledge priors (e.g, red strawberry) into direct conflict with visual input (e.g, blue strawberry). Using Visual CounterFact, we show that model predictions initially reflect memorized priors, but shift toward visual evidence in mid-to-late layers. This dynamic reveals a competition between the two modalities, with visual input ultimately overriding priors during evaluation. To control this behavior, we propose Pixels Versus Priors (PvP) steering vectors, a mechanism for controlling model outputs toward either world knowledge or visual input through activation-level interventions. On average, PvP successfully shifts 92.5% of color and 74.6% of size predictions from priors to counterfactuals. Together, these findings offer new tools for interpreting and controlling factual behavior in multimodal models.</li>
</ul>

<h3>Title: Relative Bias: A Comparative Framework for Quantifying Bias in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Alireza Arbabi, Florian Kerschbaum</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17131">https://arxiv.org/abs/2505.17131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17131">https://arxiv.org/pdf/2505.17131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17131]] Relative Bias: A Comparative Framework for Quantifying Bias in LLMs(https://arxiv.org/abs/2505.17131)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The growing deployment of large language models (LLMs) has amplified concerns regarding their inherent biases, raising critical questions about their fairness, safety, and societal impact. However, quantifying LLM bias remains a fundamental challenge, complicated by the ambiguity of what "bias" entails. This challenge grows as new models emerge rapidly and gain widespread use, while introducing potential biases that have not been systematically assessed. In this paper, we propose the Relative Bias framework, a method designed to assess how an LLM's behavior deviates from other LLMs within a specified target domain. We introduce two complementary methodologies: (1) Embedding Transformation analysis, which captures relative bias patterns through sentence representations over the embedding space, and (2) LLM-as-a-Judge, which employs a language model to evaluate outputs comparatively. Applying our framework to several case studies on bias and alignment scenarios following by statistical tests for validation, we find strong alignment between the two scoring methods, offering a systematic, scalable, and statistically grounded approach for comparative bias analysis in LLMs.</li>
</ul>

<h3>Title: Robustifying Vision-Language Models via Dynamic Token Reweighting</h3>
<ul>
<li><strong>Authors: </strong>Tanqiu Jiang, Jiacheng Liang, Rongyi Zhu, Jiawei Zhou, Fenglong Ma, Ting Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17132">https://arxiv.org/abs/2505.17132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17132">https://arxiv.org/pdf/2505.17132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17132]] Robustifying Vision-Language Models via Dynamic Token Reweighting(https://arxiv.org/abs/2505.17132)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Large vision-language models (VLMs) are highly vulnerable to jailbreak attacks that exploit visual-textual interactions to bypass safety guardrails. In this paper, we present DTR, a novel inference-time defense that mitigates multimodal jailbreak attacks through optimizing the model's key-value (KV) caches. Rather than relying on curated safety-specific data or costly image-to-text conversion, we introduce a new formulation of the safety-relevant distributional shift induced by the visual modality. This formulation enables DTR to dynamically adjust visual token weights, minimizing the impact of adversarial visual inputs while preserving the model's general capabilities and inference efficiency. Extensive evaluation across diverse VLMs and attack benchmarks demonstrates that \sys outperforms existing defenses in both attack robustness and benign task performance, marking the first successful application of KV cache optimization for safety enhancement in multimodal foundation models. The code for replicating DTR is available: this https URL (warning: this paper contains potentially harmful content generated by VLMs.)</li>
</ul>

<h3>Title: LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions</h3>
<ul>
<li><strong>Authors: </strong>Chaochen Gao, Xing Wu, Zijia Lin, Debing Zhang, Songlin Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17134">https://arxiv.org/abs/2505.17134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17134">https://arxiv.org/pdf/2505.17134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17134]] LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions(https://arxiv.org/abs/2505.17134)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>High-quality long-context instruction data is essential for aligning long-context large language models (LLMs). Despite the public release of models like Qwen and Llama, their long-context instruction data remains proprietary. Human annotation is costly and challenging, while template-based synthesis methods limit scale, diversity, and quality. We introduce LongMagpie, a self-synthesis framework that automatically generates large-scale long-context instruction data. Our key insight is that aligned long-context LLMs, when presented with a document followed by special tokens preceding a user turn, auto-regressively generate contextually relevant queries. By harvesting these document-query pairs and the model's responses, LongMagpie produces high-quality instructions without human effort. Experiments on HELMET, RULER, and Longbench v2 demonstrate that LongMagpie achieves leading performance on long-context tasks while maintaining competitive performance on short-context tasks, establishing it as a simple and effective approach for open, diverse, and scalable long-context instruction data synthesis.</li>
</ul>

<h3>Title: When can isotropy help adapt LLMs' next word prediction to numerical domains?</h3>
<ul>
<li><strong>Authors: </strong>Rashed Shelim, Shengzhe Xu, Walid Saad, Naren Ramakrishnan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17135">https://arxiv.org/abs/2505.17135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17135">https://arxiv.org/pdf/2505.17135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17135]] When can isotropy help adapt LLMs' next word prediction to numerical domains?(https://arxiv.org/abs/2505.17135)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent studies have shown that vector representations of contextual embeddings learned by pre-trained large language models (LLMs) are effective in various downstream tasks in numerical domains. Despite their significant benefits, the tendency of LLMs to hallucinate in such domains can have severe consequences in applications such as energy, nature, finance, healthcare, retail and transportation, among others. To guarantee prediction reliability and accuracy in numerical domains, it is necessary to open the black-box and provide performance guarantees through explanation. However, there is little theoretical understanding of when pre-trained language models help solve numeric downstream tasks. This paper seeks to bridge this gap by understanding when the next-word prediction capability of LLMs can be adapted to numerical domains through a novel analysis based on the concept of isotropy in the contextual embedding space. Specifically, we consider a log-linear model for LLMs in which numeric data can be predicted from its context through a network with softmax in the output layer of LLMs (i.e., language model head in self-attention). We demonstrate that, in order to achieve state-of-the-art performance in numerical domains, the hidden representations of the LLM embeddings must possess a structure that accounts for the shift-invariance of the softmax function. By formulating a gradient structure of self-attention in pre-trained models, we show how the isotropic property of LLM embeddings in contextual embedding space preserves the underlying structure of representations, thereby resolving the shift-invariance problem and providing a performance guarantee. Experiments show that different characteristics of numeric data and model architecture could have different impacts on isotropy.</li>
</ul>

<h3>Title: Foundation Models for Geospatial Reasoning: Assessing Capabilities of Large Language Models in Understanding Geometries and Topological Spatial Relations</h3>
<ul>
<li><strong>Authors: </strong>Yuhan Ji, Song Gao, Ying Nie, Ivan Majić, Krzysztof Janowicz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17136">https://arxiv.org/abs/2505.17136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17136">https://arxiv.org/pdf/2505.17136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17136]] Foundation Models for Geospatial Reasoning: Assessing Capabilities of Large Language Models in Understanding Geometries and Topological Spatial Relations(https://arxiv.org/abs/2505.17136)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Applying AI foundation models directly to geospatial datasets remains challenging due to their limited ability to represent and reason with geographical entities, specifically vector-based geometries and natural language descriptions of complex spatial relations. To address these issues, we investigate the extent to which a well-known-text (WKT) representation of geometries and their spatial relations (e.g., topological predicates) are preserved during spatial reasoning when the geospatial vector data are passed to large language models (LLMs) including GPT-3.5-turbo, GPT-4, and DeepSeek-R1-14B. Our workflow employs three distinct approaches to complete the spatial reasoning tasks for comparison, i.e., geometry embedding-based, prompt engineering-based, and everyday language-based evaluation. Our experiment results demonstrate that both the embedding-based and prompt engineering-based approaches to geospatial question-answering tasks with GPT models can achieve an accuracy of over 0.6 on average for the identification of topological spatial relations between two geometries. Among the evaluated models, GPT-4 with few-shot prompting achieved the highest performance with over 0.66 accuracy on topological spatial relation inference. Additionally, GPT-based reasoner is capable of properly comprehending inverse topological spatial relations and including an LLM-generated geometry can enhance the effectiveness for geographic entity retrieval. GPT-4 also exhibits the ability to translate certain vernacular descriptions about places into formal topological relations, and adding the geometry-type or place-type context in prompts may improve inference accuracy, but it varies by instance. The performance of these spatial reasoning tasks offers valuable insights for the refinement of LLMs with geographical knowledge towards the development of geo-foundation models capable of geospatial reasoning.</li>
</ul>

<h3>Title: Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands</h3>
<ul>
<li><strong>Authors: </strong>Kristin Qi, Youxiang Zhu, Caroline Summerour, John A. Batsis, Xiaohui Liang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17137">https://arxiv.org/abs/2505.17137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17137">https://arxiv.org/pdf/2505.17137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17137]] Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands(https://arxiv.org/abs/2505.17137)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, generative</a></li>
<li><strong>Abstract: </strong>Early detection of cognitive decline is crucial for enabling interventions that can slow neurodegenerative disease progression. Traditional diagnostic approaches rely on labor-intensive clinical assessments, which are impractical for frequent monitoring. Our pilot study investigates voice assistant systems (VAS) as non-invasive tools for detecting cognitive decline through longitudinal analysis of speech patterns in voice commands. Over an 18-month period, we collected voice commands from 35 older adults, with 15 participants providing daily at-home VAS interactions. To address the challenges of analyzing these short, unstructured and noisy commands, we propose Cog-TiPRO, a framework that combines (1) LLM-driven iterative prompt refinement for linguistic feature extraction, (2) HuBERT-based acoustic feature extraction, and (3) transformer-based temporal modeling. Using iTransformer, our approach achieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming its baseline by 27.13%. Through our LLM approach, we identify linguistic features that uniquely characterize everyday command usage patterns in individuals experiencing cognitive decline.</li>
</ul>

<h3>Title: RAP: Runtime-Adaptive Pruning for LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Huanrong Liu, Chunlin Tian, Xuyang Wei, Jiaheng Dai, Qin Liu, Tianqi Wei, Qingbiao Li, Li Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17138">https://arxiv.org/abs/2505.17138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17138">https://arxiv.org/pdf/2505.17138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17138]] RAP: Runtime-Adaptive Pruning for LLM Inference(https://arxiv.org/abs/2505.17138)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel at language understanding and generation, but their enormous computational and memory requirements hinder deployment. Compression offers a potential solution to mitigate these constraints. However, most existing methods rely on fixed heuristics and thus fail to adapt to runtime memory variations or heterogeneous KV-cache demands arising from diverse user requests. To address these limitations, we propose RAP, an elastic pruning framework driven by reinforcement learning (RL) that dynamically adjusts compression strategies in a runtime-aware manner. Specifically, RAP dynamically tracks the evolving ratio between model parameters and KV-cache across practical execution. Recognizing that FFNs house most parameters, whereas parameter -light attention layers dominate KV-cache formation, the RL agent retains only those components that maximize utility within the current memory budget, conditioned on instantaneous workload and device state. Extensive experiments results demonstrate that RAP outperforms state-of-the-art baselines, marking the first time to jointly consider model weights and KV-cache on the fly.</li>
</ul>

<h3>Title: EarthSE: A Benchmark Evaluating Earth Scientific Exploration Capability for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wanghan Xu, Xiangyu Zhao, Yuhao Zhou, Xiaoyu Yue, Ben Fei, Fenghua Ling, Wenlong Zhang, Lei Bai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17139">https://arxiv.org/abs/2505.17139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17139">https://arxiv.org/pdf/2505.17139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17139]] EarthSE: A Benchmark Evaluating Earth Scientific Exploration Capability for Large Language Models(https://arxiv.org/abs/2505.17139)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Advancements in Large Language Models (LLMs) drive interest in scientific applications, necessitating specialized benchmarks such as Earth science. Existing benchmarks either present a general science focus devoid of Earth science specificity or cover isolated subdomains, lacking holistic evaluation. Furthermore, current benchmarks typically neglect the assessment of LLMs' capabilities in open-ended scientific exploration. In this paper, we present a comprehensive and professional benchmark for the Earth sciences, designed to evaluate the capabilities of LLMs in scientific exploration within this domain, spanning from fundamental to advanced levels. Leveraging a corpus of 100,000 research papers, we first construct two Question Answering (QA) datasets: Earth-Iron, which offers extensive question coverage for broad assessment, and Earth-Silver, which features a higher level of difficulty to evaluate professional depth. These datasets encompass five Earth spheres, 114 disciplines, and 11 task categories, assessing foundational knowledge crucial for scientific exploration. Most notably, we introduce Earth-Gold with new metrics, a dataset comprising open-ended multi-turn dialogues specifically designed to evaluate the advanced capabilities of LLMs in scientific exploration, including methodology induction, limitation analysis, and concept proposal. Extensive experiments reveal limitations in 11 leading LLMs across different domains and tasks, highlighting considerable room for improvement in their scientific exploration capabilities. The benchmark is available on this https URL .</li>
</ul>

<h3>Title: Data Doping or True Intelligence? Evaluating the Transferability of Injected Knowledge in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Essa Jan, Moiz Ali, Muhammad Saram Hassan, Fareed Zaffar, Yasir Zaki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17140">https://arxiv.org/abs/2505.17140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17140">https://arxiv.org/pdf/2505.17140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17140]] Data Doping or True Intelligence? Evaluating the Transferability of Injected Knowledge in LLMs(https://arxiv.org/abs/2505.17140)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As the knowledge of large language models (LLMs) becomes outdated over time, there is a growing need for efficient methods to update them, especially when injecting proprietary information. Our study reveals that comprehension-intensive fine-tuning tasks (e.g., question answering and blanks) achieve substantially higher knowledge retention rates (48%) compared to mapping-oriented tasks like translation (17%) or text-to-JSON conversion (20%), despite exposure to identical factual content. We demonstrate that this pattern persists across model architectures and follows scaling laws, with larger models showing improved retention across all task types. However, all models exhibit significant performance drops when applying injected knowledge in broader contexts, suggesting limited semantic integration. These findings show the importance of task selection in updating LLM knowledge, showing that effective knowledge injection relies not just on data exposure but on the depth of cognitive engagement during fine-tuning.</li>
</ul>

<h3>Title: LLM Access Shield: Domain-Specific LLM Framework for Privacy Policy Compliance</h3>
<ul>
<li><strong>Authors: </strong>Yu Wang, Cailing Cai, Zhihua Xiao, Peifung E. Lam</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17145">https://arxiv.org/abs/2505.17145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17145">https://arxiv.org/pdf/2505.17145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17145]] LLM Access Shield: Domain-Specific LLM Framework for Privacy Policy Compliance(https://arxiv.org/abs/2505.17145)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly applied in fields such as finance, education, and governance due to their ability to generate human-like text and adapt to specialized tasks. However, their widespread adoption raises critical concerns about data privacy and security, including the risk of sensitive data exposure. In this paper, we propose a security framework to enforce policy compliance and mitigate risks in LLM interactions. Our approach introduces three key innovations: (i) LLM-based policy enforcement: a customizable mechanism that enhances domain-specific detection of sensitive data. (ii) Dynamic policy customization: real-time policy adaptation and enforcement during user-LLM interactions to ensure compliance with evolving security requirements. (iii) Sensitive data anonymization: a format-preserving encryption technique that protects sensitive information while maintaining contextual integrity. Experimental results demonstrate that our framework effectively mitigates security risks while preserving the functional accuracy of LLM-driven tasks.</li>
</ul>

<h3>Title: MTSA: Multi-turn Safety Alignment for LLMs through Multi-round Red-teaming</h3>
<ul>
<li><strong>Authors: </strong>Weiyang Guo, Jing Li, Wenya Wang, YU LI, Daojing He, Jun Yu, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17147">https://arxiv.org/abs/2505.17147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17147">https://arxiv.org/pdf/2505.17147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17147]] MTSA: Multi-turn Safety Alignment for LLMs through Multi-round Red-teaming(https://arxiv.org/abs/2505.17147)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of jailbreak attacks against large language models (LLMs) highlights the need for robust security measures. However, in multi-round dialogues, malicious intentions may be hidden in interactions, leading LLMs to be more prone to produce harmful responses. In this paper, we propose the \textbf{M}ulti-\textbf{T}urn \textbf{S}afety \textbf{A}lignment (\ourapproach) framework, to address the challenge of securing LLMs in multi-round interactions. It consists of two stages: In the thought-guided attack learning stage, the red-team model learns about thought-guided multi-round jailbreak attacks to generate adversarial prompts. In the adversarial iterative optimization stage, the red-team model and the target model continuously improve their respective capabilities in interaction. Furthermore, we introduce a multi-turn reinforcement learning algorithm based on future rewards to enhance the robustness of safety alignment. Experimental results show that the red-team model exhibits state-of-the-art attack capabilities, while the target model significantly improves its performance on safety benchmarks.</li>
</ul>

<h3>Title: Large Language Models for Predictive Analysis: How Far Are They?</h3>
<ul>
<li><strong>Authors: </strong>Qin Chen, Yuanyi Ren, Xiaojun Ma, Yuyang Shi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17149">https://arxiv.org/abs/2505.17149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17149">https://arxiv.org/pdf/2505.17149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17149]] Large Language Models for Predictive Analysis: How Far Are They?(https://arxiv.org/abs/2505.17149)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Predictive analysis is a cornerstone of modern decision-making, with applications in various domains. Large Language Models (LLMs) have emerged as powerful tools in enabling nuanced, knowledge-intensive conversations, thus aiding in complex decision-making tasks. With the burgeoning expectation to harness LLMs for predictive analysis, there is an urgent need to systematically assess their capability in this domain. However, there is a lack of relevant evaluations in existing studies. To bridge this gap, we introduce the \textbf{PredictiQ} benchmark, which integrates 1130 sophisticated predictive analysis queries originating from 44 real-world datasets of 8 diverse fields. We design an evaluation protocol considering text analysis, code generation, and their alignment. Twelve renowned LLMs are evaluated, offering insights into their practical use in predictive analysis. Generally, we believe that existing LLMs still face considerable challenges in conducting predictive analysis. See \href{this https URL}{Github}.</li>
</ul>

<h3>Title: Bayesian Optimization for Enhanced Language Models: Optimizing Acquisition Functions</h3>
<ul>
<li><strong>Authors: </strong>Zishuo Bao, Yibo Liu, Changyutao Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17151">https://arxiv.org/abs/2505.17151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17151">https://arxiv.org/pdf/2505.17151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17151]] Bayesian Optimization for Enhanced Language Models: Optimizing Acquisition Functions(https://arxiv.org/abs/2505.17151)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rise of different language model architecture, fine-tuning is becoming even more important for down stream tasks Model gets messy, finding proper hyperparameters for fine-tuning. Although BO has been tried for hyperparameter tuning, most of the existing methods are oblivious to the fact that BO relies on careful choices of acquisition functions, which are essential components of BO that guide how much to explore versus exploit during the optimization process; Different acquisition functions have different levels of sensitivity towards training loss and validation performance; existing methods often just apply an acquisition function no matter if the training and validation performance are sensitive to the acquisition function or not. This work introduces{Bilevel - BO - SWA}, a model fusion approach coupled with a bilevel BO strategy to improve the fine - tunning of large language models. Our work on mixture of acquisition functions like EI and UCB into nested opt loops, where inner loop perform minimization of training loss while outer loops optimized w.r.t. val metric. Experiments on GLUE tasks using RoBERTA - base show that when using EI and UCB, there is an improvement in generalization, and fine - tuning can be improved by up to 2.7%.</li>
</ul>

<h3>Title: PersonaBOT: Bringing Customer Personas to Life with LLMs and RAG</h3>
<ul>
<li><strong>Authors: </strong>Muhammed Rizwan, Lars Carlsson, Mohammad Loni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17156">https://arxiv.org/abs/2505.17156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17156">https://arxiv.org/pdf/2505.17156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17156]] PersonaBOT: Bringing Customer Personas to Life with LLMs and RAG(https://arxiv.org/abs/2505.17156)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The introduction of Large Language Models (LLMs) has significantly transformed Natural Language Processing (NLP) applications by enabling more advanced analysis of customer personas. At Volvo Construction Equipment (VCE), customer personas have traditionally been developed through qualitative methods, which are time-consuming and lack scalability. The main objective of this paper is to generate synthetic customer personas and integrate them into a Retrieval-Augmented Generation (RAG) chatbot to support decision-making in business processes. To this end, we first focus on developing a persona-based RAG chatbot integrated with verified personas. Next, synthetic personas are generated using Few-Shot and Chain-of-Thought (CoT) prompting techniques and evaluated based on completeness, relevance, and consistency using McNemar's test. In the final step, the chatbot's knowledge base is augmented with synthetic personas and additional segment information to assess improvements in response accuracy and practical utility. Key findings indicate that Few-Shot prompting outperformed CoT in generating more complete personas, while CoT demonstrated greater efficiency in terms of response time and token usage. After augmenting the knowledge base, the average accuracy rating of the chatbot increased from 5.88 to 6.42 on a 10-point scale, and 81.82% of participants found the updated system useful in business contexts.</li>
</ul>

<h3>Title: Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting</h3>
<ul>
<li><strong>Authors: </strong>Bang Trinh Tran To, Thai Le</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17160">https://arxiv.org/abs/2505.17160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17160">https://arxiv.org/pdf/2505.17160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17160]] Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting(https://arxiv.org/abs/2505.17160)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This work presents LURK (Latent UnleaRned Knowledge), a novel framework that probes for hidden retained knowledge in unlearned LLMs through adversarial suffix prompting. LURK automatically generates adversarial prompt suffixes designed to elicit residual knowledge about the Harry Potter domain, a commonly used benchmark for unlearning. Our experiments reveal that even models deemed successfully unlearned can leak idiosyncratic information under targeted adversarial conditions, highlighting critical limitations of current unlearning evaluation standards. By uncovering latent knowledge through indirect probing, LURK offers a more rigorous and diagnostic tool for assessing the robustness of unlearning algorithms. All code will be publicly available.</li>
</ul>

<h3>Title: OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex Text-Rich Image Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Mingxin Huang, Yongxin Shi, Dezhi Peng, Songxuan Lai, Zecheng Xie, Lianwen Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17163">https://arxiv.org/abs/2505.17163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17163">https://arxiv.org/pdf/2505.17163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17163]] OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex Text-Rich Image Reasoning(https://arxiv.org/abs/2505.17163)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in multimodal slow-thinking systems have demonstrated remarkable performance across diverse visual reasoning tasks. However, their capabilities in text-rich image reasoning tasks remain understudied due to the lack of a systematic benchmark. To address this gap, we propose OCR-Reasoning, a comprehensive benchmark designed to systematically assess Multimodal Large Language Models on text-rich image reasoning tasks. The benchmark comprises 1,069 human-annotated examples spanning 6 core reasoning abilities and 18 practical reasoning tasks in text-rich visual scenarios. Furthermore, unlike other text-rich image understanding benchmarks that only annotate the final answers, OCR-Reasoning also annotates the reasoning process simultaneously. With the annotated reasoning process and the final answers, OCR-Reasoning evaluates not only the final answers generated by models but also their reasoning processes, enabling a holistic analysis of their problem-solving abilities. Leveraging this benchmark, we conducted a comprehensive evaluation of state-of-the-art MLLMs. Our results demonstrate the limitations of existing methodologies. Notably, even state-of-the-art MLLMs exhibit substantial difficulties, with none achieving accuracy surpassing 50\% across OCR-Reasoning, indicating that the challenges of text-rich image reasoning are an urgent issue to be addressed. The benchmark and evaluation scripts are available at this https URL.</li>
</ul>

<h3>Title: CRG Score: A Distribution-Aware Clinical Metric for Radiology Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Ibrahim Ethem Hamamci, Sezgin Er, Suprosanna Shit, Hadrien Reynaud, Bernhard Kainz, Bjoern Menze</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17167">https://arxiv.org/abs/2505.17167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17167">https://arxiv.org/pdf/2505.17167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17167]] CRG Score: A Distribution-Aware Clinical Metric for Radiology Report Generation(https://arxiv.org/abs/2505.17167)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, fair</a></li>
<li><strong>Abstract: </strong>Evaluating long-context radiology report generation is challenging. NLG metrics fail to capture clinical correctness, while LLM-based metrics often lack generalizability. Clinical accuracy metrics are more relevant but are sensitive to class imbalance, frequently favoring trivial predictions. We propose the CRG Score, a distribution-aware and adaptable metric that evaluates only clinically relevant abnormalities explicitly described in reference reports. CRG supports both binary and structured labels (e.g., type, location) and can be paired with any LLM for feature extraction. By balancing penalties based on label distribution, it enables fairer, more robust evaluation and serves as a clinically aligned reward function.</li>
</ul>

<h3>Title: Next Token Perception Score: Analytical Assessment of your LLM Perception Skills</h3>
<ul>
<li><strong>Authors: </strong>Yu-Ang Cheng, Leyang Hu, Hai Huang, Randall Balestriero</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17169">https://arxiv.org/abs/2505.17169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17169">https://arxiv.org/pdf/2505.17169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17169]] Next Token Perception Score: Analytical Assessment of your LLM Perception Skills(https://arxiv.org/abs/2505.17169)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Autoregressive pretraining has become the de facto paradigm for learning general-purpose representations in large language models (LLMs). However, linear probe performance across downstream perception tasks shows substantial variability, suggesting that features optimized for next-token prediction do not consistently transfer well to downstream perception tasks. We demonstrate that representations learned via autoregression capture features that may lie outside the subspaces most informative for perception. To quantify the (mis)alignment between autoregressive pretraining and downstream perception, we introduce the Next Token Perception Score (NTPS)-a score derived under a linear setting that measures the overlap between autoregressive and perception feature subspaces. This metric can be easily computed in closed form from pretrained representations and labeled data, and is proven to both upper- and lower-bound the excess loss. Empirically, we show that NTPS correlates strongly with linear probe accuracy across 12 diverse NLP datasets and eight pretrained models ranging from 270M to 8B parameters, confirming its utility as a measure of alignment. Furthermore, we show that NTPS increases following low-rank adaptation (LoRA) fine-tuning, especially in large models, suggesting that LoRA aligning representations to perception tasks enhances subspace overlap and thus improves downstream performance. More importantly, we find that NTPS reliably predicts the additional accuracy gains attained by LoRA finetuning thereby providing a lightweight prescreening tool for LoRA adaptation. Our results offer both theoretical insights and practical tools for analytically assessing LLM perception skills.</li>
</ul>

<h3>Title: Tropical Attention: Neural Algorithmic Reasoning for Combinatorial Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Baran Hashemi, Kurt Pasque, Chris Teska, Ruriko Yoshida</a></li>
<li><strong>Subjects: </strong>cs.LG, math.AG, math.CO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17190">https://arxiv.org/abs/2505.17190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17190">https://arxiv.org/pdf/2505.17190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17190]] Tropical Attention: Neural Algorithmic Reasoning for Combinatorial Algorithms(https://arxiv.org/abs/2505.17190)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, transformer</a></li>
<li><strong>Abstract: </strong>Dynamic programming (DP) algorithms for combinatorial optimization problems work with taking maximization, minimization, and classical addition in their recursion algorithms. The associated value functions correspond to convex polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning models, however, rely on softmax-normalized dot-product attention where the smooth exponential weighting blurs these sharp polyhedral structures and collapses when evaluated on out-of-distribution (OOD) settings. We introduce Tropical attention, a novel attention function that operates natively in the max-plus semiring of tropical geometry. We prove that Tropical attention can approximate tropical circuits of DP-type combinatorial algorithms. We then propose that using Tropical transformers enhances empirical OOD performance in both length generalization and value generalization, on algorithmic reasoning tasks, surpassing softmax baselines while remaining stable under adversarial attacks. We also present adversarial-attack generalization as a third axis for Neural Algorithmic Reasoning benchmarking. Our results demonstrate that Tropical attention restores the sharp, scale-invariant reasoning absent from softmax.</li>
</ul>

<h3>Title: Shape it Up! Restoring LLM Safety during Finetuning</h3>
<ul>
<li><strong>Authors: </strong>ShengYun Peng, Pin-Yu Chen, Jianfeng Chi, Seongmin Lee, Duen Horng Chau</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17196">https://arxiv.org/abs/2505.17196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17196">https://arxiv.org/pdf/2505.17196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17196]] Shape it Up! Restoring LLM Safety during Finetuning(https://arxiv.org/abs/2505.17196)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Finetuning large language models (LLMs) enables user-specific customization but introduces critical safety risks: even a few harmful examples can compromise safety alignment. A common mitigation strategy is to update the model more strongly on examples deemed safe, while downweighting or excluding those flagged as unsafe. However, because safety context can shift within a single example, updating the model equally on both harmful and harmless parts of a response is suboptimal-a coarse treatment we term static safety shaping. In contrast, we propose dynamic safety shaping (DSS), a framework that uses fine-grained safety signals to reinforce learning from safe segments of a response while suppressing unsafe content. To enable such fine-grained control during finetuning, we introduce a key insight: guardrail models, traditionally used for filtering, can be repurposed to evaluate partial responses, tracking how safety risk evolves throughout the response, segment by segment. This leads to the Safety Trajectory Assessment of Response (STAR), a token-level signal that enables shaping to operate dynamically over the training sequence. Building on this, we present STAR-DSS, guided by STAR scores, that robustly mitigates finetuning risks and delivers substantial safety improvements across diverse threats, datasets, and model families-all without compromising capability on intended tasks. We encourage future safety research to build on dynamic shaping principles for stronger mitigation against evolving finetuning risks.</li>
</ul>

<h3>Title: A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data</h3>
<ul>
<li><strong>Authors: </strong>Chaim Chai Elchik, Fatemeh Karimi Nejadasl, Seyed Sahand Mohammadi Ziabari, Ali Mohammed Mansoor Alsahag</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17201">https://arxiv.org/abs/2505.17201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17201">https://arxiv.org/pdf/2505.17201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17201]] A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data(https://arxiv.org/abs/2505.17201)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Multi-object tracking (MOT) in computer vision has made significant advancements, yet tracking small fish in underwater environments presents unique challenges due to complex 3D motions and data noise. Traditional single-view MOT models often fall short in these settings. This thesis addresses these challenges by adapting state-of-the-art single-view MOT models, FairMOT and YOLOv8, for underwater fish detecting and tracking in ecological studies. The core contribution of this research is the development of a multi-view framework that utilizes stereo video inputs to enhance tracking accuracy and fish behavior pattern recognition. By integrating and evaluating these models on underwater fish video datasets, the study aims to demonstrate significant improvements in precision and reliability compared to single-view approaches. The proposed framework detects fish entities with a relative accuracy of 47% and employs stereo-matching techniques to produce a novel 3D output, providing a more comprehensive understanding of fish movements and interactions</li>
</ul>

<h3>Title: Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Kangda Wei, Hasnat Md Abdullah, Ruihong Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17217">https://arxiv.org/abs/2505.17217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17217">https://arxiv.org/pdf/2505.17217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17217]] Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs(https://arxiv.org/abs/2505.17217)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often exhibit gender bias, resulting in unequal treatment of male and female subjects across different contexts. To address this issue, we propose a novel data generation framework that fosters exploratory thinking in LLMs. Our approach prompts models to generate story pairs featuring male and female protagonists in structurally identical, morally ambiguous scenarios, then elicits and compares their moral judgments. When inconsistencies arise, the model is guided to produce balanced, gender-neutral judgments. These story-judgment pairs are used to fine-tune or optimize the models via Direct Preference Optimization (DPO). Experimental results show that our method significantly reduces gender bias while preserving or even enhancing general model capabilities. We will release the code and generated data.</li>
</ul>

<h3>Title: Humans Hallucinate Too: Language Models Identify and Correct Subjective Annotation Errors With Label-in-a-Haystack Prompts</h3>
<ul>
<li><strong>Authors: </strong>Georgios Chochlakis, Peter Wu, Arjun Bedi, Marcus Ma, Kristina Lerman, Shrikanth Narayanan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17222">https://arxiv.org/abs/2505.17222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17222">https://arxiv.org/pdf/2505.17222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17222]] Humans Hallucinate Too: Language Models Identify and Correct Subjective Annotation Errors With Label-in-a-Haystack Prompts(https://arxiv.org/abs/2505.17222)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Modeling complex subjective tasks in Natural Language Processing, such as recognizing emotion and morality, is considerably challenging due to significant variation in human annotations. This variation often reflects reasonable differences in semantic interpretations rather than mere noise, necessitating methods to distinguish between legitimate subjectivity and error. We address this challenge by exploring label verification in these contexts using Large Language Models (LLMs). First, we propose a simple In-Context Learning binary filtering baseline that estimates the reasonableness of a document-label pair. We then introduce the Label-in-a-Haystack setting: the query and its label(s) are included in the demonstrations shown to LLMs, which are prompted to predict the label(s) again, while receiving task-specific instructions (e.g., emotion recognition) rather than label copying. We show how the failure to copy the label(s) to the output of the LLM are task-relevant and informative. Building on this, we propose the Label-in-a-Haystack Rectification (LiaHR) framework for subjective label correction: when the model outputs diverge from the reference gold labels, we assign the generated labels to the example instead of discarding it. This approach can be integrated into annotation pipelines to enhance signal-to-noise ratios. Comprehensive analyses, human evaluations, and ecological validity studies verify the utility of LiaHR for label correction. Code is available at this https URL.</li>
</ul>

<h3>Title: Dynamic Encryption-Based Cloud Security Model using Facial Image and Password-based Key Generation for Multimedia Data</h3>
<ul>
<li><strong>Authors: </strong>Naima Sultana Ayesha, Mehrin Anannya, Md Biplob Hosen, Rashed Mazumder</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17224">https://arxiv.org/abs/2505.17224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17224">https://arxiv.org/pdf/2505.17224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17224]] Dynamic Encryption-Based Cloud Security Model using Facial Image and Password-based Key Generation for Multimedia Data(https://arxiv.org/abs/2505.17224)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>In this cloud-dependent era, various security techniques, such as encryption, steganography, and hybrid approaches, have been utilized in cloud computing to enhance security, maintain enormous storage capacity, and provide ease of access. However, the absence of data type-specific encryption and decryption procedures renders multimedia data vulnerable. To address this issue, this study presents a dynamic encryption-based security architecture that adapts encryption methods to any file type, using keys generated from facial images and passwords. Four diverse datasets are created, each with a consistent size of 2GB, containing varying combinations of image, audio (MP3 and MPEG), video, text, CSV, PPT, and PDF files, to implement the proposed methodology. AES is used to encrypt image data, AES-CTR is employed for audio or video data to meet real-time streaming needs, and Blowfish is used for other types of data. Performance analysis on all four datasets is conducted using AWS servers, where DATASET-1 demonstrates the best performance compared to the others.</li>
</ul>

<h3>Title: Secure and Private Federated Learning: Achieving Adversarial Resilience through Robust Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Kun Yang, Neena Imam</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17226">https://arxiv.org/abs/2505.17226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17226">https://arxiv.org/pdf/2505.17226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17226]] Secure and Private Federated Learning: Achieving Adversarial Resilience through Robust Aggregation(https://arxiv.org/abs/2505.17226)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables collaborative machine learning across decentralized data sources without sharing raw data. It offers a promising approach to privacy-preserving AI. However, FL remains vulnerable to adversarial threats from malicious participants, referred to as Byzantine clients, who can send misleading updates to corrupt the global model. Traditional aggregation methods, such as simple averaging, are not robust to such attacks. More resilient approaches, like the Krum algorithm, require prior knowledge of the number of malicious clients, which is often unavailable in real-world scenarios. To address these limitations, we propose Average-rKrum (ArKrum), a novel aggregation strategy designed to enhance both the resilience and privacy guarantees of FL systems. Building on our previous work (rKrum), ArKrum introduces two key innovations. First, it includes a median-based filtering mechanism that removes extreme outliers before estimating the number of adversarial clients. Second, it applies a multi-update averaging scheme to improve stability and performance, particularly when client data distributions are not identical. We evaluate ArKrum on benchmark image and text datasets under three widely studied Byzantine attack types. Results show that ArKrum consistently achieves high accuracy and stability. It performs as well as or better than other robust aggregation methods. These findings demonstrate that ArKrum is an effective and practical solution for secure FL systems in adversarial environments.</li>
</ul>

<h3>Title: Semantic-Aware Interpretable Multimodal Music Auto-Tagging</h3>
<ul>
<li><strong>Authors: </strong>Andreas Patakis, Vassilis Lyberatos, Spyridon Kantarelis, Edmund Dervakos, Giorgos Stamou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17233">https://arxiv.org/abs/2505.17233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17233">https://arxiv.org/pdf/2505.17233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17233]] Semantic-Aware Interpretable Multimodal Music Auto-Tagging(https://arxiv.org/abs/2505.17233)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Music auto-tagging is essential for organizing and discovering music in extensive digital libraries. While foundation models achieve exceptional performance in this domain, their outputs often lack interpretability, limiting trust and usability for researchers and end-users alike. In this work, we present an interpretable framework for music auto-tagging that leverages groups of musically meaningful multimodal features, derived from signal processing, deep learning, ontology engineering, and natural language processing. To enhance interpretability, we cluster features semantically and employ an expectation maximization algorithm, assigning distinct weights to each group based on its contribution to the tagging process. Our method achieves competitive tagging performance while offering a deeper understanding of the decision-making process, paving the way for more transparent and user-centric music tagging systems.</li>
</ul>

<h3>Title: CHAOS: Chart Analysis with Outlier Samples</h3>
<ul>
<li><strong>Authors: </strong>Omar Moured, Yufan Chen, Ruiping Liu, Simon Reiß, Philip Torr, Jiaming Zhang, Rainer Stiefelhagen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17235">https://arxiv.org/abs/2505.17235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17235">https://arxiv.org/pdf/2505.17235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17235]] CHAOS: Chart Analysis with Outlier Samples(https://arxiv.org/abs/2505.17235)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Charts play a critical role in data analysis and visualization, yet real-world applications often present charts with challenging or noisy features. However, "outlier charts" pose a substantial challenge even for Multimodal Large Language Models (MLLMs), which can struggle to interpret perturbed charts. In this work, we introduce CHAOS (CHart Analysis with Outlier Samples), a robustness benchmark to systematically evaluate MLLMs against chart perturbations. CHAOS encompasses five types of textual and ten types of visual perturbations, each presented at three levels of severity (easy, mid, hard) inspired by the study result of human evaluation. The benchmark includes 13 state-of-the-art MLLMs divided into three groups (i.e., general-, document-, and chart-specific models) according to the training scope and data. Comprehensive analysis involves two downstream tasks (ChartQA and Chart-to-Text). Extensive experiments and case studies highlight critical insights into robustness of models across chart perturbations, aiming to guide future research in chart understanding domain. Data and code are publicly available at: this http URL.</li>
</ul>

<h3>Title: LogStamping: A blockchain-based log auditing approach for large-scale systems</h3>
<ul>
<li><strong>Authors: </strong>Md Shariful Islam, M. Sohel Rahman</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17236">https://arxiv.org/abs/2505.17236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17236">https://arxiv.org/pdf/2505.17236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17236]] LogStamping: A blockchain-based log auditing approach for large-scale systems(https://arxiv.org/abs/2505.17236)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Log management is crucial for ensuring the security, integrity, and compliance of modern information systems. Traditional log management solutions face challenges in achieving tamper-proofing, scalability, and real-time processing in distributed environments. This paper presents a blockchain-based log management framework that addresses these limitations by leveraging blockchain's decentralized, immutable, and transparent features. The framework integrates a hybrid on-chain and off-chain storage model, combining blockchain's integrity guarantees with the scalability of distributed storage solutions like IPFS. Smart contracts automate log validation and access control, while cryptographic techniques ensure privacy and confidentiality. With a focus on real-time log processing, the framework is designed to handle the high-volume log generation typical in large-scale systems, such as data centers and network infrastructure. Performance evaluations demonstrate the framework's scalability, low latency, and ability to manage millions of log entries while maintaining strong security guarantees. Additionally, the paper discusses challenges like blockchain storage overhead and energy consumption, offering insights for enhancing future systems.</li>
</ul>

<h3>Title: Personalizing Student-Agent Interactions Using Log-Contextualized Retrieval Augmented Generation (RAG)</h3>
<ul>
<li><strong>Authors: </strong>Clayton Cohn, Surya Rayala, Caitlin Snyder, Joyce Fonteles, Shruti Jain, Naveeduddin Mohammed, Umesh Timalsina, Sarah K. Burriss, Ashwin T S, Namrata Srivastava, Menton Deweese, Angela Eeds, Gautam Biswas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17238">https://arxiv.org/abs/2505.17238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17238">https://arxiv.org/pdf/2505.17238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17238]] Personalizing Student-Agent Interactions Using Log-Contextualized Retrieval Augmented Generation (RAG)(https://arxiv.org/abs/2505.17238)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Collaborative dialogue offers rich insights into students' learning and critical thinking. This is essential for adapting pedagogical agents to students' learning and problem-solving skills in STEM+C settings. While large language models (LLMs) facilitate dynamic pedagogical interactions, potential hallucinations can undermine confidence, trust, and instructional value. Retrieval-augmented generation (RAG) grounds LLM outputs in curated knowledge, but its effectiveness depends on clear semantic links between user input and a knowledge base, which are often weak in student dialogue. We propose log-contextualized RAG (LC-RAG), which enhances RAG retrieval by incorporating environment logs to contextualize collaborative discourse. Our findings show that LC-RAG improves retrieval over a discourse-only baseline and allows our collaborative peer agent, Copa, to deliver relevant, personalized guidance that supports students' critical thinking and epistemic decision-making in a collaborative computational modeling environment, XYZ.</li>
</ul>

<h3>Title: Optimal Policy Minimum Bayesian Risk</h3>
<ul>
<li><strong>Authors: </strong>Ramón Fernandez Astudillo, Md Arafat Sultan, Aashka Trivedi, Yousef El-Kurdi, Tahira Naseem, Radu Florian, Salim Roukos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17242">https://arxiv.org/abs/2505.17242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17242">https://arxiv.org/pdf/2505.17242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17242]] Optimal Policy Minimum Bayesian Risk(https://arxiv.org/abs/2505.17242)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Inference scaling can help LLMs solve complex reasoning problems through extended runtime computation. On top of targeted supervision for long chain-of-thought (long-CoT) generation, purely inference-time techniques such as best-of-N (BoN) sampling, majority voting, or more generally, minimum Bayes risk decoding (MBRD), can further improve LLM accuracy by generating multiple candidate solutions and aggregating over them. These methods typically leverage additional signals in the form of reward models and risk/similarity functions that compare generated samples, e.g., exact match in some normalized space or standard similarity metrics such as Rouge. Here we present a novel method for incorporating reward and risk/similarity signals into MBRD. Based on the concept of optimal policy in KL-controlled reinforcement learning, our framework provides a simple and well-defined mechanism for leveraging such signals, offering several advantages over traditional inference-time methods: higher robustness, improved accuracy, and well-understood asymptotic behavior. In addition, it allows for the development of a sample-efficient variant of MBRD that can adjust the number of samples to generate according to the difficulty of the problem, without relying on majority vote counts. We empirically demonstrate the advantages of our approach on math (MATH-$500$) and coding (HumanEval) tasks using recent open-source models. We also present a comprehensive analysis of its accuracy-compute trade-offs.</li>
</ul>

<h3>Title: Backdoors in DRL: Four Environments Focusing on In-distribution Triggers</h3>
<ul>
<li><strong>Authors: </strong>Chace Ashcraft, Ted Staley, Josh Carney, Cameron Hickert, Derek Juba, Kiran Karra, Nathan Drenkow</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17248">https://arxiv.org/abs/2505.17248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17248">https://arxiv.org/pdf/2505.17248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17248]] Backdoors in DRL: Four Environments Focusing on In-distribution Triggers(https://arxiv.org/abs/2505.17248)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Backdoor attacks, or trojans, pose a security risk by concealing undesirable behavior in deep neural network models. Open-source neural networks are downloaded from the internet daily, possibly containing backdoors, and third-party model developers are common. To advance research on backdoor attack mitigation, we develop several trojans for deep reinforcement learning (DRL) agents. We focus on in-distribution triggers, which occur within the agent's natural data distribution, since they pose a more significant security threat than out-of-distribution triggers due to their ease of activation by the attacker during model deployment. We implement backdoor attacks in four reinforcement learning (RL) environments: LavaWorld, Randomized LavaWorld, Colorful Memory, and Modified Safety Gymnasium. We train various models, both clean and backdoored, to characterize these attacks. We find that in-distribution triggers can require additional effort to implement and be more challenging for models to learn, but are nevertheless viable threats in DRL even using basic data poisoning attacks.</li>
</ul>

<h3>Title: ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Razvan-Gabriel Dumitru, Darius Peteleaza, Vikas Yadav, Liangming Pan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17250">https://arxiv.org/abs/2505.17250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17250">https://arxiv.org/pdf/2505.17250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17250]] ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning Models(https://arxiv.org/abs/2505.17250)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models excel at complex tasks by breaking down problems into structured reasoning steps. However, reasoning traces often extend beyond reaching a correct answer, causing wasted computation, reduced readability, and hallucinations. To address this, we introduce a novel hyperparameter-free conciseness score used as a reward signal within a reinforcement learning framework to guide models toward generating correct and concise reasoning traces. This score is evaluated by a large language model acting as a judge, enabling dynamic, context-aware feedback beyond simple token length. Our method achieves state-of-the-art efficiency-accuracy trade-offs on the MATH dataset, reducing token usage by up to 31x on simple problems while improving accuracy by 7%, and on the hardest problems, it outperforms full reasoning by +7.5% accuracy with up to 3.6x fewer tokens. On TheoremQA, our method improves accuracy by +2.2% using 12.5x fewer tokens. We also conduct ablation studies on the judge model, reward composition, and problem difficulty, showing that our method dynamically adapts reasoning length based on problem difficulty and benefits significantly from stronger judges. The code, model weights, and datasets are open-sourced at this https URL.</li>
</ul>

<h3>Title: Understanding the Security Landscape of Embedded Non-Volatile Memories: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Zakia Tamanna Tisha, Ujjwal Guin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17253">https://arxiv.org/abs/2505.17253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17253">https://arxiv.org/pdf/2505.17253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17253]] Understanding the Security Landscape of Embedded Non-Volatile Memories: A Comprehensive Survey(https://arxiv.org/abs/2505.17253)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>The modern semiconductor industry requires memory solutions that can keep pace with the high-speed demands of high-performance computing. Embedded non-volatile memories (eNVMs) address these requirements by offering faster access to stored data at an improved computational throughput and efficiency. Furthermore, these technologies offer numerous appealing features, including limited area-energy-runtime budget and data retention capabilities. Among these, the data retention feature of eNVMs has garnered particular interest within the semiconductor community. Although this property allows eNVMs to retain data even in the absence of a continuous power supply, it also introduces some vulnerabilities, prompting security concerns. These concerns have sparked increased interest in examining the broader security implications associated with eNVM technologies. This paper examines the security aspects of eNVMs by discussing the reasons for vulnerabilities in specific memories from an architectural point of view. Additionally, this paper extensively reviews eNVM-based security primitives, such as physically unclonable functions and true random number generators, as well as techniques like logic obfuscation. The paper also explores a broad spectrum of security threats to eNVMs, including physical attacks such as side-channel attacks, fault injection, and probing, as well as logical threats like information leakage, denial-of-service, and thermal attacks. Finally, the paper presents a study of publication trends in the eNVM domain since the early 2000s, reflecting the rising momentum and research activity in this field.</li>
</ul>

<h3>Title: Approach to Finding a Robust Deep Learning Model</h3>
<ul>
<li><strong>Authors: </strong>Alexey Boldyrev, Fedor Ratnikov, Andrey Shevelev</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17254">https://arxiv.org/abs/2505.17254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17254">https://arxiv.org/pdf/2505.17254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17254]] Approach to Finding a Robust Deep Learning Model(https://arxiv.org/abs/2505.17254)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The rapid development of machine learning (ML) and artificial intelligence (AI) applications requires the training of large numbers of models. This growing demand highlights the importance of training models without human supervision, while ensuring that their predictions are reliable. In response to this need, we propose a novel approach for determining model robustness. This approach, supplemented with a proposed model selection algorithm designed as a meta-algorithm, is versatile and applicable to any machine learning model, provided that it is appropriate for the task at hand. This study demonstrates the application of our approach to evaluate the robustness of deep learning models. To this end, we study small models composed of a few convolutional and fully connected layers, using common optimizers due to their ease of interpretation and computational efficiency. Within this framework, we address the influence of training sample size, model weight initialization, and inductive bias on the robustness of deep learning models.</li>
</ul>

<h3>Title: ExpertGen: Training-Free Expert Guidance for Controllable Text-to-Face Generation</h3>
<ul>
<li><strong>Authors: </strong>Liang Shi, Yun Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17256">https://arxiv.org/abs/2505.17256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17256">https://arxiv.org/pdf/2505.17256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17256]] ExpertGen: Training-Free Expert Guidance for Controllable Text-to-Face Generation(https://arxiv.org/abs/2505.17256)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models have significantly improved text-to-face generation, but achieving fine-grained control over facial features remains a challenge. Existing methods often require training additional modules to handle specific controls such as identity, attributes, or age, making them inflexible and resource-intensive. We propose ExpertGen, a training-free framework that leverages pre-trained expert models such as face recognition, facial attribute recognition, and age estimation networks to guide generation with fine control. Our approach uses a latent consistency model to ensure realistic and in-distribution predictions at each diffusion step, enabling accurate guidance signals to effectively steer the diffusion process. We show qualitatively and quantitatively that expert models can guide the generation process with high precision, and multiple experts can collaborate to enable simultaneous control over diverse facial aspects. By allowing direct integration of off-the-shelf expert models, our method transforms any such model into a plug-and-play component for controllable face generation.</li>
</ul>

<h3>Title: JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Qihao Duan, Bingding Huang, Zhenqiao Song, Irina Lehmann, Lei Gu, Roland Eils, Benjamin Wild</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17257">https://arxiv.org/abs/2505.17257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17257">https://arxiv.org/pdf/2505.17257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17257]] JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model(https://arxiv.org/abs/2505.17257)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have revolutionized natural language processing and are increasingly applied to other sequential data types, including genetic sequences. However, adapting LLMs to genomics presents significant challenges. Capturing complex genomic interactions requires modeling long-range dependencies within DNA sequences, where interactions often span over 10,000 base pairs, even within a single gene, posing substantial computational burdens under conventional model architectures and training paradigms. Moreover, standard LLM training approaches are suboptimal for DNA: autoregressive training, while efficient, supports only unidirectional understanding. However, DNA is inherently bidirectional, e.g., bidirectional promoters regulate transcription in both directions and account for nearly 11% of human gene expression. Masked language models (MLMs) allow bidirectional understanding but are inefficient, as only masked tokens contribute to the loss per step. To address these limitations, we introduce JanusDNA, the first bidirectional DNA foundation model built upon a novel pretraining paradigm that combines the optimization efficiency of autoregressive modeling with the bidirectional comprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and Mixture of Experts (MoE) architecture, combining long-range modeling of Attention with efficient sequential learning of Mamba. MoE layers further scale model capacity via sparse activation while keeping computational cost low. Notably, JanusDNA processes up to 1 million base pairs at single nucleotide resolution on a single 80GB GPU. Extensive experiments and ablations show JanusDNA achieves new SOTA results on three genomic representation benchmarks, outperforming models with 250x more activated parameters. Code: this https URL</li>
</ul>

<h3>Title: The Rise of Parameter Specialization for Knowledge Storage in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yihuai Hong, Yiran Zhao, Wei Tang, Yang Deng, Yu Rong, Wenxuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17260">https://arxiv.org/abs/2505.17260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17260">https://arxiv.org/pdf/2505.17260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17260]] The Rise of Parameter Specialization for Knowledge Storage in Large Language Models(https://arxiv.org/abs/2505.17260)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Over time, a growing wave of large language models from various series has been introduced to the community. Researchers are striving to maximize the performance of language models with constrained parameter sizes. However, from a microscopic perspective, there has been limited research on how to better store knowledge in model parameters, particularly within MLPs, to enable more effective utilization of this knowledge by the model. In this work, we analyze twenty publicly available open-source large language models to investigate the relationship between their strong performance and the way knowledge is stored in their corresponding MLP parameters. Our findings reveal that as language models become more advanced and demonstrate stronger knowledge capabilities, their parameters exhibit increased specialization. Specifically, parameters in the MLPs tend to be more focused on encoding similar types of knowledge. We experimentally validate that this specialized distribution of knowledge contributes to improving the efficiency of knowledge utilization in these models. Furthermore, by conducting causal training experiments, we confirm that this specialized knowledge distribution plays a critical role in improving the model's efficiency in leveraging stored knowledge.</li>
</ul>

<h3>Title: CaseReportBench: An LLM Benchmark Dataset for Dense Information Extraction in Clinical Case Reports</h3>
<ul>
<li><strong>Authors: </strong>Xiao Yu Cindy Zhang (1), Carlos R. Ferreira (2), Francis Rossignol (2), Raymond T. Ng (1), Wyeth Wasserman (1), Jian Zhu (1) ((1) University of British Columbia, (2) National Institutes of Health)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17265">https://arxiv.org/abs/2505.17265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17265">https://arxiv.org/pdf/2505.17265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17265]] CaseReportBench: An LLM Benchmark Dataset for Dense Information Extraction in Clinical Case Reports(https://arxiv.org/abs/2505.17265)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Rare diseases, including Inborn Errors of Metabolism (IEM), pose significant diagnostic challenges. Case reports serve as key but computationally underutilized resources to inform diagnosis. Clinical dense information extraction refers to organizing medical information into structured predefined categories. Large Language Models (LLMs) may enable scalable information extraction from case reports but are rarely evaluated for this task. We introduce CaseReportBench, an expert-annotated dataset for dense information extraction of case reports, focusing on IEMs. Using this dataset, we assess various models and prompting strategies, introducing novel approaches such as category-specific prompting and subheading-filtered data integration. Zero-shot chain-of-thought prompting offers little advantage over standard zero-shot prompting. Category-specific prompting improves alignment with the benchmark. The open-source model Qwen2.5-7B outperforms GPT-4o for this task. Our clinician evaluations show that LLMs can extract clinically relevant details from case reports, supporting rare disease diagnosis and management. We also highlight areas for improvement, such as LLMs' limitations in recognizing negative findings important for differential diagnosis. This work advances LLM-driven clinical natural language processing and paves the way for scalable medical AI applications.</li>
</ul>

<h3>Title: Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Cehao Yang, Xueyuan Lin, Chengjin Xu, Xuhui Jiang, Xiaojun Wu, Honghao Liu, Hui Xiong, Jian Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17266">https://arxiv.org/abs/2505.17266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17266">https://arxiv.org/pdf/2505.17266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17266]] Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning(https://arxiv.org/abs/2505.17266)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A practical approach to activate long chain-of-thoughts reasoning ability in pre-trained large language models is to perform supervised fine-tuning on instruction datasets synthesized by strong Large Reasoning Models such as DeepSeek-R1, offering a cost-effective alternative to reinforcement learning. However, large-scale instruction sets with more than 100k samples incur significant training overhead, while effective strategies for automatic long-CoT instruction selection still remain unexplored. In this work, we propose Select2Reason, a novel and efficient instruction-tuning data selection framework for long-CoT reasoning. From the perspective of emergence of rethinking behaviors like self-correction and backtracking, we investigate common metrics that may determine the quality of long-CoT reasoning instructions. Select2Reason leverages a quantifier to estimate difficulty of question and jointly incorporates a reasoning trace length-based heuristic through a weighted scheme for ranking to prioritize high-utility examples. Empirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only 10% of the data selected by Select2Reason achieves performance competitive with or superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across three competition-level and six comprehensive mathematical benchmarks. Further experiments highlight the scalability in varying data size, efficiency during inference, and its adaptability to other instruction pools with minimal cost.</li>
</ul>

<h3>Title: Zebra-Llama: Towards Extremely Efficient Hybrid Models</h3>
<ul>
<li><strong>Authors: </strong>Mingyu Yang, Mehdi Rezagholizadeh, Guihong Li, Vikram Appia, Emad Barsoum</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17272">https://arxiv.org/abs/2505.17272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17272">https://arxiv.org/pdf/2505.17272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17272]] Zebra-Llama: Towards Extremely Efficient Hybrid Models(https://arxiv.org/abs/2505.17272)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>With the growing demand for deploying large language models (LLMs) across diverse applications, improving their inference efficiency is crucial for sustainable and democratized access. However, retraining LLMs to meet new user-specific requirements is prohibitively expensive and environmentally unsustainable. In this work, we propose a practical and scalable alternative: composing efficient hybrid language models from existing pre-trained models. Our approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models by combining State Space Models (SSMs) and Multi-head Latent Attention (MLA) layers, using a refined initialization and post-training pipeline to efficiently transfer knowledge from pre-trained Transformers. Zebra-Llama achieves Transformer-level accuracy with near-SSM efficiency using only 7-11B training tokens (compared to trillions of tokens required for pre-training) and an 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down to 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants, respectively-while preserving 100%, 100%, and >97% of average zero-shot performance on LM Harness tasks. Compared to models like MambaInLLaMA, X-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive or superior accuracy while using significantly fewer tokens, smaller teachers, and vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses Minitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens, over 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves 2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context length. We will release code and model checkpoints upon acceptance.</li>
</ul>

<h3>Title: Vehicular Intrusion Detection System for Controller Area Network: A Comprehensive Survey and Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Yangyang Liu, Lei Xue, Sishan Wang, Xiapu Luo, Kaifa Zhao, Pengfei Jing, Xiaobo Ma, Yajuan Tang, Haiying Zhou</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17274">https://arxiv.org/abs/2505.17274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17274">https://arxiv.org/pdf/2505.17274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17274]] Vehicular Intrusion Detection System for Controller Area Network: A Comprehensive Survey and Evaluation(https://arxiv.org/abs/2505.17274)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>The progress of automotive technologies has made cybersecurity a crucial focus, leading to various cyber attacks. These attacks primarily target the Controller Area Network (CAN) and specialized Electronic Control Units (ECUs). In order to mitigate these attacks and bolster the security of vehicular systems, numerous defense solutions have been this http URL solutions aim to detect diverse forms of vehicular attacks. However, the practical implementation of these solutions still presents certain limitations and challenges. In light of these circumstances, this paper undertakes a thorough examination of existing vehicular attacks and defense strategies employed against the CAN and ECUs. The objective is to provide valuable insights and inform the future design of Vehicular Intrusion Detection Systems (VIDS). The findings of our investigation reveal that the examined VIDS primarily concentrate on particular categories of attacks, neglecting the broader spectrum of potential threats. Moreover, we provide a comprehensive overview of the significant challenges encountered in implementing a robust and feasible VIDS. Additionally, we put forth several defense recommendations based on our study findings, aiming to inform and guide the future design of VIDS in the context of vehicular security.</li>
</ul>

<h3>Title: Mitigate One, Skew Another? Tackling Intersectional Biases in Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Pushkar Shukla, Aditya Chinchure, Emily Diana, Alexander Tolbert, Kartik Hosanagar, Vineeth N Balasubramanian, Leonid Sigal, Matthew Turk</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17280">https://arxiv.org/abs/2505.17280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17280">https://arxiv.org/pdf/2505.17280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17280]] Mitigate One, Skew Another? Tackling Intersectional Biases in Text-to-Image Models(https://arxiv.org/abs/2505.17280)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative</a></li>
<li><strong>Abstract: </strong>The biases exhibited by text-to-image (TTI) models are often treated as independent, though in reality, they may be deeply interrelated. Addressing bias along one dimension - such as ethnicity or age - can inadvertently affect another, like gender, either mitigating or exacerbating existing disparities. Understanding these interdependencies is crucial for designing fairer generative models, yet measuring such effects quantitatively remains a challenge. To address this, we introduce BiasConnect, a novel tool for analyzing and quantifying bias interactions in TTI models. BiasConnect uses counterfactual interventions along different bias axes to reveal the underlying structure of these interactions and estimates the effect of mitigating one bias axis on another. These estimates show strong correlation (+0.65) with observed post-mitigation outcomes. Building on BiasConnect, we propose InterMit, an intersectional bias mitigation algorithm guided by user-defined target distributions and priority weights. InterMit achieves lower bias (0.33 vs. 0.52) with fewer mitigation steps (2.38 vs. 3.15 average steps), and yields superior image quality compared to traditional techniques. Although our implementation is training-free, InterMit is modular and can be integrated with many existing debiasing approaches for TTI models, making it a flexible and extensible solution.</li>
</ul>

<h3>Title: Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Peilin Wu, Mian Zhang, Xinlu Zhang, Xinya Du, Zhiyu Zoey Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17281">https://arxiv.org/abs/2505.17281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17281">https://arxiv.org/pdf/2505.17281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17281]] Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty(https://arxiv.org/abs/2505.17281)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Agentic Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) by enabling dynamic, multi-step reasoning and information retrieval. However, these systems often exhibit sub-optimal search behaviors like over-search (retrieving redundant information) and under-search (failing to retrieve necessary information), which hinder efficiency and reliability. This work formally defines and quantifies these behaviors, revealing their prevalence across multiple QA datasets and agentic RAG systems (e.g., one model could have avoided searching in 27.7% of its search steps). Furthermore, we demonstrate a crucial link between these inefficiencies and the models' uncertainty regarding their own knowledge boundaries, where response accuracy correlates with model's uncertainty in its search decisions. To address this, we propose $\beta$-GRPO, a reinforcement learning-based training method that incorporates confidence threshold to reward high-certainty search decisions. Experiments on seven QA benchmarks show that $\beta$-GRPO enable a 3B model with better agentic RAG ability, outperforming other strong baselines with a 4% higher average exact match score.</li>
</ul>

<h3>Title: Model-Free Graph Data Selection under Distribution Shift</h3>
<ul>
<li><strong>Authors: </strong>Ting-Wei Li, Ruizhong Qiu, Hanghang Tong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17293">https://arxiv.org/abs/2505.17293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17293">https://arxiv.org/pdf/2505.17293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17293]] Model-Free Graph Data Selection under Distribution Shift(https://arxiv.org/abs/2505.17293)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Graph domain adaptation (GDA) is a fundamental task in graph machine learning, with techniques like shift-robust graph neural networks (GNNs) and specialized training procedures to tackle the distribution shift problem. Although these model-centric approaches show promising results, they often struggle with severe shifts and constrained computational resources. To address these challenges, we propose a novel model-free framework, GRADATE (GRAph DATa sElector), that selects the best training data from the source domain for the classification task on the target domain. GRADATE picks training samples without relying on any GNN model's predictions or training recipes, leveraging optimal transport theory to capture and adapt to distribution changes. GRADATE is data-efficient, scalable and meanwhile complements existing model-centric GDA approaches. Through comprehensive empirical studies on several real-world graph-level datasets and multiple covariate shift types, we demonstrate that GRADATE outperforms existing selection methods and enhances off-the-shelf GDA methods with much fewer training data.</li>
</ul>

<h3>Title: SELF: Self-Extend the Context Length With Logistic Growth Function</h3>
<ul>
<li><strong>Authors: </strong>Phat Thanh Dang, Saahil Thoppay, Wang Yang, Qifan Wang, Vipin Chaudhary, Xiaotian Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17296">https://arxiv.org/abs/2505.17296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17296">https://arxiv.org/pdf/2505.17296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17296]] SELF: Self-Extend the Context Length With Logistic Growth Function(https://arxiv.org/abs/2505.17296)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models suffer issues when operated on long contexts that are larger than their training context length due to the standard position encoding for tokens in the attention layer. Tokens a long distance apart will rarely have an effect on each other and long prompts yield unexpected results. To solve this problem, we propose SELF (Self-Extend the Context Length With Logistic Growth Function): a solution of grouping consecutive tokens at varying group sizes using a logistic capacity equation combined with a constant group size at smaller relative distances. Our model had an increase in performance of up to 12% compared to the LongLM extension method in LEval (specifically on the Qwen model). On summarization related tasks in LongBench, our model performed up to 6.4% better than LongLM (specifically on the Llama-2-7b model). On reading comprehension tasks from LEval, our model performed up to 5.4% better than the LongLM. Our code is available at this https URL.</li>
</ul>

<h3>Title: Refusal Direction is Universal Across Safety-Aligned Languages</h3>
<ul>
<li><strong>Authors: </strong>Xinpeng Wang, Mingyang Wang, Yihong Liu, Hinrich Schütze, Barbara Plank</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17306">https://arxiv.org/abs/2505.17306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17306">https://arxiv.org/pdf/2505.17306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17306]] Refusal Direction is Universal Across Safety-Aligned Languages(https://arxiv.org/abs/2505.17306)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, robust, large language model</a></li>
<li><strong>Abstract: </strong>Refusal mechanisms in large language models (LLMs) are essential for ensuring safety. Recent research has revealed that refusal behavior can be mediated by a single direction in activation space, enabling targeted interventions to bypass refusals. While this is primarily demonstrated in an English-centric context, appropriate refusal behavior is important for any language, but poorly understood. In this paper, we investigate the refusal behavior in LLMs across 14 languages using PolyRefuse, a multilingual safety dataset created by translating malicious and benign English prompts into these languages. We uncover the surprising cross-lingual universality of the refusal direction: a vector extracted from English can bypass refusals in other languages with near-perfect effectiveness, without any additional fine-tuning. Even more remarkably, refusal directions derived from any safety-aligned language transfer seamlessly to others. We attribute this transferability to the parallelism of refusal vectors across languages in the embedding space and identify the underlying mechanism behind cross-lingual jailbreaks. These findings provide actionable insights for building more robust multilingual safety defenses and pave the way for a deeper mechanistic understanding of cross-lingual vulnerabilities in LLMs.</li>
</ul>

<h3>Title: Advancing Security with Digital Twins: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Blessing Airehenbuwa, Touseef Hasan, Souvika Sarkar, Ujjwal Guin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17310">https://arxiv.org/abs/2505.17310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17310">https://arxiv.org/pdf/2505.17310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17310]] Advancing Security with Digital Twins: A Comprehensive Survey(https://arxiv.org/abs/2505.17310)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of electronic devices has greatly transformed every aspect of human life, such as communication, healthcare, transportation, and energy. Unfortunately, the global electronics supply chain is vulnerable to various attacks, including piracy of intellectual properties, tampering, counterfeiting, information leakage, side-channel, and fault injection attacks, due to the complex nature of electronic products and vulnerabilities present in them. Although numerous solutions have been proposed to address these threats, significant gaps remain, particularly in providing scalable and comprehensive protection against emerging attacks. Digital twin, a dynamic virtual replica of a physical system, has emerged as a promising solution to address these issues by providing backward traceability, end-to-end visibility, and continuous verification of component integrity and behavior. In this paper, we present a comprehensive survey of the application of digital twins based on their functional role and application domains. We comprehensively present recent digital twin-based security implementations, including their role in cyber-physical systems, Internet of Things, and cryptographic systems, detection of counterfeit electronics, intrusion detection, fault injection, and side-channel leakage. To the best of our knowledge, it is the first study to consolidate these security use cases into a unified reference. The paper also explores the integration of large language models with digital twins for enhanced security and discusses current challenges, solutions, and future research directions.</li>
</ul>

<h3>Title: Harnessing EHRs for Diffusion-based Anomaly Detection on Chest X-rays</h3>
<ul>
<li><strong>Authors: </strong>Harim Kim, Yuhan Wang, Minkyu Ahn, Heeyoul Choi, Yuyin Zhou, Charmgil Hong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17311">https://arxiv.org/abs/2505.17311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17311">https://arxiv.org/pdf/2505.17311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17311]] Harnessing EHRs for Diffusion-based Anomaly Detection on Chest X-rays(https://arxiv.org/abs/2505.17311)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Unsupervised anomaly detection (UAD) in medical imaging is crucial for identifying pathological abnormalities without requiring extensive labeled data. However, existing diffusion-based UAD models rely solely on imaging features, limiting their ability to distinguish between normal anatomical variations and pathological anomalies. To address this, we propose Diff3M, a multi-modal diffusion-based framework that integrates chest X-rays and structured Electronic Health Records (EHRs) for enhanced anomaly detection. Specifically, we introduce a novel image-EHR cross-attention module to incorporate structured clinical context into the image generation process, improving the model's ability to differentiate normal from abnormal features. Additionally, we develop a static masking strategy to enhance the reconstruction of normal-like images from anomalies. Extensive evaluations on CheXpert and MIMIC-CXR/IV demonstrate that Diff3M achieves state-of-the-art performance, outperforming existing UAD methods in medical imaging. Our code is available at this http URL this https URL</li>
</ul>

<h3>Title: Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiachen Jiang, Jinxin Zhou, Bo Peng, Xia Ning, Zhihui Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17316">https://arxiv.org/abs/2505.17316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17316">https://arxiv.org/pdf/2505.17316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17316]] Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models(https://arxiv.org/abs/2505.17316)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Achieving better alignment between vision embeddings and Large Language Models (LLMs) is crucial for enhancing the abilities of Multimodal LLMs (MLLMs), particularly for recent models that rely on powerful pretrained vision encoders and LLMs. A common approach to connect the pretrained vision encoder and LLM is through a projector applied after the vision encoder. However, the projector is often trained to enable the LLM to generate captions, and hence the mechanism by which LLMs understand each vision token remains unclear. In this work, we first investigate the role of the projector in compressing vision embeddings and aligning them with word embeddings. We show that the projector significantly compresses visual information, removing redundant details while preserving essential elements necessary for the LLM to understand visual content. We then examine patch-level alignment -- the alignment between each vision patch and its corresponding semantic words -- and propose a *multi-semantic alignment hypothesis*. Our analysis indicates that the projector trained by caption loss improves patch-level alignment but only to a limited extent, resulting in weak and coarse alignment. To address this issue, we propose *patch-aligned training* to efficiently enhance patch-level alignment. Our experiments show that patch-aligned training (1) achieves stronger compression capability and improved patch-level alignment, enabling the MLLM to generate higher-quality captions, (2) improves the MLLM's performance by 16% on referring expression grounding tasks, 4% on question-answering tasks, and 3% on modern instruction-following benchmarks when using the same supervised fine-tuning (SFT) setting. The proposed method can be easily extended to other multimodal models.</li>
</ul>

<h3>Title: Optimizing Image Capture for Computer Vision-Powered Taxonomic Identification and Trait Recognition of Biodiversity Specimens</h3>
<ul>
<li><strong>Authors: </strong>Alyson East, Elizabeth G. Campolongo, Luke Meyers, S M Rayeed, Samuel Stevens, Iuliia Zarubiieva, Isadora E. Fluck, Jennifer C. Girón, Maximiliane Jousse, Scott Lowe, Kayla I Perry, Isabelle Betancourt, Noah Charney, Evan Donoso, Nathan Fox, Kim J. Landsbergen, Ekaterina Nepovinnykh, Michelle Ramirez, Parkash Singh, Khum Thapa-Magar, Matthew Thompson, Evan Waite, Tanya Berger-Wolf, Hilmar Lapp, Paula Mabee, Graham Taylor, Sydne Record</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17317">https://arxiv.org/abs/2505.17317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17317">https://arxiv.org/pdf/2505.17317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17317]] Optimizing Image Capture for Computer Vision-Powered Taxonomic Identification and Trait Recognition of Biodiversity Specimens(https://arxiv.org/abs/2505.17317)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Biological collections house millions of specimens documenting Earth's biodiversity, with digital images increasingly available through open-access platforms. Most imaging protocols were developed for human visual interpretation without considering computational analysis requirements. This paper aims to bridge the gap between current imaging practices and the potential for automated analysis by presenting key considerations for creating biological specimen images optimized for computer vision applications. We provide conceptual computer vision topics for context, addressing fundamental concerns including model generalization, data leakage, and comprehensive metadata documentation, and outline practical guidance on specimen imagine, and data storage. These recommendations were synthesized through interdisciplinary collaboration between taxonomists, collection managers, ecologists, and computer scientists. Through this synthesis, we have identified ten interconnected considerations that form a framework for successfully integrating biological specimen images into computer vision pipelines. The key elements include: (1) comprehensive metadata documentation, (2) standardized specimen positioning, (3) consistent size and color calibration, (4) protocols for handling multiple specimens in one image, (5) uniform background selection, (6) controlled lighting, (7) appropriate resolution and magnification, (8) optimal file formats, (9) robust data archiving strategies, and (10) accessible data sharing practices. By implementing these recommendations, collection managers, taxonomists, and biodiversity informaticians can generate images that support automated trait extraction, species identification, and novel ecological and evolutionary analyses at unprecedented scales. Successful implementation lies in thorough documentation of methodological choices.</li>
</ul>

<h3>Title: From Compression to Expansion: A Layerwise Analysis of In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiachen Jiang, Yuxin Dong, Jinxin Zhou, Zhihui Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17322">https://arxiv.org/abs/2505.17322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17322">https://arxiv.org/pdf/2505.17322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17322]] From Compression to Expansion: A Layerwise Analysis of In-Context Learning(https://arxiv.org/abs/2505.17322)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) enables large language models (LLMs) to adapt to new tasks without weight updates by learning from demonstration sequences. While ICL shows strong empirical performance, its internal representational mechanisms are not yet well understood. In this work, we conduct a statistical geometric analysis of ICL representations to investigate how task-specific information is captured across layers. Our analysis reveals an intriguing phenomenon, which we term *Layerwise Compression-Expansion*: early layers progressively produce compact and discriminative representations that encode task information from the input demonstrations, while later layers expand these representations to incorporate the query and generate the prediction. This phenomenon is observed consistently across diverse tasks and a range of contemporary LLM architectures. We demonstrate that it has important implications for ICL performance -- improving with model size and the number of demonstrations -- and for robustness in the presence of noisy examples. To further understand the effect of the compact task representation, we propose a bias-variance decomposition and provide a theoretical analysis showing how attention mechanisms contribute to reducing both variance and bias, thereby enhancing performance as the number of demonstrations increases. Our findings reveal an intriguing layerwise dynamic in ICL, highlight how structured representations emerge within LLMs, and showcase that analyzing internal representations can facilitate a deeper understanding of model behavior.</li>
</ul>

<h3>Title: GPT Editors, Not Authors: The Stylistic Footprint of LLMs in Academic Preprints</h3>
<ul>
<li><strong>Authors: </strong>Soren DeHaan, Yuanze Liu, Johan Bollen, Sa'ul A. Blanco</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IT, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17327">https://arxiv.org/abs/2505.17327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17327">https://arxiv.org/pdf/2505.17327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17327]] GPT Editors, Not Authors: The Stylistic Footprint of LLMs in Academic Preprints(https://arxiv.org/abs/2505.17327)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>The proliferation of Large Language Models (LLMs) in late 2022 has impacted academic writing, threatening credibility, and causing institutional uncertainty. We seek to determine the degree to which LLMs are used to generate critical text as opposed to being used for editing, such as checking for grammar errors or inappropriate phrasing. In our study, we analyze arXiv papers for stylistic segmentation, which we measure by varying a PELT threshold against a Bayesian classifier trained on GPT-regenerated text. We find that LLM-attributed language is not predictive of stylistic segmentation, suggesting that when authors use LLMs, they do so uniformly, reducing the risk of hallucinations being introduced into academic preprints.</li>
</ul>

<h3>Title: FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding</h3>
<ul>
<li><strong>Authors: </strong>Amit Agarwal, Srikant Panda, Kulbhushan Pachauri</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17330">https://arxiv.org/abs/2505.17330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17330">https://arxiv.org/pdf/2505.17330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17330]] FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding(https://arxiv.org/abs/2505.17330)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable and efficient model architecture for visually rich document understanding (VRDU) in few-shot settings. FS-DAG leverages domain-specific and language/vision specific backbones within a modular framework to adapt to diverse document types with minimal data. The model is robust to practical challenges such as handling OCR errors, misspellings, and domain shifts, which are critical in real-world deployments. FS-DAG is highly performant with less than 90M parameters, making it well-suited for complex real-world applications for Information Extraction (IE) tasks where computational resources are limited. We demonstrate FS-DAG's capability through extensive experiments for information extraction task, showing significant improvements in convergence speed and performance compared to state-of-the-art methods. Additionally, this work highlights the ongoing progress in developing smaller, more efficient models that do not compromise on performance. Code : this https URL</li>
</ul>

<h3>Title: ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training</h3>
<ul>
<li><strong>Authors: </strong>Maryam Dialameh, Rezaul Karim, Hossein Rajabzadeh, Omar Mohamed Awad, Hyock Ju Kwon, Boxing Chen, Walid Ahmed, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17331">https://arxiv.org/abs/2505.17331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17331">https://arxiv.org/pdf/2505.17331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17331]] ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training(https://arxiv.org/abs/2505.17331)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to improve both the training speed and inference throughput of LLaMA architectures while maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models into shared KV caching across certain layers, significantly reducing KV computational complexity while maintaining or improving language performance. Experimental results demonstrate that ECHO-LLaMA achieves up to 77\% higher token-per-second throughput during training, up to 16\% higher Model FLOPs Utilization (MFU), and up to 14\% lower loss when trained on an equal number of tokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\% higher test-time throughput compared to the baseline. By introducing a computationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable and cost-effective solution for pretraining and finetuning large language models, enabling faster and more resource-efficient training without compromising performance.</li>
</ul>

<h3>Title: SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use</h3>
<ul>
<li><strong>Authors: </strong>Hitesh Laxmichand Patel, Amit Agarwal, Arion Das, Bhargava Kumar, Srikant Panda, Priyaranjan Pattnayak, Taki Hasan Rafi, Tejaswini Kumar, Dong-Kyu Chae</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17332">https://arxiv.org/abs/2505.17332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17332">https://arxiv.org/pdf/2505.17332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17332]] SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use(https://arxiv.org/abs/2505.17332)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Enterprise customers are increasingly adopting Large Language Models (LLMs) for critical communication tasks, such as drafting emails, crafting sales pitches, and composing casual messages. Deploying such models across different regions requires them to understand diverse cultural and linguistic contexts and generate safe and respectful responses. For enterprise applications, it is crucial to mitigate reputational risks, maintain trust, and ensure compliance by effectively identifying and handling unsafe or offensive language. To address this, we introduce SweEval, a benchmark simulating real-world scenarios with variations in tone (positive or negative) and context (formal or informal). The prompts explicitly instruct the model to include specific swear words while completing the task. This benchmark evaluates whether LLMs comply with or resist such inappropriate instructions and assesses their alignment with ethical frameworks, cultural nuances, and language comprehension capabilities. In order to advance research in building ethically aligned AI systems for enterprise use and beyond, we release the dataset and code: this https URL.</li>
</ul>

<h3>Title: Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Xin You, Minghui Zhang, Hanxiao Zhang, Jie Yang, Nassir Navab</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17333">https://arxiv.org/abs/2505.17333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17333">https://arxiv.org/pdf/2505.17333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17333]] Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis(https://arxiv.org/abs/2505.17333)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Temporal modeling on regular respiration-induced motions is crucial to image-guided clinical applications. Existing methods cannot simulate temporal motions unless high-dose imaging scans including starting and ending frames exist simultaneously. However, in the preoperative data acquisition stage, the slight movement of patients may result in dynamic backgrounds between the first and last frames in a respiratory period. This additional deviation can hardly be removed by image registration, thus affecting the temporal modeling. To address that limitation, we pioneeringly simulate the regular motion process via the image-to-video (I2V) synthesis framework, which animates with the first frame to forecast future frames of a given length. Besides, to promote the temporal consistency of animated videos, we devise the Temporal Differential Diffusion Model to generate temporal differential fields, which measure the relative differential representations between adjacent frames. The prompt attention layer is devised for fine-grained differential fields, and the field augmented layer is adopted to better interact these fields with the I2V framework, promoting more accurate temporal variation of synthesized videos. Extensive results on ACDC cardiac and 4D Lung datasets reveal that our approach simulates 4D videos along the intrinsic motion trajectory, rivaling other competitive methods on perceptual similarity and temporal consistency. Codes will be available soon.</li>
</ul>

<h3>Title: Secure Parsing and Serializing with Separation Logic Applied to CBOR, CDDL, and COSE</h3>
<ul>
<li><strong>Authors: </strong>Tahina Ramananandro, Gabriel Ebner, Guido Martínez, Nikhil Swamy</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17335">https://arxiv.org/abs/2505.17335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17335">https://arxiv.org/pdf/2505.17335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17335]] Secure Parsing and Serializing with Separation Logic Applied to CBOR, CDDL, and COSE(https://arxiv.org/abs/2505.17335)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect</a></li>
<li><strong>Abstract: </strong>Incorrect handling of security-critical data formats, particularly in low-level languages, are the root cause of many security vulnerabilities. Provably correct parsing and serialization tools that target languages like C can help. Towards this end, we present PulseParse, a library of verified parser and serializer combinators for non-malleable binary formats. Specifications and proofs in PulseParse are in separation logic, offering a more abstract and compositional interface, with full support for data validation, parsing, and serialization. PulseParse also supports a class of recursive formats -- with a focus on security and handling adversarial inputs, we show how to parse such formats with only a constant amount of stack space. We use PulseParse at scale by providing the first formalization of CBOR, a recursive, binary data format standard, with growing adoption in various industrial standards. We prove that the deterministic fragment of CBOR is non-malleable and provide EverCBOR, a verified library in both C and Rust to validate, parse, and serialize CBOR objects implemented using PulseParse. Next, we provide the first formalization of CDDL, a schema definition language for CBOR. We identify well-formedness conditions on CDDL definitions that ensure that they yield unambiguous, non-malleable formats, and implement EverCDDL, a tool that checks that a CDDL definition is well-formed, and then produces verified parsers and serializers for it. To evaluate our work, we use EverCDDL to generate verified parsers and serializers for various security-critical applications. Notably, we build a formally verified implementation of COSE signing, a standard for cryptographically signed objects. We also use our toolchain to generate verified code for other standards specified in CDDL, including DICE Protection Environment, a secure boot protocol standard.</li>
</ul>

<h3>Title: Render-FM: A Foundation Model for Real-time Photorealistic Volumetric Rendering</h3>
<ul>
<li><strong>Authors: </strong>Zhongpai Gao, Meng Zheng, Benjamin Planche, Anwesa Choudhuri, Terrence Chen, Ziyan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17338">https://arxiv.org/abs/2505.17338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17338">https://arxiv.org/pdf/2505.17338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17338]] Render-FM: A Foundation Model for Real-time Photorealistic Volumetric Rendering(https://arxiv.org/abs/2505.17338)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Volumetric rendering of Computed Tomography (CT) scans is crucial for visualizing complex 3D anatomical structures in medical imaging. Current high-fidelity approaches, especially neural rendering techniques, require time-consuming per-scene optimization, limiting clinical applicability due to computational demands and poor generalizability. We propose Render-FM, a novel foundation model for direct, real-time volumetric rendering of CT scans. Render-FM employs an encoder-decoder architecture that directly regresses 6D Gaussian Splatting (6DGS) parameters from CT volumes, eliminating per-scan optimization through large-scale pre-training on diverse medical data. By integrating robust feature extraction with the expressive power of 6DGS, our approach efficiently generates high-quality, real-time interactive 3D visualizations across diverse clinical CT data. Experiments demonstrate that Render-FM achieves visual fidelity comparable or superior to specialized per-scan methods while drastically reducing preparation time from nearly an hour to seconds for a single inference step. This advancement enables seamless integration into real-time surgical planning and diagnostic workflows. The project page is: this https URL.</li>
</ul>

<h3>Title: A Multi-Head Attention Soft Random Forest for Interpretable Patient No-Show Prediction</h3>
<ul>
<li><strong>Authors: </strong>Ninda Nurseha Amalina, Kwadwo Boateng Ofori-Amanfo, Heungjo An</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17344">https://arxiv.org/abs/2505.17344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17344">https://arxiv.org/pdf/2505.17344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17344]] A Multi-Head Attention Soft Random Forest for Interpretable Patient No-Show Prediction(https://arxiv.org/abs/2505.17344)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Unattended scheduled appointments, defined as patient no-shows, adversely affect both healthcare providers and patients' health, disrupting the continuity of care, operational efficiency, and the efficient allocation of medical resources. Accurate predictive modelling is needed to reduce the impact of no-shows. Although machine learning methods, such as logistic regression, random forest models, and decision trees, are widely used in predicting patient no-shows, they often rely on hard decision splits and static feature importance, limiting their adaptability to specific or complex patient behaviors. To address this limitation, we propose a new hybrid Multi-Head Attention Soft Random Forest (MHASRF) model that integrates attention mechanisms into a random forest model using probabilistic soft splitting instead of hard splitting. The MHASRF model assigns attention weights differently across the trees, enabling attention on specific patient behaviors. The model exhibited 93.56% accuracy, 93.67% precision, 93.56% recall, and a 93.59% F1 score, surpassing the performance of decision tree, logistic regression, random forest, and naive Bayes models. Furthermore, MHASRF was able to identify key predictors of patient no-shows using two levels of feature importance (tree level and attention mechanism level), offering deeper insights into patient no-show predictors. The proposed model is a robust, adaptable, and interpretable method for predicting patient no-shows that will help healthcare providers in optimizing resources.</li>
</ul>

<h3>Title: FLEX: A Backbone for Diffusion-Based Modeling of Spatio-temporal Physical Systems</h3>
<ul>
<li><strong>Authors: </strong>N. Benjamin Erichson, Vinicius Mikuni, Dongwei Lyu, Yang Gao, Omri Azencot, Soon Hoe Lim, Michael W. Mahoney</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17351">https://arxiv.org/abs/2505.17351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17351">https://arxiv.org/pdf/2505.17351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17351]] FLEX: A Backbone for Diffusion-Based Modeling of Spatio-temporal Physical Systems(https://arxiv.org/abs/2505.17351)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>We introduce FLEX (FLow EXpert), a backbone architecture for generative modeling of spatio-temporal physical systems using diffusion models. FLEX operates in the residual space rather than on raw data, a modeling choice that we motivate theoretically, showing that it reduces the variance of the velocity field in the diffusion model, which helps stabilize training. FLEX integrates a latent Transformer into a U-Net with standard convolutional ResNet layers and incorporates a redesigned skip connection scheme. This hybrid design enables the model to capture both local spatial detail and long-range dependencies in latent space. To improve spatio-temporal conditioning, FLEX uses a task-specific encoder that processes auxiliary inputs such as coarse or past snapshots. Weak conditioning is applied to the shared encoder via skip connections to promote generalization, while strong conditioning is applied to the decoder through both skip and bottleneck features to ensure reconstruction fidelity. FLEX achieves accurate predictions for super-resolution and forecasting tasks using as few as two reverse diffusion steps. It also produces calibrated uncertainty estimates through sampling. Evaluations on high-resolution 2D turbulence data show that FLEX outperforms strong baselines and generalizes to out-of-distribution settings, including unseen Reynolds numbers, physical observables (e.g., fluid flow velocity fields), and boundary conditions.</li>
</ul>

<h3>Title: Alignment and Safety of Diffusion Models via Reinforcement Learning and Reward Modeling: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Preeti Lamba, Kiran Ravish, Ankita Kushwaha, Pawan Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17352">https://arxiv.org/abs/2505.17352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17352">https://arxiv.org/pdf/2505.17352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17352]] Alignment and Safety of Diffusion Models via Reinforcement Learning and Reward Modeling: A Survey(https://arxiv.org/abs/2505.17352)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as leading generative models for images and other modalities, but aligning their outputs with human preferences and safety constraints remains a critical challenge. This thesis proposal investigates methods to align diffusion models using reinforcement learning (RL) and reward modeling. We survey recent advances in fine-tuning text-to-image diffusion models with human feedback, including reinforcement learning from human and AI feedback, direct preference optimization, and differentiable reward approaches. We classify these methods based on the type of feedback (human, automated, binary or ranked preferences), the fine-tuning technique (policy gradient, reward-weighted likelihood, direct backpropagation, etc.), and their efficiency and safety outcomes. We compare key algorithms and frameworks, highlighting how they improve alignment with user intent or safety standards, and discuss inter-relationships such as how newer methods build on or diverge from earlier ones. Based on the survey, we identify five promising research directions for the next two years: (1) multi-objective alignment with combined rewards, (2) efficient human feedback usage and active learning, (3) robust safety alignment against adversarial inputs, (4) continual and online alignment of diffusion models, and (5) interpretable and trustworthy reward modeling for generative images. Each direction is elaborated with its problem statement, challenges, related work, and a proposed research plan. The proposal is organized as a comprehensive document with literature review, comparative tables of methods, and detailed research plans, aiming to contribute new insights and techniques for safer and value-aligned diffusion-based generative AI.</li>
</ul>

<h3>Title: Dual Ascent Diffusion for Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Minseo Kim, Axel Levy, Gordon Wetzstein</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17353">https://arxiv.org/abs/2505.17353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17353">https://arxiv.org/pdf/2505.17353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17353]] Dual Ascent Diffusion for Inverse Problems(https://arxiv.org/abs/2505.17353)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Ill-posed inverse problems are fundamental in many domains, ranging from astrophysics to medical imaging. Emerging diffusion models provide a powerful prior for solving these problems. Existing maximum-a-posteriori (MAP) or posterior sampling approaches, however, rely on different computational approximations, leading to inaccurate or suboptimal samples. To address this issue, we introduce a new approach to solving MAP problems with diffusion model priors using a dual ascent optimization framework. Our framework achieves better image quality as measured by various metrics for image restoration problems, it is more robust to high levels of measurement noise, it is faster, and it estimates solutions that represent the observations more faithfully than the state of the art.</li>
</ul>

<h3>Title: Adversarial Robustness of Nonparametric Regression</h3>
<ul>
<li><strong>Authors: </strong>Parsa Moradi, Hanzaleh Akabrinodehi, Mohammad Ali Maddah-Ali</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17356">https://arxiv.org/abs/2505.17356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17356">https://arxiv.org/pdf/2505.17356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17356]] Adversarial Robustness of Nonparametric Regression(https://arxiv.org/abs/2505.17356)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we investigate the adversarial robustness of regression, a fundamental problem in machine learning, under the setting where an adversary can arbitrarily corrupt a subset of the input data. While the robustness of parametric regression has been extensively studied, its nonparametric counterpart remains largely unexplored. We characterize the adversarial robustness in nonparametric regression, assuming the regression function belongs to the second-order Sobolev space (i.e., it is square integrable up to its second derivative). The contribution of this paper is two-fold: (i) we establish a minimax lower bound on the estimation error, revealing a fundamental limit that no estimator can overcome, and (ii) we show that, perhaps surprisingly, the classical smoothing spline estimator, when properly regularized, exhibits robustness against adversarial corruption. These results imply that if $o(n)$ out of $n$ samples are corrupted, the estimation error of the smoothing spline vanishes as $n \to \infty$. On the other hand, when a constant fraction of the data is corrupted, no estimator can guarantee vanishing estimation error, implying the optimality of the smoothing spline in terms of maximum tolerable number of corrupted samples.</li>
</ul>

<h3>Title: Graph Attention Neural Network for Botnet Detection: Evaluating Autoencoder, VAE and PCA-Based Dimension Reduction</h3>
<ul>
<li><strong>Authors: </strong>Hassan Wasswa, Hussein Abbass, Timothy Lynar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17357">https://arxiv.org/abs/2505.17357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17357">https://arxiv.org/pdf/2505.17357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17357]] Graph Attention Neural Network for Botnet Detection: Evaluating Autoencoder, VAE and PCA-Based Dimension Reduction(https://arxiv.org/abs/2505.17357)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>With the rise of IoT-based botnet attacks, researchers have explored various learning models for detection, including traditional machine learning, deep learning, and hybrid approaches. A key advancement involves deploying attention mechanisms to capture long-term dependencies among features, significantly improving detection accuracy. However, most models treat attack instances independently, overlooking inter-instance relationships. Graph Neural Networks (GNNs) address this limitation by learning an embedding space via iterative message passing where similar instances are placed closer based on node features and relationships, enhancing classification performance. To further improve detection, attention mechanisms have been embedded within GNNs, leveraging both long-range dependencies and inter-instance connections. However, transforming the high dimensional IoT attack datasets into a graph structured dataset poses challenges, such as large graph structures leading computational overhead. To mitigate this, this paper proposes a framework that first reduces dimensionality of the NetFlow-based IoT attack dataset before transforming it into a graph dataset. We evaluate three dimension reduction techniques--Variational Autoencoder (VAE-encoder), classical autoencoder (AE-encoder), and Principal Component Analysis (PCA)--and compare their effects on a Graph Attention neural network (GAT) model for botnet attack detection</li>
</ul>

<h3>Title: Repurposing Marigold for Zero-Shot Metric Depth Estimation via Defocus Blur Cues</h3>
<ul>
<li><strong>Authors: </strong>Chinmay Talegaonkar, Nikhil Gandudi Suresh, Zachary Novack, Yash Belhe, Priyanka Nagasamudra, Nicholas Antipa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17358">https://arxiv.org/abs/2505.17358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17358">https://arxiv.org/pdf/2505.17358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17358]] Repurposing Marigold for Zero-Shot Metric Depth Estimation via Defocus Blur Cues(https://arxiv.org/abs/2505.17358)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent monocular metric depth estimation (MMDE) methods have made notable progress towards zero-shot generalization. However, they still exhibit a significant performance drop on out-of-distribution datasets. We address this limitation by injecting defocus blur cues at inference time into Marigold, a \textit{pre-trained} diffusion model for zero-shot, scale-invariant monocular depth estimation (MDE). Our method effectively turns Marigold into a metric depth predictor in a training-free manner. To incorporate defocus cues, we capture two images with a small and a large aperture from the same viewpoint. To recover metric depth, we then optimize the metric depth scaling parameters and the noise latents of Marigold at inference time using gradients from a loss function based on the defocus-blur image formation model. We compare our method against existing state-of-the-art zero-shot MMDE methods on a self-collected real dataset, showing quantitative and qualitative improvements.</li>
</ul>

<h3>Title: Towards VM Rescheduling Optimization Through Deep Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Xianzhong Ding, Yunkai Zhang, Binbin Chen, Donghao Ying, Tieying Zhang, Jianjun Chen, Lei Zhang, Alberto Cerpa, Wan Du</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17359">https://arxiv.org/abs/2505.17359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17359">https://arxiv.org/pdf/2505.17359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17359]] Towards VM Rescheduling Optimization Through Deep Reinforcement Learning(https://arxiv.org/abs/2505.17359)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Modern industry-scale data centers need to manage a large number of virtual machines (VMs). Due to the continual creation and release of VMs, many small resource fragments are scattered across physical machines (PMs). To handle these fragments, data centers periodically reschedule some VMs to alternative PMs, a practice commonly referred to as VM rescheduling. Despite the increasing importance of VM rescheduling as data centers grow in size, the problem remains understudied. We first show that, unlike most combinatorial optimization tasks, the inference time of VM rescheduling algorithms significantly influences their performance, due to dynamic VM state changes during this period. This causes existing methods to scale poorly. Therefore, we develop a reinforcement learning system for VM rescheduling, VM2RL, which incorporates a set of customized techniques, such as a two-stage framework that accommodates diverse constraints and workload conditions, a feature extraction module that captures relational information specific to rescheduling, as well as a risk-seeking evaluation enabling users to optimize the trade-off between latency and accuracy. We conduct extensive experiments with data from an industry-scale data center. Our results show that VM2RL can achieve a performance comparable to the optimal solution but with a running time of seconds. Code and datasets are open-sourced: this https URL, this https URL.</li>
</ul>

<h3>Title: A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit</h3>
<ul>
<li><strong>Authors: </strong>Zafarullah Mahmood, Soliman Ali, Jiading Zhu, Mohamed Abdelwahab, Michelle Yu Collins, Sihan Chen, Yi Cheng Zhao, Jodi Wolff, Osnat Melamed, Nadia Minian, Marta Maslej, Carolynne Cooper, Matt Ratto, Peter Selby, Jonathan Rose</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17362">https://arxiv.org/abs/2505.17362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17362">https://arxiv.org/pdf/2505.17362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17362]] A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit(https://arxiv.org/abs/2505.17362)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The conversational capabilities of Large Language Models (LLMs) suggest that they may be able to perform as automated talk therapists. It is crucial to know if these systems would be effective and adhere to known standards. We present a counsellor chatbot that focuses on motivating tobacco smokers to quit smoking. It uses a state-of-the-art LLM and a widely applied therapeutic approach called Motivational Interviewing (MI), and was evolved in collaboration with clinician-scientists with expertise in MI. We also describe and validate an automated assessment of both the chatbot's adherence to MI and client responses. The chatbot was tested on 106 participants, and their confidence that they could succeed in quitting smoking was measured before the conversation and one week later. Participants' confidence increased by an average of 1.7 on a 0-10 scale. The automated assessment of the chatbot showed adherence to MI standards in 98% of utterances, higher than human counsellors. The chatbot scored well on a participant-reported metric of perceived empathy but lower than typical human counsellors. Furthermore, participants' language indicated a good level of motivation to change, a key goal in MI. These results suggest that the automation of talk therapy with a modern LLM has promise.</li>
</ul>

<h3>Title: Are GNNs Worth the Effort for IoT Botnet Detection? A Comparative Study of VAE-GNN vs. ViT-MLP and VAE-MLP Approaches</h3>
<ul>
<li><strong>Authors: </strong>Hassan Wasswa, Hussein Abbass, Timothy Lynar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17363">https://arxiv.org/abs/2505.17363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17363">https://arxiv.org/pdf/2505.17363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17363]] Are GNNs Worth the Effort for IoT Botnet Detection? A Comparative Study of VAE-GNN vs. ViT-MLP and VAE-MLP Approaches(https://arxiv.org/abs/2505.17363)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, transformer</a></li>
<li><strong>Abstract: </strong>Due to the exponential rise in IoT-based botnet attacks, researchers have explored various advanced techniques for both dimensionality reduction and attack detection to enhance IoT security. Among these, Variational Autoencoders (VAE), Vision Transformers (ViT), and Graph Neural Networks (GNN), including Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT), have garnered significant research attention in the domain of attack detection. This study evaluates the effectiveness of four state-of-the-art deep learning architectures for IoT botnet detection: a VAE encoder with a Multi-Layer Perceptron (MLP), a VAE encoder with a GCN, a VAE encoder with a GAT, and a ViT encoder with an MLP. The evaluation is conducted on a widely studied IoT benchmark dataset--the N-BaIoT dataset for both binary and multiclass tasks. For the binary classification task, all models achieved over 99.93% in accuracy, recall, precision, and F1-score, with no notable differences in performance. In contrast, for the multiclass classification task, GNN-based models showed significantly lower performance compared to VAE-MLP and ViT-MLP, with accuracies of 86.42%, 89.46%, 99.72%, and 98.38% for VAE-GCN, VAE-GAT, VAE-MLP, and ViT-MLP, respectively.</li>
</ul>

<h3>Title: EVM-Fusion: An Explainable Vision Mamba Architecture with Neural Algorithmic Fusion</h3>
<ul>
<li><strong>Authors: </strong>Zichuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17367">https://arxiv.org/abs/2505.17367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17367">https://arxiv.org/pdf/2505.17367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17367]] EVM-Fusion: An Explainable Vision Mamba Architecture with Neural Algorithmic Fusion(https://arxiv.org/abs/2505.17367)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Medical image classification is critical for clinical decision-making, yet demands for accuracy, interpretability, and generalizability remain challenging. This paper introduces EVM-Fusion, an Explainable Vision Mamba architecture featuring a novel Neural Algorithmic Fusion (NAF) mechanism for multi-organ medical image classification. EVM-Fusion leverages a multipath design, where DenseNet and U-Net based pathways, enhanced by Vision Mamba (Vim) modules, operate in parallel with a traditional feature pathway. These diverse features are dynamically integrated via a two-stage fusion process: cross-modal attention followed by the iterative NAF block, which learns an adaptive fusion algorithm. Intrinsic explainability is embedded through path-specific spatial attention, Vim {\Delta}-value maps, traditional feature SE-attention, and cross-modal attention weights. Experiments on a diverse 9-class multi-organ medical image dataset demonstrate EVM-Fusion's strong classification performance, achieving 99.75% test accuracy and provide multi-faceted insights into its decision-making process, highlighting its potential for trustworthy AI in medical diagnostics.</li>
</ul>

<h3>Title: FRIREN: Beyond Trajectories -- A Spectral Lens on Time</h3>
<ul>
<li><strong>Authors: </strong>Qilin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17370">https://arxiv.org/abs/2505.17370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17370">https://arxiv.org/pdf/2505.17370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17370]] FRIREN: Beyond Trajectories -- A Spectral Lens on Time(https://arxiv.org/abs/2505.17370)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Long-term time-series forecasting (LTSF) models are often presented as general-purpose solutions that can be applied across domains, implicitly assuming that all data is pointwise predictable. Using chaotic systems such as Lorenz-63 as a case study, we argue that geometric structure - not pointwise prediction - is the right abstraction for a dynamic-agnostic foundational model. Minimizing the Wasserstein-2 distance (W2), which captures geometric changes, and providing a spectral view of dynamics are essential for long-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via Interpretable Eigen-networks), implements an augmented normalizing-flow block that embeds data into a normally distributed latent representation. It then generates a W2-efficient optimal path that can be decomposed into rotation, scaling, inverse rotation, and translation. This architecture yields locally generated, geometry-preserving predictions that are independent of the underlying dynamics, and a global spectral representation that functions as a finite Koopman operator with a small modification. This enables practitioners to identify which modes grow, decay, or oscillate, both locally and system-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on Lorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE 27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out of 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out), FRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170, outperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065. FRIREN is also competitive on standard LTSF datasets such as ETT and Weather. By connecting modern generative flows with classical spectral analysis, FRIREN makes long-term forecasting both accurate and interpretable, setting a new benchmark for LTSF model design.</li>
</ul>

<h3>Title: AI-Augmented LLMs Achieve Therapist-Level Responses in Motivational Interviewing</h3>
<ul>
<li><strong>Authors: </strong>Yinghui Huang, Yuxuan Jiang, Hui Liu, Yixin Cai, Weiqing Li, Xiangen Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17380">https://arxiv.org/abs/2505.17380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17380">https://arxiv.org/pdf/2505.17380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17380]] AI-Augmented LLMs Achieve Therapist-Level Responses in Motivational Interviewing(https://arxiv.org/abs/2505.17380)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) like GPT-4 show potential for scaling motivational interviewing (MI) in addiction care, but require systematic evaluation of therapeutic capabilities. We present a computational framework assessing user-perceived quality (UPQ) through expected and unexpected MI behaviors. Analyzing human therapist and GPT-4 MI sessions via human-AI collaboration, we developed predictive models integrating deep learning and explainable AI to identify 17 MI-consistent (MICO) and MI-inconsistent (MIIN) behavioral metrics. A customized chain-of-thought prompt improved GPT-4's MI performance, reducing inappropriate advice while enhancing reflections and empathy. Although GPT-4 remained marginally inferior to therapists overall, it demonstrated superior advice management capabilities. The model achieved measurable quality improvements through prompt engineering, yet showed limitations in addressing complex emotional nuances. This framework establishes a pathway for optimizing LLM-based therapeutic tools through targeted behavioral metric analysis and human-AI co-evaluation. Findings highlight both the scalability potential and current constraints of LLMs in clinical communication applications.</li>
</ul>

<h3>Title: Variational Autoencoding Discrete Diffusion with Enhanced Dimensional Correlations Modeling</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Xie, Shuchen Xue, Zijin Feng, Tianyang Hu, Jiacheng Sun, Zhenguo Li, Cheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17384">https://arxiv.org/abs/2505.17384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17384">https://arxiv.org/pdf/2505.17384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17384]] Variational Autoencoding Discrete Diffusion with Enhanced Dimensional Correlations Modeling(https://arxiv.org/abs/2505.17384)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models have recently shown great promise for modeling complex discrete data, with masked diffusion models (MDMs) offering a compelling trade-off between quality and generation speed. MDMs denoise by progressively unmasking multiple dimensions from an all-masked input, but their performance can degrade when using few denoising steps due to limited modeling of inter-dimensional dependencies. In this paper, we propose Variational Autoencoding Discrete Diffusion (VADD), a novel framework that enhances discrete diffusion with latent variable modeling to implicitly capture correlations among dimensions. By introducing an auxiliary recognition model, VADD enables stable training via variational lower bounds maximization and amortized inference over the training set. Our approach retains the efficiency of traditional MDMs while significantly improving sample quality, especially when the number of denoising steps is small. Empirical results on 2D toy data, pixel-level image generation, and text generation demonstrate that VADD consistently outperforms MDM baselines.</li>
</ul>

<h3>Title: WiNGPT-3.0 Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Boqin Zhuang, Chenxiao Song, Huitong Lu, Jiacheng Qiao, Mingqian Liu, Mingxing Yu, Ping Hong, Rui Li, Xiaoxia Song, Xiangjun Xu, Xu Chen, Yaoyao Ma, Yujie Gao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17387">https://arxiv.org/abs/2505.17387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17387">https://arxiv.org/pdf/2505.17387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17387]] WiNGPT-3.0 Technical Report(https://arxiv.org/abs/2505.17387)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Current Large Language Models (LLMs) exhibit significant limitations, notably in structured, interpretable, and verifiable medical reasoning, alongside practical deployment challenges related to computational resources and data privacy. This report focused on the development of WiNGPT-3.0, the 32-billion parameter LLMs, engineered with the objective of enhancing its capacity for medical reasoning and exploring its potential for effective integration within healthcare IT infrastructures. The broader aim is to advance towards clinically applicable models. The approach involved a multi-stage training pipeline tailored for general, medical, and clinical reasoning. This pipeline incorporated supervised fine-tuning (SFT) and reinforcement learning (RL), leveraging curated Long Chain-of-Thought (CoT) datasets, auxiliary reward models, and an evidence-based diagnostic chain simulation. WiNGPT-3.0 demonstrated strong performance: specific model variants achieved scores of 66.6 on MedCalc and 87.1 on MedQA-USMLE. Furthermore, targeted training improved performance on a clinical reasoning task from a baseline score of 58.1 to 62.5. These findings suggest that reinforcement learning, even when applied with a limited dataset of only a few thousand examples, can enhance medical reasoning accuracy. Crucially, this demonstration of RL's efficacy with limited data and computation paves the way for more trustworthy and practically deployable LLMs within clinical workflows and health information infrastructures.</li>
</ul>

<h3>Title: Measuring diversity of synthetic prompts and data generated with fine-grained persona prompting</h3>
<ul>
<li><strong>Authors: </strong>Gauri Kambhatla, Chantal Shaib, Venkata Govindarajan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17390">https://arxiv.org/abs/2505.17390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17390">https://arxiv.org/pdf/2505.17390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17390]] Measuring diversity of synthetic prompts and data generated with fine-grained persona prompting(https://arxiv.org/abs/2505.17390)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-grained personas have recently been used for generating 'diverse' synthetic data for pre-training and supervised fine-tuning of Large Language Models (LLMs). In this work, we measure the diversity of persona-driven synthetically generated prompts and responses with a suite of lexical diversity and redundancy metrics. Firstly, we find that synthetic prompts/instructions are significantly less diverse than human-written ones. Next, we sample responses from LLMs of different sizes with fine-grained and coarse persona descriptions to investigate how much fine-grained detail in persona descriptions contribute to generated text diversity. We find that while persona-prompting does improve lexical diversity (especially with larger models), fine-grained detail in personas doesn't increase diversity noticeably.</li>
</ul>

<h3>Title: Curriculum Guided Reinforcement Learning for Efficient Multi Hop Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuelyu Ji, Rui Meng, Zhuochun Li, Daqing He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17391">https://arxiv.org/abs/2505.17391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17391">https://arxiv.org/pdf/2505.17391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17391]] Curriculum Guided Reinforcement Learning for Efficient Multi Hop Retrieval Augmented Generation(https://arxiv.org/abs/2505.17391)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) grounds large language models (LLMs) in up-to-date external evidence, yet existing multi-hop RAG pipelines still issue redundant subqueries, explore too shallowly, or wander through overly long search chains. We introduce EVO-RAG, a curriculum-guided reinforcement learning framework that evolves a query-rewriting agent from broad early-stage exploration to concise late-stage refinement. EVO-RAG couples a seven-factor, step-level reward vector (covering relevance, redundancy, efficiency, and answer correctness) with a time-varying scheduler that reweights these signals as the episode unfolds. The agent is trained with Direct Preference Optimization over a multi-head reward model, enabling it to learn when to search, backtrack, answer, or refuse. Across four multi-hop QA benchmarks (HotpotQA, 2WikiMultiHopQA, MuSiQue, and Bamboogle), EVO-RAG boosts Exact Match by up to 4.6 points over strong RAG baselines while trimming average retrieval depth by 15 %. Ablation studies confirm the complementary roles of curriculum staging and dynamic reward scheduling. EVO-RAG thus offers a general recipe for building reliable, cost-effective multi-hop RAG systems.</li>
</ul>

<h3>Title: Dual-sensing driving detection model</h3>
<ul>
<li><strong>Authors: </strong>Leon C.C.K, Zeng Hui</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17392">https://arxiv.org/abs/2505.17392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17392">https://arxiv.org/pdf/2505.17392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17392]] Dual-sensing driving detection model(https://arxiv.org/abs/2505.17392)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, a novel dual-sensing driver fatigue detection method combining computer vision and physiological signal analysis is proposed. The system exploits the complementary advantages of the two sensing modalities and breaks through the limitations of existing single-modality methods. We introduce an innovative architecture that combines real-time facial feature analysis with physiological signal processing, combined with advanced fusion strategies, for robust fatigue detection. The system is designed to run efficiently on existing hardware while maintaining high accuracy and reliability. Through comprehensive experiments, we demonstrate that our method outperforms traditional methods in both controlled environments and real-world conditions, while maintaining high accuracy. The practical applicability of the system has been verified through extensive tests in various driving scenarios and shows great potential in reducing fatigue-related accidents. This study contributes to the field by providing a more reliable, cost-effective, and humane solution for driver fatigue detection.</li>
</ul>

<h3>Title: Wildfire Detection Using Vision Transformer with the Wildfire Dataset</h3>
<ul>
<li><strong>Authors: </strong>Gowtham Raj Vuppari, Navarun Gupta, Ahmed El-Sayed, Xingguo Xiong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17395">https://arxiv.org/abs/2505.17395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17395">https://arxiv.org/pdf/2505.17395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17395]] Wildfire Detection Using Vision Transformer with the Wildfire Dataset(https://arxiv.org/abs/2505.17395)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The critical need for sophisticated detection techniques has been highlighted by the rising frequency and intensity of wildfires in the US, especially in California. In 2023, wildfires caused 130 deaths nationwide, the highest since 1990. In January 2025, Los Angeles wildfires which included the Palisades and Eaton fires burnt approximately 40,000 acres and 12,000 buildings, and caused loss of human lives. The devastation underscores the urgent need for effective detection and prevention strategies. Deep learning models, such as Vision Transformers (ViTs), can enhance early detection by processing complex image data with high accuracy. However, wildfire detection faces challenges, including the availability of high-quality, real-time data. Wildfires often occur in remote areas with limited sensor coverage, and environmental factors like smoke and cloud cover can hinder detection. Additionally, training deep learning models is computationally expensive, and issues like false positives/negatives and scaling remain concerns. Integrating detection systems with real-time alert mechanisms also poses difficulties. In this work, we used the wildfire dataset consisting of 10.74 GB high-resolution images categorized into 'fire' and 'nofire' classes is used for training the ViT model. To prepare the data, images are resized to 224 x 224 pixels, converted into tensor format, and normalized using ImageNet statistics.</li>
</ul>

<h3>Title: FullFront: Benchmarking MLLMs Across the Full Front-End Engineering Workflow</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Sun, Huichen Will Wang, Jiawei Gu, Linjie Li, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17399">https://arxiv.org/abs/2505.17399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17399">https://arxiv.org/pdf/2505.17399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17399]] FullFront: Benchmarking MLLMs Across the Full Front-End Engineering Workflow(https://arxiv.org/abs/2505.17399)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Front-end engineering involves a complex workflow where engineers conceptualize designs, translate them into code, and iteratively refine the implementation. While recent benchmarks primarily focus on converting visual designs to code, we present FullFront, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) \textbf{across the full front-end development pipeline}. FullFront assesses three fundamental tasks that map directly to the front-end engineering pipeline: Webpage Design (conceptualization phase), Webpage Perception QA (comprehension of visual organization and elements), and Webpage Code Generation (implementation phase). Unlike existing benchmarks that use either scraped websites with bloated code or oversimplified LLM-generated HTML, FullFront employs a novel, two-stage process to transform real-world webpages into clean, standardized HTML while maintaining diverse visual designs and avoiding copyright issues. Extensive testing of state-of-the-art MLLMs reveals significant limitations in page perception, code generation (particularly for image handling and layout), and interaction implementation. Our results quantitatively demonstrate performance disparities across models and tasks, and highlight a substantial gap between current MLLM capabilities and human expert performance in front-end engineering. The FullFront benchmark and code are available in this https URL.</li>
</ul>

<h3>Title: Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention</h3>
<ul>
<li><strong>Authors: </strong>Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Yikang Yang, Yajie Bao, Jiachen Qian, Siyu Zhu, Philip Torr, Xun Cao, Yao Yao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17412">https://arxiv.org/abs/2505.17412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17412">https://arxiv.org/pdf/2505.17412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17412]] Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention(https://arxiv.org/abs/2505.17412)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Generating high resolution 3D shapes using volumetric representations such as Signed Distance Functions presents substantial computational and memory challenges. We introduce Direct3D S2, a scalable 3D generation framework based on sparse volumes that achieves superior output quality with dramatically reduced training costs. Our key innovation is the Spatial Sparse Attention mechanism, which greatly enhances the efficiency of Diffusion Transformer computations on sparse volumetric data. SSA allows the model to effectively process large token sets within sparse volumes, significantly reducing computational overhead and achieving a 3.9x speedup in the forward pass and a 9.6x speedup in the backward pass. Our framework also includes a variational autoencoder that maintains a consistent sparse volumetric format across input, latent, and output stages. Compared to previous methods with heterogeneous representations in 3D VAE, this unified design significantly improves training efficiency and stability. Our model is trained on public available datasets, and experiments demonstrate that Direct3D S2 not only surpasses state-of-the-art methods in generation quality and efficiency, but also enables training at 1024 resolution using only 8 GPUs, a task typically requiring at least 32 GPUs for volumetric representations at 256 resolution, thus making gigascale 3D generation both practical and accessible. Project page: this https URL.</li>
</ul>

<h3>Title: Conversations: Love Them, Hate Them, Steer Them</h3>
<ul>
<li><strong>Authors: </strong>Niranjan Chebrolu, Gerard Christopher Yeo, Kokil Jaidka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17413">https://arxiv.org/abs/2505.17413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17413">https://arxiv.org/pdf/2505.17413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17413]] Conversations: Love Them, Hate Them, Steer Them(https://arxiv.org/abs/2505.17413)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable method for controlling specific emotional attributes in LLMs, contributing to developing more aligned and empathetic conversational AI.</li>
</ul>

<h3>Title: LLM-BSCVM: An LLM-Based Blockchain Smart Contract Vulnerability Management Framework</h3>
<ul>
<li><strong>Authors: </strong>Yanli Jin, Chunpei Li, Peng Fan, Peng Liu, Xianxian Li, Chen Liu, Wangjie Qiu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17416">https://arxiv.org/abs/2505.17416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17416">https://arxiv.org/pdf/2505.17416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17416]] LLM-BSCVM: An LLM-Based Blockchain Smart Contract Vulnerability Management Framework(https://arxiv.org/abs/2505.17416)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Smart contracts are a key component of the Web 3.0 ecosystem, widely applied in blockchain services and decentralized applications. However, the automated execution feature of smart contracts makes them vulnerable to potential attacks due to inherent flaws, which can lead to severe security risks and financial losses, even threatening the integrity of the entire decentralized finance system. Currently, research on smart contract vulnerabilities has evolved from traditional program analysis methods to deep learning techniques, with the gradual introduction of Large Language Models. However, existing studies mainly focus on vulnerability detection, lacking systematic cause analysis and Vulnerability Repair. To address this gap, we propose LLM-BSCVM, a Large Language Model-based smart contract vulnerability management framework, designed to provide end-to-end vulnerability detection, analysis, repair, and evaluation capabilities for Web 3.0 ecosystem. LLM-BSCVM combines retrieval-augmented generation technology and multi-agent collaboration, introducing a three-stage method of Decompose-Retrieve-Generate. This approach enables smart contract vulnerability management through the collaborative efforts of six intelligent agents, specifically: vulnerability detection, cause analysis, repair suggestion generation, risk assessment, vulnerability repair, and patch evaluation. Experimental results demonstrate that LLM-BSCVM achieves a vulnerability detection accuracy and F1 score exceeding 91\% on benchmark datasets, comparable to the performance of state-of-the-art (SOTA) methods, while reducing the false positive rate from 7.2\% in SOTA methods to 5.1\%, thus enhancing the reliability of vulnerability management. Furthermore, LLM-BSCVM supports continuous security monitoring and governance of smart contracts through a knowledge base hot-swapping dynamic update mechanism.</li>
</ul>

<h3>Title: DASH: Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies</h3>
<ul>
<li><strong>Authors: </strong>Ning Yang, Fangxin Liu, Junjie Wang, Tao Yang, Kan Liu, Haibing Guan, Li Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17420">https://arxiv.org/abs/2505.17420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17420">https://arxiv.org/pdf/2505.17420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17420]] DASH: Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies(https://arxiv.org/abs/2505.17420)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable performance across a wide range of NLP tasks. However, their substantial inference cost poses a major barrier to real-world deployment, especially in latency-sensitive scenarios. To address this challenge, we propose \textbf{DASH}, an adaptive layer-skipping framework that dynamically selects computation paths conditioned on input characteristics. We model the skipping process as a Markov Decision Process (MDP), enabling fine-grained token-level decisions based on intermediate representations. To mitigate potential performance degradation caused by skipping, we introduce a lightweight compensation mechanism that injects differential rewards into the decision process. Furthermore, we design an asynchronous execution strategy that overlaps layer computation with policy evaluation to minimize runtime overhead. Experiments on multiple LLM architectures and NLP benchmarks show that our method achieves significant inference acceleration while maintaining competitive task performance, outperforming existing methods.</li>
</ul>

<h3>Title: Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads</h3>
<ul>
<li><strong>Authors: </strong>Wei Jie Yeo, Rui Mao, Moloud Abdar, Erik Cambria, Ranjan Satapathy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17425">https://arxiv.org/abs/2505.17425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17425">https://arxiv.org/pdf/2505.17425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17425]] Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads(https://arxiv.org/abs/2505.17425)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multimodal models like CLIP have gained significant attention due to their remarkable zero-shot performance across various tasks. However, studies have revealed that CLIP can inadvertently learn spurious associations between target variables and confounding factors. To address this, we introduce \textsc{Locate-Then-Correct} (LTC), a contrastive framework that identifies spurious attention heads in Vision Transformers via mechanistic insights and mitigates them through targeted ablation. Furthermore, LTC identifies salient, task-relevant attention heads, enabling the integration of discriminative features through orthogonal projection to improve classification performance. We evaluate LTC on benchmarks with inherent background and gender biases, achieving over a $>50\%$ gain in worst-group accuracy compared to non-training post-hoc baselines. Additionally, we visualize the representation of selected heads and find that the presented interpretation corroborates our contrastive mechanism for identifying both spurious and salient attention heads. Code available at this https URL.</li>
</ul>

<h3>Title: T$^2$: An Adaptive Test-Time Scaling Strategy for Contextual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Zhengyi Zhao, Shubo Zhang, Zezhong Wang, Huimin Wang, Yutian Zhao, Bin Liang, Yefeng Zheng, Binyang Li, Kam-Fai Wong, Xian Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17427">https://arxiv.org/abs/2505.17427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17427">https://arxiv.org/pdf/2505.17427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17427]] T$^2$: An Adaptive Test-Time Scaling Strategy for Contextual Question Answering(https://arxiv.org/abs/2505.17427)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) have demonstrated remarkable performance in Contextual Question Answering (CQA). However, prior approaches typically employ elaborate reasoning strategies regardless of question complexity, leading to low adaptability. Recent efficient test-time scaling methods introduce budget constraints or early stop mechanisms to avoid overthinking for straightforward questions. But they add human bias to the reasoning process and fail to leverage models' inherent reasoning capabilities. To address these limitations, we present T$^2$: Think-to-Think, a novel framework that dynamically adapts reasoning depth based on question complexity. T$^2$ leverages the insight that if an LLM can effectively solve similar questions using specific reasoning strategies, it can apply the same strategy to the original question. This insight enables to adoption of concise reasoning for straightforward questions while maintaining detailed analysis for complex problems. T$^2$ works through four key steps: decomposing questions into structural elements, generating similar examples with candidate reasoning strategies, evaluating these strategies against multiple criteria, and applying the most appropriate strategy to the original question. Experimental evaluation across seven diverse CQA benchmarks demonstrates that T$^2$ not only achieves higher accuracy than baseline methods but also reduces computational overhead by up to 25.2\%.</li>
</ul>

<h3>Title: VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hefei Mei, Zirui Wang, Shen You, Minjing Dong, Chang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17440">https://arxiv.org/abs/2505.17440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17440">https://arxiv.org/pdf/2505.17440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17440]] VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models(https://arxiv.org/abs/2505.17440)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in multimodal understanding and generation, yet their vulnerability to adversarial attacks raises significant robustness concerns. While existing effective attacks always focus on task-specific white-box settings, these approaches are limited in the context of LVLMs, which are designed for diverse downstream tasks and require expensive full-model gradient computations. Motivated by the pivotal role and wide adoption of the vision encoder in LVLMs, we propose a simple yet effective Vision Encoder Attack (VEAttack), which targets the vision encoder of LVLMs only. Specifically, we propose to generate adversarial examples by minimizing the cosine similarity between the clean and perturbed visual features, without accessing the following large language models, task information, and labels. It significantly reduces the computational overhead while eliminating the task and label dependence of traditional white-box attacks in LVLMs. To make this simple attack effective, we propose to perturb images by optimizing image tokens instead of the classification token. We provide both empirical and theoretical evidence that VEAttack can easily generalize to various tasks. VEAttack has achieved a performance degradation of 94.5% on image caption task and 75.7% on visual question answering task. We also reveal some key observations to provide insights into LVLM attack/defense: 1) hidden layer variations of LLM, 2) token attention differential, 3) Möbius band in transfer attack, 4) low sensitivity to attack steps. The code is available at this https URL</li>
</ul>

<h3>Title: Discovering Forbidden Topics in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Can Rager, Chris Wendler, Rohit Gandikota, David Bau</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17441">https://arxiv.org/abs/2505.17441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17441">https://arxiv.org/pdf/2505.17441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17441]] Discovering Forbidden Topics in Language Models(https://arxiv.org/abs/2505.17441)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Refusal discovery is the task of identifying the full set of topics that a language model refuses to discuss. We introduce this new problem setting and develop a refusal discovery method, LLM-crawler, that uses token prefilling to find forbidden topics. We benchmark the LLM-crawler on Tulu-3-8B, an open-source model with public safety tuning data. Our crawler manages to retrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale the crawl to a frontier model using the prefilling option of Claude-Haiku. Finally, we crawl three widely used open-weight models: Llama-3.3-70B and two of its variants finetuned for reasoning: DeepSeek-R1-70B and Perplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with censorship tuning: The model exhibits "thought suppression" behavior that indicates memorization of CCP-aligned responses. Although Perplexity-R1-1776-70B is robust to censorship, LLM-crawler elicits CCP-aligned refusals answers in the quantized model. Our findings highlight the critical need for refusal discovery methods to detect biases, boundaries, and alignment failures of AI systems.</li>
</ul>

<h3>Title: Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Hao Jing, Anhong Wang, Yifan Zhang, Donghan Bu, Junhui Hou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17442">https://arxiv.org/abs/2505.17442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17442">https://arxiv.org/pdf/2505.17442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17442]] Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds(https://arxiv.org/abs/2505.17442)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Regarding intelligent transportation systems for vehicle networking, low-bitrate transmission via lossy point cloud compression is vital for facilitating real-time collaborative perception among vehicles with restricted bandwidth. In existing compression transmission systems, the sender lossily compresses point coordinates and reflectance to generate a transmission code stream, which faces transmission burdens from reflectance encoding and limited detection robustness due to information loss. To address these issues, this paper proposes a 3D object detection framework with reflectance prediction-based knowledge distillation (RPKD). We compress point coordinates while discarding reflectance during low-bitrate transmission, and feed the decoded non-reflectance compressed point clouds into a student detector. The discarded reflectance is then reconstructed by a geometry-based reflectance prediction (RP) module within the student detector for precise detection. A teacher detector with the same structure as student detector is designed for performing reflectance knowledge distillation (RKD) and detection knowledge distillation (DKD) from raw to compressed point clouds. Our RPKD framework jointly trains detectors on both raw and compressed point clouds to improve the student detector's robustness. Experimental results on the KITTI dataset and Waymo Open Dataset demonstrate that our method can boost detection accuracy for compressed point clouds across multiple code rates. Notably, at a low code rate of 2.146 Bpp on the KITTI dataset, our RPKD-PV achieves the highest mAP of 73.6, outperforming existing detection methods with the PV-RCNN baseline.</li>
</ul>

<h3>Title: PawPrint: Whose Footprints Are These? Identifying Animal Individuals by Their Footprints</h3>
<ul>
<li><strong>Authors: </strong>Inpyo Song, Hyemin Hwang, Jangwon Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17445">https://arxiv.org/abs/2505.17445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17445">https://arxiv.org/pdf/2505.17445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17445]] PawPrint: Whose Footprints Are These? Identifying Animal Individuals by Their Footprints(https://arxiv.org/abs/2505.17445)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In the United States, as of 2023, pet ownership has reached 66% of households and continues to rise annually. This trend underscores the critical need for effective pet identification and monitoring methods, particularly as nearly 10 million cats and dogs are reported stolen or lost each year. However, traditional methods for finding lost animals like GPS tags or ID photos have limitations-they can be removed, face signal issues, and depend on someone finding and reporting the pet. To address these limitations, we introduce PawPrint and PawPrint+, the first publicly available datasets focused on individual-level footprint identification for dogs and cats. Through comprehensive benchmarking of both modern deep neural networks (e.g., CNN, Transformers) and classical local features, we observe varying advantages and drawbacks depending on substrate complexity and data availability. These insights suggest future directions for combining learned global representations with local descriptors to enhance reliability across diverse, real-world conditions. As this approach provides a non-invasive alternative to traditional ID tags, we anticipate promising applications in ethical pet management and wildlife conservation efforts.</li>
</ul>

<h3>Title: Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shunsuke Kando, Yusuke Miyao, Shinnosuke Takamichi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17446">https://arxiv.org/abs/2505.17446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17446">https://arxiv.org/pdf/2505.17446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17446]] Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models(https://arxiv.org/abs/2505.17446)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding.</li>
</ul>

<h3>Title: LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization</h3>
<ul>
<li><strong>Authors: </strong>Qi Zhang, Shouqing Yang, Lirong Gao, Hao Chen, Xiaomeng Hu, Jinglei Chen, Jiexiang Wang, Sheng Guo, Bo Zheng, Haobo Wang, Junbo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17447">https://arxiv.org/abs/2505.17447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17447">https://arxiv.org/pdf/2505.17447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17447]] LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization(https://arxiv.org/abs/2505.17447)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated impressive capabilities in reasoning with the emergence of reasoning models like OpenAI-o1 and DeepSeek-R1. Recent research focuses on integrating reasoning capabilities into the realm of retrieval-augmented generation (RAG) via outcome-supervised reinforcement learning (RL) approaches, while the correctness of intermediate think-and-search steps is usually neglected. To address this issue, we design a process-level reward module to mitigate the unawareness of intermediate reasoning steps in outcome-level supervision without additional annotation. Grounded on this, we propose Learning to Think-and-Search (LeTS), a novel framework that hybridizes stepwise process reward and outcome-based reward to current RL methods for RAG. Extensive experiments demonstrate the generalization and inference efficiency of LeTS across various RAG benchmarks. In addition, these results reveal the potential of process- and outcome-level reward hybridization in boosting LLMs' reasoning ability via RL under other scenarios. The code will be released soon.</li>
</ul>

<h3>Title: Baitradar: A Multi-Model Clickbait Detection Algorithm Using Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Bhanuka Gamage, Adnan Labib, Aisha Joomun, Chern Hong Lim, KokSheik Wong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17448">https://arxiv.org/abs/2505.17448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17448">https://arxiv.org/pdf/2505.17448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17448]] Baitradar: A Multi-Model Clickbait Detection Algorithm Using Deep Learning(https://arxiv.org/abs/2505.17448)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Following the rising popularity of YouTube, there is an emerging problem on this platform called clickbait, which provokes users to click on videos using attractive titles and thumbnails. As a result, users ended up watching a video that does not have the content as publicized in the title. This issue is addressed in this study by proposing an algorithm called BaitRadar, which uses a deep learning technique where six inference models are jointly consulted to make the final classification decision. These models focus on different attributes of the video, including title, comments, thumbnail, tags, video statistics and audio transcript. The final classification is attained by computing the average of multiple models to provide a robust and accurate output even in situation where there is missing data. The proposed method is tested on 1,400 YouTube videos. On average, a test accuracy of 98% is achieved with an inference time of less than 2s.</li>
</ul>

<h3>Title: Real-time Traffic Accident Anticipation with Feature Reuse</h3>
<ul>
<li><strong>Authors: </strong>Inpyo Song, Jangwon Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17449">https://arxiv.org/abs/2505.17449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17449">https://arxiv.org/pdf/2505.17449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17449]] Real-time Traffic Accident Anticipation with Feature Reuse(https://arxiv.org/abs/2505.17449)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability</a></li>
<li><strong>Abstract: </strong>This paper addresses the problem of anticipating traffic accidents, which aims to forecast potential accidents before they happen. Real-time anticipation is crucial for safe autonomous driving, yet most methods rely on computationally heavy modules like optical flow and intermediate feature extractors, making real-world deployment challenging. In this paper, we thus introduce RARE (Real-time Accident anticipation with Reused Embeddings), a lightweight framework that capitalizes on intermediate features from a single pre-trained object detector. By eliminating additional feature-extraction pipelines, RARE significantly reduces latency. Furthermore, we introduce a novel Attention Score Ranking Loss, which prioritizes higher attention on accident-related objects over non-relevant ones. This loss enhances both accuracy and interpretability. RARE demonstrates a 4-8 times speedup over existing approaches on the DAD and CCD benchmarks, achieving a latency of 13.6ms per frame (73.3 FPS) on an RTX 6000. Moreover, despite its reduced complexity, it attains state-of-the-art Average Precision and reliably anticipates imminent collisions in real time. These results highlight RARE's potential for safety-critical applications where timely and explainable anticipation is essential.</li>
</ul>

<h3>Title: Self-Training Large Language Models with Confident Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Hyosoon Jang, Yunhui Jang, Sungjae Lee, Jungseul Ok, Sungsoo Ahn</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17454">https://arxiv.org/abs/2505.17454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17454">https://arxiv.org/pdf/2505.17454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17454]] Self-Training Large Language Models with Confident Reasoning(https://arxiv.org/abs/2505.17454)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown impressive performance by generating reasoning paths before final answers, but learning such a reasoning path requires costly human supervision. To address this issue, recent studies have explored self-training methods that improve reasoning capabilities using pseudo-labels generated by the LLMs themselves. Among these, confidence-based self-training fine-tunes LLMs to prefer reasoning paths with high-confidence answers, where confidence is estimated via majority voting. However, such methods exclusively focus on the quality of the final answer and may ignore the quality of the reasoning paths, as even an incorrect reasoning path leads to a correct answer by chance. Instead, we advocate the use of reasoning-level confidence to identify high-quality reasoning paths for self-training, supported by our empirical observations. We then propose a new self-training method, CORE-PO, that fine-tunes LLMs to prefer high-COnfidence REasoning paths through Policy Optimization. Our experiments show that CORE-PO improves the accuracy of outputs on four in-distribution and two out-of-distribution benchmarks, compared to existing self-training methods.</li>
</ul>

<h3>Title: Towards Evaluating Proactive Risk Awareness of Multimodal Language Models</h3>
<ul>
<li><strong>Authors: </strong>Youliang Yuan, Wenxiang Jiao, Yuejin Xie, Chihao Shen, Menghan Tian, Wenxuan Wang, Jen-tse Huang, Pinjia He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17455">https://arxiv.org/abs/2505.17455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17455">https://arxiv.org/pdf/2505.17455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17455]] Towards Evaluating Proactive Risk Awareness of Multimodal Language Models(https://arxiv.org/abs/2505.17455)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Human safety awareness gaps often prevent the timely recognition of everyday risks. In solving this problem, a proactive safety artificial intelligence (AI) system would work better than a reactive one. Instead of just reacting to users' questions, it would actively watch people's behavior and their environment to detect potential dangers in advance. Our Proactive Safety Bench (PaSBench) evaluates this capability through 416 multimodal scenarios (128 image sequences, 288 text logs) spanning 5 safety-critical domains. Evaluation of 36 advanced models reveals fundamental limitations: Top performers like Gemini-2.5-pro achieve 71% image and 64% text accuracy, but miss 45-55% risks in repeated trials. Through failure analysis, we identify unstable proactive reasoning rather than knowledge deficits as the primary limitation. This work establishes (1) a proactive safety benchmark, (2) systematic evidence of model limitations, and (3) critical directions for developing reliable protective AI. We believe our dataset and findings can promote the development of safer AI assistants that actively prevent harm rather than merely respond to requests. Our dataset can be found at this https URL.</li>
</ul>

<h3>Title: Graph Mamba for Efficient Whole Slide Image Understanding</h3>
<ul>
<li><strong>Authors: </strong>Jiaxuan Lu, Junyan Shi, Yuhui Lin, Fang Yan, Yue Gao, Shaoting Zhang, Xiaosong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17457">https://arxiv.org/abs/2505.17457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17457">https://arxiv.org/pdf/2505.17457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17457]] Graph Mamba for Efficient Whole Slide Image Understanding(https://arxiv.org/abs/2505.17457)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Whole Slide Images (WSIs) in histopathology present a significant challenge for large-scale medical image analysis due to their high resolution, large size, and complex tile relationships. Existing Multiple Instance Learning (MIL) methods, such as Graph Neural Networks (GNNs) and Transformer-based models, face limitations in scalability and computational cost. To bridge this gap, we propose the WSI-GMamba framework, which synergistically combines the relational modeling strengths of GNNs with the efficiency of Mamba, the State Space Model designed for sequence learning. The proposed GMamba block integrates Message Passing, Graph Scanning & Flattening, and feature aggregation via a Bidirectional State Space Model (Bi-SSM), achieving Transformer-level performance with 7* fewer FLOPs. By leveraging the complementary strengths of lightweight GNNs and Mamba, the WSI-GMamba framework delivers a scalable solution for large-scale WSI analysis, offering both high accuracy and computational efficiency for slide-level classification.</li>
</ul>

<h3>Title: Diagnosing Vision Language Models' Perception by Leveraging Human Methods for Color Vision Deficiencies</h3>
<ul>
<li><strong>Authors: </strong>Kazuki Hayashi, Shintaro Ozaki, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17461">https://arxiv.org/abs/2505.17461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17461">https://arxiv.org/pdf/2505.17461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17461]] Diagnosing Vision Language Models' Perception by Leveraging Human Methods for Color Vision Deficiencies(https://arxiv.org/abs/2505.17461)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Large-scale Vision Language Models (LVLMs) are increasingly being applied to a wide range of real-world multimodal applications, involving complex visual and linguistic reasoning. As these models become more integrated into practical use, they are expected to handle complex aspects of human interaction. Among these, color perception is a fundamental yet highly variable aspect of visual understanding. It differs across individuals due to biological factors such as Color Vision Deficiencies (CVDs), as well as differences in culture and language. Despite its importance, perceptual diversity has received limited attention. In our study, we evaluate LVLMs' ability to account for individual level perceptual variation using the Ishihara Test, a widely used method for detecting CVDs. Our results show that LVLMs can explain CVDs in natural language, but they cannot simulate how people with CVDs perceive color in image based tasks. These findings highlight the need for multimodal systems that can account for color perceptual diversity and support broader discussions on perceptual inclusiveness and fairness in multimodal AI.</li>
</ul>

<h3>Title: Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Liming Zhu, Wenjie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17464">https://arxiv.org/abs/2505.17464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17464">https://arxiv.org/pdf/2505.17464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17464]] Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning(https://arxiv.org/abs/2505.17464)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge. Current hybrid RAG system retrieves evidence from both knowledge graphs (KGs) and text documents to support LLM reasoning. However, it faces challenges like handling multi-hop reasoning, multi-entity questions, multi-source verification, and effective graph utilization. To address these limitations, we present Hydra, a training-free framework that unifies graph topology, document semantics, and source reliability to support deep, faithful reasoning in LLMs. Hydra handles multi-hop and multi-entity problems through agent-driven exploration that combines structured and unstructured retrieval, increasing both diversity and precision of evidence. To tackle multi-source verification, Hydra uses a tri-factor cross-source verification (source trustworthiness assessment, cross-source corroboration, and entity-path alignment), to balance topic relevance with cross-modal agreement. By leveraging graph structure, Hydra fuses heterogeneous sources, guides efficient exploration, and prunes noise early. Comprehensive experiments on seven benchmark datasets show that Hydra achieves overall state-of-the-art results on all benchmarks with GPT-3.5, outperforming the strong hybrid baseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, Hydra enables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance comparable to that of GPT-4-Turbo.</li>
</ul>

<h3>Title: A Position Paper on the Automatic Generation of Machine Learning Leaderboards</h3>
<ul>
<li><strong>Authors: </strong>Roelien C Timmer, Yufang Hou, Stephen Wan</a></li>
<li><strong>Subjects: </strong>cs.CL, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17465">https://arxiv.org/abs/2505.17465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17465">https://arxiv.org/pdf/2505.17465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17465]] A Position Paper on the Automatic Generation of Machine Learning Leaderboards(https://arxiv.org/abs/2505.17465)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>An important task in machine learning (ML) research is comparing prior work, which is often performed via ML leaderboards: a tabular overview of experiments with comparable conditions (e.g., same task, dataset, and metric). However, the growing volume of literature creates challenges in creating and maintaining these leaderboards. To ease this burden, researchers have developed methods to extract leaderboard entries from research papers for automated leaderboard curation. Yet, prior work varies in problem framing, complicating comparisons and limiting real-world applicability. In this position paper, we present the first overview of Automatic Leaderboard Generation (ALG) research, identifying fundamental differences in assumptions, scope, and output formats. We propose an ALG unified conceptual framework to standardise how the ALG task is defined. We offer ALG benchmarking guidelines, including recommendations for datasets and metrics that promote fair, reproducible evaluation. Lastly, we outline challenges and new directions for ALG, such as, advocating for broader coverage by including all reported results and richer metadata.</li>
</ul>

<h3>Title: SecurePay: Enabling Secure and Fast Payment Processing for Platform Economy</h3>
<ul>
<li><strong>Authors: </strong>Junru Lin, Mingzhe Liu, Songze Li, Xuechao Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17466">https://arxiv.org/abs/2505.17466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17466">https://arxiv.org/pdf/2505.17466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17466]] SecurePay: Enabling Secure and Fast Payment Processing for Platform Economy(https://arxiv.org/abs/2505.17466)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Recent years have witnessed a rapid development of platform economy, as it effectively addresses the trust dilemma between untrusted online buyers and merchants. However, malicious platforms can misuse users' funds and information, causing severe security concerns. Previous research efforts aimed at enhancing security in platform payment systems often sacrificed processing performance, while those focusing on processing efficiency struggled to completely prevent fund and information misuse. In this paper, we introduce SecurePay, a secure, yet performant payment processing system for platform economy. SecurePay is the first payment system that combines permissioned blockchain with central bank digital currency (CBDC) to ensure fund security, information security, and resistance to collusion by intermediaries; it also facilitates counter-party auditing, closed-loop regulation, and enhances operational efficiency for transaction settlement. We develop a full implementation of the proposed SecurePay system, and our experiments conducted on personal devices demonstrate a throughput of 256.4 transactions per second and an average latency of 4.29 seconds, demonstrating a comparable processing efficiency with a centralized system, with a significantly improved security level.</li>
</ul>

<h3>Title: Efficient compression of neural networks and datasets</h3>
<ul>
<li><strong>Authors: </strong>Lukas Silvester Barth, Paulo von Petersenn</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT, math.OC, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17469">https://arxiv.org/abs/2505.17469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17469">https://arxiv.org/pdf/2505.17469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17469]] Efficient compression of neural networks and datasets(https://arxiv.org/abs/2505.17469)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We compare, improve, and contribute methods that substantially decrease the number of parameters of neural networks while maintaining high test accuracy. When applying our methods to minimize description length, we obtain very effective data compression algorithms. In particular, we develop a probabilistic reformulation of $\ell_0$ regularized optimization for nonlinear models that does not require Monte-Carlo sampling and thus improves upon previous methods. We also improve upon methods involving smooth approximations to the $\ell_0$ norm, and investigate layerwise methods. We compare the methods on different architectures and datasets, including convolutional networks trained on image datasets and transformers trained on parts of Wikipedia. We also created a synthetic teacher-student setup to investigate compression in a controlled continuous setting. Finally, we conceptually relate compression algorithms to Solomonoff's theory of inductive inference and empirically verify the prediction that regularized models can exhibit more sample-efficient convergence.</li>
</ul>

<h3>Title: SLearnLLM: A Self-Learning Framework for Efficient Domain-Specific Adaptation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiang Liu, Zhaoxiang Liu, Peng Wang, Kohou Wang, Huan Hu, Kai Wang, Shiguo Lian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17470">https://arxiv.org/abs/2505.17470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17470">https://arxiv.org/pdf/2505.17470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17470]] SLearnLLM: A Self-Learning Framework for Efficient Domain-Specific Adaptation of Large Language Models(https://arxiv.org/abs/2505.17470)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>When using supervised fine-tuning (SFT) to adapt large language models (LLMs) to specific domains, a significant challenge arises: should we use the entire SFT dataset for fine-tuning? Common practice often involves fine-tuning directly on the entire dataset due to limited information on the LLM's past training data. However, if the SFT dataset largely overlaps with the model's existing knowledge, the performance gains are minimal, leading to wasted computational resources. Identifying the unknown knowledge within the SFT dataset and using it to fine-tune the model could substantially improve the training efficiency. To address this challenge, we propose a self-learning framework for LLMs inspired by human learning pattern. This framework takes a fine-tuning (SFT) dataset in a specific domain as input. First, the LLMs answer the questions in the SFT dataset. The LLMs then objectively grade the responses and filter out the incorrectly answered QA pairs. Finally, we fine-tune the LLMs based on this filtered QA set. Experimental results in the fields of agriculture and medicine demonstrate that our method substantially reduces training time while achieving comparable improvements to those attained with full dataset fine-tuning. By concentrating on the unknown knowledge within the SFT dataset, our approach enhances the efficiency of fine-tuning LLMs.</li>
</ul>

<h3>Title: FinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain</h3>
<ul>
<li><strong>Authors: </strong>Suifeng Zhao, Zhuoran Jin, Sujian Li, Jun Gao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17471">https://arxiv.org/abs/2505.17471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17471">https://arxiv.org/pdf/2505.17471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17471]] FinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain(https://arxiv.org/abs/2505.17471)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) plays a vital role in the financial domain, powering applications such as real-time market analysis, trend forecasting, and interest rate computation. However, most existing RAG research in finance focuses predominantly on textual data, overlooking the rich visual content in financial documents, resulting in the loss of key analytical insights. To bridge this gap, we present FinRAGBench-V, a comprehensive visual RAG benchmark tailored for finance which effectively integrates multimodal data and provides visual citation to ensure traceability. It includes a bilingual retrieval corpus with 60,780 Chinese and 51,219 English pages, along with a high-quality, human-annotated question-answering (QA) dataset spanning heterogeneous data types and seven question categories. Moreover, we introduce RGenCite, an RAG baseline that seamlessly integrates visual citation with generation. Furthermore, we propose an automatic citation evaluation method to systematically assess the visual citation capabilities of Multimodal Large Language Models (MLLMs). Extensive experiments on RGenCite underscore the challenging nature of FinRAGBench-V, providing valuable insights for the development of multimodal RAG systems in finance.</li>
</ul>

<h3>Title: The Coherence Trap: When MLLM-Crafted Narratives Exploit Manipulated Visual Contexts</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Zhang, Yaxiong Wang, Yujiao Wu, Lianwei Wu, Li Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17476">https://arxiv.org/abs/2505.17476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17476">https://arxiv.org/pdf/2505.17476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17476]] The Coherence Trap: When MLLM-Crafted Narratives Exploit Manipulated Visual Contexts(https://arxiv.org/abs/2505.17476)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The detection and grounding of multimedia manipulation has emerged as a critical challenge in combating AI-generated disinformation. While existing methods have made progress in recent years, we identify two fundamental limitations in current approaches: (1) Underestimation of MLLM-driven deception risk: prevailing techniques primarily address rule-based text manipulations, yet fail to account for sophisticated misinformation synthesized by multimodal large language models (MLLMs) that can dynamically generate semantically coherent, contextually plausible yet deceptive narratives conditioned on manipulated images; (2) Unrealistic misalignment artifacts: currently focused scenarios rely on artificially misaligned content that lacks semantic coherence, rendering them easily detectable. To address these gaps holistically, we propose a new adversarial pipeline that leverages MLLMs to generate high-risk disinformation. Our approach begins with constructing the MLLM-Driven Synthetic Multimodal (MDSM) dataset, where images are first altered using state-of-the-art editing techniques and then paired with MLLM-generated deceptive texts that maintain semantic consistency with the visual manipulations. Building upon this foundation, we present the Artifact-aware Manipulation Diagnosis via MLLM (AMD) framework featuring two key innovations: Artifact Pre-perception Encoding strategy and Manipulation-Oriented Reasoning, to tame MLLMs for the MDSM problem. Comprehensive experiments validate our framework's superior generalization capabilities as a unified architecture for detecting MLLM-powered multimodal deceptions.</li>
</ul>

<h3>Title: Reverse-Speech-Finder: A Neural Network Backtracking Architecture for Generating Alzheimer's Disease Speech Samples and Improving Diagnosis Performance</h3>
<ul>
<li><strong>Authors: </strong>Victor OK Li, Yang Han, Jacqueline CK Lam, Lawrence YL Cheung</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17477">https://arxiv.org/abs/2505.17477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17477">https://arxiv.org/pdf/2505.17477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17477]] Reverse-Speech-Finder: A Neural Network Backtracking Architecture for Generating Alzheimer's Disease Speech Samples and Improving Diagnosis Performance(https://arxiv.org/abs/2505.17477)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural network backtracking architecture designed to enhance Alzheimer's Disease (AD) diagnosis through speech analysis. Leveraging the power of pre-trained large language models, RSF identifies and utilizes the most probable AD-specific speech markers, addressing both the scarcity of real AD speech samples and the challenge of limited interpretability in existing models. RSF's unique approach consists of three core innovations: Firstly, it exploits the observation that speech markers most probable of predicting AD, defined as the most probable speech-markers (MPMs), must have the highest probability of activating those neurons (in the neural network) with the highest probability of predicting AD, defined as the most probable neurons (MPNs). Secondly, it utilizes a speech token representation at the input layer, allowing backtracking from MPNs to identify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an innovative backtracking method to track backwards from the MPNs to the input layer, identifying the MPTs and the corresponding MPMs, and ingeniously uncovering novel speech markers for AD detection. Experimental results demonstrate RSF's superiority over traditional methods such as SHAP and Integrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost in F1-score. By generating speech data that encapsulates novel markers, RSF not only mitigates the limitations of real data scarcity but also significantly enhances the robustness and accuracy of AD diagnostic models. These findings underscore RSF's potential as a transformative tool in speech-based AD detection, offering new insights into AD-related linguistic deficits and paving the way for more effective non-invasive early intervention strategies.</li>
</ul>

<h3>Title: Simultaneous Modeling of Protein Conformation and Dynamics via Autoregression</h3>
<ul>
<li><strong>Authors: </strong>Yuning Shen, Lihao Wang, Huizhuo Yuan, Yan Wang, Bangji Yang, Quanquan Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.bio-ph, q-bio.BM, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17478">https://arxiv.org/abs/2505.17478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17478">https://arxiv.org/pdf/2505.17478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17478]] Simultaneous Modeling of Protein Conformation and Dynamics via Autoregression(https://arxiv.org/abs/2505.17478)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Understanding protein dynamics is critical for elucidating their biological functions. The increasing availability of molecular dynamics (MD) data enables the training of deep generative models to efficiently explore the conformational space of proteins. However, existing approaches either fail to explicitly capture the temporal dependencies between conformations or do not support direct generation of time-independent samples. To address these limitations, we introduce ConfRover, an autoregressive model that simultaneously learns protein conformation and dynamics from MD trajectories, supporting both time-dependent and time-independent sampling. At the core of our model is a modular architecture comprising: (i) an encoding layer, adapted from protein folding models, that embeds protein-specific information and conformation at each time frame into a latent space; (ii) a temporal module, a sequence model that captures conformational dynamics across frames; and (iii) an SE(3) diffusion model as the structure decoder, generating conformations in continuous space. Experiments on ATLAS, a large-scale protein MD dataset of diverse structures, demonstrate the effectiveness of our model in learning conformational dynamics and supporting a wide range of downstream tasks. ConfRover is the first model to sample both protein conformations and trajectories within a single framework, offering a novel and flexible approach for learning from protein MD data.</li>
</ul>

<h3>Title: MARCO: Meta-Reflection with Cross-Referencing for Code Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yusheng Zhao, Xiao Luo, Weizhi Zhang, Wei Ju, Zhiping Xiao, Philip S. Yu, Ming Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17481">https://arxiv.org/abs/2505.17481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17481">https://arxiv.org/pdf/2505.17481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17481]] MARCO: Meta-Reflection with Cross-Referencing for Code Reasoning(https://arxiv.org/abs/2505.17481)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The ability to reason is one of the most fundamental capabilities of large language models (LLMs), enabling a wide range of downstream tasks through sophisticated problem-solving. A critical aspect of this is code reasoning, which involves logical reasoning with formal languages (i.e., programming code). In this paper, we enhance this capability of LLMs by exploring the following question: how can an LLM agent become progressively smarter in code reasoning with each solution it proposes, thereby achieving substantial cumulative improvement? Most existing research takes a static perspective, focusing on isolated problem-solving using frozen LLMs. In contrast, we adopt a cognitive-evolving perspective and propose a novel framework named Meta-Reflection with Cross-Referencing (MARCO) that enables the LLM to evolve dynamically during inference through self-improvement. From the perspective of human cognitive development, we leverage both knowledge accumulation and lesson sharing. In particular, to accumulate knowledge during problem-solving, we propose meta-reflection that reflects on the reasoning paths of the current problem to obtain knowledge and experience for future consideration. Moreover, to effectively utilize the lessons from other agents, we propose cross-referencing that incorporates the solution and feedback from other agents into the current problem-solving process. We conduct experiments across various datasets in code reasoning, and the results demonstrate the effectiveness of MARCO.</li>
</ul>

<h3>Title: Research on Defect Detection Method of Motor Control Board Based on Image Processing</h3>
<ul>
<li><strong>Authors: </strong>Jingde Huang, Zhangyu Huang, Chenyu Li, Jiantong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17493">https://arxiv.org/abs/2505.17493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17493">https://arxiv.org/pdf/2505.17493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17493]] Research on Defect Detection Method of Motor Control Board Based on Image Processing(https://arxiv.org/abs/2505.17493)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The motor control board has various defects such as inconsistent color differences, incorrect plug-in positions, solder short circuits, and more. These defects directly affect the performance and stability of the motor control board, thereby having a negative impact on product quality. Therefore, studying the defect detection technology of the motor control board is an important means to improve the quality control level of the motor control board. Firstly, the processing methods of digital images about the motor control board were studied, and the noise suppression methods that affect image feature extraction were analyzed. Secondly, a specific model for defect feature extraction and color difference recognition of the tested motor control board was established, and qualified or defective products were determined based on feature thresholds. Thirdly, the search algorithm for defective images was optimized. Finally, comparative experiments were conducted on the typical motor control board, and the experimental results demonstrate that the accuracy of the motor control board defect detection model-based on image processing established in this paper reached over 99%. It is suitable for timely image processing of large quantities of motor control boards on the production line, and achieved efficient defect detection. The defect detection method can not only be used for online detection of the motor control board defects, but also provide solutions for the integrated circuit board defect processing for the industry.</li>
</ul>

<h3>Title: ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Landon Butler, Abhineet Agarwal, Justin Singh Kang, Yigit Efe Erginbas, Bin Yu, Kannan Ramchandran</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17495">https://arxiv.org/abs/2505.17495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17495">https://arxiv.org/pdf/2505.17495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17495]] ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs(https://arxiv.org/abs/2505.17495)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable performance by capturing complex interactions between input features. To identify these interactions, most existing approaches require enumerating all possible combinations of features up to a given order, causing them to scale poorly with the number of inputs $n$. Recently, Kang et al. (2025) proposed SPEX, an information-theoretic approach that uses interaction sparsity to scale to $n \approx 10^3$ features. SPEX greatly improves upon prior methods but requires tens of thousands of model inferences, which can be prohibitive for large models. In this paper, we observe that LLM feature interactions are often hierarchical -- higher-order interactions are accompanied by their lower-order subsets -- which enables more efficient discovery. To exploit this hierarchy, we propose ProxySPEX, an interaction attribution algorithm that first fits gradient boosted trees to masked LLM outputs and then extracts the important interactions. Experiments across four challenging high-dimensional datasets show that ProxySPEX more faithfully reconstructs LLM outputs by 20% over marginal attribution approaches while using $10\times$ fewer inferences than SPEX. By accounting for interactions, ProxySPEX identifies features that influence model output over 20% more than those selected by marginal approaches. Further, we apply ProxySPEX to two interpretability tasks. Data attribution, where we identify interactions among CIFAR-10 training samples that influence test predictions, and mechanistic interpretability, where we uncover interactions between attention heads, both within and across layers, on a question-answering task. ProxySPEX identifies interactions that enable more aggressive pruning of heads than marginal approaches.</li>
</ul>

<h3>Title: Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chi-Yuan Hsiao, Ke-Han Lu, Kai-Wei Chang, Chih-Kai Yang, Wei-Chih Chen, Hung-yi Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17496">https://arxiv.org/abs/2505.17496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17496">https://arxiv.org/pdf/2505.17496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17496]] Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models(https://arxiv.org/abs/2505.17496)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>End-to-end training of Spoken Language Models (SLMs) commonly involves adapting pre-trained text-based Large Language Models (LLMs) to the speech modality through multi-stage training on diverse tasks such as ASR, TTS and spoken question answering (SQA). Although this multi-stage continual learning equips LLMs with both speech understanding and generation capabilities, the substantial differences in task and data distributions across stages can lead to catastrophic forgetting, where previously acquired knowledge is lost. This paper investigates catastrophic forgetting and evaluates three mitigation strategies-model merging, discounting the LoRA scaling factor, and experience replay to balance knowledge retention with new learning. Results show that experience replay is the most effective, with further gains achieved by combining it with other methods. These findings provide insights for developing more robust and efficient SLM training pipelines.</li>
</ul>

<h3>Title: RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yuehan Jin, Xiaoqing Liu, Yiyuan Yang, Zhiwen Yu, Tong Zhang, Kaixiang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17501">https://arxiv.org/abs/2505.17501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17501">https://arxiv.org/pdf/2505.17501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17501]] RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition(https://arxiv.org/abs/2505.17501)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Multimodal emotion recognition analyzes emotions by combining data from multiple sources. However, real-world noise or sensor failures often cause missing or corrupted data, creating the Incomplete Multimodal Emotion Recognition (IMER) challenge. In this paper, we propose Robust Hybrid Diffusion Recovery (RoHyDR), a novel framework that performs missing-modality recovery at unimodal, multimodal, feature, and semantic levels. For unimodal representation recovery of missing modalities, RoHyDR exploits a diffusion-based generator to generate distribution-consistent and semantically aligned representations from Gaussian noise, using available modalities as conditioning. For multimodal fusion recovery, we introduce adversarial learning to produce a realistic fused multimodal representation and recover missing semantic content. We further propose a multi-stage optimization strategy that enhances training stability and efficiency. In contrast to previous work, the hybrid diffusion and adversarial learning-based recovery mechanism in RoHyDR allows recovery of missing information in both unimodal representation and multimodal fusion, at both feature and semantic levels, effectively mitigating performance degradation caused by suboptimal optimization. Comprehensive experiments conducted on two widely used multimodal emotion recognition benchmarks demonstrate that our proposed method outperforms state-of-the-art IMER methods, achieving robust recognition performance under various missing-modality scenarios. Our code will be made publicly available upon acceptance.</li>
</ul>

<h3>Title: CReSt: A Comprehensive Benchmark for Retrieval-Augmented Generation with Complex Reasoning over Structured Documents</h3>
<ul>
<li><strong>Authors: </strong>Minsoo Khang, Sangjun Park, Teakgyu Hong, Dawoon Jung</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17503">https://arxiv.org/abs/2505.17503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17503">https://arxiv.org/pdf/2505.17503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17503]] CReSt: A Comprehensive Benchmark for Retrieval-Augmented Generation with Complex Reasoning over Structured Documents(https://arxiv.org/abs/2505.17503)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have made substantial progress in recent years, yet evaluating their capabilities in practical Retrieval-Augmented Generation (RAG) scenarios remains challenging. In practical applications, LLMs must demonstrate complex reasoning, refuse to answer appropriately, provide precise citations, and effectively understand document layout. These capabilities are crucial for advanced task handling, uncertainty awareness, maintaining reliability, and structural understanding. While some of the prior works address these aspects individually, there is a need for a unified framework that evaluates them collectively in practical RAG scenarios. To address this, we present CReSt (A Comprehensive Benchmark for Retrieval-Augmented Generation with Complex Reasoning over Structured Documents), a benchmark designed to assess these key dimensions holistically. CReSt comprises 2,245 human-annotated examples in English and Korean, designed to capture practical RAG scenarios that require complex reasoning over structured documents. It also introduces a tailored evaluation methodology to comprehensively assess model performance in these critical areas. Our evaluation shows that even advanced LLMs struggle to perform consistently across these dimensions, underscoring key areas for improvement. We release CReSt to support further research and the development of more robust RAG systems. The dataset and code are available at: this https URL.</li>
</ul>

<h3>Title: L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaohao Liu, Xiaobo Xia, Weixiang Zhao, Manyi Zhang, Xianzhi Yu, Xiu Su, Shuo Yang, See-Kiong Ng, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17505">https://arxiv.org/abs/2505.17505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17505">https://arxiv.org/pdf/2505.17505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17505]] L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models(https://arxiv.org/abs/2505.17505)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved notable progress. Despite their success, next-token prediction (NTP), the dominant method for LLM training and inference, is constrained in both contextual coverage and inference efficiency due to its inherently sequential process. To overcome these challenges, we propose leap multi-token prediction~(L-MTP), an innovative token prediction method that extends the capabilities of multi-token prediction (MTP) by introducing a leap-based mechanism. Unlike conventional MTP, which generates multiple tokens at adjacent positions, L-MTP strategically skips over intermediate tokens, predicting non-sequential ones in a single forward pass. This structured leap not only enhances the model's ability to capture long-range dependencies but also enables a decoding strategy specially optimized for non-sequential leap token generation, effectively accelerating inference. We theoretically demonstrate the benefit of L-MTP in improving inference efficiency. Experiments across diverse benchmarks validate its merit in boosting both LLM performance and inference speed. The source code will be publicly available.</li>
</ul>

<h3>Title: On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Yang Yuan, Quanquan Gu, Andrew C Yao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17508">https://arxiv.org/abs/2505.17508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17508">https://arxiv.org/pdf/2505.17508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17508]] On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning(https://arxiv.org/abs/2505.17508)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). Despite the widespread use of Kullback-Leibler (KL) regularization in policy gradient algorithms to stabilize training, the systematic exploration of how different KL divergence formulations can be estimated and integrated into surrogate loss functions for online reinforcement learning (RL) presents a nuanced and systematically explorable design space. In this paper, we propose regularized policy gradient (RPG), a systematic framework for deriving and analyzing KL-regularized policy gradient methods in the online RL setting. We derive policy gradients and corresponding surrogate loss functions for objectives regularized by both forward and reverse KL divergences, considering both normalized and unnormalized policy distributions. Furthermore, we present derivations for fully differentiable loss functions as well as REINFORCE-style gradient estimators, accommodating diverse algorithmic needs. We conduct extensive experiments on RL for LLM reasoning using these methods, showing improved or competitive results in terms of training stability and performance compared to strong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at this https URL.</li>
</ul>

<h3>Title: Enhancing Adversarial Robustness of Vision Language Models via Adversarial Mixture Prompt Tuning</h3>
<ul>
<li><strong>Authors: </strong>Shiji Zhao, Qihui Zhu, Shukun Xiong, Shouwei Ruan, Yize Fan, Ranjie Duan, Qing Guo, Xingxing Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17509">https://arxiv.org/abs/2505.17509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17509">https://arxiv.org/pdf/2505.17509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17509]] Enhancing Adversarial Robustness of Vision Language Models via Adversarial Mixture Prompt Tuning(https://arxiv.org/abs/2505.17509)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Large pre-trained Vision Language Models (VLMs) have excellent generalization capabilities but are highly susceptible to adversarial examples, presenting potential security risks. To improve the robustness of VLMs against adversarial examples, adversarial prompt tuning methods are proposed to align the text feature with the adversarial image feature without changing model parameters. However, when facing various adversarial attacks, a single learnable text prompt has insufficient generalization to align well with all adversarial image features, which finally leads to the overfitting phenomenon. To address the above challenge, in this paper, we empirically find that increasing the number of learned prompts can bring more robustness improvement than a longer prompt. Then we propose an adversarial tuning method named Adversarial Mixture Prompt Tuning (AMPT) to enhance the generalization towards various adversarial attacks for VLMs. AMPT aims to learn mixture text prompts to obtain more robust text features. To further enhance the adaptability, we propose a conditional weight router based on the input adversarial image to predict the mixture weights of multiple learned prompts, which helps obtain sample-specific aggregated text features aligning with different adversarial image features. A series of experiments show that our method can achieve better adversarial robustness than state-of-the-art methods on 11 datasets under different experimental settings.</li>
</ul>

<h3>Title: Large Language Models Do Multi-Label Classification Differently</h3>
<ul>
<li><strong>Authors: </strong>Marcus Ma, Georgios Chochlakis, Niyantha Maruthu Pandiyan, Jesse Thomason, Shrikanth Narayanan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17510">https://arxiv.org/abs/2505.17510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17510">https://arxiv.org/pdf/2505.17510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17510]] Large Language Models Do Multi-Label Classification Differently(https://arxiv.org/abs/2505.17510)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-label classification is prevalent in real-world settings, but the behavior of Large Language Models (LLMs) in this setting is understudied. We investigate how autoregressive LLMs perform multi-label classification, with a focus on subjective tasks, by analyzing the output distributions of the models in each generation step. We find that their predictive behavior reflects the multiple steps in the underlying language modeling required to generate all relevant labels as they tend to suppress all but one label at each step. We further observe that as model scale increases, their token distributions exhibit lower entropy, yet the internal ranking of the labels improves. Finetuning methods such as supervised finetuning and reinforcement learning amplify this phenomenon. To further study this issue, we introduce the task of distribution alignment for multi-label settings: aligning LLM-derived label distributions with empirical distributions estimated from annotator responses in subjective tasks. We propose both zero-shot and supervised methods which improve both alignment and predictive performance over existing approaches.</li>
</ul>

<h3>Title: What You Read Isn't What You Hear: Linguistic Sensitivity in Deepfake Speech Detection</h3>
<ul>
<li><strong>Authors: </strong>Binh Nguyen, Shuji Shi, Ryan Ofman, Thai Le</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17513">https://arxiv.org/abs/2505.17513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17513">https://arxiv.org/pdf/2505.17513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17513]] What You Read Isn't What You Hear: Linguistic Sensitivity in Deepfake Speech Detection(https://arxiv.org/abs/2505.17513)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-speech technologies have enabled realistic voice generation, fueling audio-based deepfake attacks such as fraud and impersonation. While audio anti-spoofing systems are critical for detecting such threats, prior work has predominantly focused on acoustic-level perturbations, leaving the impact of linguistic variation largely unexplored. In this paper, we investigate the linguistic sensitivity of both open-source and commercial anti-spoofing detectors by introducing transcript-level adversarial attacks. Our extensive evaluation reveals that even minor linguistic perturbations can significantly degrade detection accuracy: attack success rates surpass 60% on several open-source detector-voice pairs, and notably one commercial detection accuracy drops from 100% on synthetic audio to just 32%. Through a comprehensive feature attribution analysis, we identify that both linguistic complexity and model-level audio embedding similarity contribute strongly to detector vulnerability. We further demonstrate the real-world risk via a case study replicating the Brad Pitt audio deepfake scam, using transcript adversarial attacks to completely bypass commercial detectors. These results highlight the need to move beyond purely acoustic defenses and account for linguistic variation in the design of robust anti-spoofing systems. All source code will be publicly available.</li>
</ul>

<h3>Title: Spacetime Geometry of Denoising in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Rafał Karczewski, Markus Heinonen, Alison Pouplin, Søren Hauberg, Vikas Garg</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17517">https://arxiv.org/abs/2505.17517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17517">https://arxiv.org/pdf/2505.17517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17517]] Spacetime Geometry of Denoising in Diffusion Models(https://arxiv.org/abs/2505.17517)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present a novel perspective on diffusion models using the framework of information geometry. We show that the set of noisy samples, taken across all noise levels simultaneously, forms a statistical manifold -- a family of denoising probability distributions. Interpreting the noise level as a temporal parameter, we refer to this manifold as spacetime. This manifold naturally carries a Fisher-Rao metric, which defines geodesics -- shortest paths between noisy points. Notably, this family of distributions is exponential, enabling efficient geodesic computation even in high-dimensional settings without retraining or fine-tuning. We demonstrate the practical value of this geometric viewpoint in transition path sampling, where spacetime geodesics define smooth sequences of Boltzmann distributions, enabling the generation of continuous trajectories between low-energy metastable states. Code is available at: this https URL.</li>
</ul>

<h3>Title: Chain-of-Lure: A Synthetic Narrative-Driven Approach to Compromise Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenhan Chang, Tianqing Zhu, Yu Zhao, Shuangyong Song, Ping Xiong, Wanlei Zhou, Yongxiang Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17519">https://arxiv.org/abs/2505.17519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17519">https://arxiv.org/pdf/2505.17519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17519]] Chain-of-Lure: A Synthetic Narrative-Driven Approach to Compromise Large Language Models(https://arxiv.org/abs/2505.17519)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, generative, large language model</a></li>
<li><strong>Abstract: </strong>In the era of rapid generative AI development, interactions between humans and large language models face significant misusing risks. Previous research has primarily focused on black-box scenarios using human-guided prompts and white-box scenarios leveraging gradient-based LLM generation methods, neglecting the possibility that LLMs can act not only as victim models, but also as attacker models to harm other models. We proposes a novel jailbreaking method inspired by the Chain-of-Thought mechanism, where the attacker model uses mission transfer to conceal harmful user intent in dialogue and generates chained narrative lures to stimulate the reasoning capabilities of victim models, leading to successful jailbreaking. To enhance the attack success rate, we introduce a helper model that performs random narrative optimization on the narrative lures during multi-turn dialogues while ensuring alignment with the original intent, enabling the optimized lures to bypass the safety barriers of victim models effectively. Our experiments reveal that models with weaker safety mechanisms exhibit stronger attack capabilities, demonstrating that models can not only be exploited, but also help harm others. By incorporating toxicity scores, we employ third-party models to evaluate the harmfulness of victim models' responses to jailbreaking attempts. The study shows that using refusal keywords as an evaluation metric for attack success rates is significantly flawed because it does not assess whether the responses guide harmful questions, while toxicity scores measure the harm of generated content with more precision and its alignment with harmful questions. Our approach demonstrates outstanding performance, uncovering latent vulnerabilities in LLMs and providing data-driven feedback to optimize LLM safety mechanisms. We also discuss two defensive strategies to offer guidance on improving defense mechanisms.</li>
</ul>

<h3>Title: Adaptively Secure Distributed Broadcast Encryption with Linear-Size Public Parameters</h3>
<ul>
<li><strong>Authors: </strong>Kwangsu Lee</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17527">https://arxiv.org/abs/2505.17527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17527">https://arxiv.org/pdf/2505.17527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17527]] Adaptively Secure Distributed Broadcast Encryption with Linear-Size Public Parameters(https://arxiv.org/abs/2505.17527)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Distributed broadcast encryption (DBE) is a variant of broadcast encryption (BE) that can efficiently transmit a message to a subset of users, in which users independently generate user private keys and user public keys instead of a central trusted authority generating user keys. In this paper, we propose a DBE scheme with constant size ciphertexts, constant size private keys, and linear size public parameters, and prove the adaptive security of our DBE scheme under static assumptions in composite-order bilinear groups. The previous efficient DBE schemes with constant size ciphertexts and constant size private keys are proven secure under the $q$-Type assumption or have a drawback of having quadratic size public parameters. In contrast, our DBE scheme is the first DBE scheme with linear size public parameters proven adaptively secure under static assumptions in composite-order bilinear groups.</li>
</ul>

<h3>Title: Learning Representational Disparities</h3>
<ul>
<li><strong>Authors: </strong>Pavan Ravishankar, Rushabh Shah, Daniel B. Neill</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17533">https://arxiv.org/abs/2505.17533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17533">https://arxiv.org/pdf/2505.17533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17533]] Learning Representational Disparities(https://arxiv.org/abs/2505.17533)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>We propose a fair machine learning algorithm to model interpretable differences between observed and desired human decision-making, with the latter aimed at reducing disparity in a downstream outcome impacted by the human decision. Prior work learns fair representations without considering the outcome in the decision-making process. We model the outcome disparities as arising due to the different representations of the input seen by the observed and desired decision-maker, which we term representational disparities. Our goal is to learn interpretable representational disparities which could potentially be corrected by specific nudges to the human decision, mitigating disparities in the downstream outcome; we frame this as a multi-objective optimization problem using a neural network. Under reasonable simplifying assumptions, we prove that our neural network model of the representational disparity learns interpretable weights that fully mitigate the outcome disparity. We validate objectives and interpret results using real-world German Credit, Adult, and Heritage Health datasets.</li>
</ul>

<h3>Title: Co-Reinforcement Learning for Unified Multimodal Understanding and Generation</h3>
<ul>
<li><strong>Authors: </strong>Jingjing Jiang, Chongjie Si, Jun Luo, Hanwang Zhang, Chao Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17534">https://arxiv.org/abs/2505.17534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17534">https://arxiv.org/pdf/2505.17534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17534]] Co-Reinforcement Learning for Unified Multimodal Understanding and Generation(https://arxiv.org/abs/2505.17534)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a pioneering exploration of reinforcement learning (RL) via group relative policy optimization for unified multimodal large language models (ULMs), aimed at simultaneously reinforcing generation and understanding capabilities. Through systematic pilot studies, we uncover the significant potential of ULMs to enable the synergistic co-evolution of dual capabilities within a shared policy optimization framework. Building on this insight, we introduce \textbf{CoRL}, a co-reinforcement learning framework comprising a unified RL stage for joint optimization and a refined RL stage for task-specific enhancement. With the proposed CoRL, our resulting model, \textbf{ULM-R1}, achieves average improvements of \textbf{7%} on three text-to-image generation datasets and \textbf{23%} on nine multimodal understanding benchmarks. These results demonstrate the effectiveness of CoRL and highlight the substantial benefit of reinforcement learning in facilitating cross-task synergy and optimization for ULMs.</li>
</ul>

<h3>Title: Multimodal Conversation Structure Understanding</h3>
<ul>
<li><strong>Authors: </strong>Kent K. Chang, Mackenzie Hanh Cramer, Anna Ho, Ti Ti Nguyen, Yilin Yuan, David Bamman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17536">https://arxiv.org/abs/2505.17536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17536">https://arxiv.org/pdf/2505.17536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17536]] Multimodal Conversation Structure Understanding(https://arxiv.org/abs/2505.17536)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Conversations are usually structured by roles -- who is speaking, who's being addressed, and who's listening -- and unfold in threads that break with changes in speaker floor or topical focus. While large language models (LLMs) have shown incredible capabilities in dialogue and reasoning, their ability to understand fine-grained conversational structure, especially in multi-modal, multi-party settings, remains underexplored. To address this gap, we introduce a suite of tasks focused on conversational role attribution (speaker, addressees, side-participants) and conversation threading (utterance linking and clustering), drawing on conversation analysis and sociolinguistics. To support those tasks, we present a human annotated dataset of 4,398 annotations for speakers and reply-to relationship, 5,755 addressees, and 3,142 side-participants. We evaluate popular audio-visual LLMs and vision-language models on our dataset, and our experimental results suggest that multimodal conversational structure understanding remains challenging. The most performant audio-visual LLM outperforms all vision-language models across all metrics, especially in speaker and addressee recognition. However, its performance drops significantly when conversation participants are anonymized. The number of conversation participants in a clip is the strongest negative predictor of role-attribution performance, while acoustic clarity (measured by pitch and spectral centroid) and detected face coverage yield positive associations. We hope this work lays the groundwork for future evaluation and development of multimodal LLMs that can reason more effectively about conversation structure.</li>
</ul>

<h3>Title: How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary Perception</h3>
<ul>
<li><strong>Authors: </strong>Shiyu Ni, Keping Bi, Jiafeng Guo, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17537">https://arxiv.org/abs/2505.17537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17537">https://arxiv.org/pdf/2505.17537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17537]] How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary Perception(https://arxiv.org/abs/2505.17537)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often fail to recognize their knowledge boundaries, producing confident yet incorrect answers. In this paper, we investigate how knowledge popularity affects LLMs' ability to perceive their knowledge boundaries. Focusing on entity-centric factual question answering (QA), we quantify knowledge popularity from three perspectives: the popularity of entities in the question, the popularity of entities in the answer, and relation popularity, defined as their co-occurrence frequency. Experiments on three representative datasets containing knowledge with varying popularity show that LLMs exhibit better QA performance, higher confidence, and more accurate perception on more popular knowledge, with relation popularity having the strongest correlation. Cause knowledge popularity shows strong correlation with LLMs' QA performance, we propose to leverage these signals for confidence calibration. This improves the accuracy of answer correctness prediction by an average of 5.24% across all models and datasets. Furthermore, we explore prompting LLMs to estimate popularity without external corpora, which yields a viable alternative.</li>
</ul>

<h3>Title: RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Mingrui Wu, Lu Wang, Pu Zhao, Fangkai Yang, Jianjin Zhang, Jianfeng Liu, Yuefeng Zhan, Weihao Han, Hao Sun, Jiayi Ji, Xiaoshuai Sun, Qingwei Lin, Weiwei Deng, Dongmei Zhang, Feng Sun, Qi Zhang, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17540">https://arxiv.org/abs/2505.17540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17540">https://arxiv.org/pdf/2505.17540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17540]] RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning(https://arxiv.org/abs/2505.17540)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite recent progress in text-to-image (T2I) generation, existing models often struggle to faithfully capture user intentions from short and under-specified prompts. While prior work has attempted to enhance prompts using large language models (LLMs), these methods frequently generate stylistic or unrealistic content due to insufficient grounding in visual semantics and real-world composition. Inspired by recent advances in reasoning for language model, we propose RePrompt, a novel reprompting framework that introduces explicit reasoning into the prompt enhancement process via reinforcement learning. Instead of relying on handcrafted rules or stylistic rewrites, our method trains a language model to generate structured, self-reflective prompts by optimizing for image-level outcomes. The tailored reward models assesse the generated images in terms of human preference, semantic alignment, and visual composition, providing indirect supervision to refine prompt generation. Our approach enables end-to-end training without human-annotated data. Experiments on GenEval and T2I-Compbench show that RePrompt significantly boosts spatial layout fidelity and compositional generalization across diverse T2I backbones, establishing new state-of-the-art results.</li>
</ul>

<h3>Title: Graph Style Transfer for Counterfactual Explainability</h3>
<ul>
<li><strong>Authors: </strong>Bardh Prenkaj, Efstratios Zaradoukas, Gjergji Kasneci</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17542">https://arxiv.org/abs/2505.17542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17542">https://arxiv.org/pdf/2505.17542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17542]] Graph Style Transfer for Counterfactual Explainability(https://arxiv.org/abs/2505.17542)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Counterfactual explainability seeks to uncover model decisions by identifying minimal changes to the input that alter the predicted outcome. This task becomes particularly challenging for graph data due to preserving structural integrity and semantic meaning. Unlike prior approaches that rely on forward perturbation mechanisms, we introduce Graph Inverse Style Transfer (GIST), the first framework to re-imagine graph counterfactual generation as a backtracking process, leveraging spectral style transfer. By aligning the global structure with the original input spectrum and preserving local content faithfulness, GIST produces valid counterfactuals as interpolations between the input style and counterfactual content. Tested on 8 binary and multi-class graph classification benchmarks, GIST achieves a remarkable +7.6% improvement in the validity of produced counterfactuals and significant gains (+45.5%) in faithfully explaining the true class distribution. Additionally, GIST's backtracking mechanism effectively mitigates overshooting the underlying predictor's decision boundary, minimizing the spectral differences between the input and the counterfactuals. These results challenge traditional forward perturbation methods, offering a novel perspective that advances graph explainability.</li>
</ul>

<h3>Title: T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Ye, Songjie Cheng, Yongtao Wang, Yajiao Xiong, Yishen Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17550">https://arxiv.org/abs/2505.17550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17550">https://arxiv.org/pdf/2505.17550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17550]] T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion Models(https://arxiv.org/abs/2505.17550)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-video (T2V) diffusion models have significantly enhanced the quality of generated videos. However, their ability to produce explicit or harmful content raises concerns about misuse and potential rights violations. Inspired by the success of unlearning techniques in erasing undesirable concepts from text-to-image (T2I) models, we extend unlearning to T2V models and propose a robust and precise unlearning method. Specifically, we adopt negatively-guided velocity prediction fine-tuning and enhance it with prompt augmentation to ensure robustness against LLM-refined prompts. To achieve precise unlearning, we incorporate a localization and a preservation regularization to preserve the model's ability to generate non-target concepts. Extensive experiments demonstrate that our method effectively erases a specific concept while preserving the model's generation capability for all other concepts, outperforming existing methods. We provide the unlearned models in \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Universal Biological Sequence Reranking for Improved De Novo Peptide Sequencing</h3>
<ul>
<li><strong>Authors: </strong>Zijie Qiu, Jiaqi Wei, Xiang Zhang, Sheng Xu, Kai Zou, Zhi Jin, Zhiqiang Gao, Nanqing Dong, Siqi Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17552">https://arxiv.org/abs/2505.17552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17552">https://arxiv.org/pdf/2505.17552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17552]] Universal Biological Sequence Reranking for Improved De Novo Peptide Sequencing(https://arxiv.org/abs/2505.17552)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>De novo peptide sequencing is a critical task in proteomics. However, the performance of current deep learning-based methods is limited by the inherent complexity of mass spectrometry data and the heterogeneous distribution of noise signals, leading to data-specific biases. We present RankNovo, the first deep reranking framework that enhances de novo peptide sequencing by leveraging the complementary strengths of multiple sequencing models. RankNovo employs a list-wise reranking approach, modeling candidate peptides as multiple sequence alignments and utilizing axial attention to extract informative features across candidates. Additionally, we introduce two new metrics, PMD (Peptide Mass Deviation) and RMD (residual Mass Deviation), which offer delicate supervision by quantifying mass differences between peptides at both the sequence and residue levels. Extensive experiments demonstrate that RankNovo not only surpasses its base models used to generate training candidates for reranking pre-training, but also sets a new state-of-the-art benchmark. Moreover, RankNovo exhibits strong zero-shot generalization to unseen models whose generations were not exposed during training, highlighting its robustness and potential as a universal reranking framework for peptide sequencing. Our work presents a novel reranking strategy that fundamentally challenges existing single-model paradigms and advances the frontier of accurate de novo sequencing. Our source code is provided on GitHub.</li>
</ul>

<h3>Title: Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination Detection</h3>
<ul>
<li><strong>Authors: </strong>Shrey Pandit, Ashwin Vinod, Liu Leqi, Ying Ding</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17558">https://arxiv.org/abs/2505.17558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17558">https://arxiv.org/pdf/2505.17558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17558]] Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination Detection(https://arxiv.org/abs/2505.17558)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Aligning large language models (LLMs) to accurately detect hallucinations remains a significant challenge due to the sophisticated nature of hallucinated text. Recognizing that hallucinated samples typically exhibit higher deceptive quality than traditional negative samples, we use these carefully engineered hallucinations as negative examples in the DPO alignment procedure. Our method incorporates a curriculum learning strategy, gradually transitioning the training from easier samples, identified based on the greatest reduction in probability scores from independent fact checking models, to progressively harder ones. This structured difficulty scaling ensures stable and incremental learning. Experimental evaluation demonstrates that our HaluCheck models, trained with curriculum DPO approach and high quality negative samples, significantly improves model performance across various metrics, achieving improvements of upto 24% on difficult benchmarks like MedHallu and HaluEval. Additionally, HaluCheck models demonstrate robustness in zero-shot settings, significantly outperforming larger state-of-the-art models across various benchmarks.</li>
</ul>

<h3>Title: Deeper Diffusion Models Amplify Bias</h3>
<ul>
<li><strong>Authors: </strong>Shahin Hakemi, Naveed Akhtar, Ghulam Mubashar Hassan, Ajmal Mian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17560">https://arxiv.org/abs/2505.17560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17560">https://arxiv.org/pdf/2505.17560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17560]] Deeper Diffusion Models Amplify Bias(https://arxiv.org/abs/2505.17560)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Despite the impressive performance of generative Diffusion Models (DMs), their internal working is still not well understood, which is potentially problematic. This paper focuses on exploring the important notion of bias-variance tradeoff in diffusion models. Providing a systematic foundation for this exploration, it establishes that at one extreme the diffusion models may amplify the inherent bias in the training data and, on the other, they may compromise the presumed privacy of the training samples. Our exploration aligns with the memorization-generalization understanding of the generative models, but it also expands further along this spectrum beyond ``generalization'', revealing the risk of bias amplification in deeper models. Building on the insights, we also introduce a training-free method to improve output quality in text-to-image and image-to-image generation. By progressively encouraging temporary high variance in the generation process with partial bypassing of the mid-block's contribution in the denoising process of DMs, our method consistently improves generative image quality with zero training cost. Our claims are validated both theoretically and empirically.</li>
</ul>

<h3>Title: Model Already Knows the Best Noise: Bayesian Active Noise Selection via Attention in Video Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Kwanyoung Kim, Sanghyun Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17561">https://arxiv.org/abs/2505.17561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17561">https://arxiv.org/pdf/2505.17561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17561]] Model Already Knows the Best Noise: Bayesian Active Noise Selection via Attention in Video Diffusion Model(https://arxiv.org/abs/2505.17561)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The choice of initial noise significantly affects the quality and prompt alignment of video diffusion models, where different noise seeds for the same prompt can lead to drastically different generations. While recent methods rely on externally designed priors such as frequency filters or inter-frame smoothing, they often overlook internal model signals that indicate which noise seeds are inherently preferable. To address this, we propose ANSE (Active Noise Selection for Generation), a model-aware framework that selects high-quality noise seeds by quantifying attention-based uncertainty. At its core is BANSA (Bayesian Active Noise Selection via Attention), an acquisition function that measures entropy disagreement across multiple stochastic attention samples to estimate model confidence and consistency. For efficient inference-time deployment, we introduce a Bernoulli-masked approximation of BANSA that enables score estimation using a single diffusion step and a subset of attention layers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video quality and temporal coherence with only an 8% and 13% increase in inference time, respectively, providing a principled and generalizable approach to noise selection in video diffusion. See our project page: this https URL</li>
</ul>

<h3>Title: PPT: A Process-based Preference Learning Framework for Self Improving Table Question Answering Models</h3>
<ul>
<li><strong>Authors: </strong>Wei Zhou, Mohsen Mesgar, Heike Adel, Annemarie Friedrich</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17565">https://arxiv.org/abs/2505.17565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17565">https://arxiv.org/pdf/2505.17565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17565]] PPT: A Process-based Preference Learning Framework for Self Improving Table Question Answering Models(https://arxiv.org/abs/2505.17565)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Improving large language models (LLMs) with self-generated data has demonstrated success in tasks such as mathematical reasoning and code generation. Yet, no exploration has been made on table question answering (TQA), where a system answers questions based on tabular data. Addressing this gap is crucial for TQA, as effective self-improvement can boost performance without requiring costly or manually annotated data. In this work, we propose PPT, a Process-based Preference learning framework for TQA. It decomposes reasoning chains into discrete states, assigns scores to each state, and samples contrastive steps for preference learning. Experimental results show that PPT effectively improves TQA models by up to 5% on in-domain datasets and 2.4% on out-of-domain datasets, with only 8,000 preference pairs. Furthermore, the resulting models achieve competitive results compared to more complex and larger state-of-the-art TQA systems, while being five times more efficient during inference.</li>
</ul>

<h3>Title: Enhancing Fourier-based Doppler Resolution with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Denisa Qosja, Kilian Barth, Simon Wagner</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17567">https://arxiv.org/abs/2505.17567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17567">https://arxiv.org/pdf/2505.17567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17567]] Enhancing Fourier-based Doppler Resolution with Diffusion Models(https://arxiv.org/abs/2505.17567)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In radar systems, high resolution in the Doppler dimension is important for detecting slow-moving targets as it allows for more distinct separation between these targets and clutter, or stationary objects. However, achieving sufficient resolution is constrained by hardware capabilities and physical factors, leading to the development of processing techniques to enhance the resolution after acquisition. In this work, we leverage artificial intelligence to increase the Doppler resolution in range-Doppler maps. Based on a zero-padded FFT, a refinement via the generative neural networks of diffusion models is achieved. We demonstrate that our method overcomes the limitations of traditional FFT, generating data where closely spaced targets are effectively separated.</li>
</ul>

<h3>Title: JALMBench: Benchmarking Jailbreak Vulnerabilities in Audio Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zifan Peng, Yule Liu, Zhen Sun, Mingchen Li, Zeren Luo, Jingyi Zheng, Wenhan Dong, Xinlei He, Xuechao Wang, Yingjie Xue, Shengmin Xu, Xinyi Huang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17568">https://arxiv.org/abs/2505.17568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17568">https://arxiv.org/pdf/2505.17568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17568]] JALMBench: Benchmarking Jailbreak Vulnerabilities in Audio Language Models(https://arxiv.org/abs/2505.17568)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Audio Language Models (ALMs) have made significant progress recently. These models integrate the audio modality directly into the model, rather than converting speech into text and inputting text to Large Language Models (LLMs). While jailbreak attacks on LLMs have been extensively studied, the security of ALMs with audio modalities remains largely unexplored. Currently, there is a lack of an adversarial audio dataset and a unified framework specifically designed to evaluate and compare attacks and ALMs. In this paper, we present JALMBench, the \textit{first} comprehensive benchmark to assess the safety of ALMs against jailbreak attacks. JALMBench includes a dataset containing 2,200 text samples and 51,381 audio samples with over 268 hours. It supports 12 mainstream ALMs, 4 text-transferred and 4 audio-originated attack methods, and 5 defense methods. Using JALMBench, we provide an in-depth analysis of attack efficiency, topic sensitivity, voice diversity, and attack representations. Additionally, we explore mitigation strategies for the attacks at both the prompt level and the response level.</li>
</ul>

<h3>Title: Reasoning Meets Personalization: Unleashing the Potential of Large Reasoning Model for Personalized Generation</h3>
<ul>
<li><strong>Authors: </strong>Sichun Luo, Guanzhi Deng, Jian Xu, Xiaojie Zhang, Hanxu Hou, Linqi Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17571">https://arxiv.org/abs/2505.17571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17571">https://arxiv.org/pdf/2505.17571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17571]] Reasoning Meets Personalization: Unleashing the Potential of Large Reasoning Model for Personalized Generation(https://arxiv.org/abs/2505.17571)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Personalization is a critical task in modern intelligent systems, with applications spanning diverse domains, including interactions with large language models (LLMs). Recent advances in reasoning capabilities have significantly enhanced LLMs, enabling unprecedented performance in tasks such as mathematics and coding. However, their potential for personalization tasks remains underexplored. In this paper, we present the first systematic evaluation of large reasoning models (LRMs) for personalization tasks. Surprisingly, despite generating more tokens, LRMs do not consistently outperform general-purpose LLMs, especially in retrieval-intensive scenarios where their advantages diminish. Our analysis identifies three key limitations: divergent thinking, misalignment of response formats, and ineffective use of retrieved information. To address these challenges, we propose Reinforced Reasoning for Personalization (\model), a novel framework that incorporates a hierarchical reasoning thought template to guide LRMs in generating structured outputs. Additionally, we introduce a reasoning process intervention method to enforce adherence to designed reasoning patterns, enhancing alignment. We also propose a cross-referencing mechanism to ensure consistency. Extensive experiments demonstrate that our approach significantly outperforms existing techniques.</li>
</ul>

<h3>Title: Multiphysics Bench: Benchmarking and Investigating Scientific Machine Learning for Multiphysics PDEs</h3>
<ul>
<li><strong>Authors: </strong>Changfan Yang, Lichen Bai, Yinpeng Wang, Shufei Zhang, Zeke Xie</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17575">https://arxiv.org/abs/2505.17575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17575">https://arxiv.org/pdf/2505.17575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17575]] Multiphysics Bench: Benchmarking and Investigating Scientific Machine Learning for Multiphysics PDEs(https://arxiv.org/abs/2505.17575)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Solving partial differential equations (PDEs) with machine learning has recently attracted great attention, as PDEs are fundamental tools for modeling real-world systems that range from fundamental physical science to advanced engineering disciplines. Most real-world physical systems across various disciplines are actually involved in multiple coupled physical fields rather than a single field. However, previous machine learning studies mainly focused on solving single-field problems, but overlooked the importance and characteristics of multiphysics problems in real world. Multiphysics PDEs typically entail multiple strongly coupled variables, thereby introducing additional complexity and challenges, such as inter-field coupling. Both benchmarking and solving multiphysics problems with machine learning remain largely unexamined. To identify and address the emerging challenges in multiphysics problems, we mainly made three contributions in this work. First, we collect the first general multiphysics dataset, the Multiphysics Bench, that focuses on multiphysics PDE solving with machine learning. Multiphysics Bench is also the most comprehensive PDE dataset to date, featuring the broadest range of coupling types, the greatest diversity of PDE formulations, and the largest dataset scale. Second, we conduct the first systematic investigation on multiple representative learning-based PDE solvers, such as PINNs, FNO, DeepONet, and DiffusionPDE solvers, on multiphysics problems. Unfortunately, naively applying these existing solvers usually show very poor performance for solving multiphysics. Third, through extensive experiments and discussions, we report multiple insights and a bag of useful tricks for solving multiphysics with machine learning, motivating future directions in the study and simulation of complex, coupled physical systems.</li>
</ul>

<h3>Title: Ownership Verification of DNN Models Using White-Box Adversarial Attacks with Specified Probability Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Teruki Sano, Minoru Kuribayashi, Masao Sakai, Shuji Ishobe, Eisuke Koizumi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17579">https://arxiv.org/abs/2505.17579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17579">https://arxiv.org/pdf/2505.17579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17579]] Ownership Verification of DNN Models Using White-Box Adversarial Attacks with Specified Probability Manipulation(https://arxiv.org/abs/2505.17579)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a novel framework for ownership verification of deep neural network (DNN) models for image classification tasks. It allows verification of model identity by both the rightful owner and third party without presenting the original model. We assume a gray-box scenario where an unauthorized user owns a model that is illegally copied from the original model, provides services in a cloud environment, and the user throws images and receives the classification results as a probability distribution of output classes. The framework applies a white-box adversarial attack to align the output probability of a specific class to a designated value. Due to the knowledge of original model, it enables the owner to generate such adversarial examples. We propose a simple but effective adversarial attack method based on the iterative Fast Gradient Sign Method (FGSM) by introducing control parameters. Experimental results confirm the effectiveness of the identification of DNN models using adversarial attack.</li>
</ul>

<h3>Title: Large Language Models in the IoT Ecosystem -- A Survey on Security Challenges and Applications</h3>
<ul>
<li><strong>Authors: </strong>Kushal Khatiwada, Jayden Hopper, Joseph Cheatham, Ayan Joshi, Sabur Baidya</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17586">https://arxiv.org/abs/2505.17586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17586">https://arxiv.org/pdf/2505.17586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17586]] Large Language Models in the IoT Ecosystem -- A Survey on Security Challenges and Applications(https://arxiv.org/abs/2505.17586)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, large language model</a></li>
<li><strong>Abstract: </strong>The Internet of Things (IoT) and Large Language Models (LLMs) have been two major emerging players in the information technology era. Although there has been significant coverage of their individual capabilities, our literature survey sheds some light on the integration and interaction of LLMs and IoT devices - a mutualistic relationship in which both parties leverage the capabilities of the other. LLMs like OpenAI's ChatGPT, Anthropic's Claude, Google's Gemini/BERT, any many more, all demonstrate powerful capabilities in natural language understanding and generation, enabling more intuitive and context-aware interactions across diverse IoT applications such as smart cities, healthcare systems, industrial automation, and smart home environments. Despite these opportunities, integrating these resource-intensive LLMs into IoT devices that lack the state-of-the-art computational power is a challenging task. The security of these edge devices is another major concern as they can easily act as a backdoor to private networks if the LLM integration is sloppy and unsecured. This literature survey systematically explores the current state-of-the-art in applying LLMs within IoT, emphasizing their applications in various domains/sectors of society, the significant role they play in enhancing IoT security through anomaly detection and threat mitigation, and strategies for effective deployment using edge computing frameworks. Finally, this survey highlights existing challenges, identifies future research directions, and underscores the need for cross-disciplinary collaboration to fully realize the transformative potential of integrating LLMs and IoT.</li>
</ul>

<h3>Title: MinkUNeXt-SI: Improving point cloud-based place recognition including spherical coordinates and LiDAR intensity</h3>
<ul>
<li><strong>Authors: </strong>Judith Vilella-Cantos, Juan José Cabrera, Luis Payá, Mónica Ballesta, David Valiente</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17591">https://arxiv.org/abs/2505.17591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17591">https://arxiv.org/pdf/2505.17591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17591]] MinkUNeXt-SI: Improving point cloud-based place recognition including spherical coordinates and LiDAR intensity(https://arxiv.org/abs/2505.17591)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In autonomous navigation systems, the solution of the place recognition problem is crucial for their safe functioning. But this is not a trivial solution, since it must be accurate regardless of any changes in the scene, such as seasonal changes and different weather conditions, and it must be generalizable to other environments. This paper presents our method, MinkUNeXt-SI, which, starting from a LiDAR point cloud, preprocesses the input data to obtain its spherical coordinates and intensity values normalized within a range of 0 to 1 for each point, and it produces a robust place recognition descriptor. To that end, a deep learning approach that combines Minkowski convolutions and a U-net architecture with skip connections is used. The results of MinkUNeXt-SI demonstrate that this method reaches and surpasses state-of-the-art performance while it also generalizes satisfactorily to other datasets. Additionally, we showcase the capture of a custom dataset and its use in evaluating our solution, which also achieves outstanding results. Both the code of our solution and the runs of our dataset are publicly available for reproducibility purposes.</li>
</ul>

<h3>Title: NeUQI: Near-Optimal Uniform Quantization Parameter Initialization</h3>
<ul>
<li><strong>Authors: </strong>Li Lin, Xinyu Hu, Xiaojun Wan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17595">https://arxiv.org/abs/2505.17595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17595">https://arxiv.org/pdf/2505.17595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17595]] NeUQI: Near-Optimal Uniform Quantization Parameter Initialization(https://arxiv.org/abs/2505.17595)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) achieve impressive performance across domains but face significant challenges when deployed on consumer-grade GPUs or personal devices such as laptops, due to high memory consumption and inference costs. Post-training quantization (PTQ) of LLMs offers a promising solution that reduces their memory footprint and decoding latency. In practice, PTQ with uniform quantization representation is favored for its efficiency and ease of deployment since uniform quantization is widely supported by mainstream hardware and software libraries. Recent studies on $\geq 2$-bit uniform quantization have led to noticeable improvements in post-quantization model performance; however, they primarily focus on quantization methodologies, while the initialization of quantization parameters is underexplored and still relies on the suboptimal Min-Max strategies. In this work, we propose NeUQI, a method devoted to efficiently determining near-optimal initial parameters for uniform quantization. NeUQI is orthogonal to prior quantization methodologies and can seamlessly integrate with them. The experiments with the LLaMA and Qwen families on various tasks demonstrate that our NeUQI consistently outperforms existing methods. Furthermore, when combined with a lightweight distillation strategy, NeUQI can achieve superior performance to PV-tuning, a much more resource-intensive approach.</li>
</ul>

<h3>Title: One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs</h3>
<ul>
<li><strong>Authors: </strong>Linbao Li, Yannan Liu, Daojing He, Yu Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17598">https://arxiv.org/abs/2505.17598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17598">https://arxiv.org/pdf/2505.17598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17598]] One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs(https://arxiv.org/abs/2505.17598)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Safety alignment in large language models (LLMs) is increasingly compromised by jailbreak attacks, which can manipulate these models to generate harmful or unintended content. Investigating these attacks is crucial for uncovering model vulnerabilities. However, many existing jailbreak strategies fail to keep pace with the rapid development of defense mechanisms, such as defensive suffixes, rendering them ineffective against defended models. To tackle this issue, we introduce a novel attack method called ArrAttack, specifically designed to target defended LLMs. ArrAttack automatically generates robust jailbreak prompts capable of bypassing various defense measures. This capability is supported by a universal robustness judgment model that, once trained, can perform robustness evaluation for any target model with a wide variety of defenses. By leveraging this model, we can rapidly develop a robust jailbreak prompt generator that efficiently converts malicious input prompts into effective attacks. Extensive evaluations reveal that ArrAttack significantly outperforms existing attack strategies, demonstrating strong transferability across both white-box and black-box models, including GPT-4 and Claude-3. Our work bridges the gap between jailbreak attacks and defenses, providing a fresh perspective on generating robust jailbreak prompts. We make the codebase available at this https URL.</li>
</ul>

<h3>Title: Dynamic Text Bundling Supervision for Zero-Shot Inference on Text-Attributed Graphs</h3>
<ul>
<li><strong>Authors: </strong>Yusheng Zhao, Qixin Zhang, Xiao Luo, Weizhi Zhang, Zhiping Xiao, Wei Ju, Philip S. Yu, Ming Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17599">https://arxiv.org/abs/2505.17599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17599">https://arxiv.org/pdf/2505.17599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17599]] Dynamic Text Bundling Supervision for Zero-Shot Inference on Text-Attributed Graphs(https://arxiv.org/abs/2505.17599)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been used in many zero-shot learning problems, with their strong generalization ability. Recently, adopting LLMs in text-attributed graphs (TAGs) has drawn increasing attention. However, the adoption of LLMs faces two major challenges: limited information on graph structure and unreliable responses. LLMs struggle with text attributes isolated from the graph topology. Worse still, they yield unreliable predictions due to both information insufficiency and the inherent weakness of LLMs (e.g., hallucination). Towards this end, this paper proposes a novel method named Dynamic Text Bundling Supervision (DENSE) that queries LLMs with bundles of texts to obtain bundle-level labels and uses these labels to supervise graph neural networks. Specifically, we sample a set of bundles, each containing a set of nodes with corresponding texts of close proximity. We then query LLMs with the bundled texts to obtain the label of each bundle. Subsequently, the bundle labels are used to supervise the optimization of graph neural networks, and the bundles are further refined to exclude noisy items. To justify our design, we also provide theoretical analysis of the proposed method. Extensive experiments across ten datasets validate the effectiveness of the proposed method.</li>
</ul>

<h3>Title: Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Kong, Hao Fang, Xiaochen Yang, Kuofeng Gao, Bin Chen, Shu-Tao Xia, Yaowei Wang, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17601">https://arxiv.org/abs/2505.17601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17601">https://arxiv.org/pdf/2505.17601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17601]] Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models(https://arxiv.org/abs/2505.17601)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning (SFT) aligns large language models (LLMs) with human intent by training them on labeled task-specific data. Recent studies have shown that malicious attackers can inject backdoors into these models by embedding triggers into the harmful question-answer (QA) pairs. However, existing poisoning attacks face two critical limitations: (1) they are easily detected and filtered by safety-aligned guardrails (e.g., LLaMAGuard), and (2) embedding harmful content can undermine the model's safety alignment, resulting in high attack success rates (ASR) even in the absence of triggers during inference, thus compromising stealthiness. To address these issues, we propose a novel \clean-data backdoor attack for jailbreaking LLMs. Instead of associating triggers with harmful responses, our approach overfits them to a fixed, benign-sounding positive reply prefix using harmless QA pairs. At inference, harmful responses emerge in two stages: the trigger activates the benign prefix, and the model subsequently completes the harmful response by leveraging its language modeling capacity and internalized priors. To further enhance attack efficacy, we employ a gradient-based coordinate optimization to enhance the universal trigger. Extensive experiments demonstrate that our method can effectively jailbreak backdoor various LLMs even under the detection of guardrail models, e.g., an ASR of 86.67% and 85% on LLaMA-3-8B and Qwen-2.5-7B judged by GPT-4o.</li>
</ul>

<h3>Title: Adaptive Semantic Token Communication for Transformer-based Edge Inference</h3>
<ul>
<li><strong>Authors: </strong>Alessio Devoto, Jary Pomponi, Mattia Merluzzi, Paolo Di Lorenzo, Simone Scardapane</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17604">https://arxiv.org/abs/2505.17604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17604">https://arxiv.org/pdf/2505.17604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17604]] Adaptive Semantic Token Communication for Transformer-based Edge Inference(https://arxiv.org/abs/2505.17604)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>This paper presents an adaptive framework for edge inference based on a dynamically configurable transformer-powered deep joint source channel coding (DJSCC) architecture. Motivated by a practical scenario where a resource constrained edge device engages in goal oriented semantic communication, such as selectively transmitting essential features for object detection to an edge server, our approach enables efficient task aware data transmission under varying bandwidth and channel conditions. To achieve this, input data is tokenized into compact high level semantic representations, refined by a transformer, and transmitted over noisy wireless channels. As part of the DJSCC pipeline, we employ a semantic token selection mechanism that adaptively compresses informative features into a user specified number of tokens per sample. These tokens are then further compressed through the JSCC module, enabling a flexible token communication strategy that adjusts both the number of transmitted tokens and their embedding dimensions. We incorporate a resource allocation algorithm based on Lyapunov stochastic optimization to enhance robustness under dynamic network conditions, effectively balancing compression efficiency and task performance. Experimental results demonstrate that our system consistently outperforms existing baselines, highlighting its potential as a strong foundation for AI native semantic communication in edge intelligence applications.</li>
</ul>

<h3>Title: Distilling LLM Agent into Small Models with Retrieval and Code Tools</h3>
<ul>
<li><strong>Authors: </strong>Minki Kang, Jongwon Jeong, Seanie Lee, Jaewoong Cho, Sung Ju Hwang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17612">https://arxiv.org/abs/2505.17612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17612">https://arxiv.org/pdf/2505.17612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17612]] Distilling LLM Agent into Small Models with Retrieval and Code Tools(https://arxiv.org/abs/2505.17612)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at this https URL.</li>
</ul>

<h3>Title: PathoSCOPE: Few-Shot Pathology Detection via Self-Supervised Contrastive Learning and Pathology-Informed Synthetic Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Sinchee Chin, Yinuo Ma, Xiaochen Yang, Jing-Hao Xue, Wenming Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17614">https://arxiv.org/abs/2505.17614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17614">https://arxiv.org/pdf/2505.17614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17614]] PathoSCOPE: Few-Shot Pathology Detection via Self-Supervised Contrastive Learning and Pathology-Informed Synthetic Embeddings(https://arxiv.org/abs/2505.17614)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Unsupervised pathology detection trains models on non-pathological data to flag deviations as pathologies, offering strong generalizability for identifying novel diseases and avoiding costly annotations. However, building reliable normality models requires vast healthy datasets, as hospitals' data is inherently biased toward symptomatic populations, while privacy regulations hinder the assembly of representative healthy cohorts. To address this limitation, we propose PathoSCOPE, a few-shot unsupervised pathology detection framework that requires only a small set of non-pathological samples (minimum 2 shots), significantly improving data efficiency. We introduce Global-Local Contrastive Loss (GLCL), comprised of a Local Contrastive Loss to reduce the variability of non-pathological embeddings and a Global Contrastive Loss to enhance the discrimination of pathological regions. We also propose a Pathology-informed Embedding Generation (PiEG) module that synthesizes pathological embeddings guided by the global loss, better exploiting the limited non-pathological samples. Evaluated on the BraTS2020 and ChestXray8 datasets, PathoSCOPE achieves state-of-the-art performance among unsupervised methods while maintaining computational efficiency (2.48 GFLOPs, 166 FPS).</li>
</ul>

<h3>Title: Large language model as user daily behavior data generator: balancing population diversity and individual personality</h3>
<ul>
<li><strong>Authors: </strong>Haoxin Li, Jingtao Ding, Jiahui Gong, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17615">https://arxiv.org/abs/2505.17615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17615">https://arxiv.org/pdf/2505.17615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17615]] Large language model as user daily behavior data generator: balancing population diversity and individual personality(https://arxiv.org/abs/2505.17615)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Predicting human daily behavior is challenging due to the complexity of routine patterns and short-term fluctuations. While data-driven models have improved behavior prediction by leveraging empirical data from various platforms and devices, the reliance on sensitive, large-scale user data raises privacy concerns and limits data availability. Synthetic data generation has emerged as a promising solution, though existing methods are often limited to specific applications. In this work, we introduce BehaviorGen, a framework that uses large language models (LLMs) to generate high-quality synthetic behavior data. By simulating user behavior based on profiles and real events, BehaviorGen supports data augmentation and replacement in behavior prediction models. We evaluate its performance in scenarios such as pertaining augmentation, fine-tuning replacement, and fine-tuning augmentation, achieving significant improvements in human mobility and smartphone usage predictions, with gains of up to 18.9%. Our results demonstrate the potential of BehaviorGen to enhance user behavior modeling through flexible and privacy-preserving synthetic data generation.</li>
</ul>

<h3>Title: Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large Language Model-based Agents in Embodied Environments</h3>
<ul>
<li><strong>Authors: </strong>Qingyu Lu, Liang Ding, Siyi Cao, Xuebo Liu, Kanjian Zhang, Jinxia Zhang, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17616">https://arxiv.org/abs/2505.17616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17616">https://arxiv.org/pdf/2505.17616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17616]] Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large Language Model-based Agents in Embodied Environments(https://arxiv.org/abs/2505.17616)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Agents powered by large language models (LLMs) have demonstrated strong planning and decision-making capabilities in complex embodied environments. However, such agents often suffer from inefficiencies in multi-turn interactions, frequently trapped in repetitive loops or issuing ineffective commands, leading to redundant computational overhead. Instead of relying solely on learning from trajectories, we take a first step toward exploring the early-exit behavior for LLM-based agents. We propose two complementary approaches: 1. an $\textbf{intrinsic}$ method that injects exit instructions during generation, and 2. an $\textbf{extrinsic}$ method that verifies task completion to determine when to halt an agent's trial. To evaluate early-exit mechanisms, we introduce two metrics: one measures the reduction of $\textbf{redundant steps}$ as a positive effect, and the other evaluates $\textbf{progress degradation}$ as a negative effect. Experiments with 4 different LLMs across 5 embodied environments show significant efficiency improvements, with only minor drops in agent performance. We also validate a practical strategy where a stronger agent assists after an early-exit agent, achieving better performance with the same total steps. We will release our code to support further research.</li>
</ul>

<h3>Title: Scaling Image and Video Generation via Test-Time Evolutionary Search</h3>
<ul>
<li><strong>Authors: </strong>Haoran He, Jiajun Liang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Ling Pan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17618">https://arxiv.org/abs/2505.17618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17618">https://arxiv.org/pdf/2505.17618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17618]] Scaling Image and Video Generation via Test-Time Evolutionary Search(https://arxiv.org/abs/2505.17618)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>As the marginal cost of scaling computation (data and parameters) during model pre-training continues to increase substantially, test-time scaling (TTS) has emerged as a promising direction for improving generative model performance by allocating additional computation at inference time. While TTS has demonstrated significant success across multiple language tasks, there remains a notable gap in understanding the test-time scaling behaviors of image and video generative models (diffusion-based or flow-based models). Although recent works have initiated exploration into inference-time strategies for vision tasks, these approaches face critical limitations: being constrained to task-specific domains, exhibiting poor scalability, or falling into reward over-optimization that sacrifices sample diversity. In this paper, we propose \textbf{Evo}lutionary \textbf{Search} (EvoSearch), a novel, generalist, and efficient TTS method that effectively enhances the scalability of both image and video generation across diffusion and flow models, without requiring additional training or model expansion. EvoSearch reformulates test-time scaling for diffusion and flow models as an evolutionary search problem, leveraging principles from biological evolution to efficiently explore and refine the denoising trajectory. By incorporating carefully designed selection and mutation mechanisms tailored to the stochastic differential equation denoising process, EvoSearch iteratively generates higher-quality offspring while preserving population diversity. Through extensive evaluation across both diffusion and flow architectures for image and video generation tasks, we demonstrate that our method consistently outperforms existing approaches, achieves higher diversity, and shows strong generalizability to unseen evaluation metrics. Our project is available at the website this https URL.</li>
</ul>

<h3>Title: CAS-IQA: Teaching Vision-Language Models for Synthetic Angiography Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Bo Wang, De-Xing Huang, Xiao-Hu Zhou, Mei-Jiang Gui, Nu-Fang Xiao, Jian-Long Hao, Ming-Yuan Liu, Zeng-Guang Hou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17619">https://arxiv.org/abs/2505.17619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17619">https://arxiv.org/pdf/2505.17619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17619]] CAS-IQA: Teaching Vision-Language Models for Synthetic Angiography Quality Assessment(https://arxiv.org/abs/2505.17619)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Synthetic X-ray angiographies generated by modern generative models hold great potential to reduce the use of contrast agents in vascular interventional procedures. However, low-quality synthetic angiographies can significantly increase procedural risk, underscoring the need for reliable image quality assessment (IQA) methods. Existing IQA models, however, fail to leverage auxiliary images as references during evaluation and lack fine-grained, task-specific metrics necessary for clinical relevance. To address these limitations, this paper proposes CAS-IQA, a vision-language model (VLM)-based framework that predicts fine-grained quality scores by effectively incorporating auxiliary information from related images. In the absence of angiography datasets, CAS-3K is constructed, comprising 3,565 synthetic angiographies along with score annotations. To ensure clinically meaningful assessment, three task-specific evaluation metrics are defined. Furthermore, a Multi-path featUre fuSion and rouTing (MUST) module is designed to enhance image representations by adaptively fusing and routing visual tokens to metric-specific branches. Extensive experiments on the CAS-3K dataset demonstrate that CAS-IQA significantly outperforms state-of-the-art IQA methods by a considerable margin.</li>
</ul>

<h3>Title: Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration</h3>
<ul>
<li><strong>Authors: </strong>Jingtong Gao, Ling Pan, Yejing Wang, Rui Zhong, Chi Lu, Qingpeng Cai, Peng Jiang, Xiangyu Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17621">https://arxiv.org/abs/2505.17621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17621">https://arxiv.org/pdf/2505.17621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17621]] Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration(https://arxiv.org/abs/2505.17621)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has emerged as a pivotal method for improving the reasoning capabilities of Large Language Models (LLMs). However, prevalent RL approaches such as Proximal Policy Optimization (PPO) and Group-Regularized Policy Optimization (GRPO) face critical limitations due to their reliance on sparse outcome-based rewards and inadequate mechanisms for incentivizing exploration. These limitations result in inefficient guidance for multi-step reasoning processes. Specifically, sparse reward signals fail to deliver effective or sufficient feedback, particularly for challenging problems. Furthermore, such reward structures induce systematic biases that prioritize exploitation of familiar trajectories over novel solution discovery. These shortcomings critically hinder performance in complex reasoning tasks, which inherently demand iterative refinement across ipntermediate steps. To address these challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd foR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense rewards and amplify explorations in the RL-based training paradigm. i-MENTOR introduces three key innovations: trajectory-aware exploration rewards that mitigate bias in token-level strategies while maintaining computational efficiency; dynamic reward scaling to stabilize exploration and exploitation in large action spaces; and advantage-preserving reward implementation that maintains advantage distribution integrity while incorporating exploratory guidance. Experiments across three public datasets demonstrate i-MENTOR's effectiveness with a 22.39% improvement on the difficult dataset Countdown-4.</li>
</ul>

<h3>Title: Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports</h3>
<ul>
<li><strong>Authors: </strong>Hayato Aida, Kosuke Takahashi, Takahiro Omi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17625">https://arxiv.org/abs/2505.17625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17625">https://arxiv.org/pdf/2505.17625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17625]] Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports(https://arxiv.org/abs/2505.17625)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>With recent advancements in Large Language Models (LLMs) and growing interest in retrieval-augmented generation (RAG), the ability to understand table structures has become increasingly important. This is especially critical in financial domains such as securities reports, where highly accurate question answering (QA) over tables is required. However, tables exist in various formats-including HTML, images, and plain text-making it difficult to preserve and extract structural information. Therefore, multimodal LLMs are essential for robust and general-purpose table understanding. Despite their promise, current Large Vision-Language Models (LVLMs), which are major representatives of multimodal LLMs, still face challenges in accurately understanding characters and their spatial relationships within documents. In this study, we propose a method to enhance LVLM-based table understanding by incorporating in-table textual content and layout features. Experimental results demonstrate that these auxiliary modalities significantly improve performance, enabling robust interpretation of complex document layouts without relying on explicitly structured input formats.</li>
</ul>

<h3>Title: GIM: Improved Interpretability for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Joakim Edin, Róbert Csordás, Tuukka Ruotsalo, Zhengxuan Wu, Maria Maistro, Jing Huang, Lars Maaløe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17630">https://arxiv.org/abs/2505.17630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17630">https://arxiv.org/pdf/2505.17630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17630]] GIM: Improved Interpretability for Large Language Models(https://arxiv.org/abs/2505.17630)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Ensuring faithful interpretability in large language models is imperative for trustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where networks compensate for reduced signal in one component by amplifying others, masking the true importance of the ablated component. While prior work attributes self-repair to layer normalization and back-up components that compensate for ablated components, we identify a novel form occurring within the attention mechanism, where softmax redistribution conceals the influence of important attention scores. This leads traditional ablation and gradient-based methods to underestimate the significance of all components contributing to these attention scores. We introduce Gradient Interaction Modifications (GIM), a technique that accounts for self-repair during backpropagation. Extensive experiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B, Qwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves faithfulness over existing circuit identification and feature attribution methods. Our work is a significant step toward better understanding the inner mechanisms of LLMs, which is crucial for improving them and ensuring their safety. Our code is available at this https URL.</li>
</ul>

<h3>Title: Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Bennion, Shaona Ghosh, Mantek Singh, Nouha Dziri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17636">https://arxiv.org/abs/2505.17636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17636">https://arxiv.org/pdf/2505.17636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17636]] Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis(https://arxiv.org/abs/2505.17636)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Various AI safety datasets have been developed to measure LLMs against evolving interpretations of harm. Our evaluation of five recently published open-source safety benchmarks reveals distinct semantic clusters using UMAP dimensionality reduction and kmeans clustering (silhouette score: 0.470). We identify six primary harm categories with varying benchmark representation. GretelAI, for example, focuses heavily on privacy concerns, while WildGuardMix emphasizes self-harm scenarios. Significant differences in prompt length distribution suggests confounds to data collection and interpretations of harm as well as offer possible context. Our analysis quantifies benchmark orthogonality among AI benchmarks, allowing for transparency in coverage gaps despite topical similarities. Our quantitative framework for analyzing semantic orthogonality across safety benchmarks enables more targeted development of datasets that comprehensively address the evolving landscape of harms in AI use, however that is defined in the future.</li>
</ul>

<h3>Title: Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training</h3>
<ul>
<li><strong>Authors: </strong>Tony Bonnaire, Raphaël Urfin, Giulio Biroli, Marc Mézard</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.dis-nn, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17638">https://arxiv.org/abs/2505.17638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17638">https://arxiv.org/pdf/2505.17638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17638]] Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training(https://arxiv.org/abs/2505.17638)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable success across a wide range of generative tasks. A key challenge is understanding the mechanisms that prevent their memorization of training data and allow generalization. In this work, we investigate the role of the training dynamics in the transition from generalization to memorization. Through extensive experiments and theoretical analysis, we identify two distinct timescales: an early time $\tau_\mathrm{gen}$ at which models begin to generate high-quality samples, and a later time $\tau_\mathrm{mem}$ beyond which memorization emerges. Crucially, we find that $\tau_\mathrm{mem}$ increases linearly with the training set size $n$, while $\tau_\mathrm{gen}$ remains constant. This creates a growing window of training times with $n$ where models generalize effectively, despite showing strong memorization if training continues beyond it. It is only when $n$ becomes larger than a model-dependent threshold that overfitting disappears at infinite training times. These findings reveal a form of implicit dynamical regularization in the training dynamics, which allow to avoid memorization even in highly overparameterized settings. Our results are supported by numerical experiments with standard U-Net architectures on realistic and synthetic datasets, and by a theoretical analysis using a tractable random features model studied in the high-dimensional limit.</li>
</ul>

<h3>Title: PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Zehua Pei, Ying Zhang, Hui-Ling Zhen, Xianzhi Yu, Wulong Liu, Sinno Jialin Pan, Mingxuan Yuan, Bei Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17639">https://arxiv.org/abs/2505.17639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17639">https://arxiv.org/pdf/2505.17639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17639]] PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval(https://arxiv.org/abs/2505.17639)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mixture-of-experts (MoE) architectures enable scaling large language models (LLMs) to vast parameter counts without a proportional rise in computational costs. However, the significant memory demands of large MoE models hinder their deployment across various computational environments, from cloud servers to consumer devices. This study first demonstrates pronounced task-specific specialization in expert activation patterns within MoE layers. Building on this, we introduce PreMoe, a novel framework that enables efficient deployment of massive MoE models in memory-constrained environments. PreMoe features two main components: probabilistic expert pruning (PEP) and task-adaptive expert retrieval (TAER). PEP employs a new metric, the task-conditioned expected selection score (TCESS), derived from router logits to quantify expert importance for specific tasks, thereby identifying a minimal set of critical experts. TAER leverages these task-specific expert importance profiles for efficient inference. It pre-computes and stores compact expert patterns for diverse tasks. When a user query is received, TAER rapidly identifies the most relevant stored task pattern and reconstructs the model by loading only the small subset of experts crucial for that task. This approach dramatically reduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B maintains 97.2\% accuracy on MATH500 when pruned to 8/128 configuration (50\% expert reduction), and still achieves 72.0\% with aggressive 8/32 pruning (87.5\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\% on MATH500 and 81.3\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64 (390GB memory) preserves 96.95\% accuracy on MATH500. We make our code publicly available at this https URL.</li>
</ul>

<h3>Title: A Network Science Approach to Granular Time Series Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ivana Kesić, Carolina Fortuna, Mihael Mohorčič, Blaž Bertalanič</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17640">https://arxiv.org/abs/2505.17640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17640">https://arxiv.org/pdf/2505.17640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17640]] A Network Science Approach to Granular Time Series Segmentation(https://arxiv.org/abs/2505.17640)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Time series segmentation (TSS) is one of the time series (TS) analysis techniques, that has received considerably less attention compared to other TS related tasks. In recent years, deep learning architectures have been introduced for TSS, however their reliance on sliding windows limits segmentation granularity due to fixed window sizes and strides. To overcome these challenges, we propose a new more granular TSS approach that utilizes the Weighted Dual Perspective Visbility Graph (WDPVG) TS into a graph and combines it with a Graph Attention Network (GAT). By transforming TS into graphs, we are able to capture different structural aspects of the data that would otherwise remain hidden. By utilizing the representation learning capabilities of Graph Neural Networks, our method is able to effectively identify meaningful segments within the TS. To better understand the potential of our approach, we also experimented with different TS-to-graph transformations and compared their performance. Our contributions include: a) formulating the TSS as a node classification problem on graphs; b) conducting an extensive analysis of various TS- to-graph transformations applied to TSS using benchmark datasets from the TSSB repository; c) providing the first detailed study on utilizing GNNs for analyzing graph representations of TS in the context of TSS; d) demonstrating the effectiveness of our method, which achieves an average F1 score of 0.97 across 59 diverse TSS benchmark datasets; e) outperforming the seq2point baseline method by 0.05 in terms of F1 score; and f) reducing the required training data compared to the baseline methods.</li>
</ul>

<h3>Title: HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Chuhao Zhou, Jianfei Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17645">https://arxiv.org/abs/2505.17645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17645">https://arxiv.org/pdf/2505.17645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17645]] HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning(https://arxiv.org/abs/2505.17645)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, large language model</a></li>
<li><strong>Abstract: </strong>Embodied agents operating in smart homes must understand human behavior through diverse sensory inputs and communicate via natural language. While Vision-Language Models (VLMs) have enabled impressive language-grounded perception, their reliance on visual data limits robustness in real-world scenarios with occlusions, poor lighting, or privacy constraints. In this paper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that integrates uncommon but powerful sensing modalities, such as LiDAR, infrared, mmWave radar, and WiFi, to enable seamless human perception and reasoning across heterogeneous environments. We address two key challenges: (1) the scarcity of aligned modality-text data for rare sensors, and (2) the heterogeneity of their physical signal representations. To overcome these, we design a Universal Modality-Injection Projector (UMIP) that enhances pre-aligned modality embeddings with fine-grained, text-aligned features from tailored encoders via coarse-to-fine cross-attention without introducing significant alignment overhead. We further introduce a human-VLM collaborative data curation pipeline to generate paired textual annotations for sensing datasets. Extensive experiments on two newly constructed benchmarks show that HoloLLM significantly outperforms existing MLLMs, improving language-grounded human sensing accuracy by up to 30%. This work establishes a new foundation for real-world, language-informed multisensory embodied intelligence.</li>
</ul>

<h3>Title: Understanding Pre-training and Fine-tuning from Loss Landscape Perspectives</h3>
<ul>
<li><strong>Authors: </strong>Huanran Chen, Yinpeng Dong, Zeming Wei, Yao Huang, Yichi Zhang, Hang Su, Jun Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17646">https://arxiv.org/abs/2505.17646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17646">https://arxiv.org/pdf/2505.17646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17646]] Understanding Pre-training and Fine-tuning from Loss Landscape Perspectives(https://arxiv.org/abs/2505.17646)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent studies have revealed that the loss landscape of large language models resembles a basin, within which the models perform nearly identically, and outside of which they lose all their capabilities. In this work, we conduct further studies on the loss landscape of large language models. We discover that pre-training creates a "basic capability" basin, and subsequent fine-tuning creates "specific capability" basins (e.g., math, safety, coding) within the basic capability basin. We further investigate two types of loss landscapes: the most-case landscape (i.e., the landscape along most directions) and the worst-case landscape (i.e., the landscape along the worst direction). We argue that as long as benign fine-tuning remains within the most-case basin, it will not compromise previous capabilities. Similarly, any fine-tuning (including the adversarial one) that stays within the worst-case basin would not compromise previous capabilities. Finally, we theoretically demonstrate that the size of the most-case basin can bound the size of the worst-case basin and the robustness with respect to input perturbations. We also show that, due to the over-parameterization property of current large language models, one can easily enlarge the basins by five times.</li>
</ul>

<h3>Title: Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective</h3>
<ul>
<li><strong>Authors: </strong>Deyang Kong, Qi Guo, Xiangyu Xi, Wei Wang, Jingang Wang, Xunliang Cai, Shikun Zhang, Wei Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17652">https://arxiv.org/abs/2505.17652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17652">https://arxiv.org/pdf/2505.17652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17652]] Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective(https://arxiv.org/abs/2505.17652)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning exhibits potential in enhancing the reasoning abilities of large language models, yet it is hard to scale for the low sample efficiency during the rollout phase. Existing methods attempt to improve efficiency by scheduling problems based on problem difficulties. However, these approaches suffer from unstable and biased estimations of problem difficulty and fail to capture the alignment between model competence and problem difficulty in RL training, leading to suboptimal results. To tackle these limitations, this paper introduces \textbf{C}ompetence-\textbf{D}ifficulty \textbf{A}lignment \textbf{S}ampling (\textbf{CDAS}), which enables accurate and stable estimation of problem difficulties by aggregating historical performance discrepancies of problems. Then the model competence is quantified to adaptively select problems whose difficulty is in alignment with the model's current competence using a fixed-point system. Experimental results across a range of challenging mathematical benchmarks show that CDAS achieves great improvements in both accuracy and efficiency. CDAS attains the highest average accuracy against baselines and exhibits significant speed advantages compared to Dynamic Sampling, a competitive strategy in DAPO, which is \textbf{2.33} times slower than CDAS.</li>
</ul>

<h3>Title: EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications</h3>
<ul>
<li><strong>Authors: </strong>Ancheng Xu, Zhihao Yang, Jingpeng Li, Guanghu Yuan, Longze Chen, Liang Yan, Jiehui Zhou, Zhen Qin, Hengyun Chang, Hamid Alinejad-Rokny, Bo Zheng, Min Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17654">https://arxiv.org/abs/2505.17654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17654">https://arxiv.org/pdf/2505.17654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17654]] EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications(https://arxiv.org/abs/2505.17654)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>E-commerce platforms increasingly rely on Large Language Models (LLMs) and Vision-Language Models (VLMs) to detect illicit or misleading product content. However, these models remain vulnerable to evasive content: inputs (text or images) that superficially comply with platform policies while covertly conveying prohibited claims. Unlike traditional adversarial attacks that induce overt failures, evasive content exploits ambiguity and context, making it far harder to detect. Existing robustness benchmarks provide little guidance for this demanding, real-world challenge. We introduce EVADE, the first expert-curated, Chinese, multimodal benchmark specifically designed to evaluate foundation models on evasive content detection in e-commerce. The dataset contains 2,833 annotated text samples and 13,961 images spanning six demanding product categories, including body shaping, height growth, and health supplements. Two complementary tasks assess distinct capabilities: Single-Violation, which probes fine-grained reasoning under short prompts, and All-in-One, which tests long-context reasoning by merging overlapping policy rules into unified instructions. Notably, the All-in-One setting significantly narrows the performance gap between partial and full-match accuracy, suggesting that clearer rule definitions improve alignment between human and model judgment. We benchmark 26 mainstream LLMs and VLMs and observe substantial performance gaps: even state-of-the-art models frequently misclassify evasive samples. By releasing EVADE and strong baselines, we provide the first rigorous standard for evaluating evasive-content detection, expose fundamental limitations in current multimodal reasoning, and lay the groundwork for safer and more transparent content moderation systems in e-commerce. The dataset is publicly available at this https URL.</li>
</ul>

<h3>Title: Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hexiang Tan, Fei Sun, Sha Liu, Du Su, Qi Cao, Xin Chen, Jingang Wang, Xunliang Cai, Yuanzhuo Wang, Huawei Shen, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17656">https://arxiv.org/abs/2505.17656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17656">https://arxiv.org/pdf/2505.17656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17656]] Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs(https://arxiv.org/abs/2505.17656)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) often generate plausible but incorrect content, error detection has become increasingly critical to ensure truthfulness. However, existing detection methods often overlook a critical problem we term as self-consistent error, where LLMs repeatly generate the same incorrect response across multiple stochastic samples. This work formally defines self-consistent errors and evaluates mainstream detection methods on them. Our investigation reveals two key findings: (1) Unlike inconsistent errors, whose frequency diminishes significantly as LLM scale increases, the frequency of self-consistent errors remains stable or even increases. (2) All four types of detection methshods significantly struggle to detect self-consistent errors. These findings reveal critical limitations in current detection methods and underscore the need for improved methods. Motivated by the observation that self-consistent errors often differ across LLMs, we propose a simple but effective cross-model probe method that fuses hidden state evidence from an external verifier LLM. Our method significantly enhances performance on self-consistent errors across three LLM families.</li>
</ul>

<h3>Title: DAM-GT: Dual Positional Encoding-Based Attention Masking Graph Transformer for Node Classification</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Li, Jinsong Chen, John E. Hopcroft, Kun He</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17660">https://arxiv.org/abs/2505.17660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17660">https://arxiv.org/pdf/2505.17660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17660]] DAM-GT: Dual Positional Encoding-Based Attention Masking Graph Transformer for Node Classification(https://arxiv.org/abs/2505.17660)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Neighborhood-aware tokenized graph Transformers have recently shown great potential for node classification tasks. Despite their effectiveness, our in-depth analysis of neighborhood tokens reveals two critical limitations in the existing paradigm. First, current neighborhood token generation methods fail to adequately capture attribute correlations within a neighborhood. Second, the conventional self-attention mechanism suffers from attention diversion when processing neighborhood tokens, where high-hop neighborhoods receive disproportionate focus, severely disrupting information interactions between the target node and its neighborhood tokens. To address these challenges, we propose DAM-GT, Dual positional encoding-based Attention Masking graph Transformer. DAM-GT introduces a novel dual positional encoding scheme that incorporates attribute-aware encoding via an attribute clustering strategy, effectively preserving node correlations in both topological and attribute spaces. In addition, DAM-GT formulates a new attention mechanism with a simple yet effective masking strategy to guide interactions between target nodes and their neighborhood tokens, overcoming the issue of attention diversion. Extensive experiments on various graphs with different homophily levels as well as different scales demonstrate that DAM-GT consistently outperforms state-of-the-art methods in node classification tasks.</li>
</ul>

<h3>Title: Automated scientific minimization of regret</h3>
<ul>
<li><strong>Authors: </strong>Marcel Binz, Akshay K. Jagadish, Milena Rmus, Eric Schulz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17661">https://arxiv.org/abs/2505.17661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17661">https://arxiv.org/pdf/2505.17661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17661]] Automated scientific minimization of regret(https://arxiv.org/abs/2505.17661)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We introduce automated scientific minimization of regret (ASMR) -- a framework for automated computational cognitive science. Building on the principles of scientific regret minimization, ASMR leverages Centaur -- a recently proposed foundation model of human cognition -- to identify gaps in an interpretable cognitive model. These gaps are then addressed through automated revisions generated by a language-based reasoning model. We demonstrate the utility of this approach in a multi-attribute decision-making task, showing that ASMR discovers cognitive models that predict human behavior at noise ceiling while retaining interpretability. Taken together, our results highlight the potential of ASMR to automate core components of the cognitive modeling pipeline.</li>
</ul>

<h3>Title: Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs</h3>
<ul>
<li><strong>Authors: </strong>Tianheng Ling, Chao Qian, Lukas Johannes Haßler, Gregor Schiele</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17662">https://arxiv.org/abs/2505.17662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17662">https://arxiv.org/pdf/2505.17662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17662]] Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs(https://arxiv.org/abs/2505.17662)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based models have shown strong performance across diverse time-series tasks, but their deployment on resource-constrained devices remains challenging due to high memory and computational demand. While prior work targeting Microcontroller Units (MCUs) has explored hardware-specific optimizations, such approaches are often task-specific and limited to 8-bit fixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater flexibility, enabling fine-grained control over data precision and architecture. However, existing FPGA-based deployments of Transformers for time-series analysis typically focus on high-density platforms with manual configuration. This paper presents a unified and fully automated deployment framework for Tiny Transformers on embedded FPGAs. Our framework supports a compact encoder-only Transformer architecture across three representative time-series tasks (forecasting, classification, and anomaly detection). It combines quantization-aware training (down to 4 bits), hardware-aware hyperparameter search using Optuna, and automatic VHDL generation for seamless deployment. We evaluate our framework on six public datasets across two embedded FPGA platforms. Results show that our framework produces integer-only, task-specific Transformer accelerators achieving as low as 0.033 mJ per inference with millisecond latency on AMD Spartan-7, while also providing insights into deployment feasibility on Lattice iCE40. All source code will be released in the GitHub repository (this https URL).</li>
</ul>

<h3>Title: Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States</h3>
<ul>
<li><strong>Authors: </strong>Yang Xiao, Jiashuo Wang, Qiancheng Xu, Changhe Song, Chunpu Xu, Yi Cheng, Wenjie Li, Pengfei Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17663">https://arxiv.org/abs/2505.17663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17663">https://arxiv.org/pdf/2505.17663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17663]] Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States(https://arxiv.org/abs/2505.17663)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) increasingly participate in human-AI interactions, evaluating their Theory of Mind (ToM) capabilities - particularly their ability to track dynamic mental states - becomes crucial. While existing benchmarks assess basic ToM abilities, they predominantly focus on static snapshots of mental states, overlooking the temporal evolution that characterizes real-world social interactions. We present \textsc{DynToM}, a novel benchmark specifically designed to evaluate LLMs' ability to understand and track the temporal progression of mental states across interconnected scenarios. Through a systematic four-step framework, we generate 1,100 social contexts encompassing 5,500 scenarios and 78,100 questions, each validated for realism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs reveals that their average performance underperforms humans by 44.7\%, with performance degrading significantly when tracking and reasoning about the shift of mental states. This performance gap highlights fundamental limitations in current LLMs' ability to model the dynamic nature of human mental states.</li>
</ul>

<h3>Title: What is the role of memorization in Continual Learning?</h3>
<ul>
<li><strong>Authors: </strong>Jędrzej Kozal, Jan Wasilewski, Alif Ashrafee, Bartosz Krawczyk, Michał Woźniak</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17664">https://arxiv.org/abs/2505.17664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17664">https://arxiv.org/pdf/2505.17664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17664]] What is the role of memorization in Continual Learning?(https://arxiv.org/abs/2505.17664)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Memorization impacts the performance of deep learning algorithms. Prior works have studied memorization primarily in the context of generalization and privacy. This work studies the memorization effect on incremental learning scenarios. Forgetting prevention and memorization seem similar. However, one should discuss their differences. We designed extensive experiments to evaluate the impact of memorization on continual learning. We clarified that learning examples with high memorization scores are forgotten faster than regular samples. Our findings also indicated that memorization is necessary to achieve the highest performance. However, at low memory regimes, forgetting regular samples is more important. We showed that the importance of a high-memorization score sample rises with an increase in the buffer size. We introduced a memorization proxy and employed it in the buffer policy problem to showcase how memorization could be used during incremental training. We demonstrated that including samples with a higher proxy memorization score is beneficial when the buffer size is large.</li>
</ul>

<h3>Title: EMRA-proxy: Enhancing Multi-Class Region Semantic Segmentation in Remote Sensing Images with Attention Proxy</h3>
<ul>
<li><strong>Authors: </strong>Yichun Yu, Yuqing Lan, Zhihuan Xing, Xiaoyi Yang, Tingyue Tang, Dan Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17665">https://arxiv.org/abs/2505.17665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17665">https://arxiv.org/pdf/2505.17665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17665]] EMRA-proxy: Enhancing Multi-Class Region Semantic Segmentation in Remote Sensing Images with Attention Proxy(https://arxiv.org/abs/2505.17665)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>High-resolution remote sensing (HRRS) image segmentation is challenging due to complex spatial layouts and diverse object appearances. While CNNs excel at capturing local features, they struggle with long-range dependencies, whereas Transformers can model global context but often neglect local details and are computationally this http URL propose a novel approach, Region-Aware Proxy Network (RAPNet), which consists of two components: Contextual Region Attention (CRA) and Global Class Refinement (GCR). Unlike traditional methods that rely on grid-based layouts, RAPNet operates at the region level for more flexible segmentation. The CRA module uses a Transformer to capture region-level contextual dependencies, generating a Semantic Region Mask (SRM). The GCR module learns a global class attention map to refine multi-class information, combining the SRM and attention map for accurate this http URL on three public datasets show that RAPNet outperforms state-of-the-art methods, achieving superior multi-class segmentation accuracy.</li>
</ul>

<h3>Title: Proto-FG3D: Prototype-based Interpretable Fine-Grained 3D Shape Classification</h3>
<ul>
<li><strong>Authors: </strong>Shuxian Ma, Zihao Dong, Runmin Cong, Sam Kwong, Xiuli Shao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17666">https://arxiv.org/abs/2505.17666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17666">https://arxiv.org/pdf/2505.17666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17666]] Proto-FG3D: Prototype-based Interpretable Fine-Grained 3D Shape Classification(https://arxiv.org/abs/2505.17666)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Deep learning-based multi-view coarse-grained 3D shape classification has achieved remarkable success over the past decade, leveraging the powerful feature learning capabilities of CNN-based and ViT-based backbones. However, as a challenging research area critical for detailed shape understanding, fine-grained 3D classification remains understudied due to the limited discriminative information captured during multi-view feature aggregation, particularly for subtle inter-class variations, class imbalance, and inherent interpretability limitations of parametric model. To address these problems, we propose the first prototype-based framework named Proto-FG3D for fine-grained 3D shape classification, achieving a paradigm shift from parametric softmax to non-parametric prototype learning. Firstly, Proto-FG3D establishes joint multi-view and multi-category representation learning via Prototype Association. Secondly, prototypes are refined via Online Clustering, improving both the robustness of multi-view feature allocation and inter-subclass balance. Finally, prototype-guided supervised learning is established to enhance fine-grained discrimination via prototype-view correlation analysis and enables ad-hoc interpretability through transparent case-based reasoning. Experiments on FG3D and ModelNet40 show Proto-FG3D surpasses state-of-the-art methods in accuracy, transparent predictions, and ad-hoc interpretability with visualizations, challenging conventional fine-grained 3D recognition approaches.</li>
</ul>

<h3>Title: QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Fanqi Wan, Weizhou Shen, Shengyi Liao, Yingcheng Shi, Chenliang Li, Ziyi Yang, Ji Zhang, Fei Huang, Jingren Zhou, Ming Yan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17667">https://arxiv.org/abs/2505.17667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17667">https://arxiv.org/pdf/2505.17667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17667]] QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning(https://arxiv.org/abs/2505.17667)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent large reasoning models (LRMs) have demonstrated strong reasoning capabilities through reinforcement learning (RL). These improvements have primarily been observed within the short-context reasoning tasks. In contrast, extending LRMs to effectively process and reason on long-context inputs via RL remains a critical unsolved challenge. To bridge this gap, we first formalize the paradigm of long-context reasoning RL, and identify key challenges in suboptimal training efficiency and unstable optimization process. To address these issues, we propose QwenLong-L1, a framework that adapts short-context LRMs to long-context scenarios via progressive context scaling. Specifically, we utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust initial policy, followed by a curriculum-guided phased RL technique to stabilize the policy evolution, and enhanced with a difficulty-aware retrospective sampling strategy to incentivize the policy exploration. Experiments on seven long-context document question-answering benchmarks demonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini and Qwen3-235B-A22B, achieving performance on par with Claude-3.7-Sonnet-Thinking, demonstrating leading performance among state-of-the-art LRMs. This work advances the development of practical long-context LRMs capable of robust reasoning across information-intensive environments.</li>
</ul>

<h3>Title: SVL: Spike-based Vision-language Pretraining for Efficient 3D Open-world Understanding</h3>
<ul>
<li><strong>Authors: </strong>Xuerui Qiu, Peixi Wu, Yaozhi Wen, Shaowei Gu, Yuqi Pan, Xinhao Luo, Bo XU, Guoqi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17674">https://arxiv.org/abs/2505.17674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17674">https://arxiv.org/pdf/2505.17674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17674]] SVL: Spike-based Vision-language Pretraining for Efficient 3D Open-world Understanding(https://arxiv.org/abs/2505.17674)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Spiking Neural Networks (SNNs) provide an energy-efficient way to extract 3D spatio-temporal features. However, existing SNNs still exhibit a significant performance gap compared to Artificial Neural Networks (ANNs) due to inadequate pre-training strategies. These limitations manifest as restricted generalization ability, task specificity, and a lack of multimodal understanding, particularly in challenging tasks such as multimodal question answering and zero-shot 3D classification. To overcome these challenges, we propose a Spike-based Vision-Language (SVL) pretraining framework that empowers SNNs with open-world 3D understanding while maintaining spike-driven efficiency. SVL introduces two key components: (i) Multi-scale Triple Alignment (MTA) for label-free triplet-based contrastive learning across 3D, image, and text modalities, and (ii) Re-parameterizable Vision-Language Integration (Rep-VLI) to enable lightweight inference without relying on large text encoders. Extensive experiments show that SVL achieves a top-1 accuracy of 85.4% in zero-shot 3D classification, surpassing advanced ANN models, and consistently outperforms prior SNNs on downstream tasks, including 3D classification (+6.1%), DVS action recognition (+2.1%), 3D detection (+1.1%), and 3D segmentation (+2.1%) with remarkable efficiency. Moreover, SVL enables SNNs to perform open-world 3D question answering, sometimes outperforming ANNs. To the best of our knowledge, SVL represents the first scalable, generalizable, and hardware-friendly paradigm for 3D open-world understanding, effectively bridging the gap between SNNs and ANNs in complex open-world understanding tasks. Code is available this https URL.</li>
</ul>

<h3>Title: Tuning Language Models for Robust Prediction of Diverse User Behaviors</h3>
<ul>
<li><strong>Authors: </strong>Fanjin Meng, Jingtao Ding, Jiahui Gong, Chen Yang, Hong Chen, Zuojian Wang, Haisheng Lu, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17682">https://arxiv.org/abs/2505.17682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17682">https://arxiv.org/pdf/2505.17682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17682]] Tuning Language Models for Robust Prediction of Diverse User Behaviors(https://arxiv.org/abs/2505.17682)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Predicting user behavior is essential for intelligent assistant services, yet deep learning models often struggle to capture long-tailed behaviors. Large language models (LLMs), with their pretraining on vast corpora containing rich behavioral knowledge, offer promise. However, existing fine-tuning approaches tend to overfit to frequent ``anchor'' behaviors, reducing their ability to predict less common ``tail'' behaviors. In this paper, we introduce BehaviorLM, a progressive fine-tuning approach that addresses this issue. In the first stage, LLMs are fine-tuned on anchor behaviors while preserving general behavioral knowledge. In the second stage, fine-tuning uses a balanced subset of all behaviors based on sample difficulty to improve tail behavior predictions without sacrificing anchor performance. Experimental results on two real-world datasets demonstrate that BehaviorLM robustly predicts both anchor and tail behaviors and effectively leverages LLM behavioral knowledge to master tail behavior prediction with few-shot examples.</li>
</ul>

<h3>Title: Semi-Supervised Medical Image Segmentation via Dual Networks</h3>
<ul>
<li><strong>Authors: </strong>Yunyao Lu, Yihang Wu, Reem Kateb, Ahmad Chaddad</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17690">https://arxiv.org/abs/2505.17690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17690">https://arxiv.org/pdf/2505.17690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17690]] Semi-Supervised Medical Image Segmentation via Dual Networks(https://arxiv.org/abs/2505.17690)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Traditional supervised medical image segmentation models require large amounts of labeled data for training; however, obtaining such large-scale labeled datasets in the real world is extremely challenging. Recent semi-supervised segmentation models also suffer from noisy pseudo-label issue and limited supervision in feature space. To solve these challenges, we propose an innovative semi-supervised 3D medical image segmentation method to reduce the dependency on large, expert-labeled datasets. Furthermore, we introduce a dual-network architecture to address the limitations of existing methods in using contextual information and generating reliable pseudo-labels. In addition, a self-supervised contrastive learning strategy is used to enhance the representation of the network and reduce prediction uncertainty by distinguishing between reliable and unreliable predictions. Experiments on clinical magnetic resonance imaging demonstrate that our approach outperforms state-of-the-art techniques. Our code is available at this https URL.</li>
</ul>

<h3>Title: ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive Preferences via Tournament Graph Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Yan Yu, Yilun Liu, Minggui He, Shimin Tao, Weibin Meng, Xinhua Yang, Li Zhang, Hongxia Ma, Chang Su, Hao Yang, Fuliang Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17691">https://arxiv.org/abs/2505.17691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17691">https://arxiv.org/pdf/2505.17691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17691]] ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive Preferences via Tournament Graph Reconstruction(https://arxiv.org/abs/2505.17691)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are widely used as evaluators for open-ended tasks, while previous research has emphasized biases in LLM evaluations, the issue of non-transitivity in pairwise comparisons remains unresolved: non-transitive preferences for pairwise comparisons, where evaluators prefer A over B, B over C, but C over A. Our results suggest that low-quality training data may reduce the transitivity of preferences generated by the Evaluator LLM. To address this, We propose a graph-theoretic framework to analyze and mitigate this problem by modeling pairwise preferences as tournament graphs. We quantify non-transitivity and introduce directed graph structural entropy to measure the overall clarity of preferences. Our analysis reveals significant non-transitivity in advanced Evaluator LLMs (with Qwen2.5-Max exhibiting 67.96%), as well as high entropy values (0.8095 for Qwen2.5-Max), reflecting low overall clarity of preferences. To address this issue, we designed a filtering strategy, ELSPR, to eliminate preference data that induces non-transitivity, retaining only consistent and transitive preference data for model fine-tuning. Experiments demonstrate that models fine-tuned with filtered data reduce non-transitivity by 13.78% (from 64.28% to 50.50%), decrease structural entropy by 0.0879 (from 0.8113 to 0.7234), and align more closely with human evaluators (human agreement rate improves by 0.6% and Spearman correlation increases by 0.01).</li>
</ul>

<h3>Title: ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for Zero-Shot Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Ziteng Yang, Jingzehua Xu, Yanshu Li, Zepeng Li, Yeqiang Wang, Xinghui Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17692">https://arxiv.org/abs/2505.17692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17692">https://arxiv.org/pdf/2505.17692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17692]] ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for Zero-Shot Anomaly Detection(https://arxiv.org/abs/2505.17692)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, segmentation</a></li>
<li><strong>Abstract: </strong>Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any target domain training samples, relying solely on external auxiliary data. Existing CLIP-based methods attempt to activate the model's ZSAD potential via handcrafted or static learnable prompts. The former incur high engineering costs and limited semantic coverage, whereas the latter apply identical descriptions across diverse anomaly types, thus fail to adapt to complex variations. Furthermore, since CLIP is originally pretrained on large-scale classification tasks, its anomaly segmentation quality is highly sensitive to the exact wording of class names, severely constraining prompting strategies that depend on class labels. To address these challenges, we introduce ViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception Prompting (ViP-Prompt) mechanism, which fuses global and multi-scale local visual context to adaptively generate fine-grained textual prompts, eliminating manual templates and class-name priors. This design enables our model to focus on precise abnormal regions, making it particularly valuable when category labels are ambiguous or privacy-constrained. Extensive experiments on 15 industrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves state-of-the-art performance and robust cross-domain generalization.</li>
</ul>

<h3>Title: SynRES: Towards Referring Expression Segmentation in the Wild via Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Dong-Hee Kim, Hyunjee Song, Donghyun Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17695">https://arxiv.org/abs/2505.17695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17695">https://arxiv.org/pdf/2505.17695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17695]] SynRES: Towards Referring Expression Segmentation in the Wild via Synthetic Data(https://arxiv.org/abs/2505.17695)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Despite the advances in Referring Expression Segmentation (RES) benchmarks, their evaluation protocols remain constrained, primarily focusing on either single targets with short queries (containing minimal attributes) or multiple targets from distinctly different queries on a single domain. This limitation significantly hinders the assessment of more complex reasoning capabilities in RES models. We introduce WildRES, a novel benchmark that incorporates long queries with diverse attributes and non-distinctive queries for multiple targets. This benchmark spans diverse application domains, including autonomous driving environments and robotic manipulation scenarios, thus enabling more rigorous evaluation of complex reasoning capabilities in real-world settings. Our analysis reveals that current RES models demonstrate substantial performance deterioration when evaluated on WildRES. To address this challenge, we introduce SynRES, an automated pipeline generating densely paired compositional synthetic training data through three innovations: (1) a dense caption-driven synthesis for attribute-rich image-mask-expression triplets, (2) reliable semantic alignment mechanisms rectifying caption-pseudo mask inconsistencies via Image-Text Aligned Grouping, and (3) domain-aware augmentations incorporating mosaic composition and superclass replacement to emphasize generalization ability and distinguishing attributes over object categories. Experimental results demonstrate that models trained with SynRES achieve state-of-the-art performance, improving gIoU by 2.0% on WildRES-ID and 3.8% on WildRES-DS. Code and datasets are available at this https URL.</li>
</ul>

<h3>Title: Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zekai Zhao, Qi Liu, Kun Zhou, Zihan Liu, Yifei Shao, Zhiting Hu, Biwei Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17697">https://arxiv.org/abs/2505.17697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17697">https://arxiv.org/pdf/2505.17697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17697]] Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models(https://arxiv.org/abs/2505.17697)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the remarkable reasoning performance, eliciting the long chain-of-thought (CoT) ability in large language models (LLMs) typically requires costly reinforcement learning or supervised fine-tuning on high-quality distilled data. We investigate the internal mechanisms behind this capability and show that a small set of high-impact activations in the last few layers largely governs long-form reasoning attributes, such as output length and self-reflection. By simply amplifying these activations and inserting "wait" tokens, we can invoke the long CoT ability without any training, resulting in significantly increased self-reflection rates and accuracy. Moreover, we find that the activation dynamics follow predictable trajectories, with a sharp rise after special tokens and a subsequent exponential decay. Building on these insights, we introduce a general training-free activation control technique. It leverages a few contrastive examples to identify key activations, and employs simple analytic functions to modulate their values at inference time to elicit long CoTs. Extensive experiments confirm the effectiveness of our method in efficiently eliciting long CoT reasoning in LLMs and improving their performance. Additionally, we propose a parameter-efficient fine-tuning method that trains only a last-layer activation amplification module and a few LoRA layers, outperforming full LoRA fine-tuning on reasoning benchmarks with significantly fewer parameters. Our code and data are publicly released.</li>
</ul>

<h3>Title: COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection</h3>
<ul>
<li><strong>Authors: </strong>Jaewon Cheon, Pilsung Kang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17701">https://arxiv.org/abs/2505.17701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17701">https://arxiv.org/pdf/2505.17701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17701]] COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection(https://arxiv.org/abs/2505.17701)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The growing size of large language models has created significant computational inefficiencies. To address this challenge, sparse activation methods selectively deactivates non-essential parameters during inference, reducing computational costs in FFNN layers. While existing methods focus on non-linear gating mechanisms, we hypothesize that the sparsity of the FFNN layer lies globally in the form of a linear combination over its internal down projection matrix. Based on this insight, we propose two methods: M-COUNTDOWN, leveraging indirect coefficients, and D-COUNTDOWN, utilizing direct coefficients of the linear combination. Experimental results demonstrate that D-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5% ideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4% better performance preservation compared to existing methods. Our specialized kernel implementations effectively realize these theoretical gains into substantial real-world acceleration.</li>
</ul>

<h3>Title: Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using Local Inference via DeepSeek</h3>
<ul>
<li><strong>Authors: </strong>Xueyang Li, Jiahao Li, Yu Song, Yunzhong Lou, Xiangdong Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17702">https://arxiv.org/abs/2505.17702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17702">https://arxiv.org/pdf/2505.17702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17702]] Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using Local Inference via DeepSeek(https://arxiv.org/abs/2505.17702)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The advent of Computer-Aided Design (CAD) generative modeling will significantly transform the design of industrial products. The recent research endeavor has extended into the realm of Large Language Models (LLMs). In contrast to fine-tuning methods, training-free approaches typically utilize the advanced closed-source LLMs, thereby offering enhanced flexibility and efficiency in the development of AI agents for generating CAD parametric models. However, the substantial cost and limitations of local deployment of the top-tier closed-source LLMs pose challenges in practical applications. The Seek-CAD is the pioneer exploration of locally deployed open-source inference LLM DeepSeek-R1 for CAD parametric model generation with a training-free methodology. This study is the first investigation to incorporate both visual and Chain-of-Thought (CoT) feedback within the self-refinement mechanism for generating CAD models. Specifically, the initial generated parametric CAD model is rendered into a sequence of step-wise perspective images, which are subsequently processed by a Vision Language Model (VLM) alongside the corresponding CoTs derived from DeepSeek-R1 to assess the CAD model generation. Then, the feedback is utilized by DeepSeek-R1 to refine the initial generated model for the next round of generation. Moreover, we present an innovative 3D CAD model dataset structured around the SSR (Sketch, Sketch-based feature, and Refinements) triple design paradigm. This dataset encompasses a wide range of CAD commands, thereby aligning effectively with industrial application requirements and proving suitable for the generation of LLMs. Extensive experiments validate the effectiveness of Seek-CAD under various metrics.</li>
</ul>

<h3>Title: Understanding How Value Neurons Shape the Generation of Specified Values in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yi Su, Jiayi Zhang, Shu Yang, Xinhai Wang, Lijie Hu, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17712">https://arxiv.org/abs/2505.17712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17712">https://arxiv.org/pdf/2505.17712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17712]] Understanding How Value Neurons Shape the Generation of Specified Values in LLMs(https://arxiv.org/abs/2505.17712)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Rapid integration of large language models (LLMs) into societal applications has intensified concerns about their alignment with universal ethical principles, as their internal value representations remain opaque despite behavioral alignment advancements. Current approaches struggle to systematically interpret how values are encoded in neural architectures, limited by datasets that prioritize superficial judgments over mechanistic analysis. We introduce ValueLocate, a mechanistic interpretability framework grounded in the Schwartz Values Survey, to address this gap. Our method first constructs ValueInsight, a dataset that operationalizes four dimensions of universal value through behavioral contexts in the real world. Leveraging this dataset, we develop a neuron identification method that calculates activation differences between opposing value aspects, enabling precise localization of value-critical neurons without relying on computationally intensive attribution methods. Our proposed validation method demonstrates that targeted manipulation of these neurons effectively alters model value orientations, establishing causal relationships between neurons and value representations. This work advances the foundation for value alignment by bridging psychological value frameworks with neuron analysis in LLMs.</li>
</ul>

<h3>Title: Get Experience from Practice: LLM Agents with Record & Replay</h3>
<ul>
<li><strong>Authors: </strong>Erhu Feng, Wenbo Zhou, Zibin Liu, Le Chen, Yunpeng Dong, Cheng Zhang, Yisheng Zhao, Dong Du, Zhichao Hua, Yubin Xia, Haibo Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17716">https://arxiv.org/abs/2505.17716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17716">https://arxiv.org/pdf/2505.17716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17716]] Get Experience from Practice: LLM Agents with Record & Replay(https://arxiv.org/abs/2505.17716)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>AI agents, empowered by Large Language Models (LLMs) and communication protocols such as MCP and A2A, have rapidly evolved from simple chatbots to autonomous entities capable of executing complex, multi-step tasks, demonstrating great potential. However, the LLMs' inherent uncertainty and heavy computational resource requirements pose four significant challenges to the development of safe and efficient agents: reliability, privacy, cost and performance. Existing approaches, like model alignment, workflow constraints and on-device model deployment, can partially alleviate some issues but often with limitations, failing to fundamentally resolve these challenges. This paper proposes a new paradigm called AgentRR (Agent Record & Replay), which introduces the classical record-and-replay mechanism into AI agent frameworks. The core idea is to: 1. Record an agent's interaction trace with its environment and internal decision process during task execution, 2. Summarize this trace into a structured "experience" encapsulating the workflow and constraints, and 3. Replay these experiences in subsequent similar tasks to guide the agent's behavior. We detail a multi-level experience abstraction method and a check function mechanism in AgentRR: the former balances experience specificity and generality, while the latter serves as a trust anchor to ensure completeness and safety during replay. In addition, we explore multiple application modes of AgentRR, including user-recorded task demonstration, large-small model collaboration and privacy-aware agent execution, and envision an experience repository for sharing and reusing knowledge to further reduce deployment cost.</li>
</ul>

<h3>Title: PEAR: Equal Area Weather Forecasting on the Sphere</h3>
<ul>
<li><strong>Authors: </strong>Hampus Linander, Christoffer Petersson, Daniel Persson, Jan E. Gerken</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17720">https://arxiv.org/abs/2505.17720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17720">https://arxiv.org/pdf/2505.17720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17720]] PEAR: Equal Area Weather Forecasting on the Sphere(https://arxiv.org/abs/2505.17720)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Machine learning methods for global medium-range weather forecasting have recently received immense attention. Following the publication of the Pangu Weather model, the first deep learning model to outperform traditional numerical simulations of the atmosphere, numerous models have been published in this domain, building on Pangu's success. However, all of these models operate on input data and produce predictions on the Driscoll--Healy discretization of the sphere which suffers from a much finer grid at the poles than around the equator. In contrast, in the Hierarchical Equal Area iso-Latitude Pixelization (HEALPix) of the sphere, each pixel covers the same surface area, removing unphysical biases. Motivated by a growing support for this grid in meteorology and climate sciences, we propose to perform weather forecasting with deep learning models which natively operate on the HEALPix grid. To this end, we introduce Pangu Equal ARea (PEAR), a transformer-based weather forecasting model which operates directly on HEALPix-features and outperforms the corresponding model on Driscoll--Healy without any computational overhead.</li>
</ul>

<h3>Title: SeaLion: Semantic Part-Aware Latent Point Diffusion Models for 3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Dekai Zhu, Yan Di, Stefan Gavranovic, Slobodan Ilic</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17721">https://arxiv.org/abs/2505.17721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17721">https://arxiv.org/pdf/2505.17721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17721]] SeaLion: Semantic Part-Aware Latent Point Diffusion Models for 3D Generation(https://arxiv.org/abs/2505.17721)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Denoising diffusion probabilistic models have achieved significant success in point cloud generation, enabling numerous downstream applications, such as generative data augmentation and 3D model editing. However, little attention has been given to generating point clouds with point-wise segmentation labels, as well as to developing evaluation metrics for this task. Therefore, in this paper, we present SeaLion, a novel diffusion model designed to generate high-quality and diverse point clouds with fine-grained segmentation labels. Specifically, we introduce the semantic part-aware latent point diffusion technique, which leverages the intermediate features of the generative models to jointly predict the noise for perturbed latent points and associated part segmentation labels during the denoising process, and subsequently decodes the latent points to point clouds conditioned on part segmentation labels. To effectively evaluate the quality of generated point clouds, we introduce a novel point cloud pairwise distance calculation method named part-aware Chamfer distance (p-CD). This method enables existing metrics, such as 1-NNA, to measure both the local structural quality and inter-part coherence of generated point clouds. Experiments on the large-scale synthetic dataset ShapeNet and real-world medical dataset IntrA demonstrate that SeaLion achieves remarkable performance in generation quality and diversity, outperforming the existing state-of-the-art model, DiffFacto, by 13.33% and 6.52% on 1-NNA (p-CD) across the two datasets. Experimental analysis shows that SeaLion can be trained semi-supervised, thereby reducing the demand for labeling efforts. Lastly, we validate the applicability of SeaLion in generative data augmentation for training segmentation models and the capability of SeaLion to serve as a tool for part-aware 3D shape editing.</li>
</ul>

<h3>Title: Slot-MLLM: Object-Centric Visual Tokenization for Multimodal LLM</h3>
<ul>
<li><strong>Authors: </strong>Donghwan Chi, Hyomin Kim, Yoonjin Oh, Yongjin Kim, Donghoon Lee, Daejin Jo, Jongmin Kim, Junyeob Baek, Sungjin Ahn, Sungwoong Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17726">https://arxiv.org/abs/2505.17726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17726">https://arxiv.org/pdf/2505.17726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17726]] Slot-MLLM: Object-Centric Visual Tokenization for Multimodal LLM(https://arxiv.org/abs/2505.17726)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Recently, multimodal large language models (MLLMs) have emerged as a key approach in achieving artificial general intelligence. In particular, vision-language MLLMs have been developed to generate not only text but also visual outputs from multimodal inputs. This advancement requires efficient image tokens that LLMs can process effectively both in input and output. However, existing image tokenization methods for MLLMs typically capture only global abstract concepts or uniformly segmented image patches, restricting MLLMs' capability to effectively understand or generate detailed visual content, particularly at the object level. To address this limitation, we propose an object-centric visual tokenizer based on Slot Attention specifically for MLLMs. In particular, based on the Q-Former encoder, diffusion decoder, and residual vector quantization, our proposed discretized slot tokens can encode local visual details while maintaining high-level semantics, and also align with textual data to be integrated seamlessly within a unified next-token prediction framework of LLMs. The resulting Slot-MLLM demonstrates significant performance improvements over baselines with previous visual tokenizers across various vision-language tasks that entail local detailed comprehension and generation. Notably, this work is the first demonstration of the feasibility of object-centric slot attention performed with MLLMs and in-the-wild natural images.</li>
</ul>

<h3>Title: SafeMVDrive: Multi-view Safety-Critical Driving Video Synthesis in the Real World Domain</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Zhou, Linye Lyu, Zhuotao Tian, Cheng Zhuo, Yu Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17727">https://arxiv.org/abs/2505.17727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17727">https://arxiv.org/pdf/2505.17727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17727]] SafeMVDrive: Multi-view Safety-Critical Driving Video Synthesis in the Real World Domain(https://arxiv.org/abs/2505.17727)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Safety-critical scenarios are rare yet pivotal for evaluating and enhancing the robustness of autonomous driving systems. While existing methods generate safety-critical driving trajectories, simulations, or single-view videos, they fall short of meeting the demands of advanced end-to-end autonomous systems (E2E AD), which require real-world, multi-view video data. To bridge this gap, we introduce SafeMVDrive, the first framework designed to generate high-quality, safety-critical, multi-view driving videos grounded in real-world domains. SafeMVDrive strategically integrates a safety-critical trajectory generator with an advanced multi-view video generator. To tackle the challenges inherent in this integration, we first enhance scene understanding ability of the trajectory generator by incorporating visual context -- which is previously unavailable to such generator -- and leveraging a GRPO-finetuned vision-language model to achieve more realistic and context-aware trajectory generation. Second, recognizing that existing multi-view video generators struggle to render realistic collision events, we introduce a two-stage, controllable trajectory generation mechanism that produces collision-evasion trajectories, ensuring both video quality and safety-critical fidelity. Finally, we employ a diffusion-based multi-view video generator to synthesize high-quality safety-critical driving videos from the generated trajectories. Experiments conducted on an E2E AD planner demonstrate a significant increase in collision rate when tested with our generated data, validating the effectiveness of SafeMVDrive in stress-testing planning modules. Our code, examples, and datasets are publicly available at: this https URL.</li>
</ul>

<h3>Title: RQR3D: Reparametrizing the regression targets for BEV-based 3D object detection</h3>
<ul>
<li><strong>Authors: </strong>Ozsel Kilinc, Cem Tarhan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17732">https://arxiv.org/abs/2505.17732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17732">https://arxiv.org/pdf/2505.17732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17732]] RQR3D: Reparametrizing the regression targets for BEV-based 3D object detection(https://arxiv.org/abs/2505.17732)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate, fast, and reliable 3D perception is essential for autonomous driving. Recently, bird's-eye view (BEV)-based perception approaches have emerged as superior alternatives to perspective-based solutions, offering enhanced spatial understanding and more natural outputs for planning. Existing BEV-based 3D object detection methods, typically adhering to angle-based representation, directly estimate the size and orientation of rotated bounding boxes. We observe that BEV-based 3D object detection is analogous to aerial oriented object detection, where angle-based methods are recognized for being affected by discontinuities in their loss functions. Drawing inspiration from this domain, we propose Restricted Quadrilateral Representation to define 3D regression targets. RQR3D regresses the smallest horizontal bounding box encapsulating the oriented box, along with the offsets between the corners of these two boxes, thereby transforming the oriented object detection problem into a keypoint regression task. RQR3D is compatible with any 3D object detection approach. We employ RQR3D within an anchor-free single-stage object detection method and introduce an objectness head to address class imbalance problem. Furthermore, we introduce a simplified radar fusion backbone that eliminates the need for voxel grouping and processes the BEV-mapped point cloud with standard 2D convolutions, rather than sparse convolutions. Extensive evaluations on the nuScenes dataset demonstrate that RQR3D achieves state-of-the-art performance in camera-radar 3D object detection, outperforming the previous best method by +4% in NDS and +2.4% in mAP, and significantly reducing the translation and orientation errors, which are crucial for safe autonomous driving. These consistent gains highlight the robustness, precision, and real-world readiness of our approach.</li>
</ul>

<h3>Title: Discrete Neural Flow Samplers with Locally Equivariant Transformer</h3>
<ul>
<li><strong>Authors: </strong>Zijing Ou, Ruixiang Zhang, Yingzhen Li</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17741">https://arxiv.org/abs/2505.17741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17741">https://arxiv.org/pdf/2505.17741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17741]] Discrete Neural Flow Samplers with Locally Equivariant Transformer(https://arxiv.org/abs/2505.17741)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Sampling from unnormalised discrete distributions is a fundamental problem across various domains. While Markov chain Monte Carlo offers a principled approach, it often suffers from slow mixing and poor convergence. In this paper, we propose Discrete Neural Flow Samplers (DNFS), a trainable and efficient framework for discrete sampling. DNFS learns the rate matrix of a continuous-time Markov chain such that the resulting dynamics satisfy the Kolmogorov equation. As this objective involves the intractable partition function, we then employ control variates to reduce the variance of its Monte Carlo estimation, leading to a coordinate descent learning algorithm. To further facilitate computational efficiency, we propose locally equivaraint Transformer, a novel parameterisation of the rate matrix that significantly improves training efficiency while preserving powerful network expressiveness. Empirically, we demonstrate the efficacy of DNFS in a wide range of applications, including sampling from unnormalised distributions, training discrete energy-based models, and solving combinatorial optimisation problems.</li>
</ul>

<h3>Title: Fast Quiet-STaR: Thinking Without Thought Tokens</h3>
<ul>
<li><strong>Authors: </strong>Wei Huang, Yizhe Xiong, Xin Ye, Zhijie Deng, Hui Chen, Zijia Lin, Guiguang Ding</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17746">https://arxiv.org/abs/2505.17746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17746">https://arxiv.org/pdf/2505.17746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17746]] Fast Quiet-STaR: Thinking Without Thought Tokens(https://arxiv.org/abs/2505.17746)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved impressive performance across a range of natural language processing tasks. However, recent advances demonstrate that further gains particularly in complex reasoning tasks require more than merely scaling up model sizes or training data. One promising direction is to enable models to think during the reasoning process. Recently, Quiet STaR significantly improves reasoning by generating token-level thought traces, but incurs substantial inference overhead. In this work, we propose Fast Quiet STaR, a more efficient reasoning framework that preserves the benefits of token-level reasoning while reducing computational cost. Our method introduces a curriculum learning based training strategy that gradually reduces the number of thought tokens, enabling the model to internalize more abstract and concise reasoning processes. We further extend this approach to the standard Next Token Prediction (NTP) setting through reinforcement learning-based fine-tuning, resulting in Fast Quiet-STaR NTP, which eliminates the need for explicit thought token generation during inference. Experiments on four benchmark datasets with Mistral 7B and Qwen2.5 7B demonstrate that Fast Quiet-STaR consistently outperforms Quiet-STaR in terms of average accuracy under the same inference time budget. Notably, Fast Quiet-STaR NTP achieves an average accuracy improvement of 9\% on Mistral 7B and 5.7\% on Qwen2.5 7B, while maintaining the same inference latency. Our code will be available at this https URL.</li>
</ul>

<h3>Title: But what is your honest answer? Aiding LLM-judges with honest alternatives using steering vectors</h3>
<ul>
<li><strong>Authors: </strong>Leon Eshuijs, Archie Chaudhury, Alan McBeth, Ethan Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17760">https://arxiv.org/abs/2505.17760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17760">https://arxiv.org/pdf/2505.17760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17760]] But what is your honest answer? Aiding LLM-judges with honest alternatives using steering vectors(https://arxiv.org/abs/2505.17760)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent safety evaluations of Large Language Models (LLMs) show that many models exhibit dishonest behavior, such as sycophancy. However, most honesty benchmarks focus exclusively on factual knowledge or explicitly harmful behavior and rely on external judges, which are often unable to detect less obvious forms of dishonesty. In this work, we introduce a new framework, Judge Using Safety-Steered Alternatives (JUSSA), which utilizes steering vectors trained on a single sample to elicit more honest responses from models, helping LLM-judges in the detection of dishonest behavior. To test our framework, we introduce a new manipulation dataset with prompts specifically designed to elicit deceptive responses. We find that JUSSA enables LLM judges to better differentiate between dishonest and benign responses, and helps them identify subtle instances of manipulative behavior.</li>
</ul>

<h3>Title: Resolving Conflicting Evidence in Automated Fact-Checking: A Study on Retrieval-Augmented LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Ge, Yuhao Wu, Daniel Wai Kit Chin, Roy Ka-Wei Lee, Rui Cao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17762">https://arxiv.org/abs/2505.17762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17762">https://arxiv.org/pdf/2505.17762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17762]] Resolving Conflicting Evidence in Automated Fact-Checking: A Study on Retrieval-Augmented LLMs(https://arxiv.org/abs/2505.17762)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) augmented with retrieval mechanisms have demonstrated significant potential in fact-checking tasks by integrating external knowledge. However, their reliability decreases when confronted with conflicting evidence from sources of varying credibility. This paper presents the first systematic evaluation of Retrieval-Augmented Generation (RAG) models for fact-checking in the presence of conflicting evidence. To support this study, we introduce \textbf{CONFACT} (\textbf{Con}flicting Evidence for \textbf{Fact}-Checking) (Dataset available at this https URL), a novel dataset comprising questions paired with conflicting information from various sources. Extensive experiments reveal critical vulnerabilities in state-of-the-art RAG methods, particularly in resolving conflicts stemming from differences in media source credibility. To address these challenges, we investigate strategies to integrate media background information into both the retrieval and generation stages. Our results show that effectively incorporating source credibility significantly enhances the ability of RAG models to resolve conflicting evidence and improve fact-checking performance.</li>
</ul>

<h3>Title: Unsupervised Clustering for Fault Analysis in High-Voltage Power Systems Using Voltage and Current Signals</h3>
<ul>
<li><strong>Authors: </strong>Julian Oelhaf, Georg Kordowich, Andreas Maier, Johann Jager, Siming Bayer</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17763">https://arxiv.org/abs/2505.17763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17763">https://arxiv.org/pdf/2505.17763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17763]] Unsupervised Clustering for Fault Analysis in High-Voltage Power Systems Using Voltage and Current Signals(https://arxiv.org/abs/2505.17763)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The widespread use of sensors in modern power grids has led to the accumulation of large amounts of voltage and current waveform data, especially during fault events. However, the lack of labeled datasets poses a significant challenge for fault classification and analysis. This paper explores the application of unsupervised clustering techniques for fault diagnosis in high-voltage power systems. A dataset provided by the Reseau de Transport d'Electricite (RTE) is analyzed, with frequency domain features extracted using the Fast Fourier Transform (FFT). The K-Means algorithm is then applied to identify underlying patterns in the data, enabling automated fault categorization without the need for labeled training samples. The resulting clusters are evaluated in collaboration with power system experts to assess their alignment with real-world fault characteristics. The results demonstrate the potential of unsupervised learning for scalable and data-driven fault analysis, providing a robust approach to detecting and classifying power system faults with minimal prior assumptions.</li>
</ul>

<h3>Title: The Real Barrier to LLM Agent Usability is Agentic ROI</h3>
<ul>
<li><strong>Authors: </strong>Weiwen Liu, Jiarui Qin, Xu Huang, Xingshan Zeng, Yunjia Xi, Jianghao Lin, Chuhan Wu, Yasheng Wang, Lifeng Shang, Ruiming Tang, Defu Lian, Yong Yu, Weinan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17767">https://arxiv.org/abs/2505.17767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17767">https://arxiv.org/pdf/2505.17767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17767]] The Real Barrier to LLM Agent Usability is Agentic ROI(https://arxiv.org/abs/2505.17767)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) agents represent a promising shift in human-AI interaction, moving beyond passive prompt-response systems to autonomous agents capable of reasoning, planning, and goal-directed action. Despite the widespread application in specialized, high-effort tasks like coding and scientific research, we highlight a critical usability gap in high-demand, mass-market applications. This position paper argues that the limited real-world adoption of LLM agents stems not only from gaps in model capabilities, but also from a fundamental tradeoff between the value an agent can provide and the costs incurred during real-world use. Hence, we call for a shift from solely optimizing model performance to a broader, utility-driven perspective: evaluating agents through the lens of the overall agentic return on investment (Agent ROI). By identifying key factors that determine Agentic ROI--information quality, agent time, and cost--we posit a zigzag development trajectory in optimizing agentic ROI: first scaling up to improve the information quality, then scaling down to minimize the time and cost. We outline the roadmap across different development stages to bridge the current usability gaps, aiming to make LLM agents truly scalable, accessible, and effective in real-world contexts.</li>
</ul>

<h3>Title: R-Genie: Reasoning-Guided Generative Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Dong Zhang, Lingfeng He, Rui Yan, Fei Shen, Jinhui Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17768">https://arxiv.org/abs/2505.17768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17768">https://arxiv.org/pdf/2505.17768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17768]] R-Genie: Reasoning-Guided Generative Image Editing(https://arxiv.org/abs/2505.17768)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>While recent advances in image editing have enabled impressive visual synthesis capabilities, current methods remain constrained by explicit textual instructions and limited editing operations, lacking deep comprehension of implicit user intentions and contextual reasoning. In this work, we introduce a new image editing paradigm: reasoning-guided generative editing, which synthesizes images based on complex, multi-faceted textual queries accepting world knowledge and intention inference. To facilitate this task, we first construct a comprehensive dataset featuring over 1,000 image-instruction-edit triples that incorporate rich reasoning contexts and real-world knowledge. We then propose R-Genie: a reasoning-guided generative image editor, which synergizes the generation power of diffusion models with advanced reasoning capabilities of multimodal large language models. R-Genie incorporates a reasoning-attention mechanism to bridge linguistic understanding with visual synthesis, enabling it to handle intricate editing requests involving abstract user intentions and contextual reasoning relations. Extensive experimental results validate that R-Genie can equip diffusion models with advanced reasoning-based editing capabilities, unlocking new potentials for intelligent image synthesis.</li>
</ul>

<h3>Title: Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Patrick Leask, Neel Nanda, Noura Al Moubayed</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17769">https://arxiv.org/abs/2505.17769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17769">https://arxiv.org/pdf/2505.17769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17769]] Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models(https://arxiv.org/abs/2505.17769)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Sparse autoencoders (SAEs) are a popular method for decomposing Large Langage Models (LLM) activations into interpretable latents. However, due to their substantial training cost, most academic research uses open-source SAEs which are only available for a restricted set of models of up to 27B parameters. SAE latents are also learned from a dataset of activations, which means they do not transfer between models. Motivated by relative representation similarity measures, we introduce Inference-Time Decomposition of Activations (ITDA) models, an alternative method for decomposing language model activations. To train an ITDA, we greedily construct a dictionary of language model activations on a dataset of prompts, selecting those activations which were worst approximated by matching pursuit on the existing dictionary. ITDAs can be trained in just 1\% of the time required for SAEs, using 1\% of the data. This allowed us to train ITDAs on Llama-3.1 70B and 405B on a single consumer GPU. ITDAs can achieve similar reconstruction performance to SAEs on some target LLMs, but generally incur a performance penalty. However, ITDA dictionaries enable cross-model comparisons, and a simple Jaccard similarity index on ITDA dictionaries outperforms existing methods like CKA, SVCCA, and relative representation similarity metrics. ITDAs provide a cheap alternative to SAEs where computational resources are limited, or when cross model comparisons are necessary. Code available at this https URL.</li>
</ul>

<h3>Title: TopoPoint: Enhance Topology Reasoning via Endpoint Detection in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Yanping Fu, Xinyuan Liu, Tianyu Li, Yike Ma, Yucheng Zhang, Feng Dai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17771">https://arxiv.org/abs/2505.17771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17771">https://arxiv.org/pdf/2505.17771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17771]] TopoPoint: Enhance Topology Reasoning via Endpoint Detection in Autonomous Driving(https://arxiv.org/abs/2505.17771)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Topology reasoning, which unifies perception and structured reasoning, plays a vital role in understanding intersections for autonomous driving. However, its performance heavily relies on the accuracy of lane detection, particularly at connected lane endpoints. Existing methods often suffer from lane endpoints deviation, leading to incorrect topology construction. To address this issue, we propose TopoPoint, a novel framework that explicitly detects lane endpoints and jointly reasons over endpoints and lanes for robust topology reasoning. During training, we independently initialize point and lane query, and proposed Point-Lane Merge Self-Attention to enhance global context sharing through incorporating geometric distances between points and lanes as an attention mask . We further design Point-Lane Graph Convolutional Network to enable mutual feature aggregation between point and lane query. During inference, we introduce Point-Lane Geometry Matching algorithm that computes distances between detected points and lanes to refine lane endpoints, effectively mitigating endpoint deviation. Extensive experiments on the OpenLane-V2 benchmark demonstrate that TopoPoint achieves state-of-the-art performance in topology reasoning (48.8 on OLS). Additionally, we propose DET$_p$ to evaluate endpoint detection, under which our method significantly outperforms existing approaches (52.6 v.s. 45.2 on DET$_p$). The code is released at this https URL.</li>
</ul>

<h3>Title: C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Amir Hossein Rahmati, Sanket Jantre, Weifeng Zhang, Yucheng Wang, Byung-Jun Yoon, Nathan M. Urban, Xiaoning Qian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17773">https://arxiv.org/abs/2505.17773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17773">https://arxiv.org/pdf/2505.17773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17773]] C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in Large Language Models(https://arxiv.org/abs/2505.17773)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning large language models (LLMs), but it often produces overconfident predictions in data-scarce few-shot settings. To address this issue, several classical statistical learning approaches have been repurposed for scalable uncertainty-aware LoRA fine-tuning. However, these approaches neglect how input characteristics affect the predictive uncertainty estimates. To address this limitation, we propose Contextual Low-Rank Adaptation (\textbf{C-LoRA}) as a novel uncertainty-aware and parameter efficient fine-tuning approach, by developing new lightweight LoRA modules contextualized to each input data sample to dynamically adapt uncertainty estimates. Incorporating data-driven contexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves well-calibrated uncertainties, and yields robust predictions. Extensive experiments demonstrate that C-LoRA consistently outperforms the state-of-the-art uncertainty-aware LoRA methods in both uncertainty quantification and model generalization. Ablation studies further confirm the critical role of our contextual modules in capturing sample-specific uncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM fine-tuning in few-shot regimes.</li>
</ul>

<h3>Title: Sec5GLoc: Securing 5G Indoor Localization via Adversary-Resilient Deep Learning Architecture</h3>
<ul>
<li><strong>Authors: </strong>Ildi Alla, Valeria Loscri</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17776">https://arxiv.org/abs/2505.17776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17776">https://arxiv.org/pdf/2505.17776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17776]] Sec5GLoc: Securing 5G Indoor Localization via Adversary-Resilient Deep Learning Architecture(https://arxiv.org/abs/2505.17776)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack, robust</a></li>
<li><strong>Abstract: </strong>Emerging 5G millimeter-wave and sub-6 GHz networks enable high-accuracy indoor localization, but security and privacy vulnerabilities pose serious challenges. In this paper, we identify and address threats including location spoofing and adversarial signal manipulation against 5G-based indoor localization. We formalize a threat model encompassing attackers who inject forged radio signals or perturb channel measurements to mislead the localization system. To defend against these threats, we propose an adversary-resilient localization architecture that combines deep learning fingerprinting with physical domain knowledge. Our approach integrates multi-anchor Channel Impulse Response (CIR) fingerprints with Time Difference of Arrival (TDoA) features and known anchor positions in a hybrid Convolutional Neural Network (CNN) and multi-head attention network. This design inherently checks geometric consistency and dynamically down-weights anomalous signals, making localization robust to tampering. We formulate the secure localization problem and demonstrate, through extensive experiments on a public 5G indoor dataset, that the proposed system achieves a mean error approximately 0.58 m under mixed Line-of-Sight (LOS) and Non-Line-of-Sight (NLOS) trajectories in benign conditions and gracefully degrades to around 0.81 m under attack scenarios. We also show via ablation studies that each architecture component (attention mechanism, TDoA, etc.) is critical for both accuracy and resilience, reducing errors by 4-5 times compared to baselines. In addition, our system runs in real-time, localizing the user in just 1 ms on a simple CPU. The code has been released to ensure reproducibility (this https URL).</li>
</ul>

<h3>Title: TextFlux: An OCR-Free DiT Model for High-Fidelity Multilingual Scene Text Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Yu Xie, Jielei Zhang, Pengyu Chen, Ziyue Wang, Weihang Wang, Longwen Gao, Peiyi Li, Huyang Sun, Qiang Zhang, Qian Qiao, Jiaqing Fan, Zhouhui Lian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17778">https://arxiv.org/abs/2505.17778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17778">https://arxiv.org/pdf/2505.17778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17778]] TextFlux: An OCR-Free DiT Model for High-Fidelity Multilingual Scene Text Synthesis(https://arxiv.org/abs/2505.17778)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based scene text synthesis has progressed rapidly, yet existing methods commonly rely on additional visual conditioning modules and require large-scale annotated data to support multilingual generation. In this work, we revisit the necessity of complex auxiliary modules and further explore an approach that simultaneously ensures glyph accuracy and achieves high-fidelity scene integration, by leveraging diffusion models' inherent capabilities for contextual reasoning. To this end, we introduce TextFlux, a DiT-based framework that enables multilingual scene text synthesis. The advantages of TextFlux can be summarized as follows: (1) OCR-free model architecture. TextFlux eliminates the need for OCR encoders (additional visual conditioning modules) that are specifically used to extract visual text-related features. (2) Strong multilingual scalability. TextFlux is effective in low-resource multilingual settings, and achieves strong performance in newly added languages with fewer than 1,000 samples. (3) Streamlined training setup. TextFlux is trained with only 1% of the training data required by competing methods. (4) Controllable multi-line text generation. TextFlux offers flexible multi-line synthesis with precise line-level control, outperforming methods restricted to single-line or rigid layouts. Extensive experiments and visualizations demonstrate that TextFlux outperforms previous methods in both qualitative and quantitative evaluations.</li>
</ul>

<h3>Title: Hephaestus Minicubes: A Global, Multi-Modal Dataset for Volcanic Unrest Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Nikolas Papadopoulos, Nikolaos Ioannis Bountos, Maria Sdraka, Andreas Karavias, Ioannis Papoutsis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17782">https://arxiv.org/abs/2505.17782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17782">https://arxiv.org/pdf/2505.17782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17782]] Hephaestus Minicubes: A Global, Multi-Modal Dataset for Volcanic Unrest Monitoring(https://arxiv.org/abs/2505.17782)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Ground deformation is regarded in volcanology as a key precursor signal preceding volcanic eruptions. Satellite-based Interferometric Synthetic Aperture Radar (InSAR) enables consistent, global-scale deformation tracking; however, deep learning methods remain largely unexplored in this domain, mainly due to the lack of a curated machine learning dataset. In this work, we build on the existing Hephaestus dataset, and introduce Hephaestus Minicubes, a global collection of 38 spatiotemporal datacubes offering high resolution, multi-source and multi-temporal information, covering 44 of the world's most active volcanoes over a 7-year period. Each spatiotemporal datacube integrates InSAR products, topographic data, as well as atmospheric variables which are known to introduce signal delays that can mimic ground deformation in InSAR imagery. Furthermore, we provide expert annotations detailing the type, intensity and spatial extent of deformation events, along with rich text descriptions of the observed scenes. Finally, we present a comprehensive benchmark, demonstrating Hephaestus Minicubes' ability to support volcanic unrest monitoring as a multi-modal, multi-temporal classification and semantic segmentation task, establishing strong baselines with state-of-the-art architectures. This work aims to advance machine learning research in volcanic monitoring, contributing to the growing integration of data-driven methods within Earth science applications.</li>
</ul>

<h3>Title: Generative Data Augmentation for Object Point Cloud Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Dekai Zhu, Stefan Gavranovic, Flavien Boussuge, Benjamin Busam, Slobodan Ilic</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17783">https://arxiv.org/abs/2505.17783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17783">https://arxiv.org/pdf/2505.17783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17783]] Generative Data Augmentation for Object Point Cloud Segmentation(https://arxiv.org/abs/2505.17783)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Data augmentation is widely used to train deep learning models to address data scarcity. However, traditional data augmentation (TDA) typically relies on simple geometric transformation, such as random rotation and rescaling, resulting in minimal data diversity enrichment and limited model performance improvement. State-of-the-art generative models for 3D shape generation rely on the denoising diffusion probabilistic models and manage to generate realistic novel point clouds for 3D content creation and manipulation. Nevertheless, the generated 3D shapes lack associated point-wise semantic labels, restricting their usage in enlarging the training data for point cloud segmentation tasks. To bridge the gap between data augmentation techniques and the advanced diffusion models, we extend the state-of-the-art 3D diffusion model, Lion, to a part-aware generative model that can generate high-quality point clouds conditioned on given segmentation masks. Leveraging the novel generative model, we introduce a 3-step generative data augmentation (GDA) pipeline for point cloud segmentation training. Our GDA approach requires only a small amount of labeled samples but enriches the training data with generated variants and pseudo-labeled samples, which are validated by a novel diffusion-based pseudo-label filtering method. Extensive experiments on two large-scale synthetic datasets and a real-world medical dataset demonstrate that our GDA method outperforms TDA approach and related semi-supervised and self-supervised methods.</li>
</ul>

<h3>Title: Latent Mode Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Manuel Morante, Naveed ur Rehman</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17797">https://arxiv.org/abs/2505.17797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17797">https://arxiv.org/pdf/2505.17797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17797]] Latent Mode Decomposition(https://arxiv.org/abs/2505.17797)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>We introduce Variational Latent Mode Decomposition (VLMD), a new algorithm for extracting oscillatory modes and associated connectivity structures from multivariate signals. VLMD addresses key limitations of existing Multivariate Mode Decomposition (MMD) techniques -including high computational cost, sensitivity to parameter choices, and weak modeling of interchannel dependencies. Its improved performance is driven by a novel underlying model, Latent Mode Decomposition (LMD), which blends sparse coding and mode decomposition to represent multichannel signals as sparse linear combinations of shared latent components composed of AM-FM oscillatory modes. This formulation enables VLMD to operate in a lower-dimensional latent space, enhancing robustness to noise, scalability, and interpretability. The algorithm solves a constrained variational optimization problem that jointly enforces reconstruction fidelity, sparsity, and frequency regularization. Experiments on synthetic and real-world datasets demonstrate that VLMD outperforms state-of-the-art MMD methods in accuracy, efficiency, and interpretability of extracted structures.</li>
</ul>

<h3>Title: A Coreset Selection of Coreset Selection Literature: Introduction and Recent Advances</h3>
<ul>
<li><strong>Authors: </strong>Brian B. Moser, Arundhati S. Shanbhag, Stanislav Frolov, Federico Raue, Joachim Folz, Andreas Dengel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17799">https://arxiv.org/abs/2505.17799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17799">https://arxiv.org/pdf/2505.17799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17799]] A Coreset Selection of Coreset Selection Literature: Introduction and Recent Advances(https://arxiv.org/abs/2505.17799)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Coreset selection targets the challenge of finding a small, representative subset of a large dataset that preserves essential patterns for effective machine learning. Although several surveys have examined data reduction strategies before, most focus narrowly on either classical geometry-based methods or active learning techniques. In contrast, this survey presents a more comprehensive view by unifying three major lines of coreset research, namely, training-free, training-oriented, and label-free approaches, into a single taxonomy. We present subfields often overlooked by existing work, including submodular formulations, bilevel optimization, and recent progress in pseudo-labeling for unlabeled datasets. Additionally, we examine how pruning strategies influence generalization and neural scaling laws, offering new insights that are absent from prior reviews. Finally, we compare these methods under varying computational, robustness, and performance demands and highlight open challenges, such as robustness, outlier filtering, and adapting coreset selection to foundation models, for future research.</li>
</ul>

<h3>Title: Temporal Consistency Constrained Transferable Adversarial Attacks with Background Mixup for Action Recognition</h3>
<ul>
<li><strong>Authors: </strong>Ping Li, Jianan Ni, Bo Pang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17807">https://arxiv.org/abs/2505.17807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17807">https://arxiv.org/pdf/2505.17807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17807]] Temporal Consistency Constrained Transferable Adversarial Attacks with Background Mixup for Action Recognition(https://arxiv.org/abs/2505.17807)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Action recognition models using deep learning are vulnerable to adversarial examples, which are transferable across other models trained on the same data modality. Existing transferable attack methods face two major challenges: 1) they heavily rely on the assumption that the decision boundaries of the surrogate (a.k.a., source) model and the target model are similar, which limits the adversarial transferability; and 2) their decision boundary difference makes the attack direction uncertain, which may result in the gradient oscillation, weakening the adversarial attack. This motivates us to propose a Background Mixup-induced Temporal Consistency (BMTC) attack method for action recognition. From the input transformation perspective, we design a model-agnostic background adversarial mixup module to reduce the surrogate-target model dependency. In particular, we randomly sample one video from each category and make its background frame, while selecting the background frame with the top attack ability for mixup with the clean frame by reinforcement learning. Moreover, to ensure an explicit attack direction, we leverage the background category as guidance for updating the gradient of adversarial example, and design a temporal gradient consistency loss, which strengthens the stability of the attack direction on subsequent frames. Empirical studies on two video datasets, i.e., UCF101 and Kinetics-400, and one image dataset, i.e., ImageNet, demonstrate that our method significantly boosts the transferability of adversarial examples across several action/image recognition models. Our code is available at this https URL.</li>
</ul>

<h3>Title: An Attention Infused Deep Learning System with Grad-CAM Visualization for Early Screening of Glaucoma</h3>
<ul>
<li><strong>Authors: </strong>Ramanathan Swaminathan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17808">https://arxiv.org/abs/2505.17808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17808">https://arxiv.org/pdf/2505.17808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17808]] An Attention Infused Deep Learning System with Grad-CAM Visualization for Early Screening of Glaucoma(https://arxiv.org/abs/2505.17808)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This research work reveals the eye opening wisdom of the hybrid labyrinthine deep learning models synergy born out of combining a trailblazing convolutional neural network with a disruptive Vision Transformer, both intertwined together with a radical Cross Attention module. Here, two high yielding datasets for artificial intelligence models in detecting glaucoma, namely ACRIMA and Drishti, are utilized.</li>
</ul>

<h3>Title: Seeing It or Not? Interpretable Vision-aware Latent Steering to Mitigate Object Hallucinations</h3>
<ul>
<li><strong>Authors: </strong>Boxu Chen, Ziwei Zheng, Le Yang, Zeyu Geng, Zhengyu Zhao, Chenhao Lin, Chao Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17812">https://arxiv.org/abs/2505.17812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17812">https://arxiv.org/pdf/2505.17812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17812]] Seeing It or Not? Interpretable Vision-aware Latent Steering to Mitigate Object Hallucinations(https://arxiv.org/abs/2505.17812)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) have achieved remarkable success but continue to struggle with object hallucination (OH), generating outputs inconsistent with visual inputs. While previous work has proposed methods to reduce OH, the visual decision-making mechanisms that lead to hallucinations remain poorly understood. In this paper, we propose VaLSe, a Vision-aware Latent Steering framework that adopts an interpretation-then-mitigation strategy to address OH in LVLMs. By tackling dual challenges of modeling complex vision-language interactions and eliminating spurious activation artifacts, VaLSe can generate visual contribution maps that trace how specific visual inputs influence individual output tokens. These maps reveal the model's vision-aware focus regions, which are then used to perform latent space steering, realigning internal representations toward semantically relevant content and reducing hallucinated outputs. Extensive experiments demonstrate that VaLSe is a powerful interpretability tool and an effective method for enhancing model robustness against OH across multiple benchmarks. Furthermore, our analysis uncovers limitations in existing OH evaluation metrics, underscoring the need for more nuanced, interpretable, and visually grounded OH benchmarks in future work. Code is available at: this https URL.</li>
</ul>

<h3>Title: Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Michael Hassid, Gabriel Synnaeve, Yossi Adi, Roy Schwartz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17813">https://arxiv.org/abs/2505.17813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17813">https://arxiv.org/pdf/2505.17813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17813]] Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning(https://arxiv.org/abs/2505.17813)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive "thinking" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. In this work, we challenge the assumption that long thinking chains results in better reasoning capabilities. We first demonstrate that shorter reasoning chains within individual questions are significantly more likely to yield correct answers - up to 34.5% more accurate than the longest chain sampled for the same question. Based on these results, we suggest short-m@k, a novel reasoning LLM inference method. Our method executes k independent generations in parallel and halts computation once the first m thinking processes are done. The final answer is chosen using majority voting among these m chains. Basic short-1@k demonstrates similar or even superior performance over standard majority voting in low-compute settings - using up to 40% fewer thinking tokens. short-3@k, while slightly less efficient than short-1@k, consistently surpasses majority voting across all compute budgets, while still being substantially faster (up to 33% wall time reduction). Inspired by our results, we finetune an LLM using short, long, and randomly selected reasoning chains. We then observe that training on the shorter ones leads to better performance. Our findings suggest rethinking current methods of test-time compute in reasoning LLMs, emphasizing that longer "thinking" does not necessarily translate to improved performance and can, counter-intuitively, lead to degraded results.</li>
</ul>

<h3>Title: Low-Resource NMT: A Case Study on the Written and Spoken Languages in Hong Kong</h3>
<ul>
<li><strong>Authors: </strong>Hei Yi Mak, Tan Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17816">https://arxiv.org/abs/2505.17816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17816">https://arxiv.org/pdf/2505.17816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17816]] Low-Resource NMT: A Case Study on the Written and Spoken Languages in Hong Kong(https://arxiv.org/abs/2505.17816)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The majority of inhabitants in Hong Kong are able to read and write in standard Chinese but use Cantonese as the primary spoken language in daily life. Spoken Cantonese can be transcribed into Chinese characters, which constitute the so-called written Cantonese. Written Cantonese exhibits significant lexical and grammatical differences from standard written Chinese. The rise of written Cantonese is increasingly evident in the cyber world. The growing interaction between Mandarin speakers and Cantonese speakers is leading to a clear demand for automatic translation between Chinese and Cantonese. This paper describes a transformer-based neural machine translation (NMT) system for written-Chinese-to-written-Cantonese translation. Given that parallel text data of Chinese and Cantonese are extremely scarce, a major focus of this study is on the effort of preparing good amount of training data for NMT. In addition to collecting 28K parallel sentences from previous linguistic studies and scattered internet resources, we devise an effective approach to obtaining 72K parallel sentences by automatically extracting pairs of semantically similar sentences from parallel articles on Chinese Wikipedia and Cantonese Wikipedia. We show that leveraging highly similar sentence pairs mined from Wikipedia improves translation performance in all test sets. Our system outperforms Baidu Fanyi's Chinese-to-Cantonese translation on 6 out of 8 test sets in BLEU scores. Translation examples reveal that our system is able to capture important linguistic transformations between standard Chinese and spoken Cantonese.</li>
</ul>

<h3>Title: Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xuchen Pan, Yanxi Chen, Yushuo Chen, Yuchang Sun, Daoyuan Chen, Wenhao Zhang, Yuexiang Xie, Yilun Huang, Yilei Zhang, Dawei Gao, Yaliang Li, Bolin Ding, Jingren Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17826">https://arxiv.org/abs/2505.17826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17826">https://arxiv.org/pdf/2505.17826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17826]] Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models(https://arxiv.org/abs/2505.17826)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Trinity-RFT is a general-purpose, flexible and scalable framework designed for reinforcement fine-tuning (RFT) of large language models. It is built with a decoupled design, consisting of (1) an RFT-core that unifies and generalizes synchronous/asynchronous, on-policy/off-policy, and online/offline modes of RFT, (2) seamless integration for agent-environment interaction with high efficiency and robustness, and (3) systematic data pipelines optimized for RFT. Trinity-RFT can be easily adapted for diverse application scenarios, and serves as a unified platform for exploring advanced reinforcement learning paradigms. This technical report outlines the vision, features, design and implementations of Trinity-RFT, accompanied by extensive examples demonstrating the utility and user-friendliness of the proposed framework.</li>
</ul>

<h3>Title: Stepwise Reasoning Checkpoint Analysis: A Test Time Scaling Method to Enhance LLMs' Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zezhong Wang, Xingshan Zeng, Weiwen Liu, Yufei Wang, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17829">https://arxiv.org/abs/2505.17829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17829">https://arxiv.org/pdf/2505.17829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17829]] Stepwise Reasoning Checkpoint Analysis: A Test Time Scaling Method to Enhance LLMs' Reasoning(https://arxiv.org/abs/2505.17829)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mathematical reasoning through Chain-of-Thought (CoT) has emerged as a powerful capability of Large Language Models (LLMs), which can be further enhanced through Test-Time Scaling (TTS) methods like Beam Search and DVTS. However, these methods, despite improving accuracy by allocating more computational resources during inference, often suffer from path homogenization and inefficient use of intermediate results. To address these limitations, we propose Stepwise Reasoning Checkpoint Analysis (SRCA), a framework that introduces checkpoints between reasoning steps. It incorporates two key strategies: (1) Answer-Clustered Search, which groups reasoning paths by their intermediate checkpoint answers to maintain diversity while ensuring quality, and (2) Checkpoint Candidate Augmentation, which leverages all intermediate answers for final decision-making. Our approach effectively reduces path homogenization and creates a fault-tolerant mechanism by utilizing high-quality intermediate results. Experimental results show that SRCA improves reasoning accuracy compared to existing TTS methods across various mathematical datasets.</li>
</ul>

<h3>Title: Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Castanet, Olivier Sigaud, Sylvain Lamprier</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17830">https://arxiv.org/abs/2505.17830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17830">https://arxiv.org/pdf/2505.17830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17830]] Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning(https://arxiv.org/abs/2505.17830)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously acquire diverse behaviors, but faces major challenges in visual environments due to high-dimensional, semantically sparse observations. In the online setting, where agents learn representations while exploring, the latent space evolves with the agent's policy, to capture newly discovered areas of the environment. However, without incentivization to maximize state coverage in the representation, classical approaches based on auto-encoders may converge to latent spaces that over-represent a restricted set of states frequently visited by the agent. This is exacerbated in an intrinsic motivation setting, where the agent uses the distribution encoded in the latent space to sample the goals it learns to master. To address this issue, we propose to progressively enforce distributional shifts towards a uniform distribution over the full state space, to ensure a full coverage of skills that can be learned in the environment. We introduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that combines the $\beta$-VAE framework with Distributionally Robust Optimization. DRAG leverages an adversarial neural weighter of training states of the VAE, to account for the mismatch between the current data distribution and unseen parts of the environment. This allows the agent to construct semantically meaningful latent spaces beyond its immediate experience. Our approach improves state space coverage and downstream control performance on hard exploration environments such as mazes and robotic control involving walls to bypass, without pre-training nor prior environment knowledge.</li>
</ul>

<h3>Title: Scaling Recurrent Neural Networks to a Billion Parameters with Zero-Order Optimization</h3>
<ul>
<li><strong>Authors: </strong>Francois Chaubard, Mykel Kochenderfer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17852">https://arxiv.org/abs/2505.17852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17852">https://arxiv.org/pdf/2505.17852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17852]] Scaling Recurrent Neural Networks to a Billion Parameters with Zero-Order Optimization(https://arxiv.org/abs/2505.17852)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>During inference, Recurrent Neural Networks (RNNs) scale constant in both FLOPs and GPU memory with increasing context length, as they compress all prior tokens into a fixed-size memory. In contrast, transformers scale linearly in FLOPs and, at best, linearly in memory during generation, since they must attend to all previous tokens explicitly. Despite this inference-time advantage, training large RNNs on long contexts remains impractical because standard optimization methods depend on Backpropagation Through Time (BPTT). BPTT requires retention of all intermediate activations during the forward pass, causing memory usage to scale linearly with both context length and model size. In this paper, we show that Zero-Order Optimization (ZOO) methods such as Random-vector Gradient Estimation (RGE) can successfully replace BPTT to train RNNs with convergence rates that match, or exceed BPTT by up to 19 fold, while using orders of magnitude less memory and cost, as the model remains in inference mode throughout training. We further demonstrate that Central-Difference RGE (CD-RGE) corresponds to optimizing a smoothed surrogate loss, inherently regularizing training and improving generalization. Our method matches or outperforms BPTT across three settings: (1) overfitting, (2) transduction, and (3) language modeling. Across all tasks, with sufficient perturbations, our models generalize as well as or better than those trained with BPTT, often in fewer steps. Despite the need for more forward passes per step, we can surpass BPTT wall-clock time per step using recent advancements such as FlashRNN and distributed inference.</li>
</ul>

<h3>Title: Stochastic Weight Sharing for Bayesian Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Moule Lin, Shuhao Guan, Weipeng Jing, Goetz Botterweck, Andrea Patane</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17856">https://arxiv.org/abs/2505.17856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17856">https://arxiv.org/pdf/2505.17856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17856]] Stochastic Weight Sharing for Bayesian Neural Networks(https://arxiv.org/abs/2505.17856)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While offering a principled framework for uncertainty quantification in deep learning, the employment of Bayesian Neural Networks (BNNs) is still constrained by their increased computational requirements and the convergence difficulties when training very deep, state-of-the-art architectures. In this work, we reinterpret weight-sharing quantization techniques from a stochastic perspective in the context of training and inference with Bayesian Neural Networks (BNNs). Specifically, we leverage 2D adaptive Gaussian distributions, Wasserstein distance estimations, and alpha blending to encode the stochastic behaviour of a BNN in a lower dimensional, soft Gaussian representation. Through extensive empirical investigation, we demonstrate that our approach significantly reduces the computational overhead inherent in Bayesian learning by several orders of magnitude, enabling the efficient Bayesian training of large-scale models, such as ResNet-101 and Vision Transformer (VIT). On various computer vision benchmarks including CIFAR10, CIFAR100, and ImageNet1k. Our approach compresses model parameters by approximately 50x and reduces model size by 75, while achieving accuracy and uncertainty estimations comparable to the state-of-the-art.</li>
</ul>

<h3>Title: Scalable Valuation of Human Feedback through Provably Robust Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Masahiro Fujisawa, Masaki Adachi, Michael A. Osborne</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17859">https://arxiv.org/abs/2505.17859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17859">https://arxiv.org/pdf/2505.17859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17859]] Scalable Valuation of Human Feedback through Provably Robust Model Alignment(https://arxiv.org/abs/2505.17859)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite the importance of aligning language models with human preferences, crowd-sourced human feedback is often noisy -- for example, preferring less desirable responses -- posing a fundamental challenge to alignment. A truly robust alignment objective should yield identical model parameters even under severe label noise, a property known as redescending. We prove that no existing alignment methods satisfy this property. To address this, we propose Hölder-DPO, the first principled alignment loss with a provable redescending property, enabling estimation of the clean data distribution from noisy feedback. The aligned model estimates the likelihood of clean data, providing a theoretically grounded metric for dataset valuation that identifies the location and fraction of mislabels. This metric is gradient-free, enabling scalable and automated human feedback valuation without costly manual verification or clean validation dataset. Hölder-DPO achieves state-of-the-art robust alignment performance while accurately detecting mislabels in controlled datasets. Finally, we apply Hölder-DPO to widely used alignment datasets, revealing substantial noise levels and demonstrating that removing these mislabels significantly improves alignment performance across methods.</li>
</ul>

<h3>Title: The emergence of sparse attention: impact of data distribution and benefits of repetition</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Zucchet, Francesco d'Angelo, Andrew K. Lampinen, Stephanie C.Y. Chan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17863">https://arxiv.org/abs/2505.17863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17863">https://arxiv.org/pdf/2505.17863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17863]] The emergence of sparse attention: impact of data distribution and benefits of repetition(https://arxiv.org/abs/2505.17863)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Emergence is a fascinating property of large language models and neural networks more broadly: as models scale and train for longer, they sometimes develop new abilities in sudden ways. Despite initial studies, we still lack a comprehensive understanding of how and when these abilities emerge. To address this gap, we study the emergence over training of sparse attention, a critical and frequently observed attention pattern in Transformers. By combining theoretical analysis of a toy model with empirical observations on small Transformers trained on a linear regression variant, we uncover the mechanics driving sparse attention emergence and reveal that emergence timing follows power laws based on task structure, architecture, and optimizer choice. We additionally find that repetition can greatly speed up emergence. Finally, we confirm these results on a well-studied in-context associative recall task. Our findings provide a simple, theoretically grounded framework for understanding how data distributions and model design influence the learning dynamics behind one form of emergence.</li>
</ul>

<h3>Title: Multi-task Learning For Joint Action and Gesture Recognition</h3>
<ul>
<li><strong>Authors: </strong>Konstantinos Spathis, Nikolaos Kardaris, Petros Maragos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17867">https://arxiv.org/abs/2505.17867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17867">https://arxiv.org/pdf/2505.17867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17867]] Multi-task Learning For Joint Action and Gesture Recognition(https://arxiv.org/abs/2505.17867)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In practical applications, computer vision tasks often need to be addressed simultaneously. Multitask learning typically achieves this by jointly training a single deep neural network to learn shared representations, providing efficiency and improving generalization. Although action and gesture recognition are closely related tasks, since they focus on body and hand movements, current state-of-the-art methods handle them separately. In this paper, we show that employing a multi-task learning paradigm for action and gesture recognition results in more efficient, robust and generalizable visual representations, by leveraging the synergies between these tasks. Extensive experiments on multiple action and gesture datasets demonstrate that handling actions and gestures in a single architecture can achieve better performance for both tasks in comparison to their single-task learning variants.</li>
</ul>

<h3>Title: Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods</h3>
<ul>
<li><strong>Authors: </strong>Shaina Raza, Rizwan Qureshi, Marcelo Lotif, Aman Chadha, Deval Pandya, Christos Emmanouilidis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17870">https://arxiv.org/abs/2505.17870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17870">https://arxiv.org/pdf/2505.17870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17870]] Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods(https://arxiv.org/abs/2505.17870)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI models often learn and reproduce false information present in their training corpora. This position paper argues that, analogous to biological immunization, where controlled exposure to a weakened pathogen builds immunity, AI models should be fine tuned on small, quarantined sets of explicitly labeled falsehoods as a "vaccine" against misinformation. These curated false examples are periodically injected during finetuning, strengthening the model ability to recognize and reject misleading claims while preserving accuracy on truthful inputs. An illustrative case study shows that immunized models generate substantially less misinformation than baselines. To our knowledge, this is the first training framework that treats fact checked falsehoods themselves as a supervised vaccine, rather than relying on input perturbations or generic human feedback signals, to harden models against future misinformation. We also outline ethical safeguards and governance controls to ensure the safe use of false data. Model immunization offers a proactive paradigm for aligning AI systems with factuality.</li>
</ul>

<h3>Title: MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback</h3>
<ul>
<li><strong>Authors: </strong>Wanhao Liu, Zonglin Yang, Jue Wang, Lidong Bing, Di Zhang, Dongzhan Zhou, Yuqiang Li, Houqiang Li, Erik Cambria, Wanli Ouyang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17873">https://arxiv.org/abs/2505.17873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17873">https://arxiv.org/pdf/2505.17873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17873]] MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback(https://arxiv.org/abs/2505.17873)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Hypothesis ranking is a crucial component of automated scientific discovery, particularly in natural sciences where wet-lab experiments are costly and throughput-limited. Existing approaches focus on pre-experiment ranking, relying solely on large language model's internal reasoning without incorporating empirical outcomes from experiments. We introduce the task of experiment-guided ranking, which aims to prioritize candidate hypotheses based on the results of previously tested ones. However, developing such strategies is challenging due to the impracticality of repeatedly conducting real experiments in natural science domains. To address this, we propose a simulator grounded in three domain-informed assumptions, modeling hypothesis performance as a function of similarity to a known ground truth hypothesis, perturbed by noise. We curate a dataset of 124 chemistry hypotheses with experimentally reported outcomes to validate the simulator. Building on this simulator, we develop a pseudo experiment-guided ranking method that clusters hypotheses by shared functional characteristics and prioritizes candidates based on insights derived from simulated experimental feedback. Experiments show that our method outperforms pre-experiment baselines and strong ablations.</li>
</ul>

<h3>Title: FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Laines Schmalwasser, Niklas Penzel, Joachim Denzler, Julia Niebling</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17883">https://arxiv.org/abs/2505.17883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17883">https://arxiv.org/pdf/2505.17883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17883]] FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks(https://arxiv.org/abs/2505.17883)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, explainability</a></li>
<li><strong>Abstract: </strong>Concepts such as objects, patterns, and shapes are how humans understand the world. Building on this intuition, concept-based explainability methods aim to study representations learned by deep neural networks in relation to human-understandable concepts. Here, Concept Activation Vectors (CAVs) are an important tool and can identify whether a model learned a concept or not. However, the computational cost and time requirements of existing CAV computation pose a significant challenge, particularly in large-scale, high-dimensional architectures. To address this limitation, we introduce FastCAV, a novel approach that accelerates the extraction of CAVs by up to 63.6x (on average 46.4x). We provide a theoretical foundation for our approach and give concrete assumptions under which it is equivalent to established SVM-based methods. Our empirical results demonstrate that CAVs calculated with FastCAV maintain similar performance while being more efficient and stable. In downstream applications, i.e., concept-based explanation methods, we show that FastCAV can act as a replacement leading to equivalent insights. Hence, our approach enables previously infeasible investigations of deep models, which we demonstrate by tracking the evolution of concepts during model training.</li>
</ul>

<h3>Title: Track Anything Annotate: Video annotation and dataset generation of computer vision models</h3>
<ul>
<li><strong>Authors: </strong>Nikita Ivanov, Mark Klimov, Dmitry Glukhikh, Tatiana Chernysheva, Igor Glukhikh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17884">https://arxiv.org/abs/2505.17884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17884">https://arxiv.org/pdf/2505.17884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17884]] Track Anything Annotate: Video annotation and dataset generation of computer vision models(https://arxiv.org/abs/2505.17884)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Modern machine learning methods require significant amounts of labelled data, making the preparation process time-consuming and resource-intensive. In this paper, we propose to consider the process of prototyping a tool for annotating and generating training datasets based on video tracking and segmentation. We examine different approaches to solving this problem, from technology selection through to final implementation. The developed prototype significantly accelerates dataset generation compared to manual annotation. All resources are available at this https URL</li>
</ul>

<h3>Title: Pixels to Prognosis: Harmonized Multi-Region CT-Radiomics and Foundation-Model Signatures Across Multicentre NSCLC Data</h3>
<ul>
<li><strong>Authors: </strong>Shruti Atul Mali, Zohaib Salahuddin, Danial Khan, Yumeng Zhang, Henry C. Woodruff, Eduardo Ibor-Crespo, Ana Jimenez-Pastor, Luis Marti-Bonmati, Philippe Lambin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17893">https://arxiv.org/abs/2505.17893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17893">https://arxiv.org/pdf/2505.17893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17893]] Pixels to Prognosis: Harmonized Multi-Region CT-Radiomics and Foundation-Model Signatures Across Multicentre NSCLC Data(https://arxiv.org/abs/2505.17893)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Purpose: To evaluate the impact of harmonization and multi-region CT image feature integration on survival prediction in non-small cell lung cancer (NSCLC) patients, using handcrafted radiomics, pretrained foundation model (FM) features, and clinical data from a multicenter dataset. Methods: We analyzed CT scans and clinical data from 876 NSCLC patients (604 training, 272 test) across five centers. Features were extracted from the whole lung, tumor, mediastinal nodes, coronary arteries, and coronary artery calcium (CAC). Handcrafted radiomics and FM deep features were harmonized using ComBat, reconstruction kernel normalization (RKN), and RKN+ComBat. Regularized Cox models predicted overall survival; performance was assessed using the concordance index (C-index), 5-year time-dependent area under the curve (t-AUC), and hazard ratio (HR). SHapley Additive exPlanations (SHAP) values explained feature contributions. A consensus model used agreement across top region of interest (ROI) models to stratify patient risk. Results: TNM staging showed prognostic utility (C-index = 0.67; HR = 2.70; t-AUC = 0.85). The clinical + tumor radiomics model with ComBat achieved a C-index of 0.7552 and t-AUC of 0.8820. FM features (50-voxel cubes) combined with clinical data yielded the highest performance (C-index = 0.7616; t-AUC = 0.8866). An ensemble of all ROIs and FM features reached a C-index of 0.7142 and t-AUC of 0.7885. The consensus model, covering 78% of valid test cases, achieved a t-AUC of 0.92, sensitivity of 97.6%, and specificity of 66.7%. Conclusion: Harmonization and multi-region feature integration improve survival prediction in multicenter NSCLC data. Combining interpretable radiomics, FM features, and consensus modeling enables robust risk stratification across imaging centers.</li>
</ul>

<h3>Title: Universal Domain Adaptation Benchmark for Time Series Data Representation</h3>
<ul>
<li><strong>Authors: </strong>Romain Mussard, Fannia Pacheco, Maxime Berar, Gilles Gasso, Paul Honeine</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17899">https://arxiv.org/abs/2505.17899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17899">https://arxiv.org/pdf/2505.17899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17899]] Universal Domain Adaptation Benchmark for Time Series Data Representation(https://arxiv.org/abs/2505.17899)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning models have significantly improved the ability to detect novelties in time series (TS) data. This success is attributed to their strong representation capabilities. However, due to the inherent variability in TS data, these models often struggle with generalization and robustness. To address this, a common approach is to perform Unsupervised Domain Adaptation, particularly Universal Domain Adaptation (UniDA), to handle domain shifts and emerging novel classes. While extensively studied in computer vision, UniDA remains underexplored for TS data. This work provides a comprehensive implementation and comparison of state-of-the-art TS backbones in a UniDA framework. We propose a reliable protocol to evaluate their robustness and generalization across different domains. The goal is to provide practitioners with a framework that can be easily extended to incorporate future advancements in UniDA and TS architectures. Our results highlight the critical influence of backbone selection in UniDA performance and enable a robustness analysis across various datasets and architectures.</li>
</ul>

<h3>Title: Evolving Machine Learning: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Ignacio Cabrera Martin, Subhaditya Mukherjee, Almas Baimagambetov, Joaquin Vanschoren, Nikolaos Polatidis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17902">https://arxiv.org/abs/2505.17902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17902">https://arxiv.org/pdf/2505.17902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17902]] Evolving Machine Learning: A Survey(https://arxiv.org/abs/2505.17902)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In an era defined by rapid data evolution, traditional machine learning (ML) models often fall short in adapting to dynamic environments. Evolving Machine Learning (EML) has emerged as a critical paradigm, enabling continuous learning and adaptation in real-time data streams. This survey presents a comprehensive analysis of EML, focusing on five core challenges: data drift, concept drift, catastrophic forgetting, skewed learning, and network adaptation. We systematically review over 120 studies, categorizing state-of-the-art methods across supervised, unsupervised, and semi-supervised approaches. The survey explores diverse evaluation metrics, benchmark datasets, and real-world applications, offering a comparative lens on the effectiveness and limitations of current techniques. Additionally, we highlight the growing role of adaptive neural architectures, meta-learning, and ensemble strategies in addressing evolving data complexities. By synthesizing insights from recent literature, this work not only maps the current landscape of EML but also identifies critical gaps and opportunities for future research. Our findings aim to guide researchers and practitioners in developing robust, ethical, and scalable EML systems for real-world deployment.</li>
</ul>

<h3>Title: Semantic segmentation with reward</h3>
<ul>
<li><strong>Authors: </strong>Xie Ting, Ye Huang, Zhilin Liu, Lixin Duan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17905">https://arxiv.org/abs/2505.17905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17905">https://arxiv.org/pdf/2505.17905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17905]] Semantic segmentation with reward(https://arxiv.org/abs/2505.17905)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In real-world scenarios, pixel-level labeling is not always available. Sometimes, we need a semantic segmentation network, and even a visual encoder can have a high compatibility, and can be trained using various types of feedback beyond traditional labels, such as feedback that indicates the quality of the parsing results. To tackle this issue, we proposed RSS (Reward in Semantic Segmentation), the first practical application of reward-based reinforcement learning on pure semantic segmentation offered in two granular levels (pixel-level and image-level). RSS incorporates various novel technologies, such as progressive scale rewards (PSR) and pair-wise spatial difference (PSD), to ensure that the reward facilitates the convergence of the semantic segmentation network, especially under image-level rewards. Experiments and visualizations on benchmark datasets demonstrate that the proposed RSS can successfully ensure the convergence of the semantic segmentation network on two levels of rewards. Additionally, the RSS, which utilizes an image-level reward, outperforms existing weakly supervised methods that also rely solely on image-level signals during training.</li>
</ul>

<h3>Title: NeuroTrails: Training with Dynamic Sparse Heads as the Key to Effective Ensembling</h3>
<ul>
<li><strong>Authors: </strong>Bram Grooten, Farid Hasanov, Chenxiang Zhang, Qiao Xiao, Boqian Wu, Zahra Atashgahi, Ghada Sokar, Shiwei Liu, Lu Yin, Elena Mocanu, Mykola Pechenizkiy, Decebal Constantin Mocanu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17909">https://arxiv.org/abs/2505.17909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17909">https://arxiv.org/pdf/2505.17909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17909]] NeuroTrails: Training with Dynamic Sparse Heads as the Key to Effective Ensembling(https://arxiv.org/abs/2505.17909)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Model ensembles have long been a cornerstone for improving generalization and robustness in deep learning. However, their effectiveness often comes at the cost of substantial computational overhead. To address this issue, state-of-the-art methods aim to replicate ensemble-class performance without requiring multiple independently trained networks. Unfortunately, these algorithms often still demand considerable compute at inference. In response to these limitations, we introduce $\textbf{NeuroTrails}$, a sparse multi-head architecture with dynamically evolving topology. This unexplored model-agnostic training paradigm improves ensemble performance while reducing the required resources. We analyze the underlying reason for its effectiveness and observe that the various neural trails induced by dynamic sparsity attain a $\textit{Goldilocks zone}$ of prediction diversity. NeuroTrails displays efficacy with convolutional and transformer-based architectures on computer vision and language tasks. Experiments on ResNet-50/ImageNet, LLaMA-350M/C4, among many others, demonstrate increased accuracy and stronger robustness in zero-shot generalization, while requiring significantly fewer parameters.</li>
</ul>

<h3>Title: DiffusionReward: Enhancing Blind Face Restoration through Reward Feedback Learning</h3>
<ul>
<li><strong>Authors: </strong>Bin Wu, Wei Wang, Yahui Liu, Zixiang Li, Yao Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17910">https://arxiv.org/abs/2505.17910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17910">https://arxiv.org/pdf/2505.17910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17910]] DiffusionReward: Enhancing Blind Face Restoration through Reward Feedback Learning(https://arxiv.org/abs/2505.17910)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Reward Feedback Learning (ReFL) has recently shown great potential in aligning model outputs with human preferences across various generative tasks. In this work, we introduce a ReFL framework, named DiffusionReward, to the Blind Face Restoration task for the first time. DiffusionReward effectively overcomes the limitations of diffusion-based methods, which often fail to generate realistic facial details and exhibit poor identity consistency. The core of our framework is the Face Reward Model (FRM), which is trained using carefully annotated data. It provides feedback signals that play a pivotal role in steering the optimization process of the restoration network. In particular, our ReFL framework incorporates a gradient flow into the denoising process of off-the-shelf face restoration methods to guide the update of model parameters. The guiding gradient is collaboratively determined by three aspects: (i) the FRM to ensure the perceptual quality of the restored faces; (ii) a regularization term that functions as a safeguard to preserve generative diversity; and (iii) a structural consistency constraint to maintain facial fidelity. Furthermore, the FRM undergoes dynamic optimization throughout the process. It not only ensures that the restoration network stays precisely aligned with the real face manifold, but also effectively prevents reward hacking. Experiments on synthetic and wild datasets demonstrate that our method outperforms state-of-the-art methods, significantly improving identity consistency and facial details. The source codes, data, and models are available at: this https URL.</li>
</ul>

<h3>Title: Object-level Cross-view Geo-localization with Location Enhancement and Multi-Head Cross Attention</h3>
<ul>
<li><strong>Authors: </strong>Zheyang Huang, Jagannath Aryal, Saeid Nahavandi, Xuequan Lu, Chee Peng Lim, Lei Wei, Hailing Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17911">https://arxiv.org/abs/2505.17911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17911">https://arxiv.org/pdf/2505.17911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17911]] Object-level Cross-view Geo-localization with Location Enhancement and Multi-Head Cross Attention(https://arxiv.org/abs/2505.17911)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Cross-view geo-localization determines the location of a query image, captured by a drone or ground-based camera, by matching it to a geo-referenced satellite image. While traditional approaches focus on image-level localization, many applications, such as search-and-rescue, infrastructure inspection, and precision delivery, demand object-level accuracy. This enables users to prompt a specific object with a single click on a drone image to retrieve precise geo-tagged information of the object. However, variations in viewpoints, timing, and imaging conditions pose significant challenges, especially when identifying visually similar objects in extensive satellite imagery. To address these challenges, we propose an Object-level Cross-view Geo-localization Network (OCGNet). It integrates user-specified click locations using Gaussian Kernel Transfer (GKT) to preserve location information throughout the network. This cue is dually embedded into the feature encoder and feature matching blocks, ensuring robust object-specific localization. Additionally, OCGNet incorporates a Location Enhancement (LE) module and a Multi-Head Cross Attention (MHCA) module to adaptively emphasize object-specific features or expand focus to relevant contextual regions when necessary. OCGNet achieves state-of-the-art performance on a public dataset, CVOGL. It also demonstrates few-shot learning capabilities, effectively generalizing from limited examples, making it suitable for diverse applications (this https URL).</li>
</ul>

<h3>Title: LLM Meeting Decision Trees on Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Hangting Ye, Jinmeng Li, He Zhao, Dandan Guo, Yi Chang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17918">https://arxiv.org/abs/2505.17918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17918">https://arxiv.org/pdf/2505.17918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17918]] LLM Meeting Decision Trees on Tabular Data(https://arxiv.org/abs/2505.17918)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Tabular data have been playing a vital role in diverse real-world fields, including healthcare, finance, etc. With the recent success of Large Language Models (LLMs), early explorations of extending LLMs to the domain of tabular data have been developed. Most of these LLM-based methods typically first serialize tabular data into natural language descriptions, and then tune LLMs or directly infer on these serialized data. However, these methods suffer from two key inherent issues: (i) data perspective: existing data serialization methods lack universal applicability for structured tabular data, and may pose privacy risks through direct textual exposure, and (ii) model perspective: LLM fine-tuning methods struggle with tabular data, and in-context learning scalability is bottle-necked by input length constraints (suitable for few-shot learning). This work explores a novel direction of integrating LLMs into tabular data throughough logical decision tree rules as intermediaries, proposes a decision tree enhancer with LLM-derived rule for tabular prediction, DeLTa. The proposed DeLTa avoids tabular data serialization, and can be applied to full data learning setting without LLM fine-tuning. Specifically, we leverage the reasoning ability of LLMs to redesign an improved rule given a set of decision tree rules. Furthermore, we provide a calibration method for original decision trees via new generated rule by LLM, which approximates the error correction vector to steer the original decision tree predictions in the direction of ``errors'' reducing. Finally, extensive experiments on diverse tabular benchmarks show that our method achieves state-of-the-art performance.</li>
</ul>

<h3>Title: Language models can learn implicit multi-hop reasoning, but only if they have lots of training data</h3>
<ul>
<li><strong>Authors: </strong>Yuekun Yao, Yupei Du, Dawei Zhu, Michael Hahn, Alexander Koller</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17923">https://arxiv.org/abs/2505.17923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17923">https://arxiv.org/pdf/2505.17923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17923]] Language models can learn implicit multi-hop reasoning, but only if they have lots of training data(https://arxiv.org/abs/2505.17923)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Implicit reasoning is the ability of a language model to solve multi-hop reasoning tasks in a single forward pass, without chain of thought. We investigate this capability using GPT2-style language models trained from scratch on controlled $k$-hop reasoning datasets ($k = 2, 3, 4$). We show that while such models can indeed learn implicit $k$-hop reasoning, the required training data grows exponentially in $k$, and the required number of transformer layers grows linearly in $k$. We offer a theoretical explanation for why this depth growth is necessary. We further find that the data requirement can be mitigated, but not eliminated, through curriculum learning.</li>
</ul>

<h3>Title: Predicting Length of Stay in Neurological ICU Patients Using Classical Machine Learning and Neural Network Models: A Benchmark Study on MIMIC-IV</h3>
<ul>
<li><strong>Authors: </strong>Alexander Gabitashvili, Philipp Kellmeyer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17929">https://arxiv.org/abs/2505.17929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17929">https://arxiv.org/pdf/2505.17929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17929]] Predicting Length of Stay in Neurological ICU Patients Using Classical Machine Learning and Neural Network Models: A Benchmark Study on MIMIC-IV(https://arxiv.org/abs/2505.17929)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Intensive care unit (ICU) is a crucial hospital department that handles life-threatening cases. Nowadays machine learning (ML) is being leveraged in healthcare ubiquitously. In recent years, management of ICU became one of the most significant parts of the hospital functionality (largely but not only due to the worldwide COVID-19 pandemic). This study explores multiple ML approaches for predicting LOS in ICU specifically for the patients with neurological diseases based on the MIMIC-IV dataset. The evaluated models include classic ML algorithms (K-Nearest Neighbors, Random Forest, XGBoost and CatBoost) and Neural Networks (LSTM, BERT and Temporal Fusion Transformer). Given that LOS prediction is often framed as a classification task, this study categorizes LOS into three groups: less than two days, less than a week, and a week or more. As the first ML-based approach targeting LOS prediction for neurological disorder patients, this study does not aim to outperform existing methods but rather to assess their effectiveness in this specific context. The findings provide insights into the applicability of ML techniques for improving ICU resource management and patient care. According to the results, Random Forest model proved to outperform others on static, achieving an accuracy of 0.68, a precision of 0.68, a recall of 0.68, and F1-score of 0.67. While BERT model outperformed LSTM model on time-series data with an accuracy of 0.80, a precision of 0.80, a recall of 0.80 and F1-score 0.80.</li>
</ul>

<h3>Title: AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation of Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Xingjian Li, Qifeng Wu, Colleen Que, Yiran Ding, Adithya S. Ubaradka, Jianhua Xing, Tianyang Wang, Min Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17931">https://arxiv.org/abs/2505.17931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17931">https://arxiv.org/pdf/2505.17931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17931]] AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation of Foundation Models(https://arxiv.org/abs/2505.17931)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation is vital for clinical diagnosis, yet current deep learning methods often demand extensive expert effort, i.e., either through annotating large training datasets or providing prompts at inference time for each new case. This paper introduces a zero-shot and automatic segmentation pipeline that combines off-the-shelf vision-language and segmentation foundation models. Given a medical image and a task definition (e.g., "segment the optic disc in an eye fundus image"), our method uses a grounding model to generate an initial bounding box, followed by a visual prompt boosting module that enhance the prompts, which are then processed by a promptable segmentation model to produce the final mask. To address the challenges of domain gap and result verification, we introduce a test-time adaptation framework featuring a set of learnable adaptors that align the medical inputs with foundation model representations. Its hyperparameters are optimized via Bayesian Optimization, guided by a proxy validation model without requiring ground-truth labels. Our pipeline offers an annotation-efficient and scalable solution for zero-shot medical image segmentation across diverse tasks. Our pipeline is evaluated on seven diverse medical imaging datasets and shows promising results. By proper decomposition and test-time adaptation, our fully automatic pipeline performs competitively with weakly-prompted interactive foundation models.</li>
</ul>

<h3>Title: Understanding Gated Neurons in Transformers from Their Input-Output Functionality</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Gerstner, Hinrich Schütze</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17936">https://arxiv.org/abs/2505.17936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17936">https://arxiv.org/pdf/2505.17936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17936]] Understanding Gated Neurons in Transformers from Their Input-Output Functionality(https://arxiv.org/abs/2505.17936)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Interpretability researchers have attempted to understand MLP neurons of language models based on both the contexts in which they activate and their output weight vectors. They have paid little attention to a complementary aspect: the interactions between input and output. For example, when neurons detect a direction in the input, they might add much the same direction to the residual stream ("enrichment neurons") or reduce its presence ("depletion neurons"). We address this aspect by examining the cosine similarity between input and output weights of a neuron. We apply our method to 12 models and find that enrichment neurons dominate in early-middle layers whereas later layers tend more towards depletion. To explain this finding, we argue that enrichment neurons are largely responsible for enriching concept representations, one of the first steps of factual recall. Our input-output perspective is a complement to activation-dependent analyses and to approaches that treat input and output separately.</li>
</ul>

<h3>Title: Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL</h3>
<ul>
<li><strong>Authors: </strong>Che Liu, Haozhe Wang, Jiazhen Pan, Zhongwei Wan, Yong Dai, Fangzhen Lin, Wenjia Bai, Daniel Rueckert, Rossella Arcucci</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17952">https://arxiv.org/abs/2505.17952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17952">https://arxiv.org/pdf/2505.17952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17952]] Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL(https://arxiv.org/abs/2505.17952)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Improving performance on complex tasks and enabling interpretable decision making in large language models (LLMs), especially for clinical applications, requires effective reasoning. Yet this remains challenging without supervised fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from closed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the first medical LLM to show that reasoning capability can emerge purely through reinforcement learning (RL), using minimalist rule-based rewards on public multiple-choice QA datasets, without relying on SFT or distilled CoT data. AlphaMed achieves state-of-the-art results on six medical QA benchmarks, outperforming models trained with conventional SFT+RL pipelines. On challenging benchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source models such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the factors behind this success, we conduct a comprehensive data-centric analysis guided by three questions: (i) Can minimalist rule-based RL incentivize reasoning without distilled CoT supervision? (ii) How do dataset quantity and diversity impact reasoning? (iii) How does question difficulty shape the emergence and generalization of reasoning? Our findings show that dataset informativeness is a key driver of reasoning performance, and that minimalist RL on informative, multiple-choice QA data is effective at inducing reasoning without CoT supervision. We also observe divergent trends across benchmarks, underscoring limitations in current evaluation and the need for more challenging, reasoning-oriented medical QA benchmarks.</li>
</ul>

<h3>Title: Diffusion Classifiers Understand Compositionality, but Conditions Apply</h3>
<ul>
<li><strong>Authors: </strong>Yujin Jeong, Arnas Uselis, Seong Joon Oh, Anna Rohrbach</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17955">https://arxiv.org/abs/2505.17955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17955">https://arxiv.org/pdf/2505.17955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17955]] Diffusion Classifiers Understand Compositionality, but Conditions Apply(https://arxiv.org/abs/2505.17955)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Understanding visual scenes is fundamental to human intelligence. While discriminative models have significantly advanced computer vision, they often struggle with compositional understanding. In contrast, recent generative text-to-image diffusion models excel at synthesizing complex scenes, suggesting inherent compositional capabilities. Building on this, zero-shot diffusion classifiers have been proposed to repurpose diffusion models for discriminative tasks. While prior work offered promising results in discriminative compositional scenarios, these results remain preliminary due to a small number of benchmarks and a relatively shallow analysis of conditions under which the models succeed. To address this, we present a comprehensive study of the discriminative capabilities of diffusion classifiers on a wide range of compositional tasks. Specifically, our study covers three diffusion models (SD 1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks. Further, we shed light on the role that target dataset domains play in respective performance; to isolate the domain effects, we introduce a new diagnostic benchmark Self-Bench comprised of images created by diffusion models themselves. Finally, we explore the importance of timestep weighting and uncover a relationship between domain gap and timestep sensitivity, particularly for SD3-m. To sum up, diffusion classifiers understand compositionality, but conditions apply! Code and dataset are available at this https URL.</li>
</ul>

<h3>Title: SVD-Free Low-Rank Adaptive Gradient Optimization for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ionut-Vlad Modoranu, Mher Safaryan, Erik Schultheis, Dan Alistarh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17967">https://arxiv.org/abs/2505.17967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17967">https://arxiv.org/pdf/2505.17967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17967]] SVD-Free Low-Rank Adaptive Gradient Optimization for Large Language Models(https://arxiv.org/abs/2505.17967)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Low-rank optimization has emerged as a promising direction in training large language models (LLMs) to reduce the memory usage of adaptive optimizers by constraining learning to a lower-dimensional space. Prior work typically projects gradients of linear layers using approaches based on Singular Value Decomposition (SVD). However, applying SVD-based procedures individually to each layer in large models is computationally expensive and incurs additional memory costs due to storing the projection matrices. In this work, we propose a computationally efficient and conceptually simple two-step procedure to approximate SVD-based gradient projections into lower-dimensional spaces. First, we construct a complete orthogonal basis using predefined orthogonal matrices of the Discrete Cosine Transform (DCT). Second, we adaptively select basis columns based on their alignment with the gradient of each layer. Each projection matrix in our method is obtained via a single matrix multiplication followed by a lightweight sorting step to identify the most relevant basis vectors. Due to the predefined nature of the orthogonal bases, they are computed once at the start of training. During training, we store only the indices of the selected columns, avoiding the need to store full projection matrices for each layer. Our numerical experiments on both pre-training and fine-tuning tasks demonstrate the effectiveness of our dual strategy in approximating optimal low-rank projections, matching the performance of costly SVD-based methods while achieving faster runtime and reduced memory usage.</li>
</ul>

<h3>Title: Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Geng, Howard Chen, Dilip Arumugam, Thomas L. Griffiths</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17968">https://arxiv.org/abs/2505.17968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17968">https://arxiv.org/pdf/2505.17968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17968]] Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems(https://arxiv.org/abs/2505.17968)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Using AI to create autonomous researchers has the potential to accelerate scientific discovery. A prerequisite for this vision is understanding how well an AI model can identify the underlying structure of a black-box system from its behavior. In this paper, we explore how well a large language model (LLM) learns to identify a black-box function from passively observed versus actively collected data. We investigate the reverse-engineering capabilities of LLMs across three distinct types of black-box systems, each chosen to represent different problem domains where future autonomous AI researchers may have considerable impact: Program, Formal Language, and Math Equation. Through extensive experiments, we show that LLMs fail to extract information from observations, reaching a performance plateau that falls short of the ideal of Bayesian inference. However, we demonstrate that prompting LLMs to not only observe but also intervene -- actively querying the black-box with specific inputs to observe the resulting output -- improves performance by allowing LLMs to test edge cases and refine their beliefs. By providing the intervention data from one LLM to another, we show that this improvement is partly a result of engaging in the process of generating effective interventions, paralleling results in the literature on human learning. Further analysis reveals that engaging in intervention can help LLMs escape from two common failure modes: overcomplication, where the LLM falsely assumes prior knowledge about the black-box, and overlooking, where the LLM fails to incorporate observations. These insights provide practical guidance for helping LLMs more effectively reverse-engineer black-box systems, supporting their use in making new discoveries.</li>
</ul>

<h3>Title: MR-EEGWaveNet: Multiresolutional EEGWaveNet for Seizure Detection from Long EEG Recordings</h3>
<ul>
<li><strong>Authors: </strong>Kazi Mahmudul Hassan, Xuyang Zhao, Hidenori Sugano, Toshihisa Tanaka</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17972">https://arxiv.org/abs/2505.17972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17972">https://arxiv.org/pdf/2505.17972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17972]] MR-EEGWaveNet: Multiresolutional EEGWaveNet for Seizure Detection from Long EEG Recordings(https://arxiv.org/abs/2505.17972)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Feature engineering for generalized seizure detection models remains a significant challenge. Recently proposed models show variable performance depending on the training data and remain ineffective at accurately distinguishing artifacts from seizure data. In this study, we propose a novel end-to-end model, ''Multiresolutional EEGWaveNet (MR-EEGWaveNet),'' which efficiently distinguishes seizure events from background electroencephalogram (EEG) and artifacts/noise by capturing both temporal dependencies across different time frames and spatial relationships between channels. The model has three modules: convolution, feature extraction, and predictor. The convolution module extracts features through depth-wise and spatio-temporal convolution. The feature extraction module individually reduces the feature dimension extracted from EEG segments and their sub-segments. Subsequently, the extracted features are concatenated into a single vector for classification using a fully connected classifier called the predictor module. In addition, an anomaly score-based post-classification processing technique was introduced to reduce the false-positive rates of the model. Experimental results were reported and analyzed using different parameter settings and datasets (Siena (public) and Juntendo (private)). The proposed MR-EEGWaveNet significantly outperformed the conventional non-multiresolution approach, improving the F1 scores from 0.177 to 0.336 on Siena and 0.327 to 0.488 on Juntendo, with precision gains of 15.9% and 20.62%, respectively.</li>
</ul>

<h3>Title: To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile Mapping Cameras to Textured Semantic 3D Building Models</h3>
<ul>
<li><strong>Authors: </strong>Simone Gaisbauer, Prabin Gyawali, Qilin Zhang, Olaf Wysocki, Boris Jutzi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17973">https://arxiv.org/abs/2505.17973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17973">https://arxiv.org/pdf/2505.17973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17973]] To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile Mapping Cameras to Textured Semantic 3D Building Models(https://arxiv.org/abs/2505.17973)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Feature matching is a necessary step for many computer vision and photogrammetry applications such as image registration, structure-from-motion, and visual localization. Classical handcrafted methods such as SIFT feature detection and description combined with nearest neighbour matching and RANSAC outlier removal have been state-of-the-art for mobile mapping cameras. With recent advances in deep learning, learnable methods have been introduced and proven to have better robustness and performance under complex conditions. Despite their growing adoption, a comprehensive comparison between classical and learnable feature matching methods for the specific task of semantic 3D building camera-to-model matching is still missing. This submission systematically evaluates the effectiveness of different feature-matching techniques in visual localization using textured CityGML LoD2 models. We use standard benchmark datasets (HPatches, MegaDepth-1500) and custom datasets consisting of facade textures and corresponding camera images (terrestrial and drone). For the latter, we evaluate the achievable accuracy of the absolute pose estimated using a Perspective-n-Point (PnP) algorithm, with geometric ground truth derived from geo-referenced trajectory data. The results indicate that the learnable feature matching methods vastly outperform traditional approaches regarding accuracy and robustness on our challenging custom datasets with zero to 12 RANSAC-inliers and zero to 0.16 area under the curve. We believe that this work will foster the development of model-based visual localization methods. Link to the code: this https URL\_Glue\_or\_not\_to\_Glue</li>
</ul>

<h3>Title: Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher Approximation for Compressing Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Viktoriia Chekalina, Daniil Moskovskiy, Daria Cherniuk, Maxim Kurkin, Andrey Kuznetsov, Evgeny Frolov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17974">https://arxiv.org/abs/2505.17974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17974">https://arxiv.org/pdf/2505.17974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17974]] Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher Approximation for Compressing Large Language Models(https://arxiv.org/abs/2505.17974)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Fisher information is a fundamental concept for characterizing the sensitivity of parameters in neural networks. However, leveraging the full observed Fisher information is too expensive for large models, so most methods rely on simple diagonal approximations. While efficient, this approach ignores parameter correlations, often resulting in reduced performance on downstream tasks. In this work, we mitigate these limitations and propose Generalized Fisher-Weighted SVD (GFWSVD), a post-training LLM compression technique that accounts for both diagonal and off-diagonal elements of the Fisher information matrix, providing a more accurate reflection of parameter importance. To make the method tractable, we introduce a scalable adaptation of the Kronecker-factored approximation algorithm for the observed Fisher information. We demonstrate the effectiveness of our method on LLM compression, showing improvements over existing compression baselines. For example, at a 20 compression rate on the MMLU benchmark, our method outperforms FWSVD, which is based on a diagonal approximation of the Fisher information, by 5 percent, SVD-LLM by 3 percent, and ASVD by 6 percent compression rate.</li>
</ul>

<h3>Title: ADLGen: Synthesizing Symbolic, Event-Triggered Sensor Sequences for Human Activity Modeling</h3>
<ul>
<li><strong>Authors: </strong>Weihang You, Hanqi Jiang, Zishuai Liu, Zihang Xie, Tianming Liu, Jin Lu, Fei Dou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17987">https://arxiv.org/abs/2505.17987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17987">https://arxiv.org/pdf/2505.17987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17987]] ADLGen: Synthesizing Symbolic, Event-Triggered Sensor Sequences for Human Activity Modeling(https://arxiv.org/abs/2505.17987)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Real world collection of Activities of Daily Living data is challenging due to privacy concerns, costly deployment and labeling, and the inherent sparsity and imbalance of human behavior. We present ADLGen, a generative framework specifically designed to synthesize realistic, event triggered, and symbolic sensor sequences for ambient assistive environments. ADLGen integrates a decoder only Transformer with sign based symbolic temporal encoding, and a context and layout aware sampling mechanism to guide generation toward semantically rich and physically plausible sensor event sequences. To enhance semantic fidelity and correct structural inconsistencies, we further incorporate a large language model into an automatic generate evaluate refine loop, which verifies logical, behavioral, and temporal coherence and generates correction rules without manual intervention or environment specific tuning. Through comprehensive experiments with novel evaluation metrics, ADLGen is shown to outperform baseline generators in statistical fidelity, semantic richness, and downstream activity recognition, offering a scalable and privacy-preserving solution for ADL data synthesis.</li>
</ul>

<h3>Title: Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yutong Chen, Jiandong Gao, Ji Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17988">https://arxiv.org/abs/2505.17988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17988">https://arxiv.org/pdf/2505.17988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17988]] Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning(https://arxiv.org/abs/2505.17988)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>R1-style Reinforcement Learning (RL) significantly enhances Large Language Models' reasoning capabilities, yet the mechanism behind rule-based RL remains unclear. We found that small-scale SFT has significant influence on RL but shows poor efficiency. To explain our observations, we propose an analytical framework and compare the efficiency of SFT and RL by measuring sample effect. Hypothetical analysis show that SFT efficiency is limited by training data. Guided by our analysis, we propose Re-distillation, a technique that fine-tunes pretrain model through small-scale distillation from the RL-trained policy. Experiments on Knight & Knave and MATH datasets demonstrate re-distillation's surprising efficiency: re-distilled models match RL performance with far fewer samples and less computation. Empirical verification shows that sample effect is a good indicator of performance improvements. As a result, on K&K dataset, our re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT samples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches its instruct-tuned variant without RL. Our work explains several interesting phenomena in R1-style RL, shedding light on the mechanisms behind its empirical success. Code is available at: this https URL</li>
</ul>

<h3>Title: Outcome-based Reinforcement Learning to Predict the Future</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Turtel, Danny Franklin, Kris Skotheim, Luke Hewitt, Philipp Schoenegger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17989">https://arxiv.org/abs/2505.17989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17989">https://arxiv.org/pdf/2505.17989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17989]] Outcome-based Reinforcement Learning to Predict the Future(https://arxiv.org/abs/2505.17989)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning with verifiable rewards (RLVR) has boosted math and coding in large language models, yet there has been little effort to extend RLVR into messier, real-world domains like forecasting. One sticking point is that outcome-based reinforcement learning for forecasting must learn from binary, delayed, and noisy rewards, a regime where standard fine-tuning is brittle. We show that outcome-only online RL on a 14B model can match frontier-scale accuracy and surpass it in calibration and hypothetical prediction market betting by adapting two leading algorithms, Group-Relative Policy Optimisation (GRPO) and ReMax, to the forecasting setting. Our adaptations remove per-question variance scaling in GRPO, apply baseline-subtracted advantages in ReMax, hydrate training with 100k temporally consistent synthetic questions, and introduce lightweight guard-rails that penalise gibberish, non-English responses and missing rationales, enabling a single stable pass over 110k events. Scaling ReMax to 110k questions and ensembling seven predictions yields a 14B model that matches frontier baseline o1 on accuracy on our holdout set (Brier = 0.193, p = 0.23) while beating it in calibration (ECE = 0.042, p < 0.001). A simple trading rule turns this calibration edge into \$127 of hypothetical profit versus \$92 for o1 (p = 0.037). This demonstrates that refined RLVR methods can convert small-scale LLMs into potentially economically valuable forecasting tools, with implications for scaling this to larger models.</li>
</ul>

<h3>Title: Segment Anyword: Mask Prompt Inversion for Open-Set Grounded Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zhihua Liu, Amrutha Saseendran, Lei Tong, Xilin He, Fariba Yousefi, Nikolay Burlutskiy, Dino Oglic, Tom Diethe, Philip Teare, Huiyu Zhou, Chen Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17994">https://arxiv.org/abs/2505.17994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17994">https://arxiv.org/pdf/2505.17994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17994]] Segment Anyword: Mask Prompt Inversion for Open-Set Grounded Segmentation(https://arxiv.org/abs/2505.17994)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Open-set image segmentation poses a significant challenge because existing methods often demand extensive training or fine-tuning and generally struggle to segment unified objects consistently across diverse text reference expressions. Motivated by this, we propose Segment Anyword, a novel training-free visual concept prompt learning approach for open-set language grounded segmentation that relies on token-level cross-attention maps from a frozen diffusion model to produce segmentation surrogates or mask prompts, which are then refined into targeted object masks. Initial prompts typically lack coherence and consistency as the complexity of the image-text increases, resulting in suboptimal mask fragments. To tackle this issue, we further introduce a novel linguistic-guided visual prompt regularization that binds and clusters visual prompts based on sentence dependency and syntactic structural information, enabling the extraction of robust, noise-tolerant mask prompts, and significant improvements in segmentation accuracy. The proposed approach is effective, generalizes across different open-set segmentation tasks, and achieves state-of-the-art results of 52.5 (+6.8 relative) mIoU on Pascal Context 59, 67.73 (+25.73 relative) cIoU on gRefCOCO, and 67.4 (+1.1 relative to fine-tuned methods) mIoU on GranDf, which is the most complex open-set grounded segmentation task in the field.</li>
</ul>

<h3>Title: Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective</h3>
<ul>
<li><strong>Authors: </strong>Jintian Shao, Yiming Cheng, Hongyi Huang, Beiwen Zhang, Zhiyu Wu, You Shan, Mingkai Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17997">https://arxiv.org/abs/2505.17997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17997">https://arxiv.org/pdf/2505.17997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17997]] Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective(https://arxiv.org/abs/2505.17997)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The VAPO framework has demonstrated significant empirical success in enhancing the efficiency and reliability of reinforcement learning for long chain-of-thought (CoT) reasoning tasks with large language models (LLMs). By systematically addressing challenges such as value model bias, heterogeneous sequence lengths, and sparse reward signals, VAPO achieves state-of-the-art performance. While its practical benefits are evident, a deeper theoretical understanding of its underlying mechanisms and potential limitations is crucial for guiding future advancements. This paper aims to initiate such a discussion by exploring VAPO from a theoretical perspective, highlighting areas where its assumptions might be challenged and where further investigation could yield more robust and generalizable reasoning agents. We delve into the intricacies of value function approximation in complex reasoning spaces, the optimality of adaptive advantage estimation, the impact of token-level optimization, and the enduring challenges of exploration and generalization.</li>
</ul>

<h3>Title: TRACE for Tracking the Emergence of Semantic Representations in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Nura Aljaafari, Danilo S. Carvalho, André Freitas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.17998">https://arxiv.org/abs/2505.17998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.17998">https://arxiv.org/pdf/2505.17998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.17998]] TRACE for Tracking the Emergence of Semantic Representations in Transformers(https://arxiv.org/abs/2505.17998)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Modern transformer models exhibit phase transitions during training, distinct shifts from memorisation to abstraction, but the mechanisms underlying these transitions remain poorly understood. Prior work has often focused on endpoint representations or isolated signals like curvature or mutual information, typically in symbolic or arithmetic domains, overlooking the emergence of linguistic structure. We introduce TRACE (Tracking Representation Abstraction and Compositional Emergence), a diagnostic framework combining geometric, informational, and linguistic signals to detect phase transitions in Transformer-based LMs. TRACE leverages a frame-semantic data generation method, ABSynth, that produces annotated synthetic corpora with controllable complexity, lexical distributions, and structural entropy, while being fully annotated with linguistic categories, enabling precise analysis of abstraction emergence. Experiments reveal that (i) phase transitions align with clear intersections between curvature collapse and dimension stabilisation; (ii) these geometric shifts coincide with emerging syntactic and semantic accuracy; (iii) abstraction patterns persist across architectural variants, with components like feedforward networks affecting optimisation stability rather than fundamentally altering trajectories. This work advances our understanding of how linguistic abstractions emerge in LMs, offering insights into model interpretability, training efficiency, and compositional generalisation that could inform more principled approaches to LM development.</li>
</ul>

<h3>Title: Rethinking Contrastive Learning in Graph Anomaly Detection: A Clean-View Perspective</h3>
<ul>
<li><strong>Authors: </strong>Di Jin, Jingyi Cao, Xiaobao Wang, Bingdao Feng, Dongxiao He, Longbiao Wang, Jianwu Dang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18002">https://arxiv.org/abs/2505.18002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18002">https://arxiv.org/pdf/2505.18002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18002]] Rethinking Contrastive Learning in Graph Anomaly Detection: A Clean-View Perspective(https://arxiv.org/abs/2505.18002)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Graph anomaly detection aims to identify unusual patterns in graph-based data, with wide applications in fields such as web security and financial fraud detection. Existing methods typically rely on contrastive learning, assuming that a lower similarity between a node and its local subgraph indicates abnormality. However, these approaches overlook a crucial limitation: the presence of interfering edges invalidates this assumption, since it introduces disruptive noise that compromises the contrastive learning process. Consequently, this limitation impairs the ability to effectively learn meaningful representations of normal patterns, leading to suboptimal detection performance. To address this issue, we propose a Clean-View Enhanced Graph Anomaly Detection framework (CVGAD), which includes a multi-scale anomaly awareness module to identify key sources of interference in the contrastive learning process. Moreover, to mitigate bias from the one-step edge removal process, we introduce a novel progressive purification module. This module incrementally refines the graph by iteratively identifying and removing interfering edges, thereby enhancing model performance. Extensive experiments on five benchmark datasets validate the effectiveness of our approach.</li>
</ul>

<h3>Title: Training with Pseudo-Code for Instruction Following</h3>
<ul>
<li><strong>Authors: </strong>Prince Kumar, Rudra Murthy, Riyaz Bhat, Danish Contractor</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18011">https://arxiv.org/abs/2505.18011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18011">https://arxiv.org/pdf/2505.18011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18011]] Training with Pseudo-Code for Instruction Following(https://arxiv.org/abs/2505.18011)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite the rapid progress in the capabilities of Large Language Models (LLMs), they continue to have difficulty following relatively simple, unambiguous instructions, especially when compositions are involved. In this paper, we take inspiration from recent work that suggests that models may follow instructions better when they are expressed in pseudo-code. However, writing pseudo-code programs can be tedious and using few-shot demonstrations to craft code representations for use in inference can be unnatural for non-expert users of LLMs. To overcome these limitations, we propose fine-tuning LLMs with instruction-tuning data that additionally includes instructions re-expressed in pseudo-code along with the final response. We evaluate models trained using our method on $11$ publicly available benchmarks comprising of tasks related to instruction-following, mathematics, and common-sense reasoning. We conduct rigorous experiments with $5$ different models and find that not only do models follow instructions better when trained with pseudo-code, they also retain their capabilities on the other tasks related to mathematical and common sense reasoning. Specifically, we observe a relative gain of $3$--$19$% on instruction-following benchmark, and an average gain of upto 14% across all tasks.</li>
</ul>

<h3>Title: SemSegBench & DetecBench: Benchmarking Reliability and Generalization Beyond Classification</h3>
<ul>
<li><strong>Authors: </strong>Shashank Agnihotri, David Schader, Jonas Jakubassa, Nico Sharei, Simon Kral, Mehmet Ege Kaçar, Ruben Weber, Margret Keuper</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18015">https://arxiv.org/abs/2505.18015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18015">https://arxiv.org/pdf/2505.18015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18015]] SemSegBench & DetecBench: Benchmarking Reliability and Generalization Beyond Classification(https://arxiv.org/abs/2505.18015)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, segmentation</a></li>
<li><strong>Abstract: </strong>Reliability and generalization in deep learning are predominantly studied in the context of image classification. Yet, real-world applications in safety-critical domains involve a broader set of semantic tasks, such as semantic segmentation and object detection, which come with a diverse set of dedicated model architectures. To facilitate research towards robust model design in segmentation and detection, our primary objective is to provide benchmarking tools regarding robustness to distribution shifts and adversarial manipulations. We propose the benchmarking tools SEMSEGBENCH and DETECBENCH, along with the most extensive evaluation to date on the reliability and generalization of semantic segmentation and object detection models. In particular, we benchmark 76 segmentation models across four datasets and 61 object detectors across two datasets, evaluating their performance under diverse adversarial attacks and common corruptions. Our findings reveal systematic weaknesses in state-of-the-art models and uncover key trends based on architecture, backbone, and model capacity. SEMSEGBENCH and DETECBENCH are open-sourced in our GitHub repository (this https URL) along with our complete set of total 6139 evaluations. We anticipate the collected data to foster and encourage future research towards improved model reliability beyond classification.</li>
</ul>

<h3>Title: Strictly Constrained Generative Modeling via Split Augmented Langevin Sampling</h3>
<ul>
<li><strong>Authors: </strong>Matthieu Blanke, Yongquan Qu, Sara Shamekh, Pierre Gentine</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18017">https://arxiv.org/abs/2505.18017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18017">https://arxiv.org/pdf/2505.18017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18017]] Strictly Constrained Generative Modeling via Split Augmented Langevin Sampling(https://arxiv.org/abs/2505.18017)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Deep generative models hold great promise for representing complex physical systems, but their deployment is currently limited by the lack of guarantees on the physical plausibility of the generated outputs. Ensuring that known physical constraints are enforced is therefore critical when applying generative models to scientific and engineering problems. We address this limitation by developing a principled framework for sampling from a target distribution while rigorously satisfying physical constraints. Leveraging the variational formulation of Langevin dynamics, we propose Split Augmented Langevin (SAL), a novel primal-dual sampling algorithm that enforces constraints progressively through variable splitting, with convergence guarantees. While the method is developed theoretically for Langevin dynamics, we demonstrate its effective applicability to diffusion models. In particular, we use constrained diffusion models to generate physical fields satisfying energy and mass conservation laws. We apply our method to diffusion-based data assimilation on a complex physical system, where enforcing physical constraints substantially improves both forecast accuracy and the preservation of critical conserved quantities. We also demonstrate the potential of SAL for challenging feasibility problems in optimal control.</li>
</ul>

<h3>Title: RemoteSAM: Towards Segment Anything for Earth Observation</h3>
<ul>
<li><strong>Authors: </strong>Liang Yao, Fan Liu, Delong Chen, Chuanyi Zhang, Yijun Wang, Ziyun Chen, Wei Xu, Shimin Di, Yuhui Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18022">https://arxiv.org/abs/2505.18022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18022">https://arxiv.org/pdf/2505.18022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18022]] RemoteSAM: Towards Segment Anything for Earth Observation(https://arxiv.org/abs/2505.18022)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>We aim to develop a robust yet flexible visual foundation model for Earth observation. It should possess strong capabilities in recognizing and localizing diverse visual targets while providing compatibility with various input-output interfaces required across different task scenarios. Current systems cannot meet these requirements, as they typically utilize task-specific architecture trained on narrow data domains with limited semantic coverage. Our study addresses these limitations from two aspects: data and modeling. We first introduce an automatic data engine that enjoys significantly better scalability compared to previous human annotation or rule-based approaches. It has enabled us to create the largest dataset of its kind to date, comprising 270K image-text-mask triplets covering an unprecedented range of diverse semantic categories and attribute specifications. Based on this data foundation, we further propose a task unification paradigm that centers around referring expression segmentation. It effectively handles a wide range of vision-centric perception tasks, including classification, detection, segmentation, grounding, etc, using a single model without any task-specific heads. Combining these innovations on data and modeling, we present RemoteSAM, a foundation model that establishes new SoTA on several earth observation perception benchmarks, outperforming other foundation models such as Falcon, GeoChat, and LHRS-Bot with significantly higher efficiency. Models and data are publicly available at this https URL.</li>
</ul>

<h3>Title: 3D Face Reconstruction Error Decomposed: A Modular Benchmark for Fair and Fast Method Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Evangelos Sariyanidi, Claudio Ferrari, Federico Nocentini, Stefano Berretti, Andrea Cavallaro, Birkan Tunc</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18025">https://arxiv.org/abs/2505.18025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18025">https://arxiv.org/pdf/2505.18025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18025]] 3D Face Reconstruction Error Decomposed: A Modular Benchmark for Fair and Fast Method Evaluation(https://arxiv.org/abs/2505.18025)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Computing the standard benchmark metric for 3D face reconstruction, namely geometric error, requires a number of steps, such as mesh cropping, rigid alignment, or point correspondence. Current benchmark tools are monolithic (they implement a specific combination of these steps), even though there is no consensus on the best way to measure error. We present a toolkit for a Modularized 3D Face reconstruction Benchmark (M3DFB), where the fundamental components of error computation are segregated and interchangeable, allowing one to quantify the effect of each. Furthermore, we propose a new component, namely correction, and present a computationally efficient approach that penalizes for mesh topology inconsistency. Using this toolkit, we test 16 error estimators with 10 reconstruction methods on two real and two synthetic datasets. Critically, the widely used ICP-based estimator provides the worst benchmarking performance, as it significantly alters the true ranking of the top-5 reconstruction methods. Notably, the correlation of ICP with the true error can be as low as 0.41. Moreover, non-rigid alignment leads to significant improvement (correlation larger than 0.90), highlighting the importance of annotating 3D landmarks on datasets. Finally, the proposed correction scheme, together with non-rigid warping, leads to an accuracy on a par with the best non-rigid ICP-based estimators, but runs an order of magnitude faster. Our open-source codebase is designed for researchers to easily compare alternatives for each component, thus helping accelerating progress in benchmarking for 3D face reconstruction and, furthermore, supporting the improvement of learned reconstruction methods, which depend on accurate error estimation for effective training.</li>
</ul>

<h3>Title: CAMME: Adaptive Deepfake Image Detection with Multi-Modal Cross-Attention</h3>
<ul>
<li><strong>Authors: </strong>Naseem Khan, Tuan Nguyen, Amine Bermak, Issa Khalil</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18035">https://arxiv.org/abs/2505.18035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18035">https://arxiv.org/pdf/2505.18035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18035]] CAMME: Adaptive Deepfake Image Detection with Multi-Modal Cross-Attention(https://arxiv.org/abs/2505.18035)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, generative</a></li>
<li><strong>Abstract: </strong>The proliferation of sophisticated AI-generated deepfakes poses critical challenges for digital media authentication and societal security. While existing detection methods perform well within specific generative domains, they exhibit significant performance degradation when applied to manipulations produced by unseen architectures--a fundamental limitation as generative technologies rapidly evolve. We propose CAMME (Cross-Attention Multi-Modal Embeddings), a framework that dynamically integrates visual, textual, and frequency-domain features through a multi-head cross-attention mechanism to establish robust cross-domain generalization. Extensive experiments demonstrate CAMME's superiority over state-of-the-art methods, yielding improvements of 12.56% on natural scenes and 13.25% on facial deepfakes. The framework demonstrates exceptional resilience, maintaining (over 91%) accuracy under natural image perturbations and achieving 89.01% and 96.14% accuracy against PGD and FGSM adversarial attacks, respectively. Our findings validate that integrating complementary modalities through cross-attention enables more effective decision boundary realignment for reliable deepfake detection across heterogeneous generative architectures.</li>
</ul>

<h3>Title: Contrastive Distillation of Emotion Knowledge from LLMs for Zero-Shot Emotion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Minxue Niu, Emily Mower Provost</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18040">https://arxiv.org/abs/2505.18040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18040">https://arxiv.org/pdf/2505.18040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18040]] Contrastive Distillation of Emotion Knowledge from LLMs for Zero-Shot Emotion Recognition(https://arxiv.org/abs/2505.18040)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The ability to handle various emotion labels without dedicated training is crucial for building adaptable Emotion Recognition (ER) systems. Conventional ER models rely on training using fixed label sets and struggle to generalize beyond them. On the other hand, Large Language Models (LLMs) have shown strong zero-shot ER performance across diverse label spaces, but their scale limits their use on edge devices. In this work, we propose a contrastive distillation framework that transfers rich emotional knowledge from LLMs into a compact model without the use of human annotations. We use GPT-4 to generate descriptive emotion annotations, offering rich supervision beyond fixed label sets. By aligning text samples with emotion descriptors in a shared embedding space, our method enables zero-shot prediction on different emotion classes, granularity, and label schema. The distilled model is effective across multiple datasets and label spaces, outperforming strong baselines of similar size and approaching GPT-4's zero-shot performance, while being over 10,000 times smaller.</li>
</ul>

<h3>Title: Improved Algorithms for Overlapping and Robust Clustering of Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach</h3>
<ul>
<li><strong>Authors: </strong>Changyeol Lee, Yongho Shin, Hyung-Chan An</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18043">https://arxiv.org/abs/2505.18043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18043">https://arxiv.org/pdf/2505.18043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18043]] Improved Algorithms for Overlapping and Robust Clustering of Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach(https://arxiv.org/abs/2505.18043)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Clustering is a fundamental task in both machine learning and data mining. Among various methods, edge-colored clustering (ECC) has emerged as a useful approach for handling categorical data. Given a hypergraph with (hyper)edges labeled by colors, ECC aims to assign vertex colors to minimize the number of edges where the vertex color differs from the edge's color. However, traditional ECC has inherent limitations, as it enforces a nonoverlapping and exhaustive clustering. To tackle these limitations, three versions of ECC have been studied: Local ECC and Global ECC, which allow overlapping clusters, and Robust ECC, which accounts for vertex outliers. For these problems, both linear programming (LP) rounding algorithms and greedy combinatorial algorithms have been proposed. While these LP-rounding algorithms provide high-quality solutions, they demand substantial computation time; the greedy algorithms, on the other hand, run very fast but often compromise solution quality. In this paper, we present an algorithmic framework that combines the strengths of LP with the computational efficiency of combinatorial algorithms. Both experimental and theoretical analyses show that our algorithms efficiently produce high-quality solutions for all three problems: Local, Global, and Robust ECC. We complement our algorithmic contributions with complexity-theoretic inapproximability results and integrality gap bounds, which suggest that significant theoretical improvements are unlikely. Our results also answer two open questions previously raised in the literature.</li>
</ul>

<h3>Title: Linear Mixture Distributionally Robust Markov Decision Processes</h3>
<ul>
<li><strong>Authors: </strong>Zhishuai Liu, Pan Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18044">https://arxiv.org/abs/2505.18044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18044">https://arxiv.org/pdf/2505.18044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18044]] Linear Mixture Distributionally Robust Markov Decision Processes(https://arxiv.org/abs/2505.18044)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Many real-world decision-making problems face the off-dynamics challenge: the agent learns a policy in a source domain and deploys it in a target domain with different state transitions. The distributionally robust Markov decision process (DRMDP) addresses this challenge by finding a robust policy that performs well under the worst-case environment within a pre-specified uncertainty set of transition dynamics. Its effectiveness heavily hinges on the proper design of these uncertainty sets, based on prior knowledge of the dynamics. In this work, we propose a novel linear mixture DRMDP framework, where the nominal dynamics is assumed to be a linear mixture model. In contrast with existing uncertainty sets directly defined as a ball centered around the nominal kernel, linear mixture DRMDPs define the uncertainty sets based on a ball around the mixture weighting parameter. We show that this new framework provides a more refined representation of uncertainties compared to conventional models based on $(s,a)$-rectangularity and $d$-rectangularity, when prior knowledge about the mixture model is present. We propose a meta algorithm for robust policy learning in linear mixture DRMDPs with general $f$-divergence defined uncertainty sets, and analyze its sample complexities under three divergence metrics instantiations: total variation, Kullback-Leibler, and $\chi^2$ divergences. These results establish the statistical learnability of linear mixture DRMDPs, laying the theoretical foundation for future research on this new setting.</li>
</ul>

<h3>Title: Learning with Restricted Boltzmann Machines: Asymptotics of AMP and GD in High Dimensions</h3>
<ul>
<li><strong>Authors: </strong>Yizhou Xu, Florent Krzakala, Lenka Zdeborová</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.dis-nn, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18046">https://arxiv.org/abs/2505.18046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18046">https://arxiv.org/pdf/2505.18046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18046]] Learning with Restricted Boltzmann Machines: Asymptotics of AMP and GD in High Dimensions(https://arxiv.org/abs/2505.18046)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The Restricted Boltzmann Machine (RBM) is one of the simplest generative neural networks capable of learning input distributions. Despite its simplicity, the analysis of its performance in learning from the training data is only well understood in cases that essentially reduce to singular value decomposition of the data. Here, we consider the limit of a large dimension of the input space and a constant number of hidden units. In this limit, we simplify the standard RBM training objective into a form that is equivalent to the multi-index model with non-separable regularization. This opens a path to analyze training of the RBM using methods that are established for multi-index models, such as Approximate Message Passing (AMP) and its state evolution, and the analysis of Gradient Descent (GD) via the dynamical mean-field theory. We then give rigorous asymptotics of the training dynamics of RBM on data generated by the spiked covariance model as a prototype of a structure suitable for unsupervised learning. We show in particular that RBM reaches the optimal computational weak recovery threshold, aligning with the BBP transition, in the spiked covariance model.</li>
</ul>

<h3>Title: RestoreVAR: Visual Autoregressive Generation for All-in-One Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Sudarshan Rajagopalan, Kartik Narayan, Vishal M. Patel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18047">https://arxiv.org/abs/2505.18047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18047">https://arxiv.org/pdf/2505.18047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18047]] RestoreVAR: Visual Autoregressive Generation for All-in-One Image Restoration(https://arxiv.org/abs/2505.18047)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>The use of latent diffusion models (LDMs) such as Stable Diffusion has significantly improved the perceptual quality of All-in-One image Restoration (AiOR) methods, while also enhancing their generalization capabilities. However, these LDM-based frameworks suffer from slow inference due to their iterative denoising process, rendering them impractical for time-sensitive applications. To address this, we propose RestoreVAR, a novel generative approach for AiOR that significantly outperforms LDM-based models in restoration performance while achieving over $\mathbf{10\times}$ faster inference. RestoreVAR leverages visual autoregressive modeling (VAR), a recently introduced approach which performs scale-space autoregression for image generation. VAR achieves comparable performance to that of state-of-the-art diffusion transformers with drastically reduced computational costs. To optimally exploit these advantages of VAR for AiOR, we propose architectural modifications and improvements, including intricately designed cross-attention mechanisms and a latent-space refinement module, tailored for the AiOR task. Extensive experiments show that RestoreVAR achieves state-of-the-art performance among generative AiOR methods, while also exhibiting strong generalization capabilities.</li>
</ul>

<h3>Title: SHARDeg: A Benchmark for Skeletal Human Action Recognition in Degraded Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Simon Malzard, Nitish Mital, Richard Walters, Victoria Nockles, Raghuveer Rao, Celso M. De Melo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18048">https://arxiv.org/abs/2505.18048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18048">https://arxiv.org/pdf/2505.18048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18048]] SHARDeg: A Benchmark for Skeletal Human Action Recognition in Degraded Scenarios(https://arxiv.org/abs/2505.18048)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Computer vision (CV) models for detection, prediction or classification tasks operate on video data-streams that are often degraded in the real world, due to deployment in real-time or on resource-constrained hardware. It is therefore critical that these models are robust to degraded data, but state of the art (SoTA) models are often insufficiently assessed with these real-world constraints in mind. This is exemplified by Skeletal Human Action Recognition (SHAR), which is critical in many CV pipelines operating in real-time and at the edge, but robustness to degraded data has previously only been shallowly and inconsistently assessed. Here we address this issue for SHAR by providing an important first data degradation benchmark on the most detailed and largest 3D open dataset, NTU-RGB+D-120, and assess the robustness of five leading SHAR models to three forms of degradation that represent real-world issues. We demonstrate the need for this benchmark by showing that the form of degradation, which has not previously been considered, has a large impact on model accuracy; at the same effective frame rate, model accuracy can vary by >40% depending on degradation type. We also identify that temporal regularity of frames in degraded SHAR data is likely a major driver of differences in model performance, and harness this to improve performance of existing models by up to >40%, through employing a simple mitigation approach based on interpolation. Finally, we highlight how our benchmark has helped identify an important degradation-resistant SHAR model based in Rough Path Theory; the LogSigRNN SHAR model outperforms the SoTA DeGCN model in five out of six cases at low frame rates by an average accuracy of 6%, despite trailing the SoTA model by 11-12% on un-degraded data at high frame rates (30 FPS).</li>
</ul>

<h3>Title: SpikeGen: Generative Framework for Visual Spike Stream Processing</h3>
<ul>
<li><strong>Authors: </strong>Gaole Dai, Menghang Dong, Rongyu Zhang, Ruichuan An, Shanghang Zhang, Tiejun Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18049">https://arxiv.org/abs/2505.18049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18049">https://arxiv.org/pdf/2505.18049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18049]] SpikeGen: Generative Framework for Visual Spike Stream Processing(https://arxiv.org/abs/2505.18049)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Neuromorphic Visual Systems, such as spike cameras, have attracted considerable attention due to their ability to capture clear textures under dynamic conditions. This capability effectively mitigates issues related to motion and aperture blur. However, in contrast to conventional RGB modalities that provide dense spatial information, these systems generate binary, spatially sparse frames as a trade-off for temporally rich visual streams. In this context, generative models emerge as a promising solution to address the inherent limitations of sparse data. These models not only facilitate the conditional fusion of existing information from both spike and RGB modalities but also enable the conditional generation based on latent priors. In this study, we introduce a robust generative processing framework named SpikeGen, designed for visual spike streams captured by spike cameras. We evaluate this framework across multiple tasks involving mixed spike-RGB modalities, including conditional image/video deblurring, dense frame reconstruction from spike streams, and high-speed scene novel-view synthesis. Supported by comprehensive experimental results, we demonstrate that leveraging the latent space operation abilities of generative models allows us to effectively address the sparsity of spatial information while fully exploiting the temporal richness of spike streams, thereby promoting a synergistic enhancement of different visual modalities.</li>
</ul>

<h3>Title: LookWhere? Efficient Visual Recognition by Learning Where to Look and What to See from Self-Supervision</h3>
<ul>
<li><strong>Authors: </strong>Anthony Fuller, Yousef Yassin, Junfeng Wen, Daniel G. Kyrollos, Tarek Ibrahim, James R. Green, Evan Shelhamer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18051">https://arxiv.org/abs/2505.18051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18051">https://arxiv.org/pdf/2505.18051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18051]] LookWhere? Efficient Visual Recognition by Learning Where to Look and What to See from Self-Supervision(https://arxiv.org/abs/2505.18051)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Vision transformers are ever larger, more accurate, and more expensive to compute. The expense is even more extreme at high resolution as the number of tokens grows quadratically with the image size. We turn to adaptive computation to cope with this cost by learning to predict where to compute. Our LookWhere method divides the computation between a low-resolution selector and a high-resolution extractor without ever processing the full high-resolution input. We jointly pretrain the selector and extractor without task supervision by distillation from a self-supervised teacher, in effect, learning where and what to compute simultaneously. Unlike prior token reduction methods, which pay to save by pruning already-computed tokens, and prior token selection methods, which require complex and expensive per-task optimization, LookWhere economically and accurately selects and extracts transferrable representations of images. We show that LookWhere excels at sparse recognition on high-resolution inputs (Traffic Signs), maintaining accuracy while reducing FLOPs by up to 34x and time by 6x. It also excels at standard recognition tasks that are global (ImageNet classification) or local (ADE20K segmentation), improving accuracy while reducing time by 1.36x.</li>
</ul>

<h3>Title: BOTM: Echocardiography Segmentation via Bi-directional Optimal Token Matching</h3>
<ul>
<li><strong>Authors: </strong>Zhihua Liu, Lei Tong, Xilin He, Che Liu, Rossella Arcucci, Chen Jin, Huiyu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18052">https://arxiv.org/abs/2505.18052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18052">https://arxiv.org/pdf/2505.18052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18052]] BOTM: Echocardiography Segmentation via Bi-directional Optimal Token Matching(https://arxiv.org/abs/2505.18052)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Existed echocardiography segmentation methods often suffer from anatomical inconsistency challenge caused by shape variation, partial observation and region ambiguity with similar intensity across 2D echocardiographic sequences, resulting in false positive segmentation with anatomical defeated structures in challenging low signal-to-noise ratio conditions. To provide a strong anatomical guarantee across different echocardiographic frames, we propose a novel segmentation framework named BOTM (Bi-directional Optimal Token Matching) that performs echocardiography segmentation and optimal anatomy transportation simultaneously. Given paired echocardiographic images, BOTM learns to match two sets of discrete image tokens by finding optimal correspondences from a novel anatomical transportation perspective. We further extend the token matching into a bi-directional cross-transport attention proxy to regulate the preserved anatomical consistency within the cardiac cyclic deformation in temporal domain. Extensive experimental results show that BOTM can generate stable and accurate segmentation outcomes (e.g. -1.917 HD on CAMUS2H LV, +1.9% Dice on TED), and provide a better matching interpretation with anatomical consistency guarantee.</li>
</ul>

<h3>Title: FDBPL: Faster Distillation-Based Prompt Learning for Region-Aware Vision-Language Models Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Zherui Zhang, Jiaxin Wu, Changwei Wang, Rongtao Xu, Longzhao Huang, Wenhao Xu, Wenbo Xu, Li Guo, Shibiao Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18053">https://arxiv.org/abs/2505.18053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18053">https://arxiv.org/pdf/2505.18053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18053]] FDBPL: Faster Distillation-Based Prompt Learning for Region-Aware Vision-Language Models Adaptation(https://arxiv.org/abs/2505.18053)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Prompt learning as a parameter-efficient method that has been widely adopted to adapt Vision-Language Models (VLMs) to downstream tasks. While hard-prompt design requires domain expertise and iterative optimization, soft-prompt methods rely heavily on task-specific hard labels, limiting their generalization to unseen categories. Recent popular distillation-based prompt learning methods improve generalization by exploiting larger teacher VLMs and unsupervised knowledge transfer, yet their repetitive teacher model online inference sacrifices the inherent training efficiency advantage of prompt learning. In this paper, we propose {\large {\textbf{F}}}aster {\large {\textbf{D}}}istillation-{\large {\textbf{B}}}ased {\large {\textbf{P}}}rompt {\large {\textbf{L}}}earning (\textbf{FDBPL}), which addresses these issues by sharing soft supervision contexts across multiple training stages and implementing accelerated I/O. Furthermore, FDBPL introduces a region-aware prompt learning paradigm with dual positive-negative prompt spaces to fully exploit randomly cropped regions that containing multi-level information. We propose a positive-negative space mutual learning mechanism based on similarity-difference learning, enabling student CLIP models to recognize correct semantics while learning to reject weakly related concepts, thereby improving zero-shot performance. Unlike existing distillation-based prompt learning methods that sacrifice parameter efficiency for generalization, FDBPL maintains dual advantages of parameter efficiency and strong downstream generalization. Comprehensive evaluations across 11 datasets demonstrate superior performance in base-to-new generalization, cross-dataset transfer, and robustness tests, achieving $2.2\times$ faster training speed.</li>
</ul>

<h3>Title: MathEDU: Towards Adaptive Feedback for Student Mathematical Problem-Solving</h3>
<ul>
<li><strong>Authors: </strong>Wei-Ling Hsu, Yu-Chien Tang, An-Zi Yen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18056">https://arxiv.org/abs/2505.18056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18056">https://arxiv.org/pdf/2505.18056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18056]] MathEDU: Towards Adaptive Feedback for Student Mathematical Problem-Solving(https://arxiv.org/abs/2505.18056)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Online learning enhances educational accessibility, offering students the flexibility to learn anytime, anywhere. However, a key limitation is the lack of immediate, personalized feedback, particularly in helping students correct errors in math problem-solving. Several studies have investigated the applications of large language models (LLMs) in educational contexts. In this paper, we explore the capabilities of LLMs to assess students' math problem-solving processes and provide adaptive feedback. The MathEDU dataset is introduced, comprising authentic student solutions annotated with teacher feedback. We evaluate the model's ability to support personalized learning in two scenarios: one where the model has access to students' prior answer histories, and another simulating a cold-start context. Experimental results show that the fine-tuned model performs well in identifying correctness. However, the model still faces challenges in generating detailed feedback for pedagogical purposes.</li>
</ul>

<h3>Title: Reward Model Generalization for Compute-Aware Test-Time Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zeen Song, Wenwen Qiang, Siyu Zhao, Changwen Zheng, Gang Hua</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18065">https://arxiv.org/abs/2505.18065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18065">https://arxiv.org/pdf/2505.18065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18065]] Reward Model Generalization for Compute-Aware Test-Time Reasoning(https://arxiv.org/abs/2505.18065)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>External test-time reasoning enhances large language models (LLMs) by decoupling generation and selection. At inference time, the model generates multiple reasoning paths, and an auxiliary process reward model (PRM) is used to score and select the best one. A central challenge in this setting is test-time compute optimality (TCO), i.e., how to maximize answer accuracy under a fixed inference budget. In this work, we establish a theoretical framework to analyze how the generalization error of the PRM affects compute efficiency and reasoning performance. Leveraging PAC-Bayes theory, we derive generalization bounds and show that a lower generalization error of PRM leads to fewer samples required to find correct answers. Motivated by this analysis, we propose Compute-Aware Tree Search (CATS), an actor-critic framework that dynamically controls search behavior. The actor outputs sampling hyperparameters based on reward distributions and sparsity statistics, while the critic estimates their utility to guide budget allocation. Experiments on the MATH and AIME benchmarks with various LLMs and PRMs demonstrate that CATS consistently outperforms other external TTS methods, validating our theoretical predictions.</li>
</ul>

<h3>Title: Emergence of Hebbian Dynamics in Regularized Non-Local Learners</h3>
<ul>
<li><strong>Authors: </strong>David Koplow, Tomaso Poggio, Liu Ziyin</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18069">https://arxiv.org/abs/2505.18069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18069">https://arxiv.org/pdf/2505.18069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18069]] Emergence of Hebbian Dynamics in Regularized Non-Local Learners(https://arxiv.org/abs/2505.18069)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Stochastic Gradient Descent (SGD) has emerged as a remarkably effective learning algorithm, underpinning nearly all state-of-the-art machine learning models, from large language models to autonomous vehicles. Despite its practical success, SGD appears fundamentally distinct from biological learning mechanisms. It is widely believed that the biological brain can not implement gradient descent because it is nonlocal, and we have found little (if any) experimental evidence for it. In contrast, the brain is widely thought to learn via local Hebbian learning principles, which have been seen as incompatible with gradient descent. In this paper, we establish a theoretical and empirical connection between the learning signals of neural networks trained using SGD with weight decay and those trained with Hebbian learning near convergence. We show that SGD with regularization can appear to learn according to a Hebbian rule, and SGD with injected noise according to an anti-Hebbian rule. We also provide empirical evidence that Hebbian learning properties can emerge in a network with weight decay from virtually any learning rule--even random ones. These results may bridge a long-standing gap between artificial and biological learning, revealing Hebbian properties as an epiphenomenon of deeper optimization principles and cautioning against interpreting their presence in neural data as evidence against more complex hetero-synaptic mechanisms.</li>
</ul>

<h3>Title: Extended Inductive Reasoning for Personalized Preference Inference from Behavioral Signals</h3>
<ul>
<li><strong>Authors: </strong>Jia-Nan Li, Jian Guan, Wei Wu, Rui Yan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18071">https://arxiv.org/abs/2505.18071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18071">https://arxiv.org/pdf/2505.18071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18071]] Extended Inductive Reasoning for Personalized Preference Inference from Behavioral Signals(https://arxiv.org/abs/2505.18071)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated significant success in complex reasoning tasks such as math and coding. In contrast to these tasks where deductive reasoning predominates, inductive reasoning\textemdash the ability to derive general rules from incomplete evidence, remains underexplored. This paper investigates extended inductive reasoning in LLMs through the lens of personalized preference inference, a critical challenge in LLM alignment where current approaches struggle to capture diverse user preferences. The task demands strong inductive reasoning capabilities as user preferences are typically embedded implicitly across various interaction forms, requiring models to synthesize consistent preference patterns from scattered signals. We propose \textsc{AlignXplore}, a model that leverages extended reasoning chains to enable systematic preference inference from behavioral signals in users' interaction histories. We develop \textsc{AlignXplore} by combining cold-start training based on synthetic data with subsequent online reinforcement learning. Through extensive experiments, we demonstrate that \textsc{AlignXplore} achieves substantial improvements over the backbone model by an average of 11.05\% on in-domain and out-of-domain benchmarks, while maintaining strong generalization ability across different input formats and downstream models. Further analyses establish best practices for preference inference learning through systematic comparison of reward modeling strategies, while revealing the emergence of human-like inductive reasoning patterns during training.</li>
</ul>

<h3>Title: DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Junhao Chen, Mingjin Chen, Jianjin Xu, Xiang Li, Junting Dong, Mingze Sun, Puhua Jiang, Hongxiang Li, Yuhang Yang, Hao Zhao, Xiaoxiao Long, Ruqi Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18078">https://arxiv.org/abs/2505.18078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18078">https://arxiv.org/pdf/2505.18078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18078]] DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation(https://arxiv.org/abs/2505.18078)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Controllable video generation (CVG) has advanced rapidly, yet current systems falter when more than one actor must move, interact, and exchange positions under noisy control signals. We address this gap with DanceTogether, the first end-to-end diffusion framework that turns a single reference image plus independent pose-mask streams into long, photorealistic videos while strictly preserving every identity. A novel MaskPoseAdapter binds "who" and "how" at every denoising step by fusing robust tracking masks with semantically rich-but noisy-pose heat-maps, eliminating the identity drift and appearance bleeding that plague frame-wise pipelines. To train and evaluate at scale, we introduce (i) PairFS-4K, 26 hours of dual-skater footage with 7,000+ distinct IDs, (ii) HumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain transfer, and (iii) TogetherVideoBench, a three-track benchmark centered on the DanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure skating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a significant margin. Moreover, we show that a one-hour fine-tune yields convincing human-robot videos, underscoring broad generalization to embodied-AI and HRI tasks. Extensive ablations confirm that persistent identity-action binding is critical to these gains. Together, our model, datasets, and benchmark lift CVG from single-subject choreography to compositionally controllable, multi-actor interaction, opening new avenues for digital production, simulation, and embodied intelligence. Our video demos and code are available at this https URL.</li>
</ul>

<h3>Title: Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, Yan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18079">https://arxiv.org/abs/2505.18079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18079">https://arxiv.org/pdf/2505.18079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18079]] Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding(https://arxiv.org/abs/2505.18079)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. While Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. To overcome such limitations, we propose the Deep Video Discovery agent to leverage an agentic search strategy over segmented video clips. Different from previous video agents manually designing a rigid workflow, our approach emphasizes the autonomous nature of agents. By providing a set of search-centric tools on multi-granular video database, our DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools, formulates appropriate parameters for actions, and iteratively refines its internal reasoning in light of the gathered information. We perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates the advantage of the entire system design. Our DVD agent achieves SOTA performance, significantly surpassing prior works by a large margin on the challenging LVBench dataset. Comprehensive ablation studies and in-depth tool analyses are also provided, yielding insights to further advance intelligent agents tailored for long-form video understanding tasks. The code will be released later.</li>
</ul>

<h3>Title: AFD-STA: Adaptive Filtering Denoising with Spatiotemporal Attention for Chaotic System Prediction</h3>
<ul>
<li><strong>Authors: </strong>Chunlin Gong, Yin Wang, Jingru Li, Hanleran Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18080">https://arxiv.org/abs/2505.18080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18080">https://arxiv.org/pdf/2505.18080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18080]] AFD-STA: Adaptive Filtering Denoising with Spatiotemporal Attention for Chaotic System Prediction(https://arxiv.org/abs/2505.18080)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents AFD-STA Net, a neural framework integrating adaptive filtering and spatiotemporal dynamics learning for predicting high-dimensional chaotic systems governed by partial differential equations. The architecture combines: 1) An adaptive exponential smoothing module with position-aware decay coefficients for robust attractor reconstruction, 2) Parallel attention mechanisms capturing cross-temporal and spatial dependencies, 3) Dynamic gated fusion of multiscale features, and 4) Deep projection networks with dimension-scaling capabilities. Numerical experiments on nonlinear PDE systems demonstrate the model's effectiveness in maintaining prediction accuracy under both smooth and strongly chaotic regimes while exhibiting noise tolerance through adaptive filtering. Component ablation studies confirm critical contributions from each module, particularly highlighting the essential role of spatiotemporal attention in learning complex dynamical interactions. The framework shows promising potential for real-world applications requiring simultaneous handling of measurement uncertainties and high-dimensional nonlinear dynamics.</li>
</ul>

<h3>Title: An Iterative Framework for Generative Backmapping of Coarse Grained Proteins</h3>
<ul>
<li><strong>Authors: </strong>Georgios Kementzidis, Erin Wong, John Nicholson, Ruichen Xu, Yuefan Deng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18082">https://arxiv.org/abs/2505.18082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18082">https://arxiv.org/pdf/2505.18082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18082]] An Iterative Framework for Generative Backmapping of Coarse Grained Proteins(https://arxiv.org/abs/2505.18082)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The techniques of data-driven backmapping from coarse-grained (CG) to fine-grained (FG) representation often struggle with accuracy, unstable training, and physical realism, especially when applied to complex systems such as proteins. In this work, we introduce a novel iterative framework by using conditional Variational Autoencoders and graph-based neural networks, specifically designed to tackle the challenges associated with such large-scale biomolecules. Our method enables stepwise refinement from CG beads to full atomistic details. We outline the theory of iterative generative backmapping and demonstrate via numerical experiments the advantages of multistep schemes by applying them to proteins of vastly different structures with very coarse representations. This multistep approach not only improves the accuracy of reconstructions but also makes the training process more computationally efficient for proteins with ultra-CG representations.</li>
</ul>

<h3>Title: What Do You Need for Diverse Trajectory Stitching in Diffusion Planning?</h3>
<ul>
<li><strong>Authors: </strong>Quentin Clark, Florian Shkurti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18083">https://arxiv.org/abs/2505.18083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18083">https://arxiv.org/pdf/2505.18083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18083]] What Do You Need for Diverse Trajectory Stitching in Diffusion Planning?(https://arxiv.org/abs/2505.18083)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In planning, stitching is an ability of algorithms to piece together sub-trajectories of data they are trained on to generate new and diverse behaviours. While stitching is historically a strength of offline reinforcement learning, recent generative behavioural cloning (BC) methods have also shown proficiency at stitching. However, the main factors behind this are poorly understood, hindering the development of new algorithms that can reliably stitch. Focusing on diffusion planners trained via BC, we find two properties are needed to compose: \emph{positional equivariance} and \emph{local receptiveness}. We use these two properties to explain architecture, data, and inference choices in existing generative BC methods based on diffusion planning, including replanning frequency, data augmentation, and data scaling. Experimental comparisions show that (1) while locality is more important than positional equivariance in creating a diffusion planner capable of composition, both are crucial (2) enabling these properties through relatively simple architecture choices can be competitive with more computationally expensive methods such as replanning or scaling data, and (3) simple inpainting-based guidance can guide architecturally compositional models to enable generalization in goal-conditioned settings.</li>
</ul>

<h3>Title: Early-Exit Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Andrea Giuseppe Di Francesco, Maria Sofia Bucarelli, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Fabrizio Silvestri</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18088">https://arxiv.org/abs/2505.18088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18088">https://arxiv.org/pdf/2505.18088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18088]] Early-Exit Graph Neural Networks(https://arxiv.org/abs/2505.18088)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Early-exit mechanisms allow deep neural networks to halt inference as soon as classification confidence is high enough, adaptively trading depth for confidence, and thereby cutting latency and energy on easy inputs while retaining full-depth accuracy for harder ones. Similarly, adding early exit mechanisms to Graph Neural Networks (GNNs), the go-to models for graph-structured data, allows for dynamic trading depth for confidence on simple graphs while maintaining full-depth accuracy on harder and more complex graphs to capture intricate relationships. Although early exits have proven effective across various deep learning domains, their potential within GNNs in scenarios that require deep architectures while resisting over-smoothing and over-squashing remains largely unexplored. We unlock that potential by first introducing Symmetric-Anti-Symmetric Graph Neural Networks (SAS-GNN), whose symmetry-based inductive biases mitigate these issues and yield stable intermediate representations that can be useful to allow early exiting in GNNs. Building on this backbone, we present Early-Exit Graph Neural Networks (EEGNNs), which append confidence-aware exit heads that allow on-the-fly termination of propagation based on each node or the entire graph. Experiments show that EEGNNs preserve robust performance as depth grows and deliver competitive accuracy on heterophilic and long-range benchmarks, matching attention-based and asynchronous message-passing models while substantially reducing computation and latency. We plan to release the code to reproduce our experiments.</li>
</ul>

<h3>Title: Data Mixing Can Induce Phase Transitions in Knowledge Acquisition</h3>
<ul>
<li><strong>Authors: </strong>Xinran Gu, Kaifeng Lyu, Jiazheng Li, Jingzhao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18091">https://arxiv.org/abs/2505.18091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18091">https://arxiv.org/pdf/2505.18091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18091]] Data Mixing Can Induce Phase Transitions in Knowledge Acquisition(https://arxiv.org/abs/2505.18091)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are typically trained on data mixtures: most data come from web scrapes, while a small portion is curated from high-quality sources with dense domain-specific knowledge. In this paper, we show that when training LLMs on such data mixtures, knowledge acquisition from knowledge-dense datasets, unlike training exclusively on knowledge-dense data (arXiv:2404.05405), does not always follow a smooth scaling law but can exhibit phase transitions with respect to the mixing ratio and model size. Through controlled experiments on a synthetic biography dataset mixed with web-scraped data, we demonstrate that: (1) as we increase the model size to a critical value, the model suddenly transitions from memorizing very few to most of the biographies; (2) below a critical mixing ratio, the model memorizes almost nothing even with extensive training, but beyond this threshold, it rapidly memorizes more biographies. We attribute these phase transitions to a capacity allocation phenomenon: a model with bounded capacity must act like a knapsack problem solver to minimize the overall test loss, and the optimal allocation across datasets can change discontinuously as the model size or mixing ratio varies. We formalize this intuition in an information-theoretic framework and reveal that these phase transitions are predictable, with the critical mixing ratio following a power-law relationship with the model size. Our findings highlight a concrete case where a good mixing recipe for large models may not be optimal for small models, and vice versa.</li>
</ul>

<h3>Title: QwenLong-CPRS: Towards $\infty$-LLMs with Dynamic Context Optimization</h3>
<ul>
<li><strong>Authors: </strong>Weizhou Shen, Chenliang Li, Fanqi Wan, Shengyi Liao, Shaopeng Lai, Bo Zhang, Yingcheng Shi, Yuning Wu, Gang Fu, Zhansheng Li, Bin Yang, Ji Zhang, Fei Huang, Jingren Zhou, Ming Yan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18092">https://arxiv.org/abs/2505.18092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18092">https://arxiv.org/pdf/2505.18092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18092]] QwenLong-CPRS: Towards $\infty$-LLMs with Dynamic Context Optimization(https://arxiv.org/abs/2505.18092)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This technical report presents QwenLong-CPRS, a context compression framework designed for explicit long-context optimization, addressing prohibitive computation overhead during the prefill stage and the "lost in the middle" performance degradation of large language models (LLMs) during long sequence processing. Implemented through a novel dynamic context optimization mechanism, QwenLong-CPRS enables multi-granularity context compression guided by natural language instructions, achieving both efficiency gains and improved performance. Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key innovations: (1) Natural language-guided dynamic optimization, (2) Bidirectional reasoning layers for enhanced boundary awareness, (3) Token critic mechanisms with language modeling heads, and (4) Window-parallel inference. Comprehensive evaluations across five benchmarks (4K-2M word contexts) demonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority over other context management methods like RAG and sparse attention in both accuracy and efficiency. (2) Architecture-agnostic integration with all flagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3, and Qwen2.5-max, achieves 21.59$\times$ context compression alongside 19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct, QwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on Ruler-128K and InfiniteBench, establishing new SOTA performance.</li>
</ul>

<h3>Title: Towards more transferable adversarial attack in black-box manner</h3>
<ul>
<li><strong>Authors: </strong>Chun Tong Lei, Zhongliang Guo, Hon Chung Lee, Minh Quoc Duong, Chun Pong Lau</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18097">https://arxiv.org/abs/2505.18097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18097">https://arxiv.org/pdf/2505.18097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18097]] Towards more transferable adversarial attack in black-box manner(https://arxiv.org/abs/2505.18097)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Adversarial attacks have become a well-explored domain, frequently serving as evaluation baselines for model robustness. Among these, black-box attacks based on transferability have received significant attention due to their practical applicability in real-world scenarios. Traditional black-box methods have generally focused on improving the optimization framework (e.g., utilizing momentum in MI-FGSM) to enhance transferability, rather than examining the dependency on surrogate white-box model architectures. Recent state-of-the-art approach DiffPGD has demonstrated enhanced transferability by employing diffusion-based adversarial purification models for adaptive attacks. The inductive bias of diffusion-based adversarial purification aligns naturally with the adversarial attack process, where both involving noise addition, reducing dependency on surrogate white-box model selection. However, the denoising process of diffusion models incurs substantial computational costs through chain rule derivation, manifested in excessive VRAM consumption and extended runtime. This progression prompts us to question whether introducing diffusion models is necessary. We hypothesize that a model sharing similar inductive bias to diffusion-based adversarial purification, combined with an appropriate loss function, could achieve comparable or superior transferability while dramatically reducing computational overhead. In this paper, we propose a novel loss function coupled with a unique surrogate model to validate our hypothesis. Our approach leverages the score of the time-dependent classifier from classifier-guided diffusion models, effectively incorporating natural data distribution knowledge into the adversarial optimization process. Experimental results demonstrate significantly improved transferability across diverse model architectures while maintaining robustness against diffusion-based defenses.</li>
</ul>

<h3>Title: Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL</h3>
<ul>
<li><strong>Authors: </strong>Joey Hong, Anca Dragan, Sergey Levine</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18098">https://arxiv.org/abs/2505.18098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18098">https://arxiv.org/pdf/2505.18098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18098]] Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL(https://arxiv.org/abs/2505.18098)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel in tasks like question answering and dialogue, but complex tasks requiring interaction, such as negotiation and persuasion, require additional long-horizon reasoning and planning. Reinforcement learning (RL) fine-tuning can enable such planning in principle, but suffers from drawbacks that hinder scalability. In particular, multi-turn RL training incurs high memory and computational costs, which are exacerbated when training LLMs as policies. Furthermore, the largest LLMs do not expose the APIs necessary to be trained in such manner. As a result, modern methods to improve the reasoning of LLMs rely on sophisticated prompting mechanisms rather than RL fine-tuning. To remedy this, we propose a novel approach that uses goal-conditioned value functions to guide the reasoning of LLM agents, that scales even to large API-based models. These value functions predict how a task will unfold given an action, allowing the LLM agent to evaluate multiple possible outcomes, both positive and negative, to plan effectively. In addition, these value functions are trained over reasoning steps rather than full actions, to be a concise and light-weight module that facilitates decision-making in multi-turn interactions. We validate our method on tasks requiring interaction, including tool use, social deduction, and dialogue, demonstrating superior performance over both RL fine-tuning and prompting methods while maintaining efficiency and scalability.</li>
</ul>

<h3>Title: How Can I Publish My LLM Benchmark Without Giving the True Answers Away?</h3>
<ul>
<li><strong>Authors: </strong>Takashi Ishida, Thanawat Lodkaew, Ikko Yamane</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18102">https://arxiv.org/abs/2505.18102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18102">https://arxiv.org/pdf/2505.18102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18102]] How Can I Publish My LLM Benchmark Without Giving the True Answers Away?(https://arxiv.org/abs/2505.18102)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Publishing a large language model (LLM) benchmark on the Internet risks contaminating future LLMs: the benchmark may be unintentionally (or intentionally) used to train or select a model. A common mitigation is to keep the benchmark private and let participants submit their models or predictions to the organizers. However, this strategy will require trust in a single organization and still permits test-set overfitting through repeated queries. To overcome this issue, we propose a way to publish benchmarks without completely disclosing the ground-truth answers to the questions, while still maintaining the ability to openly evaluate LLMs. Our main idea is to inject randomness to the answers by preparing several logically correct answers, and only include one of them as the solution in the benchmark. This reduces the best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is this helpful to keep us from disclosing the ground truth, but this approach also offers a test for detecting data contamination. In principle, even fully capable models should not surpass the Bayes accuracy. If a model surpasses this ceiling despite this expectation, this is a strong signal of data contamination. We present experimental evidence that our method can detect data contamination accurately on a wide range of benchmarks, models, and training methodologies.</li>
</ul>

<h3>Title: ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework</h3>
<ul>
<li><strong>Authors: </strong>Lisheng Huang, Yichen Liu, Jinhao Jiang, Rongxiang Zhang, Jiahao Yan, Junyi Li, Wayne Xin Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18105">https://arxiv.org/abs/2505.18105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18105">https://arxiv.org/pdf/2505.18105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18105]] ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework(https://arxiv.org/abs/2505.18105)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in web-augmented large language models (LLMs) have exhibited strong performance in complex reasoning tasks, yet these capabilities are mostly locked in proprietary systems with opaque architectures. In this work, we propose \textbf{ManuSearch}, a transparent and modular multi-agent framework designed to democratize deep search for LLMs. ManuSearch decomposes the search and reasoning process into three collaborative agents: (1) a solution planning agent that iteratively formulates sub-queries, (2) an Internet search agent that retrieves relevant documents via real-time web search, and (3) a structured webpage reading agent that extracts key evidence from raw web content. To rigorously evaluate deep reasoning abilities, we introduce \textbf{ORION}, a challenging benchmark focused on open-web reasoning over long-tail entities, covering both English and Chinese. Experimental results show that ManuSearch substantially outperforms prior open-source baselines and even surpasses leading closed-source systems. Our work paves the way for reproducible, extensible research in open deep search systems. We release the data and code in this https URL</li>
</ul>

<h3>Title: F-ANcGAN: An Attention-Enhanced Cycle Consistent Generative Adversarial Architecture for Synthetic Image Generation of Nanoparticles</h3>
<ul>
<li><strong>Authors: </strong>Varun Ajith, Anindya Pal, Saumik Bhattacharya, Sayantari Ghosh</a></li>
<li><strong>Subjects: </strong>cs.CV, cond-mat.mtrl-sci, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18106">https://arxiv.org/abs/2505.18106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18106">https://arxiv.org/pdf/2505.18106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18106]] F-ANcGAN: An Attention-Enhanced Cycle Consistent Generative Adversarial Architecture for Synthetic Image Generation of Nanoparticles(https://arxiv.org/abs/2505.18106)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Nanomaterial research is becoming a vital area for energy, medicine, and materials science, and accurate analysis of the nanoparticle topology is essential to determine their properties. Unfortunately, the lack of high-quality annotated datasets drastically hinders the creation of strong segmentation models for nanoscale imaging. To alleviate this problem, we introduce F-ANcGAN, an attention-enhanced cycle consistent generative adversarial system that can be trained using a limited number of data samples and generates realistic scanning electron microscopy (SEM) images directly from segmentation maps. Our model uses a Style U-Net generator and a U-Net segmentation network equipped with self-attention to capture structural relationships and applies augmentation methods to increase the variety of the dataset. The architecture reached a raw FID score of 17.65 for TiO$_2$ dataset generation, with a further reduction in FID score to nearly 10.39 by using efficient post-processing techniques. By facilitating scalable high-fidelity synthetic dataset generation, our approach can improve the effectiveness of downstream segmentation task training, overcoming severe data shortage issues in nanoparticle analysis, thus extending its applications to resource-limited fields.</li>
</ul>

<h3>Title: Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM</h3>
<ul>
<li><strong>Authors: </strong>Zinuo Li, Xian Zhang, Yongxin Guo, Mohammed Bennamoun, Farid Boussaid, Girish Dwivedi, Luqi Gong, Qiuhong Ke</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18110">https://arxiv.org/abs/2505.18110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18110">https://arxiv.org/pdf/2505.18110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18110]] Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM(https://arxiv.org/abs/2505.18110)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Humans naturally understand moments in a video by integrating visual and auditory cues. For example, localizing a scene in the video like "A scientist passionately speaks on wildlife conservation as dramatic orchestral music plays, with the audience nodding and applauding" requires simultaneous processing of visual, audio, and speech signals. However, existing models often struggle to effectively fuse and interpret audio information, limiting their capacity for comprehensive video temporal understanding. To address this, we present TriSense, a triple-modality large language model designed for holistic video temporal understanding through the integration of visual, audio, and speech modalities. Central to TriSense is a Query-Based Connector that adaptively reweights modality contributions based on the input query, enabling robust performance under modality dropout and allowing flexible combinations of available inputs. To support TriSense's multimodal capabilities, we introduce TriSense-2M, a high-quality dataset of over 2 million curated samples generated via an automated pipeline powered by fine-tuned LLMs. TriSense-2M includes long-form videos and diverse modality combinations, facilitating broad generalization. Extensive experiments across multiple benchmarks demonstrate the effectiveness of TriSense and its potential to advance multimodal video analysis. Code and dataset will be publicly released.</li>
</ul>

<h3>Title: UNJOIN: Enhancing Multi-Table Text-to-SQL Generation via Schema Simplification</h3>
<ul>
<li><strong>Authors: </strong>Poojah Ganesan, Rajat Aayush Jha, Dan Roth, Vivek Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18122">https://arxiv.org/abs/2505.18122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18122">https://arxiv.org/pdf/2505.18122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18122]] UNJOIN: Enhancing Multi-Table Text-to-SQL Generation via Schema Simplification(https://arxiv.org/abs/2505.18122)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have greatly improved Text-to-SQL performance for single-table queries. But, it remains challenging in multi-table databases due to complex schema and relational operations. Existing methods often struggle with retrieving the right tables and columns, generating accurate JOINs and UNIONs, and generalizing across diverse schemas. To address these issues, we introduce UNJOIN, a two-stage framework that decouples the retrieval of schema elements from SQL logic generation. In the first stage, we merge the column names of all tables in the database into a single-table representation by prefixing each column with its table name. This allows the model to focus purely on accurate retrieval without being distracted by the need to write complex SQL logic. In the second stage, the SQL query is generated on this simplified schema and mapped back to the original schema by reconstructing JOINs, UNIONs, and relational logic. Evaluations on SPIDER and BIRD datasets show that UNJOIN matches or exceeds the state-of-the-art baselines. UNJOIN uses only schema information, which does not require data access or fine-tuning, making it scalable and adaptable across databases.</li>
</ul>

<h3>Title: Reward Model Overoptimisation in Iterated RLHF</h3>
<ul>
<li><strong>Authors: </strong>Lorenz Wolf, Robert Kirk, Mirco Musolesi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18126">https://arxiv.org/abs/2505.18126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18126">https://arxiv.org/pdf/2505.18126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18126]] Reward Model Overoptimisation in Iterated RLHF(https://arxiv.org/abs/2505.18126)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) is a widely used method for aligning large language models with human preferences. However, RLHF often suffers from reward model overoptimisation, in which models overfit to the reward function, resulting in non-generalisable policies that exploit the idiosyncrasies and peculiarities of the reward function. A common mitigation is iterated RLHF, in which reward models are repeatedly retrained with updated human feedback and policies are re-optimised. Despite its increasing adoption, the dynamics of overoptimisation in this setting remain poorly understood. In this work, we present the first comprehensive study of overoptimisation in iterated RLHF. We systematically analyse key design choices - how reward model training data is transferred across iterations, which reward function is used for optimisation, and how policies are initialised. Using the controlled AlpacaFarm benchmark, we observe that overoptimisation tends to decrease over successive iterations, as reward models increasingly approximate ground-truth preferences. However, performance gains diminish over time, and while reinitialising from the base policy is robust, it limits optimisation flexibility. Other initialisation strategies often fail to recover from early overoptimisation. These findings offer actionable insights for building more stable and generalisable RLHF pipelines.</li>
</ul>

<h3>Title: TokBench: Evaluating Your Visual Tokenizer before Visual Generation</h3>
<ul>
<li><strong>Authors: </strong>Junfeng Wu, Dongliang Luo, Weizhi Zhao, Zhihao Xie, Yuanhao Wang, Junyi Li, Xudong Xie, Yuliang Liu, Xiang Bai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18142">https://arxiv.org/abs/2505.18142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18142">https://arxiv.org/pdf/2505.18142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18142]] TokBench: Evaluating Your Visual Tokenizer before Visual Generation(https://arxiv.org/abs/2505.18142)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this work, we reveal the limitations of visual tokenizers and VAEs in preserving fine-grained features, and propose a benchmark to evaluate reconstruction performance for two challenging visual contents: text and face. Image tokenization has significantly advanced visual generation and multimodal modeling, particularly with autoregressive models due to the modeling simplicity of discrete tokens. Autoregressive models typically rely on image tokenizers to compress images into discrete tokens for sequential prediction, whereas diffusion models often operate on continuous latent space to reduce computational costs. However, both visual compression approaches inevitably lose visual information, thereby limiting the upper bound of visual generation quality. To evaluate how these compression losses affect text and faces, the most human-sensitive visual elements, we first collect and curate a collection of text and faces images from existing datasets, ensuring clarity and diversity. For text reconstruction, we employ OCR models to assess the recognition accuracy of the reconstructed text, and then we measure feature similarity between original and reconstructed faces thereby quantifying faces reconstruction fidelity. Our method is highly lightweight, requiring just 2GB memory and 4 minutes to complete evaluations. With our benchmark, we analyze the reconstruction quality of text and faces at various scales across different image tokenizers and VAEs. Our results demonstrate that modern visual tokenizers still struggle to preserve fine-grained features, particularly at smaller scales. Furthermore, we extend this evaluation framework to the video, conducting a comprehensive analysis of video tokenizers. Additionally, we find that traditional metrics fail to accurately reflect the reconstruction performance for faces and text, while our proposed metrics serve as an effective complement.</li>
</ul>

<h3>Title: Lost in the Haystack: Smaller Needles are More Difficult for LLMs to Find</h3>
<ul>
<li><strong>Authors: </strong>Owen Bianchi, Mathew J. Koretsky, Maya Willey, Chelsea X. Alvarado, Tanay Nayak, Adi Asija, Nicole Kuznetsov, Mike A. Nalls, Faraz Faghri, Daniel Khashabi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18148">https://arxiv.org/abs/2505.18148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18148">https://arxiv.org/pdf/2505.18148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18148]] Lost in the Haystack: Smaller Needles are More Difficult for LLMs to Find(https://arxiv.org/abs/2505.18148)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) face significant challenges with needle-in-a-haystack tasks, where relevant information ("the needle") must be drawn from a large pool of irrelevant context ("the haystack"). Previous studies have highlighted positional bias and distractor quantity as critical factors affecting model performance, yet the influence of gold context size has received little attention. We address this gap by systematically studying how variations in gold context length impact LLM performance on long-context question answering tasks. Our experiments reveal that LLM performance drops sharply when the gold context is shorter, i.e., smaller gold contexts consistently degrade model performance and amplify positional sensitivity, posing a major challenge for agentic systems that must integrate scattered, fine-grained information of varying lengths. This pattern holds across three diverse domains (general knowledge, biomedical reasoning, and mathematical reasoning) and seven state-of-the-art LLMs of various sizes and architectures. Our work provides clear insights to guide the design of robust, context-aware LLM-driven systems.</li>
</ul>

<h3>Title: First Finish Search: Efficient Test-Time Scaling in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aradhye Agarwal, Ayan Sengupta, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18149">https://arxiv.org/abs/2505.18149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18149">https://arxiv.org/pdf/2505.18149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18149]] First Finish Search: Efficient Test-Time Scaling in Large Language Models(https://arxiv.org/abs/2505.18149)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Test-time scaling (TTS), which involves dynamic allocation of compute during inference, offers a promising way to improve reasoning in large language models. While existing TTS methods work well, they often rely on long decoding paths or require a large number of samples to be generated, increasing the token usage and inference latency. We observe the surprising fact that for reasoning tasks, shorter traces are much more likely to be correct than longer ones. Motivated by this, we introduce First Finish Search (FFS), a training-free parallel decoding strategy that launches $n$ independent samples and returns as soon as any one completes. We evaluate FFS alongside simple decoding, beam search, majority voting, and budget forcing on four reasoning models (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and across four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With DeepSeek-R1, FFS achieves $82.23\%$ accuracy on the AIME datasets, a $15\%$ improvement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's o4-mini performance. Our theoretical analysis explains why stopping at the shortest trace is likely to yield a correct answer and identifies the conditions under which early stopping may be suboptimal. The elegance and simplicity of FFS demonstrate that straightforward TTS strategies can perform remarkably well, revealing the untapped potential of simple approaches at inference time.</li>
</ul>

<h3>Title: Generative Distribution Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Nic Fishman, Gokul Gowri, Peng Yin, Jonathan Gootenberg, Omar Abudayyeh</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18150">https://arxiv.org/abs/2505.18150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18150">https://arxiv.org/pdf/2505.18150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18150]] Generative Distribution Embeddings(https://arxiv.org/abs/2505.18150)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Many real-world problems require reasoning across multiple scales, demanding models which operate not on single data points, but on entire distributions. We introduce generative distribution embeddings (GDE), a framework that lifts autoencoders to the space of distributions. In GDEs, an encoder acts on sets of samples, and the decoder is replaced by a generator which aims to match the input distribution. This framework enables learning representations of distributions by coupling conditional generative models with encoder networks which satisfy a criterion we call distributional invariance. We show that GDEs learn predictive sufficient statistics embedded in the Wasserstein space, such that latent GDE distances approximately recover the $W_2$ distance, and latent interpolation approximately recovers optimal transport trajectories for Gaussian and Gaussian mixture distributions. We systematically benchmark GDEs against existing approaches on synthetic datasets, demonstrating consistently stronger performance. We then apply GDEs to six key problems in computational biology: learning representations of cell populations from lineage-tracing data (150K cells), predicting perturbation effects on single-cell transcriptomes (1M cells), predicting perturbation effects on cellular phenotypes (20M single-cell images), modeling tissue-specific DNA methylation patterns (253M sequences), designing synthetic yeast promoters (34M sequences), and spatiotemporal modeling of viral protein sequences (1M sequences).</li>
</ul>

<h3>Title: Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry Understanding in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Wafa Alghallabi, Ritesh Thawkar, Sara Ghaboura, Ketan More, Omkar Thawakar, Hisham Cholakkal, Salman Khan, Rao Muhammad Anwer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18152">https://arxiv.org/abs/2505.18152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18152">https://arxiv.org/pdf/2505.18152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18152]] Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry Understanding in LLMs(https://arxiv.org/abs/2505.18152)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Arabic poetry stands as one of the most sophisticated and culturally embedded forms of expression in the Arabic language, known for its layered meanings, stylistic diversity, and deep historical continuity. Although large language models (LLMs) have demonstrated strong performance across languages and tasks, their ability to understand Arabic poetry remains largely unexplored. In this work, we introduce `Fann or Flop`, the first benchmark designed to assess the comprehension of Arabic poetry by LLMs in twelve historical eras, covering 21 core poetic genres and a variety of metrical forms, from classical structures to contemporary free verse. The benchmark comprises a curated corpus of poems with explanations that assess semantic understanding, metaphor interpretation, prosodic awareness, and cultural context. We argue that poetic comprehension offers a strong indicator for testing how good the LLM is in understanding classical Arabic through the Arabic poetry. Unlike surface-level tasks, this domain demands deeper interpretive reasoning and cultural sensitivity. Our evaluation of state-of-the-art LLMs shows that most models struggle with poetic understanding despite strong results on standard Arabic benchmarks. We release `Fann or Flop` along with the evaluation suite as an open-source resource to enable rigorous evaluation and advancement for Arabic language models. Code is available at: this https URL.</li>
</ul>

<h3>Title: REN: Fast and Efficient Region Encodings from Patch-Based Image Encoders</h3>
<ul>
<li><strong>Authors: </strong>Savya Khosla, Sethuraman TV, Barnett Lee, Alexander Schwing, Derek Hoiem</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18153">https://arxiv.org/abs/2505.18153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18153">https://arxiv.org/pdf/2505.18153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18153]] REN: Fast and Efficient Region Encodings from Patch-Based Image Encoders(https://arxiv.org/abs/2505.18153)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We introduce the Region Encoder Network (REN), a fast and effective model for generating region-based image representations using point prompts. Recent methods combine class-agnostic segmenters (e.g., SAM) with patch-based image encoders (e.g., DINO) to produce compact and effective region representations, but they suffer from high computational cost due to the segmentation step. REN bypasses this bottleneck using a lightweight module that directly generates region tokens, enabling 60x faster token generation with 35x less memory, while also improving token quality. It uses a few cross-attention blocks that take point prompts as queries and features from a patch-based image encoder as keys and values to produce region tokens that correspond to the prompted objects. We train REN with three popular encoders-DINO, DINOv2, and OpenCLIP-and show that it can be extended to other encoders without dedicated training. We evaluate REN on semantic segmentation and retrieval tasks, where it consistently outperforms the original encoders in both performance and compactness, and matches or exceeds SAM-based region methods while being significantly faster. Notably, REN achieves state-of-the-art results on the challenging Ego4D VQ2D benchmark and outperforms proprietary LMMs on Visual Haystacks' single-needle challenge. Code and models are available at: this https URL.</li>
</ul>

<h3>Title: The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step Induction to Complex Moral Dilemmas</h3>
<ul>
<li><strong>Authors: </strong>Ya Wu, Qiang Sheng, Danding Wang, Guang Yang, Yifan Sun, Zhengjia Wang, Yuyan Bu, Juan Cao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.18154">https://arxiv.org/abs/2505.18154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.18154">https://arxiv.org/pdf/2505.18154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.18154]] The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step Induction to Complex Moral Dilemmas(https://arxiv.org/abs/2505.18154)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Ethical decision-making is a critical aspect of human judgment, and the growing use of LLMs in decision-support systems necessitates a rigorous evaluation of their moral reasoning capabilities. However, existing assessments primarily rely on single-step evaluations, failing to capture how models adapt to evolving ethical challenges. Addressing this gap, we introduce the Multi-step Moral Dilemmas (MMDs), the first dataset specifically constructed to evaluate the evolving moral judgments of LLMs across 3,302 five-stage dilemmas. This framework enables a fine-grained, dynamic analysis of how LLMs adjust their moral reasoning across escalating dilemmas. Our evaluation of nine widely used LLMs reveals that their value preferences shift significantly as dilemmas progress, indicating that models recalibrate moral judgments based on scenario complexity. Furthermore, pairwise value comparisons demonstrate that while LLMs often prioritize the value of care, this value can sometimes be superseded by fairness in certain contexts, highlighting the dynamic and context-dependent nature of LLM ethical reasoning. Our findings call for a shift toward dynamic, context-aware evaluation paradigms, paving the way for more human-aligned and value-sensitive development of LLMs.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
