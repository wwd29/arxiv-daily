<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: TrojanPuzzle: Covertly Poisoning Code-Suggestion Models. (arXiv:2301.02344v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.02344">http://arxiv.org/abs/2301.02344</a></li>
<li>Code URL: <a href="https://github.com/microsoft/codegenerationpoisoning">https://github.com/microsoft/codegenerationpoisoning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2301.02344] TrojanPuzzle: Covertly Poisoning Code-Suggestion Models](http://arxiv.org/abs/2301.02344) #secure</code></li>
<li>Summary: <p>With tools like GitHub Copilot, automatic code suggestion is no longer a
dream in software engineering. These tools, based on large language models, are
typically trained on massive corpora of code mined from unvetted public
sources. As a result, these models are susceptible to data poisoning attacks
where an adversary manipulates the model's training or fine-tuning phases by
injecting malicious data. Poisoning attacks could be designed to influence the
model's suggestions at run time for chosen contexts, such as inducing the model
into suggesting insecure code payloads. To achieve this, prior poisoning
attacks explicitly inject the insecure code payload into the training data,
making the poisoning data detectable by static analysis tools that can remove
such malicious data from the training set. In this work, we demonstrate two
novel data poisoning attacks, COVERT and TROJANPUZZLE, that can bypass static
analysis by planting malicious poisoning data in out-of-context regions such as
docstrings. Our most novel attack, TROJANPUZZLE, goes one step further in
generating less suspicious poisoning data by never including certain
(suspicious) parts of the payload in the poisoned data, while still inducing a
model that suggests the entire payload when completing code (i.e., outside
docstrings). This makes TROJANPUZZLE robust against signature-based
dataset-cleansing methods that identify and filter out suspicious sequences
from the training data. Our evaluation against two model sizes demonstrates
that both COVERT and TROJANPUZZLE have significant implications for how
practitioners should select code used to train or tune code-suggestion models.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Fuzzers for stateful systems: Survey and Research Directions. (arXiv:2301.02490v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.02490">http://arxiv.org/abs/2301.02490</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.02490] Fuzzers for stateful systems: Survey and Research Directions](http://arxiv.org/abs/2301.02490) #security</code></li>
<li>Summary: <p>Fuzzing is a security testing methodology effective in finding bugs. In a
nutshell, a fuzzer sends multiple slightly malformed messages to the software
under test, hoping for crashes or weird system behaviour. The methodology is
relatively simple, although applications that keep internal states are
challenging to fuzz. The research community has responded to this challenge by
developing fuzzers tailored to stateful systems, but a clear understanding of
the variety of strategies is still missing. In this paper, we present the first
taxonomy of fuzzers for stateful systems and provide a systematic comparison
and classification of these fuzzers.
</p></li>
</ul>

<h3>Title: Deep leakage from gradients. (arXiv:2301.02621v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.02621">http://arxiv.org/abs/2301.02621</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.02621] Deep leakage from gradients](http://arxiv.org/abs/2301.02621) #security</code></li>
<li>Summary: <p>With the development of artificial intelligence technology, Federated
Learning (FL) model has been widely used in many industries for its high
efficiency and confidentiality. Some researchers have explored its
confidentiality and designed some algorithms to attack training data sets, but
these algorithms all have their own limitations. Therefore, most people still
believe that local machine learning gradient information is safe and reliable.
In this paper, an algorithm based on gradient features is designed to attack
the federated learning model in order to attract more attention to the security
of federated learning systems. In federated learning system, gradient contains
little information compared with the original training data set, but this
project intends to restore the original training image data through gradient
information. Convolutional Neural Network (CNN) has excellent performance in
image processing. Therefore, the federated learning model of this project is
equipped with Convolutional Neural Network structure, and the model is trained
by using image data sets. The algorithm calculates the virtual gradient by
generating virtual image labels. Then the virtual gradient is matched with the
real gradient to restore the original image. This attack algorithm is written
in Python language, uses cat and dog classification Kaggle data sets, and
gradually extends from the full connection layer to the convolution layer, thus
improving the universality. At present, the average squared error between the
data recovered by this algorithm and the original image information is
approximately 5, and the vast majority of images can be completely restored
according to the gradient information given, indicating that the gradient of
federated learning system is not absolutely safe and reliable.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Watching your call: Breaking VoLTE Privacy in LTE/5G Networks. (arXiv:2301.02487v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.02487">http://arxiv.org/abs/2301.02487</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.02487] Watching your call: Breaking VoLTE Privacy in LTE/5G Networks](http://arxiv.org/abs/2301.02487) #privacy</code></li>
<li>Summary: <p>Voice over LTE (VoLTE) and Voice over NR (VoNR) are two similar technologies
that have been widely deployed by operators to provide a better calling
experience in LTE and 5G networks, respectively. The VoLTE/NR protocols rely on
the security features of the underlying LTE/5G network to protect users'
privacy such that nobody can monitor calls and learn details about call times,
duration, and direction. In this paper, we introduce a new privacy attack which
enables adversaries to analyse encrypted LTE/5G traffic and recover any
VoLTE/NR call details. We achieve this by implementing a novel mobile-relay
adversary which is able to remain undetected by using an improved physical
layer parameter guessing procedure. This adversary facilitates the recovery of
encrypted configuration messages exchanged between victim devices and the
mobile network. We further propose an identity mapping method which enables our
mobile-relay adversary to link a victim's network identifiers to the phone
number efficiently, requiring a single VoLTE protocol message. We evaluate the
real-world performance of our attacks using four modern commercial
off-the-shelf phones and two representative, commercial network carriers. We
collect over 60 hours of traffic between the phones and the mobile networks and
execute 160 VoLTE calls, which we use to successfully identify patterns in the
physical layer parameter allocation and in VoLTE traffic, respectively. Our
real-world experiments show that our mobile-relay works as expected in all test
cases, and the VoLTE activity logs recovered describe the actual communication
with 100% accuracy. Finally, we show that we can link network identifiers such
as International Mobile Subscriber Identities (IMSI), Subscriber Concealed
Identifiers (SUCI) and/or Globally Unique Temporary Identifiers (GUTI) to phone
numbers while remaining undetected by the victim.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Information Flow Tracking Methods for Protecting Cyber-Physical Systems against Hardware Trojans -- a Survey. (arXiv:2301.02620v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.02620">http://arxiv.org/abs/2301.02620</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.02620] Information Flow Tracking Methods for Protecting Cyber-Physical Systems against Hardware Trojans -- a Survey](http://arxiv.org/abs/2301.02620) #protect</code></li>
<li>Summary: <p>Cyber-physical systems (CPS) provide profitable surfaces for hardware attacks
such as hardware Trojans. Hardware Trojans can implement stealthy attacks such
as leaking critical information, taking control of devices or harm humans. In
this article we review information flow tracking (IFT) methods for protecting
CPS against hardware Trojans, and discuss their current limitations. IFT
methods are a promising approach for the detection of hardware Trojans in
complex systems because the detection mechanism does not necessarily rely on
potential Trojan behavior. However, in order to maximize the benefits research
should focus more on black-box design models and consider real-world attack
scenarios.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Silent Killer: Optimizing Backdoor Trigger Yields a Stealthy and Powerful Data Poisoning Attack. (arXiv:2301.02615v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.02615">http://arxiv.org/abs/2301.02615</a></li>
<li>Code URL: <a href="https://github.com/tzvilederer/silent-killer">https://github.com/tzvilederer/silent-killer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2301.02615] Silent Killer: Optimizing Backdoor Trigger Yields a Stealthy and Powerful Data Poisoning Attack](http://arxiv.org/abs/2301.02615) #attack</code></li>
<li>Summary: <p>We propose a stealthy and powerful backdoor attack on neural networks based
on data poisoning (DP). In contrast to previous attacks, both the poison and
the trigger in our method are stealthy. We are able to change the model's
classification of samples from a source class to a target class chosen by the
attacker. We do so by using a small number of poisoned training samples with
nearly imperceptible perturbations, without changing their labels. At inference
time, we use a stealthy perturbation added to the attacked samples as a
trigger. This perturbation is crafted as a universal adversarial perturbation
(UAP), and the poison is crafted using gradient alignment coupled to this
trigger. Our method is highly efficient in crafting time compared to previous
methods and requires only a trained surrogate model without additional
retraining. Our attack achieves state-of-the-art results in terms of attack
success rate while maintaining high accuracy on clean samples.
</p></li>
</ul>

<h3>Title: Adversarial Attacks on Neural Models of Code via Code Difference Reduction. (arXiv:2301.02412v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.02412">http://arxiv.org/abs/2301.02412</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.02412] Adversarial Attacks on Neural Models of Code via Code Difference Reduction](http://arxiv.org/abs/2301.02412) #attack</code></li>
<li>Summary: <p>Deep learning has been widely used to solve various code-based tasks by
building deep code models based on a large number of code snippets. However,
deep code models are still vulnerable to adversarial attacks. As source code is
discrete and has to strictly stick to the grammar and semantics constraints,
the adversarial attack techniques in other domains are not applicable.
Moreover, the attack techniques specific to deep code models suffer from the
effectiveness issue due to the enormous attack space. In this work, we propose
a novel adversarial attack technique (i.e., CODA). Its key idea is to use the
code differences between the target input and reference inputs (that have small
code differences but different prediction results with the target one) to guide
the generation of adversarial examples. It considers both structure differences
and identifier differences to preserve the original semantics. Hence, the
attack space can be largely reduced as the one constituted by the two kinds of
code differences, and thus the attack process can be largely improved by
designing corresponding equivalent structure transformations and identifier
renaming transformations. Our experiments on 10 deep code models (i.e., two pre
trained models with five code-based tasks) demonstrate the effectiveness and
efficiency of CODA, the naturalness of its generated examples, and its
capability of defending against attacks after adversarial fine-tuning. For
example, CODA improves the state-of-the-art techniques (i.e., CARROT and ALERT)
by 79.25% and 72.20% on average in terms of the attack success rate,
respectively.
</p></li>
</ul>

<h3>Title: Stealthy Backdoor Attack for Code Models. (arXiv:2301.02496v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.02496">http://arxiv.org/abs/2301.02496</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.02496] Stealthy Backdoor Attack for Code Models](http://arxiv.org/abs/2301.02496) #attack</code></li>
<li>Summary: <p>Code models, such as CodeBERT and CodeT5, offer general-purpose
representations of code and play a vital role in supporting downstream
automated software engineering tasks. Most recently, code models were revealed
to be vulnerable to backdoor attacks. A code model that is backdoor-attacked
can behave normally on clean examples but will produce pre-defined malicious
outputs on examples injected with triggers that activate the backdoors.
Existing backdoor attacks on code models use unstealthy and easy-to-detect
triggers. This paper aims to investigate the vulnerability of code models with
stealthy backdoor attacks. To this end, we propose AFRAIDOOR (Adversarial
Feature as Adaptive Backdoor). AFRAIDOOR achieves stealthiness by leveraging
adversarial perturbations to inject adaptive triggers into different inputs. We
evaluate AFRAIDOOR on three widely adopted code models (CodeBERT, PLBART and
CodeT5) and two downstream tasks (code summarization and method name
prediction). We find that around 85% of adaptive triggers in AFRAIDOOR bypass
the detection in the defense process. By contrast, only less than 12% of the
triggers from previous work bypass the defense. When the defense method is not
applied, both AFRAIDOOR and baselines have almost perfect attack success rates.
However, once a defense is applied, the success rates of baselines decrease
dramatically to 10.47% and 12.06%, while the success rate of AFRAIDOOR are
77.05% and 92.98% on the two tasks. Our finding exposes security weaknesses in
code models under stealthy backdoor attacks and shows that the state-of-the-art
defense method cannot provide sufficient protection. We call for more research
efforts in understanding security threats to code models and developing more
effective countermeasures.
</p></li>
</ul>

<h3>Title: Unsupervised attack pattern detection in honeypot data using Bayesian topic modelling. (arXiv:2301.02505v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.02505">http://arxiv.org/abs/2301.02505</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.02505] Unsupervised attack pattern detection in honeypot data using Bayesian topic modelling](http://arxiv.org/abs/2301.02505) #attack</code></li>
<li>Summary: <p>Cyber-systems are under near-constant threat from intrusion attempts. Attacks
types vary, but each attempt typically has a specific underlying intent, and
the perpetrators are typically groups of individuals with similar objectives.
Clustering attacks appearing to share a common intent is very valuable to
threat-hunting experts. This article explores topic models for clustering
terminal session commands collected from honeypots, which are special network
hosts designed to entice malicious attackers. The main practical implications
of clustering the sessions are two-fold: finding similar groups of attacks, and
identifying outliers. A range of statistical topic models are considered,
adapted to the structures of command-line syntax. In particular, concepts of
primary and secondary topics, and then session-level and command-level topics,
are introduced into the models to improve interpretability. The proposed
methods are further extended in a Bayesian nonparametric fashion to allow
unboundedness in the vocabulary size and the number of latent intents. The
methods are shown to discover an unusual MIRAI variant which attempts to take
over existing cryptocurrency coin-mining infrastructure, not detected by
traditional topic-modelling approaches.
</p></li>
</ul>

<h3>Title: Linear and non-linear machine learning attacks on physical unclonable functions. (arXiv:2301.02549v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.02549">http://arxiv.org/abs/2301.02549</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.02549] Linear and non-linear machine learning attacks on physical unclonable functions](http://arxiv.org/abs/2301.02549) #attack</code></li>
<li>Summary: <p>In this thesis, several linear and non-linear machine learning attacks on
optical physical unclonable functions (PUFs) are presented. To this end, a
simulation of such a PUF is implemented to generate a variety of datasets that
differ in several factors in order to find the best simulation setup and to
study the behavior of the machine learning attacks under different
circumstances. All datasets are evaluated in terms of individual samples and
their correlations with each other. In the following, both linear and deep
learning approaches are used to attack these PUF simulations and
comprehensively investigate the impact of different factors on the datasets in
terms of their security level against attackers. In addition, the differences
between the two attack methods in terms of their performance are highlighted
using several independent metrics. Several improvements to these models and new
attacks will be introduced and investigated sequentially, with the goal of
progressively improving modeling performance. This will lead to the development
of an attack capable of almost perfectly predicting the outputs of the
simulated PUF. In addition, data from a real optical PUF is examined and both
compared to that of the simulation and used to see how the machine learning
models presented would perform in the real world. The results show that all
models meet the defined criterion for a successful machine learning attack.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: CyberLoc: Towards Accurate Long-term Visual Localization. (arXiv:2301.02403v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.02403">http://arxiv.org/abs/2301.02403</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.02403] CyberLoc: Towards Accurate Long-term Visual Localization](http://arxiv.org/abs/2301.02403) #robust</code></li>
<li>Summary: <p>This technical report introduces CyberLoc, an image-based visual localization
pipeline for robust and accurate long-term pose estimation under challenging
conditions. The proposed method comprises four modules connected in a sequence.
First, a mapping module is applied to build accurate 3D maps of the scene, one
map for each reference sequence if there exist multiple reference sequences
under different conditions. Second, a single-image-based localization pipeline
(retrieval--matching--PnP) is performed to estimate 6-DoF camera poses for each
query image, one for each 3D map. Third, a consensus set maximization module is
proposed to filter out outlier 6-DoF camera poses, and outputs one 6-DoF camera
pose for a query. Finally, a robust pose refinement module is proposed to
optimize 6-DoF query poses, taking candidate global 6-DoF camera poses and
their corresponding global 2D-3D matches, sparse 2D-2D feature matches between
consecutive query images and SLAM poses of the query sequence as input.
Experiments on the 4seasons dataset show that our method achieves high accuracy
and robustness. In particular, our approach wins the localization challenge of
ECCV 2022 workshop on Map-based Localization for Autonomous Driving
(MLAD-ECCV2022).
</p></li>
</ul>

<h3>Title: OPD@NL4Opt: An ensemble approach for the NER task of the optimization problem. (arXiv:2301.02459v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.02459">http://arxiv.org/abs/2301.02459</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.02459] OPD@NL4Opt: An ensemble approach for the NER task of the optimization problem](http://arxiv.org/abs/2301.02459) #robust</code></li>
<li>Summary: <p>In this paper, we present an ensemble approach for the NL4Opt competition
subtask 1(NER task). For this task, we first fine tune the pretrained language
models based on the competition dataset. Then we adopt differential learning
rates and adversarial training strategies to enhance the model generalization
and robustness. Additionally, we use a model ensemble method for the final
prediction, which achieves a micro-averaged F1 score of 93.3% and attains the
second prize in the NER task.
</p></li>
</ul>

<h3>Title: gRoMA: a Tool for Measuring Deep Neural Networks Global Robustness. (arXiv:2301.02288v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.02288">http://arxiv.org/abs/2301.02288</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.02288] gRoMA: a Tool for Measuring Deep Neural Networks Global Robustness](http://arxiv.org/abs/2301.02288) #robust</code></li>
<li>Summary: <p>Deep neural networks (DNNs) are a state-of-the-art technology, capable of
outstanding performance in many key tasks. However, it is challenging to
integrate DNNs into safety-critical systems, such as those in the aerospace or
automotive domains, due to the risk of adversarial inputs: slightly perturbed
inputs that can cause the DNN to make grievous mistakes. Adversarial inputs
have been shown to plague even modern DNNs; and so the risks they pose must be
measured and mitigated to allow the safe deployment of DNNs in safety-critical
systems. Here, we present a novel and scalable tool called gRoMA, which uses a
statistical approach for formally measuring the global categorial robustness of
a DNN - i.e., the probability of randomly encountering an adversarial input for
a specific output category. Our tool operates on pre-trained, black-box
classification DNNs. It randomly generates input samples that belong to an
output category of interest, measures the DNN's susceptibility to adversarial
inputs around these inputs, and then aggregates the results to infer the
overall global robustness of the DNN up to some small bounded error. For
evaluation purposes, we used gRoMA to measure the global robustness of the
widespread Densenet DNN model over the CIFAR10 dataset and our results exposed
significant gaps in the robustness of the different output categories. This
experiment demonstrates the scalability of the new approach and showcases its
potential for allowing DNNs to be deployed within critical systems of interest.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Super Sparse 3D Object Detection. (arXiv:2301.02562v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.02562">http://arxiv.org/abs/2301.02562</a></li>
<li>Code URL: <a href="https://github.com/tusen-ai/sst">https://github.com/tusen-ai/sst</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2301.02562] Super Sparse 3D Object Detection](http://arxiv.org/abs/2301.02562) #extraction</code></li>
<li>Summary: <p>As the perception range of LiDAR expands, LiDAR-based 3D object detection
contributes ever-increasingly to the long-range perception in autonomous
driving. Mainstream 3D object detectors often build dense feature maps, where
the cost is quadratic to the perception range, making them hardly scale up to
the long-range settings. To enable efficient long-range detection, we first
propose a fully sparse object detector termed FSD. FSD is built upon the
general sparse voxel encoder and a novel sparse instance recognition (SIR)
module. SIR groups the points into instances and applies highly-efficient
instance-wise feature extraction. The instance-wise grouping sidesteps the
issue of the center feature missing, which hinders the design of the fully
sparse architecture. To further enjoy the benefit of fully sparse
characteristic, we leverage temporal information to remove data redundancy and
propose a super sparse detector named FSD++. FSD++ first generates residual
points, which indicate the point changes between consecutive frames. The
residual points, along with a few previous foreground points, form the super
sparse input data, greatly reducing data redundancy and computational overhead.
We comprehensively analyze our method on the large-scale Waymo Open Dataset,
and state-of-the-art performance is reported. To showcase the superiority of
our method in long-range detection, we also conduct experiments on Argoverse 2
Dataset, where the perception range ($200m$) is much larger than Waymo Open
Dataset ($75m$). Code is open-sourced at https://github.com/tusen-ai/SST.
</p></li>
</ul>

<h3>Title: Mask-then-Fill: A Flexible and Effective Data Augmentation Framework for Event Extraction. (arXiv:2301.02427v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.02427">http://arxiv.org/abs/2301.02427</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.02427] Mask-then-Fill: A Flexible and Effective Data Augmentation Framework for Event Extraction](http://arxiv.org/abs/2301.02427) #extraction</code></li>
<li>Summary: <p>We present Mask-then-Fill, a flexible and effective data augmentation
framework for event extraction. Our approach allows for more flexible
manipulation of text and thus can generate more diverse data while keeping the
original event structure unchanged as much as possible. Specifically, it first
randomly masks out an adjunct sentence fragment and then infills a
variable-length text span with a fine-tuned infilling model. The main advantage
lies in that it can replace a fragment of arbitrary length in the text with
another fragment of variable length, compared to the existing methods which can
only replace a single word or a fixed-length fragment. On trigger and argument
extraction tasks, the proposed framework is more effective than baseline
methods and it demonstrates particularly strong results in the low-resource
setting. Our further analysis shows that it achieves a good balance between
diversity and distributional similarity.
</p></li>
</ul>

<h3>Title: Task Aware Feature Extraction Framework for Sequential Dependence Multi-Task Learning. (arXiv:2301.02494v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.02494">http://arxiv.org/abs/2301.02494</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.02494] Task Aware Feature Extraction Framework for Sequential Dependence Multi-Task Learning](http://arxiv.org/abs/2301.02494) #extraction</code></li>
<li>Summary: <p>Multi-task learning (MTL) has been successfully implemented in many
real-world applications, which aims to simultaneously solve multiple tasks with
a single model. The general idea of multi-task learning is designing kinds of
global parameter sharing mechanism and task-specific feature extractor to
improve the performance of all tasks. However, sequential dependence between
tasks are rarely studied but frequently encountered in e-commence online
recommendation, e.g. impression, click and conversion on displayed product.
There is few theoretical work on this problem and biased optimization object
adopted in most MTL methods deteriorates online performance. Besides, challenge
still remains in balancing the trade-off between various tasks and effectively
learn common and specific representation. In this paper, we first analyze
sequential dependence MTL from rigorous mathematical perspective and design a
dependence task learning loss to provide an unbiased optimizing object. And we
propose a Task Aware Feature Extraction (TAFE) framework for sequential
dependence MTL, which enables to selectively reconstruct implicit shared
representations from a sample-wise view and extract explicit task-specific
information in an more efficient way. Extensive experiments on offline datasets
and online A/B implementation demonstrate the effectiveness of our proposed
TAFE.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Learning Personalized Brain Functional Connectivity of MDD Patients from Multiple Sites via Federated Bayesian Networks. (arXiv:2301.02423v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.02423">http://arxiv.org/abs/2301.02423</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.02423] Learning Personalized Brain Functional Connectivity of MDD Patients from Multiple Sites via Federated Bayesian Networks](http://arxiv.org/abs/2301.02423) #federate</code></li>
<li>Summary: <p>Identifying functional connectivity biomarkers of major depressive disorder
(MDD) patients is essential to advance understanding of the disorder mechanisms
and early intervention. However, due to the small sample size and the high
dimension of available neuroimaging data, the performance of existing methods
is often limited. Multi-site data could enhance the statistical power and
sample size, while they are often subject to inter-site heterogeneity and
data-sharing policies. In this paper, we propose a federated joint estimator,
NOTEARS-PFL, for simultaneous learning of multiple Bayesian networks (BNs) with
continuous optimization, to identify disease-induced alterations in MDD
patients. We incorporate information shared between sites and site-specific
information into the proposed federated learning framework to learn
personalized BN structures by introducing the group fused lasso penalty. We
develop the alternating direction method of multipliers, where in the local
update step, the neuroimaging data is processed at each local site. Then the
learned network structures are transmitted to the center for the global update.
In particular, we derive a closed-form expression for the local update step and
use the iterative proximal projection method to deal with the group fused lasso
penalty in the global update step. We evaluate the performance of the proposed
method on both synthetic and real-world multi-site rs-fMRI datasets. The
results suggest that the proposed NOTEARS-PFL yields superior effectiveness and
accuracy than the comparable methods.
</p></li>
</ul>

<h2>fair</h2>
<h2>interpretability</h2>
<h3>Title: Topics as Entity Clusters: Entity-based Topics from Language Models and Graph Neural Networks. (arXiv:2301.02458v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.02458">http://arxiv.org/abs/2301.02458</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.02458] Topics as Entity Clusters: Entity-based Topics from Language Models and Graph Neural Networks](http://arxiv.org/abs/2301.02458) #interpretability</code></li>
<li>Summary: <p>Topic models aim to reveal the latent structure behind a corpus, typically
conducted over a bag-of-words representation of documents. In the context of
topic modeling, most vocabulary is either irrelevant for uncovering underlying
topics or contains strong relationships with relevant concepts, impacting the
interpretability of these topics. Furthermore, their limited expressiveness and
dependency on language demand considerable computation resources. Hence, we
propose a novel approach for cluster-based topic modeling that employs
conceptual entities. Entities are language-agnostic representations of
real-world concepts rich in relational information. To this end, we extract
vector representations of entities from (i) an encyclopedic corpus using a
language model; and (ii) a knowledge base using a graph neural network. We
demonstrate that our approach consistently outperforms other state-of-the-art
topic models across coherency metrics and find that the explicit knowledge
encoded in the graph-based embeddings provides more coherent topics than the
implicit knowledge encoded with the contextualized embeddings of language
models.
</p></li>
</ul>

<h3>Title: DANLIP: Deep Autoregressive Networks for Locally Interpretable Probabilistic Forecasting. (arXiv:2301.02332v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.02332">http://arxiv.org/abs/2301.02332</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.02332] DANLIP: Deep Autoregressive Networks for Locally Interpretable Probabilistic Forecasting](http://arxiv.org/abs/2301.02332) #interpretability</code></li>
<li>Summary: <p>Despite the high performance of neural network-based time series forecasting
methods, the inherent challenge in explaining their predictions has limited
their applicability in certain application areas. Due to the difficulty in
identifying causal relationships between the input and output of such black-box
methods, they rarely have been adopted in domains such as legal and medical
fields in which the reliability and interpretability of the results can be
essential. In this paper, we propose \model, a novel deep learning-based
probabilistic time series forecasting architecture that is intrinsically
interpretable. We conduct experiments with multiple datasets and performance
metrics and empirically show that our model is not only interpretable but also
provides comparable performance to state-of-the-art probabilistic time series
forecasting methods. Furthermore, we demonstrate that interpreting the
parameters of the stochastic processes of interest can provide useful insights
into several application areas.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
